<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/333922</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Scientific Communication</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>SciReader: A Cloud-based Recommender System for Biomedical Literature</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Desai</surname><given-names>Priya</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Telis</surname><given-names>Natalie</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Lehmann</surname><given-names>Ben</given-names></name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Bettinger</surname><given-names>Keith</given-names></name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Pritchard</surname><given-names>Jonathan K.</given-names></name>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Datta</surname><given-names>Somalee</given-names></name>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Stanford Center for Genetics and Personalized Medicine Stanford University Palo Alto</institution>, CA, <email>prd@stanford.edu</email></aff>
<aff id="a2"><label>2</label><institution>Department of Biomedical Informatics Stanford University Palo Alto</institution>, CA, <email>natalie.telis@gmail.com</email></aff>
<aff id="a3"><label>3</label><institution>Department of Physics University of Santa Cruz Santa Cruz</institution>, CA, <email>blehmann@ucsc.edu</email></aff>
<aff id="a4"><label>4</label><institution>Stanford Center for Genetics and Personalized Medicine Stanford University Palo Alto</institution>, CA, <email>bettingr@stanford.edu</email></aff>
<aff id="a5"><label>5</label><institution>Department of Genetics and Biology Stanford University Palo Alto</institution>, CA, <email>pritch@stanford.edu</email></aff>
<aff id="a6"><label>6</label><institution>Stanford Center for Genetics and Personalized Medicine Stanford University Palo Alto</institution>, CA, <email>somalee@stanford.edu</email></aff>
</contrib-group>
<pub-date pub-type="epub">
<year>2018</year>
</pub-date>
<elocation-id>333922</elocation-id>
<history>
<date date-type="received">
<day>29</day>
<month>5</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>29</day>
<month>5</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>30</day>
<month>5</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="333922.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>With the growing number of biomedical papers published each year, keeping up with relevant literature has become increasingly important, and yet more challenging. SciReader (<ext-link ext-link-type="uri" xlink:href="http://www.scireader.com">www.scireader.com</ext-link>) is a cloud-based personalized recommender system that specifically aims to assist biomedical researchers and clinicians identify publications of interest to them. SciReader uses topic modeling and other machine learning algorithms to provide users with recommendations that are recent, relevant, and of high quality<sup><xref rid="fn1" ref-type="fn">1</xref></sup>.</p>
</abstract>
<counts>
<page-count count="8"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<label>I.</label>
<title>INTRODUCTION</title>
<p>Today&#x2019;s researchers have access to large online scientific repositories of literature and face the monumental challenge of sifting through the vast amounts of available information to find articles relevant to their interests. These digital archives are growing rapidly as new articles are put online and old publications are scanned, indexed, and made digitally available. While keeping up with the ongoing work in their discipline is an essential task for scientists, it can be tedious and time-consuming. Furthermore, with the growing number of publications, it is nearly impossible to access and track the most relevant and promising research articles. This points to the need for new technology solutions that enable researchers to access publications central to their disciplines, understand the current trends, and collect references for their own research. While, historically, researchers have found articles by following citations in other articles of interest, today&#x2019;s fast-paced research world makes that method cumbersome and insufficiently comprehensive. Today, the most common strategies for finding relevant articles are through keyword-based web searches, Google Scholar searches, or by scanning through RSS feeds from popular journals. Sites like <italic>CiteU-Like</italic>[<xref rid="c11" ref-type="bibr">11</xref>] and <italic>Mendeley</italic>[<xref rid="c17" ref-type="bibr">17</xref>] allow researchers to create their own reference libraries for articles they are interested in and enable sharing with peers[<xref rid="c13" ref-type="bibr">13</xref>]. These shareable libraries and vast amounts of literature makes this field ripe for using a fil-tration system&#x2014;and a good recommender system[<xref rid="c9" ref-type="bibr">9</xref>] (which filters out the least relevant articles) could be invaluable.</p>
<p>Recommender systems have long been widely used in e-commerce as valuable tools for personalizing purchase recommendations. <italic>Amazon</italic> and <italic>Netflix</italic> suggest products and movies based on each user&#x2019;s profile, previous purchase history, and online behavior. <italic>Facebook</italic> and <italic>Google</italic> news recommend news articles based on user profiles and behavior. As the amount of information available grows, personalized recommendation systems have become crucial tools for finding useful content.</p>
</sec>
<sec id="s2">
<label>II.</label>
<title>B<sc>ACKGROUND AND</sc> R<sc>ELATED</sc> W<sc>ORK</sc></title>
<p>While an explosion of available scientific literature is occurring in many fields, we focus specifically on the area of biomedical literature (<xref rid="fig1" ref-type="fig">Fig 1</xref>). In the burgeoning field of biomedicine, a staggering 3,000-5,000 papers are published every day (<xref rid="fig2" ref-type="fig">Fig 2</xref>). It is not practical to browse through so many articles to identify those that may be relevant. For newcomers to the field, it is hard to establish a baseline understanding of seminal work. New journals are popping up on a regular basis[<xref rid="c27" ref-type="bibr">27</xref>]. Good recommender systems can play a valuable role in winnowing down publications of interest.There are primarily two kinds of recommender systems:
<fig id="fig1" position="float" fig-type="figure">
<label>Fig. 1.</label>
<caption><p>Number of Papers Uploaded to PubMed per Year.</p></caption>
<graphic xlink:href="333922_fig1.tif"/>
</fig>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig. 2.</label>
<caption><p>Number of Papers Uploaded to PubMed per day between 3-20-2018 and 4-20-2018.</p></caption>
<graphic xlink:href="333922_fig2.tif"/>
</fig></p>
<list list-type="order">
<list-item><p>Content-based recommender systems, and</p></list-item>
<list-item><p>Collaborative filtering-based recommender systems.</p></list-item>
</list>
<p>Content-based filtering methods are usually based on a description of the item and a profile of the user&#x2019;s preferences. They utilize discrete characteristics (attributes) of an item in order to recommend additional items with similar properties.These algorithms recommend items that are similar to those that the user liked in the past. An important drawback of content-based filtering is that the system is unable to interpret user preferences in one context and use them across other content types. It tends to recommend only similar items and the recommendations have very little novelty factor. <italic>Rotten Tomatoes</italic>[<xref rid="c14" ref-type="bibr">14</xref>] and <italic>Pandora Radio</italic>[<xref rid="c16" ref-type="bibr">16</xref>] are popular content-based recommendation systems.</p>
<p>Collaborative filtering-based approaches[<xref rid="c10" ref-type="bibr">10</xref>] typically build models based on user profiles, user&#x2019;s past behaviors (items previously purchased or selected and/or numerical ratings given to those items), and the behavior of similar users to recommend items[<xref rid="c15" ref-type="bibr">15</xref>]. A key advantage of the collaborative filtering approach is that it does not require a large number of item attributes. However, it suffers from the cold start problem &#x2013; it typically requires a large amount of user data to make meaningful recommendations. <italic>Last.fm</italic>[<xref rid="c30" ref-type="bibr">30</xref>] is a music station that primarily uses the collaborative filtering approach.</p>
<p>Modern recommenders often follow a <italic>hybrid</italic>[<xref rid="c6" ref-type="bibr">6</xref>] approach, employing both content-based and collaborative-filtering-based techniques. <italic>Netflix</italic>[<xref rid="c18" ref-type="bibr">18</xref>] is a good example of a hybrid recommender system.</p>
<p>In recent years, a number of websites that can provide scientific recommendations have been developed: e.g. Scien-stein[<xref rid="c6" ref-type="bibr">6</xref>], Google Scholar[<xref rid="c32" ref-type="bibr">32</xref>], PubChase[<xref rid="c20" ref-type="bibr">20</xref>], Mendeley[<xref rid="c13" ref-type="bibr">13</xref>], [<xref rid="c17" ref-type="bibr">17</xref>], Sparrho[<xref rid="c31" ref-type="bibr">31</xref>] and others. Many of these sites focus mainly on citation counts and classic text mining strategies, each of which has advantages and potential pitfalls, thus limiting its general usability. For example, citation databases are often incomplete, and have issues disambiguating homo-graphs. They may also be subject to the Matthew Effect[<xref rid="c21" ref-type="bibr">21</xref>],which is the sociological phenomenon that eminent scientists will often get more credit than a comparatively unknown researcher, even if their work is similar. Text-based recommenders often have trouble with synonyms and context-based words, and often cannot identify papers that may be related.</p>
</sec>
<sec id="s3">
<label>III.</label>
<title>S<sc>CI</sc>R<sc>EADER</sc></title>
<sec id="s3a">
<label>A.</label>
<title>An Overview</title>
<p>SciReader is a hybrid recommender system and takes advantage of both the representation of the content as well as the similarities among users. While most biomedical recommender systems like PubChase focus on finding new items, we believe that sharing, discussing, and reviewing papers are an integral part of a scientist&#x2019;s professional activities, and these motivations are reflected in our requirements for the SciReader site. Our general goals for SciReader are:</p>
<list list-type="roman-lower">
<list-item><p>The articles recommended should be relevant to the user.</p></list-item>
<list-item><p>The articles should be fairly recent. While we believe it is important to be able to recommend older relevant articles, our immediate motivation is to address the problem of current data deluge.</p></list-item>
<list-item><p>Users should be able to view, comment, and share via email and social media, articles they find relevant and interesting.</p></list-item>
<list-item><p>Users should be able to bookmark articles they like or plan to read and should be able to organize articles from different projects.</p></list-item>
<list-item><p>Twitter, which is increasingly used to announce new results and publications, should be integrated into SciReader.</p></list-item>
<list-item><p>It should be to useful to newcomers in the field &#x2013; not just established scientists with many publications. It should allow users to easily navigate the extensive body of biomedical literature.</p></list-item>
</list>
<p>The two main parts of this system:</p>
<list list-type="roman-lower">
<list-item><p>Recommendation Algorithm</p></list-item>
<list-item><p>User-system Interaction</p></list-item>
</list>
<p>are discussed in the following sections. <xref rid="fig3" ref-type="fig">Fig 3</xref> below is an overview of the SciReader recommender system.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig. 3.</label>
<caption><p>The Recommendation system:An Overview</p></caption>
<graphic xlink:href="333922_fig3.tif"/>
</fig>
</sec>
<sec id="s3b">
<label>B.</label>
<title>Recommendation Algorithm</title>
<p>Generating recommendations is a multi-step matching and filtering process that combines multiple algorithmic approaches as described below.</p>
<list list-type="order">
<list-item><p><italic>Probabilistic Topic Models:</italic> Topics can be thought of as &#x201C;recurring patterns of co-occurring words&#x201D;[<xref rid="c28" ref-type="bibr">28</xref>], and a topic model is a type of statistical model often used to discover abstract topics that occur in a collection of documents[<xref rid="c1" ref-type="bibr">1</xref>], [<xref rid="c2" ref-type="bibr">2</xref>]. Topic models are frequently used to discover hidden semantic structures in a large text body[<xref rid="c7" ref-type="bibr">7</xref>], [<xref rid="c8" ref-type="bibr">8</xref>]. One of the most commonly used topic models is latent Dirichlet allocation (LDA)[<xref rid="c2" ref-type="bibr">2</xref>]. LDA is a matrix factorization technique and assumes documents are produced from a mixture of topics. Unlike a clustering algorithm, where each document would be assigned to one cluster, LDA allows documents to belong to multiple topics with varying probability. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place. LDA specifies a <italic>generative process</italic>, an imaginary probabilistic recipe that produces both the hidden topic structure and the observed words of the texts. Those topics then generate words based on their probability distribution.</p>
<p>For example: Assume there are <italic>K</italic> topics <italic>&#x03B2;</italic>= <italic>&#x03B2;</italic><sub>1:<italic>K</italic></sub>, each of which is a distribution over a fixed vocabulary. The generative process of LDA is as follows:</p>
<p>For each article <italic>w</italic><sub><italic>j</italic></sub> in the corpus,</p>
<list list-type="order">
<list-item><p>Draw topic proportions <italic>&#x03B8;</italic><sub><italic>j</italic></sub> &#x223C; Dirichlet(<italic>&#x03B1;</italic>).</p></list-item>
<list-item><p>For each word <italic>n</italic>, (a)</p>
<list list-type="alpha-lower">
<list-item><p>Draw topic assignment <italic>z</italic><sub><italic>jn</italic></sub>&#x223C; Mult(<italic>&#x03B8;</italic><sub><italic>j</italic></sub>).</p></list-item>
<list-item><p>Draw word <inline-formula><alternatives><inline-graphic xlink:href="333922_inline1.gif"/></alternatives></inline-formula>.</p></list-item>
</list></list-item>
</list>
<p>This process reveals how the topic proportions are document-specific, but the set of topics is shared by the corpus.</p>
<p>Topic modeling algorithms perform what is called probabilistic inference. Given a collection of texts, they reverse the imaginary generative process to answer the question: &#x201C;What is the likely hidden topical structure that generated my observed documents?&#x201D; The posterior distribution (or maximum likelihood estimate) of the topics reveals the K topics that likely generated its documents [<xref rid="c23" ref-type="bibr">23</xref>], [<xref rid="c24" ref-type="bibr">24</xref>].</p></list-item>
<list-item><p><italic>PubMed Topic Model:</italic> The cornerstone of SciReader is its topic model of PubMed. PubMed (<ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed">http://www.ncbi.nlm.nih.gov/pubmed</ext-link>) is a free search engine maintained by the NIH primarily accessing the MEDLINE database of references and abstracts on life sciences and biomedical topics. It provides a fairly comprehensive coverage of all articles published in biomedicine. Metadata information about each article, including authors, title, institution, and abstract, is freely available. We used titles, abstracts, and keywords from all articles available in PubMed and published in 2012 to create our document collection (or text corpus) and then generate topics. Only abstracts (not full text) were chosen because they are freely available for all articles. Full texts are not available for most publications.</p>
<p>We use MALLET (Machine Learning for Language Toolkit)[<xref rid="c5" ref-type="bibr">5</xref>], [<xref rid="c22" ref-type="bibr">22</xref>], a Java-based package for statistical natural language processing developed at the University of Massachusetts at Amherst, to run an implementation of LDA. The corpus from PubMed that we used contained approximately one million unique articles, and for each article, we concatenated its title, abstract, and keywords. We applied heuristics to clean up the data, like removing standard English stop words (like &#x201C;a&#x201D;, &#x201C;do&#x201D;, and &#x201C;these&#x201D;)and word stemming. To generate topics, we used the vector space model representation of the corpus[<xref rid="c33" ref-type="bibr">33</xref>]. The final training corpus had approximately 650,000 distinct words and was used to generate a topic inferencer which classified biomedical papers into 150 topic areas. The number of topics was determined after much manual parameter adjustments and based on multiple factors including Silhouette clustering[<xref rid="c26" ref-type="bibr">26</xref>] and perplexity[<xref rid="c25" ref-type="bibr">25</xref>].</p>
<p>For ease of use, we manually examined the LDA output to provide an informative name for each automatically defined topic. This inspection was done by examining the most frequent words in the topic, the typical journals where articles with a high probability of this topic were published, and the actual articles containing a high probability of that topic. Some topics which seemed to capture a large number of non-english words and had a noticeably fewer documents with high probabilities were dropped in the final topic display(<xref rid="fig7" ref-type="fig">Fig 7a</xref>). <xref rid="fig4" ref-type="fig">Fig 4</xref> displays the most frequent words of a topic as a word cloud along with the curated topic name. The size of the word in the word cloud is proportional to the relative frequency of the word in that topic. The topic names are human-curated.
<fig id="fig4" position="float" fig-type="figure">
<label>Fig. 4.</label>
<caption><p>Examples of Topics generated by LDA:</p></caption>
<graphic xlink:href="333922_fig4.tif"/>
</fig></p>
<p>For better navigation on the website, we further grouped these topics manually into twenty <italic>supertopics</italic>. <xref rid="fig5" ref-type="fig">Fig 5</xref> is an example where <italic>Genetics and Genomics</italic> is the supertopic created to contain seven sub-topics, each of which were generated by the algorithm.
<fig id="fig5" position="float" fig-type="figure">
<label>Fig. 5.</label>
<caption><p>Supertopics and Corresponding sub-topics</p></caption>
<graphic xlink:href="333922_fig5.tif"/>
</fig></p>
<p>The topic inferencer was stored and is used to infer topics for all new documents. All the older articles in PubMed have also been processed using this inferencer, and stored in terms of their topics. Thus, each paper is classified into one or more topics by using the same LDA inferencer model with fixed topic definitions. This topic model can be considered to be an interpretable, low-dimensional representation of PubMed and we exploit this representation to find <italic>topically similar</italic> papers.</p></list-item>
<list-item><p><italic>Tf-idf and cosine similarity:</italic> As part of preprocessing, all articles or documents are converted into a vector space representation matrix. The documents are tokenized using the same <italic>&#x2248;</italic> 650,000 word dictionary that was generated while creating the LDA inferencer. We then apply the tf-idf normalization[<xref rid="c34" ref-type="bibr">34</xref>], [<xref rid="c35" ref-type="bibr">35</xref>] to the matrix of documents and use that representation to calculate the cosine distance between the documents. The tf-idf weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus by assigning a higher weight to rarer words in a collection of documents. It can be expressed as:
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="333922_eqn1.gif"/></alternatives>
</disp-formula>
where <italic>tf</italic><sub><italic>t,d</italic></sub> is the number of occurrences of term <italic>t</italic> in document <italic>d</italic>, <italic>N</italic> is the total number of documents in a collection and <italic>df</italic><sub><italic>t</italic></sub> is the document frequency of term <italic>t</italic>.</p>
<p>Document similarity or distance between documents is a standard method used in information retrieval to measure how semantically similar two documents are[<xref rid="c33" ref-type="bibr">33</xref>], [<xref rid="c35" ref-type="bibr">35</xref>]. While there are different distance metrics (Jaccardean, Euclidean, Manhattan) that can be used, cosine measure, which is the Euclidean dot product between the vector representation of the two documents, is frequently used as a measure of document similarity and the metric that gave us the best results.
<disp-formula id="ueqn1">
<alternatives><graphic xlink:href="333922_ueqn1.gif"/></alternatives>
</disp-formula>
</p>
<p>The <italic>smaller</italic> the distance, the more <italic>similar</italic> the documents. To make recommendations, we generate matrices: one using the articles in the user&#x2019;s library, and the other using the articles in the corpus, and then we find the papers most similar to the user&#x2019;s library in the corpus.</p></list-item>
</list>
</sec>
<sec id="s3c">
<label>C.</label>
<title>User-System Interaction</title>
<p>Since SciReader is focused on providing personalized recommendations in the biomedical literature space, we need to gather a variety of different kinds of information about the user to understand their specific research interests. When a user initially registers with the site, they are led through a series of questions and asked to provide at least one of the following:</p>
<list list-type="order">
<list-item><p>topics of interest from the predefined list of <italic>&#x2248;</italic> 150 topics. These are the topics generated by the topic modeling algorithm as discussed above (<xref rid="fig7" ref-type="fig">Fig 7b</xref>).</p></list-item>
<list-item><p>keywords that describe their research interests(entered as free-format text) (<xref rid="fig7" ref-type="fig">Fig 7b</xref>).</p></list-item>
<list-item><p>authors of interest.</p></list-item>
<list-item><p>an existing reference library/bibliography (<xref rid="fig7" ref-type="fig">Fig 7c</xref>).</p></list-item>
<list-item><p>a personal publication list (<xref rid="fig7" ref-type="fig">Fig 7c</xref>).</p></list-item>
</list>
<p>All users are provided with a couple of basic libraries: <italic>Reading List</italic> and <italic>My Publications</italic>. These libraries are meant to serve as placeholders for the users (yet to be populated) reading list and publications. As useful papers from the recommendation list are found, they can be added to the reading list. The users are free to create as many more libraries as they need to organize their recommendations and references. The site has a built-in interface with PubMed, so a user can easily search for their own and other publications and add them to their libraries. The central idea is that newcomers to the field can get a general flavor of the current publications by providing only topics and keywords of interest. They can then peruse the recommendations by topic which are updated daily, and slowly build their digital library. Established practitioners of the field who may already have personal reference libraries can upload them right away (<xref rid="fig7" ref-type="fig">Fig 7c</xref>) in addition to providing basic keywords and topics. All this information gives the recommender more data to work with and helps provide more personalized recommendations.</p>
<p>A user can interact with the site by liking, upvoting, sharing or commenting on an article. These modes of feedback provide the recommender with valuable information about the user&#x2019;s current interests. Our philosophy is that a small amount of initial information can assist us with identifying general areas of interest, but after that, the user&#x2019;s behavior within the site will be most helpful for fine-tuning the recommendations. For example, we encourage users to create separate libraries for different projects, because separate libraries allow the recommender to tailor its recommendations to each project. Recommendations are created for each individual library and an <italic>Overall</italic> recommendation set is compiled of the recommendations from all the libraries. SciReader tracks every time a recommended paper is uploaded into one of the user&#x2019;s libraries and records all interactions the user has with the site.</p>
<p>In addition to paper recommendations, SciReader also provides users with regular Twitter updates relating to papers in their topics of interest. We monitor Twitter accounts from most prominent journals and publications and crosslink them with articles from PubMed. Tweets that refer to publications are then displayed as part of the users recommendation feed. <xref rid="fig6" ref-type="fig">Fig 6</xref> describes the typical user workflow and <xref rid="fig7" ref-type="fig">Figs 7(a-c)</xref> show different aspects of the user portal.
<fig id="fig6" position="float" fig-type="figure">
<label>Fig. 6.</label>
<caption><p>Typical user workflow.</p></caption>
<graphic xlink:href="333922_fig6.tif"/>
</fig>
<fig id="fig7" position="float" fig-type="figure">
<label>Fig. 7.</label>
<caption><p>Different facets of the User portal</p></caption>
<graphic xlink:href="333922_fig7.tif"/>
</fig></p>
</sec>
<sec id="s3d">
<label>D.</label>
<title>Finding relevant papers</title>
<p>We assume that a user <italic>k</italic> has a library <italic>L</italic> of <italic>n</italic> papers, and that we want to compare these against a corpus <italic>C</italic>,containing <italic>m</italic> new papers. We use a filtering step to prepare recommendation set <italic>R</italic>, in which we only include papers in <italic>C</italic> if they have substantial topic overlap with papers in <italic>L</italic>. This intersection is done by comparing the topic representations of papers in the user&#x2019;s library versus the corpus. Next, we compute a word similarity matrix <italic>W</italic><sub><italic>ij</italic></sub>, between each paper <italic>&#x2208; l</italic><sub><italic>I,</italic></sub> <italic>L</italic> and <italic>c</italic><sub><italic>j</italic></sub>, <italic>&#x2208;C</italic>. The entries in <italic>W</italic> are in [0, 1] with 1 indicating complete similarity, and 0 indicating complete difference. We then identify papers in <italic>C</italic> that are especially close to at least some papers in <italic>L</italic>. We use the following distance function to assign a score to each paper <italic>c</italic><sub><italic>j</italic></sub>, as follows:
<disp-formula id="ueqn2">
<alternatives><graphic xlink:href="333922_ueqn2.gif"/></alternatives>
</disp-formula>
where <italic>d</italic>(<italic>l</italic><sub><italic>i</italic></sub>) is a function of the date of publication of paper <italic>l</italic><sub><italic>I</italic></sub> such that similarity to older papers in the library is down weighted (since those may reflect less recent user interests), and <italic>&#x03B1;&#x2264;</italic> 1 is a constant that upweights high similarities. Our rationale here is that medium values of <italic>&#x03B1;&#x2264;</italic> 1 result in upweighting of papers which are overall similar to clusters of papers in the library, rather than picking either papers that have similarity to everything but not very similar to anything, or papers that are extremely similar to one paper only.</p>
<p>The impact factor of a journal is frequently used as a proxy for the relative importance of a journal within its field. We upweight papers from journals with higher impact factors, as well as journals which published the papers in the user&#x2019;s library and contain the keywords provided by the user. Furthermore, we would like our recommendations to be strongly biased towards more recent papers. We have developed a function that models these characteristics which we use to rank papers. &#x2018;&#x2019;</p>
<p>Mentions of papers in social media such as Twitter or blogs can be a leading indicator of the future importance of a paper. We search Twitter accounts for URLs that link to scientific papers and match these to papers in our database. For each paper, we record all tweets that are positively matched as referring to that paper. These data are currently summarized simply as the number of tweets referring to any given paper, but, in the future, we will consider more complicated weighting schemes, such as estimates of the tweeter quality based on past performance at identifying good papers.</p>
<p>We are further refining our prediction algorithm to in-corporate a variety of features to predict how important a paper is likely to become. Currently, the key features used by our prediction algorithm include journal impact factor and Tweet&#x002B;Like counts. Citation counts are frequently used as a measure of the importance of papers. However, it usually takes more than a year before a paper starts to accumulate citations, rendering this approach inapplicable for recommending new literature. We are working on developing an author and institution quality score based on citation count data and author position.</p>
</sec>
<sec id="s3e">
<label>E.</label>
<title>Putting it all together: Creating recommendations</title>
<p>We routinely download new papers from PubMed to a local database. All new papers are then processed through the LDA inferencer and stored in a separate table. As part of pre-processing, a <italic>current corpus</italic> matrix of all the papers published the last 90 days is generated, using both the topic-model representation, as well as the tf-idf normalized form. There are typically between 275,000 - 400,000 papers in the current corpus. The dictionary used to tokenize the papers is the same one that was used in generating the LDA inferencer and is thus common for all papers. This <italic>current corpus</italic> is then used to generate all the daily and immediate recommendations.</p>
<p>All the user libraries are similarly processed and stored using the topic-model representation as well as the tf-idf normalized matrix representation using the same word vo-cabulary (dictionary). Whenever a user adds/removes papers from any of their libraries, or creates a new library, the matrices are updated accordingly. When a user uploads a new library, the <italic>immediate recommendations</italic> function is triggered. The user library is pre-processed and topic comparison is done between the library papers and current corpus. We filter out papers from the current corpus that don&#x2019;t pass a minimum threshold of similarity with the user&#x2019;s library, thus ensuring that only topically similar papers are further considered for recommendation. We then compute the word-similarity matrix between the user&#x2019;s library and this <italic>filtered current corpus</italic> and generate a score for each paper. The top <italic>n</italic> (typically a few hundred) papers are taken and only these will be used to generate the final recommendations. These <italic>n</italic> papers are then re-scored using the weighting function that factors in the date of publication, journal impact factor, journals included in the user&#x2019;s library, keywords, etc., and re-ranked. The papers are sorted by this new ranking and then displayed as the user&#x2019;s recommendations. Overall recommendations are a compilation of recommendations from all the libraries for a specific user, re-ranked using certain criteria. The process for generating daily recommendations is similar except that we run this recommendation for all libraries and for all users. <xref rid="fig8" ref-type="fig">Fig 8</xref> shows a schematic workflow and <xref rid="fig9" ref-type="fig">Fig 9</xref> shows a screenshot of a typical users library recommendations.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Fig. 8.</label>
<caption><p>Recommendation workflow</p></caption>
<graphic xlink:href="333922_fig8.tif"/>
</fig>
<fig id="fig9" position="float" fig-type="figure">
<label>Fig. 9.</label>
<caption><p>Individual library recommendations.</p></caption>
<graphic xlink:href="333922_fig9.tif"/>
</fig>
<p>In addition to paper recommendations, we also provide a daily updated tweet feed. As stated earlier, we have created a database linking tweets and the digital object identifier (<italic>doi</italic>) of the publications and keep track of how many times a publication is tweeted or re-tweeted about. Since the publication has been through our topic inferencer, we know the main topics in the publication, and thus can assign the tweet the corresponding topic(s). The tweet is thus labeled as belonging to a certain topic. The tweet feed is a list of tweets that refer to papers that refer to the users preferred topics and had the highest number of tweets. Tweets that do not refer to a specific topic, but are tweeted about very frequently are labeled as &#x201C;General&#x201D;. Thus, each user gets a Twitter feed uniquely personalized to them. <xref rid="fig10" ref-type="fig">Fig 10</xref> shows a typical Twitter feed.</p>
<fig id="fig10" position="float" fig-type="figure">
<label>Fig. 10.</label>
<caption><p>Screenshot of a user&#x2019;s Twitter feed.</p></caption>
<graphic xlink:href="333922_fig10.tif"/>
</fig>
<p>We also generate a weekly digest of recommendations which emailed to each user. This digest is comprised of the papers that were at the top of their recommendation feed for that week. All the data processing on the backend is done using python and the web interface uses the Django 1.7 framework.</p>
</sec>
<sec id="s3f">
<label>F.</label>
<title>Optimizing Calculations</title>
<p>Providing on demand, instantaneous recommendations is computationally intensive and input/output of data can time-consuming. The actual word-similarity calculation often in-volves multiplications between matrices of size <italic>&#x2248;</italic> 650,000 &#x00D7; 15,000 and, reading these in and performing the calculation can be both computationally and memory intensive. However, since our vector space model matrices are sparse, we can use the sparse matrix format to significantly reduce the memory footprint. Hierarchical Data Format version 5 (HDF5) [<xref rid="c36" ref-type="bibr">36</xref>] is a great mechanism for storing large quantities of numerical data which can be read in rapidly and allow for sub-setting and partial I/O of datasets. We use the H5py package/PyTables, a pythonic interface to the HDF5 binary data format which allows easy manipulation of data. The sparse tf-idf matrices are stored in the hdf5 format and calculations done using Scipy are coded in C/C&#x002B;&#x002B;, thus significantly speeding performance. cPickle written in C is a python module that implements a fundamental, but powerful algorithm for serializing and de-serializing a Python object structure and is used to store the word token dictionary. We pre-calculate as many of the computations as possible so the recommendations can be generated efficiently.</p>
<fig id="fig11" position="float" fig-type="figure">
<label>Fig. 11.</label>
<caption><p>Library recommendations for a specific user.</p></caption>
<graphic xlink:href="333922_fig11.tif"/>
</fig>
</sec>
<sec id="s3g">
<label>G.</label>
<title>Cloud Architecture</title>
<p>SciReader was initially developed on Amazon Web Services (AWS) but has since been migrated to the Google Cloud Platform. <xref rid="fig12" ref-type="fig">Fig 12</xref> shows the system architecture on Google Cloud. The user data as well as the PubMed metadata is stored in a MySQL database on the cloud. Google buckets store the precomputed matrices and the computations are done on a Google Compute Engine virtual machine. The website is hosted on a small virtual instance and an elastic load balancer is used to spin up more instances based on demand.</p>
<fig id="fig12" position="float" fig-type="figure">
<label>Fig. 12.</label>
<caption><p>A schematic diagram of the current SciReader architecture on Google Cloud.</p></caption>
<graphic xlink:href="333922_fig12.tif"/>
</fig>
</sec>
<sec id="s3h">
<label>H.</label>
<title>Discussion and Future Work</title>
<p>SciReader is a fully functional recommender system for biomedical literature and it can be accessed at <ext-link ext-link-type="uri" xlink:href="http://www.scireader.com">http://www.scireader.com</ext-link>. It currently has <italic>&#x2248;</italic> 1500 unique active users. We have conducted multiple informal focus groups to better understand our user base, and based on the feedback, we are working on improving and adding certain features and functionalities to the site. We are also planning on adding content from NIH RePORTER, arXiv, and bioRxiv to the recommendations. Further, we are developing a SciReader API so that the highly annotated database containing the topic representation for all biomedical articles used by SciReader can be accessible to other researchers for future bibliometric and longitudinal studies. In addition to improving the speed to generating recommendations, we hope to provide <italic>user-tunable recommendations</italic> where the user could explicitly choose which criteria to use to generate their personalized recommendations.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>A<sc>CKNOWLEDGMENTS</sc></title>
<p>This project was initially developed in the Pritchard Lab and supported by funding from Stanford University and the Howard Hughes Medical Institute. It is now maintained by SCGPM. We thank Yonggan Wu for his help in building the SciReader website.</p>
</ack>
<ref-list>
<title>R<sc>EFERENCES</sc></title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><surname>Pritchard</surname>, <given-names>J. K.</given-names></string-name>, <string-name><surname>Stephens</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Donnelly</surname>, <given-names>P.</given-names></string-name> (<year>2000</year>). <article-title>Inference of population structure using multilocus genotype data</article-title>. <source>Genetics</source>, <volume>155</volume>(<issue>2</issue>), <fpage>945</fpage>&#x2013;<lpage>959</lpage>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><string-name><surname>Blei</surname>, <given-names>D. M.</given-names></string-name>, <string-name><surname>Ng</surname>, <given-names>A. Y.</given-names></string-name>, &#x0026; <string-name><surname>Jordan</surname>, <given-names>M. I.</given-names></string-name> (<year>2003</year>). <article-title>Latent dirichlet allocation</article-title>. <source>Journal of machine Learning research</source>, <volume>3</volume>(Jan), <fpage>993</fpage>&#x2013;<lpage>1022</lpage>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><surname>Blei</surname>, <given-names>D. M.</given-names></string-name> (<year>2012</year>). <article-title>Probabilistic topic models</article-title>. <source>Communications of the ACM</source>, <volume>55</volume>(<issue>4</issue>), <fpage>77</fpage>&#x2013;<lpage>84</lpage>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>C.</given-names></string-name>, &#x0026; <string-name><surname>Blei</surname>, <given-names>D. M.</given-names></string-name> (<year>2011</year>, August). <article-title>Collaborative topic modeling for recommending scientific articles</article-title>. <source>In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</source> (pp. <fpage>448</fpage>&#x2013;<lpage>456</lpage>). ACM.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="website"><string-name><surname>McCallum</surname>, <given-names>Andrew Kachites</given-names></string-name>. <source>&#x201C;MALLET: A Machine Learning for Language Toolkit.&#x201D;</source> <ext-link ext-link-type="uri" xlink:href="http://mallet.cs.umass.edu">http://mallet.cs.umass.edu</ext-link>. <year>2002</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><string-name><surname>Gipp</surname>, <given-names>B.</given-names></string-name>,<string-name><surname>Beel</surname>, <given-names>J.</given-names></string-name>, <string-name><given-names>Christian</given-names> <surname>Hentschel</surname></string-name>, <source>Scienstein: A Research Paper Recommender System</source></mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="website"><string-name><given-names>D.</given-names> <surname>Blei</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Lafferty</surname></string-name>. <article-title>Topic models</article-title>. In <person-group person-group-type="editor"><string-name><given-names>A.</given-names> <surname>Srivastava</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Sahami</surname></string-name></person-group>, editors, <source>Text Mining: Theory and Applications</source>. <publisher-name>Taylor and Francis</publisher-name>, <year>2009</year></mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><string-name><surname>Park</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>HK</given-names></string-name>, <string-name><surname>Choi</surname> <given-names>IY</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>JK</given-names></string-name>. <article-title>A literature review and classification of recommender systems research</article-title>. <source>Expert Systems with Applications</source>. <year>2012</year> Sep 1;<volume>39</volume>(<issue>11</issue>):<fpage>10059</fpage>&#x2013;<lpage>72</lpage>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><surname>Adomavicius</surname> <given-names>G</given-names></string-name>, <string-name><surname>Tuzhilin</surname> <given-names>A.</given-names></string-name> <article-title>Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions</article-title>. <source>IEEE transactions on knowledge and data engineering</source>. <year>2005</year> Jun;<volume>17</volume>(<issue>6</issue>):<fpage>734</fpage>&#x2013;<lpage>49</lpage>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><string-name><surname>Herlocker</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Konstan</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Terveen</surname> <given-names>LG</given-names></string-name>, <string-name><surname>Riedl</surname> <given-names>JT</given-names></string-name>. <article-title>Evaluating collaborative filtering recommender systems</article-title>. <source>ACM Transactions on Information Systems (TOIS)</source>. <year>2004</year> Jan 1;<volume>22</volume>(<issue>1</issue>):<fpage>5</fpage>&#x2013;<lpage>3</lpage>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><string-name><surname>Bogers</surname> <given-names>T</given-names></string-name>, <string-name><surname>Van den Bosch</surname> <given-names>A.</given-names></string-name> <article-title>Recommending scientific articles using citeulike</article-title>. <source>In Proceedings of the 2008 ACM conference on Recommender systems</source> <year>2008</year> Oct <volume>23</volume> (pp. <fpage>287</fpage>-<lpage>290</lpage>). ACM.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><string-name><surname>Griffiths</surname> <given-names>TL</given-names></string-name>, <string-name><surname>Steyvers</surname> <given-names>M.</given-names></string-name> <article-title>Finding scientific topics</article-title>. <source>Proceedings of the National academy of Sciences</source>. <year>2004</year> Apr 6;<volume>101</volume>(<issue>Suppl 1</issue>):<fpage>5228</fpage>&#x2013;<lpage>35</lpage>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><string-name><surname>Zaugg</surname> <given-names>H</given-names></string-name>, <string-name><surname>West</surname> <given-names>RE</given-names></string-name>, <string-name><surname>Tateishi</surname> <given-names>I</given-names></string-name>, <string-name><surname>Randall</surname> <given-names>DL</given-names></string-name>. <article-title>Mendeley: Creating communities of scholarly inquiry through research collaboration</article-title>. <source>TechTrends</source>. <year>2011</year> Jan 1;<volume>55</volume>(<issue>1</issue>):<fpage>32</fpage>&#x2013;<lpage>6</lpage>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><surname>Tomatoes</surname>, <given-names>R.</given-names></string-name> (<year>2008</year>). <source>Rotten Tomatoes</source>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><string-name><surname>Amatriain</surname> <given-names>X</given-names></string-name>, <string-name><surname>Lathia</surname> <given-names>N</given-names></string-name>, <string-name><surname>Pujol</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Kwak</surname> <given-names>H</given-names></string-name>, <string-name><surname>Oliver</surname> <given-names>N.</given-names></string-name> <article-title>The wisdom of the few: a collaborative filtering approach based on expert opinions from the web</article-title>. <source>InProceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval</source> <year>2009</year> Jul <volume>19</volume> (pp. <fpage>532</fpage>&#x2013;<lpage>539</lpage>). ACM.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><string-name><surname>Layton</surname> <given-names>J.</given-names></string-name> <article-title>How Pandora Radio works</article-title>. <source>HowStuffWorks. com</source>. <year>2006</year> May.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="book"><string-name><surname>Henning</surname> <given-names>V</given-names></string-name>, <string-name><surname>Reichelt</surname> <given-names>J.</given-names></string-name> <chapter-title>Mendeley-A Last. fm for research?</chapter-title>. <source>IneScience, 2008. eScience&#x2019;08</source>. <publisher-loc>IEEE Fourth International Conference</publisher-loc> on <year>2008</year> Dec <volume>7</volume> (pp. <fpage>327</fpage>&#x2013;<lpage>328</lpage>). IEEE.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><string-name><surname>Adhikari</surname> <given-names>VK</given-names></string-name>, <string-name><surname>Guo</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Hao</surname> <given-names>F</given-names></string-name>, <string-name><surname>Varvello</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hilt</surname> <given-names>V</given-names></string-name>, <string-name><surname>Steiner</surname> <given-names>M</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>ZL</given-names></string-name>. <article-title>Unreeling netflix: Understanding and improving multi-cdn movie delivery</article-title>. <source>InINFOCOM, 2012 Proceedings IEEE</source> <year>2012</year> Mar <volume>25</volume> (pp. <fpage>1620</fpage>&#x2013;<lpage>1628</lpage>). IEEE.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="book"><string-name><surname>Adomavicius</surname> <given-names>G</given-names></string-name>, <string-name><surname>Tuzhilin</surname> <given-names>A.</given-names></string-name> <chapter-title>Context-aware recommender systems</chapter-title>. <source>InRecommender systems handbook</source> <year>2015</year> (pp. <fpage>191</fpage>&#x2013;<lpage>226</lpage>). <publisher-name>Springer, Boston, MA</publisher-name>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="website">Website: &#x201C;<ext-link ext-link-type="uri" xlink:href="http://www.pubchase.com">www.pubchase.com</ext-link>&#x201D;.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname> <given-names>J.</given-names></string-name> <article-title>Unpacking the Matthew effect in citations</article-title>. <source>Journal of Informetrics</source>. <year>2014</year> Apr 1;<volume>8</volume>(<issue>2</issue>):<fpage>329</fpage>&#x2013;<lpage>39</lpage>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><string-name><surname>Graham</surname> <given-names>S</given-names></string-name>, <string-name><surname>Weingart</surname> <given-names>S</given-names></string-name>, <string-name><surname>Milligan</surname> <given-names>I.</given-names></string-name> <article-title>Getting started with topic modeling and MALLET</article-title>. <source>The Editorial Board of the Programming Historian</source>; <year>2012</year> Sep 2.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><string-name><surname>Chang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Gerrish</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>C</given-names></string-name>, <string-name><surname>Boyd-Graber</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Blei</surname> <given-names>DM</given-names></string-name>. <article-title>Reading tea leaves: How humans interpret topic models</article-title>. <source>InAdvances in neural information processing systems</source> <year>2009</year> (pp. <fpage>288</fpage>&#x2013;<lpage>296</lpage>)</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><string-name><surname>Blei</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Lafferty</surname> <given-names>JD</given-names></string-name>. <article-title>Topic models</article-title>. <source>Text mining: classification, clustering, and applications</source>. <year>2009</year> Jun 15;<volume>10</volume>(<issue>71</issue>):<fpage>34</fpage>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="book"><string-name><surname>Blei</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Lafferty</surname> <given-names>JD</given-names></string-name>. <chapter-title>Correlated topic models</chapter-title>. <source>InProceedings of the 18th International Conference on Neural Information Processing Systems</source> <year>2005</year> Dec <volume>5</volume> (pp. <fpage>147</fpage>&#x2013;<lpage>154</lpage>). <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><string-name><surname>Rousseeuw</surname> <given-names>PJ</given-names></string-name>. <article-title>Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</article-title>. <source>Journal of computational and applied mathematics</source>. <year>1987</year> Nov 1;<volume>20</volume>:<fpage>53</fpage>&#x2013;<lpage>65</lpage>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><string-name><surname>aMabe</surname> <given-names>M</given-names></string-name>, (<year>2003</year>). <source>The growth and number of journals</source>. Serials.<volume>16</volume>(<issue>2</issue>),pp.<fpage>191197</fpage>. DOI: <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1629/16191">http://doi.org/10.1629/16191</ext-link>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><string-name><surname>Merton</surname> <given-names>RK</given-names></string-name>. <article-title>The Matthew effect in science: The reward and communication systems of science are considered</article-title>. <source>Science</source>. <year>1968</year> Jan 5;<volume>159</volume>(<issue>3810</issue>):<fpage>56</fpage>&#x2013;<lpage>63</lpage>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><string-name><given-names>Brett</given-names> <surname>Megan</surname></string-name>, <article-title>Topic Modeling: A Basic Introduction</article-title>, <source>Journal of Digital Humanities</source>, Vol <volume>2</volume>,<fpage>1</fpage></mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="website"><collab>Website:</collab> &#x201C;<ext-link ext-link-type="uri" xlink:href="https://www.last.fm/">https://www.last.fm/"</ext-link></mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="website"><collab>Website:</collab> &#x201C;<ext-link ext-link-type="uri" xlink:href="https://www.sparrho.com/">https://www.sparrho.com/"</ext-link></mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="website"><collab>Website:</collab> &#x201C;<ext-link ext-link-type="uri" xlink:href="https://scholar.google.com/intl/en/scholar/about.html">https://scholar.google.com/intl/en/scholar/about.html"</ext-link></mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><string-name><surname>Salton</surname> <given-names>G</given-names></string-name>, <string-name><surname>Wong</surname> <given-names>A</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>CS</given-names></string-name>. <article-title>A vector space model for automatic indexing</article-title>. <source>Communications of the ACM</source>. <year>1975</year> Nov 1;<volume>18</volume>(<issue>11</issue>):<fpage>613</fpage>&#x2013;<lpage>20</lpage>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><string-name><surname>Larson</surname> <given-names>RR</given-names></string-name>. <article-title>Introduction to information retrieval</article-title>. <source>Journal of the American Society for Information Science and Technology</source>. <year>2010</year> Apr;<volume>61</volume>(<issue>4</issue>):<fpage>852</fpage>&#x2013;<lpage>3</lpage>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><string-name><surname>Larson</surname> <given-names>RR</given-names></string-name>. <article-title>Introduction to information retrieval</article-title>. <source>Journal of the American Society for Information Science and Technology</source>. <year>2010</year> Apr;<volume>61</volume>(<issue>4</issue>):<fpage>852</fpage>&#x2013;<lpage>3</lpage>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><string-name><surname>Folk</surname> <given-names>M</given-names></string-name>, <string-name><surname>Heber</surname> <given-names>G</given-names></string-name>, <string-name><surname>Koziol</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Pourmal</surname> <given-names>E</given-names></string-name>, <string-name><surname>Robinson</surname> <given-names>D.</given-names></string-name> <article-title>An overview of the HDF5 technology suite and its applications</article-title>. <source>InProceedings of the EDBT/ICDT 2011 Workshop on Array Databases</source> <year>2011</year> Mar <volume>25</volume> (pp. <fpage>36</fpage>&#x2013;<lpage>47</lpage>). ACM.</mixed-citation></ref>
</ref-list>
<fn-group>
<fn id="fn1"><label>&#x002A;</label><p><ext-link ext-link-type="uri" xlink:href="http://www.scireader.com">www.scireader.com</ext-link></p></fn>
<fn id="fn2">
<label><sup>1</sup></label><p>SciReader codebase will soon be released as open-source code.</p></fn>
</fn-group>
</back>
</article>