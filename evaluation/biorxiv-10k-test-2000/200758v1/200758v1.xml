<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/200758</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Bioinformatics</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A Deep Recurrent Neural Network Discovers Complex Biological Rules to Decipher RNA Protein-Coding Potential</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Hill</surname><given-names>Steven</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">&#x2020;</xref>
<xref ref-type="author-notes" rid="n3">&#x00A7;</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Kuintzle</surname><given-names>Rachael</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n2">&#x2021;</xref>
<xref ref-type="author-notes" rid="n3">&#x00A7;</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Teegarden</surname><given-names>Amy</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Merrill</surname><given-names>Erich</given-names><suffix>III</suffix></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Danaee</surname><given-names>Padideh</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Hendrix</surname><given-names>David A.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<aff id="a1"><label>1</label><institution>School of Electrical Engineering and Computer Science, Oregon State University, 1148 Kelley Engineering Center</institution>, Corvallis, OR 97331</aff>
<aff id="a2"><label>2</label><institution>Department of Biochemistry and Biophysics, Oregon State University, 2011 Ag&#x2026;Life Sciences Bldg</institution>, Corvallis, OR 97331</aff>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="present-address"><label>&#x2020;</label><p>Current address: New York Genome Center, 101 Avenue of the Americas, New York, NY 10013</p></fn>
<fn id="n2" fn-type="present-address"><label>&#x2021;</label><p>Current address: Department of Chemistry and Chemical Engineering, California Institute of Technology, 1200 East California Blvd, Pasadena, CA 91125</p></fn>
<fn id="n3" fn-type="equal"><label>&#x00A7;</label><p>These authors contributed equally</p></fn>
<corresp id="cor1"><label>&#x002A;</label>Correspondence to: <email>david.hendrix@oregonstate.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub"><year>2017</year></pub-date>
<elocation-id>200758</elocation-id>
<history>
<date date-type="received">
<day>12</day>
<month>10</month>
<year>2017</year>
</date>
<date date-type="rev-recd">
<day>12</day>
<month>10</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>13</day>
<month>10</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="200758.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract><title>Abstract</title>
<p>The current deluge of newly identified RNA transcripts presents a singular opportunity for improved assessment of coding potential, a cornerstone of genome annotation, and for machine-driven discovery of biological knowledge. While traditional, feature-based methods for RNA classification are limited by current scientific knowledge, deep learning methods can independently discover complex biological rules in the data <italic>de novo</italic>. We trained a gated recurrent neural network (RNN) on human messenger RNA (mRNA) and long noncoding RNA (lncRNA) sequences. Our model, mRNA RNN (mRNN), surpasses state-of-the-art methods at predicting protein-coding potential. To understand what mRNN learned, we probed the network and uncovered several context-sensitive codons highly predictive of coding potential. Our results suggest that gated RNNs can learn complex and long-range patterns in full-length human transcripts, making them ideal for performing a wide range of difficult classification tasks and, most importantly, for harvesting new biological insights from the rising flood of sequencing data.</p>
<sec><title>One Sentence Summary</title>
<p>A deep recurrent neural network is adapted to harvest complex biological information from long RNAs to substantially improve accuracy in coding potential assessment.</p>
</sec>
</abstract>
<counts>
<page-count count="15"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1"><title>Introduction</title>
<p>Deep sequencing technology has yielded a torrent of new transcript annotations, creating a need for fresh approaches to unlock the full information potential of these vast datasets. Existing state-of-the-art methods for classification of long RNAs as protein-coding RNAs (mRNAs) or long noncoding RNAs (lncRNAs) rely on human-engineered features, such as the coverage and length of a predicted open reading frame (ORF). These features predispose such models to misclassification of mRNAs encoding small proteins and of lncRNAs with long, un-translated ORFs. Nucleotide hexamer frequency is another commonly used feature, but while it can capture the frequency of codon pairs, it does not benefit from the larger sequence context. These limitations and the annotation challenges ahead demand new approaches to biological sequence classification that are capable of detecting complex, variable-length patterns.</p>
<p>In contrast to conventional machine learning methods, &#x201C;deep learning&#x201D;-the application of multi-layered artificial neural networks to learning tasks-can discover useful features independently, avoiding biases introduced by human-engineered features(<italic><xref ref-type="bibr" rid="c1">1</xref></italic>). Deep learning methods have repeatedly outperformed state-of-the-art &#x201C;shallow&#x201D; machine learning algorithms, such as support vector machines (SVM) and logistic regression, as approaches to biological problems in recent years. Multiple bioinformatics applications of deep convolutional neural networks (CNNs) have been published(<italic><xref ref-type="bibr" rid="c2">2</xref>&#x2013;<xref ref-type="bibr" rid="c4">4</xref></italic>); however, while CNNs adeptly learn spatial information, recurrent neural networks (RNNs) are better suited for learning sequential patterns because of their serialized structure and ability to handle variable-length inputs(<italic><xref ref-type="bibr" rid="c1">1</xref></italic>). Following the success of RNN-based approaches in the fields of natural language and music(<italic><xref ref-type="bibr" rid="c5">5</xref></italic>), researchers have only recently begun to apply RNNs to biological problems(<italic><xref ref-type="bibr" rid="c6">6</xref>&#x2013;<xref ref-type="bibr" rid="c10">10</xref></italic>). While basic RNNs are challenged by most biologically-relevant input sequence lengths due to the &#x201C;vanishing gradient problem,&#x201D; a difficulty encountered during training due to the multiplication of many small terms when computing the gradient of an error function by the chain rule (<italic><xref ref-type="bibr" rid="c11">11</xref></italic>), several recent adaptations addressed this issue. Among the most popular of these modified RNNs are long-short-term-memory (LSTM) RNNs (<italic><xref ref-type="bibr" rid="c12">12</xref></italic>) and gated recurrent unit (GRU) RNNs (<italic><xref ref-type="bibr" rid="c13">13</xref></italic>), which manage memory to improve the learning of long-range dependencies. Recent studies demonstrated superior performance of GRUs compared to LSTMs for bioinformatics tasks(<italic><xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c14">14</xref></italic>). We report the successful implementation of a GRU network to accurately predict protein-coding potential of complete, variable-length transcripts (<xref ref-type="fig" rid="fig1">Fig. 1</xref>). Our method, &#x201C;mRNA RNN&#x201D; (mRNN), not only outperforms existing state-of-the-art classifiers, but also learns complex biological rules in the process.</p>
<fig id="fig1" position="float" fig-type="figure"><label>Figure 1.</label>
<caption><title>mRNN Output and Model Schematic</title>
<p>Coding probability and coding potential score is shown at nucleotide-level resolution for the transcript ENST00000371732.9, which encodes caspase recruitment domain family member 9. Values at position <italic>i</italic> correspond to the mRNN coding probability or <italic>S</italic><sub><italic>trunc</italic></sub>(<italic>i</italic>), the mRNN output for the truncated sequence from 1 to <italic>i</italic>. Vertical dashed lines demarcate the annotated start and end of the CDS. A schematic of the gated recurrent neural network is shown below. Equilateral triangles signify reset gates, and the height of the grey fill represents the proportional contribution of the previous hidden state (<italic>h</italic><sub><italic>t-1</italic></sub>) to the new candidate hidden state <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="200758_inline1.gif"/></alternatives></inline-formula>. The update gate is shown as two circles representing the proportional contributions of the previous hidden state (<italic>h</italic><sub><italic>t-2</italic></sub>) and the new candidate hidden state <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="200758_inline2.gif"/></alternatives></inline-formula> to the new hidden state (<italic>h</italic><sub><italic>t</italic></sub>). Arrows represent matrix products. The embedding layer maps nucleotides to 128-dimnensional vectors.</p></caption>
<graphic xlink:href="200758_fig1.tif"/>
</fig>
</sec>
<sec id="s2"><title>Results</title>
<p>The best resulting mRNN model after training selected by accuracy on the validation set is referred to hereafter as &#x201C;mRNN&#x201D;. We also implemented an ensemble testing method called &#x201C;mRNN ensemble,&#x201D; which uses the weighted average of the five best mRNN models. For comparison, we used the same test set to assess performance of two non-comparative classifiers considered to be state-of-the-art in speed and performance: CPAT(<italic><xref ref-type="bibr" rid="c16">16</xref></italic>) and FEELnc(<italic><xref ref-type="bibr" rid="c17">17</xref></italic>). While mRNN matched or outperformed CPAT and FEELnc on the test set, the mRNN ensemble method showed significant improvements in performance over these methods in accuracy, specificity, and other metrics (<xref ref-type="fig" rid="fig2">Fig. 2A</xref>). We also compared the classifiers using a set of more challenging transcripts, including 500 mRNAs with short ORFs (&#x2264; 50 codons), and 500 lncRNAs with long (untranslated) ORFs (&#x2265; 50 codons) (<xref ref-type="fig" rid="fig2">Fig. 2B</xref>). Both mRNN and mRNN ensemble methods significantly outperformed CPAT and FEELnc in all metrics on this challenge set. Notably, CPAT and FEELnc showed low sensitivity (62.2&#x0025; and 67.8&#x0025; respectively), indicating a bias against classification of mRNAs with short ORFs as protein-coding, while mRNN ensemble achieved a sensitivity of 78.6&#x0025;, demonstrating its superior predictive power for these atypical transcripts.</p>
<fig id="fig2" position="float" fig-type="figure"><label>Figure 2.</label>
<caption><title>Comparison of Classifier Performance</title>
<p>(A-C) Performance of four classifiers trained with human transcript sequences. Error bars are 95&#x0025; confidence intervals computed from 100,000 bootstrap trials. Asterisks above mRNN or ensemble mRNN indicate the method&#x2019;s improvement over both CPAT and FEELnc with an empirical p-value less than 0.05 computed from the bootstrap trials. (A) Performance on human test set transcripts, consisting of 500 mRNAs and 500 lncRNAs. (B) Performance on human challenge set transcripts, including 500 mRNAs with ORFs &#x003C; 50 codons and 500 lncRNAs with ORFs &#x003E; 50 codons. (C) Performance on a test set of mouse transcripts, including 500 mRNAs and 500 lncRNAs, using models trained with human data.</p></caption>
<graphic xlink:href="200758_fig2.tif"/>
</fig>
<p>We next evaluated all models on a randomly selected test set from mouse GENCODE transcripts consisting of 500 mRNAs and 500 lncRNAs. For each method, we trained on the same human training set, and tested on mouse. The mRNN ensemble and mRNN classifiers had the highest accuracy and specificity, showing that they can be used for classification of long RNAs in a new transcriptome (<xref ref-type="fig" rid="fig2">Fig. 2C</xref>).</p>
<p>To begin deducing what mRNN learned, we conducted sequence perturbation analyses (Supplementary Methods). Score changes for sequences with shuffled coding sequence (CDS) regions compared to those with shuffled 3&#x2019; or 5&#x2019; untranslated regions (UTRs) demonstrate that mRNN primarily utilizes organized sequence information in the CDS (Fig. S4). We next conducted a point-mutation analysis to evaluate changes in score resulting from every possible single-nucleotide substitution for all GENCODE mRNA transcripts under 2,000 nt in length (<xref ref-type="fig" rid="fig3">Fig. 3</xref>). The annotated start codon marked a clear boundary, with low score changes preceding it and strong changes following it, indicating that sequence perturbations early in the CDS erase more predictive information than perturbations in the 5&#x2019; UTR. In contrast, score changes around non-start ATGs in the 5&#x2019; UTR were more symmetric before and after the ATG, and significantly lower on average. Strikingly, the pattern of average score changes in the true CDS exhibited three-nucleotide periodicity with a persistent aversion to mutations that made codons more similar to in-frame stop codons. This pattern was not observed upstream of the annotated start codon (5&#x2019; UTR), nor in the regions flanking either ATGs in the 5&#x2019; UTR or control CTGs (Fig. S5). An aversion to stop codon-like trinucleotides was also observed preceding, but not following, annotated stop codons, suggesting that mRNN recognizes the end of the CDS. This pattern was not observed in regions preceding TGA/TAA/TAG trinucleotides in the 3&#x2019; UTR. Notably, mutation of the annotated stop codon significantly increased the coding potential score, showing that mRNN displays a preference for longer ORFs.</p>
<fig id="fig3" position="float" fig-type="figure"><label>Figure 3.</label>
<caption><title>Transcript Point Mutation Maps</title>
<p>Heat maps representing the average change in score for point mutations at positions relative to the following elements (from top to bottom): annotated start codons, ATGs in 5&#x2019; UTRs, annotated stop codons, and TGA/TAA/TAGs in 3&#x2019; UTRs. Sequence logos present the nucleotide composition of the sequences analyzed around the same windows. Asterisks represent cells that are statistically significant at an FDR of 0.0001 using a two-tailed t-test comparing score changes from mutations at a given position to all score changes from mutations of the same base in the corresponding background region. Background regions are 5&#x2019; UTRs for the start codons and ATGs, or 3&#x2019; UTRs for the stop codons and TGA/TAA/TAGs.</p></caption>
<graphic xlink:href="200758_fig3.tif"/>
</fig>
<p>To evaluate whether mRNN learned relationships between distinct features, we performed pairwise-mutations of the transcript SPANXB1, a cancer/testis-specific antigen. This transcript is relatively short (under 500 nt) and has both 5&#x2019; and 3&#x2019; UTRs. The coding score for this transcript starts an upward trajectory within the CDS (<xref ref-type="fig" rid="fig4">Fig. 4A</xref>). We examined this transcript by altering every possible combination of two nucleotides within the sequence. We define &#x0394;<italic>S</italic><sub><italic>syn</italic></sub>(<italic>i</italic>,<italic>j</italic>) as the minimum of the difference between the coding score change resulting from mutations at two positions <italic>i</italic> and <italic>j</italic> and the sum of score changes associated with the individual mutations. Therefore, &#x0394;<italic>S</italic><sub><italic>syn</italic></sub>(<italic>i</italic>, <italic>j</italic>) quantifies the &#x201C;score change synergy&#x201D; of the pair of mutations, and is strongly negative for highly related positions. We identified several pairs of synergistic mutations, including a point mutation that changed an AAC codon to AAA and another that changed an AAG codon to TAG, which, when combined, resulted in a score change 6.8 times the sum of the individual score changes (<xref ref-type="fig" rid="fig4">Fig. 4B</xref>). In other examples, we identified compensatory changes, such as a decrease in score from a nonsense mutation that was significantly diminished when another mutation changed a TGT codon to TAT (<xref ref-type="fig" rid="fig4">Fig. 4C</xref>). In both cases described, the two mutated positions are located within the coding score spike, despite being separated by 18 and 29 bases, respectively. The fact that some mutations can exacerbate or compensate for the effect of ORF-truncating mutations on mRNN&#x2019;s output demonstrates that mRNN learned complex and long-range sequence information dependencies.</p>
<fig id="fig4" position="float" fig-type="figure"><label>Figure 4.</label>
<caption><title>Pair-wise mutation analysis</title>
<p>(A) The mRNN coding trajectory (as in <xref ref-type="fig" rid="fig1">Fig. 1</xref>), for ENST00000449283.1, a transcript encoding SPANX1.</p>
<p>(B) Pair-wise mutation heat map of synergistic score changes for the same transcript. Values are the score change synergy for a pair of mutated bases at positions <italic>i</italic> and <italic>j</italic>, where <italic>i</italic> &#x003C; <italic>j</italic>. Score change synergy is the minimum difference between the resulting change in score when the pair of bases is mutated and the sum of the score changes from individual mutations of each base in the pair. (C) Pair-wise mutation heat map of compensatory score changes for the same transcript. Values are the compensatory score change for a pair of mutated bases at positions <italic>i</italic> and <italic>j</italic>, where <italic>i</italic> &#x003C; <italic>j</italic>. Compensatory score change is the maximum difference between the resulting change in score when the pair of bases is mutated and the sum of the score changes from individual mutations of each base in the pair. (B-C) Bottom-right of each heat map shows a zoomed-in view of a position pair with a highly compensatory or synergistic score change. Each line spanning three nucleotides represents a codon.</p></caption>
<graphic xlink:href="200758_fig4.tif"/>
</fig>
<p>To visualize mRNN&#x2019;s decision-making process, we defined a coding trajectory <italic>S</italic><sub><italic>trunc</italic></sub>(<italic>i</italic>) as the score of the truncated sequence from 1 to <italic>i</italic> for transcripts in the human test set (<xref ref-type="fig" rid="fig5">Fig. 5A</xref>). Remarkably, we found several examples of mRNAs with long 5&#x2019; UTRs that mRNN classified as coding only after observing the CDS more than 4,000 nt from the transcript start (Fig. S6A-D). Thus, mRNN remains sensitive to information toward the end of transcripts longer than sequences previously used in any bioinformatics RNN applications that we are aware of, despite being trained only on sequences shorter than 1,000 nucleotides long.</p>
<fig id="fig5" position="float" fig-type="figure"><label>Figure 5.</label>
<caption><title>Model Interrogation for Feature Discovery</title>
<p>(A) mRNN coding score trajectories without smoothing for each transcript in the test set. Blue, protein-coding; red, noncoding. Bold lines represent average coding probability when 5 or more transcripts had lengths at of least <italic>i</italic> nt. (B) Coding score trajectory for transcript ENST00000458629.1, which encodes C-X-C motif chemokine receptor 6. Vertical dashed lines mark CDS boundaries. (C) Histogram of significant spike locations in test set mRNAs relative to true CDS start positions. (D) Scatterplot showing codons enriched in the spike regions (&#x002B;/- 25 nt around most significant spike position) compared to 50-nt regions upstream of the spikes. The x-axis is the frequency of each codon in the full set of GENCODE annotated coding regions. The y-axis represents the frequency of the codon in the indicated region. Each pair of points represents a codon. Large, labeled points are translation-indicating codons (TICs)&#x2014;codons statistically enriched (FDR 0.05) in spike regions compared to the regions upstream of spikes. The dashed line corresponds to global codon frequency, and the blue band is the range of standard error computed from a binomial model.</p>
<p>(E) Receiver operator characteristic analysis for five prediction methods including our mRNN ensemble, the best single mRNN model, FEELnc, CPAT, and TIC frequency. TIC frequency is the number of occurrences of TICs within 1000 nt of, and in-frame with, an upstream ATG, but not after an in-frame TGA/TAA/TAG. AUROC values for each method are presented in the legend. (F) mRNN coding score changes resulting from <italic>in silico</italic> TIC mutations. While the majority of mutations to TICs lead to a decrease in coding score, mutations to control codons (the codons least enriched in the spike regions) result in smaller score changes on average.</p></caption>
<graphic xlink:href="200758_fig5.tif"/>
</fig>
<p>To identify regions of the sequence that most strongly impact mRNN&#x0027;s decision, we performed unweighted sliding-average smoothing of the coding potential trajectories, then computed the change in score &#x0394;<italic>S</italic><sub><italic>trunc</italic></sub>(<italic>i</italic>) across the sequence for a window <italic>w</italic> of 50 nucleotides (Fig. 5B). Statistically significant spikes (Fig. S7) were identified in 412 of the 500 test mRNAs, and in only 47 of the 500 lncRNAs. The distribution of the spike positions for mRNAs peaked within the CDS, shortly after the start codon (<xref ref-type="fig" rid="fig5">Fig. 5C</xref>, Fig. S8, and S9A-B).</p>
<p>To identify the sequence elements associated with significant spikes in coding potential score, we compared the frequency of in-frame codons in a 50-nt window centered at the spike to codon frequencies in the 50-nt window preceding the spike. We found 11 significantly enriched codons using a t-test and an FDR of 0.05 (<xref ref-type="fig" rid="fig5">Fig. 5D</xref>, Table S1); we named these translation-indicating codons (TICs). 9 of the 11 TICs were also significantly enriched in spike regions of an independent set of mRNAs with long 5&#x2019; UTRs (Fig. S9C). Notably, two codons in the synergistic and compensatory pairwise mutation examples above (AAC and TAT) are TICs.</p>
<p>To assess the predictive power of TICs, we defined a TIC-score as the maximum number of TICs occurring within 1000 nt downstream of an in-frame ATG, and preceding the first in-frame stop codon. This TIC-score was able to accurately predict coding potential in the test set with an AUROC of 0.942, just below that of CPAT at 0.957 (<xref ref-type="fig" rid="fig5">Fig. 5E</xref>). The same rule distinguished mRNAs from lncRNAs in the full GENCODE datasets with an AUROC of 0.931 for human and 0. 935 for mouse. We next computed the reduction in the spike magnitude&#x2014;the change in &#x0394;<italic>S</italic><sub><italic>trunc</italic></sub>(<italic>i</italic>)&#x2014;resulting from the mutation of a given TIC codon <italic>in silico</italic>. Mutation of TICs resulted in spike height decreases 94.7&#x0025; of the time, while mutations to the least enriched codons in the spike regions decreased spike height only 59.9&#x0025; of the time (<xref ref-type="fig" rid="fig5">Fig. 5F</xref>, S10), demonstrating that TICs are an important part of mRNN&#x2019;s classification process. Of note, we also identified a frame-biased, 12-mer motif enriched in spike regions, but it possessed lower predictive power than the TICs (Fig. S11, Table S2-3). Strikingly, the TICs are among the codons most enriched in GENCODE CDSs relative to UTRs and out-of-frame triplets, demonstrating that mRNN learned the complex sequence context that gives these codons predictive power (Fig. S12).</p>
</sec>
<sec id="s3"><title>Discussion</title>
<p>In this study, we have shown that GRU networks can successfully model full-length human transcripts. Previous bioinformatics applications of RNNs restricted input sequence length to 2,000 nt or fewer by one of three strategies: filtering the dataset on a length threshold(<italic><xref ref-type="bibr" rid="c18">18</xref></italic>), dividing input sequences into segments of a fixed size(<italic><xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c20">20</xref></italic>), or truncating input sequences(<italic><xref ref-type="bibr" rid="c21">21</xref></italic>). However, one important advantage that deep RNNs have over other deep learning methods is the ability to interpret context and long-range information dependencies. In order to exploit the full power of our GRU network, we did not truncate or segment our training sequences, and we did not constrain our test set inputs by sequence length in any way. Our test inputs included 222 mRNAs longer than 2,000 nucleotides, and our model showed no impairment in classifying even the longest sequences, which exceeded 100,000 nucleotides.</p>
<p>Despite mRNN&#x2019;s featureless architecture, which precluded it from integrating human knowledge of mRNA structure into its learning process, mRNN was able to learn true defining features of mRNAs, including trinucleotide patterns and depletion of in-frame stop codons after the start of an open-reading frame. Furthermore, the recurrent nature of mRNN enabled it to leverage long-range information dependencies for classification, as evidenced by pairwise mutation analysis. In addition to surpassing state-of-the-art accuracy in assessment of transcript coding potential, we demonstrate that the &#x201C;black-box&#x201D; GRU network can be harnessed for identifying specific biological attributes, such as the translation-indicating codons (TICs), that distinguish sequence classes. We anticipate that GRU-based approaches will be highly useful for future bioinformatics classification tasks, as well as for unlocking hidden biological insights in the vast amounts of available sequencing data.</p>
</sec>
<sec id="s4"><title>Materials and Methods</title>
<p>For training, we provided mRNN with a dataset containing full-length human transcript sequences labeled as mRNAs or lncRNAs. All training and test sets were selected from GENCODE Release 25(<italic><xref ref-type="bibr" rid="c15">15</xref></italic>). We evaluated mRNN&#x2019;s performance using a test set&#x2014;an unbiased random sample of human transcripts composed of 500 mRNAs and 500 lncRNAs selected from the full GENCODE annotation. Similarly, we selected a validation set for hyper-parameter tuning and model selection (Fig. S1-3) equal in size to the test set. No length limit or other filter was imposed on sequences in the test or validation sets.</p>
<p>To reduce computation time and to prevent the learning of transcript length as a feature, we imposed constraints on the training set sequence length for mRNN. After selecting the test and validation set transcripts, we selected 16,000 mRNAs and 16,000 lncRNAs from the remaining sequences between 200 and 1000 nt long as our training set. We used a combination of &#x201C;data augmentation&#x201D; and &#x201C;early stopping,&#x201D; which exits training if loss on the training set decreases while validation loss does not, to prevent over-fitting during training. No length restrictions were imposed on sequences in the training set for CPAT and FEELnc, giving these classifiers substantially more training data than mRNN. For detailed methods see Supplementary Methods.</p>
<sec id="s4a"><title>Software availability</title>
<p>Source code implementing data preprocessing, training, and downstream analysis is available in the package mRNN from <ext-link ext-link-type="uri" xlink:href="http://github.com/hendrixlab/mRNN">http://github.com/hendrixlab/mRNN</ext-link>.</p>
</sec>
</sec>
</body><back><ack><title>Acknowledgementsd</title>
<p>The authors would like to thank Prof. Stephen Ramsey, Prof. Christopher K. Mathews, Prof. Liang Huang, Prof. Colin Johnson, Prof. P. Andy Karplus, and Prof. Michael Freitag for feedback on the manuscript and helpful discussions.</p>
</ack>
<ref-list><title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><given-names>I.</given-names> <surname>Goodfellow</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Courville</surname></string-name>, <source>Deep learning. 2015</source>, (<year>2016</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>O. G.</given-names> <surname>Troyanskaya</surname></string-name>, <article-title>Predicting effects of noncoding variants with deep learning-based sequence model</article-title>. <source>Nat Methods</source> <volume>12</volume>, <fpage>931</fpage>&#x2013;<lpage>934</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Alipanahi</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Delong</surname></string-name>, <string-name><given-names>M. T.</given-names> <surname>Weirauch</surname></string-name>, <string-name><given-names>B. J.</given-names> <surname>Frey</surname></string-name>, <article-title>Predicting the sequence specificities of DNA&#x002D; and RNA-binding proteins by deep learning</article-title>. <source>Nat Biotechnol</source> <volume>33</volume>, <fpage>831</fpage>&#x2013;<lpage>838</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><given-names>D. R.</given-names> <surname>Kelley</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Snoek</surname></string-name>, <string-name><given-names>J. L.</given-names> <surname>Rinn</surname></string-name>, <article-title>Basset: learning the regulatory code of the accessible genome with deep convolutional neural networks</article-title>. <source>Genome research</source> <volume>26</volume>, <fpage>990</fpage>&#x2013;<lpage>999</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="other"><string-name><given-names>J.</given-names> <surname>Chung</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Gulcehre</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Cho</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name>, <article-title>Empirical evaluation of gated recurrent neural networks on sequence modeling</article-title>. <source>arXiv preprint</source> arXiv:1412.3555, (<year>2014</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="other"><string-name><given-names>S.</given-names> <surname>Park</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Min</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Choi</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Yoon</surname></string-name>, <article-title>deepMiRGene: Deep Neural Network based Precursor microRNA Prediction</article-title>. <source>arXiv preprint</source> arXiv:1605.00017, (<year>2016</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="other"><string-name><given-names>B.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Baek</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Park</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Yoon</surname></string-name>, <article-title>deepTarget: End-to-end Learning Framework for microRNA Target Prediction using Deep Recurrent Neural Networks</article-title>. <source>arXiv preprint</source> arXiv:1603.09123, (<year>2016</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="other"><string-name><given-names>B.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Na</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Yoon</surname></string-name>, <article-title>DNA-Level Splice Junction Prediction using Deep Recurrent Neural Networks</article-title>. <source>arXiv preprint</source> arXiv:1512.05135, (<year>2015</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="confproc"><string-name><given-names>H. R.</given-names> <surname>Hassanzadeh</surname></string-name>, <string-name><given-names>M. D.</given-names> <surname>Wang</surname></string-name>, in <conf-name>Bioinformatics and Biomedicine (BIBM), 2016 IEEE International Conference on</conf-name>. (<conf-sponsor>IEEE</conf-sponsor>, <conf-date>2016</conf-date>), pp. <fpage>178</fpage>&#x2013;<lpage>183</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Angermueller</surname></string-name>, <string-name><given-names>H. J.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Reik</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Stegle</surname></string-name>, <article-title>DeepCpG: accurate prediction of singlecell DNA methylation states using deep learning</article-title>. <source>Genome biology</source> <volume>18</volume>, <fpage>67</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="book"><string-name><given-names>S.</given-names> <surname>Hochreiter</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Frasconi</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Schmidhuber</surname></string-name>. (<source>A field guide to dynamical recurrent neural networks.</source> <publisher-name>IEEE Press</publisher-name>, <year>2001</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Hochreiter</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Schmidhuber</surname></string-name>, <article-title>Long short-term memory</article-title>. <source>Neural computation</source> <volume>9</volume>, <fpage>1735</fpage>&#x2013;<lpage>1780</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="other"><string-name><given-names>K.</given-names> <surname>Cho</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Van Merri&#x00EB;nboer</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Bahdanau</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name>, <article-title>On the properties of neural machine translation: Encoder-decoder approaches</article-title>. <source>arXiv preprint</source> arXiv:1409.1259, (<year>2014</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><given-names>J. M.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>G. M.</given-names> <surname>Kamath</surname></string-name>, <source>Learning the Language of the Genome using RNNs.</source></mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Harrow</surname></string-name> <etal>et al.</etal>, <article-title>GENCODE: the reference human genome annotation for The ENCODE Project</article-title>. <source>Genome research</source> <volume>22</volume>, <fpage>1760</fpage>&#x2013;<lpage>1774</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><given-names>L.</given-names> <surname>Wang</surname></string-name> <etal>et al.</etal>, <article-title>CPAT: Coding-Potential Assessment Tool using an alignment-free logistic regression model</article-title>. <source>Nucleic acids research</source> <volume>41</volume>, <fpage>e74</fpage>&#x2013;<lpage>e74</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><given-names>V.</given-names> <surname>Wucher</surname></string-name> <etal>et al.</etal>, <article-title>FEELnc: a tool for long non-coding RNA annotation and its application to the dog transcriptome</article-title>. <source>Nucleic Acids Research</source>, gkw1306 (<year>2017</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="other"><string-name><given-names>X.</given-names> <surname>Liu</surname></string-name>, <article-title>Deep Recurrent Neural Network for Protein Function Prediction from Sequence</article-title>. <source>arXiv preprint</source> arXiv:1701.08318, (<year>2017</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Hochreiter</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Heusel</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Obermayer</surname></string-name>, <article-title>Fast model-based protein homology detection without alignment</article-title>. <source>Bioinformatics</source> <volume>23</volume>, <fpage>1728</fpage>&#x2013;<lpage>1736</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="other"><string-name><given-names>S.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Hu</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Jiang</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Zeng</surname></string-name>, <article-title>TITER: predicting translation initiation sites by deep learning</article-title>. <source>bioRxiv</source>, <fpage>103374</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="book"><string-name><given-names>S. K.</given-names> <surname>S&#x00F8;nderby</surname></string-name>, <string-name><given-names>C. K.</given-names> <surname>S&#x00F8;nderby</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Nielsen</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Winther</surname></string-name>, in <source>International Conference on Algorithms for Computational Biology.</source> (<publisher-name>Springer</publisher-name>, <year>2015</year>), pp. <fpage>68</fpage>&#x2013;<lpage>80</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>