<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/069369</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The field of view available to the ventral occipito-temporal reading circuitry</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Le</surname>
<given-names>Rosemary</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Witthoft</surname>
<given-names>Nathan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ben-Shachar</surname>
<given-names>Michal</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wandell</surname>
<given-names>Brian</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Psychology Department, Stanford University</institution></aff>
<aff id="a2"><label>2</label><institution>The Gonda Brain Research Center and Department of English Literature and Linguistics</institution></aff>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><p><sup>&#x002A;</sup>These authors contributed equally to this work</p></fn>
<fn><p>Contact: Rosemary Le (<email>rosemary.le@stanford.edu</email>)</p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2016</year>
</pub-date>
<elocation-id>069369</elocation-id>
<history>
<date date-type="received">
<day>12</day>
<month>8</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>12</day>
<month>8</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2016, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2016</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="069369.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Skilled reading requires rapidly recognizing letters and word forms; people learn this skill best for words presented in the central visual field. Measurements over the last decade have shown that when children learn to read, responses within ventral occipito-temporal cortex (VOT) become increasingly selective to word forms. We call these regions the VOT reading circuitry (VOTRC). The portion of the visual field that evokes a response in the VOTRC is called the <italic>field of view (FOV)</italic>. We measured the FOV of the VOTRC and found that it is a small subset of the entire field of view available to the human visual system. For the typical subject, the FOV of the VOTRC in each hemisphere is contralaterally and foveally biased. The FOV of the left VOTRC extends &#x007E;9&#x00B0; into the right visual field and &#x007E;4&#x00B0; into the left visual field along the horizontal meridian. The FOV of the right VOTRC is roughly mirror symmetric to that of the left VOTRC. The size and shape of the FOV covers the region of the visual field that contains relevant information for reading English. It may be that the size and shape of the FOV, which varies between subjects, will prove useful in predicting behavioral aspects of reading.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>pRF</kwd>
<kwd>retinotopy</kwd>
<kwd>functional MRI</kwd>
<kwd>word perception</kwd>
<kwd>stimulus-dependence</kwd>
<kwd>model accuracy</kwd>
</kwd-group>
<counts>
<page-count count="41"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Learning to identify letters and words rapidly and reliably requires extensive visuospatial training. After training, the ability to rapidly recognize words varies across the visual field. Letters and words are recognized most efficiently in the central visual field; even when letter size is appropriately scaled, reading in the periphery remains slow (<xref ref-type="bibr" rid="c14">Chung, Mansfield, &#x0026; Legge, 1998</xref>). With eccentric viewing, word recognition is faster for text presented in the lower visual field than in the left or right visual field (<xref ref-type="bibr" rid="c44">Petre, Hazel, Fine, &#x0026; Rubin, 2000</xref>).</p>
<p>Neuroimaging and intracranial electrode measurements also provide evidence that regions in ventral occipitotemporal (VOT) cortex are part of the neural circuitry that subserves reading (<xref ref-type="bibr" rid="c18">Dehaene &#x0026; Cohen, 2011</xref>; <xref ref-type="bibr" rid="c34">Kravitz, Vinson, &#x0026; Baker, 2008a</xref>; <xref ref-type="bibr" rid="c45">Price &#x0026; Devlin, 2011</xref>; <xref ref-type="bibr" rid="c46">A. M. Rauschecker, Bowen, Parvizi, &#x0026; Wandell, 2012</xref>; Brian A. Wandell, Rauschecker, &#x0026; Yeatman, 2012). These regions are involved in the perception of visually presented word forms and are reproducibly activated in fMRI experiments across individuals and orthographies (<xref ref-type="bibr" rid="c3">Baker et al., 2007</xref>; <xref ref-type="bibr" rid="c9">Bolger, Perfetti, &#x0026; Schneider, 2005</xref>; <xref ref-type="bibr" rid="c25">Glezer &#x0026; Riesenhuber, 2013</xref>; <xref ref-type="bibr" rid="c33">Krafnick et al., 2016</xref>; <xref ref-type="bibr" rid="c40">Martin, Schurz, Kronbichler, &#x0026; Richlan, 2015</xref>). We analyze the regions within VOT that are more responsive to visually-presented words than to other categories of visual stimuli, and we refer to these regions as the VOT reading circuitry (VOTRC).</p>
<p>Yu et al. used neuroimaging and stimuli at different temporal frequencies to investigate why word identification is better in the fovea than the periphery (<xref ref-type="bibr" rid="c69">Yu, Jiang, Legge, &#x0026; He, 2015</xref>). We further analyze the role of visual field in reading by measuring the portion of the visual field where the presentation of stimuli evokes responses in the VOTRC. We refer to this part of the visual field as the region&#x2019;s field of view (FOV), and we measure it using population receptive field (pRF) methods (<xref ref-type="bibr" rid="c22">Dumoulin &#x0026; Wandell, 2008</xref>; <xref ref-type="bibr" rid="c31">K. N. Kay, Winawer, Mezer, &#x0026; Wandell, 2013</xref>).</p>
<p>The analyses described here quantify the FOV in the VOTRC in typical adult readers in a range of conditions. First, we assess the shape and position of the FOV in the VOTRC within the left hemisphere. We show that the FOV can be reliably measured and differs between subjects. The VOTRC responds to letters most strongly, but it also responds to simple contrast patterns. We find that the FOV of the same VOTRC voxels increases when we measure with checkerboards rather than words. We further analyze the responses in the VOTRC of the right hemisphere.</p>
<p>Given that responses in the VOTRC change with reading development (<xref ref-type="bibr" rid="c6">Michal Ben-Shachar, Dougherty, Deutsch, &#x0026; Wandell, 2011a</xref>; <xref ref-type="bibr" rid="c13">Brem et al., 2006</xref>; <xref ref-type="bibr" rid="c20">Dehaene et al., 2010</xref>) it is likely that the visual information relayed to the VOTRC influences reading performance. The signals in this region may serve as a bottleneck, or may influence the way a subject reads (e.g., their eye movement pattern). The FOV measurements we describe can serve as a foundation for future analyses of reading in clinical and developmental populations.</p>
</sec>
<sec id="s2">
<title>Methods</title>
<sec id="s2a">
<title>Subjects</title>
<p>Twenty right-handed subjects (12 females, median age 24 y, range 20 - 35y) participated in the study, which was approved by the Institutional Review Board at Stanford University. All subjects gave informed consent, were right-handed native English speakers, and had normal or corrected-to-normal vision with no reported history of learning disabilities.</p>
</sec>
<sec id="s2b">
<title>Displays</title>
<p>Stimuli were presented with two types of displays. Retinotopic-mapping stimuli were presented with an Eiki LC-WUL100L projector. The Eiki projector has a native resolution of 1920 &#x00D7; 1200 pixels and 10-bit color resolution. Localizer stimuli were presented with either the Eiki projector or a 47&#x2033; LCD display. The 47&#x2033; LCD display has a native resolution of 1920 &#x00D7; 1080 and 8-bit color resolution. Both displays were viewed through a mirror placed &#x007E;5cm (projector) &#x007E;15cm (LCD display) from the eyes. Stimuli projected using the LCD display had a vertical extent of 12&#x00B0;, while stimuli presented with the Eiki projector had a vertical extent of 32&#x00B0;.</p>
</sec>
<sec id="s2c">
<title>MR Acquisitions</title>
<p>Data were acquired at the Stanford Center for Cognitive and Neurobiological Imaging using a 3T General Electric MR 750 scanner with either a Nova 16- or 32-channel head coil. The Nova 16-channel is similar to the Nova 32-channel coil, but with the front half of the coil removed to allow for an unobstructed field of view. Head motion was minimized by padding around the head.</p>
<sec id="s2c1">
<title>Anatomical</title>
<p>Anatomical data were acquired with the 32-channel coil using a 3D Fast SPGR scan (166 sagittal slices, resolution 0.9375 &#x00D7; 0.9375 &#x00D7; 1mm). For each subject, 1-3 anatomical volumes were acquired and then averaged. Data were resampled to 1mm isotropic voxels and aligned to the anterior commissure - posterior commissure (AC-PC) plane using an affine transformation.</p>
</sec>
<sec id="s2c2">
<title>Functional</title>
<p>Functional data for pRF mapping and the large-field localizer (see below) were acquired with the 16-channel coil. 36 slices covering occipitotemporal cortex were defined: 2.5mm isotropic voxels, TR 2000ms, TE 29ms, flip angle 77&#x00B0;, field-of-view 200x200 mm. Functional data for the small-field localizer (see below) were acquired with the 32-channel coil. 48 slices covering occipitotemporal cortex were defined: 2.4mm isotropic voxels, TR 1000ms, TE 30ms, flip angle 62&#x00B0;, field-of-view 192 &#x00D7; 192mm. An in-plane anatomical image that matched the functional slice prescription was acquired before each set of functional runs. These images were used to align the functional data to the anatomical volume data.</p>
</sec>
</sec>
<sec id="s2d">
<title>MR Data Analysis</title>
<sec id="s2d1">
<title>Preprocessing</title>
<p>MR data analyses relied on the open-source code in vistalab (<ext-link ext-link-type="uri" xlink:href="https://github.com/vistalab/vistasoft">https://github.com/vistalab/vistasoft</ext-link>). The basic pre-processing steps included estimation and removal of motion artifacts, and registration of the functional data to the high resolution anatomical images. Motion artifacts within and across runs were corrected using an affine transformation of each volume in a session to the first volume of the first run. In all subjects, head movement was less than 1 voxel (in most cases less than 0.4 voxel). The first 6 time frames of each functional run were discarded. Baseline drifts were removed from the time series by high-pass temporal filtering.</p>
<p>The inplane anatomical image was aligned to the average whole brain T1-weighted anatomical image by calculating a rigid body transformation that maximized the mutual information between the inplane anatomy and the resliced volume anatomy. These alignment parameters were then used to align the functional data to the anatomical data.</p>
</sec>
<sec id="s2d2">
<title>Defining the VOT reading circuitry (VOTRC)</title>
<p>We defined the VOTRC using a combination of functional and anatomical constraints. One of two localizers was used to choose voxels based on their functional selectivity: a small-field or a large-field localizer. In the small-field localizer, stimuli consisted of a single item (pseudoword, body, face, place, object, number) overlaid on a phase-scrambled background (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Each phase scrambled background was generated from an image selected randomly from the entire set of images. The background image subtended a 12&#x00B0; &#x00D7; 12&#x00B0; portion of the visual field. For the word category, each word superimposed on the background spanned approximately 3&#x00B0; &#x00D7; 8&#x00B0;. We call this the small-field localizer because the category of interest was only present in the more central part of the visual field. The small-field localizer was used to identify the VOTRC in Subjects 1-12.</p>
<p>In the large-field localizer, a 30&#x00B0; &#x00D7; 30&#x00B0; portion of the visual field was tiled with either words, faces, or a phase-scrambled object (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The purpose of this localizer was to present words to both central and peripheral locations in the visual field in order to avoid biasing voxel selection to only voxels responsive to the central visual field. English words were presented rather than pseudowords (both words and pseudowords are well known to evoke strong responses in the VOTRC, (<xref ref-type="bibr" rid="c19">Dehaene, Le Clec&#x2019;H, Poline, Le Bihan, &#x0026; Cohen, 2002</xref>). The words were 1-6 letters long, had a minimum frequency of 200 per million, were displayed in Helvetica font, and the lowercase &#x2018;x&#x2019; character had a vertical size of 1.3&#x00B0;. For each of the face images, 16 faces were chosen at random from a database of 144 faces and arranged in a 4&#x00D7;4 configuration. Each face had a vertical size of approximately 6&#x00B0;. In this localizer, participants were only presented with word, face, and phase-scrambled object categories. The large-field localizer was used to identify the VOTRC in Subjects 1, 13-20.</p>
<p>In both localizers, stimuli were presented in blocks of 8 images from the same category at a rate of 2 Hz. Each category was presented 6 times per run, and 3 runs were collected per subject. The categorical stimuli for both localizers were taken from the fLoc functional localizer package ((<xref ref-type="bibr" rid="c55">Stigliani, Weiner, &#x0026; Grill-Spector, 2015</xref>), <ext-link ext-link-type="uri" xlink:href="http://vpnl.stanford.edu/fLoc/">http://vpnl.stanford.edu/fLoc/</ext-link>). Participants fixated a small red dot at the center of the screen. Zero, one, or two phase-scrambled images appeared in a block with probabilities of 0.25, 0.5, and 0.25, respectively. Participants pressed a button when a phase-scrambled image appeared.</p>
<p>The boundaries of the VOT are the inferior temporal sulcus (lateral), the collateral sulcus (medial), hV4 (posterior), and an imaginary horizontal line drawn from the collateral sulcus to the inferior temporal sulcus, starting at the point where the parieto-occipital sulcus would meet the collateral sulcus (anterior). In most subjects, hV4 is found in the posterior transverse collateral sulcus (<xref ref-type="bibr" rid="c66">N. Witthoft et al., 2014</xref>). This region is illustrated in <xref ref-type="fig" rid="fig1">Figure 1C</xref>. The selective voxels within this region in each hemisphere are defined as the left or right VOTRC. Selective voxels are more responsive to words than to other visual categories (t-test, p&#x003C;0.001, uncorrected), but not necessarily contiguous. The VOTRC falls in or near the occipital temporal sulcus (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) in both hemispheres. The VOTRC as defined for all subjects is shown in <xref ref-type="fig" rid="figS1">Supplementary Figure 1</xref>.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Functional and anatomical definition of the VOTRC.</title>
<p><bold>A. The small-field localizer</bold> presents a series of images from a single category, such as words, faces, objects and phase-scrambled objects. Each image is presented within the central 10&#x00B0; of the visual field. <bold>B. The large-field localizer</bold> presents a series of stimuli that each span the extent of the screen (32&#x00B0;). Stimuli are tiled with words or faces, or contain phase scrambled versions of the other categories. <bold>C. The VOTRC</bold> is the set of voxels (not-necessarily contiguous) within the VOT that are more responsive to words than to other categories. The VOT is bounded medially by the collateral sulcus, laterally by the inferior temporal sulcus, posteriorly by hV4, and anteriorly by an imaginary line drawn from the collateral sulcus to the inferior temporal sulcus, starting at the point where the parieto-occipital sulcus meets the collateral sulcus. Shown here is Sub01, large-field localizer.</p></caption>
<graphic xlink:href="069369_fig1.tif"/>
</fig>
</sec>
<sec id="s2d3">
<title>PRF mapping</title>
<p>Stimuli for the the pRF mapping experiment were presented using the Eiki LC-WUL100L projector and controlled by code in the Psychophysics Toolbox (<xref ref-type="bibr" rid="c12">Brainard, 1997</xref>). A small dot (0.15&#x00B0; visual angle in diameter) at the center of the screen served as the fixation point. Participants were instructed to fixate the dot and to press a button when the dot changed color (randomly between red, green, and black every 1-5 seconds). Participants maintained fixation as a moving bar swept through the visual field in 4 orientations (0&#x00B0;, 45&#x00B0;, 90&#x00B0;, and 135&#x00B0; from vertical) and 2 motion directions. Eye-tracking was not used, but sharp hemifield and quarterfield pRFs in early visual field maps (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, <xref ref-type="fig" rid="fig5">Figure 5CD</xref>) suggest that stable fixation was maintained by participants.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Stimulus dependence in word-responsive regions.</title>
<p>Panels show the group average FOV (n=20) for the left VOTRC (A-B) and left V1 (C-D). The FOV in 5A is identical to the one shown in 3A. Population RF measurements made using word stimuli are shown on the left (A, C). Population RF measurements made using checkerboard stimuli are shown on the right (B, D). The dashed lines are the half-max contour of the group average. The FOV for each stimulus is estimated using only voxels where the pRF model explains at least 20&#x0025; of the variance. For this reason, voxels included in the calculation of the FOV shown in A and B overlap, but are not identical. See <xref ref-type="fig" rid="fig6">Figure 6C</xref> for a comparison of group average FOVs within identical voxels.</p></caption>
<graphic xlink:href="069369_fig5.tif"/>
</fig>
<p>In one set of runs, checkerboard stimuli were used as the contrast pattern within the bar. The checkerboards within the bars drifted parallel to the orientation of the bar and had a contrast reversal rate of 2 Hz. The size of each checkerboard square was approximately 1.3&#x00B0; per side. The checkerboard stimuli are described in more detail in previous work (<xref ref-type="bibr" rid="c1">Amano, Wandell, &#x0026; Dumoulin, 2009</xref>; <xref ref-type="bibr" rid="c22">Dumoulin &#x0026; Wandell, 2008</xref>). Each run was 192 seconds long. Three runs of checkerboard retinotopy were collected for each subject. Four blank periods (12s each) were interleaved to estimate larger pRF sizes (<xref ref-type="bibr" rid="c22">Dumoulin &#x0026; Wandell, 2008</xref>).</p>
<p>In the other set of runs, word stimuli were used as the contrast pattern within the bar to better drive responses in the VOTRC. The words were rendered in black Helvetica font on a white background. The vertical extent of the lower-case letter &#x2018;x&#x2019; was 1.3&#x00B0;. One-to-six letter words with frequency greater than 200 per million were used as the lexicon; words were randomly chosen from the lexicon to create a page of text. The text within the aperture was refreshed at 4 Hz. Each run lasted 5 min and two runs were collected. The runs of word and checkerboard stimuli were interleaved, and the order balanced across subjects.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>PRF mapping: Stimuli and visual field coverage.</title>
<p><bold>A. Stimuli.</bold> Stimuli consisted of bars (4&#x00B0; wide) that slowly and repeatedly traversed the visual field in eight different directions. The pattern within the bar contained dynamic checkerboards or words. <bold>B. FOV for left V1, V2v and V3v from single subjects.</bold> The points represent the pRF centers. The color indicates the largest pRF value across the collection of voxels in the ROI. This value can be interpreted as the relative effectiveness of stimuli evoking a response in the ROI. Population RFs were measured with word stimuli. Shown are the pRFs in V1 (left panels), V2v (middle panels) and V3v (right panels) of 2 participants, Sub04 (top row) and Sub16 (bottom row).</p></caption>
<graphic xlink:href="069369_fig2.tif"/>
</fig>
<p>We model the population receptive field in each voxel using the compressive spatial summation (CSS) model (<xref ref-type="bibr" rid="c31">K. N. Kay et al., 2013</xref>). The CSS model estimates the pRF of a voxel by selecting parameters for a 2D Gaussian that optimally predicts the response to a translating stimulus (moving bars). The parameters of the pRF include location (x&#x00B0;,y&#x00B0;) and size (&#x03C1;&#x00B0;) in degrees of visual angle. The value of the 2D Gaussian at its peak is normalized to 1. The CSS model extends earlier linear models (<xref ref-type="bibr" rid="c1">Amano et al., 2009</xref>; <xref ref-type="bibr" rid="c22">Dumoulin &#x0026; Wandell, 2008</xref>) by including a compressive nonlinear response exponent to account for subadditive responses which appear to increase across the visual hierarchy.</p>
</sec>
<sec id="s2d4">
<title>Estimating the FOV for a region of interest (ROI)</title>
<p>The FOV of a region of interest (ROI) defines the portion of the visual field that reliably evokes a response in any of the voxels in the ROI. The FOV of an ROI is obtained by aggregating over the voxels&#x2019; pRFs (the portion of the visual field where stimuli reliably drive responses). Each voxel has a pRF that is modeled as a 2D Gaussian in the visual field. The peak of the Gaussian is normalized to 1. The FOV of an ROI is obtained in the following way: at each point in the visual field, the FOV value is the maximum pRF value over the voxels. Unless otherwise stated, only voxels with greater than 20&#x0025; variance explained by the pRF model are used in measuring the FOV.</p>
<p>To reduce the effect of pRFs that are anomalous in size and/or position, a bootstrapping procedure is applied (<xref ref-type="bibr" rid="c65">Winawer &#x0026; Witthoft 2015</xref>, <xref ref-type="bibr" rid="c64">Winawer et al 2010</xref>, <xref ref-type="bibr" rid="c1">Amano et al 2009</xref>). Given a collection of N voxels in the subject&#x2019;s ROI, N voxels are sampled with replacement, and the FOV is calculated for these N voxels. This procedure is repeated 50 times. The average of the 50 FOV samples is the FOV of the ROI. The FOV is a map of continuous values, so to characterize its size and shape, we describe the half-max contour, a contour encompassing the part of the visual field where the FOV values exceed 0.5. Example measurements showing the FOV and half-max contour are shown in <xref ref-type="fig" rid="fig2">Figure 2B</xref> (for V1, V2v, and V3v) and <xref ref-type="fig" rid="fig3">Figure 3B</xref> (for left VOTRC). The group summary FOV is obtained from averaging over the individuals&#x2019; FOVs (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). In FOV measurements where voxels are thresholded at 20&#x0025; variance explained, subjects with no voxels pass threshold are assigned a FOV of all zeros.</p>
</sec>
<sec id="s2d5">
<title>Quantifying the similarity between FOVs</title>
<p>The Dice similarity coefficient is used to quantify the similarity between FOVs. This is done by first representing the FOV as 128&#x00D7;128 matrix F, where each element of F can be thought of as a pixel with a value ranging between 0 and 1. A 128&#x00D7;128 binary matrix B is then obtained from F, where an element in B equals 1 if the corresponding pixel in F is greater than or equal to 0.5. In other words:
<disp-formula id="ueqn1"><alternatives><graphic xlink:href="069369_ueqn1.gif"/></alternatives></disp-formula>
For binary matrices B1 and B2 from the two FOVs being compared, the Dice coefficient equals the number of pixel locations where both B1 and B2 equal 1 divided by the average of the number of pixels in B1 and B2 that equal 1. In other words:
<disp-formula id="ueqn2"><alternatives><graphic xlink:href="069369_ueqn2.gif"/></alternatives></disp-formula>
For two FOVs with exactly overlapping half-max contours, the Dice coefficient is 1. If the half-max contours do not overlap at all, the Dice coefficient is 0.</p>
</sec>
</sec>
</sec>
<sec id="s4">
<title>Results</title>
<sec id="s4a">
<title>The FOV of the VOTRC</title>
<p>We begin by considering FOV measurements made using words as the contrast pattern. The left VOTRC and the FOV for a single subject are shown in <xref ref-type="fig" rid="fig3">Figure 3A</xref> and <xref ref-type="fig" rid="fig3">B</xref>. The FOV is biased for the central portion of the contralateral visual field. In the typical subject, more than 98&#x0025; of pRF centers (median across subjects, SD 11&#x0025;) are located within 5 degrees of fixation. More than 97&#x0025; of pRF centers (median across subjects, SD 18&#x0025;) are located in the right visual field. The half-max contour extends further along the horizontal than the vertical meridian (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). The FOV of the group (n=20) is shown in <xref ref-type="fig" rid="fig3">Figure 3C</xref>. The group average FOV demonstrates the same foveal and contralateral bias as the individual data.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>The FOV of the left VOT reading circuitry.</title>
<p><bold>A. Left VOTRC definition.</bold> A representative example of the left VOTRC (outlined in black) in a single subject. These voxels are more responsive to words than to other categories of visual stimuli (t-test, p &#x003C; 0.001 uncorrected) in the large-field localizer (stimuli are shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref>). Colors show the variance explained by the pRF fits using the CSS model with word stimuli. Colored voxels (with greater than 20&#x0025; variance explained) were included in the FOV calculation. <bold>B. The FOV of the left VOTRC.</bold> Population RFs are estimated in response to word stimuli (the stimulus is shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref>, right panel). The color map indicates the FOV value across the visual field. The dashed line is the half-max contour. <bold>C. The group average FOV of the left VOTRC.</bold> The average FOV of all 20 subjects is shown. The dashed line indicates the half-max contour of the average FOV.</p></caption>
<graphic xlink:href="069369_fig3.tif"/>
</fig>
</sec>
<sec id="s4b">
<title>The FOV is reliable within subjects and varies between subjects</title>
<p>To assess within-subject reliability, we visualize the half-max contours from pRF models
fit to individual runs of data (<xref ref-type="fig" rid="fig4">Figure 4</xref>). For the most part, the independent half-max contours cover similar parts of the visual field (e.g., Sub04-07, Sub10-11, Sub16). In some cases the contours from the two runs differ substantially (e.g., Sub14, Sub20). In those cases, we typically find that one of the runs matches the FOV fit, and that this run was better fit by the pRF model. The data suggest that accurate FOV estimates require multiple (at least 2) independent runs.</p>
<p>The FOV of the left VOTRC varies between individuals. There are subjects in which both runs reveal small FOVs (Sub04) and others in which individual runs reveal large FOVs (Sub16). In other subjects the FOV seems reliably oriented in different directions (Sub05 and Sub06). Despite these differences, the half-max contour covers the ipsilateral foveal portion of the visual field for all subjects. The median between-subject Dice coefficient is 0.62 (minimum 0.26, maximum 0.88).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Field of view for the left VOTRC differs between subjects (n=20).</title>
<p>Each panel depicts the FOV of the left VOTRC of a single participant. The color map and the gray dots (representing pRF centers) are calculated using voxels that exceed 20&#x0025; variance explained from the pRF model fit to two runs of word stimuli. For the same voxels, the dashed lines outline the half-max contours from pRF models fit independently to run 1 and run 2.</p></caption>
<graphic xlink:href="069369_fig4.tif"/>
</fig>
</sec>
<sec id="s4c">
<title>The FOV of the VOTRC, but not V1, is stimulus dependent</title>
<p>We measured pRFs using two different types of stimuli within the moving bar aperture: checkerboards and words. When measured with words, the average FOV of the left VOTRC extends 9&#x00B0; along the horizontal meridian in the contralateral visual field and 4&#x00B0; in the ipsilateral visual field (N=20, <xref ref-type="fig" rid="fig5">Figure 5A</xref>). When measured with checkerboard stimuli, the half-max contour of these same voxels extends further into the periphery but remains biased for the horizontal meridian (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). The FOV does not change with the stimulus in early visual cortex. For example, the FOV of left V1 covers the contralateral hemifield when measured with words or checkerboard stimuli (<xref ref-type="fig" rid="fig5">Figure 5CD</xref>), in agreement with <xref ref-type="bibr" rid="c65">Winawer &#x0026; Witthoft (2015)</xref> and <xref ref-type="bibr" rid="c1">Amano (2009)</xref>.</p>
</sec>
<sec id="s4d">
<title>Stimulus dependency of the FOV is not explained by a difference in voxel selection or model accuracy</title>
<p>We consider two methodological explanations for the stimulus dependence of the FOV in left VOTRC (<xref ref-type="fig" rid="fig5">Figure 5A</xref> vs. <xref ref-type="fig" rid="fig5">5B</xref>). One explanation may be due to the fact that non-identical voxels in left VOTRC pass the 20&#x0025; variance explained threshold for each stimulus type. This means that different voxels are used in the calculation of the FOV in <xref ref-type="fig" rid="fig5">Figures 5A</xref> and <xref ref-type="fig" rid="fig5">5B</xref>. However, choosing identical voxels for the calculation of the FOV results in the same stimulus dependence (see <xref ref-type="fig" rid="fig6">Figure 6C</xref> and <xref ref-type="fig" rid="figS4">Supplementary 4</xref> and <xref ref-type="fig" rid="figS5">5</xref>), thus the stimulus dependency is not due to the inclusion of different voxels for each stimulus type. Another explanation for the stimulus dependence may be due to differences in model accuracy: the pRF model explains the responses to words more accurately compared to checkerboards (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). To investigate the possibility of the stimulus dependency being due to decreased model accuracy, we added Gaussian noise (&#x007E;N(0,1.5)), to the left VOTRC responses (dashed blue lines, <xref ref-type="fig" rid="fig6">Figure 6B</xref>) and refit the pRF model. After adding noise, the mean model accuracy for this noisy time series is less than the mean model accuracy for the original word time series and the checkerboard time series (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). We then estimated the FOVs for the noisy time series in the same set of voxels that were used to calculate the word FOVs of each participant. We find that the group averaged FOV estimated using the words&#x002B;noise time course is very similar to the FOV obtained using the word responses (Dice coefficient 0.97, <xref ref-type="fig" rid="fig6">Figure 6C</xref>). This simulation demonstrates that the stimulus dependency on words and checkerboards is not explained by lower model accuracy and that the model is relatively robust to noise.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Noise simulation.</title>
<p><bold>A. Variance explained.</bold> Purple dots represents the mean variance explained in the left VOTRC for individual subjects, restricted to the voxels that have &#x003E; 20&#x0025; variance explained when measured with word stimuli. Error bars represent the standard deviation over voxels. Green dots show the variance explained in the same voxels when measured using checkerboards. Blue dots show the variance explained when Gaussian noise (s.d. 1.5&#x0025;) is added to these voxels. <bold>B. Example time series.</bold> The time series of a voxel in the left VOTRC of Sub01 averaged over two runs as the subject viewed moving bars with words in the aperture (purple). The time series with artificially introduced Gaussian noise (s.d. 1.5&#x0025;, dashed blue line). <bold>C. Group averaged FOVs of left VOTRC.</bold> Each panel shows the FOV for a different condition: words (left, same as <xref ref-type="fig" rid="fig3">Figure 3A</xref>), checkerboards (middle), and words&#x002B;noise (right). The dashed contours indicate the half-max of the group average. These analyses are restricted to voxels that exceed 20&#x0025; variance explained when measured with word stimuli, so that the voxels for each condition are identical (See <xref ref-type="fig" rid="figS4">Supplementary 4</xref> and <xref ref-type="fig" rid="figS5">5</xref> for the complementary analysis using checkerboard responses for voxel selection). Other details as in <xref ref-type="fig" rid="fig3">Figures 3C</xref> and <xref ref-type="fig" rid="fig5">5</xref>.</p></caption>
<graphic xlink:href="069369_fig6.tif"/>
</fig>
</sec>
<sec id="s4e">
<title>The FOV of VOTRC is similar for two different localizers</title>
<p>In the small-field functional localizer, word images appear within the central 8&#x00B0; of the visual field (collected on Subjects 1-12). Restricting the stimuli to a small portion of the visual field may bias the voxel selection process towards voxels with a foveal preference. To address this concern, we use a large-field localizer in 9 additional subjects (Subjects 1, 13-20). The large-field localizer contains words that tile the central 32&#x00B0; and should identify word-responsive voxels in VOT that have positional preferences anywhere within this part of the visual field.</p>
<p>The VOTRC defined with the large-field localizer is similar to the definition with the small-field localizer (see <xref ref-type="fig" rid="figS1">Supplementary 1</xref>). One subject took part in both localizers and the difference is roughly what we find for repeated measurements (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). The mean surface area of word-responsive cortex in VOT is 31 mm2 for the small-field and 39 mm2 for the large-field functional localizer, both with a standard error near 8 mm2 (surface areas for each subject are listed in <xref ref-type="fig" rid="figS2">Supplementary 2B</xref>). The group average FOV estimated with the two localizers is also similar (Dice coefficient = 0.91). In both cases, the FOV of the left VOTRC is biased for the horizontal meridian, extending &#x007E;9&#x00B0; in the contralateral visual field and &#x007E;4&#x00B0; in the ipsilateral visual field (<xref ref-type="fig" rid="fig7">Figure 7B</xref>).</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><title>FOV comparison using the small- and large-field localizers.</title>
<p><bold>A.</bold> The left VOTRC (black outline) in Sub01 as defined by the 2 localizers: small-field (left) and large-field (right). Colored voxels have variance explained greater than 20&#x0025; by the pRF model (word stimuli). <bold>B.</bold> The left panel shows the average FOV for Sub1-12 whose left VOTRC was defined using the small-field localizer. The right panel shows the average FOV for Sub1,13-20 whose left VOTRC was defined using the large-field localizer. The gray curves are 50 half-max contours of the bootstrapped group averages.</p></caption>
<graphic xlink:href="069369_fig7.tif"/>
</fig>
</sec>
<sec id="s4f">
<title>The FOV of right VOTRC is contralaterally- and foveally- blased</title>
<p>The localizer experiments produced word-selective voxels in both hemispheres, allowing for the definition of a right VOTRC in every subject (<xref ref-type="bibr" rid="c8">M. Ben-Shachar, Dougherty, Deutsch, &#x0026; Wandell, 2007</xref>; <xref ref-type="bibr" rid="c15">Cohen et al., 2002</xref>; <xref ref-type="bibr" rid="c24">Glezer, Jiang, &#x0026; Riesenhuber, 2009</xref>; <xref ref-type="bibr" rid="c46">A. M. Rauschecker et al., 2012</xref>; <xref ref-type="bibr" rid="c57">Vigneau, Jobard, Mazoyer, &#x0026; Tzourio-Mazoyer, 2005</xref>). Like the left VOTRC, the FOV of the right VOTRC exhibits a foveal and contralateral bias, extending 8&#x00B0; into the contralateral visual field and 3&#x00B0; into the ipsilateral visual field (<xref ref-type="fig" rid="fig8">Figure 8A</xref>). The Dice coefficient between the FOVs of the left and right VOTRC (when flipped) is 0.86; the two FOVs are mirror symmetric, with the right VOTRC FOV being slightly smaller (compare <xref ref-type="fig" rid="fig8">Figure 8B</xref> with <xref ref-type="fig" rid="fig5">Figure 5A</xref>, or see <xref ref-type="fig" rid="figS2">Supplementary 2A</xref>).</p>
<p>The distribution of pRF parameters in the left and right VOTRC are shown in <xref ref-type="fig" rid="figS2">Supplementary 2C-F</xref>. The principle difference is that there are more word-selective voxels in the left hemisphere (<xref ref-type="fig" rid="figS2">Supplementary 2B</xref>). Of the word-selective voxels, 49&#x0025; in left VOTRC and 48&#x0025; in right VOTRC have variance explained that exceeds 20&#x0025;. The FOV of the combined left-right VOTRC covers an elliptical portion of the visual field that is centered on the fovea and extends mainly along the horizontal meridian (<xref ref-type="fig" rid="fig8">Figure 8D</xref>).</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8.</label>
<caption><title>FOV of the right VOTRC and of the combined VOTRC.</title>
<p><bold>A.</bold> Right VOTRC FOV for a representative subject. <bold>B.</bold> Group average FOV of the right VOTRC. In one subject, no voxels passed threshold. <bold>C.</bold> Left and right (combined) VOTRC FOV for a representative subject. <bold>D.</bold> Group average FOV of the combined VOTRC. The dashed lines indicate the half-max contours of individual runs. Gray dots in A and C represent pRF centers in the model fit to the average of the two runs. The gray contours are the half-max of 50 bootstrapped group averages.</p></caption>
<graphic xlink:href="069369_fig8.tif"/>
</fig>
</sec>
<sec id="s4g">
<title>The VOTRC&#x2019;s FOV is restricted to a central region of the VOT&#x2019;s FOV</title>
<p>While there is variation in the size and shape of the FOV between subjects, the FOV of the combined left and right VOTRC of the average subject has a distinctive shape. The average FOV extends about 9&#x00B0; along the horizontal meridian and about 6&#x00B0; along the vertical meridian. The FOVs of the left and right VOTRC are roughly mirror symmetric, although the FOV of the right VOTRC is slightly smaller. The FOV of the combined VOTRC is a fraction of the FOV of the full VOT, which extends farther into the periphery in both the horizontal and vertical directions (<xref ref-type="fig" rid="fig9">Figure 9</xref>).</p>
<fig id="fig9" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9.</label>
<caption><title>Comparison of FOVs for the VOT and the VOTRC.</title>
<p>The dashed lines indicate half-max contours of the group average FOV. The outer contour and the color map show the FOV of the VOT, both hemispheres (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). The inner contour shows the FOV of the combined VOTRC (<xref ref-type="fig" rid="fig8">Figure 8D</xref>).</p></caption>
<graphic xlink:href="069369_fig9.tif"/>
</fig>
</sec>
<sec id="s4h">
<title>PRF model accuracy</title>
<p>Model accuracy is derived from two calculations that use the root mean squared error (RMSE)
<disp-formula id="ueqn3"><alternatives><graphic xlink:href="069369_ueqn3.gif"/></alternatives></disp-formula>
where n = 144 (number of time points) and xi and yi correspond to the values of the time series being compared at a specific time point i. The first RMSE calculation is between independent time series data collected in runs 1 and 2. We call this Datarmse, and it can be interpreted as a measure of test-retest reliability. The second RMSE calculation is between the the model prediction obtained from run 1 and the actual time series measured in run 2. We call this Modelrmse. Since Run 2 is independent of run 1, Modelrmse is a cross-validated error measure of the model prediction. Finally, Relativermse is the ratio of the two RMSE calculations: Modelrmse / Datarmsee and we use this ratio to summarize model accuracy (see <xref ref-type="fig" rid="fig9">Figure 9</xref>). All time series are responses obtained from pRF mapping with word stimuli.</p>
<p>More than 99&#x0025; of the combined VOTRC voxels have a Relativermse &#x003C; 1. This means the model predicts run 2 with higher accuracy (smaller error) than the test-retest reliability (Modelrmse is smaller than Datarmse). A model is better than test-retest reliability when it captures an essential part of the response and averages out measurement noise. If a model is a perfect description of the mean signal and the measurement noise at each time point is a Gaussian distribution, the Relativermse values should be at 1/sqrt(2) = 0.707 (<xref ref-type="bibr" rid="c49">Rokem et al., 2015</xref>). The median value of the Relativermse across all VOTRC voxels is 0.73, close to the level of a model that extracts all of the reliable information available in the data.</p>
<fig id="ufig1" position="float" orientation="portrait" fig-type="figure">
<graphic xlink:href="069369_ufig1.tif"/>
</fig>
</sec>
</sec>
<sec id="s5">
<title>Discussion</title>
<sec id="s5a">
<title>Position sensitivity in the VOT</title>
<p>The VOTRC is embedded within a larger cortical territory (VOT) that contains several other functionally specialized regions (e.g. <xref ref-type="bibr" rid="c23">Epstein &#x0026; Kanwisher, 1998</xref>; <xref ref-type="bibr" rid="c26">Grill-Spector &#x0026; Weiner, 2014</xref>). We consider position sensitivity and FOV estimates of the VOT generally and then discuss specific features of the reading circuitry.</p>
<p>There is consensus that the functional responses of many VOT voxels are sensitive to stimulus visual field position (e.g., (<xref ref-type="bibr" rid="c2">Arcaro, McMains, Singer, &#x0026; Kastner, 2009</xref>; <xref ref-type="bibr" rid="c35">Kravitz, Vinson, &#x0026; Baker, 2008b</xref>; <xref ref-type="bibr" rid="c56">Uyar, Shomstein, Greenberg, &#x0026; Behrmann, 2016</xref>). For example, more lateral VOT regions including the fusiform gyrus are more responsive to stimuli within 1.5&#x00B0; of the fovea while medial regions, including the collateral sulcus and lingual gyrus, are relatively more responsive to stimuli presented at greater than 7.5&#x00B0; eccentricity (see <xref ref-type="fig" rid="fig4">Figure 4</xref> in <xref ref-type="bibr" rid="c28">Hasson, Levy, Behrmann, Hendler, &#x0026; Malach, 2002</xref>. The VOT in general is more responsive to stimuli presented in the contralateral than ipsilateral visual field (<xref ref-type="bibr" rid="c34">Kravitz et al., 2008a</xref>; <xref ref-type="bibr" rid="c46">A. M. Rauschecker et al., 2012</xref>).</p>
<p>Several groups have used pRF mapping and FOV measurements to characterize extrastriate visual cortex (<xref ref-type="bibr" rid="c1">Amano et al., 2009</xref>; <xref ref-type="bibr" rid="c37">Larsson &#x0026; Heeger, 2006</xref>; <xref ref-type="bibr" rid="c60">Brian A. Wandell &#x0026; Winawer, 2011</xref>). A number of recent papers have specifically measured pRFs and FOV in VOT regions located near the reading circuitry (<xref ref-type="bibr" rid="c29">Kendrick N. Kay, Weiner, &#x0026; Grill-Spector, 2015</xref>; <xref ref-type="bibr" rid="c51">Silson, Chan, Reynolds, Kravitz, &#x0026; Baker, 2015</xref>). <xref ref-type="bibr" rid="c51">Silson et al. (2015)</xref> report that pRF centers in the occipital face area (OFA) and fusiform face area (FFA) are located within the central 5&#x00B0; and have a contralateral bias. They report that the FOVs in these two regions represent the lower (OFA) and upper (FFA) quarterfields. <xref ref-type="bibr" rid="c29">Kay et al. (2015)</xref> and <xref ref-type="bibr" rid="c67">Witthoft et al. (2016)</xref> agree that face-selective regions have a contralateral bias and that pRF centers are consistently near the fovea. They do not report the lower/upper field bias in apparently corresponding face-selective regions (IOG and FFA respectively).</p>
<p>The pRF model implementations and the cortical regions of interest differ between these three studies. Kay et al. use a model that includes a compressive nonlinearity (<xref ref-type="bibr" rid="c31">K. N. Kay et al., 2013</xref>). Silson et al. use a linear model implemented within AFNI, and Witthoft et al. use a linear model implemented in Vistasoft. In addition to the model differences, the anatomical definitions differ. Kay et al. and Witthoft et al. subdivide the FFA into two distinct regions, mid-fusiform (mFus) and posterior fusiform (pFus) (<xref ref-type="bibr" rid="c62">Weiner &#x0026; Grill-Spector, 2012</xref>). <xref ref-type="bibr" rid="c51">Silson et al. (2015)</xref> refer to the OFA which may not be precisely the same as the IOG. Hence, the differences in findings, primarily the upper/lower visual field bias, may be due to the variation of methods used between groups. The contralateral and foveal biases that remain despite the methodological differences suggest that these are robust properties.</p>
</sec>
<sec id="s5b">
<title>Lateralization of the VOTRC</title>
<p>Most work on word-responsive regions in visual cortex focuses on the left hemisphere (e.g. <xref ref-type="bibr" rid="c11">Bouhali et al., 2014</xref>) because the responses in the left are more selective and larger than those in the right (<xref ref-type="bibr" rid="c15">Cohen et al., 2002</xref>; others). Additionally, there is a substantial neurological literature that associates alexia with lesions to the left hemisphere and emphasizes differences in representational capacity between the two hemispheres (<xref ref-type="bibr" rid="c17">Damasio &#x0026; Damasio, 1983</xref>; <xref ref-type="bibr" rid="c18">Dehaene &#x0026; Cohen, 2011</xref>; <xref ref-type="bibr" rid="c21">Dejerine, 1892</xref>). Nonetheless, it is important to recognize that word-selective responses are reliably observed in both hemispheres (<xref ref-type="bibr" rid="c5">M. Ben-Shachar et al., 2007</xref>; <xref ref-type="bibr" rid="c15">Cohen et al., 2002</xref>; <xref ref-type="bibr" rid="c24">Glezer et al., 2009</xref>; <xref ref-type="bibr" rid="c46">A. M. Rauschecker et al., 2012</xref>; <xref ref-type="bibr" rid="c57">Vigneau et al., 2005</xref>). Evidence in callosal patients suggests that the right hemisphere alone is sufficient for some reading functionality (<xref ref-type="bibr" rid="c4">Baynes, Tramo, &#x0026; Gazzaniga, 1992</xref>). The left lateralization of responses emerges during development, and some authors propose that learning to read reduces word-selective responses in the right hemisphere (<xref ref-type="bibr" rid="c71">Shaywitz et al., 2007</xref>; <xref ref-type="bibr" rid="c52">Simos, Breier, Fletcher, Bergman, &#x0026; Papanicolaou, 2000</xref>). The FOV of the left VOTRC is approximately mirror symmetric to the right VOTRC (<xref ref-type="fig" rid="fig8">Figure 8CD</xref>). The principal difference is that there are more word-selective voxels in the left hemisphere (<xref ref-type="fig" rid="figS2">Supplementary 2B</xref>).</p>
<p>Some earlier models of object recognition (<xref ref-type="bibr" rid="c48">Riesenhuber &#x0026; Poggio, 1999</xref>; <xref ref-type="bibr" rid="c50">Rolls, 2000</xref>) and earlier experiments involving the visual word form area (VWFA; (<xref ref-type="bibr" rid="c15">Cohen et al., 2002</xref>; <xref ref-type="bibr" rid="c18">Dehaene &#x0026; Cohen, 2011</xref>) report that left hemisphere responses to words are invariant to position in the visual field. This summary is at odds with the contralateral bias of the FOV described here. But examination of the data in these earlier experiments shows that responses in the VWFA are indeed elevated for words presented in the right visual field (e.g., <xref ref-type="fig" rid="fig5">Figure 5</xref> in <xref ref-type="bibr" rid="c15">Cohen et al., 2002</xref>, also see <xref ref-type="fig" rid="fig5">Figure 5</xref> in <xref ref-type="bibr" rid="c5">M. Ben-Shachar et al., 2007</xref>; <xref ref-type="fig" rid="fig5">Figure 5</xref> in (<xref ref-type="bibr" rid="c46">A. M. Rauschecker et al., 2012</xref>). Hence, the findings here are consistent with those reports, and we extend them by establishing a quantitative FOV measurement.</p>
</sec>
<sec id="s5c">
<title>Stimulus dependency within the VOT</title>
<p>Using checkerboards rather than words within the aperture decreases the variance explained by the pRF model and increases the FOV (<xref ref-type="fig" rid="fig5">Figure 5</xref>). This effect is not well explained by methodological considerations, as we showed in detail (<xref ref-type="fig" rid="fig6">Figure 6</xref>, <xref ref-type="fig" rid="figS4">Supplementary 4</xref> and <xref ref-type="fig" rid="figS5">5</xref>). We discuss two possible explanations of the stimulus dependence of the FOV.</p>
<p>One possibility is that word stimuli and checkerboard stimuli are carried to the VOT through different pathways that carry information from slightly different FOVs. For example, <xref ref-type="bibr" rid="c47">Rauschecker et al. (2011)</xref> proposed that pattern contrast and motion contrast are communicated to the VOTRC via different routes. An alternative possibility is that different neuronal populations within the voxel preferentially respond to checkerboard and word stimuli (<xref ref-type="bibr" rid="c27">Harvey, Fracasso, Petridou, &#x0026; Dumoulin, 2015</xref>). Stimulus dependence could occur if these populations have different FOVs.</p>
<p>Currently our model represents the input stimulus as a binary mask that indicates where there is some stimulus contrast (i.e. only indicating the position of the moving bar and not the image within the bar). A model that takes into account the image within the moving bar may be able to account for some of the stimulus dependency. Further, it may be possible to create circuit models that account for the contributions of multiple cortical processing regions (<xref ref-type="bibr" rid="c32">K. Kay &#x0026; Yeatman, 2016</xref>); such models may also account for the stimulus dependence.</p>
</sec>
<sec id="s5d">
<title>Relationship to psychophysical measures of reading: Visual and perceptual span</title>
<p>The data suggest that the relatively large differences we observe between the FOVs in different subjects are reliable (<xref ref-type="fig" rid="fig4">Figure 4</xref>, compare Sub13 and Sub16). The FOV measurements describe a characteristic of the reading circuitry, and the size and shape of the FOV may be related to certain psychophysical measures of reading. We suspect there may be a compliance range for the field of view in the VOTRC, such that an inadequate (too small) or an irrelevant (too large) FOV disrupts processes that are necessary for learning to rapidly recognize words. If this is true, then the FOV will be predictive of some reading-related measures. Two candidate psychophysical measures that may be predicted by the shape and size of the FOV are the visual span (<xref ref-type="bibr" rid="c38">Gordon E. Legge, Ahn, Klitz, &#x0026; Luebker, 1997</xref>) and the perceptual span (<xref ref-type="bibr" rid="c41">McConkie &#x0026; Rayner, 1975</xref>).</p>
<p>The visual span is the number of letters of a given size, arranged horizontally, that can be reliably recognized without moving the eyes. In typical readers of English under typical reading conditions, the expected range of visual span values is about 7-11 letters (<xref ref-type="bibr" rid="c38">Gordon E. Legge et al., 1997</xref>). Individual differences in reading speed depend on multiple factors, and the visual span positively correlates with reading speed and accounts for a 34-52&#x0025; of this variation (<xref ref-type="bibr" rid="c36">Kwon, Legge, &#x0026; Dubbels, 2007</xref>; <xref ref-type="bibr" rid="c39">G. E. Legge et al., 2007</xref>). The visual span can also be thought of as the number of letters that are not crowded (<xref ref-type="bibr" rid="c43">Pelli et al., 2007</xref>). At the age where children learn to read, the crowding effect decreases with age (<xref ref-type="bibr" rid="c10">Bondarko &#x0026; Semenov, 2004</xref>).</p>
<p>The perceptual span, assesses reading performance while allowing for the influence of contextual information and linguistic factors. The perceptual span is defined as the region of the visual field available to the reader at a single fixation which allows the individual to maintain their typical reading speed (<xref ref-type="bibr" rid="c41">McConkie &#x0026; Rayner, 1975</xref>). In typical English readers, the perceptual span encompasses about 15 characters to the right of fixation and 3-4 characters to the left of fixation (<xref ref-type="bibr" rid="c41">McConkie &#x0026; Rayner, 1975</xref>). <xref ref-type="bibr" rid="c42">O&#x2019;Regan (1991)</xref> describes the difference between perceptual span and visual span: &#x201C;&#x2026;&#x2018;visual span&#x2019; refers to what can be seen without the help of linguistic knowledge or context, whereas perceptual span includes what can be seen with that help&#x201D;. The visual and perceptual spans are often studied separately but it seems likely that the size of the perceptual span is influenced by the size of the visual span.</p>
<p>Variations of the input from early visual cortex to the reading circuitry may influence reading performance. The between-subject differences in the FOV (<xref ref-type="fig" rid="fig4">Figure 4</xref>) may predict individual differences in visual span or even reading speed at different locations in the visual field. However, linking the FOV to behavior will require more theoretical development and additional measurements. As an example of the complexity, note that other systems (such as eye movements, language and memory) contribute to reading so that differences in the FOV may not map directly to differences in reading performance.</p>
</sec>
<sec id="s5e">
<title>Implications for reading development for impaired and non-impaired readers</title>
<p>The VOTRC represents a restricted subset of the information available from earlier retinotopic cortical regions (Brian A. Wandell et al., 2012). From a developmental perspective, one possibility is that the restricted FOV is present even prior to print exposure. Another possibility is that the foveal and horizontal bias in the FOV of the VOTRC is carved out during the early school years, as the VOT circuitry develops enhanced sensitivity to written words (<xref ref-type="bibr" rid="c7">Michal Ben-Shachar, Dougherty, Deutsch, &#x0026; Wandell, 2011b</xref>). These two hypotheses could be discriminated by measuring the FOV in pre-reading children (4-5y) using words and checkerboards. If the FOV develops through education, the word FOV in pre-readers will resemble the checkerboard FOV in both groups. With greater exposure to print, the word-FOV may show increased bias for the central and horizontal parts of the visual field.</p>
<p>The reduction of the word-FOV may reduce irrelevant signals to the reading circuitry and improve word-processing efficiency (<xref ref-type="bibr" rid="c36">Kwon et al., 2007</xref>). On the other hand, a FOV that is too small may constrain the use of direct access, holistic strategies for visual word recognition (<xref ref-type="bibr" rid="c16">Coltheart, Rastle, Perry, Langdon, &#x0026; Ziegler, 2001</xref>). This may lead to slower, piecemeal recognition of written words, particularly for longer words. Indeed, some dyslexics exhibit letter-by-letter reading (<xref ref-type="bibr" rid="c63">Wimmer &#x0026; Schurz, 2010</xref>), and it may be the case that such behavior is correlated with a reduced FOV in the VOTRC. Importantly, reading acquisition is clearly limited by other factors, most prominently phonological processing (e.g., <xref ref-type="bibr" rid="c70">Ziegler &#x0026; Goswami, 2005</xref>). Reading relies on multiple brain circuits that may be constrained or impaired in multiple ways, giving rise to the highly variable individual profiles of poor readers (<xref ref-type="bibr" rid="c8">Michal Ben-Shachar, Dougherty, &#x0026; Wandell, 2007</xref>; <xref ref-type="bibr" rid="c54">Stanovich, 1986</xref>; <xref ref-type="bibr" rid="c61">Brian A. Wandell &#x0026; Yeatman, 2013</xref>).</p>
</sec>
<sec id="s5f">
<title>Limitations</title>
<p>These measurements and analyses have both biological and experimental limitations. The BOLD measurements are limited by the presence of large sinuses in the neighborhood of the ventral occipito-temporal region. The transverse sinus interferes with the functional responses, which may have an impact on the FOV measurements (<xref ref-type="bibr" rid="c64">Winawer, Horiguchi, Sayres, Amano, &#x0026; Wandell, 2010</xref>). The variable position of this sinus across participants may account for some of the between-subject variation that we observe in the FOV of the reading circuitry.</p>
<p>A very large number of experimental parameter choices must be made in designing the localizer. In some cases, the choice does not alter the FOV (small-field vs. large-field localizer). In other cases, the FOV measurements may be less robust to experimental changes. Experimental parameters include stimulus categories, baseline stimuli, and task requirements. Additional experimental choices are made in the design of the pRF portion of the experiment. Here, relevant parameters include the size of the words, the size of the bar aperture, and the rate at which the words refresh within the aperture, in addition to the task requirements. Further experiments will be needed to assess the effect of these choices on the FOV estimates.</p>
<p>Data analysis introduces further choices. In defining the VOTRC, we select voxels exceeding an uncorrected statistical threshold of p&#x003C;0.001. In calculating the FOV in response to word stimuli, we include voxels for which the pRF model explains greater than 20&#x0025; of the variance in the data. These parameters select the voxels with the largest and best-modeled responses, but there are many voxels with smaller responses that may contribute to reading performance. The results we report are robust to a wide range of parameters at the level of the group average, but individual FOV estimates may be sensitive to these parameters in a way that is yet to be determined in future studies.</p>
</sec>
</sec>
</body>
<back>
<sec id="s5g">
<title>Conclusion</title>
<p>The FOV of the VOTRC is a small subset of the entire field of view available to the human visual system. On average, the FOV extends about 9&#x00B0; into the contralateral visual field and 4&#x00B0; into the ipsilateral visual field, and is largely confined to the horizontal meridian in each visual field. Unlike V1, FOV measurements in the VOTRC depend on stimulus features, and the FOV is more foveally biased for words than checkerboards.</p>
<p>In VOT broadly, the size and center of pRFs depend on factors such as attention, task requirements, and stimulus structure (e.g. the size of the words in the aperture) (<xref ref-type="bibr" rid="c27">Harvey et al., 2015</xref>; <xref ref-type="bibr" rid="c29">Kendrick N. Kay et al., 2015</xref>; <xref ref-type="bibr" rid="c31">K. N. Kay et al., 2013</xref>; <xref ref-type="bibr" rid="c51">Silson et al., 2015</xref>; <xref ref-type="bibr" rid="c53">Sprague &#x0026; Serences, 2013</xref>). The restricted FOV of the VOT circuitry may be determined by the properties of the neurons within the VOT or the major projections to these neurons from other parts of cortex. It is also possible that voxels contain multiple populations of neurons with different sensitivities (<xref ref-type="bibr" rid="c27">Harvey et al., 2015</xref>). Current pRF models do not capture such stimulus and task sensitivities, and thus a next generation of models may be required to explain the broader properties of the VOTRC responses (<xref ref-type="bibr" rid="c32">K. Kay &#x0026; Yeatman, 2016</xref>). We have archived the experimental data and metadata in a sharable database for anyone who would like to carry out further analyses (<xref ref-type="bibr" rid="c59">B. A. Wandell, Rokem, Perry, Schaefer, &#x0026; Dougherty, 2015</xref>).</p>
<p>The properties of the neurons and projections in VOT may develop in response to the extensive training that many children undergo in school. This training may impact several systems, including the VOT reading circuitry, eye-movement circuitry, and connections to
language cortex. There may be multiple system solutions for normal reading performance; a limitation in one part of the circuitry (e.g. a small FOV) may be compensated by another part of the reading circuitry (e.g. a more efficient eye-movement system). The timing of the training with respect to the available plasticity in these different systems may matter for these learning processes. We hope that understanding the developmental processes in the reading circuitry will contribute to effective educational practice (<xref ref-type="bibr" rid="c68">Yeatman, Dougherty, Ben-Shachar, &#x0026; Wandell, 2012</xref>).</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>This work was supported by NSF Grant to BW and the BSF Grant to MBS and BW. We thank Michael Barnett for his help with data collection as well as construction of the display in the magnet bore. We thank Justin Gardner, Karen Larocque, Kendrick Kay, Jason Yeatman, Jon Winawer, and Lee M. Perry for their helpful comments and feedback.</p>
</ack>
<app-group>
<app id="appA">
<title>Appendix</title>
<fig id="figS1" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary 1.</label>
<caption><title>VOTRC definition for all subjects.</title>
<p>The VOTRC is defined as those voxels within an anatomical region that are more responsive to words than to other categories. The black outlines show the left and right VOTRC in 20 subjects. In Sub 1-12 the regions were obtained with the small-field localizer; in Sub 13-20 the regions were measured with the large field localizer. The color overlays show the variance explained by the pRF model when using word stimuli. Only voxels with responses that exceed 20&#x0025; variance explained are colored.</p></caption>
<graphic xlink:href="069369_figS1.tif"/>
</fig>
<fig id="figS2" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary 2.</label>
<caption><title>Distribution of pRF parameters.</title>
<p><bold>A.</bold> Group average (n=20) FOV of left and right VOTRC. Dashed curves are the half-max contour. <bold>B.</bold> Surface area (mm<sup>2</sup>) of the left and right VOTRC for individual subjects. Subject numbering follows the convention of <xref ref-type="fig" rid="fig4">Figure 4</xref>. <bold>C,D,E, F.</bold> Distributions of pRF parameters. 200 voxels are sampled with replacement from each subject and plotted for left and right VOTRC. Only voxels that exceed 20&#x0025; variance explained are included.</p></caption>
<graphic xlink:href="069369_figS2.tif"/>
</fig>
<fig id="figS3" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary 3.</label>
<caption><title>(Related to <xref ref-type="fig" rid="fig4">Figure 4</xref>).</title>
<p>Between- and within-subject reliability in the right VOTRC. Dashed lines indicate the half-max contour to pRF models fit to run 1 and run 2. The color map and the centers correspond to the pRF model that is fit to the average of the two runs. For subject 07 no right hemisphere voxels passed the threshold criterion.</p></caption>
<graphic xlink:href="069369_figS3.tif"/>
</fig>
<fig id="figS4" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary 4.</label>
<caption><title>Group average FOVs in left VOTRC for various subset of voxels.</title>
<p>FOVs in A and C were measured in response to word stimuli. FOVs in B and D were measured in response to checker stimuli. The left VOTRC voxels used in A and B are those that exceed 20&#x0025; variance explained when measured with word stimuli. The left VOTRC voxels in C and D are those that exceed 20&#x0025; variance explained when measured with checker stimuli.</p></caption>
<graphic xlink:href="069369_figS4.tif"/>
</fig>
<fig id="figS5" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary 5.</label>
<caption><title>Group average right VOTRC FOVs for various subsets of voxels.</title>
<p>Details as in <xref ref-type="fig" rid="figS4">Supplementary 4</xref>.</p></caption>
<graphic xlink:href="069369_figS5.tif"/>
</fig>
</app>
</app-group>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Amano</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name>, &#x0026; <string-name><surname>Dumoulin</surname>, <given-names>S. O.</given-names></string-name> (<year>2009</year>). <article-title>Visual field maps, population receptive field sizes, and visual field coverage in the human MT&#x002B; complex</article-title>. <source>Journal of Neurophysiology</source>, <volume>102</volume>(<issue>5</issue>), <fpage>2704</fpage>&#x2013;<lpage>2718</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Arcaro</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>McMains</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Singer</surname>, <given-names>B. D.</given-names></string-name>, &#x0026; <string-name><surname>Kastner</surname>, <given-names>S.</given-names></string-name> (<year>2009</year>). <article-title>Retinotopic Organization of Human Ventral Visual Cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>29</volume>(<issue>34</issue>), <fpage>10638</fpage>&#x2013;<lpage>10652</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1523/JNEUR0SCI.2807-09.2009">http://doi.org/10.1523/JNEUR0SCI.2807-09.2009</ext-link></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wald</surname>, <given-names>L. L.</given-names></string-name>, <string-name><surname>Kwong</surname>, <given-names>K. K.</given-names></string-name>, <string-name><surname>Benner</surname>, <given-names>T.</given-names></string-name>, &#x0026; <string-name><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name> (<year>2007</year>). <article-title>Visual word processing and experiential origins of functional selectivity in human extrastriate cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>104</volume>(<issue>21</issue>), <fpage>9087</fpage>&#x2013;<lpage>9092</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1073/pnas.0703300104">http://doi.org/10.1073/pnas.0703300104</ext-link></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Baynes</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Tramo</surname>, <given-names>M. J.</given-names></string-name>, &#x0026; <string-name><surname>Gazzaniga</surname>, <given-names>M. S.</given-names></string-name> (<year>1992</year>). <article-title>Reading with a limited lexicon in the right hemisphere of a callosotomy patient</article-title>. <source>Neuropsychologia</source>, <volume>30</volume>(<issue>2</issue>), <fpage>187</fpage>&#x2013;<lpage>200</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Ben-Shachar</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dougherty</surname>, <given-names>R. F.</given-names></string-name>, <string-name><surname>Deutsch</surname>, <given-names>G. K.</given-names></string-name>, &#x0026; <string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name> (<year>2007</year>). <article-title>Differential Sensitivity to Words and Shapes in Ventral Occipito-Temporal Cortex</article-title>. <source>Cerebral Cortex</source>, <volume>17</volume>(<issue>7</issue>), <fpage>1604</fpage>&#x2013;<lpage>1611</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1093/cercor/bhl071">http://doi.org/10.1093/cercor/bhl071</ext-link></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Ben-Shachar</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dougherty</surname>, <given-names>R. F.</given-names></string-name>, <string-name><surname>Deutsch</surname>, <given-names>G. K.</given-names></string-name>, &#x0026; <string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name> (<year>2011a</year>). <article-title>The development of cortical sensitivity to visual word forms</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>23</volume>(<issue>9</issue>), <fpage>2387</fpage>&#x2013;<lpage>2399</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Ben-Shachar</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dougherty</surname>, <given-names>R. F.</given-names></string-name>, <string-name><surname>Deutsch</surname>, <given-names>G. K.</given-names></string-name>, &#x0026; <string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name> (<year>2011b</year>). <article-title>The Development of Cortical Sensitivity to Visual Word Forms</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>23</volume>(<issue>9</issue>), <fpage>2387</fpage>&#x2013;<lpage>2399</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1162/jocn.2011.21615">http://doi.org/10.1162/jocn.2011.21615</ext-link></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Ben-Shachar</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dougherty</surname>, <given-names>R. F.</given-names></string-name>, &#x0026; <string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name> (<year>2007</year>). <article-title>White matter pathways in reading</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>17</volume>(<issue>2</issue>), <fpage>258</fpage>&#x2013;<lpage>270</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Bolger</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Perfetti</surname>, <given-names>C. A.</given-names></string-name>, &#x0026; <string-name><surname>Schneider</surname>, <given-names>W.</given-names></string-name> (<year>2005</year>). <article-title>Cross-cultural effect on the brain revisited: Universal structures plus writing system variation</article-title>. <source>Human Brain Mapping</source>, <volume>25</volume>(<issue>1</issue>), <fpage>92</fpage>&#x2013;<lpage>104</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1002/hbm.20124">http://doi.org/10.1002/hbm.20124</ext-link></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Bondarko</surname>, <given-names>V. M.</given-names></string-name>, &#x0026; <string-name><surname>Semenov</surname>, <given-names>L. A.</given-names></string-name> (<year>2004</year>). <article-title>Size estimates in Ebbinghaus illusion in adults and children of different age</article-title>. <source>Human Physiology</source>, <volume>30</volume>(<issue>1</issue>), <fpage>24</fpage>&#x2013;<lpage>30</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Bouhali</surname>, <given-names>F.</given-names></string-name>, Thiebaut de <string-name><surname>Schotten</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pinel</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Poupon</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Mangin</surname>, <given-names>J.-F.</given-names></string-name>, <string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Cohen</surname>, <given-names>L.</given-names></string-name> (<year>2014</year>). <article-title>Anatomical Connections of the Visual Word Form Area</article-title>. <source>Journal of Neuroscience</source>, <volume>34</volume>(<issue>46</issue>), <fpage>15402</fpage>&#x2013;<lpage>15414</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1523/JNEUROSCI.4918-13.2014">http://doi.org/10.1523/JNEUROSCI.4918-13.2014</ext-link></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Brainard</surname>, <given-names>D. H.</given-names></string-name> (<year>1997</year>). <article-title>The psychophysics toolbox</article-title>. <source>Spatial Vision</source>, <volume>10</volume>, <fpage>433</fpage>&#x2013;<lpage>436</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Brem</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Bucher</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Halder</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Summers</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Dietrich</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Martin</surname>, <given-names>E.</given-names></string-name>, &#x0026; <string-name><surname>Brandeis</surname>, <given-names>D.</given-names></string-name> (<year>2006</year>). <article-title>Evidence for developmental changes in the visual word processing network beyond adolescence</article-title>. <source>Neuroimage</source>, <volume>29</volume>(<issue>3</issue>), <fpage>822</fpage>&#x2013;<lpage>837</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Chung</surname>, <given-names>S. T.</given-names></string-name> L., <string-name><surname>Mansfield</surname>, <given-names>J. S.</given-names></string-name>, &#x0026; <string-name><surname>Legge</surname>, <given-names>G. E.</given-names></string-name> (<year>1998</year>). <article-title>Psychophysics of reading. XVIII. The effect of print size on reading speed in normal peripheral vision</article-title>. <source>Vision Research</source>, <volume>38</volume>(<issue>19</issue>), <fpage>2949</fpage>&#x2013;<lpage>2962</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1016/S0042-6989(98)00072-8">http://doi.org/10.1016/S0042-6989(98)00072-8</ext-link></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Cohen</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Leh&#x00E9;ricy</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Chochon</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Lemer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Rivaud</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name> (<year>2002</year>). <article-title>Language-specific tuning of visual cortex? Functional properties of the Visual Word Form Area</article-title>. <source>Brain</source>, <volume>125</volume>(<issue>5</issue>), <fpage>1054</fpage>&#x2013;<lpage>1069</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Coltheart</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rastle</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Perry</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Langdon</surname>, <given-names>R.</given-names></string-name>, &#x0026; <string-name><surname>Ziegler</surname>, <given-names>J.</given-names></string-name> (<year>2001</year>). <article-title>DRC: a dual route cascaded model of visual word recognition and reading aloud</article-title>. <source>Psychological Review</source>, <volume>108</volume>(<issue>1</issue>), <fpage>204</fpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Damasio</surname>, <given-names>A. R.</given-names></string-name>, &#x0026; <string-name><surname>Damasio</surname>, <given-names>H.</given-names></string-name> (<year>1983</year>). <article-title>The anatomic basis of pure alexia</article-title>. <source>Neurology</source>, <volume>33</volume>(<issue>12</issue>), <fpage>1573</fpage>&#x2013;<lpage>1573</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Cohen</surname>, <given-names>L.</given-names></string-name> (<year>2011</year>). <article-title>The unique role of the visual word form area in reading</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>15</volume>(<issue>6</issue>), <fpage>254</fpage>&#x2013;<lpage>262</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Le Clec&#x2019;H</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Poline</surname>, <given-names>J.-B.</given-names></string-name>, <string-name><surname>Le Bihan</surname>, <given-names>D.</given-names></string-name>, &#x0026; <string-name><surname>Cohen</surname>, <given-names>L.</given-names></string-name> (<year>2002</year>). <article-title>The visual word form area: a prelexical representation of visual words in the fusiform gyrus</article-title>. <source>Neuroreport</source>, <volume>13</volume>(<issue>3</issue>), <fpage>321</fpage>&#x2013;<lpage>325</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pegado</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Braga</surname>, <given-names>L. W.</given-names></string-name>, <string-name><surname>Ventura</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Nunes Filho</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Jobert</surname>, <given-names>A.</given-names></string-name>, &#x2026; <string-name><surname>Cohen</surname>, <given-names>L.</given-names></string-name> (<year>2010</year>). <article-title>How learning to read changes the cortical networks for vision and language</article-title>. <source>Science</source>, <volume>330</volume>(<issue>6009</issue>), <fpage>1359</fpage>&#x2013;<lpage>1364</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="website"><string-name><surname>Dejerine</surname>, <given-names>J. J.</given-names></string-name> (<year>1892</year>). <article-title>Contribution &#x00E0; l&#x2019;&#x00E9;tude anatomo-pathologique et clinique des diff&#x00E9;rentes vari&#x00E9;t&#x00E9;s de c&#x00E9;cit&#x00E9; verbale</article-title>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://publikationen.ub.uni-frankfurt.de/frontdoor/index/index/year/2013/docId/23183">http://publikationen.ub.uni-frankfurt.de/frontdoor/index/index/year/2013/docId/23183</ext-link></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Dumoulin</surname>, <given-names>S. O.</given-names></string-name>, &#x0026; <string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name> (<year>2008</year>). <article-title>Population receptive field estimates in human visual cortex</article-title>. <source>Neuroimage</source>, <volume>39</volume>(<issue>2</issue>), <fpage>647</fpage>&#x2013;<lpage>660</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Epstein</surname>, <given-names>R.</given-names></string-name>, &#x0026; <string-name><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name> (<year>1998</year>). <article-title>A cortical representation of the local visual environment</article-title>. <source>Nature</source>, <volume>392</volume>(<issue>6676</issue>), <fpage>598</fpage>&#x2013;<lpage>601</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Glezer</surname>, <given-names>L. S.</given-names></string-name>, <string-name><surname>Jiang</surname>, <given-names>X.</given-names></string-name>, &#x0026; <string-name><surname>Riesenhuber</surname>, <given-names>M.</given-names></string-name> (<year>2009</year>). <article-title>Evidence for Highly Selective Neuronal Tuning to Whole Words in the &#x201C;Visual Word Form Area.&#x201D;</article-title> <source>Neuron</source>, <volume>62</volume>(<issue>2</issue>), <fpage>199</fpage>&#x2013;<lpage>204</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1016/j.neuron.2009.03.017">http://doi.org/10.1016/j.neuron.2009.03.017</ext-link></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Glezer</surname>, <given-names>L. S.</given-names></string-name>, &#x0026; <string-name><surname>Riesenhuber</surname>, <given-names>M.</given-names></string-name> (<year>2013</year>). <article-title>Individual Variability in Location Impacts Orthographic Selectivity in the &#x201C;Visual Word Form Area.&#x201D;</article-title> <source>The Journal of Neuroscience</source>, <volume>33</volume>(<issue>27</issue>), <fpage>11221</fpage>&#x2013;<lpage>11226</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1523/JNEUR0SCI.5002-12.201">http://doi.org/10.1523/JNEUR0SCI.5002-12.2013</ext-link></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name>, &#x0026; <string-name><surname>Weiner</surname>, <given-names>K. S.</given-names></string-name> (<year>2014</year>). <article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>15</volume>(<issue>8</issue>), <fpage>536</fpage>&#x2013;<lpage>548</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.108/nrn747">http://doi.org/10.108/nrn747</ext-link></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Harvey</surname>, <given-names>B. M.</given-names></string-name>, <string-name><surname>Fracasso</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Petridou</surname>, <given-names>N.</given-names></string-name>, &#x0026; <string-name><surname>Dumoulin</surname>, <given-names>S. O.</given-names></string-name> (<year>2015</year>). <article-title>Topographic representations of object size and relationships with numerosity reveal generalized quantity processing in human parietal cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <fpage>201515414</fpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1073/pnas.1515414112">http://doi.org/10.1073/pnas.1515414112</ext-link></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Hasson</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Levy</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Behrmann</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hendler</surname>, <given-names>T.</given-names></string-name>, &#x0026; <string-name><surname>Malach</surname>, <given-names>R.</given-names></string-name> (<year>2002</year>). <article-title>Eccentricity bias as an organizing principle for human high-order object areas</article-title>. <source>Neuron</source>, <volume>34</volume>(<issue>3</issue>), <fpage>479</fpage>&#x2013;<lpage>490</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, <string-name><surname>Weiner</surname>, <given-names>K. S.</given-names></string-name>, &#x0026; <string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name> (<year>2015</year>). <article-title>Attention Reduces Spatial Uncertainty in Human Ventral Temporal Cortex</article-title>. <source>Current Biology</source>, <volume>25</volume>(<issue>5</issue>), <fpage>595</fpage>&#x2013;<lpage>600</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1016/jxub.2014.12.050">http://doi.org/10.1016/jxub.2014.12.050</ext-link></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, <string-name><surname>Weiner</surname>, <given-names>K. S.</given-names></string-name>, &#x0026; <string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name> (<year>2015</year>). <article-title>Attention reduces spatial uncertainty in human ventral temporal cortex</article-title>. <source>Current Biology</source>, <volume>25</volume>(<issue>5</issue>), <fpage>595</fpage>&#x2013;<lpage>600</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, <string-name><surname>Winawer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Mezer</surname>, <given-names>A.</given-names></string-name>, &#x0026; <string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name> (<year>2013</year>). <article-title>Compressive spatial summation in human visual cortex</article-title>. <source>Journal of Neurophysiology</source>, <volume>110</volume>(<issue>2</issue>), <fpage>481</fpage>&#x2013;<lpage>494</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1152/jn.00105.201">http://doi.org/10.1152/jn.00105.201</ext-link></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Kay</surname>, <given-names>K.</given-names></string-name>, &#x0026; <string-name><surname>Yeatman</surname>, <given-names>J.</given-names></string-name> (<year>2016</year>). <source>Bottom-up and top-down computations in high-level visual cortex</source> (No. biorxiv;<pub-id pub-id-type="biorxiv">053 595v1</pub-id>). Retrieved from <ext-link ext-link-type="uri" xlink:href="http://biorxiv.org/lookup/doi/10.1101/053595">http://biorxiv.org/lookup/doi/10.1101/053595</ext-link></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Krafnick</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Tan</surname>, <given-names>L.-H.</given-names></string-name>, <string-name><surname>Flowers</surname>, <given-names>D. L.</given-names></string-name>, <string-name><surname>Luetje</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Napoliello</surname>, <given-names>E. M.</given-names></string-name>, <string-name><surname>Siok</surname>, <given-names>W.-T.</given-names></string-name>, &#x2026; <string-name><surname>Eden</surname>, <given-names>G. F.</given-names></string-name> (<year>2016</year>). <article-title>Chinese Character and English Word processing in children&#x2019;s ventral occipitotemporal cortex: fMRI evidence for script invariance</article-title>. <source>NeuroImage</source>, <volume>133</volume>, <fpage>302</fpage>&#x2013;<lpage>312</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1016/j.neuroimage.2016.03.021">http://doi.org/10.1016/j.neuroimage.2016.03.021</ext-link></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Kravitz</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Vinson</surname>, <given-names>L. D.</given-names></string-name>, &#x0026; <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name> (<year>2008a</year>). <article-title>How position dependent is visual object recognition?</article-title> <source>Trends in Cognitive Sciences</source>, <volume>12</volume>(<issue>3</issue>), <fpage>114</fpage>&#x2013;<lpage>122</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1016Zj.tics.2007.12.006">http://doi.org/10.1016Zj.tics.2007.12.006</ext-link></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Kravitz</surname>, <given-names>D. J.</given-names></string-name>, <string-name><surname>Vinson</surname>, <given-names>L. D.</given-names></string-name>, &#x0026; <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name> (<year>2008b</year>). <article-title>How position dependent is visual object recognition?</article-title> <source>Trends in Cognitive Sciences</source>, <volume>12</volume>(<issue>3</issue>), <fpage>114</fpage>&#x2013;<lpage>122</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1016/j.tics.2007.12.006">http://doi.org/10.1016/j.tics.2007.12.006</ext-link></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Kwon</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Legge</surname>, <given-names>G. E.</given-names></string-name>, &#x0026; <string-name><surname>Dubbels</surname>, <given-names>B. R.</given-names></string-name> (<year>2007</year>). <article-title>Developmental changes in the visual span for reading</article-title>. <source>Vision Research</source>, <volume>47</volume>(<issue>22</issue>), <fpage>2889</fpage>&#x2013;<lpage>2900</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Larsson</surname>, <given-names>J.</given-names></string-name>, &#x0026; <string-name><surname>Heeger</surname>, <given-names>D. J.</given-names></string-name> (<year>2006</year>). <article-title>Two retinotopic visual areas in human lateral occipital cortex</article-title>. <source>The Journal of Neuroscience</source>, <volume>26</volume>(<issue>51</issue>), <fpage>13128</fpage>&#x2013;<lpage>13142</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Legge</surname>, <given-names>G. E.</given-names></string-name>, <string-name><surname>Ahn</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Klitz</surname>, <given-names>T. S.</given-names></string-name>, &#x0026; <string-name><surname>Luebker</surname>, <given-names>A.</given-names></string-name> (<year>1997</year>). <article-title>Psychophysics of reading&#x2014;XVI. The visual span in normal and low vision</article-title>. <source>Vision Research</source>, <volume>37</volume>(<issue>14</issue>), <fpage>1999</fpage>&#x2013;<lpage>2010</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Legge</surname>, <given-names>G. E.</given-names></string-name>, <string-name><surname>Cheung</surname>, <given-names>S. H.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Chung</surname>, <given-names>S. T.</given-names></string-name> L., <string-name><surname>Lee</surname>, <given-names>H. W.</given-names></string-name>, &#x0026; <string-name><surname>Owens</surname>, <given-names>D. P.</given-names></string-name> (<year>2007</year>). <article-title>The case for the visual span as a sensory bottleneck in reading</article-title>. <source>Journal of Vision</source>, <volume>7</volume>(<issue>2</issue>), <fpage>9</fpage>&#x2013;<lpage>9</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1167/7.2.9">http://doi.org/10.1167/7.2.9</ext-link></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Martin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schurz</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kronbichler</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Richlan</surname>, <given-names>F.</given-names></string-name> (<year>2015</year>). <article-title>Reading in the brain of children and adults: a meta-analysis of 40 functional magnetic resonance imaging studies</article-title>. <source>Human Brain Mapping</source>, <volume>36</volume>(<issue>5</issue>), <fpage>1963</fpage>&#x2013;<lpage>1981</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1002/hbm.22749">http://doi.org/10.1002/hbm.22749</ext-link></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>McConkie</surname>, <given-names>G. W.</given-names></string-name>, &#x0026; <string-name><surname>Rayner</surname>, <given-names>K.</given-names></string-name> (<year>1975</year>). <article-title>The span of the effective stimulus during a fixation in reading</article-title>. <source>Perception &#x0026; Psychophysics</source>, <volume>17</volume>(<issue>6</issue>), <fpage>578</fpage>&#x2013;<lpage>586</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="website"><string-name><surname>O&#x2019;Regan</surname>, <given-names>J. K.</given-names></string-name> (<year>1991</year>). <article-title>Understanding visual search and reading using the concepts of stimulus&#x2019; grain</article-title>&#x201D;. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://repository.tue.nl/689488">http://repository.tue.nl/689488</ext-link></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Pelli</surname>, <given-names>D. G.</given-names></string-name>, <string-name><surname>Tillman</surname>, <given-names>K. A.</given-names></string-name>, <string-name><surname>Freeman</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Su</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Berger</surname>, <given-names>T. D.</given-names></string-name>, &#x0026; <string-name><surname>Majaj</surname>, <given-names>N. J.</given-names></string-name> (<year>2007</year>). <article-title>Crowding and eccentricity determine reading rate</article-title>. <source>Journal of Vision</source>, <volume>7</volume>(<issue>2</issue>), <fpage>20</fpage>&#x2013;<lpage>20</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Petre</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Hazel</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Fine</surname>, <given-names>E. M.</given-names></string-name>, &#x0026; <string-name><surname>Rubin</surname>, <given-names>G. S.</given-names></string-name> (<year>2000</year>). <article-title>Reading with eccentric fixation is faster in inferior visual field than in left visual field</article-title>. <source>Optometry &#x0026; Vision Science</source>, <volume>77</volume>(<issue>1</issue>), <fpage>34</fpage>&#x2013;<lpage>39</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Price</surname>, <given-names>C. J.</given-names></string-name>, &#x0026; <string-name><surname>Devlin</surname>, <given-names>J. T.</given-names></string-name> (<year>2011</year>). <article-title>The Interactive Account of ventral occipitotemporal contributions to reading</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>15</volume>(<issue>6</issue>), <fpage>246</fpage>&#x2013;<lpage>253</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1016Zj.tics.2011.04.0012">http://doi.org/10.1016Zj.tics.2011.04.001</ext-link></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Rauschecker</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Bowen</surname>, <given-names>R. F.</given-names></string-name>, <string-name><surname>Parvizi</surname>, <given-names>J.</given-names></string-name>, &#x0026; <string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name> (<year>2012</year>). <article-title>Position sensitivity in the visual word form area</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>109</volume>(<issue>24</issue>), <fpage>E1568</fpage>&#x2013;<lpage>E1577</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1073/pnas.1121304109">http://doi.org/10.1073/pnas.1121304109</ext-link></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Rauschecker</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Bowen</surname>, <given-names>R. F.</given-names></string-name>, <string-name><surname>Perry</surname>, <given-names>L. M.</given-names></string-name>, <string-name><surname>Kevan</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Dougherty</surname>, <given-names>R. F.</given-names></string-name>, &#x0026; <string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name> (<year>2011</year>). <article-title>Visual Feature-Tolerance in the Reading Network</article-title>. <source>Neuron</source>, <volume>71</volume>(<issue>5</issue>), <fpage>941</fpage>&#x2013;<lpage>953</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1016/j.neuron.2011.06.036">http://doi.org/10.1016/j.neuron.2011.06.036</ext-link></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Riesenhuber</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Poggio</surname>, <given-names>T.</given-names></string-name> (<year>1999</year>). <article-title>Hierarchical models of object recognition in cortex</article-title>. <source>Nature Neuroscience</source>, <volume>2</volume>(<issue>11</issue>), <fpage>1019</fpage>&#x2013;<lpage>1025</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Rokem</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Yeatman</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Pestilli</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, <string-name><surname>Mezer</surname>, <given-names>A.</given-names></string-name>, van der <string-name><surname>Walt</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name> (<year>2015</year>). <article-title>Evaluating the accuracy of diffusion MRI models in white matter</article-title>. <source>PloS One</source>, <volume>10</volume>(<issue>4</issue>), <fpage>e0123272</fpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Rolls</surname>, <given-names>E. T.</given-names></string-name> (<year>2000</year>). <article-title>Functions of the primate temporal lobe cortical visual areas in invariant visual object and face recognition</article-title>. <source>Neuron</source>, <volume>27</volume>(<issue>2</issue>), <fpage>205</fpage>&#x2013;<lpage>218</lpage>.</mixed-citation></ref>
<ref id="c71"><mixed-citation publication-type="journal"><string-name><surname>Shaywitz</surname>, <given-names>B. A.</given-names></string-name>, <string-name><surname>Skudlarski</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Holahan</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Marchione</surname>, <given-names>K. E.</given-names></string-name>, <string-name><surname>Constable</surname>, <given-names>R. T.</given-names></string-name>, <string-name><surname>Fulbright</surname>, <given-names>R. K.</given-names></string-name>, &#x2026; <string-name><surname>Shaywitz</surname>, <given-names>S. E.</given-names></string-name> (<year>2007</year>). <article-title>Age-related changes in reading systems of dyslexic children</article-title>. <source>Annals of Neurology</source>, <volume>61</volume>(<issue>4</issue>), <fpage>363</fpage>&#x2013;<lpage>370</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1002/ana.21093">http://doi.org/10.1002/ana.21093</ext-link></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Silson</surname>, <given-names>E. H.</given-names></string-name>, <string-name><surname>Chan</surname>, <given-names>A. W.-Y.</given-names></string-name>, <string-name><surname>Reynolds</surname>, <given-names>R. C.</given-names></string-name>, <string-name><surname>Kravitz</surname>, <given-names>D. J.</given-names></string-name>, &#x0026; <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name> (<year>2015</year>). <article-title>A Retinotopic Basis for the Division of High-Level Scene Processing between Lateral and Ventral Human Occipitotemporal Cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>35</volume>(<issue>34</issue>), <fpage>11921</fpage>&#x2013;<lpage>11935</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.152/JNEUROSCI.017-15.2015">http://doi.org/10.152/JNEUROSCI.017-15.2015</ext-link></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Simos</surname>, <given-names>P. G.</given-names></string-name>, <string-name><surname>Breier</surname>, <given-names>J. I.</given-names></string-name>, <string-name><surname>Fletcher</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Bergman</surname>, <given-names>E.</given-names></string-name>, &#x0026; <string-name><surname>Papanicolaou</surname>, <given-names>A. C.</given-names></string-name> (<year>2000</year>). <article-title>Cerebral Mechanisms Involved in Word Reading in Dyslexic Children: a Magnetic Source Imaging Approach</article-title>. <source>Cerebral Cortex</source>, <volume>10</volume>(<issue>8</issue>), <fpage>809</fpage>&#x2013;<lpage>816</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.109/cercor/10.8.809">http://doi.org/10.109/cercor/10.8.809</ext-link></mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Sprague</surname>, <given-names>T. C.</given-names></string-name>, &#x0026; <string-name><surname>Serences</surname>, <given-names>J. T.</given-names></string-name> (<year>2013</year>). <article-title>Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices</article-title>. <source>Nature Neuroscience</source>, <volume>16</volume>(<issue>12</issue>), <fpage>1879</fpage>&#x2013;<lpage>1887</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1038/nn.3574">http://doi.org/10.1038/nn.3574</ext-link></mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Stanovich</surname>, <given-names>K. E.</given-names></string-name> (<year>1986</year>). <article-title>Matthew effects in reading: Some consequences of individual differences in the acquisition of literacy</article-title>. <source>Reading Research Quarterly</source>, <fpage>360</fpage>&#x2013;<lpage>407</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Stigliani</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Weiner</surname>, <given-names>K. S.</given-names></string-name>, &#x0026; <string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name> (<year>2015</year>). <article-title>Temporal Processing Capacity in High-Level Visual Cortex Is Domain Specific</article-title>. <source>Journal of Neuroscience</source>, <volume>35</volume>(<issue>36</issue>), <fpage>12412</fpage>&#x2013;<lpage>12424</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1523/JNEUR0SCI.4822-14.2015">http://doi.org/10.1523/JNEUR0SCI.4822-14.2015</ext-link></mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Uyar</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Shomstein</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Greenberg</surname>, <given-names>A. S.</given-names></string-name>, &#x0026; <string-name><surname>Behrmann</surname>, <given-names>M.</given-names></string-name> (<year>2016</year>). <article-title>Retinotopic information interacts with category selectivity in human ventral cortex</article-title>. <source>Neuropsychologia</source>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1016/j.neuropsychologia.2016.05.022">http://doi.org/10.1016/j.neuropsychologia.2016.05.022</ext-link></mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Vigneau</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Jobard</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Mazoyer</surname>, <given-names>B.</given-names></string-name>, &#x0026; <string-name><surname>Tzourio-Mazoyer</surname>, <given-names>N.</given-names></string-name> (<year>2005</year>). <article-title>Word and non-word reading: what role for the visual word form area?</article-title> <source>Neuroimage</source>, <volume>27</volume>(<issue>3</issue>), <fpage>694</fpage>&#x2013;<lpage>705</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name>, <string-name><surname>Rauschecker</surname>, <given-names>A. M.</given-names></string-name>, &#x0026; <string-name><surname>Yeatman</surname>, <given-names>J. D.</given-names></string-name> (<year>2012</year>). <article-title>Learning to see words</article-title>. <source>Annual Review of Psychology</source>, <volume>63</volume>, <fpage>31</fpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="website"><string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name>, <string-name><surname>Rokem</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Perry</surname>, <given-names>L. M.</given-names></string-name>, <string-name><surname>Schaefer</surname>, <given-names>G.</given-names></string-name>, &#x0026; <string-name><surname>Dougherty</surname>, <given-names>R. F.</given-names></string-name> (<year>2015</year>). <article-title>Data management to support reproducible research</article-title>. arXiv Preprint arXiv:<pub-id pub-id-type="arxiv">1502.06900</pub-id>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1502.06900">http://arxiv.org/abs/1502.06900</ext-link></mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name>, &#x0026; <string-name><surname>Winawer</surname>, <given-names>J.</given-names></string-name> (<year>2011</year>). <article-title>Imaging retinotopic maps in the human brain</article-title>. <source>Vision Research</source>, <volume>51</volume>(<issue>7</issue>), <fpage>718</fpage>&#x2013;<lpage>737</lpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name>, &#x0026; <string-name><surname>Yeatman</surname>, <given-names>J. D.</given-names></string-name> (<year>2013</year>). <article-title>Biological development of reading circuits</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>23</volume>(<issue>2</issue>), <fpage>261</fpage>&#x2013;<lpage>268</lpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><string-name><surname>Weiner</surname>, <given-names>K. S.</given-names></string-name>, &#x0026; <string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name> (<year>2012</year>). <article-title>The improbable simplicity of the fusiform face area</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>16</volume>(<issue>5</issue>), <fpage>251</fpage>&#x2013;<lpage>254</lpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><string-name><surname>Wimmer</surname>, <given-names>H.</given-names></string-name>, &#x0026; <string-name><surname>Schurz</surname>, <given-names>M.</given-names></string-name> (<year>2010</year>). <article-title>Dyslexia in regular orthographies: manifestation and causation</article-title>. <source>Dyslexia</source>, <volume>16</volume>(<issue>4</issue>), <fpage>283</fpage>&#x2013;<lpage>299</lpage>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><string-name><surname>Winawer</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Horiguchi</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Sayres</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Amano</surname>, <given-names>K.</given-names></string-name>, &#x0026; <string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name> (<year>2010</year>). <article-title>Mapping hV4 and ventral occipital cortex: the venous eclipse</article-title>. <source>Journal of Vision</source>, <volume>10</volume>(<issue>5</issue>), <fpage>1</fpage>&#x2013;<lpage>1</lpage>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><string-name><surname>Winawer</surname>, <given-names>J.</given-names></string-name>, &#x0026; <string-name><surname>Witthoft</surname>, <given-names>N.</given-names></string-name> (<year>2015</year>). <article-title>Human V4 and ventral occipital retinotopic maps</article-title>. <source>Visual Neuroscience</source>, <fpage>32</fpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1017/S0952523815000176">http://doi.org/10.1017/S0952523815000176</ext-link></mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><string-name><surname>Witthoft</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>M. L.</given-names></string-name>, <string-name><surname>Golarai</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>LaRocque</surname>, <given-names>K. F.</given-names></string-name>, <string-name><surname>Liberman</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>M. E.</given-names></string-name>, &#x0026; <string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name> (<year>2014</year>). <article-title>Where Is Human V4? Predicting the Location of hV4 and VO1 from Cortical Folding</article-title>. <source>Cerebral Cortex</source>, <volume>24</volume>(<issue>9</issue>), <fpage>2401</fpage>&#x2013;<lpage>2408</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1093/cercor/bht092">http://doi.org/10.1093/cercor/bht092</ext-link></mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="website"><string-name><surname>Witthoft</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Poltoratski</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Golarai</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Liberman</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>LaRocque</surname>, <given-names>K. F.</given-names></string-name>, &#x2026; <string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name> (<year>2016</year>). <article-title>Reduced spatial integration in the ventral visual cortex underlies face recognition deficits in developmental prosopagnosia (No. biorxiv;051102v1)</article-title>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://biorxiv.org/lookup/doi/10.1101/051102">http://biorxiv.org/lookup/doi/10.1101/051102</ext-link></mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><string-name><surname>Yeatman</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Dougherty</surname>, <given-names>R. F.</given-names></string-name>, <string-name><surname>Ben-Shachar</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Wandell</surname>, <given-names>B. A.</given-names></string-name> (<year>2012</year>). <article-title>Development of white matter and reading skills</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>109</volume>(<issue>44</issue>), <fpage>E3045</fpage>&#x2013;<lpage>E3053</lpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1073/pnas.1206792109">http://doi.org/10.1073/pnas.1206792109</ext-link></mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><string-name><surname>Yu</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Jiang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Legge</surname>, <given-names>G. E.</given-names></string-name>, &#x0026; <string-name><surname>He</surname>, <given-names>S.</given-names></string-name> (<year>2015</year>). <article-title>Locating the cortical bottleneck for slow reading in peripheral vision</article-title>. <source>Journal of Vision</source>, <volume>15</volume>(<issue>11</issue>), <fpage>3</fpage>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.1167/15.11.3">http://doi.org/10.1167/15.11.3</ext-link></mixed-citation></ref>
<ref id="c70"><mixed-citation publication-type="journal"><string-name><surname>Ziegler</surname>, <given-names>J. C.</given-names></string-name>, &#x0026; <string-name><surname>Goswami</surname>, <given-names>U.</given-names></string-name> (<year>2005</year>). <article-title>Reading acquisition, developmental dyslexia, and skilled reading across languages: a psycholinguistic grain size theory</article-title>. <source>Psychological Bulletin</source>, <volume>131</volume>(<issue>1</issue>), <fpage>3</fpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>