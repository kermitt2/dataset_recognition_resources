<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/059683</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Temporal Modulations Reveal Distinct Rhythmic Properties of Speech and Music</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Ding</surname><given-names>Nai</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Patel</surname><given-names>Aniruddh D.</given-names></name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Chen</surname><given-names>Lin</given-names></name>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Butler</surname><given-names>Henry</given-names></name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Luo</surname><given-names>Cheng</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Poeppel</surname><given-names>David</given-names></name>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<aff id="a1"><label>1</label><institution>College of Biomedical Engineering and Instrument Sciences, Zhejiang University</institution>, <country>China</country> 310027</aff>
<aff id="a2"><label>2</label><institution>Interdisciplinary Center for Social Sciences, Zhejiang University, China 3100272Interdisciplinary Center for Social Sciences, Zhejiang University</institution>, <country>China</country> 310027</aff>
<aff id="a3"><label>3</label><institution>Neuro and Behavior EconLab, Zhejiang University of Finance and Economics</institution>, <country>China</country> 310027</aff>
<aff id="a4"><label>4</label><institution>Department of Psychology, Tufts University</institution>, Boston, MA 02155</aff>
<aff id="a5"><label>5</label><institution>Department of Psychology, New York University</institution>, <country>New York</country>, NY 10003</aff>
<aff id="a6"><label>6</label><institution>Max Planck Institute for Empirical Aesthetics</institution>, Frankfurt, <country>Germany</country></aff>
</contrib-group>
<author-notes>
<fn><p><bold>Classification:</bold> Biological Sciences/Neuroscience</p></fn>
<corresp><bold>Corresponding author:</bold> Nai Ding, Ph.D. College of Biomedical Engineering and Instrument Sciences, <institution>Zhejiang University</institution> Hangzhou, China 310027 Email: <email>ding_nai@zju.edu.cn</email></corresp>
</author-notes>
<pub-date pub-type="epub"><year>2016</year></pub-date>
<elocation-id>059683</elocation-id>
<history>
<date date-type="received">
<day>17</day>
<month>6</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>18</day>
<month>6</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2016, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2016</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="059683.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Speech and music have structured rhythms, but these rhythms are rarely compared empirically. This study, based on large corpora, quantitatively characterizes and compares a major acoustic correlate of spoken and musical rhythms, the slow (0.25-32 Hz) temporal modulations in sound intensity. We show that the speech modulation spectrum is highly consistent cross 9 languages (including languages with typologically different rhythmic characteristics, such as English, French, and Mandarin Chinese). A different, but similarly consistent modulation spectrum is observed for Western classical music played by 6 different instruments. Western music, including classical music played by single instruments, symphonic, jazz, and rock music, contains more energy than speech in the low modulation frequency range below 4 Hz. The temporal modulations of speech and music show broad but well-separated peaks around 5 and 2 Hz, respectively. These differences in temporal modulations alone, without any spectral details, can discriminate speech and music with high accuracy. Speech and music therefore show distinct and reliable statistical regularities in their temporal modulations that likely facilitate their perceptual analysis and its neural foundations.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>speech</kwd>
<kwd>music</kwd>
<kwd>rhythm</kwd>
<kwd>temporal modulations</kwd>
</kwd-group>
<counts>
<page-count count="32"/>
</counts>
</article-meta>
</front>
<body>
<sec>
<title>Significance Statement</title>
<p>Speech and music are both rhythmic. This study quantifies and compares the acoustic rhythms of speech and music. A large corpus analysis is applied to speech across languages and to music across genres, including both classical music played by single instruments and ensemble music such as symphonic music, rock music and jazz. The analysis reveals consistent rhythmic properties within the category of speech and within the category of music, but clear distinctions between the two, highlighting potentially universal differences between these fundamental domains of auditory experience.</p>
</sec>
<sec id="s1">
<title>Introduction</title>
<p>Rhythmic structure is a fundamental feature of both speech and music. Both domains involve sequences of events (such as syllables, notes, or drum sounds) which have systematic patterns of timing, accent, and grouping (<xref rid="c1" ref-type="bibr">1</xref>). A primary acoustic correlate of perceived rhythm is the slow temporal modulation structure of sound, i.e. how sound intensity fluctuates over time (<xref ref-type="fig" rid="fig1">Fig. 1</xref>). For speech, temporal modulations below 16 Hz are related to the syllabic rhythm (<xref rid="c2" ref-type="bibr">2</xref>) and underpin speech intelligibility (<xref rid="c3" ref-type="bibr">3</xref>&#x2013;<xref rid="c5" ref-type="bibr">5</xref>). For music, slow temporal modulations are related to the onsets and offsets of notes (or runs of notes in quick succession), which support perceptual phenomena such as beat, meter, and grouping (<xref rid="c1" ref-type="bibr">1</xref>, <xref rid="c6" ref-type="bibr">6</xref>&#x2013;<xref rid="c12" ref-type="bibr">12</xref>). Recently, a number of studies have investigated the neural activation patterns associated with the temporal modulations in the human brain and assessed their relevance to speech and music perception (<xref rid="c13" ref-type="bibr">13</xref>&#x2013;<xref rid="c20" ref-type="bibr">20</xref>).</p>
<p>Both speech and music have diverse rhythmic patterns. For example, speech rhythm differs between languages which have sometimes been classified into distinct rhythm categories such as &#x2018;stress-timed&#x2019;, &#x2018;syllable-timed&#x2019;, and &#x2018;mora-timed&#x2019; (although there is debate whether rhythmic differences between languages are continuous rather than categorical (<xref rid="c21" ref-type="bibr">21</xref>)). Furthermore, the details of speech rhythm vary across speakers: people speak at different rates and pause with different patterns. The rhythms of music are even more diverse, with patterns of tempo, grouping, and metrical structure varying dramatically across genres and performances. Yet, despite high diversity, speech and music are both coherent perceptual categories, suggesting features within each domain that are widespread and principled. We explore whether acoustic rhythms constitute one such feature that can separate speech and music into two internally coherent categories.</p>
<p>In speech, temporal modulations of sound intensity show resonance around 4-5 Hz (<xref rid="c2" ref-type="bibr">2</xref>, <xref rid="c22" ref-type="bibr">22</xref>, <xref rid="c23" ref-type="bibr">23</xref>), and this acoustic modulation is a correlate of linguistically defined syllabic rhythm (<xref rid="c2" ref-type="bibr">2</xref>, <xref rid="c23" ref-type="bibr">23</xref>, <xref rid="c24" ref-type="bibr">24</xref>). Previous studies on speech modulation spectra, however, focused on a small number of speech samples and did not compare the modulation spectra across many languages and large corpora. In music, the resonance properties of the temporal modulations are not yet well characterized, and it remains unclear what units of musical structure underlie modulation peaks, e.g., notes, beats, or higher level metrical patterns.</p>
<p>Here, we first characterize the modulation spectrum of speech and investigate whether the modulation spectrum differs across 9 languages, including American and British English, French, and Mandarin Chinese. The cross-language analysis is designed to reveal whether the speech modulation spectrum is language-specific or whether it highlights a common feature across languages. The languages we analyzed fall into different rhythmic categories. For example, English is typically classified as stress-timed but French as syllable-timed (<xref rid="c21" ref-type="bibr">21</xref>). We examine whether the modulation spectrum reflects the rhythmic category of each language or if, rather, it reflects cross-linguistic features constrained by biomechanical and neural properties of the human speech production and perception system (<xref rid="c25" ref-type="bibr">25</xref>). <xref ref-type="fig" rid="fig1">Figure 1A</xref> exemplifies the materials.</p>
<p>We next characterize the modulation spectra of Western classical tonal-harmonic music, selecting pieces played by single instruments. We investigate whether single-instrument Western classical music, as a coherent perceptual category, shows a consistent modulation spectrum or whether the modulation spectra depend on the instruments being played. In addition, the modulation spectra is computed for Western ensemble music, including symphonic, jazz, and rock music, to test if there is consistency in the modulation spectra across these distinct genres. <xref ref-type="fig" rid="fig1">Figure 1B</xref>,<xref ref-type="fig" rid="fig1">C</xref> shows examples of the musical materials used. Finally, we quantify the differences between speech and music modulation spectra and test if such differences are perceptually salient.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Spectrograms of 3-second randomly chosen excerpts of speech (A), singleinstrument music (B), and ensemble music (C). The spectrograms are simulated using a cochlear model (<xref rid="c26" ref-type="bibr">26</xref>). The x-axis denotes time (3 seconds) and the y-axis denotes frequency (180 Hz to 7.24 kHz, on a logarithmic scale). The amplitude of the spectrogram is represented in a logarithmic scale and the maximal amplitude is normalized to 0 dB. The spectrogram summed over frequencies, which reflects how sound intensity fluctuates over time, is superimposed as black lines.</p></caption>
<graphic xlink:href="059683_fig1.tif"/>
</fig>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Schematic illustration of how the modulation spectrum is calculated. The sound signal is first decomposed into narrow frequency bands using a cochlear model, and the temporal envelope is extracted in each band (<xref rid="c26" ref-type="bibr">26</xref>). Envelopes for 4 frequency bands are illustrated by curves superimposed on the gray-scale figure. The root-mean-square (rms) of the Discrete Fourier Transform (DFT) of all narrowband power envelopes is the modulation spectrum.</p></caption>
<graphic xlink:href="059683_fig2.tif"/>
</fig>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Speech Modulation Spectrum</title>
<p>The modulation spectrum of speech is extracted using the procedure described in <xref ref-type="fig" rid="fig2">Fig. 2</xref>. The discourse-level speech recordings were cut into 6-second duration frames and the modulation spectrum averaged over all frames. The spectrum shows a peak between 4 and 5 Hz, and is highly consistent for the 9 languages we analyzed (<xref ref-type="fig" rid="fig3">Fig. 3A</xref>). Additionally, the modulation spectrum is shown to be highly consistent across isolated sentences, audiobooks, and conversational speech in English (<xref ref-type="fig" rid="fig3">Fig. 3B</xref>). The averaged peak frequency of the speech modulation spectra is between 4.3 and 5.4 Hz for all tested speech materials (<xref ref-type="fig" rid="fig3">Fig. 3CD</xref>; see <xref ref-type="fig" rid="figS1">Fig. S1</xref> for detailed statistics about <xref ref-type="fig" rid="fig3">Fig. 3C</xref>; no statistically significant differences are revealed between conditions in <xref ref-type="fig" rid="fig3">Fig. 3D</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><p>The modulation spectrum of speech. A) The modulation spectrum for naturalistic, discourse-level speech across 9 languages. The modulation spectrum is consistent across languages and shows a peak near 4 Hz. Each spectrum is normalized by its peak amplitude. B) The modulation spectrum for four different corpora of American English. The modulation spectrum is consistent for speech produced in different contexts, including spontaneous and read speech. The peak frequencies for the modulation spectra are shown in C and D. The error bar represents one standard deviation on each side across all the 6-second duration speech recording segments tested for each type of material.</p></caption>
<graphic xlink:href="059683_fig3.tif"/>
</fig>
</sec>
<sec id="s2b">
<title>Music Modulation Spectrum</title>
<p>The modulation spectrum of music shows important differences from that of speech. The modulation spectra of classical music played by four single-voice string instruments (violin, viola, cello, and bass, which typically play one note at a time) are shown in <xref ref-type="fig" rid="fig4">Fig. 4A</xref>. Each shows a broad peak between 1 and 2 Hz, substantially lower than the 4-5 Hz peak frequency for speech. The modulation spectra for two multi-voice instruments (piano and guitar), which typically play more than one note at a time, are shown in <xref ref-type="fig" rid="fig4">Fig.4B</xref> and do not differ substantially from the average modulation spectrum for the singlevoice instruments. Across the six different solo instruments studied here, the modulation spectrum is largely independent of instrument at values below 8 Hz. The modulation spectra for viola and guitar show secondary peaks between 16 and 32 Hz, which may be related to vibratory properties of these instruments.</p>
<p>The modulation spectra of three types of ensemble music are shown in <xref ref-type="fig" rid="fig4">Fig. 4C</xref>. The average modulation spectrum of single-voice instruments is also shown for comparison. The modulation spectrum is generally consistent for all analyzed musical styles below 4 Hz, although symphonic music tends to contain more modulation energy at very low frequencies, below about 1.5 Hz. Above 4 Hz the modulation spectra of rock and symphonic music contain more high-frequency energy than the modulation for singlevoice instruments and jazz music (the latter two have very similar modulation spectra). For the musical recordings analyzed here, only rock music contains vocals, and therefore it is compared directly with speech in <xref ref-type="fig" rid="fig4">Fig. 4D</xref>. Rock music has a broad modulation peak around 2-3 Hz, with considerably more modulation energy than speech in lower frequencies and somewhat less modulation energy than speech between 2 and 16 Hz.</p>
</sec>
<sec id="s2c">
<title>Differences between Speech and Music Modulation Spectra</title>
<p>The modulation spectrum is directly compared for different languages and musical instruments/genres in <xref ref-type="fig" rid="fig4">Fig. 4D</xref>. The spectra show a striking difference: the modulation peak for music is below 2 Hz while the peak for speech is above 4 Hz. <xref ref-type="fig" rid="fig4">Figure 4E</xref> shows that the modulation peak for speech (averaged over all materials) is consistently above the peak for music for all musical styles analyzed in the current study. The peak frequency is significantly higher for speech than each of the analyzed music material (<italic>P</italic> &#x003C; 10<sup>&#x2212;10</sup>, unpaired t-test between speech and each music category, FDR-corrected).</p>
<p>The pairwise correlation coefficients between speech and music modulation spectra are illustrated in <xref ref-type="fig" rid="fig4">Fig. 4F</xref>. The correlation matrix analysis reveals two clusters, one corresponding to speech and the other corresponding to music. The averaged pairwise correlation coefficient is 0.97 &#x00B1;0.025 (mean &#x00B1; SD) across languages and 0.98 &#x00B1;0.016 across musical instruments/genres. In contrast, the mean pairwise correlation between speech and music modulation spectra is 0.51 &#x00B1;0.13, significantly lower than the pairwise correlation within the speech or music category (<italic>P</italic> &#x003C; 0.005, unpaired t-test).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Modulation spectra of Western music. A) The modulation spectrum of classical music played by single-voice string instruments consistently shows a peak below 2 Hz. B) The modulation spectrum of multi-voice instruments, e.g., piano and guitar, is consistent with that of the single-voice string instruments. C) The modulation spectrum of multi-part music, e.g. symphonic music, jazz, and rock, shows a broader peak than single-instrument music, especially for symphonic music and rock. D) The modulation spectrum of speech (reproduced from <xref ref-type="fig" rid="fig3">Fig. 3A</xref>), single-instrument music (reproduced from <xref ref-type="fig" rid="fig4">Fig. 4AB</xref>), and multi-part music. Speech and music modulation spectra show distinct peak frequencies, and music contains more power at modulation frequencies below 4 Hz. E) The peak frequency of the modulation spectrum is consistently lower for music than speech. The error bar represents 1 standard deviation over all the 6-second duration musical recording segments analyzed for each type of material. F) Pairwise correlation coefficients between speech and music modulation spectra shown in <xref ref-type="fig" rid="fig4">Fig. 4D</xref>. See <xref ref-type="table" rid="tblS1">Table S1</xref> and <xref ref-type="table" rid="tblS2">Table S2</xref> for musical materials used in the analysis.</p></caption>
<graphic xlink:href="059683_fig4.tif"/>
</fig>
<p>To further quantify the differences between speech and music temporal modulations, we employed a linear classifier to discriminate all 6-s duration speech and music segments. The classifier was trained based on speech and single-instrument music and achieved high classification accuracy (90&#x0025; and 99&#x0025; hit rate for speech and music, respectively; 10-fold cross-validation). This classifier was then applied to ensemble music and achieved 100&#x0025;, 99&#x0025;, and 88&#x0025; hit rates for symphonic music, jazz, and rock music, even though these musical genres were not used to train the classifier, suggesting generalizable universal temporal patterning for music in contrast to speech.</p>
<p>To test whether these quantitative differences between speech and music temporal modulations are perceptually salient, listeners were trained to discriminate noise-vocoded speech and noise-vocoded music. The 1-channel noise vocoded stimuli were created to preserve the temporal modulations of speech and music while discarding spectral information, which was achieved by modulating a white noise carrier with the broadband temporal envelope of speech or music.</p>
<p>In the experiment, listeners (n&#x003D;12) had to discriminate samples of vocoded speech/music with visual feedback (see Procedures). Listeners were instructed that the two categories of sounds were made by two different &#x2018;aliens&#x2019;; and were not aware that they were vocoded speech and music. The listeners discriminated vocoded speech and music with high accuracy, and the hit rates were 0.95 &#x00B1;0.04 (mean &#x00B1; SD over listeners) for both vocoded speech and music. Furthermore, the listeners learned the task quickly and showed no clear learning effects (<xref ref-type="fig" rid="figS2">Fig. S2</xref>) These behavioral findings demonstrate that the differences between speech and music temporal modulations are perceptually salient and that listeners can detect this difference with little training.</p>
</sec>
<sec id="s2d">
<title>Correlates of the Modulation Resonance in Musical Theory</title>
<p>For the music modulation analysis, an important question is whether the spectral peak around 1-3 Hz is related to metrical structure as defined in music theory. A modulation rate of 1-3 Hz is in the range of typical musical beats (<xref rid="c1" ref-type="bibr">1</xref>). To investigate how the modulation spectrum might be related to the rate of musical beats for individual musical pieces, we conducted a further analysis based on selecting 25 additional excerpts of music with a clear beat (See Appendix for list). These excerpts were chosen from musical genres in which we could identify extended passages with a strong and steady beat: rock, funk, blues, and electronic music.</p>
<p>As shown in <xref ref-type="fig" rid="fig5">Fig. 5</xref> and quantified in <xref ref-type="table" rid="tblS3">Table S3</xref>, in these 25 excerpts the modulation spectrum generally shows a broad peak centered around 1 and 2 times the tempo, with additional narrow peaks at 1, 2, and 4 times the tempo. This suggests that the modulation spectrum peak in music reflects acoustic fluctuations near the beat rate, with additional resonance reflecting a metrical subdivision of the beat, either one metrical level below the beat (2x the beat) or two metrical levels below the beat (4x the beat). Regular subdivision of the beat is a prominent feature of Western music, and research on musical meter suggests that having one or two levels of subdivision below the primary beat level is typical for Western music (<xref rid="c7" ref-type="bibr">7</xref>). The temporal modulations provide an acoustic correlate of this musical phenomenon.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Relationship between the modulation spectra and musical tempi for 25 musical excerpts with a steady beat. A) The modulation spectrum averaged over 25 excerpts. The modulation frequency axis is normalized based on the tempo of each excerpt. The average modulation spectrum shows peaks at the tempo rate and its multiples. The strongest peaks are seen at 2 times the tempo and 4 times the tempo. B) The modulation spectrum when the frequency axis is represented in Hz. C) The relationship between musical tempo and the frequency at which the modulation spectrum shows maximal power, for all 25 excerpts. If the music tempo is denoted as <italic>T</italic> and the modulation spectrum peak frequency is denoted as F, dotted lines are plotted for the relationships: <italic>F &#x003D; T</italic>, <italic>F</italic> &#x003D; <italic>2T</italic>, <italic>F</italic> &#x003D; <italic>3T</italic>, and <italic>F</italic> &#x003D; <italic>4T</italic>. For most excerpts, the modulation spectrum peak frequency appears at <italic>2T</italic> or <italic>4T</italic>.</p></caption>
<graphic xlink:href="059683_fig5.tif"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>This study characterizes in a quantitative manner the slow temporal modulations (&#x003C; 32 Hz) in sound amplitude for speech and music. The analysis is motivated by the fact that a growing literature in perception and neuroscience research points to the relevance of these modulations in speech and music processing (<xref rid="c1" ref-type="bibr">1</xref>, <xref rid="c15" ref-type="bibr">15</xref>, <xref rid="c23" ref-type="bibr">23</xref>). The modulation spectrum of speech shows the greatest power between 2 and 10 Hz, peaking at around 5 Hz, a highly consistent pattern across the 9 languages tested in this study. These results are reminiscent of recent studies showing that the mean syllabic rate of speech is 5-8 Hz across languages (<xref rid="c24" ref-type="bibr">24</xref>), indicating universal rhythmic properties of human speech. The 210 Hz rhythm is prevalent in the speech communication chain, observed in motor cortex (<xref rid="c27" ref-type="bibr">27</xref>) and articulator movements (<xref rid="c25" ref-type="bibr">25</xref>, <xref rid="c28" ref-type="bibr">28</xref>) during speech production and in widely distributed cortical areas including auditory cortex during speech perception (<xref rid="c29" ref-type="bibr">29</xref>, <xref rid="c30" ref-type="bibr">30</xref>). Therefore, the &#x007E;5 Hz rhythm is likely an intrinsic attribute of speech, possibly imposed by the biomechanical properties of the human articulators (<xref rid="c25" ref-type="bibr">25</xref>) and the underlying neurodynamical properties of the speech production and perception systems.</p>
<p>The modulation spectrum of Western music also shows consistency across genres and musical instruments, revealing strongest activation between 0.5 and 3 Hz. This finding suggests that the different musical instruments studied here, namely violin, viola, cello, bass, piano, and guitar, do not impose strong constraints on the slow temporal modulations of music. For ensemble music, jazz has a modulation spectrum very similar to that of single-instrument music, while symphonic and rock music show broader spectral speaks. For the music analyzed here, only rock music contains vocals, which may be one reason that its modulation spectrum is closer to the modulation spectrum of speech. The plateau of the musical modulation spectrum, i.e. 0.5-3 Hz, corresponds well to the typical frequency range of musical beats. For example, dance music pieces tend to have tempi between 94 and 176 beats per minute (BPM), which corresponds to a rate of 1.6 Hz to 2.9 Hz for beats (<xref rid="c31" ref-type="bibr">31</xref>). Indeed, a subsidiary analysis demonstrates peaks in the music modulation spectra at metrical subdivision values of the basic beat rate or tempo and are thus related to the metrical structure of music.</p>
<p>Although distinct peak modulation frequencies are revealed for speech and music (<xref ref-type="fig" rid="fig1">Fig. 1</xref>&#x2013;<xref ref-type="fig" rid="fig2">2</xref>), both speech and music modulation spectra will assume a low-pass 1/f shape when the frequency axis is expressed in a linear rather than logarithmic scale (<xref rid="c10" ref-type="bibr">10</xref>, <xref rid="c32" ref-type="bibr">32</xref>, <xref rid="c33" ref-type="bibr">33</xref>). The general 1/f power fall off in the modulation spectra is not unique to speech or music but shared by many other natural sounds (<xref rid="c33" ref-type="bibr">33</xref>&#x2013;<xref rid="c35" ref-type="bibr">35</xref>). In a logarithmic frequency scale, the general 1/f trend is removed and a broad peak appears in the 3-5 Hz range for speech and the 0.5-3 Hz range for music. In other words, the distinct spectral peaks for speech and music reflect how these two sound categories deviate from a 1/f modulation spectrum. Human listeners are sensitive to such deviations and can learn to accurately discriminate the two sound categories purely based on temporal information with only a couple of training samples.</p>
<p>Temporal modulations play critical roles in speech and music processing. For speech, it has been proposed that the slow temporal modulations serve as acoustic landmarks to trigger an initial coarse analysis of speech features, which is followed by more finegrained phonetic analysis (<xref rid="c36" ref-type="bibr">36</xref>). Consistent with this hypothesis, it has been shown that neural activity in auditory cortex is synchronized to the slow temporal modulations of speech (<xref rid="c14" ref-type="bibr">14</xref>, <xref rid="c30" ref-type="bibr">30</xref>, <xref rid="c37" ref-type="bibr">37</xref>). Since the slow temporal modulations correspond to the time scale of syllables, syllable-sized acoustic chunks have been proposed as the basic unit for initial speech analysis (<xref rid="c23" ref-type="bibr">23</xref>, <xref rid="c38" ref-type="bibr">38</xref>, <xref rid="c39" ref-type="bibr">39</xref>). For music, beats are fundamental features organizing the temporal structure of music. This study shows that the acoustically salient rhythms correspond to the frequency range for common musical beats and subdivisions, suggesting that these rhythms possibly construct initial time scales for musical analysis (<xref rid="c40" ref-type="bibr">40</xref>, <xref rid="c41" ref-type="bibr">41</xref>), in parallel with the landmark hypothesis in speech (<xref rid="c36" ref-type="bibr">36</xref>). Indeed, during music listening, cortical activity has been shown to be synchronized to the perceived musical beats (<xref rid="c13" ref-type="bibr">13</xref>, <xref rid="c42" ref-type="bibr">42</xref>).</p>
<p>In sum, we characterized the slow temporal modulations of speech and music based on large corpora. The speech recordings (over 25 hours total) contain 9 languages and speech produced in different manners, e.g. during reading or telephone conversation. The music recordings (over 39 hours total) contain single-instrument music from six musical instruments, as well as symphonic, jazz, and rock music. Based on these diverse recordings, a high degree of consistency in the modulation spectra is found within the categories of speech and music &#x2013; but not across them. These findings suggest that the statistical regularities of slow temporal modulations are intrinsic signatures of speech and music. The current findings suggest that the distinctions between speech and music modulation properties may profitably be taken into account when addressing the distinctions between speech and music perceptual analysis.</p>
<p>It will be important to test to what extent the current findings generalize to languages with diverse prosodic characteristics (<xref rid="c43" ref-type="bibr">43</xref>) and to music from non-Western cultures (<xref rid="c44" ref-type="bibr">44</xref>). Additionally, it is worth bearing in mind that the analysis here concerns only &#x2018;normal&#x2019; speech. The modulation spectrum can certainly be affected by speech production pathology (<xref rid="c45" ref-type="bibr">45</xref>) or when a talker speaks exceptionally clearly or slowly (<xref rid="c46" ref-type="bibr">46</xref>). It will also be interesting to use the methods presented here to study speech and music that are primarily rhythmic in nature, e.g. metrically regular poems and North Indian tabla music (<xref rid="c47" ref-type="bibr">47</xref>, <xref rid="c48" ref-type="bibr">48</xref>), as well as music that contains no clear rhythmic beats e.g. Gregorian chant or Chinese Ch&#x2019;in music (<xref rid="c49" ref-type="bibr">49</xref>).</p>
</sec>
<sec id="s4">
<title>Materials and Methods</title>
<sec id="s4a">
<title>Speech Materials</title>
<sec id="s4a1">
<title>American English</title>
<p>Individually pronounced sentences were selected from the TIMIT database (<xref rid="c50" ref-type="bibr">50</xref>). All sentences (N &#x003D; 4,620, totaling 462 minutes of recordings) in the training set of the database were used in the analysis. Continuous discourse-level English speech materials were selected from audiobooks and two speech corpora, i.e. the Buckeye and Switchboard corpus. Four public domain audiobook chapters (from <ext-link ext-link-type="uri" xlink:href="http://librivox.org">http://librivox.org</ext-link>) were used, read by 4 different talkers (2 female). The duration of each audiobook recording varies between 15 and 83 minutes, and the total duration of the recordings was 140 minutes. The Buckeye speech corpus (<xref rid="c51" ref-type="bibr">51</xref>) contains conversations between a talker and an interviewer about everyday topics. Laughter, noise, and speech from the interviewer were removed from the recording based on the labels provided by the corpus. The first 4 audio files from the first 10 talkers in the corpus were used in the analysis, and the total duration of these recordings was 109 minutes. The Switchboard speech corpus contained conversational speech recorded over the telephone and 118 minutes of recordings were analyzed here (<xref rid="c23" ref-type="bibr">23</xref>, <xref rid="c52" ref-type="bibr">52</xref>).</p>
</sec>
<sec id="s4a2">
<title>Mandarin Chinese</title>
<p>Speech recordings in Mandarin Chinese were selected from a radio broadcast, a narrated story, and interviews. A 7-minute radio broadcast recording was selected from the program Sounds of China, on December 1 2013 (<ext-link ext-link-type="uri" xlink:href="http://aod.cnr.cn/">http://aod.cnr.cn/</ext-link>). The recording contained a single female talker with no audible background sound. A 6-minute duration narrated story, Cong Cong by Ziqing Zhu, read by a female talker (3 minutes in duration), was also included in the analysis (<ext-link ext-link-type="uri" xlink:href="http://librivox.org/multilingual-short-story-collection-001/">http://librivox.org/multilingual-short-story-collection-001/</ext-link>). Finally, ten short speeches on Chinese culture were included (<ext-link ext-link-type="uri" xlink:href="http://www.laits.utexas.edu/orkelm/chinese/">http://www.laits.utexas.edu/orkelm/chinese/</ext-link>). The 10 talkers (3 female) were from 10 different provinces in China. The total duration of the recordings was 17 minutes (1-2 minutes for each talker).</p>
</sec>
<sec id="s4a3">
<title>French</title>
<p>Audiobook recordings in French were selected from 3 chapters of Les Miserables (<ext-link ext-link-type="uri" xlink:href="http://librivox.org/les-miserables-tome-2-by-victor-hugo-1008/">http://librivox.org/les-miserables-tome-2-by-victor-hugo-1008/</ext-link>), and Le Dernier Jour d&#x2019;un Condamne (<ext-link ext-link-type="uri" xlink:href="http://archive.org/details/VictorHugo">http://archive.org/details/VictorHugo</ext-link>). The 4 sections were read by 4 different talkers (2 female). The total duration of the recordings was 64 minutes and the duration of each book chapter varies between 12 and 21 minutes. Additionally, the French materials also include short passages from the EUROM corpus, which is detailed below.</p>
</sec>
<sec id="s4a4">
<title>Seven European Languages</title>
<p>Materials include all the short passages from the EUROM corpus, which are in 7 different languages, i.e. Danish, Dutch, English, French, German, Norwegian, and Swedish. The corpus includes 60-64 speakers for each language. Three to five passages were recorded for each speaker and additional 15-20 passages were recorded from 10 of the speakers for each language. The total duration of recordings ranges from 44 minutes to 130 minutes across the 7 languages.</p>
<p>All speech recordings were resampled to 16 kHz. All recordings not from a standard speech corpus were judged as natural and representative of American English, French, or Mandarin English by a native speaker of the language. <xref ref-type="fig" rid="fig1">Figure 1A</xref> shows the auditory spectrogram simulated using a cochlear model (<xref rid="c26" ref-type="bibr">26</xref>) for a 3 second excerpt of the audiobook materials from each of the three languages.</p>
</sec>
</sec>
<sec id="s4b">
<title>Musical Materials</title>
<sec id="s4b1">
<title>Single instrument materials</title>
<p>Single instrument music was analyzed for six instruments: violin, viola, cello, bass, guitar, and piano. <xref ref-type="table" rid="tblS1">Table S1</xref> shows the different instruments analyzed, the duration of material for each instrument, representative composers whose work was included in the analysis, and one example piece for each instrument (for a full catalog of musical pieces included in this study, please see Appendix). <xref ref-type="fig" rid="fig1">Figure 1B</xref> shows the auditory spectrogram simulated using a cochlear model for a 3 second excerpt of music from each of the instruments analyzed.</p>
</sec>
<sec id="s4b2">
<title>Ensemble recordings</title>
<p><xref ref-type="table" rid="tblS2">Table S2</xref> shows the different genres analyzed, duration of material for each genre, composers whose work was included in the analysis, and one example piece for each genre. (Of the three genres analyzed, symphonic and jazz were purely instrumental music, while rock was instrumental plus vocals.) <xref ref-type="fig" rid="fig1">Figure 1C</xref> shows the auditory spectrogram simulated using a cochlear model for a 3 second excerpt for each of the ensemble musical categories analyzed.</p>
</sec>
<sec id="s4b3">
<title>Recordings for tempo analysis</title>
<p>Twenty-five excerpts of music with a clear beat were chosen, which span a wide range of tempi (55 &#x2013; 190 BPM) and ranged in duration from 60 s to 286 seconds (average duration &#x003D; 163 s). Most excerpts combined instruments and vocals (five were purely instrumental). These excerpts were selected by co-author HB, an amateur drummer (11 years experience), who verified that each except had a steady tempo and who determined the tempo of each excerpt by drumming along with it.</p>
</sec>
</sec>
<sec id="s4c">
<title>Temporal Envelopes</title>
<p>The temporal envelope was extracted using a cochlear model (<xref rid="c26" ref-type="bibr">26</xref>). The model filters the sound signal into 128 frequency bands, evenly distributed over a logarithmic frequency scale between 180 Hz and 7 kHz (<xref ref-type="fig" rid="fig2">Fig. 2</xref>). In each band, in order to extract the temporal envelope, the filtered sound signal is half-wave rectified, smoothed by a single-pole low-pass filter with a time constant of 8 ms, squared, and then decimated to 100 Hz (<xref rid="c22" ref-type="bibr">22</xref>).</p>
</sec>
<sec id="s4d">
<title>Modulation Spectrum</title>
<p>A Discrete Fourier Transform (DFT) was applied to each narrowband power envelope. Before the DFT, the sentence-level recording was cut or zero padded to 6 seconds in duration and longer speech/music recordings were cut into 6-second duration samples. No smoothing window was applied. The modulation spectrum is the absolute value of the Fourier coefficients weighted by the modulation frequency. If the Fourier coefficient is denoted as <italic>s</italic>(<italic>f</italic>), the modulation spectrum is <italic>f</italic><sup>1/2</sup>&#x007C;<italic>s</italic>(<italic>f</italic>)&#x007C;. The modulation spectrum created in this way is equivalent to filtering the temporal envelope using a logarithmically spaced filter bank and calculating the root mean square value (RMS) of the output of each filter, which is the traditional way of calculating the modulation spectrum (<xref rid="c22" ref-type="bibr">22</xref>, <xref rid="c53" ref-type="bibr">53</xref>). The final modulation spectrum is the RMS over the carrier frequency bands and is normalized by its maximal value between 0.25 and 32 Hz. The modulation spectrum is finally averaged within each corpus by calculating the RMS value.</p>
</sec>
<sec id="s4e">
<title>Linear Classification of Speech and Music Modulation Spectrum</title>
<p>A linear classifier was employed to classify the modulation spectrum of individual 6-sec duration speech and music samples. In this analysis, it was assumed that the modulation spectrum for each sound category, i.e., speech or music, was subject to a multidimensional Guassian distribution. Parameters of each Guassian distribution were fitted based on training samples. Based on the fitted Gaussian distribution, a linear hyperplane was used to separate speech samples and music samples. In the classification analysis and the correlation analysis in <xref ref-type="fig" rid="fig2">Fig. 2F</xref>, the modulation spectrum was binned and the bin size was 0.5 octave.</p>
</sec>
<sec id="s4f">
<title>Behavioral Classification of Vocoded Speech and Music</title>
<p>To test if listeners are sensitive to the differences between speech and music modulation spectra, we asked them to discriminate 1-channel noise vocoded speech and music samples. Materials include 7 European languages and 9 musical instruments/genres. Each sample was 4-seconds in duration and was randomly chosen from the relevant corpora. The temporal envelope of each speech/music sample (low-pass filtered at 4 kHz) was extracted using the Hilbert transform and low-pass filtered below 32 Hz. The temporal envelope was then used to modulate a band-limited white noise (125 Hz &#x2013; 4 kHz) to generate the 1-channel vocoded speech/music.</p>
<p>Twelve listeners (21-24 yrs old, mean 22 yrs old; 7 male) participated in the experiment. All subjects reported normal hearing and no formal musical training. Written informed consent was obtained from each participant prior to the experiment. In the experiment, the listeners sat in a quiet room before a screen and sounds were delivered using insert earphones (Sennheiser CX213). They were instructed to discriminate the sounds made by two aliens, without knowing that the two sound categories were in fact vocoded speech and music.</p>
<p>The experiment was divided into 3 stages. In stage 1, the listeners clicked on each alien to listen to one 4-s duration sample sound. In stage 2, the listeners listened to 10 vocoded stimuli and had to judge which alien made the sound by pressing different buttons. Visual feedback, i.e. right or wrong, was given after each response. Half of the 10 stimuli were vocoded speech and the half were vocoded music. Each sample was from a different language or musical instrument/genre that was randomly selected online for each listeners. If the averaged correct rate in stage 2 is above 90&#x0025;, the listeners were allowed to continue to the third stage, in which they had to discriminate 126 vocoded stimuli with visual feedback. Half of the stimuli (<italic>N</italic> &#x003D; 63) were vocoded speech (9 samples for each of the 7 languages) and the other half were vocoded music (7 samples for each of the musical instrument/genre). All listeners successfully proceeded to stage 3 after stage 2. The reported hit rates were based on stage 3, while the results from stage 2 were shown in <xref ref-type="fig" rid="figS2">Fig. S2</xref>.</p>
<p>Since speech tends to contain pauses while music does not, to ensure that pauses are not the only cue used to discriminate vocoded speech and music, we further created a condition in which pauses longer than 250 ms were removed. For this conditoin, we first randomly selected 6-seconds duration segments from speech and music corpora with the criterion that the total duration of long pauses was shorter than 1.5 seconds. Long pauses were then removed from the noise-vocoded stimulus, after which the first 4 seconds of the stimulus was retained for the experiment. Six listeners participated in this no-pause condition and the other six listeners participated in the condition in which the pauses in speech were not manipulated. The hit rates from the no-pause condition were not statistically different from the original condition in which pauses were not removed (<italic>P</italic> &#x003E; 0.1. unpaired t-test), and therefore results from these two conditions were pooled together.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank Erik Broess of Tufts University for help in selecting the music materials and Jess Rowland for help in creating the music catalog. Work supported by NIH 2R01DC05660 (DP), National Natural Science Foundation of China 31500873 (ND), Fundamental Research Funds for the Central Universities (ND), and Zhejiang Provincial Natural Science Foundation of China LR16C090002 (ND).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="book"><string-name><surname>Patel</surname> <given-names>AD</given-names></string-name> (<year>2008</year>) <source>Music, language, and the brain</source> (<publisher-name>Oxford University Press</publisher-name>, <publisher-loc>New York, NY</publisher-loc>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Greenberg</surname> <given-names>S</given-names></string-name>, <string-name><surname>Carvey</surname> <given-names>H</given-names></string-name>, <string-name><surname>Hitchcock</surname> <given-names>L</given-names></string-name>, &#x0026; <string-name><surname>Chang</surname> <given-names>S</given-names></string-name> (<year>2003</year>) <article-title>Temporal properties of spontaneous speech &#x2013; a syllable-centric perspective</article-title><source>Journal of Phonetics</source> <volume>31</volume>(<issue>3-4</issue>):<fpage>465</fpage>&#x2013;<lpage>485</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Drullman</surname> <given-names>R</given-names></string-name>, <string-name><surname>Festen</surname> <given-names>JM</given-names></string-name>, &#x0026; <string-name><surname>Plomp</surname> <given-names>R</given-names></string-name> (<year>1994</year>) <article-title>Effect of reducing slow temporal modulations on speech reception</article-title><source>Journal of the Acoustical Society of America</source> <volume>95</volume>:<fpage>2670</fpage>&#x2013;<lpage>2680</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Elliott</surname> <given-names>T</given-names></string-name> &#x0026; <string-name><surname>Theunissen</surname> <given-names>F</given-names></string-name> (<year>2009</year>) <article-title>The Modulation Transfer Function for Speech Intelligibility</article-title><source>Plos Computational Biology</source> <volume>5</volume>(<issue>3</issue>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Shannon</surname> <given-names>RV</given-names></string-name>, <string-name><surname>Zeng</surname> <given-names>F-G</given-names></string-name>, <string-name><surname>Kamath</surname> <given-names>V</given-names></string-name>, <string-name><surname>Wygonski</surname> <given-names>J</given-names></string-name>, &#x0026; <string-name><surname>Ekelid</surname> <given-names>M</given-names></string-name> (<year>1995</year>) <article-title>Speech recognition with primarily temporal cues</article-title><source>Science</source> <volume>270</volume>(<issue>5234</issue>):<fpage>303</fpage>&#x2013;<lpage>304</lpage>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Gordon</surname> <given-names>JW</given-names></string-name> (<year>1987</year>) <article-title>The perceptual attack time of musical tones</article-title><source>Journal of the Acoustical Society of America</source> <volume>82</volume>(<issue>1</issue>):<fpage>88</fpage>&#x2013;<lpage>105</lpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="book"><string-name><surname>London</surname> <given-names>J</given-names></string-name> (<year>2012</year>) <source>Hearing in time</source> (<publisher-name>Oxford University Press</publisher-name>, <publisher-loc>New York</publisher-loc>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>McKinney</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Moelants</surname> <given-names>D</given-names></string-name>, <string-name><surname>Davies</surname> <given-names>ME</given-names></string-name>, &#x0026; <string-name><surname>Klapuri</surname> <given-names>A</given-names></string-name> (<year>2007</year>) <article-title>Evaluation of audio beat tracking and music tempo extraction algorithms</article-title><source>Journal of New Music Research</source> <volume>36</volume>(<issue>1</issue>):<fpage>1</fpage>&#x2013;<lpage>16</lpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Scheirer</surname> <given-names>ED</given-names></string-name> (<year>1998</year>) <article-title>Tempo and beat analysis of acoustic musical signals</article-title><source>Journal of the Acoustical Society of America</source> <volume>103</volume>(<issue>1</issue>):<fpage>588</fpage>&#x2013;<lpage>601</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="confproc"><string-name><surname>Levitin</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Chordia</surname> <given-names>P</given-names></string-name>, &#x0026; <string-name><surname>Menon</surname> <given-names>V</given-names></string-name> (<conf-date>2012</conf-date>) <article-title>Musical rhythm spectra from Bach to Joplin obey a 1/f power law</article-title><conf-name>Proceedings of the National Academy of Sciences</conf-name> <volume>109</volume>(<issue>10</issue>):<fpage>3716</fpage>&#x2013;<lpage>3720</lpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Large</surname> <given-names>EW</given-names></string-name> &#x0026; <string-name><surname>Palmer</surname> <given-names>C</given-names></string-name> (<year>2002</year>) <article-title>Perceiving temporal regularity in music</article-title>. <source>Cognitive Science</source> <volume>26</volume>:<fpage>1</fpage>&#x2013;<lpage>37</lpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Todd</surname> <given-names>NPM</given-names></string-name> (<year>1994</year>) <article-title>The auditory &#x201C;primal sketch&#x201C;: A multiscale model of rhythmic grouping</article-title>. <source>Journal of new music Research</source> <volume>23</volume>(<issue>1</issue>):<fpage>25</fpage>&#x2013;<lpage>70</lpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Nozaradan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Peretz</surname> <given-names>I</given-names></string-name>, <string-name><surname>Missal</surname> <given-names>M</given-names></string-name>, &#x0026; <string-name><surname>Mouraux</surname> <given-names>A</given-names></string-name> (<year>2011</year>) <article-title>Tagging the neuronal entrainment to beat and meter</article-title>. <source>Journal of Neuroscience</source> <volume>31</volume>:<fpage>10234</fpage>&#x2013;<lpage>10240</lpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Ding</surname> <given-names>N</given-names></string-name> &#x0026; <string-name><surname>Simon</surname> <given-names>JZ</given-names></string-name> (<year>2014</year>) <article-title>Cortical entrainment to continuous speech: functional roles and interpretations</article-title><source>frontiers in human neuroscience</source> <volume>8</volume>(<issue>311</issue>):<pub-id pub-id-type="doi">10.3389/fnhum.2014.00311</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Giraud</surname> <given-names>A-L</given-names></string-name> &#x0026; <string-name><surname>Poeppel</surname> <given-names>D</given-names></string-name> (<year>2012</year>) <article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title>. <source>Nature Neuroscience</source> <volume>15</volume>(<issue>4</issue>):<fpage>511</fpage>&#x2013;<lpage>517</lpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Norman-Haignere</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>NG</given-names></string-name>, &#x0026; <string-name><surname>McDermott</surname> <given-names>JH</given-names></string-name> (<year>2015</year>) <article-title>Distinct Cortical Pathways for Music and Speech Revealed by Hypothesis-Free Voxel Decomposition</article-title>. <source>Neuron</source> <volume>88</volume>(<issue>6</issue>):<fpage>1281</fpage>&#x2013;<lpage>1296</lpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="confproc"><string-name><surname>Doelling</surname> <given-names>KB</given-names></string-name> &#x0026; <string-name><surname>Poeppel</surname> <given-names>D</given-names></string-name> (<conf-date>2015</conf-date>) <article-title>Cortical entrainment to music and its modulation by expertise</article-title>. <conf-name>Proceedings of the National Academy of Sciences</conf-name> <volume>112</volume>(<issue>45</issue>):<fpage>E6233</fpage>&#x2013;<lpage>E6242</lpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Overath</surname> <given-names>T</given-names></string-name>, <string-name><surname>McDermott</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Zarate</surname> <given-names>JM</given-names></string-name>, &#x0026; <string-name><surname>Poeppel</surname> <given-names>D</given-names></string-name> (<year>2015</year>) <article-title>The cortical analysis of speech-specific temporal structure revealed by responses to sound quilts</article-title>. <source>Nature neuroscience</source> <volume>18</volume>(<issue>6</issue>):<fpage>903</fpage>&#x2013;<lpage>911</lpage>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="confproc"><string-name><surname>Barton</surname> <given-names>B</given-names></string-name>, <string-name><surname>Venezia</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Saberi</surname> <given-names>K</given-names></string-name>, <string-name><surname>Hickok</surname> <given-names>G</given-names></string-name>, &#x0026; <string-name><surname>Brewer</surname> <given-names>AA</given-names></string-name> (<conf-date>2012</conf-date>) <article-title>Orthogonal acoustic dimensions define auditory field maps in human cortex</article-title>. <conf-name>Proceedings of the National Academy of Sciences</conf-name> <volume>109</volume>(<issue>50</issue>):<fpage>20738</fpage>&#x2013;<lpage>20743</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Santoro</surname> <given-names>R</given-names></string-name>, <etal>et al.</etal> (<year>2014</year>) <article-title>Encoding of Natural Sounds at Multiple Spectral and Temporal Resolutions in the Human Auditory Cortex</article-title>. <source>PLoS computational biology</source> <volume>10</volume>(<issue>1</issue>):<fpage>e1003412</fpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Turk</surname> <given-names>A</given-names></string-name> &#x0026; <string-name><surname>Shattuck-Hufnagel</surname> <given-names>S</given-names></string-name> (<year>2013</year>) <article-title>What is speech rhythm? A commentary on Arvaniti and Rodriquez, Krivokapic, and Goswami and Leong</article-title>. <source>Laboratory Phonology</source> <volume>4</volume>(<issue>1</issue>):<fpage>93</fpage>&#x2013;<lpage>118</lpage>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Houtgast</surname> <given-names>T</given-names></string-name> &#x0026; <string-name><surname>Steeneken</surname> <given-names>HJ</given-names></string-name> (<year>1985</year>) <article-title>A review of the MTF concept in room acoustics and its use for estimating speech intelligibility in auditoria</article-title>. <source>Journal of the Acoustical Society of America</source> <volume>77</volume>:<fpage>1069</fpage>&#x2013;<lpage>1077</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Greenberg</surname> <given-names>S</given-names></string-name> (<year>1999</year>) <article-title>Speaking in shorthand &#x2013; A syllable-centric perspective for understanding pronunciation variation</article-title>. <source>Speech Communication</source> <volume>29</volume>:<fpage>159</fpage>&#x2013;<lpage>176</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Pellegrino</surname> <given-names>F</given-names></string-name>, <string-name><surname>Coupe</surname> <given-names>C</given-names></string-name>, &#x0026; <string-name><surname>Marsico</surname> <given-names>E</given-names></string-name> (<year>2011</year>) <article-title>Across-language perspective on speech information rate</article-title>. <source>Language</source> <volume>87</volume>(<issue>3</issue>):<fpage>539</fpage>&#x2013;<lpage>558</lpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Chandrasekaran</surname> <given-names>C</given-names></string-name>, <string-name><surname>Trubanova</surname> <given-names>A</given-names></string-name>, <string-name><surname>Stillittano</surname> <given-names>S</given-names></string-name>, <string-name><surname>Caplier</surname> <given-names>A</given-names></string-name>, &#x0026; <string-name><surname>Ghazanfar</surname> <given-names>AA</given-names></string-name> (<year>2009</year>) <article-title>The natural statistics of audiovisual speech</article-title>. <source>PLoS computational biology</source> <volume>5</volume>(<issue>7</issue>):<fpage>e1000436</fpage>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Yang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>K</given-names></string-name>, &#x0026; <string-name><surname>Shamma</surname> <given-names>SA</given-names></string-name> (<year>1992</year>) <article-title>Auditory representations of acoustic signals</article-title>. <source>IEEE Transactions on Information Theory</source> <volume>38</volume>(<issue>2</issue>):<fpage>824</fpage>&#x2013;<lpage>839</lpage>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Ruspantini</surname> <given-names>I</given-names></string-name>, <etal>et al.</etal> (<year>2012</year>) <article-title>Corticomuscular Coherence Is Tuned to the Spontaneous Rhythmicity of Speech at 2-3 Hz</article-title>. <source>Journal of Neuroscience</source> <volume>32</volume>(<issue>11</issue>):<fpage>3786</fpage>&#x2013;<lpage>3790</lpage>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Ghazanfar</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Takahashi</surname> <given-names>DY</given-names></string-name>, <string-name><surname>Mathur</surname> <given-names>N</given-names></string-name>, &#x0026; <string-name><surname>Fitch</surname> <given-names>WT</given-names></string-name> (<year>2012</year>) <article-title>Cineradiography of monkey lip-smacking reveals putative precursors of speech dynamics</article-title>. <source>Current Biology</source> <volume>22</volume>(<issue>13</issue>):<fpage>2012</fpage>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Ding</surname> <given-names>N</given-names></string-name>, <string-name><surname>Melloni</surname> <given-names>L</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>H</given-names></string-name>, <string-name><surname>Tian</surname> <given-names>X</given-names></string-name>, &#x0026; <string-name><surname>Poeppel</surname> <given-names>D</given-names></string-name> (<year>2016</year>) <article-title>Cortical tracking of hierarchical linguistic structures in connected speech</article-title>. <source>Nature neuroscience</source> <volume>19</volume>(<issue>1</issue>):<fpage>158</fpage>&#x2013;<lpage>164</lpage>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Zion Golumbic</surname> <given-names>EM</given-names></string-name>, <etal>et al.</etal> (<year>2013</year>) <article-title>Mechanisms Underlying Selective Neuronal Tracking of Attended Speech at a &#x201C;Cocktail Party&#x201D;</article-title>. <source>Neuron</source> <volume>77</volume>:<fpage>980</fpage>&#x2013;<lpage>991</lpage>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Noorden</surname> <given-names>Lv</given-names></string-name> &#x0026; <string-name><surname>Moelants</surname> <given-names>D</given-names></string-name> (<year>1999</year>) <article-title>Resonance in the Perception of Musical Pulse</article-title>. <source>Journal of New Music Research</source> <volume>28</volume>(<issue>1</issue>):<fpage>43</fpage>&#x2013;<lpage>66</lpage>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Voss</surname> <given-names>RF</given-names></string-name> &#x0026; <string-name><surname>Clarke</surname> <given-names>J</given-names></string-name> (<year>1975</year>) <article-title>1/f &#x2018;noise&#x2019; in music and speech</article-title>. <source>Nature</source> <volume>258</volume>:<fpage>317</fpage>&#x2013;<lpage>318</lpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>De Coensel</surname> <given-names>B</given-names></string-name>, <string-name><surname>Botteldooren</surname> <given-names>D</given-names></string-name>, &#x0026; <string-name><surname>Muer</surname> <given-names>TD</given-names></string-name> (<year>2003</year>) <article-title>1/f noise in rural and urban soundscapes</article-title>. <source>Acta Acustica United with Acustica</source> <volume>89</volume>(<issue>2</issue>):<fpage>287</fpage>&#x2013;<lpage>295</lpage>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="book"><string-name><surname>Attias</surname> <given-names>H</given-names></string-name> &#x0026; <string-name><surname>Schreiner</surname> <given-names>CE</given-names></string-name> (<year>1997</year>) <chapter-title>Temporal low-order statistics of natural sounds</chapter-title><source>Advances in neural information processing systems</source>, eds <person-group person-group-type="editor"><string-name><surname>Mozer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Jordan</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kearns</surname> <given-names>M</given-names></string-name>, &#x0026; <string-name><surname>Solla</surname> <given-names>S</given-names></string-name></person-group> (<publisher-name>MIT Press</publisher-name>), pp <fpage>27</fpage>&#x2013;<lpage>33</lpage>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Singh</surname> <given-names>NC</given-names></string-name> &#x0026; <string-name><surname>Theunissen</surname> <given-names>FE</given-names></string-name> (<year>2003</year>) <article-title>Modulation spectra of natural sounds and ethological theories of auditory processing</article-title>. <source>Journal of the Acoustical Society of America</source> <volume>114</volume>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Stevens</surname> <given-names>KN</given-names></string-name> (<year>2002</year>) <article-title>Toward a model for lexical access based on acoustic landmarks and distinctive features</article-title>. <source>Journal of the Acoustical Society of America</source> <volume>111</volume>:<fpage>1872</fpage>&#x2013;<lpage>1891</lpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Luo</surname> <given-names>H</given-names></string-name> &#x0026; <string-name><surname>Poeppel</surname> <given-names>D</given-names></string-name> (<year>2007</year>) <article-title>Phase Patterns of Neuronal Responses Reliably Discriminate Speech in Human Auditory Cortex</article-title>. <source>Neuron</source> <volume>54</volume>:<fpage>1001</fpage>&#x2013;<lpage>1010</lpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Ghitza</surname> <given-names>O</given-names></string-name> (<year>2013</year>) <article-title>The theta-syllable: a unit of speech information defined by cortical function</article-title>. <source>Frontiers in psychology</source> <volume>4</volume>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Poeppel</surname> <given-names>D</given-names></string-name>, <string-name><surname>Idsardi</surname> <given-names>WJ</given-names></string-name>, &#x0026; <string-name><surname>Wassenhove</surname><given-names>Vv</given-names></string-name> (<year>2008</year>) <article-title>Speech perception at the interface of neurobiology and linguistics</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source> <volume>363</volume>(<issue>1493</issue>):<fpage>1071</fpage>&#x2013;<lpage>1086</lpage>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Farbood</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Marcus</surname> <given-names>G</given-names></string-name>, &#x0026; <string-name><surname>Poeppel</surname> <given-names>D</given-names></string-name> (<year>2013</year>) <article-title>Temporal dynamics and the identification of musical key</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source> <volume>39</volume>(<issue>4</issue>):<fpage>911</fpage>&#x2013;<lpage>918</lpage>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="book"><string-name><surname>Kraus</surname> <given-names>N</given-names></string-name> &#x0026; <string-name><surname>Slater</surname> <given-names>J</given-names></string-name> (<year>2015</year>) <source>Music and language: relations and disconnections</source> <publisher-name>The Human Auditory System: Fundamental Organization and Clinical Disorders</publisher-name>, eds <person-group person-group-type="editor"><string-name><surname>Celesia</surname> <given-names>GG</given-names></string-name> &#x0026; <string-name><surname>Hickok</surname> <given-names>G</given-names></string-name>)</person-group>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Nozaradan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Peretz</surname> <given-names>I</given-names></string-name>, &#x0026; <string-name><surname>Mouraux</surname> <given-names>A</given-names></string-name> (<year>2012</year>) <article-title>Selective neuronal entrainment to the beat and meter embedded in a musical rhythm</article-title>. <source>Journal of Neuroscience</source> <volume>32</volume>(<issue>49</issue>):<fpage>17572</fpage>&#x2013;<lpage>17581</lpage>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="book"><string-name><surname>Jun</surname> <given-names>S-A</given-names></string-name> (<year>2005</year>) <source>Prosodic Typology. Prosodic Typology: The Phonology of Intonation and Phrasing</source>, ed <person-group person-group-type="editor"><string-name><surname>Jun</surname> <given-names>S-A</given-names></string-name></person-group> (<publisher-name>Oxford University Press</publisher-name>, <publisher-loc>Oxford, UK</publisher-loc>), pp <fpage>430</fpage>&#x2013;<lpage>458</lpage>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>Brown</surname> <given-names>S</given-names></string-name> &#x0026; <string-name><surname>Jordania</surname> <given-names>J</given-names></string-name> (<year>2013</year>) <article-title>Universals in the world&#x2019;s musics</article-title>. <source>Psychology of Music</source> <volume>41</volume>:<fpage>229</fpage>&#x2013;<lpage>248</lpage>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>Falk</surname> <given-names>TH</given-names></string-name>, <string-name><surname>Chan</surname> <given-names>W-Y</given-names></string-name>, &#x0026; <string-name><surname>Shein</surname> <given-names>F</given-names></string-name> (<year>2012</year>) <article-title>Characterization of atypical vocal source excitation, temporal dynamics and prosody for objective measurement of dysarthric word intelligibility</article-title>. <source>Speech Communication</source> <volume>54</volume>:<fpage>622</fpage>&#x2013;<lpage>631</lpage>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><surname>Krause</surname> <given-names>JC</given-names></string-name> &#x0026; <string-name><surname>Braida</surname> <given-names>LD</given-names></string-name> (<year>2004</year>) <article-title>Acoustic properties of naturally produced clear speech at normal speaking rates</article-title>. <source>Journal of the Acoustical Society of America</source> <volume>115</volume> <fpage>362</fpage>&#x2013;<lpage>378</lpage>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>Turner</surname> <given-names>F</given-names></string-name> &#x0026; <string-name><surname>Poppel</surname> <given-names>E</given-names></string-name> (<year>1983</year>) <article-title>The neural lyre: Poetic meter, the brain, and time</article-title>. <source>Poetry</source> <volume>142</volume>(<issue>5</issue>):<fpage>277</fpage>&#x2013;<lpage>309</lpage>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="confproc"><string-name><surname>Patel</surname> <given-names>AD</given-names></string-name> &#x0026; <string-name><surname>Iversen</surname> <given-names>JR</given-names></string-name> (<conf-date>2003</conf-date>) <article-title>Acoustic and perceptual comparison of speech and drum sounds in the north indian tabla tradition: An empirical study of sound symbolism</article-title>. <conf-name>Proceedings of the 15th International Congress of Phonetic Sciences</conf-name>, pp <fpage>925</fpage>&#x2013;<lpage>928</lpage>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="book"><string-name><surname>Van Gulik</surname> <given-names>RH</given-names></string-name> (<year>1969</year>) <source>The lore of the Chinese lute</source> (<publisher-loc>Sophia university</publisher-loc>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Garofolo</surname> <given-names>JS</given-names></string-name> (<year>1993</year>) <article-title>TIMIT: Acoustic-phonetic Continuous Speech Corpus</article-title>. <source>Linguistic Data Consortium</source>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><surname>Pitt</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Johnson</surname> <given-names>K</given-names></string-name>, <string-name><surname>Hume</surname> <given-names>E</given-names></string-name>, <string-name><surname>Kiesling</surname> <given-names>S</given-names></string-name>, &#x0026; <string-name><surname>Raymond</surname> <given-names>W</given-names></string-name> (<year>2005</year>) <article-title>The Buckeye corpus of conversational speech: labeling conventions and a test of transcriber reliability</article-title>. <source>Speech Communication</source>:<fpage>89</fpage>&#x2013;<lpage>95</lpage>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="confproc"><string-name><surname>Godfrey</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Holliman</surname> <given-names>EC</given-names></string-name>, &#x0026; <string-name><surname>McDaniel</surname> <given-names>J</given-names></string-name> (<year>1992</year>) <article-title>SWITCHBOARD: Telephone speech corpus for research and development</article-title>. <conf-name>IEEE International Conference on Acoustics, Speech, and Signal Processing</conf-name>, pp <fpage>517</fpage>&#x2013;<lpage>520</lpage>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><string-name><surname>Payton</surname> <given-names>KL</given-names></string-name> &#x0026; <string-name><surname>Braida</surname> <given-names>LD</given-names></string-name> (<year>1999</year>) <article-title>A method to determine the speech transmission index from speech waveforms</article-title>. <source>Journal of the Acoustical Society of America</source> <volume>106</volume>:<fpage>3637</fpage>&#x2013;<lpage>3648</lpage>.</mixed-citation></ref>
</ref-list>
<sec id="s5" sec-type="supplemental-materials">
<title>Supporting Figures</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1.</label>
<caption><p>Statistical analysis of the differences between modulation peak frequencies across languages. Statistically significant differences are marked by stars (&#x002A;<italic>P</italic> &#x003C; 0.05 and &#x002A; <italic>P</italic> &#x003C; 0.05, FDR corrected). Only 4 languages, i.e., French, Swedish, Danish, and, Dutch, show significant differences from other languages, and comparisons based on each of the
4 languages is shown by a different color.</p></caption>
<graphic xlink:href="059683_figS1.tif"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2.</label>
<caption><p>Behavioral discrimination of vocoded speech and music. Data are based on stage 2 of the behavioral experiment (see Procedures for details). In brief, in stage 1, the listeners were exposed to one sample of vocoded speech and one sample of vocoded music (4 seconds in duration). In stage 2, the listeners had to discriminate 10 novel vocoded stimuli, with visual feedback. Even for the very first stimulus in stage 2, they achieved greater than 80&#x0025; accuracy, indicating that the listeners can capture the differences between vocoded speech and music based on even one example per category. Error bars show 1 standard error on each side.</p></caption>
<graphic xlink:href="059683_figS2.tif"/>
</fig>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>Table S1,</label>
<caption><p>Solo instrumental music included in this study.</p></caption>
<graphic xlink:href="059683_tblS1.tif"/>
</table-wrap>
<table-wrap id="tblS2" orientation="portrait" position="float">
<label>Table S2,</label>
<caption><p>Ensemble music included in this study.</p></caption>
<graphic xlink:href="059683_tblS2.tif"/>
</table-wrap>
<table-wrap id="tblS3" orientation="portrait" position="float">
<label>Table S3.</label>
<caption><p>The relationship between musical tempo and peaks in the music modulation spectrum for the musical excerpts shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>.The 2<sup>nd</sup> row shows whether the power at a frequency of interest (e.g., the tempo rate or its harmonically related frequencies) is significantly larger than the power averaged over neighboring frequency bins (paired t-test based on 25 excerpts). If the tempo is denoted by <italic>T</italic> and the frequency of interest is denoted as f the neighboring bins considered here fall between <italic>f &#x2013; T</italic> and <italic>f</italic>&#x002B; <italic>T</italic>. A significant p-value means that the power at the frequency interest, <italic>f</italic>, is larger than the power of any frequency bin falling between <italic>f &#x2013; T</italic> and <italic>f</italic> &#x002B; <italic>T</italic>. The average modulation spectrum shows a statistically significant peak at <italic>T</italic>, 2<italic>T</italic>, 3<italic>T</italic>, and 4<italic>T</italic>. The 3<sup>rd</sup> row shows the number of songs showing a local maximum at each frequency of interest. A local maximum is caculated between <italic>f &#x2013; T</italic> and<italic>f</italic> &#x002B; <italic>T</italic>. Most excerpts show a spectral peak at 2<italic>T</italic> or 4<italic>T</italic>.</p></caption>
<graphic xlink:href="059683_tblS3.tif"/>
</table-wrap>
</sec>
<app-group>
<app id="app1">
<label>Appendix</label>
<title><underline>Piano</underline></title>
<p>Mozart Piano Sonatas K. 310, K. 331, K. 533/494</p>
<p>Perahia, Murra, pianist.</p>
<p>Sony Classical, SK48233 (1995)
<disp-quote>
<p><italic>Mozart WA, Piano Sonata in A minor, K 330</italic></p>
<p><italic>Mozart WA, Piano Sonata in A major, K 331</italic></p>
<p><italic>Mozart WA, Piano Sonata in F major, K 533/494</italic></p>
</disp-quote>
</p>
<p>Beethoven: Sonata, Op.27,No.2 / Franck: Prelude, choral et fugue / Brahms: Paganini</p>
<p>Variations</p>
<p>Kissin, Evgeny, pianist.</p>
<p>RCA, BMG 09026-68910-2 (1998)
<disp-quote>
<p><italic>Beethoven, L. Sonata Op. 27 No. 2 &#x2018;Moonlight&#x2019; in C Sharp</italic></p>
<p><italic>Franck, C. Prelude, Choral et Fugue</italic></p>
<p><italic>Brahms, J. Paganini Variations</italic></p>
</disp-quote>
</p>
<p>Beethoven: Piano Sonatas, Op.81a &#x0026; 106</p>
<p>Brendel, Alfred, pianist.</p>
<p>Phillips Import, Philips 446 093-2 (1996)
<disp-quote>
<p><italic>Beethoven, L. Sonata for piano No. 29 in B flat major (&#x201C;Hammerklavier&#x201C;) Op. 106</italic></p>
<p><italic>Beethoven, L. Sonata for piano No. 26 in E flat major (&#x201C;Les Adieux&#x201C;) Op. 81a</italic></p>
</disp-quote>
</p>
<p>Mozart: Klaviersonaten</p>
<p>Pogorelich, Ivo, pianist.</p>
<p>Dg Imports, DG4377632 (1995)
<disp-quote>
<p><italic>Mozart WA, Fantasia for Piano in D minor K 397</italic></p>
<p><italic>Mozart WA, Piano Sonata in G major K 283</italic></p>
<p><italic>Mozart WA, Piano Sonata in A major K 331</italic></p>
</disp-quote>
</p>
</app>
<app id="app2">
<title><underline>Cello</underline></title>
<p>Unaccompanied cello suites. Johann Sebastian Bach.</p>
<p>Ma, Yo-Yo, cellist.</p>
<p>Columbia, M2K 37867 (1983)
<disp-quote>
<p><italic>Bach JS. No. 1, S. BWV 1007, G major</italic></p>
<p><italic>Bach JS. No. 4, S. BWV 1010, E-flat major</italic></p>
<p><italic>Bach JS. No. 5, S. BWV 1011, C minor</italic></p>
<p><italic>Bach JS. No. 2, S. BWV 1008, D minor</italic></p>
<p><italic>Bach JS. No. 3, S. BWV 1009, C major</italic></p>
<p><italic>Bach JS. No. 6, S. BWV 1012, D major</italic></p>
</disp-quote>
</p>
<p>Cello Sonatas</p>
<p>Epperson, Gordon, cellist.</p>
<p>Centaur, CRC 2228 (1995)
<disp-quote>
<p><italic>Eug&#x00E8;ne Ysay. Sonate pour violoncelle seul, op. 28</italic></p>
<p><italic>George Crumb. Sonata for solo violoncello</italic></p>
<p><italic>Zolt&#x00E1;n Kod&#x00E1;ly. Sonate pour violoncelle seul, op. 8</italic></p>
</disp-quote>
</p>
</app>
<app id="app3">
<title><underline>Bass</underline></title>
<p>Unaccompanied cello suites: performed on double bass</p>
<p>Meyer, Edgar, bassist.</p>
<p>Sony Classical, SK 89183 (2000)
<disp-quote>
<p><italic>Bach, JS. Suites, violoncello, BWV1008</italic></p>
<p><italic>Bach, JS. Suites, violoncello, BWV 1007</italic></p>
<p><italic>Bach, JS. Suites, violoncello, BWV 1011</italic></p>
</disp-quote>
</p>
</app>
<app id="app4">
<title><underline>Viola</underline></title>
<p>Bach, JS. Six cello suites, on viola</p>
<p>Callus, Helen, violist.</p>
<p>Analekta, AN 2 9968-9 (2011)
<disp-quote>
<p><italic>Bach, JS. Suite no. 1 in G major BWV 1007</italic></p>
<p><italic>Bach, JS. Suite no. 2 in D minor, BWV 1008</italic></p>
<p><italic>Bach, JS. Suite no. 3 in C major BWV 1009</italic></p>
<p><italic>Bach, JS. Suite no. 4 in Eflat major BWV 1010</italic></p>
<p><italic>Bach, JS. Suite no. 5 in C minor BWV 1011</italic></p>
<p><italic>Bach, JS. Suite no. 6 in D BWV 1012 (transposed in G major)</italic></p>
</disp-quote>
</p>
<p>Hindemith, Viola Sonatas</p>
<p>Levin, Robert D. Sonatas for viola alone; Sonatas for viola and piano</p>
<p>Kashkashian, Kim, violist.</p>
<p>ECM New Series, ECM 1330-32 (1988)
<disp-quote>
<p><italic>Paul Hindemith. Sonatas for viola alone. op. 31/4</italic></p>
<p><italic>Paul Hindemith. Sonatas for viola alone op. 25/1</italic></p>
<p><italic>Paul Hindemith. Sonatas for viola alone op. 11/5</italic></p>
</disp-quote>
</p>
<p>Watras, Melia. Viola solo</p>
<p>Fleur de Son Classics, FDS 57962 (2004)
<disp-quote>
<p><italic>Arad, Atar. Sonata</italic></p>
<p><italic>Bach, Johann Sebastian. Chromatische Fantasie und Fuge. Fantasia</italic>.</p>
<p><italic>Corigliano, John. Fancy on a Bach air</italic></p>
<p><italic>Waggoner, Andrew. Collines parmi &#x00E9;toiles</italic></p>
<p><italic>Stravinsky, Igor. Elegy</italic>.</p>
<p><italic>Prestini, Paola. Sympathique</italic>.</p>
<p><italic>Penderecki, Krzysztof. Cadenza</italic></p>
</disp-quote>
</p>
</app>
<app id="app5">
<title><underline>Violin</underline></title>
<p>Ysae&#x00FF; Eugen&#x00E8;. Six sonatas for solo violin, op. 27</p>
<p>Murray, Tai, violinst.</p>
<p>Harmonia Mundi USA, HMU 907569 (2012)
<disp-quote>
<p><italic>Ysae&#x00FF; Eug&#x00E8;ne. Sonata no. 1</italic></p>
<p><italic>Ysae&#x00FF; Eug&#x00E8;ne. Sonata no. 2</italic></p>
<p><italic>Ysae&#x00FF; Eug&#x00E8;ne. Sonata no. 3</italic></p>
<p><italic>Ysae&#x00FF; Eug&#x00E8;ne. Sonata no. 4</italic></p>
<p><italic>Ysae&#x00FF; Eug&#x00E8;ne. Sonata no. 5</italic></p>
<p><italic>Ysae&#x00FF; Eug&#x00E8;ne. Sonata no. 6</italic></p>
</disp-quote>
</p>
<p>Bach, JS. The complete sonatas and partitas for solo violin. Vol. 1</p>
<p>Ross, Jacqueline, violinst.</p>
<p>Gaudeamus, GAU 358 (2007)
<disp-quote>
<p><italic>Bach JS. Sonata no. 1 in g minor, BWV 1001</italic></p>
<p><italic>Bach JS. Partita no. 1 in B minor, BWV 1002</italic></p>
<p><italic>Bach JS. Sonata no. 2 in A minor, BWV 1003</italic></p>
</disp-quote>
</p>
<p>Bach, JS. The complete sonatas and partitas for solo violin. Vol. 2.</p>
<p>Ross, Jacqueline.</p>
<p>Gaudeamus, GAU 359 (2007)
<disp-quote>
<p><italic>Bach JS. Partita no. 2 in D minor, BWV 1004</italic></p>
<p><italic>Bach JS. Sonata no. 3 in C, BWV 1005</italic></p>
<p><italic>Bach JS. Partita no. 3 in E, BWV 1006</italic>.</p>
</disp-quote>
</p>
</app>
<app id="app6">
<title><underline>Guitar</underline></title>
<p>Segovia, AndrEugen&#x00E9;s. Art of Segovia</p>
<p>Deutsche Grammophon, 289 471 697-2 (2002)
<disp-quote>
<p><italic>Handel, George Frideric. Suite for harpsichord no. 4 in D minor, HWV 43 7</italic>.</p>
<p><italic>Sarabande</italic></p>
<p><italic>Bach JS. Suite for violoncello solo no. 1 in G major, BWV 1007. Pr&#x00E9;lude</italic></p>
<p><italic>Bach JS. Partita for violin solo no. 1 in B minor, BWV 1002. Tempo de bourr&#x00E9;e</italic></p>
<p><italic>Bach JS. Suite for violoncello solo no. 3 in C major, BWV 1009. Courante</italic></p>
<p><italic>Bach JS. Partita for violin solo no. 3 E major, BWV 1006.Gavotte en rondeau</italic></p>
<p><italic>Aria e corrente</italic></p>
<p><italic>Frescobaldi, Girolamo. Sonata in C minor, K. 11 (L. 352)</italic></p>
<p><italic>Scarlatti, Domenico. Nouvelles suites de pi&#x00E8;ces de clavecin</italic>.</p>
<p><italic>Rameau, Jean-Philippe. Menuet in G major</italic>.</p>
<p><italic>Manuel Ponce after Nicol&#x00F2; Paganini. Andantino variato</italic></p>
<p><italic>Chopin, Fr&#x00E9;d&#x00E9;ric. 24 pr&#x00E9;ludes, op. 28. No. 7 in A major</italic></p>
<p><italic>Mendelssohn, Felix. String quartet in E flat major, op. 12. Canzonetta</italic></p>
<p><italic>Franck, C&#x00E9;sar. L&#x2019;Organiste, FWV 41, Sept pi&#x00E8;ces en mi b&#x00E9;mol majeur et mi</italic></p>
<p><italic>b&#x00E9;mol mineur. Quasi lento, Andantino poco allegretto</italic></p>
<p><italic>Mussorgsky, Modest. Pictures at an exhibition. Il vecchio castello</italic></p>
<p><italic>Grieg, Edvard. Lyric pieces IV, op. 47. Melodie</italic></p>
<p><italic>Debussy, Claude. Pr&#x00E9;ludes, livre 1. La fille aux cheveux de lin</italic></p>
<p><italic>Scriabin, Alexander. 5 preludes, op. 16. No. 4 in E flat minor</italic></p>
<p><italic>Alb&#x00E9;niz, Isaac. Suite espa&#x00F1;ola. Asturias. Leyenda-Preludio</italic></p>
<p><italic>Alb&#x00E9;niz, Isaac. Piezas caracter&#x00ED;sticas. Zambra granadina</italic></p>
<p><italic>Segovia, Andr&#x00E9;s. Estudio sin luz</italic></p>
<p><italic>Rodrigo, Joaqu&#x00ED;n. Fantas&#x00ED;a para un gentilhombre for guitar and small orchestra</italic>.</p>
<p><italic>Danza de las hachas</italic></p>
<p><italic>Jord&#x00E0;, Enrique. Symphony of the Air</italic></p>
</disp-quote>
</p>
<p>Bream, Julian. Baroque Guitar</p>
<p>RCA Victor Gold Seal, RCA 60494 (1991)
<disp-quote>
<p><italic>Sanz, Gaspar. Pavanas</italic></p>
<p><italic>Sanz, Gaspar. Galliardas</italic></p>
<p><italic>Sanz, Gaspar. Passacalles</italic></p>
<p><italic>Sanz, Gaspar. Canarios</italic></p>
<p><italic>Guerau, Francisco. Villano</italic></p>
<p><italic>Guerau, Francisco. Canario</italic></p>
<p><italic>Bach, JS. Prelude in D minor, BWV 999</italic></p>
<p><italic>Bach, JS. Fugue in A minor, BWV 1000</italic></p>
<p><italic>Weiss, Sylvius Leopold. Passacaille</italic></p>
<p><italic>Weiss, Sylvius Leopold. Fantasie</italic></p>
<p><italic>Weiss, Sylvius Leopold. Tombeau sur la mort de M. Comte de Logy</italic></p>
<p><italic>de Vis&#x00E9;e, Robert. Suite in D minor</italic></p>
<p><italic>Frescobaldi, Girolamo. Aria con variazione detta la Frescobalda arr. Segovia</italic></p>
<p><italic>Scarlatti, Domenico. Sonata in E minor, K. 11</italic></p>
<p><italic>Scarlatti, Domenico. Sonata in E minor, K. 87, arr. Bream [K. 11], Segovia [K. 87]</italic></p>
<p><italic>Cimarosa, Domenico. Sonata in C&#x0023; minor</italic></p>
<p><italic>Cimarosa, Domenico. Sonata in A, arr. Bream</italic>.</p>
</disp-quote>
</p>
</app>
<app id="app7">
<title><underline>Symphonies</underline></title>
<p>The Beethoven Symphonies 1-9 Live From the Edinburgh Festival</p>
<p>Mackeras, Sir Charles, conductor.</p>
<p>Hyperion UK, CDS44301/5 (2007)
<disp-quote>
<p><italic>Beethoven L. Symphony &#x0023;1</italic></p>
<p><italic>Beethoven L. Symphony &#x0023;2</italic></p>
<p><italic>Beethoven L. Symphony &#x0023;3</italic></p>
<p><italic>Beethoven L. Symphony &#x0023;4</italic></p>
<p><italic>Beethoven L. Symphony &#x0023;5</italic></p>
<p><italic>Beethoven L. Symphony &#x0023;6</italic></p>
<p><italic>Beethoven L. Symphony &#x0023;7</italic></p>
<p><italic>Beethoven L. Symphony &#x0023;8</italic></p>
<p><italic>Beethoven L. Symphony &#x0023;9</italic></p>
</disp-quote>
</p>
<p>Mozart: Symphonies Nos. 39 &#x0026; 40</p>
<p>Jacobs, Ren&#x00E9;, conductor. Freiburger Barockorchester.</p>
<p>Harmonia Mundi, HMC901959 (2010)
<disp-quote>
<p><italic>Symphony 39 in E-flat major K 543</italic></p>
<p><italic>Symphony 40 in G minor K 550</italic></p>
</disp-quote>
</p>
<p>Mozart: Symphony No. 41; La Clemenza di Tito Overture</p>
<p>Br&#x00FC;ggen, Frans, conductor. Orchestra of the Eighteenth Century.</p>
<p>Philips Digital Classics, Phillips 420 241-1 (2010)
<disp-quote>
<p><italic>Mozart WA, Symphony 41</italic></p>
<p><italic>Mozart WA, La Clemenza di Tito Overture</italic></p>
</disp-quote>
</p>
<p>Bach: Brandenburg Concertos</p>
<p>Alessandrini, Rinaldo, conductor. Concerto Italiano.</p>
<p>Na&#x00EF;ve, OP 30412 (2005)
<disp-quote>
<p><italic>Bach JS. Brandenburg Concerto No. 1 in F major, BWV 1046</italic></p>
<p><italic>Bach JS. Brandenburg Concerto No. 2 in F major, BWV 1047</italic></p>
<p><italic>Bach JS. Brandenburg Concerto No. 3 in G major, BWV 1048</italic></p>
<p><italic>Bach JS. Brandenburg Concerto No. 4 in G major, BWV 1049</italic></p>
<p><italic>Bach JS. Brandenburg Concerto No. 5 in D major, BWV 1050</italic></p>
<p><italic>Bach JS. Brandenburg Concerto No. 6 in B flat major, BWV 1051</italic></p>
</disp-quote>
</p>
</app>
<app id="app8">
<title><underline>Jazz</underline></title>
<p>Davis, Miles. Kind of Blue (or. 1959)</p>
<p>Columbia/Legacy, CN 90887 (2004)
<disp-quote>
<p><italic>So what</italic></p>
<p><italic>Freddie Freeloader</italic></p>
<p><italic>Blue in green</italic></p>
<p><italic>All blues</italic></p>
<p><italic>Flamenco sketches</italic></p>
<p><italic>Flamenco sketches (alternate take)</italic></p>
</disp-quote>
</p>
<p>Dave Brubeck Quartet. Time Out (or. 1959)</p>
<p>Columbia/Legacy, CN 88697 (2009)
<disp-quote>
<p><italic>Blue rondo &#x00E0;la Turk</italic></p>
<p><italic>Strange meadow lark</italic></p>
<p><italic>Three to get ready</italic></p>
<p><italic>Kathy&#x2019;s waltz</italic></p>
<p><italic>Everybody&#x2019;s jumpin&#x2032;</italic></p>
<p><italic>Pick up sticks</italic></p>
<p><italic>The Dave Brubeck Quartet live at Newport. St. Louis Blues</italic></p>
<p><italic>Waltz limp</italic></p>
<p><italic>Since love had its way</italic></p>
<p><italic>Koto song</italic></p>
<p><italic>Pennies from heaven</italic></p>
<p><italic>You go to my head</italic></p>
<p><italic>Blue rondo &#x00E0;la Turk</italic></p>
<p><italic>Take five</italic></p>
</disp-quote>
</p>
</app>
<app id="app9">
<title><underline>Rock</underline></title>
<p>The Beatles. The Beatles 1967-1970</p>
<p>Apple Records, CDP 0777 7 97039 2 0 (1993)
<disp-quote>
<p><italic>Strawberry Fields Forever</italic></p>
<p><italic>Penny Lane</italic></p>
<p><italic>Sgt. Pepper&#x2019;s Lonely Hearts Club Band</italic></p>
<p><italic>Here Comes the Sun</italic></p>
<p><italic>Come Together</italic></p>
<p><italic>Something</italic></p>
<p><italic>Octopus&#x2019;s Garden</italic></p>
<p><italic>Let It Be</italic></p>
<p><italic>Across the Universe</italic></p>
<p><italic>The Long and Winding Road</italic></p>
</disp-quote>
</p>
<p>The Beatles. The Beatles 1962-1966</p>
<p>Apple Records, CDP 0777 7 97036 2 3 (1993)
<disp-quote>
<p><italic>Love Me Do</italic></p>
<p><italic>Please Please Me</italic></p>
<p><italic>From Me To You</italic></p>
<p><italic>She Loves You</italic></p>
<p><italic>I Want To Hold Your Hand</italic></p>
<p><italic>All My Loving</italic></p>
<p><italic>Can&#x2019;t Buy Me Love</italic></p>
<p><italic>A Hard Day&#x2019;s Night</italic></p>
<p><italic>And I Love Her</italic></p>
<p><italic>Eight Days A Week</italic></p>
<p><italic>I Feel Fine Ticket To Ride</italic></p>
<p><italic>Yesterday</italic></p>
<p><italic>Help&#x0021;</italic></p>
<p><italic>You&#x2019;ve Got To Hide Your Love Away</italic></p>
<p><italic>We Can Work It Out</italic></p>
<p><italic>Day Tripper</italic></p>
<p><italic>Drive My Car</italic></p>
<p><italic>Norwegian Wood (This Bird Has Flown)</italic></p>
<p><italic>Nowhere Man</italic></p>
<p><italic>Michelle</italic></p>
<p><italic>In My Life</italic></p>
<p><italic>Girl</italic></p>
<p><italic>Paperback Writer</italic></p>
<p><italic>Eleanor Rigby</italic></p>
<p><italic>Yellow Submarine</italic></p>
</disp-quote>
</p>
<p>U2. The Best of 1980-1990</p>
<p>Island, ADV7963-2 (1998)
<disp-quote>
<p><italic>Pride (In The Name Of Love)</italic></p>
<p><italic>New Year&#x2019;s Day</italic></p>
<p><italic>With Or Without You</italic></p>
<p><italic>I Still Haven&#x2019;t Found What I&#x2019;m Looking For</italic></p>
<p><italic>Sunday Bloody Sunday</italic></p>
<p><italic>Bad</italic></p>
<p><italic>Where The Streets Have No Name</italic></p>
<p><italic>I Will Follow</italic></p>
<p><italic>Unforgettable Fire</italic></p>
<p><italic>Sweetest Thing</italic></p>
<p><italic>Desire</italic></p>
<p><italic>When Love Comes To Town</italic></p>
<p><italic>Angel Of Harlem</italic></p>
<p><italic>All I Want Is You</italic></p>
</disp-quote>
</p>
</app>
<app id="app10">
<title><underline>30 Rock Songs for the Tempo Analysis</underline></title>
<p>CD: Deeds, Not Words</p>
<p>Player: Max Roach</p>
<p>CD Info: Riverside Label, B000000YH2, released in 1958</p>
<p>Songs: Deeds, Not Words</p>
<p>CD: Attack and Release</p>
<p>Player: The Black Keys (Dan Auerbach and Patrick Carney)</p>
<p>CD Info: Danger Mouse (producer), Nonesuch (label), B0014QABX0, released in 2008</p>
<p>Songs: All You Ever Wanted</p>
<p>CD: Rudeboy</p>
<p>Player: Zeds Dead</p>
<p>CD Info: San City High Records, Digital Download B004F7A71G, released in 2010</p>
<p>Songs: Rude Boy</p>
<p>CD: John the Conqueror</p>
<p>Player: John the Conqueror (written by Pierre Moore)</p>
<p>CD Info: Alive Natural Sound Records, B009369YMM, released in 2012</p>
<p>Songs: All Alone</p>
<p>CD: Meters</p>
<p>Player: The Meters (Art Neville, Ziggy Modeliste, Leo Nocentelli, George Porter Jr.)</p>
<p>CD Info: Sundazed Music, Inc, B0000365IM, released in 1969</p>
<p>Songs: Ease Back</p>
<p>CD: Electriclarryland</p>
<p>Player: The Butthole Surfers ( Gibby Haynes and Paul Leary)</p>
<p>CD Info: Capitol (Label), B000002TS3, produced by Paul Leary, Steve Thompson, released in 1996</p>
<p>Songs: Pepper</p>
<p>CD: Every Hour Is A Dollar Gone</p>
<p>Player: Patrick Sweany</p>
<p>CD Info: Nine Mile Records, B000QUTS0M, released in 2007</p>
<p>Songs: Them Shoes</p>
<p>CD: Conjure</p>
<p>Player: Voodelic (Earl Lundy)</p>
<p>CD Info: Topisaw Dawg (label), B002PLEFZS, Perry Trest (producer), released in 2009</p>
<p>Songs: Universal Screw</p>
<p>CD: The Great Escape of Leslie Magnafuzz</p>
<p>Player: Radio Moscow (Parker Griggs)</p>
<p>CD Info: Alive Naturalsound (label), B005IY3DH0, Park Griggs (producer), released in 2011</p>
<p>Songs: Creepin&#x2019;</p>
<p>CD: By A Thread</p>
<p>Player: Gov&#x2019;t Mule (Warren Haynes)</p>
<p>CD Info: Evil Teen Records (Label), B002MBAJ4M, produced by Gordie Johnson and Warren Haynes, released in 2009</p>
<p>Songs: Steppin&#x2019; Lightly</p>
<p>CD: Burnt Offering</p>
<p>Player: The Budos Band</p>
<p>CD Info: Daptone Records, B00N1CHKLQ, released in 2014</p>
<p>Songs: Aphasia</p>
<p>CD: The Glorious Dead</p>
<p>Player: The Heavy (Chris Ellul, Spencer Page, Kelvin Swaby, Daniel Taylor)</p>
<p>CD Info: Counter (Label), B0088MJO9A, Produced by Paul Corkett, released in 2012</p>
<p>Songs: The Big Bad Wolf</p>
<p>CD: Come On In</p>
<p>Player: R.L Burnside</p>
<p>CD Info: Fat Possum Records, B000008UMZ, released in 1998</p>
<p>Songs: Let My Baby Ride</p>
<p>CD: Funkentelechy Vs. The Placebo Syndrome</p>
<p>Player: Parliament (George Clinton)</p>
<p>CD Info: Casablanca (Label), B000001FCK, released in 1977, 1990 (reissue)</p>
<p>Songs: Bop Gun (Endangered Species) and Flashlight</p>
<p>CD: Bacano</p>
<p>Player: Orgone</p>
<p>CD Info: Killion Floor Sound (Label), B001NW4JJW, released in 2008</p>
<p>Songs: Be In Here</p>
<p>CD: Tres Hombres</p>
<p>Player: ZZ Top (Billy Gibbons, Frank Beard, Dusty Hill)</p>
<p>CD Info: Warner Bros. (Label), B000CCD0HQ, released in 1973, CD in 2006</p>
<p>Songs: Sheik</p>
<p>CD: To A New Earth EP</p>
<p>Player: Kill Paris (Corey Baker)</p>
<p>CD Info: OWLSA, LLC (label), B00U44Z0EE, released in 2013</p>
<p>Songs: Slap Me</p>
<p>CD: The Uplift Mofo Party Plan</p>
<p>Player: Red Hot Chili Peppers (song original written by Bob Dylan)</p>
<p>CD Info: EMI (Label), B000002UD8, released in 1988</p>
<p>Songs: Subterranean Homesick Blues</p>
<p>CD: Ya-Ka-May</p>
<p>Player: Galactic</p>
<p>CD Info: Anti (Label), B0030OJPA4, released in 2010</p>
<p>Songs: Cineramascope</p>
<p>CD: Pendulum</p>
<p>Player: Creedence Clearwater Revival (John Fogerty)</p>
<p>CD Info: Fantasy (label), B001AKTZOG, released in 1970, reissue 2008</p>
<p>Songs: Have You Ever Seen the Rain?</p>
<p>CD: Electric Slave</p>
<p>Player: Black Joe Lewis</p>
<p>CD Info: Vagrant (Label), B00DOQK1TM, released in 2013 Songs: Come To My Party</p>
<p>CD: Lonerism</p>
<p>Player: Tame Impala (Kevin Parker)</p>
<p>CD Info: Interscope (Label), B008JFC6F0, released in 2012</p>
<p>Songs: Elephant</p>
<p>CD: Carnival Electricos</p>
<p>Player: Galactic</p>
<p>CD Info: ANTI Records (label), B006GK2WRM, released in 2012</p>
<p>Songs: Hey Na Na</p>
<p>CD: The London Souls</p>
<p>Player: The London Souls (Tash Neal, Chris St. Hilaire, Kiyoshi Matsuyama)</p>
<p>CD Info: Soul on 10 Records (label), B0053TWVUA, released in 2011</p>
<p>Songs: I Think I Like It</p>
<p>CD: The Best of Blind Blake</p>
<p>Player: Blind Blake</p>
<p>CD Info: Yazoo (Label), B00004Y9XE, released in 2000</p>
<p>Songs: Dry Bone Shuffle</p>
<p>CD: The White Album (Disc 2)</p>
<p>Player: The Beatles (Paul McCartney, John Lennon, George Harrison, Ringo Starr)</p>
<p>CD Info: Capitol (Label), B0025KVLU6, released in 1968, reissue 2009</p>
<p>Songs: Birthday</p>
<p>CD: The Chess Box: Howlin Wolf (Disc 2)</p>
<p>Player: Howlin&#x2019; Wolf</p>
<p>CD Info: Geffen (Label), B0011U39BK, released in 1991</p>
<p>Songs: Smokestack Lightnin&#x2019;</p>
<p>CD: 100 Vocal &#x0026; Jazz Classics &#x2013; Vol. 16 (1945-1947)</p>
<p>Player: Dizzy Gillespie All Star Quintet</p>
<p>CD Info: Stardust Records (Label), B005MW5IUI, released in 2011</p>
<p>Songs: Salt Peanuts</p>
<p>CD: El Camino</p>
<p>Player: The Black Keys (Dan Auerbach and Patrick Carney)</p>
<p>CD Info: Nonesuch (label), B005URRCUY, released in 2011</p>
<p>Songs: Run Right Back</p>
<p>CD: Guns of Gold</p>
<p>Player: The Fumes</p>
<p>CD Info: Silent Partner (Label), B000GAL2P6, released in 2006</p>
<p>Songs: Grocery Store</p>
<p>CD: For the Whole World to See</p>
<p>Player: Death</p>
<p>CD Info: Drag City (label), B001NY71F4, released in 2009</p>
<p>Songs: Rock-N-Roll Victim</p>
</app>
</app-group>
</back>
</article>