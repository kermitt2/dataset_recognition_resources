<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/327288</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Self-organising coordinate transformation with peaked and monotonic gain modulation in the primate dorsal visual pathway</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0401-009X</contrib-id>
<name><surname>Navarro</surname><given-names>Daniel M.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Mender</surname><given-names>Bedeho M. W.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Smithson</surname><given-names>Hannah E.</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Stringer</surname><given-names>Simon M.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Oxford Centre for Theoretical Neuroscience and Artificial Intelligence, Department of Experimental Psychology, University of Oxford</institution>, South Parks Road, Oxford, Oxfordshire, <country>United Kingdom</country></aff>
<aff id="a2"><label>2</label><institution>Oxford Perception Lab, Department of Experimental Psychology, University of Oxford</institution>, South Parks Road, Oxford, Oxfordshire, <country>United Kingdom</country></aff>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="other"><label>&#x002A;</label><p><email>daniel.navarro@psy.ox.ac.uk</email></p></fn>
</author-notes>
<pub-date pub-type="epub"><year>2018</year></pub-date>
<elocation-id>327288</elocation-id>
<history>
<date date-type="received"><day>21</day><month>5</month><year>2018</year></date>
<date date-type="rev-recd"><day>21</day><month>5</month><year>2018</year></date>
<date date-type="accepted"><day>21</day><month>5</month><year>2018</year></date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="327288.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>We study a self-organising neural network model of how visual representations in the primate dorsal visual pathway are transformed from an eye-centred to head-centred frame of reference. The model has previously been shown to robustly develop head-centred output neurons with a standard trace learning rule [<xref rid="c1" ref-type="bibr">1</xref>], but only under limited conditions. Specifically it fails when incorporating visual input neurons with <italic>monotonic</italic> gain modulation by eye-position. Since eye-centred neurons with monotonic gain modulation are so common in the dorsal visual pathway, it is an important challenge to show how efferent synaptic connections from these neurons may self-organise to produce head-centred responses in a subpopulation of postsynaptic neurons. We show for the first time how a variety of modified, yet still biologically plausible, versions of the standard trace learning rule enable the model to perform a coordinate transformation from eye-centred to head-centred reference frames when the visual input neurons have monotonic gain modulation by eye-position.</p>
</abstract>
<counts>
<page-count count="65"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Author summary</title>
<p>Coordinate transformations are an essential aspect of behaviour. For instance, sensory information encoded in the coordinates of the retina needs to be transformed to relevant coordinates for planning and movement. Particularly, head-centred coordinates are essential for accurate motor behaviours and required to compute more complex coordinate transformations for sensorimotor integration [<xref rid="c2" ref-type="bibr">2</xref>]. Head-centred coordinates are obtained by combining information about the the retinal location of visual stimuli and the position of the eye. Previous work did not address the influence of different forms of gain modulation by eye position, albeit a variety of forms being widely reported for several cortical areas. Here we show how a biologically plausible model that successfully self-organised head-centred responses [<xref rid="c1" ref-type="bibr">1</xref>,<xref rid="c3" ref-type="bibr">3</xref>] fails when the visual input units have a commonly observed form of eye-position gain modulation, i.e. <italic>monotonic modulation</italic>. Our work makes an important contribution to understanding how head-centred responses may develop in the brain through an unsupervised process of visually-guided learning using a set of more sophisticated, and yet still biologically plausible, learning rules when visual precursor neurons have monotonic eye-position gain modulation. Furthermore, our findings may be applied to a range of other coordinate transformations with sensorimotor integration of monotonically encoded motor variables [<xref rid="c4" ref-type="bibr">4</xref>,<xref rid="c5" ref-type="bibr">5</xref>].</p>
</sec>
<sec id="s2">
<title>Introduction</title>
<p>Within the primate dorsal visual pathway, neurons encode the locations of visual objects in different body-centred frames of reference. For example, neurons at an early stage of processing encode the locations of objects within an eye-centred reference frame, while neurons in later stages may encode the positions of objects with respect to the head or hand which is more relevant for guiding motor actions. A key question is how the visual system learns to perform such coordinate transformations between different body-centered reference frames. The neural mechanisms supporting this form of transformation, critical for visually guided motor function, have long been studied in primates. A neural phenomenon which is thought to play a key role in coordinate transformations in the dorsal visual pathway is <italic>gain modulation</italic>. This refers to the modulatory effect that a particular bodily state or posture, like eye or head position, has on the firing rate of some visual neurons responding in a given reference frame. Gain modulation of neuronal firing responses has been discovered in multiple stages of the primate dorsal visual pathway.</p>
<p>Parietal area 7a [<xref rid="c6" ref-type="bibr">6</xref>,<xref rid="c7" ref-type="bibr">7</xref>], LIP [<xref rid="c8" ref-type="bibr">8</xref>] and PO [<xref rid="c9" ref-type="bibr">9</xref>] have visual neurons modulated by the position of the eye in the orbit. This form of gain modulation is thought to be involved in supporting the development of head-centred visual representations, which were later identified in the same areas [<xref ref-type="bibr" rid="c10">10</xref>&#x2013;<xref ref-type="bibr" rid="c12">12</xref>]. The parietal reach region (PRR) has hand-centred visual neurons with eye-position gain modulation [<xref rid="c5" ref-type="bibr">5</xref>], which are thought to be involved in guiding reaching to target locations. In certain regions such as parietal area 7a and LIP, the gain modulation is usually a <italic>monotonic</italic> function of the relevant bodily posture, such as eye or head position. For example, eye-position modulation often takes the form of a linear or saturating function of eye-position, which multiplicatively modulates the underlying Gaussian retinotopic receptive field of a visually responsive neuron [<xref rid="c6" ref-type="bibr">6</xref>]. While in other areas, such as V6A, there can be a higher proportion of retinotopic visual neurons with peaked eye-position gain fields [<xref rid="c13" ref-type="bibr">13</xref>].</p>
<p>Contemporary understanding of the potential role of such gain modulation in coordinate transformation has been informed by the highly influential coordinate transformation model of [<xref rid="c14" ref-type="bibr">14</xref>], reviewed below (section Previous Neural Network Models). The majority of models studying coordinate transformation since that time [<xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c15">15</xref>&#x2013;<xref ref-type="bibr" rid="c17">17</xref>] have been heavily inspired by this early work. These models set up the synaptic weight matrix by some form of supervised, error correction, learning algorithm which cannot be implemented in the cortex [<xref rid="c18" ref-type="bibr">18</xref>]. Such algorithms provide no biologically plausible learning hypothesis, and also produce circuits which violate Dale&#x2019;s law, the widely accepted neuroanatomical fact that a given presynaptic neuron cannot be both excitatory and inhibitory across its efferent synapses [<xref rid="c19" ref-type="bibr">19</xref>].</p>
<p>Our laboratory has developed a biologically plausible neural network model that self-organises its synaptic connectivity during visual experience such that the model learns to transform eye-centred visual representations to head-centred representations [<xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c3">3</xref>]. The model can achieve this using purely associative local synaptic learning rules with no supervisory training signal - i.e. unsupervised learning. The model is able to self-organise its synaptic connectivity by exploiting the natural eye and head movements of primates. However, a limitation of previous studies with this model has been their reliance on incorporating retinotopic visual input neurons with responses that are modulated by only <italic>peaked</italic> eye-position gain fields. This is not biologically realistic because many retinotopic visual neurons in the monkey brain are found to have <italic>monotonic</italic> eye-position gain fields. In the new simulations presented below, we first show that the incorporation of retinotopic visual input neurons with monotonic gain fields leads to a failure of the model to develop head-centred output neurons. We then show how model performance can be substantially improved by employing a range of more sophisticated, yet still biologically plausible, learning rules.</p>
<p>Next we present a review of relevant physiological and behavioural data along with a discussion of some previous computational models in order to provide the context for the new simulation results discussed in this paper.</p>
</sec>
<sec id="s3">
<title>Physiology</title>
<sec id="s3a">
<title>Eye-Position gain modulation of retinotopic visual neurons in parietal cortex</title>
<p>Computer simulations carried out within our laboratory have demonstrated that an important precursor to the emergence of head-centred visual neurons in the parietal cortex is likely to be the presence of retinotopic visual neurons with responses that are gain modulated by the position of the eye in the orbit [<xref rid="c1" ref-type="bibr">1</xref>,<xref rid="c3" ref-type="bibr">3</xref>]. A number of experimental studies have previously confirmed the existence of such retinotopic visual neurons with eye-position gain fields in the monkey brain.</p>
<p>The work by [<xref rid="c6" ref-type="bibr">6</xref>] was the earliest demonstration of the influence of the position of the eye in the orbit on the responses of retinotopic light sensitive neurons in area 7a of the monkey parietal cortex. The authors studied the responses of light sensitive neurons using both an attentive task and an inattentive task. In both tasks, the position of the head was kept fixed while the animal was able to move its eyes. During the attentive task the monkey had a peripheral stimulus flashed in a particular eye-centred location whilst maintaining fixation at some gaze angle. Conversely, during the inattentive task targets were flashed on random locations of the screen whilst the monkey freely oriented its gaze. The authors reported that the position of the eyes in the socket affected the responses of neurons in area 7a of the inferior parietal lobule during both attentive fixation and under the inattentive condition. Although it was also found that the responses of a much larger proportion of neurons were modulated by eye position during the attentive task than the inattentive task. The first task, the attentive task, revealed that 61&#x0025; of neurons had their responses to visual stimuli significantly changed by the eye position. The neuronal responses could be more than three times stronger when the eye position shifted by 20&#x00B0; towards the preferred direction (optimal eye position). The second task, the inattentive task, revealed that a significantly smaller proportion of examined neurons (10&#x0025;) had their responses to visual stimuli significantly changed by the eye position. The precise interaction between the visual signal encoding the retinotopic location of the target and the eye position signal was later characterized by [<xref rid="c7" ref-type="bibr">7</xref>]. These effects were also observed in the lateral intraparietal area (LIP) by [<xref rid="c8" ref-type="bibr">8</xref>]. This later work described such gain modulated responses as a multiplicative interaction between a Gaussian retinotopic receptive field and a <italic>monotonic</italic> (planar) eye position modulation component.</p>
<p>The presence of more <italic>peaked</italic> eye position gain modulation has been observed in the parietal occipital area (PO) by [<xref rid="c9" ref-type="bibr">9</xref>] and [<xref rid="c13" ref-type="bibr">13</xref>]. In the more recent of these two studies, [<xref rid="c13" ref-type="bibr">13</xref>] designed an experimental task to investigate the proportion of retinotopic visual neurons in area V6A of the primate brain that are modulated by eye position with either peaked (non-monotonic) or planar (monotonic) gain fields. During the experimental task the monkey had 9 equally spaced fixation locations organised as a 3 &#x00D7; 3 grid, with the visual stimulus always presented at the fixation point. These authors found that approximately 56&#x0025; of recorded neurons had their responses modulated by the position of the eye. Furthermore, 27&#x0025; of the neurons with responses that were modulated by eye position had planar gain fields, whilst the remaining 73&#x0025; had peaked gain fields. The authors also explored the influence of each type of gain modulation on a traditional neural network model of sensorimotor transformation proposed by [<xref rid="c20" ref-type="bibr">20</xref>]. Their main motivation was to understand the implications of the functional form of gain fields, i.e. peaked (non-monotonic) <italic>vs</italic> planar (monotonic), for sensorimotor transformations in reaching tasks. In particular, they investigated how the functional form of the gain fields affected the performance of the model proposed by [<xref rid="c20" ref-type="bibr">20</xref>] to transform eye-centred visual representations into head-centred representations. The authors found that the incorporation of planar rather than peaked eye-position gain fields led to reduced model performance, with the population of output neurons providing a less accurate representation of the location of the visual stimulus with respect to the head.</p>
</sec>
<sec id="s3b">
<title>Head-centred neural responses</title>
<p>A number of experimental studies have found visual neurons in the monkey brain that encode the locations of visual stimuli with respect to the head, i.e. within a head-centred frame of reference. The model simulations that we present below in section Results seek to explain how such head-centred neurons may develop in the brain.</p>
<p>[<xref rid="c11" ref-type="bibr">11</xref>] was the first experimental study to provide evidence of head-centred visual representations in the preoptic area (PO) of the macaque brain. A head-centred representation of visual space is assumed to be important for visually guided reaching and perceptual stability. In fact, [<xref rid="c21" ref-type="bibr">21</xref>] had previously predicted the existence of head-centred visual neurons in PO due to the presence of retinotpic visual neurons with eye-position gain modulation in areas 7a [<xref rid="c7" ref-type="bibr">7</xref>], LIP [<xref rid="c8" ref-type="bibr">8</xref>] and V3A [<xref rid="c21" ref-type="bibr">21</xref>], which are a key precursor to the development of head-centred visual representations [<xref rid="c1" ref-type="bibr">1</xref>,<xref rid="c3" ref-type="bibr">3</xref>]. In the experimental study of [<xref rid="c11" ref-type="bibr">11</xref>], a head-restrained monkey performed a series of fixations to different locations on a screen whilst a visual target was presented in various other screen locations for each fixation. This allowed the authors to assess how individual neuron&#x2019;s activity depended on either the eye-centred or head-centred location of the visual target. It was reported that the receptive fields of 11&#x0025; of recorded neurons were not completely eye-centred, with six of these neurons presenting clear head-centred receptive fields. The authors concluded that previous claim by [<xref rid="c8" ref-type="bibr">8</xref>] that there was no explicit head-centred neuronal representation of visual space was incorrect. Furthermore, head-centred neurons were suggested to originate from pooling the output of preceding eye-position gain-modulated retinotopic visual neurons.</p>
<p>The work of [<xref rid="c10" ref-type="bibr">10</xref>] was motivated by the behavioural requirement for the brain to be able to integrate visual and auditory signals, such as the sight of moving lips and the corresponding speech sound, within a common body-centred frame of reference. The authors&#x2019; investigation of the reference frames used to encode visual and auditory responses in area LIP revealed the first head-centred visual representations in this area. This contradicted the suggestion made by [<xref rid="c8" ref-type="bibr">8</xref>] that head-centred responses would not exist in area LIP. The proportion of neurons sensitive to visual target locations was 72&#x0025;, whilst 51.4&#x0025; of neurons were sensitive to auditory target locations. Moreover, 5&#x0025; to 43&#x0025; of neurons were simultaneously responsive to both visual and auditory target locations, depending on how responsiveness was defined. For visual neurons, 33&#x0025; had eye-centred responses and 18&#x0025; had head-centred responses. Regarding auditory neurons, 10&#x0025; had eye-centred responses while 23&#x0025; had head-centred responses. Neither the remaining 49&#x0025; of visual neurons nor the remaining 67&#x0025; of auditory neurons could have their responses classified as eye-centred or head-centred. In summary, it was found that both auditory and visual neurons had responses compatible with either eye-centred or head-centred frames of reference, although most neurons had complex responses that could not be classified into either of these categories. This was the first time that head-centred neuronal responses were indentified in area LIP.</p>
</sec>
</sec>
<sec id="s4">
<title>Previous Neural Network Models</title>
<p>[<xref rid="c14" ref-type="bibr">14</xref>] developed an early influemtial model that learned to transform an input representation consisting of the position of the eye in the socket and the retinotopic position of the visual target to an output representation consisting of the position of the visual target with respect to the head. Specifically, two-dimensional representations of the eye-position <bold>e</bold> and of the retinal location of the visual target <bold>r</bold> were used as input for the neural network model. The target output used to guide the error-based update of the synaptic weights during training was the head-centred location <bold>h</bold> of the visual target represented by <bold>r</bold> &#x002B; <bold>e</bold>. The model thus utilised a supervised, multi-layer backpropagation of error network architecture, in which the output layer of the network was provided with an explicit training signal representing the current head-centred location of the visual target. Such a network architecture is not biologically plausible for a couple of reasons. Firstly, it is not clear where such a training signal representing the current head-centred location of the visual target might originate from in the brain. Secondly, a multi-layer backropagation network architecture requires that the afferent weights to the hidden layer be updated using an error signal based on the efferent synaptic weights from these hidden neurons to the output layer scaled by the corresponding errors in the output layer. Such a network architecture in itself is not biologically plausible. Nevertheless, [<xref rid="c14" ref-type="bibr">14</xref>] found that training such a network to transform independent eye position and retinal target position signals to head-centred coordinates led to hidden units in the intermediate layer developing retinocentric receptive fields with planar eye-position gain fields, very similar to those found in the posterior parietal cortex by [<xref rid="c6" ref-type="bibr">6</xref>].</p>
<p>A number of later models that simulate the development of head-centred visual neurons have also relied on some form of error correction learning. For example, the model described by [<xref rid="c15" ref-type="bibr">15</xref>] relied on a form of supervised global error correction learning that utilised an error term which is unlikely to be present in the cortex. The model of sensorimotor transformation by [<xref rid="c16" ref-type="bibr">16</xref>] also used a supervised error correction learning rule to modify the synaptic connectivity within the network. So, although supervised error correction learning does not appear to offer a biologically plausible way of modelling the development of head-centred visual neurons in the brain, it has nevertheless remained a popular approach.</p>
<p>In contrast to the above error correction network models, [<xref rid="c20" ref-type="bibr">20</xref>] developed a model that utilised associative learning. These authors investigated how retinotopic visual representations could be transformed into head-centred representations that are relevant to reaching tasks. It was hypothesised that the observation of our own motor movements during a reaching task would be used to develop a map between the eye-centred visual representation of the target and the head-centred motor representation of the movement required to reach the target. In other words, the input layer of the neural network model represented the visual signals generated by observing the reaching movements, whilst the output layer represented the corresponding motor movements that were performed. As with [<xref rid="c14" ref-type="bibr">14</xref>], the input representation consisted of the position of the eye in the socket and the retinotopic position of the visual target, while the output representation consisted of the head-centred target position. Hebbian learning was then used during training to successfully associate each eye-centred visual input representation of the target to the corresponding head-centred output motor representation. In contrast to [<xref rid="c14" ref-type="bibr">14</xref>], the model developed by [<xref rid="c20" ref-type="bibr">20</xref>] did not use a backpropagation of error network architecture or any other form of error correction. Instead, they used a more biologically plausible associative learning rule to modify the synaptic connectivity within their model. However, the model was still trained in a supervised manner. That is, the network still made use of an explicit training signal in the output layer representing the head-centred location of the visual target in order to guide learning without an adequate explanation of where such a signal might originate from in the brain.</p>
<p>[<xref rid="c22" ref-type="bibr">22</xref>] proposed a hierarchical neural network model of coordinate transformation that used associative learning to set up the synaptic connectivity but did not require a biologically implausible supervisory training signal as used by [<xref rid="c20" ref-type="bibr">20</xref>]. The model developed by [<xref rid="c22" ref-type="bibr">22</xref>] had the following two main processing stages. The first processing stage used signals representing the retinotopic location of the visual target and signals representing the position of the eye to learn head-centred visual representations. The second processing stage used head position signals coupled with the head-centred visual representations developed by the first processing stage to learn body-centred visual representations. Training consisted of continously shifting a visual target presented to the network whilst randomly varying the position of the eyes and the head. Both processing stages used competitive learning with an associative learning rule to self-organise conjunctive representations of its respective inputs, and then used competitive learning with a form of temporally associative learning rule to bind representations that occurred close together in time. Learning in the second processing stage only started after the first processing stage had finished learning head-centred representations. This allowed each processing stage of the model to self-organise either head-centred or body-centred representations. Most importantly, in contrast to the models developed by [<xref rid="c14" ref-type="bibr">14</xref>] and [<xref rid="c20" ref-type="bibr">20</xref>], learning in [<xref rid="c22" ref-type="bibr">22</xref>] did not require the use of a biologically implausible supervisory training signal to guide learning of the coordinate transformations. However, the model of [<xref rid="c22" ref-type="bibr">22</xref>] did not investigate how the functional form of eye position gain modulation, i.e. monotonic <italic>vs</italic> peaked, may affect the development of head-centred visual representations. This is the focus of the current work presented below.</p>
</sec>
<sec id="s5">
<title>Our approach: A Biologically Plausible Unsupervised Self-organising Neural Network Model of Coordinate Transformation</title>
<p>Our laboratory has previously published a biologically plausible neural network model of the visually-guided development of head-centred visual neurons, which relies on associative learning rules and does not require a supervisory training signal to guide learning [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c3">3</xref>]. Instead, our model utlises an unsupervised form of competitive learning that exploits the natural statistics of how primates move their eyes and head as they shift their gaze around their visual environment. Specifically, the model employs a form of <italic>trace learning</italic>, which encourgages neurons in higher network layers to bind together input patterns that tend to occur close together in time. If a primate tends to move its eyes more frequently than adjusting the position of its head, then retinal images that occur close together in time will tend to correspond to different positions of the eyes but the same position of the head. In this case, trace learning will encourage neurons in higher layers to learn to respond to the position of a visual target in the same head-centred location across different retinal positions. Such neurons will have thus learned to represent the position of visual targets within a head-centred reference frame. The inputs to the model are eye-centred visual neurons that represent the locations of visual targets on the retina, but which have responses that are also gain modulated by the position of the eyes in the socket. In this paper, we investigate how the learning process depends on the functional form of this gain modulation by eye-position. Two forms of eye-position gain modulation are explored: monotonic gain modulation, which is dominant in most primate parietal areas (LIP, 7a, PRR), and peaked gain modulation which is primarily found in area <bold>PO</bold>.</p>
<p>In the simulations described below in section Results, the model is found to robustly develop head-centred output neurons with a standard trace learning rule when incorporating visual input neurons with <italic>peaked</italic> eye-position gain modulation [<xref rid="c1" ref-type="bibr">1</xref>], but not with <italic>monotonic</italic> eye-position gain modulation. Moreover, even if the model has its synaptic connectivity perfectly prewired to perform a coordinate transformation from eye-centred input neurons with monotonic gain modulation to head-centred output neurons, subsequently introducing plasticity into the synaptic connections with the standard trace learning rule quickly degrades the synaptic connectivity and eventually leads to elimination of the head-centred output responses. Since eye-centred visual neurons with monotonic eye-position gain modulation are so common in the dorsal visual pathway [<xref rid="c7" ref-type="bibr">7</xref>,<xref rid="c23" ref-type="bibr">23</xref>,<xref rid="c24" ref-type="bibr">24</xref>], it is an important challenge to show how efferent synaptic connections from these neurons may self-organise to produce head-centred visual responses in a subpopulation of postsynaptic receiving neurons. A subsequent analysis of the nature of the failure of the self-organisation of the synaptic connectivities led us to explore the performance of the model with a variety of modified, yet still biologically plausible, more powerful versions of the standard trace learning rule. The choice of the modified versions of the trace learning rule used in this paper was motivated by the superior performance of these learning rules reported by [<xref rid="c25" ref-type="bibr">25</xref>]. Here we show for the first time how these modified learning rules enable the model to learn to perform a coordinate transformation from eye-centred to head-centred reference frames even when the visual input neurons have monotonic gain modulation by eye-position.</p>
</sec>
<sec id="s6">
<title>Materials and methods</title>
<sec id="s6a">
<title>The Self-organisation of the Synaptic Connectivity within the Neural Network Model</title>
<p>The model uses four core components to self-organise head-centred visual representations through a biologically plausible process of visually guided learning. The first component is a population of input neurons that encode both the position of the eyes in the orbit and the retinotopic location of visual targets. Such retinotopic visual neurons with eye-position gain modulation have been identified in multiple primate cortical areas [<xref ref-type="bibr" rid="c7">7</xref>&#x2013;<xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c24">24</xref>]. The second component is a population of output neurons that compete with each other through mutual inhibitory interactions, which is a standard feature of cortical architecture [<xref rid="c18" ref-type="bibr">18</xref>]. The third component is a local synaptic <italic>trace learning</italic> rule to update the feedforward synaptic connections between the input and output neurons. The trace learning rule is a local associative learning rule that incorporates an exponentially decaying temporal trace of past neuronal activity, and it has been widely used in the context of developing invariant visual object recognition [<xref rid="c26" ref-type="bibr">26</xref>,<xref rid="c27" ref-type="bibr">27</xref>]. The trace learning rule has the effect of encouraging individual postsynaptic neurons to learn to respond to subsets of input patterns that tend to occur close together in time. Finally, the fourth component comes under the assumption that visual stimuli are relatively static in a world reference frame during natural self-motion. This assumption is justified by experimental findings in which primates adjust their gaze more often by moving their eyes rather than the head itself [<xref rid="c28" ref-type="bibr">28</xref>]. This behavioural strategy to adjust gaze is preferable because it reduces the frequency of making more energetically costly and slow head movements. In fact, it has been found that during exploration of natural environments with free head, eye and body movements, at any time when there is movement, isolated head and isolated eye movements occurred 13.3&#x0025; and 33.1&#x0025; of the time respectively, whilst a mixture of movements was observed in the remaining time. That is, during natural movement there are periods when the eyes are moving whilst the head remains stationary with respect to the visual environment and visual objects also remain stationary within the environment.</p>
<p>These four model components allow the model to self-organise its synaptic connectivity during visually guided training in the following way. If the eyes move around a scene containing a stationary visual target while the head also remains stationary, then the visual system will receive a sequence of input patterns with the visual target in different retinal locations but the same head-centred location. That is, the visual target remains stationary in the head-centred space, but changes its position in the eye-centred space. The sequence of eye-positions and resulting retinal locations of the visual target are represented by retinotopic visual input neurons with responses that are gain modulated by eye-position. The synaptic trace learning rule is able to bind subsets of input patterns corresponding to a visual target in the same head-centred location, albeit with the visual target situated in different eye-centred locations, onto the same output neurons. This is because input patterns corresponding to a visual target situated in the same head-centred location tend to occur close together in time due to the statistics of natural eye and head movements, in which the eyes tend to saccade about a static visual scene while the head remains stationary. Moreover, the naturally rapid movements of the eyes may expose the visual system to many such input pattern sequences, where each such sequence has the visual target situated in the same head-centered location but different randomised subsets of retinal locations. This process will ensure that all possible input patterns corresponding to the same head-centred location but different retinal locations are eventually brought into temporal proximity with each other as training progresses. Hence, all input patterns corresponding to the same head-centred location but different retinal locations would tend to occur clustered together in time. This process continues with the visual target seen in a different position within the head-centred space every time the position of the head is shifted. That is, the natural head movements that occur between sequences of rapid eye movements would shift the location of the visual target to new head-centred locations. In this manner, the learning process is repeated with the visual target presented in many different head-centred locations. New subsets of output cells would learn to respond to the visual target in each different head-centred location due to the competitive interactions between the output cells. Consequently, the output layer would eventually develop neurons that cover the entire space of head-centred target locations.</p>
</sec>
<sec id="s6b">
<title>The Architecture of the Self-Organising Neural Network Model</title>
<p>The neural network architecture of the model is shown in <xref ref-type="fig" rid="fig1">Fig 1</xref>. The network consists of the following two layers of neurons. The first layer is a population of input neurons that simultaneously encode the eye-position of the agent and the retinal location of the visual target. These visual input neurons are modelled as retinotopic neurons with eye-position gain modulation. The eye and retinal position spaces, representing the range of eye-positions in orbit and retinal target locations, covered [&#x2212;30&#x00B0;, 30&#x00B0;] and [&#x2212;90&#x00B0;, 90&#x00B0;], respectively. Feedforward synaptic connections project from neurons in the input layer to neurons in the second layer.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig 1.</label>
<caption><title>Architecture of the neural network model.</title><p>The competitive output layer on the right receives afferent synaptic connections from neurons in the input layer on the left. A trace learning rule is used to modify the strengths of the feedforward synaptic connections from the input layer to the output layer during learning.</p></caption>
<graphic xlink:href="327288_fig1.tif"/>
</fig>
<p>The second layer is a population of <italic>N</italic> output neurons that compete to represent patterns in the input layer [<xref rid="c18" ref-type="bibr">18</xref>]. Neurons in the second layer, the <italic>output layer</italic>, all receive the same number of afferent connections from neurons in the input layer, that is <italic>&#x03D5;</italic> percent of the input population, but each output neuron receives connections from its own randomly assigned subset of the input neurons. Neither the output layer nor the input layer is topographically organised.</p>
<p>The strengths of the feedforward synaptic connections from the input layer to the output layer are initialised to random weights in the interval [0,1] at the start of each simulation. The synaptic weight vector of each output neuron is then renormalised as is typical in competitive networks [<xref rid="c18" ref-type="bibr">18</xref>].</p>
</sec>
<sec id="s6c">
<title>The Visually-Guided Training of the Network</title>
<p>The neural network is trained on input patterns that simultaneously encode both the position of the eyes in orbit and the retinotopic position of the visual target. The position of the eyes is kept within [&#x2212;24&#x00B0;, 24&#x00B0;], whilst the retinal locations of visual targets are kept within the interval [&#x2212;63&#x00B0;, 63&#x00B0;]. Keeping the position of the eyes and the retinal locations of visual targets within these respective intervals reduces edge effects due to clipping of the input representations.</p>
<p>Similarly, all M evenly spaced head-centred locations chosen for each experiment are confined within [&#x2212;63&#x00B0;, 63&#x00B0;] to ensure that visual targets always remain in view as the eyes move. Each training epoch consists of <italic>M</italic> periods, where each period individually corresponds to one of the <italic>M</italic> chosen head-centred locations. During training, for each period a visual target is fixed in a given head-centred location whilst the eyes perform a series of saccades to <italic>P</italic> different and uniformly sampled eye-positions within [&#x2212;24&#x00B0;, 24&#x00B0;]. The saccades between successive eye-positions are performed at a constant velocity of 400&#x00B0;/<italic>s</italic>. The duration of each fixation is set to 300<italic>ms</italic>.</p>
<p>Thus, the training process consisted of presenting to the network sequences of combined visual and eye-position input signals, which represent the visual target in fixed head-centred locations, whilst the eyes randomly shifted to different positions in the orbit.</p>
</sec>
<sec id="s6d">
<title>Testing the Network</title>
<p>The responses of the output units for all combinations of <italic>T</italic> head-centred visual target locations and <italic>E</italic> eye fixation positions are recorded after training to test the model. In order to test the ability of the trained model to generalise to new input patterns, the model is tested with a set of novel combinations of eye-position and visual target location not encountered during training. Specifically, the following <italic>E</italic> &#x003D; 4 eye-positions are used during testing: &#x2212;18&#x00B0;, &#x2212;6&#x00B0;, 6&#x00B0; and 18&#x00B0;. For each of these eye-positions, the visual target is shifted in increments of 2&#x00B0; every 330ms to the next one of the <italic>T</italic> &#x003D; 80 head-centred target locations within [&#x2212;79&#x00B0;, 79&#x00B0;]. The firing rates of all output neurons are saved at the end of each fixation period to analyse the receptive field properties of the neurons, including the receptive field size, receptive field location and reference frame of response.</p>
</sec>
<sec id="s6e">
<title>The Neuronal and Synaptic Dynamics of the Model</title>
<sec id="s6e1">
<title>Input Layer</title>
<p>The firing rates of neurons in the input layer were modelled by a response function that encodes the retinotopic location of a visual target, where the responses were modulated by the position of the eyes in the orbit. The response function of each input neuron <italic>j</italic> maps the current retinal location <italic>r</italic> of the visual target and the eye-position e onto the neuron&#x2019;s instantaneous firing rate <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="327288_inline1.gif"/></alternatives></inline-formula>. The instantaneous firing rate values are defined within the range [0, 1]. We investigated the performance of the model when the response functions of the input neurons were modulated by two different functional forms of eye positon gain field as described next.</p>
<p>The first response function has a peaked eye-position gain field as shown in <xref ref-type="fig" rid="fig2">Fig 2<bold>A</bold></xref>. This form of eye-position modulation has been reported in cortical area PO by [<xref rid="c9" ref-type="bibr">9</xref>]. The full response function is described by
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="327288_eqn1.gif"/></alternatives>
</disp-formula></p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig 2.</label>
<caption><title>Examples of the two alternative response functions used to compute the firing rates of input neurons.</title><p>For each type of response function, we plot the responses of an individual input neuron as a function of eye-position and retinal location of a visual target. A: Example of a peaked response function, in which the firing rate is modulated by a peaked eye-position gain field as described by <xref ref-type="disp-formula" rid="eqn1">Eq 1</xref>. B: Example of a monotonic response function that is modulated by a sigmoidal eye-position gain field described by <xref ref-type="disp-formula" rid="eqn2">Eq 2</xref>.</p></caption>
<graphic xlink:href="327288_fig2.tif"/>
</fig>
<p>This response function is formed from a product of two components: the first component represents the eye-position signal, whilst the second component encodes the retinotopic position <italic>r</italic> of the visual target. In <xref ref-type="disp-formula" rid="eqn1">Eq 1</xref>, the neuronal response is modulatec by a peaked Gaussian function of eye-position. The parameter <italic>&#x03B2;<sub>j</sub></italic> denotes the preferred eye-position for each input neuron <italic>j</italic>. The width of the corresponding Gaussian eye-position tuning curve is determined by the standard deviation <italic>&#x03C1;</italic>. The preferred retinal location of a target stimulus for each input neuron <italic>j</italic> is denoted by the paramete <italic>&#x03B1;<sub>j</sub></italic>. The width of the corresponding Gaussian retinal tuning curve is determined by the standard deviation <italic>&#x03C3;</italic>. Each input neuron <italic>j</italic> is set to respond maximally to a unique combination of retinal target location (<italic>&#x03B1;<sub>j</sub></italic>) and eye-position (<italic>&#x03B2;<sub>j</sub></italic>). The entire two dimensional space consisting of possible combinations of eye-position and retinal target location is covered by the population of input neurons in integer steps of 1 degree in each dimension. This results in a total of 201 &#x00D7; 61 &#x003D; 12, 261 neurons in the input layer.</p>
<p>The second form of firing rate response function used to model the input neurons incorporates a sigmoid eye-position gain field as shown in <xref ref-type="fig" rid="fig2">Fig 2<bold>B</bold></xref>. This form of eye-position modulation is monotonic in the eye-position dimension, as has been observed in multiple visual areas [<xref rid="c7" ref-type="bibr">7</xref>,<xref rid="c8" ref-type="bibr">8</xref>]. Although the form of the modulation is sigmoidal whilst most empirical work has described it as planar, [<xref rid="c16" ref-type="bibr">16</xref>] showed the data i also compatible with a saturating sigmoidal gain formulation. The full response function is described by
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="327288_eqn2.gif"/></alternatives>
</disp-formula></p>
<p>In <xref ref-type="disp-formula" rid="eqn2">Eq 2</xref>, the visual receptive field is modulated by a sigmoidal function of eye-position. For each input neuron <italic>j</italic>, the parameter <italic>&#x03BA;<sub>j</sub></italic> determines the gain direction and saturation rate of the modulation, where <italic>&#x03BA;<sub>j</sub></italic> is &#x2212;2&#x00D7; the slope of the sigmoid. The inflection point <italic>&#x03B2;<sub>j</sub></italic> determines the eye-position where a firing rate response greater than 0.5 begins. The input neurons all have the same absolute saturation rate, that is &#x007C;<italic>&#x03BA;<sub>j</sub></italic>&#x007C; &#x003D; &#x007C;<italic>&#x03BA;<sub>m</sub></italic>&#x007C; for all <italic>j</italic> and <italic>m</italic>, but one half has a positive gain direction (<italic>&#x03BA;<sub>j</sub></italic> &#x003E; 0) whilst the other half has a negative gain direction (<italic>&#x03BA;<sub>j</sub></italic> &#x003C; 0). Each input neuron <italic>j</italic> is set to respond maximally to a unique combination of retinal target location <italic>&#x03B1;<sub>j</sub></italic>, eye-position <italic>&#x03B2;<sub>j</sub></italic>, and gain direction and saturation rate <italic>&#x03BA;<sub>j</sub></italic>. The population of input neurons evenly covers the entire three dimensional space resulting from such combinations. This results in an input population of 201 &#x00D7; 61 &#x00D7; 2 &#x003D; 24, 522 neurons.</p>
</sec>
<sec id="s6e2">
<title>Output Layer</title>
<p>Three dynamical quantities were defined for each neuron <italic>i</italic> in the competitive output layer: an internal activation <italic>h<sub>i</sub></italic>(<italic>t</italic>), a memory trace value <italic>q<sub>i</sub></italic>(<italic>t</italic>), and an instantaneous firing rate <italic>v<sub>i</sub></italic>(<italic>t</italic>) [<xref rid="c29" ref-type="bibr">29</xref>].</p>
<p>The internal activation is governed by the equation
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="327288_eqn3.gif"/></alternatives>
</disp-formula>
where <italic>w<sub>ij</sub></italic> denotes the strength of the synapse from the <italic>j</italic><sup>th</sup> input neuron to the <italic>i</italic><sup>th</sup> output neuron and <italic>&#x03C4;<sub>h</sub></italic> is a time constant common for all output neurons.</p>
<p>The instantaneous firing rate is given by
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="327288_eqn4.gif"/></alternatives>
</disp-formula>
where <italic>&#x03B8;</italic> and <italic>&#x03C6;</italic> denote the sigmoid threshold and slope, respectively. The level of competition between neurons in the output layer, and thereby the proportion of neurons that remained active, is controlled by the parameter <italic>p</italic><sub>&#x03C0;</sub>. Specifically, the parameter <italic>p</italic><sub>&#x03C0;</sub> is set to the &#x03C0;<sup>th</sup> percentile point of the distribution of neuronal activations within the output population. For example, <italic>p</italic><sub>&#x03C0;</sub> is set to the top tenth percentile activation value when &#x03C0; is set to 90. This way of implementing competition within the competitive output layer has been previously used in competitive neural network models of the primate visual system with trace learning [<xref rid="c30" ref-type="bibr">30</xref>]. In the cortex, lateral inhibition is implemented via inhibitory interneurons [<xref rid="c29" ref-type="bibr">29</xref>]. The trace value <italic>q<sub>i</sub></italic>(<italic>t</italic>) is defined in the following section.</p>
</sec>
<sec id="s6e3">
<title>Modification of Synaptic Weights by Trace Learning</title>
<p>Trace learning rules for synaptic modification encourage postsynaptic (output) neurons to bind together subsets of input patterns that tend to occur close together in time by incorporating a temporal trace of recent neuronal activity. The trace value <italic>q<sub>i</sub></italic>(<italic>t</italic>) for the <italic>i</italic><sup>th</sup> neuron in the output layer is given by
<disp-formula id="eqn5">
<alternatives><graphic xlink:href="327288_eqn5.gif"/></alternatives>
</disp-formula>
where <italic>v<sub>i</sub></italic> is the instantaneous firing rate of the neuron, and <italic>&#x03C4;<sub>q</sub></italic> is a time constant common for all output neurons.</p>
<p>In the first part of the paper, during training the strength of the synapse from the <italic>j</italic><sup>th</sup> input neuron to the <italic>i</italic><sup>th</sup> output neuron is governed by the standard trace learning rule previously implemented by [<xref rid="c1" ref-type="bibr">1</xref>]
<disp-formula id="eqn6">
<alternatives><graphic xlink:href="327288_eqn6.gif"/></alternatives>
</disp-formula>
where <italic>&#x03F1;</italic> is the learning rate, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="327288_inline2.gif"/></alternatives></inline-formula> is the instantaneous firing rate of the <italic>j</italic><sup>th</sup> input neuron and <italic>q<sub>i</sub></italic> is the trace value of the <italic>i</italic><sup>th</sup> output neuron. However, later in this paper we will introduce a number of new, more powerful forms of trace learning, which are in fact needed to produce head-centred output neurons when the input neurons are modulated by a sigmoidal (monotonic) function of eye-position.</p>
<p>Finally, after each weight update during training, the length of the weight vector for each output neuron <italic>i</italic>, that is <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="327288_inline3.gif"/></alternatives></inline-formula> where there are <italic>N<sub>I</sub></italic> input neurons, is renormalised by setting
<disp-formula id="eqn7">
<alternatives><graphic xlink:href="327288_eqn7.gif"/></alternatives>
</disp-formula>
This prevents unbounded growth of the synaptic weights during training [<xref rid="c29" ref-type="bibr">29</xref>]. Experimental evidence for renormalisation of synaptic weights in the brain has been reported by [<xref rid="c31" ref-type="bibr">31</xref>].</p>
</sec>
</sec>
<sec id="s6f">
<title>Simulation of the Differential Model</title>
<p>The Forward-Euler scheme is used to numerically integrate the coupled differential equations given by <xref ref-type="disp-formula" rid="eqn3">Eq 3</xref> and <xref ref-type="disp-formula" rid="eqn5">Eq 5</xref>&#x2013;<xref rid="eqn6" ref-type="disp-formula">6</xref>. The numerical time step &#x0394;<italic>t</italic> is set to one tenth of the neuronal time constant <italic>&#x03C4;<sub>h</sub></italic>. We checked that the simulation results remained similar if the time step was reduced or the number of training epochs increased.</p>
<p>During training and testing, the input patterns encoding the changing eye-position and retinotopic target location are simulated dynamically and sampled at 1kHz. Linear interpolation is used to compute the numerical inputs to the discretised Forward Euler model equations, which require input values at every numerical time step &#x0394;<italic>t</italic> &#x003D; <italic>&#x03C4;<sub>h</sub></italic>/10.</p>
</sec>
<sec id="s6g">
<title>Analysis of Network Performance</title>
<p>In order to analyse whether individual output neurons are predominantly responding in an eye-centred or head-centred frame of reference, we used a method of analysis originally developed by [<xref rid="c1" ref-type="bibr">1</xref>]. This analysis is described next with the mathematical details taken from that earlier paper.</p>
<p>Let <bold>R</bold> be a matrix containing the responses of a given neuron during testing, where <bold>R</bold>[<italic>i</italic>, <italic>j</italic>] denotes the firing rate when the model is fixating in the <italic>i</italic><sup>th</sup> eye-position <italic>e<sub>i</sub></italic> and the visual target is in the <italic>j</italic><sup>th</sup> head-centred location <italic>t<sub>j</sub></italic>, as recorded during the testing protocol described above. The vector (<bold>R</bold>[<italic>i</italic>, 1],&#x2026;, <bold>R</bold>[<italic>i</italic>, <italic>T</italic>]) is referred to as the response vector at the <italic>i</italic><sup>th</sup> eye-position. The number of eye-positions during testing is denoted by <italic>E</italic>, while the number of head-centred locations for visual targets during testing is denoted by <italic>T</italic>. The indexing of eye-positions and head-centred target locations are ordered from left (negative) to right (positive), that is <italic>e</italic><sub>1</sub> &#x2264; &#x2026; &#x2264; <italic>e<sub>E</sub></italic> and <italic>t</italic><sub>1</sub> &#x2264; &#x2026; &#x2264; <italic>t<sub>T</sub></italic>.</p>
<p>To determine which reference frame an output neuron is responding in during testing, two separate metrics are applied that reflected to what degree the neuronal response is compatible with either an eye-centred or head-centred reference frame, and then the values of these two metrics are compared.</p>
<p>The head-centredness metric computed the degree to which the head-centred response vectors of a neuron remained stable across different eye-positions. The head-centredness metric measured the degree of such stability for a given output neuron by averaging correlations between response vectors for different eye-positions, that is
<disp-formula id="eqn8">
<alternatives><graphic xlink:href="327288_eqn8.gif"/></alternatives>
</disp-formula>
where
<disp-formula id="eqn9">
<alternatives><graphic xlink:href="327288_eqn9.gif"/></alternatives>
</disp-formula>
This yielded a metric which is referred to as the <italic>head-centredness</italic> of the output neuron, and it is bounded between &#x2212;1 and 1, where a perfect correlation of 1 indicated a perfectly head-centred response.</p>
<p>A very similar analysis is done to quantify the compatibility of the responses of the output neuron with an eye-centred frame of reference. That is, a visual neuron is judged to respond in an eye-centred frame of reference to the extent that its eye-centred response vectors remain stable across different eye-positions. The eye-centred analysis proceeded as follows. To reiterate, each response vector (<bold>R</bold>[<italic>i</italic>, 1],&#x2026;, <bold>R</bold>[<italic>i</italic>, <italic>T</italic>]) is the result of testing over the same set of head-centred locations, but with the model fixated in a distinct eye-position. Therefore, each response vector also corresponded to a unique range of retinal locations. The <italic>intersection</italic> of these retinal ranges corresponded to different portions of each response vector, and it is these portions that are subject to correlation analysis. Specifically, <italic>f<sub>i</sub></italic> denotes the first vector position in the <italic>i</italic><sup>th</sup> response vector to be included, and the <italic>V</italic> &#x2212; 1 next positions are included as well such that the subvector (<bold>R</bold>[<italic>i</italic>, <italic>f<sub>i</sub></italic>],&#x2026;, <bold>R</bold>[<italic>i</italic>, <italic>f<sub>i</sub></italic> &#x002B; (<italic>V</italic> &#x2212; 1)]) is the vector being used for the correlation analysis. The derivation of <italic>f<italic>i</italic></italic> and <italic>V</italic> are found in the appendix Appendix A. This gave the metric
<disp-formula id="eqn10">
<alternatives><graphic xlink:href="327288_eqn10.gif"/></alternatives>
</disp-formula>
where
<disp-formula id="eqn11">
<alternatives><graphic xlink:href="327288_eqn11.gif"/></alternatives>
</disp-formula>
This is referred to as the <italic>eye-centredness</italic> of the output neuron, and it is bounded between &#x2212;1 and 1, where a perfect correlation of 1 indicated a perfectly eye-centred response. Response vectors which had no response for the extracted ranges are excluded from the correlation, and a neuron without a response within this range of retinal locations at any eye-position is excluded from further analysis.</p>
<p>A neuron is finally classified as head-centred if &#x041F; &#x003E; 0 and &#x041F; &#x003E; &#x03A9;, and classified as eye-centred if &#x03A9; &#x003E; 0 and &#x03A9; &#x003E; &#x041F;. If neither of these conditions is met then the neuron remains unclassified.</p>
</sec>
</sec>
<sec id="s7">
<title>Results</title>
<sec id="s7a">
<title>Self-organisation with peaked and monotonic gain fields</title>
<p>This experiment explores the feasibility of the self-organisation of head-centred receptive fields under the two different forms of eye-position gain modulation. Two models are trained and tested on the same stimuli, where one model has peaked eye-position modulation in the input population as shown in <xref ref-type="fig" rid="fig2">Fig 2<bold>A</bold></xref>, and the other model has sigmoidal modulation as shown in <xref ref-type="fig" rid="fig2">Fig 2<bold>B</bold></xref>. The training lasts for 20 epochs. During each training epoch, a visual target is presented for approximately 5<italic>s</italic> in each of the eight head-centred training locations: &#x2212;63&#x00B0;, &#x2212;45&#x00B0;, &#x2212;27&#x00B0;, &#x2212;9&#x00B0;, 9&#x00B0;, 27&#x00B0;, 45&#x00B0; and 63&#x00B0;. For each period where the visual target is in a fixed head-centred target location, the eye-position is varied continuously through time as the model makes a series of saccades and fixations. During each such period, the model performs 14 saccades interleaved with 15 fixations, where each fixation lasts 300<italic>ms</italic>. Each saccade is at a constant velocity of 400&#x00B0;/<italic>s</italic>, and it is directed to a random eye-position within the range [&#x2212;24&#x00B0;, 24&#x00B0;]. Each training epoch thus lasts for approximately 40<italic>s</italic>, and the entire training of the network is completed after about 800<italic>s</italic> of simulated time. The model is tested as previously described. The parameters for the two model simulations are given in <xref ref-type="table" rid="tbl1">Table 1</xref>.</p>
<p><xref ref-type="fig" rid="fig3">Fig 3</xref> compares the firing rate responses of the output neurons before and after training in the two models with either peaked or sigmoidal gain modulation of the visual input neurons. The responses of an output neuron from the model with peaked eye-position gain modulation before training (<xref ref-type="fig" rid="fig3">Fig 3<bold>A</bold></xref>) exhibits no consistent structure in head-centred space across the different eye-positions. However, after training (<xref ref-type="fig" rid="fig3">Fig 3<bold>B</bold></xref>) there is a clear maximal response to the same head-centred location across all eye-positions. Therefore, the self-organisation process has made the response reference frame of this output neuron strongly head-centred. The responses of an output neuron from the model with sigmoidal eye-position gain modulation before training (<xref ref-type="fig" rid="fig3">Fig 3<bold>C</bold></xref>) also has an erratic and more eye-centred response prior to training due to the randomly assigned synaptic weights. However, unlike the peaked gain modulation model, training has the effect of making the neuron almost perfectly eye-centred. This is clearly seen by the receptive fields shifting in head-centred space in register with the eye-position shifts. Therefore, the self-organisation process has made this output neuron even more compatible with an eye-centred reference frame. The miniature scatter plots show the reference frame values of all neurons in the output layer, where each neuron is plotted as a point corresponding to that neuron&#x2019;s particular combination of head-centredness and eye-centredness. The miniature scatter plots confirm the same general effects across the entire populations of output neurons. That is, subplot <bold>(B)</bold> shows that a large proportion of the output neurons in the model with peaked gain modulation have a high head-centredness and low eye-centredness, and thus respond in a head-centred reference frame. While subplot <bold>(D)</bold> shows that a large proportion of the output neurons in the model with sigmoidal gain modulation have a low head-centredness and high eye-centredness, and thus respond in an eye-centred reference frame.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Simulation parameter of self-organising models with either peaked or sigmoidal eye-position gain modulation.</title></caption>
<graphic xlink:href="327288_tbl1.tif"/>
</table-wrap>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig 3.</label>
<caption><title>Comparison of model performance with peaked and sigmoidal (monotonic) gain modulation of visual input neurons.</title><p>The figure shows the firing rate responses of output neurons before and after training with the standard trace rule (6). Specifically, each subplot shows the firing rate responses of a typical output neuron during testing for four different eye-positions: &#x2212;18&#x00B0;, &#x2212;6&#x00B0;, 6&#x00B0; and 18&#x00B0;. The top row shows output neuron &#x0023;95 from the model with peaked eye-position gain modulation before training <bold>(A)</bold> and after training <bold>(B)</bold>. The bottom row shows output neuron &#x0023;838 from the model with sigmoidal eye-position modulation before training <bold>(C)</bold> and after training <bold>(D)</bold>. In each subplot, each curve corresponds to a fixed eye-position while a visual target is presented across the same range of head-centred locations. It is evident in subplot <bold>(B)</bold> that after training the model with peaked gain modulation of the input neurons, the output neuron responds reasonably consistently when the visual target is presented within the localised interval of head-centred space [0&#x00B0;, 16&#x00B0;] regardless of the eye-position. The neuron is thus responding in a head-centred reference frame. However, in contrast, subplot <bold>(D)</bold> shows that after training the model with sigmoidal (monotonic) gain modulation, the responses of the output neuron in the head-centred visual space are much more dependent on the eye-position. Thus, this neuron is not representing the target position in a head-centred reference frame. The miniature scatter plots show the reference frame values of all neurons in the output layer, where each neuron is plotted as a point corresponding to that neuron&#x2019;s particular combination of head-centredness (ordinate) and eye-centredness (abscissa). The neuron whose firing rate responses have been plotted is shown in the scatter plot by a red mark. The miniature scatter plots confirm the same general effects across the entire populations of output neurons. That is, subplot <bold>(B)</bold> shows that a large proportion of the output neurons are clustered in the top left quadrant of the scatter plot, indicating a high head-centredness (ordinate) and low eye-centredness (abscissa). These output neurons are thus responding in a head-centred frame of reference. While subplot <bold>(D)</bold> shows that a large proportion of the output neurons are clustered in the bottom right quadrant of the scatter plot, indicating a low head-centredness (ordinate) and high eye-centredness (abscissa). Thus, with monotonic gain fields acting on the input neurons, the population of output neurons have overwhelmingly learned to respond in an eye-centred reference frame.</p></caption>
<graphic xlink:href="327288_fig3.tif"/>
</fig>
<p><xref ref-type="fig" rid="fig4">Fig 4</xref> shows the synaptic weight vectors of the same output neurons as those shown in <xref ref-type="fig" rid="fig3">Fig 3</xref>. Before training, there is no structure to the potentiated synapses of the output neurons in terms of the preferences of the presynaptic input neurons (<xref ref-type="fig" rid="fig4">Fig 4<bold>A</bold></xref> and <xref rid="fig4" ref-type="fig">Fig 4<bold>C</bold></xref>), reflecting the random weighting assigned to an untrained network. After training, the synaptic weight vector of the output neuron from the model with peaked eye-position gain modulation shows a clear diagonal structure (<xref ref-type="fig" rid="fig4">Fig 4<bold>B</bold></xref>). This synaptic weight profile is consistent with a learned response to a particular location within the head-centred frame of reference, and is thus consistent with the observed head-centred responses of this neuron during testing (<xref ref-type="fig" rid="fig3">Fig 3<bold>B</bold></xref>). The synaptic weight vector of the neuron from the model with sigmoidal (monotonic) gain modulation exhibites an entirely different pattern of potentiation after training. In this case, the strenghtened synapses have an approximately horizontal structure that is concentrated on input neurons corresponding to a small portion of retinal preference space (<xref ref-type="fig" rid="fig4">Fig 4<bold>D</bold></xref>). As a result, this output neuron has learned an eye-centred response (<xref ref-type="fig" rid="fig3">Fig 3<bold>D</bold></xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig 4.</label>
<caption><title>Strengths of the afferent synapses from the input population to a typical output neuron during testing.</title><p>Results are shown for the model with peaked eye-position gain modulation before training <bold>(A)</bold> and after training <bold>(B)</bold>, and for the model with sigmoidal eye-position modulation before training <bold>(C)</bold> and after training <bold>(D)</bold>. The output neurons correspond to those plotted in <xref ref-type="fig" rid="fig3">Fig 3</xref>. In each plot, the afferent synapses have been arranged topographically by the preference of the input neuron for retinal location <italic>&#x03B1;<sub>i</sub></italic> and eye-position <italic><italic>&#x03B2;<sub>j</sub></italic></italic>. For the model with sigmoidal gain modulation, there are two input neurons for every combination of retinal preference and eye-position preference, but with opposite eye-position gain. Consequently, the input population has been separated by gain direction. The portion of each plot to the left of the white dashed line corresponds to input neurons with positive gain <italic>&#x03BA;<sub>j</sub></italic> &#x003E; 0, while the portion of each plot to the right of the white dashed line corresponds to those input neurons with negative gain <italic>&#x03BA;<sub>j</sub></italic> &#x003C; 0. It can be seen from subplot <bold>(B)</bold> that the output neuron in the trained network with peaked gain modulation has developed a diagonal weight structure, which is consistent with a learned response to a particular location within the head-centred frame of reference. In contrast, subplot <bold>(D)</bold> shows that the output neuron in the trained network with sigmoidal (monotonic) gain modulation has developed a more horizontal weight structure, which is consistent with a learned response to a specific location within the eye-centred reference frame.</p></caption>
<graphic xlink:href="327288_fig4.tif"/>
</fig>
<p><xref ref-type="fig" rid="fig5">Fig 5</xref> shows the reference frame values for all output neurons from both models with either peaked or sigmoidal gain modulation tested before and after training. It is clear that, for the model with peaked eye-position modulation, training has the effect of making the majority of output neurons head-centred, and also with a much larger head-centredness value. Specifically, before training 26&#x0025; of output neurons are head-centred, and after training 69&#x0025; are head-centred. Moreover, among the head-centred neurons, the average head-centredness rose from 0.17 before training to 0.63 after training. For the model with sigmoidal eye-position modulation, training has the effect of keeping the majority of output neurons eye-centred, and indeed increasing their average eye-centredness from 0.88 to 0.96.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Fig 5.</label>
<caption><title>Scatter plot of eye-centredness and head-centredness values of output neurons from simulations with peaked and monotonic gain modulation.</title><p>The scatter plot shows the eye-centredness and head-centredness values of all output neurons from four separate simulations corresponding to the models with peaked and sigmoidal (monotonic) gain modulation tested before and after training with the standard trace rule (<xref ref-type="disp-formula" rid="eqn6">Eq 6</xref>). Each point in the scatter plot corresponds to an output neuron from the given simulation, plotted in terms of its eye-centredness (abscissa) and head-centredness (ordinate). The dashed diagonal line with positive unity slope separates those neurons which are classified as head-centred (above line) from those that are classified as eye-centred (below the line). It is evident that after training most of the output neurons from the network with peaked gain modulation have become head-centred, while nearly all of the output neurons from the network with monotonic gain modulation have remained eye-centred.</p></caption>
<graphic xlink:href="327288_fig5.tif"/>
</fig>
<p>In summary, when the visual input neurons have peaked eye positon gain modulation, training the network has the effect of developing head-centred output neurons. However, when the input neurons have sigmoidal (monotonic) gain modulation the training process makes most output neurons almost perfectly eye-centred.</p>
<p>Since a large proportion of visual neurons in the dorsal visual pathway have responses that are modulated by a monotonic function of eye-position, it is important to understand why monotonic gain fields make it more difficult for trace learning to produce head-centred output neurons. In the next section, we investigate this problem by carrying out a covariance analysis on the input patterns themselves.</p>
</sec>
<sec id="s7b">
<title>Covariance Analysis of the Effects of Gain Modulation</title>
<p>The preceeding model simulations failed to develop head-centred output representations during self-organisation when the input population had sigmoidal eye-position modulation, despite succeeding with peaked eye-position modulation. This raises the question of what the difference is between the two different forms of input encoding from the perspective of competitive learning [<xref rid="c18" ref-type="bibr">18</xref>]. It is well known that standard competitive networks develop weight vectors that reflect the covariance between the activities of input neurons. In particular, there is a tendency for output neurons to learn to respond to subsets of input neurons whose activities are highly correlated. Hence inspecting the covariance between input neurons across all input patterns may reveal what structure the weight vectors should converge towards under standard competitive learning conditions with a Hebbian learning rule.</p>
<p>The covariance between input neurons may be computed in the same way for both input neurons with peaked gain described by <xref ref-type="disp-formula" rid="eqn1">Eq 1</xref> and input neurons with sigmoidal gain described by <xref ref-type="disp-formula" rid="eqn2">Eq 2</xref>. In the case of an input neuron with retinal preference <italic>&#x03B1;</italic> and eye-position preference <italic>&#x03B2;</italic>, the covariance between it and a second input neuron with corresponding preferences <italic>&#x03B1;</italic>&#x002A;, <italic>&#x03B2;</italic>&#x002A; is given by
<disp-formula id="eqn12">
<alternatives><graphic xlink:href="327288_eqn12.gif"/></alternatives>
</disp-formula>
<italic>R</italic><sub><italic>&#x03B1;</italic>,<italic>&#x03B2;</italic></sub>(<italic>r</italic>, <italic>e</italic>) is the response of a neuron with preferences <italic>&#x03B1;</italic>, <italic>&#x03B2;</italic> to a visual target at retinal location <italic>r</italic> and eye-position <italic>e</italic>, as given by either <xref ref-type="disp-formula" rid="eqn1">Eq 1</xref> or <xref ref-type="disp-formula" rid="eqn2">Eq 2</xref>. The term <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="327288_inline4.gif"/></alternatives></inline-formula> is the average response of the same neuron across all possible inputs in the <italic>R</italic> &#x00D7; <italic>E</italic> space, that is
<disp-formula id="eqn13">
<alternatives><graphic xlink:href="327288_eqn13.gif"/></alternatives>
</disp-formula></p>
<p><xref ref-type="fig" rid="fig6">Fig 6<bold>A</bold></xref> shows this covariance map for an input neuron with a peaked gain and preferences <italic>&#x03B1;</italic> &#x003D; <italic>&#x03B2;</italic> &#x003D; 0&#x00B0;, and <xref ref-type="fig" rid="fig6">Fig 6<bold>B</bold></xref> shows the covariance map for an input neuron with sigmoidal gain and preferences <italic>&#x03B1;</italic> &#x003D; <italic>&#x03B2;</italic> &#x003D; 0&#x00B0; and <italic>&#x03BA;</italic> &#x003E; 0.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Fig 6.</label>
<caption><title>Covariance between a given input neuron and the rest of the input neuron population.</title><p>Each plot shows the covariance between a given input neuron and the rest of the input neuron population in the form of a topographic map analogous to the weight vector maps shown above. Subplots <bold>(A)</bold> and <bold>(B)</bold> show results for input neurons with peaked and monotonic gain, respectively. In both cases the input neuron has preferences <italic>&#x03B1;</italic> &#x003D; <italic>&#x03B2;</italic> &#x003D; 0&#x00B0;, and in the monotonic case the input has positive gain (<italic>&#x03BA;</italic> &#x003E; 0).</p></caption>
<graphic xlink:href="327288_fig6.tif"/>
</fig>
<p>The structure of covariance in the peaked modulation case is functionally identical to the response function of the input neuron, namely a two dimensional Gaussian tuning curve. The form of this covariance function is obvious by considering the correlations between the activities of input neurons with peaked gain.</p>
<p>In the sigmoidal (monotonic) modulation case, the situation is more complicated. The strong covariance is localised within the retinal preference dimension, but elongated within the eye-position dimension. This can again be understood by considering the response functions of the input neurons. Firstly, because all input neurons have a sharp, peaked tuning profile in the retinal preference dimension, any two input neurons need to have similar retinal preferences in order to have the possibility of being coactive. This explains the localisation of strong covariance in the retinal preference dimension. Secondly, the elongated form of the covariance function in the eye-position dimension results directly from the sigmoidal gain as follows. In the subpopulation of input neurons with a positive gain direction, similar to the reference input neuron (0&#x00B0;, 0&#x00B0;) itself, it is clear that other neurons with a similar retinal preference and with an eye-position preference to the right (i.e. larger than 0&#x00B0;) cofire more frequently with the reference neuron. This is because a positive gain implies that an input neuron responds to all eye-positions to the left of (i.e. smaller than) the eye-position preference of the neuron. Conversley, a negative gain implies that an input neuron responds to eye-positions to the right of (i.e. greater than) the eye-position preference of the neuron. Hence, in the subpopulation of input neurons with a negative gain direction, it can be seen that neurons with a similar retinal preference and with an eye-position preference to the left of (i.e. smaller than) 0&#x00B0; cofire more frequently with the reference neuron.</p>
<p>The covariance maps shown in <xref ref-type="fig" rid="fig6">Fig 6<bold>A</bold></xref> and <xref ref-type="fig" rid="fig6">Fig 6<bold>B</bold></xref> predict the structure of the weight vectors that we would expect to see develop in a competitive network with a standard Hebbian learning rule trained over all input patterns with either peaked or sigmoidal gain, respectively. These predictions are tested by running simulations with a Hebbian learning rule with both peaked and sigmoidal gain modulated input neurons. The Hebbian learning rule is implemented in the model by replacing the trace value <italic>q<sub>i</sub></italic> in the standard trace learning rule (<xref ref-type="disp-formula" rid="eqn6">Eq 6</xref>) by the current firing rate <italic>v<sub>i</sub></italic> of the postsynaptic neuron. For each simulation, there are 200 training patterns corresponding to random locations in the <italic>E</italic> &#x00D7; <italic>R</italic> space. The activation time constant is reduced to <italic>&#x03C4;<sub>h</sub></italic>&#x003D; 30<italic>ms</italic> to avoid any trace effect during learning. <xref ref-type="fig" rid="fig7">Fig 7<bold>A</bold></xref> and <xref ref-type="fig" rid="fig7">Fig 7<bold>C</bold></xref> show the synaptic weight vectors of two typical output neurons that developed after training with the Hebbian learning rule when the input neurons are modulated by either peaked or monotonic gain, respectively. It is clear that these synaptic weight vectors have a very similar structure to the corresponding covariance maps shown in <xref ref-type="fig" rid="fig6">Fig 6</xref>. Thus, with the Hebbian learning rule, the underlying correlations between the activities of the input neurons with either peaked or sigmoidal gain shape the synaptic weight structure that develops during training. Most importantly, with monotonic gain, the synaptic weights are localised within the retinal preference dimension, but elongated within the eye-position preference dimension. This kind of synaptic weight structure leads to eye-centred output responses.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Fig 7.</label>
<caption><title>Weight vectors of two typical output neurons.</title><p>The top row shows the weight vectors of two typical output neurons that develop when the input neurons have peaked eye-position gain modulation and the network is trained with either the Hebbian learning rule <bold>(A)</bold> or the trace learning rule <bold>(B)</bold>. The bottom row shows the weight vectors of two typical output neurons when the input neurons have monotonic eye-position gain and the network is trained with either the Hebbian learning rule <bold>(C)</bold> or the standard trace learning rule <bold>(D)</bold>.</p></caption>
<graphic xlink:href="327288_fig7.tif"/>
</fig>
<p>Next, comparison simulations are run with the trace learning rule. <xref ref-type="fig" rid="fig7">Fig 7<bold>B</bold></xref> and <xref ref-type="fig" rid="fig7">Fig 7<bold>D</bold></xref> show the synaptic weight vectors of two typical output neurons that developed after training with the trace learning rule when the input neurons are modulated by either peaked or sigmoidal gain, respectively. <xref ref-type="fig" rid="fig7">Fig 7<bold>B</bold></xref> shows a diagonal band of potentiated synaptic weights, which correspond to input neurons representing the same head-centred location but with different combinations of retinal location and eye-position. Thus, with peaked gain, the trace learning rule is able to simply bind together clusters of input neurons along a diagonal line in the (retinotopic preference &#x00D7; eye-position preference) input space corresponding to a particular head-centred location. Output neurons will then respond to particular head-centred locations regardless of eye-position or the retinal location of a visual target. However, the situation is quite different with sigmoidal gain modulated input neurons. <xref ref-type="fig" rid="fig7">Fig 7<bold>D</bold></xref> shows a similar horizontal weight structure to that obtained with the Hebbian learning rule (<xref ref-type="fig" rid="fig7">Fig 7<bold>C</bold></xref>). In particular, in both of these last two cases, the weight vector is very similar to the covariance structure found among the input neurons with sigmoidal eye-position modulation (<xref ref-type="fig" rid="fig6">Fig 6<bold>B</bold></xref>). Thus, with sigmoidal gain, even if a trace learning rule is implemented, the output neurons still learn to represent eye-centred rather than head-centred locations. This is because developing head-centred output responses would require the trace learning rule to do more than simply bind input patterns together. With sigmoidal gain, trace learning must also disrupt and break apart output representations corresponding to clusters of highly correlated input neurons, which are localised in the retinotopic preference dimension but elongated in the eye-position preference dimension. However, in practice the standard trace learning rule given by <xref ref-type="disp-formula" rid="eqn6">Eq (6)</xref> is not strong enough to achieve this. Consequently, with the standard trace learning rule, these elongated clusters of input neurons with correlated activities continue to drive the development of eye-centred output neurons, as observed in the simulations previously reported in this article (section Self-organisation with peaked and monotonic gain fields).</p>
</sec>
<sec id="s7c">
<title>Introducing Plasticity into a Prewired Model</title>
<p>The failure of self-organisation to produce head-centred output neurons in a model with sigmoidal (monotonic) eye-position gain modulation suggests the following two important questions. First, does there actually exist a synaptic weight connectivity matrix that would support a mapping to head-centred output representations, even if in practice the self-organisation process using the standard trace learning rule (6) originally implemented by [<xref rid="c1" ref-type="bibr">1</xref>] fails to converge on this solution&#x003F; Secondly, if it is possible to prewire a network with such a synaptic weight structure, then would the head-centred output representations be abolished by subsequently introducing synaptic plasticity either in the form of the standard trace learning rule (6) or normal Hebbian learning (where the trace value <italic>q<sub>i</sub></italic> is replaced by the current firing rate <italic>v<sub>i</sub></italic>)&#x003F; If so, then this would demonstrate an even deeper problem: the presence of such forms of synaptic plasticity in a network with sigmoidal gain modulation not only fails to drive the development of head-centred output representations, but would also abolish any existing head-centred representations.</p>
<p>To address the above two questions, we construct a manually prewired model that is designed to produce head-centred output neurons with input neurons that are modulated by sigmoidal functions of eye-position. The prewired model is constructed as follows. There are 24522 neurons in the input population, each corresponding to a unique combination of retinal-position preference (<italic>&#x03B1;<sub>i</sub></italic>), eye-position preference (<italic>&#x03B2;<sub>j</sub></italic>) and slope (<italic>&#x03BA;<sub>j</sub></italic>). There are 900 neurons in the output population, each given a head-centred receptive field at one among nine head-centred locations, which are &#x2212;68&#x00B0;, &#x2212;51&#x00B0;, &#x2212;34&#x00B0;, &#x2212;17&#x00B0;, 0&#x00B0;, 17&#x00B0;, 34&#x00B0;, 51&#x00B0; and 68&#x00B0;. Each neuron in the output population is postsynaptically connected to a randomly assigned subpopulation of the input population. There are only two synaptic weight values across all synapses, simply referred to as elevated and depressed. The strength of a synapse is elevated if the presynaptic input neuron responded maximally to a combination of eye-position and retinal location corresponding to a head-centred location that is closest to the head-centred location assigned to the output neuron. Otherwise the synapse is depressed. Therefore, the output neuron receives strong driving input to the extent that a visual target is near its assigned head-centred location. Specifically, the weight of a synapse with a postsynaptic neuron assigned to head-centred receptive field location <italic>h</italic> and a presynaptic neuron having retinal-preference <italic>&#x03B1;</italic>, eye-position preference <italic>&#x03B2;</italic> and <italic>&#x03BA;</italic> &#x003E; 0 is given by
<disp-formula id="eqn14">
<alternatives><graphic xlink:href="327288_eqn14.gif"/></alternatives>
</disp-formula>
where <italic>W</italic> &#x003D; 60&#x00B0; is the size of the eye-position dimension. Likewise when <italic>&#x03BA;</italic> &#x003C; 0 the weight is given by
<disp-formula id="eqn15">
<alternatives><graphic xlink:href="327288_eqn15.gif"/></alternatives>
</disp-formula></p>
<p><xref ref-type="fig" rid="fig8">Fig 8</xref> shows the structure of the canonical weight vector produced by <xref ref-type="disp-formula" rid="eqn14">Eq 14</xref>&#x2013;<xref rid="eqn15" ref-type="disp-formula">15</xref>. Before testing the network, the synaptic weight vectors of all output neurons underwent the normalization step described by <xref ref-type="disp-formula" rid="eqn7">Eq 7</xref>. To provide a baseline for comparison, a network with randomly wired synaptic connections is also tested in the same way. The parameters for both models are given in <xref ref-type="table" rid="tbl2">Table 2</xref>.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Fig 8.</label>
<caption><title>Synaptic weight structure of a network model that has been manually prewired in order to produce head-centred output neurons with input neurons that are modulated by a sigmoidal function of eye-position.</title><p>The figure shows the structure of the canonical weight vector resulting from the prewiring <xref ref-type="disp-formula" rid="eqn14">Eq 14</xref> and <xref ref-type="disp-formula" rid="eqn15">Eq 15</xref>. Each of the two rectangles represents the topographic organisation of one half of the input population in terms of retinal-preference (<italic>&#x03B1;<sub>i</sub></italic>) and eye-position preference (<italic>&#x03B2;<sub>j</sub></italic>), with the input neurons in the left rectangle having <italic>&#x03BA;</italic> &#x003E; 0 (positive gain) and the right rectangle having <italic>&#x03BA;</italic> &#x003C; 0 (negative gain). A neuron in the competitive output population which has been assigned a head-centred receptive field at location <italic>h</italic> will have elevated connections from input neurons with preferences located in the right-angled triangles of the input space, labeled <bold>A</bold> and <bold>B</bold>.</p></caption>
<graphic xlink:href="327288_fig8.tif"/>
</fig>
<p><xref ref-type="fig" rid="fig9">Fig 9</xref> shows the eye-centredness and head-centredness values of output neurons in the manually prewired model as well as a randomly wired model for comparison. It is clear that in the manually prewired model, the majority of output neurons are head-centred. In contrast, there are no head-centred neurons in the randomly wired model. In summary, the results from the manually prewired model demonstrate the existence of a synaptic weight matrix which allows the output neurons to perform the desired coordinate transformation to a head-centred reference frame even when the input neurons are modulated by a sigmoidal function of eye-position.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Simulation parameters of the network model that has been manually prewired in order to produce head-centred output neurons with monotonic modulated input neurons.</title></caption>
<graphic xlink:href="327288_tbl2.tif"/>
</table-wrap>
<fig id="fig9" position="float" fig-type="figure">
<label>Fig 9.</label>
<caption><title>Performance of the prewired network model with monotonic modulated input neurons.</title><p>The Figure shows the performance of the network model that has been manually prewired to produce head-centred output neurons with input neurons that are modulated by a sigmoidal function of eye-position. The scatter plot shows the eye-centredness and head-centredness values of all output neurons from the manually prewired model and a randomly wired model. Same conventions as in <xref ref-type="fig" rid="fig5">Fig 5</xref>. It can be seen that the majority of the output neurons in the manually prewired model display head-centred responses.</p></caption>
<graphic xlink:href="327288_fig9.tif"/>
</fig>
<p>The next question to be explored is, what would be the effect of introducing synaptic plasticity, either in the form of the standard trace learning rule (6) or normal Hebbian learning, into the manually prewired model while it is exposed to the kinds of visual input described above&#x003F; This experiment would inform whether these forms of plasticity not only failed to drive the development of head-centred output representations during self-organisation, but would even abolish existing head-centred representations. The manually prewired model is therefore subjected to the same visual training stimuli as described above over 10 training epochs. The results presented here are from a simulation using the standard trace learning rule (6). However, although not
shown, further simulations with a Hebbian learning rule without an explicit memory trace gave qualitatively similar results.</p>
<p>The impact of introducing synaptic plasticity into the manually prewired modelsas is inspected by plotting key summary statistics as a function of the number of training epochs in <xref ref-type="fig" rid="fig10">Fig 10</xref>. There is a catastrophic drop in model performance after only the first epoch of training, where the fraction of head-centred neurons decreased from &#x007E;77&#x0025; to &#x007E;0.3&#x0025;, and the average head-centredness among head-centred neurons decreased from &#x007E;0.7 to &#x007E;0.26. Subsequent training epochs remained around these levels. After epoch 5 there is not a single head-centred neuron.</p>
<fig id="fig10" position="float" fig-type="figure">
<label>Fig 10.</label>
<caption><title>The effects of introducing synaptic plasticity into the network that has been manually prewired to produce head-centred output neurons when the input neurons that are modulated by a sigmoidal (monotonic) function of eye-position.</title>
<p>The figure shows population analyses of the response properties of output neurons in the manually prewired model as the synaptic weights are further modified during ten training epochs with the standard trace learning rule (6). Three key summary statistics are given. The head-centredness rate (red) is the fraction of head-centred neurons in the output population. The average head-centredness (green) is the average head-centredness among head-centred neurons, and becomes undefined if no head-centred neurons are found to exist. The average eye-centredness (blue) is the average eye-centredness among all output neurons. The dashed lines show these values for the manually prewired network before training, while the unbroken lines show the values through successive training epochs after synaptic plasticity has been introduced. The error bars are the standard deviations. It can be seen that by the end of the first training epoch the majority of the output neurons switched from being head-centred to eye-centred.</p></caption>
<graphic xlink:href="327288_fig10.tif"/>
</fig>
<p>In summary, it is found that just a single training epoch switched most of the output neurons from being head-centred to eye-centred. We hypothesised that this is due to the same visually-guided learning dynamics described above in section Self-organisation with peaked and monotonic gain fields and in section Covariance Analysis of the Effects of Gain Modulation, which come into operation when the retinotopic input neurons have monotonic eye-position gain modulation. Thus, even if the synaptic weights are initially manually prewired to effect head-centred output responses, which might be suggested to happen in the brain through genetic specification, the introduction of just a limited amount of synaptic plasticity, either in the form of the standard trace learning rule (6) or normal Hebbian learning, and visually-guided learning led to the output neurons rapidly switching to eye-centred responses. The presence of even modest levels of such synaptic plasticity will quickly overwrite head-centred representations that have been set up through structured (e.g. genetic) prewiring. Thus, since plasticity is ubiquitous in primate cortex, this suggests that any explanation for the development of head-centred visual responses must utilise a more sophisticated visually-guided learning process than demonstrated by [<xref rid="c1" ref-type="bibr">1</xref>], who considered only models with input neurons that were modulated by peaked functions of eye-position. Moreover, the loss of head-centred representations in the manually prewired model by introducing synaptic plasticity also represents a major challenge to the plausibility of previously published models, such as that of [<xref rid="c20" ref-type="bibr">20</xref>], which rely on an initial period of supervised learning to establish the required synaptic connectivity. The problem here is that when the supervisory training signal is eventually removed, the continued presence of associative plasticity may degrade and eventually abolish the head-centred output representations.</p>
<p>In the remainder of the paper we explore a variety of biologically plausible model variations that are aimed at discovering potential mechanisms by which head-centred output neurons may still develop through visually guided learning even when the network contains input neurons with monotonic modulation by eye-position. We begin by exploring the performance of the model when it incorporates a mixture of input neurons that are modulated by peaked and sigmoidal eye-position gain fields. After this, we explore the operation of the model with a number of more sophisticated, modified synaptic learning rules originally developed by [<xref rid="c25" ref-type="bibr">25</xref>] in the context of transform invariant visual object recognition, which maintain biological plausibility by continuing to rely on the locally available activities of the pre&#x002D; and post-synaptic neurons. The choice of the modified versions of the trace learning rule used in the following sections of this paper is motivated by the superior performance of these learning rules reported by [<xref rid="c25" ref-type="bibr">25</xref>]. Finally, we conclude with the investigation of the performance of the model incorporating a mixed population of peaked and monotonic modulated visual input neurons with the synaptic weights also adjusted using a new modified version trace learning rule.</p>
</sec>
<sec id="s7d">
<title>Standard Trace Learning Rule with Mixed Peaked and Sigmoidal eye-position Modulation of Input Neurons</title>
<p>In this experiment it is investigated how mixing peaked and sigmoidal gain modulation in the input population in varying proportions would influence the development of head-centred output neurons in the self-organising model with the standard trace learning rule (6). This is an important issue since all cortical areas with eye-position gain modulation exhibit a mixture of different forms of modulation [<xref ref-type="bibr" rid="c7">7</xref>&#x2013;<xref ref-type="bibr" rid="c9">9</xref>]. A series of simulations are conducted where each neuron in the input population is independently and randomly set to have either a peaked or sigmoidal gain modulation. Specifically, each input neuron is changed from having peaked to sigmoidal modulation with a probability <italic>p</italic>, called the sigmoid modulation rate, and values of <italic>p</italic> &#x003D; 0,0.1,&#x2026;, 1.0 are explored.</p>
<p>The impact of varying the sigmoid modulation rate on the characteristics of the model is inspected by plotting key summary statistics as a function of <italic>p</italic> in <xref ref-type="fig" rid="fig11">Fig 11</xref>. As expected, the head-centredness rate decreased as the sigmoid modulation rate increased, both in the trained and untrained models. However, as long as the sigmoid modulation rate is less than 30&#x0025;, the trained model had a higher proportion of head-centred output neurons than the untrained model. In particular, for sigmoid modulation rates less than 20&#x0025;, the fraction of head-centred neurons in the trained model did not drop below &#x007E;15&#x0025;, and the average head-centredness among head-centred neurons remained no less than &#x007E;0.58.</p>
<fig id="fig11" position="float" fig-type="figure">
<label>Fig 11.</label>
<caption><title>The effects of incorporating a mixed population of input neurons with both peaked and monotonic eye-position gain modulation.</title><p>The plots show how the performance metrics vary with the monotonic modulation rate, <italic>p</italic>, which is the probability of each input neuron having a monotonic eye-position gain modulation. Results are presented showing the response characteristics of the output neurons before training <bold>(A)</bold> and after training <bold>(B)</bold>. Conventions are similar to <xref ref-type="fig" rid="fig10">Fig 10</xref>. It is evident that the head-centredness rate decreased as the sigmoid modulation rate increased, both in the trained and untrained models. However, as long as the sigmoid modulation rate is less than 30&#x0025;, the trained model had a higher proportion of head-centred output neurons than the untrained model.</p></caption>
<graphic xlink:href="327288_fig11.tif"/>
</fig>
<p>In summary, these results showed that when there is a large proportion of input neurons with peaked eye-position gain modulation, say with 0 &#x2264; <italic>p</italic> &#x2264; 0.2, then the self-organising model is still capable of developing a significant proportion, i.e. no less than &#x007E;15&#x0025;, of head-centred ouput neurons during training. However, as the sigmoid modulation rate increased, the performance of the model deteriorated with far fewer head-centred output neurons present in the trained model.</p>
<p>Given that the model incorporating a mixed population of input neurons failed to develop a significant proportion of head-centred output cells whenever <italic>p</italic> &#x003E; 0.2, we next investigated whether the introduction of more powerful, modified synaptic learning rules could produce head-centred output neurons when the entire population of input neurons were again modulated by a sigmoidal function of eye-position. Our choice of the modified versions of the trace learning rule adopted in this paper was motivated by their superior performance reported in previous work [<xref rid="c25" ref-type="bibr">25</xref>]. These modified versions of the standard trace learning rule are still biologically plausible in terms of only using locally available signals to update the synaptic weights of connections. Moreover, we expected that the introduction of a delayed trace of synaptic activity and as well as an anti-Hebbian component incorporated in these modified versions of the trace learning rule would provide the model with a way of weakening the observed Hebbian-like training behaviour evidenced, for example, by the comparison of <xref ref-type="fig" rid="fig7">Fig 7<bold>D</bold></xref> and <xref rid="fig7" ref-type="fig">Fig 7<bold>C</bold></xref>, and therefore to facilitate the self-organisation of head-centred responses with visual input neurons with monotonic eye-position gain modulation. The absence of these components makes other classic Hebbian-based learning rules (e.g. Oja&#x2019;s rule [<xref rid="c32" ref-type="bibr">32</xref>]) ineffective in this case.</p>
</sec>
<sec id="s7e">
<title>Modified Learning Rule: Delayed Postsynaptic Trace with Anti-Hebbian Learning</title>
<p>[<xref rid="c25" ref-type="bibr">25</xref>] investigated how a set of modified more powerful versions of the trace learning rule can produce improved temporal binding and invariance learning. In particular, the authors showed that the performance of the trace learning rule is substantially improved by incorporating a trace of previous neuronal activity with an explicit time delay. This had the effect of removing the purely Hebbian term of the learning rule [<xref rid="c25" ref-type="bibr">25</xref>]. In the next simulations, the learning rules proposed by [<xref rid="c25" ref-type="bibr">25</xref>] were adapted to differential formulations for the time-continuous scenario in which the simulations are performed. A Forward Euler scheme was used to numerically integrate the differential equations. In all simulations the numerical time step was kept as one tenth of the neuronal time constant <italic>&#x03C4;<sub>h</sub></italic>. Unless explicitly mentioned, the learning rule was the only change from previous simulations.</p>
<p>This section presents simulation results showing the performance of a modified learning rule incorporating a delayed postsynaptic trace with anti-Hebbian learning [<xref rid="c33" ref-type="bibr">33</xref>]. We investigate the impact of this learning rule on the self-organisation of the synaptic weights and firing rate responses of the output neurons when all input neurons were modulated by sigmoidal eye-position gain fields. The learning rule was defined by
<disp-formula id="eqn16">
<alternatives><graphic xlink:href="327288_eqn16.gif"/></alternatives>
</disp-formula>
where <italic>y<sub>i</sub></italic> and <italic>v<sub>j</sub></italic> were, respectively, the post and presynaptic firing rate values, <italic>&#x03B1;</italic> was the learning rate, <italic>&#x03B2;</italic> was an unconstrained tuning parameter, and <italic>q<sub>i</sub></italic> was the trace value of the output neuron <italic>i</italic> calculated according to <xref ref-type="disp-formula" rid="eqn5">Eq 5</xref> at time (<italic>t</italic> &#x2212; &#x0394;<italic>T</italic>)<italic>ms</italic>. <xref ref-type="disp-formula" rid="eqn16">Eq 16</xref> resembles a form of error-correction learning where the delayed-trace term <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="327288_inline5.gif"/></alternatives></inline-formula> is the target for the current postsynaptic firing rate. The learning rule is biologically plausible in that it utilises only the local activities of the pre&#x002D; and post-synaptic neurons.</p>
<p>Expanding <xref ref-type="disp-formula" rid="eqn16">Eq 16</xref> will result in
<disp-formula id="eqn17">
<alternatives><graphic xlink:href="327288_eqn17.gif"/></alternatives>
</disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="327288_inline6.gif"/></alternatives></inline-formula> is the <italic>delayed-trace term</italic> of the learning rule. This is the term which contains the tuning parameter ft. The remaining term &#x2212;<italic>&#x03B1;y<sub>i</sub></italic>(<italic>t</italic>)<italic>v<sub>j</sub></italic>(<italic>t</italic>) is minus the learning rate <italic>&#x03B1;</italic> times the product of the post and presynaptic firing rates <italic>y<sub>i</sub></italic> and <italic>v<sub>j</sub></italic>, respectively. This term is referred to as the <italic>anti-Hebbian term</italic> of the learning rule.</p>
<p>The behaviour of the learning rule shown in <xref ref-type="disp-formula" rid="eqn16">Eq 16</xref> is governed by scaling the parameter <italic>&#x03B2;</italic>. Scaling <italic>&#x03B2;</italic> up could result in the delayed-trace component dominating the learning rule and, therefore, resulting in the same trace-like training behaviour described in previous sections. Similarly, scaling <italic>&#x03B2;</italic> down could make the anti-Hebbian term dominate the behaviour of the learning rule and consequently lead to a qualitative change in the final outcome of the training.</p>
<p>The parameters for the model are given in <xref ref-type="table" rid="tbl1">Table 1</xref>. The model was trained according to the description given in section The Visually-Guided Training of the Network. Likewise, in all cases the model was tested on the same visual stimuli. The time delay &#x0394;<italic>T</italic> used to compute the trace value of each output neuron and the parameter <italic>&#x03B2;</italic> were both tuned to optimise the performance of the model at developing head-centred output neurons. The time delay &#x0394;<italic>T</italic> used to compute the trace value of each output neuron <italic>i</italic> was 50<italic>ms</italic>. The parameter <italic>&#x03B2;</italic> was set to 2.2.</p>
<p><xref ref-type="fig" rid="fig12">Fig 12</xref> shows the firing rate responses of output neuron &#x0023;168 before and after training, with results shown for four different eye-positions. The miniature scatter plots presented within each of the two subplots <bold>A</bold> and <bold>B</bold> show the reference frame values of all neurons in the output layer. Neuron &#x0023;168 is indicated in the scatter plots by a red mark. <xref ref-type="fig" rid="fig12">Fig 12<bold>A</bold></xref> shows that prior to training the response of output neuron &#x0023;168 had no consistent structure in head-centred space across different eye-positions. However, <xref ref-type="fig" rid="fig12">Fig 12<bold>B</bold></xref> shows that after training the output neuron responded maximally to the same head-centred location across all four eye-positions. This neuron has thus learned to respond in a head-centred reference frame.</p>
<fig id="fig12" position="float" fig-type="figure">
<label>Fig 12.</label>
<caption><title>Simulation results showing the firing rate responses of a model incorporating a population of monotonic modulated input neurons trained with the modified learning rule 16: Delayed Postsynaptic Trace with anti-Hebbian Learning.</title><p>The figure shows the firing rate responses of output neuron &#x0023;168 before training <bold>(A)</bold> and after training <bold>(B)</bold> during testing for four different eye-positions: &#x2212;18&#x00B0;, &#x2212;6&#x00B0;, 6&#x00B0; and 18&#x00B0;. In each subplot, each curve corresponds to a fixed eye-position while a visual target is presented across the same range of head-centred locations. The miniature scatter plot shows the reference frame values of all neurons in the output layer, where each neuron is plotted as a point corresponding to that neuron&#x2019;s particular combination of head-centredness (ordinate) and eye-centredness (abscissa). The neuron whose firing rate responses have been plotted is shown in the scatter plot by a red mark. After training it is evident that this neuron responds reasonably invariantly to a visual target presented at the same head-centred location regardless of the eye-position.</p></caption>
<graphic xlink:href="327288_fig12.tif"/>
</fig>
<p>The change in the synaptic weight structure of the same output neuron &#x0023;168 due to training is shown in <xref ref-type="fig" rid="fig13">Fig 13</xref>. Before training started the afferent synaptic weights were randomly assigned (<xref ref-type="fig" rid="fig13">Fig 13<bold>A</bold></xref>). <xref ref-type="fig" rid="fig13">Fig 13<bold>B</bold></xref> shows that after training, in contrast to the horizontal structure previously obtained with the standard trace learning rule shown in <xref ref-type="fig" rid="fig7">Fig 7<bold>D</bold></xref>, the structure obtained with the modified learning rule 16 is similar to the predicted structure shown in <xref ref-type="fig" rid="fig8">Fig 8</xref> for a head-centred neuron.</p>
<fig id="fig13" position="float" fig-type="figure">
<label>Fig 13.</label>
<caption><title>Simulation results showing the strengths of the afferent synapses of a model incorporating a population of sigmoidal modulated input neurons trained with the modified learning rule 16: Delayed Postsynaptic Trace with anti-Hebbian Learning.</title><p>The figure shows the strengths of the afferent synapses from the input population to output neuron &#x0023;168 for the untrained <bold>(A)</bold> and trained <bold>(B)</bold> model. The output neuron corresponds to the one plotted in <xref ref-type="fig" rid="fig12">Fig 12</xref>. In each plot, the afferent synapses have been arranged topographically by the preference of the input neuron for retinal location <italic>&#x03B1;<sub>i</sub></italic> and eye-position <italic>&#x03B2;<sub>j</sub></italic>. The portion of each plot to the left of the white dashed line corresponds to input neurons with positive gain <italic>&#x03BA;<sub>j</sub></italic> &#x003E; 0, whilst the portion of each plot to the right of the white dashed line corresponds to those input neurons with negative gain <italic>&#x03BA;<sub>j</sub></italic> &#x003C; 0. The synaptic weights for this output neuron after training <bold>(B)</bold> have approximately the correct structure for a head-centred neuron as shown in <xref ref-type="fig" rid="fig8">Fig 8</xref>.</p></caption>
<graphic xlink:href="327288_fig13.tif"/>
</fig>
<p><xref ref-type="fig" rid="fig14">Fig 14</xref> presents a population analysis of the reference frame response characteristics of the output neurons before and after training. In particular, the scatter plot in <xref ref-type="fig" rid="fig14">Fig 14</xref> shows that before training nearly all of the output neurons had head-centredness values close to 0 and were classified as eye-centred. However, after training the head-centredness values of many output neurons had dramatically increased, with quite a number of these neurons now classed as head-centred. Comparing the output population analysis of the model trained with the modified learning rule 16 shown in <xref ref-type="fig" rid="fig14">Fig 14</xref> with the performance of the model trained with the standard trace learning rule (6) shown in <xref ref-type="fig" rid="fig5">Fig 5</xref> confirms that the new modified learning rule 16 is far more efficacious at driving the development of head-centred output neurons when the input neurons are modulated by a sigmoidal (monotonic) function of eye-position than the standard trace learning rule (6) originally investigated by [<xref rid="c1" ref-type="bibr">1</xref>].</p>
<fig id="fig14" position="float" fig-type="figure">
<label>Fig 14.</label>
<caption><title>Simulation results showing the output reference frame response characteristics of a model incorporating a population of sigmoidal modulated input neurons trained with the modified learning rule 16: Delayed Postsynaptic Trace with anti-Hebbian Learning.</title><p>The scatter plot shows the reference frame response characteristics of all output neurons before and after training. Each neuron is represented as a point corresponding to its combination of eye-centredness (abscissa) and head-centredness (ordinate) values. Data points for the untrained model are plotted in blue and data points for the trained model are shown in red. The dashed diagonal line with positive unity slope separates those neurons which are classified as head-centred (above the line) from those that are classified as eye-centred (below the line). It can be seen that many of the output neurons have developed head-centred output responses after training.</p></caption>
<graphic xlink:href="327288_fig14.tif"/>
</fig>
<p>In summary, these results showed that training the model with the modified learning rule 16 refined the response characteristics of many output neurons to be more compatible with a head-centred frame of reference, even when all of the input neurons had monotonic eye-position gain modulation.</p>
</sec>
<sec id="s7f">
<title>Modified Learning Rule: Delayed Postsynaptic Firing Rate with Anti-Hebbian Learning</title>
<p>This section presents simulation results showing the performance of a learning rule which incorporates a delayed postsynaptic firing rate with anti-Hebbian learning [<xref rid="c25" ref-type="bibr">25</xref>]. The same training procedure of previous simulations was used in the simulations presented in this section. The learning rule is defined by
<disp-formula id="eqn18">
<alternatives><graphic xlink:href="327288_eqn18.gif"/></alternatives>
</disp-formula>
where <italic>y<sub>i</sub></italic> was the firing rate of output neuron <italic>i</italic>. In this case no postsynaptic trace value <italic>q<sub>i</sub></italic> is used to update the synaptic weights. The tuning parameter <italic>&#x03B2;</italic> works as described in section Modified Learning Rule: Delayed Postsynaptic Trace with Anti-Hebbian Learning for the anti-Hebbian learning rule with delayed trace (<xref ref-type="disp-formula" rid="eqn16">Eq 16</xref>). The time delay &#x0394;<italic>T</italic> in the firing rate of each output neuron and the parameter <italic>&#x03B2;</italic> were both tuned to optimise the performance of the model in driving the development of head-centred output neurons. The value of the time delay &#x0394;<italic>T</italic> was set to 500<italic>ms</italic> and <italic>&#x03B2;</italic> was set to 2.4. Simulation parameters for the model are shown in <xref ref-type="table" rid="tbl1">Table 1</xref>.</p>
<p><xref ref-type="fig" rid="fig15">Fig 15</xref> shows the firing rate responses of output neuron &#x0023;876 before training (<xref ref-type="fig" rid="fig15">Fig 15<bold>A</bold></xref>) and after training (<xref ref-type="fig" rid="fig15">Fig 15<bold>B</bold></xref>). <xref ref-type="fig" rid="fig15">Fig 15<bold>B</bold></xref> shows that after training output neuron &#x0023;876 responded to the same head-centred location across different eye-positions. This was not the case for the same output neuron before training (<xref ref-type="fig" rid="fig15">Fig 15<bold>A</bold></xref>). Thus, it is evident that during training the neuron has learned to respond in a head-centred reference frame.</p>
<fig id="fig15" position="float" fig-type="figure">
<label>Fig 15.</label>
<caption><title>Simulation results showing the firing rate responses of a model incorporating a population of monotonic modulated input neurons trained with the modified learning rule 18: Delayed Postsynaptic Firing Rate with anti-Hebbian Learning.</title><p>The figure shows the firing rate responses of output neuron &#x0023;876 before training <bold>(A)</bold> and after training <bold>(B)</bold> during testing for four different eye-positions: &#x2212;18&#x00B0;, &#x2212;6&#x00B0;, 6&#x00B0; and 18&#x00B0;. Conventions as for <xref ref-type="fig" rid="fig12">Fig 12</xref>. The comparison of subplot <bold>(A)</bold> and subplot <bold>(B)</bold> shows that the output neuron learned to respond to a specific head-centred location regardless of the eye-position after training.</p></caption>
<graphic xlink:href="327288_fig15.tif"/>
</fig>
<p><xref ref-type="fig" rid="fig16">Fig 16</xref> shows how the synaptic weight structure of the same output neuron &#x0023;876 changed due to training the model with learning rule 18. The final weight structure after training shown in <xref ref-type="fig" rid="fig16">Fig 16B</xref> resembles the predicted weight structure shown in <xref ref-type="fig" rid="fig8">Fig 8</xref> for head-centred neurons.</p>
<fig id="fig16" position="float" fig-type="figure">
<label>Fig 16.</label>
<caption><title>Simulation results showing the strengths of the afferent synapses of a model incorporating a population of sigmoidal modulated input neurons trained with the modified learning rule 18: Delayed Postsynaptic Firing Rate with anti-Hebbian Learning.</title><p>The figure shows the strengths of the afferent synapses from the input population to output neuron &#x0023;876 for the untrained <bold>(A)</bold> and trained <bold>(B)</bold> model. The output neuron corresponds to the one plotted in <xref ref-type="fig" rid="fig15">Fig 15</xref>. Conventions as for <xref ref-type="fig" rid="fig13">Fig 13</xref>. The synaptic weight structure for this output neuron after training shown in plot <bold>(B)</bold> has approximately the correct profile for a head-centred neuron (<xref ref-type="fig" rid="fig8">Fig 8</xref>).</p></caption>
<graphic xlink:href="327288_fig16.tif"/>
</fig>
<p><xref ref-type="fig" rid="fig17">Fig 17</xref> shows the eye-centredness and head-centredness values of output neurons in the untrained and in the trained model. It is clear that almost none of the output neurons in the untrained model had values of head-centredness higher than eye-centredness. However, after training the head-centredness values of many output neurons increased substantially, with a number of such neurons now having greater head-centredness than eye-centredness values. Such neurons are, therefore, classified as head-centred neurons (section Analysis of Network Performance). Comparing the output population analysis of the model trained with the modified learning rule 18 shown in <xref ref-type="fig" rid="fig17">Fig 17</xref> with the analysis of the model trained with the standard trace learning rule (6) shown in <xref ref-type="fig" rid="fig5">Fig 5</xref> demonstrates that the new modified learning rule 18 is also more efficacious at producing head-centred output neurons when the input neurons are modulated by a sigmoidal function of eye-position than the standard trace learning rule (6) previously implemented by [<xref rid="c1" ref-type="bibr">1</xref>].</p>
<fig id="fig17" position="float" fig-type="figure">
<label>Fig 17.</label>
<caption><title>Simulation results showing the output reference frame response characteristics of a model incorporating a population of sigmoidal modulated input neurons trained with the modified learning rule 18: Delayed Postsynaptic Firing Rate with anti-Hebbian Learning.</title><p>The scatter plot shows the reference frame response characteristics of all output neurons before and after training. Conventions as for <xref ref-type="fig" rid="fig14">Fig 14</xref>. It is evident that training had the effect of increasing the head-centredness values of most output neurons. Indeed many more head-centred output neurons are present in the trained model than in the untrained model.</p></caption>
<graphic xlink:href="327288_fig17.tif"/>
</fig>
<p>In summary, these results showed that training the model with the modified learning rule 18 resulted in the development of head-centred output neurons, even when the whole input population had sigmoidal eye-position gain modulation. However, the comparison of <xref ref-type="fig" rid="fig17">Fig 17</xref> and <xref ref-type="fig" rid="fig14">Fig 14</xref> shows that learning rule 16, which incorporated a delayed postsynaptic trace <italic>q<sub>i</sub></italic>(<italic>t</italic> &#x2212; &#x0394;<italic>T</italic>), is in fact more efficacious at driving the development of head-centred output neurons than learning rule 18, which incorporated the delayed firing rate <italic>y<sub>i</sub></italic>(<italic>t</italic> &#x2212; &#x0394;<italic>T</italic>). Thus, the incorporation of the trace value <italic>q<sub>i</sub></italic>(<italic>t</italic> &#x2212; &#x0394;<italic>T</italic>) enhances the ability of the learning rule to perform temporal binding of input patterns corresponding to the same head-centred location.</p>
</sec>
<sec id="s7g">
<title>Modified Learning Rule: Current Postsynaptic Trace with Anti-Hebbian Learning</title>
<p>This section presents simulation results showing the performance of a modified learning rule which incorporated the current postsynaptic trace with anti-Hebbian learning. Thus, this learning rule does not use an explicit time delay &#x0394;<italic>T</italic>. The learning rule was given by
<disp-formula id="eqn19">
<alternatives><graphic xlink:href="327288_eqn19.gif"/></alternatives>
</disp-formula>
where <italic>&#x03B1;</italic> was the learning rate, <italic>&#x03B2;</italic> was the tuning parameter, <italic>q<sub>i</sub></italic> was the trace value of the output neuron <italic>i</italic>, and <italic>y<sub>i</sub></italic> and <italic>v<sub>j</sub></italic> were the post&#x002D; and pre-synaptic firing rate values, respectively. What distinguishes the learning rule in <xref ref-type="disp-formula" rid="eqn19">Eq 19</xref> from the learning rule in <xref ref-type="disp-formula" rid="eqn16">Eq 16</xref> is the use of the trace value calculated at the same time <italic>t</italic> when the synaptic weight is updated.</p>
<p><xref ref-type="table" rid="tbl1">Table 1</xref> gives the parameters for the model. The value of the tuning parameter <italic>&#x03B2;</italic> was set to 2.2 for optimal performance.</p>
<p><xref ref-type="fig" rid="fig18">Fig 18</xref> shows the firing rate responses of output neuron &#x0023;328 for the untrained and trained models. Specifically, <xref ref-type="fig" rid="fig18">Fig 18<bold>A</bold></xref> shows that prior to training the response of output neuron &#x0023;328 had no consistent structure in head-centred space across different eye-positions. However, <xref ref-type="fig" rid="fig18">Fig 18<bold>B</bold></xref> shows that after training the same output neuron reponded maximally to the same head-centred location across all four eye-positions. Thus, after training, neuron &#x0023;328 responded in a head-centred frame of reference.</p>
<fig id="fig18" position="float" fig-type="figure">
<label>Fig 18.</label>
<caption><title>Simulation results showing the firing rate responses of a model incorporating a population of monotonic modulated input neurons trained with the modified learning rule 19: Current Postsynaptic Trace with anti-Hebbian Learning.</title><p>The figure shows the firing rate responses of output neuron &#x0023;328 before training <bold>(A)</bold> and after training <bold>(B)</bold> during testing for four different eye-positions: &#x2212;18&#x00B0;, &#x2212;6&#x00B0;, 6&#x00B0; and 18&#x00B0;. Conventions as for <xref ref-type="fig" rid="fig12">Fig 12</xref>. The comparison of subplot <bold>(A)</bold> and subplot <bold>(B)</bold> shows that the output neuron learned to respond to a specific head-centred location regardless of the eye-position after training.</p></caption>
<graphic xlink:href="327288_fig18.tif"/>
</fig>
<p>The change in the synaptic weight structure of the same output neuron &#x0023;328 due to 975 training is shown in <xref ref-type="fig" rid="fig19">Fig 19</xref>. Before training the afferent synaptic weights were randomly 976 assigned (<xref ref-type="fig" rid="fig19">Fig 19<bold>A</bold></xref>). After training, however, the synaptic weight structure of the same 977 output neuron (<xref ref-type="fig" rid="fig19">Fig 19<bold>B</bold></xref>) is similar to the predicted weight structure for head-centred neurons shown in <xref ref-type="fig" rid="fig8">Fig 8</xref>.</p>
<fig id="fig19" position="float" fig-type="figure">
<label>Fig 19.</label>
<caption><title>Simulation results showing the strengths of the afferent synapses of a model incorporating a population of sigmoidal modulated input neurons trained with the modified learning rule 19: Current Postsynaptic Trace with anti-Hebbian Learning.</title><p>The figure shows the strengths of the afferent synapses from the input population to output neuron &#x0023;328 for the untrained <bold>(A)</bold> and trained <bold>(B)</bold> model. The output neuron corresponds to the one plotted in <xref ref-type="fig" rid="fig18">Fig 18</xref>. Conventions as for <xref ref-type="fig" rid="fig13">Fig 13</xref>. The synaptic weight structure for this output neuron after training shown in plot <bold>(B)</bold> has the correct kind of profile for a head-centred neuron (<xref ref-type="fig" rid="fig8">Fig 8</xref>).</p></caption>
<graphic xlink:href="327288_fig19.tif"/>
</fig>
<p><xref ref-type="fig" rid="fig20">Fig 20</xref> shows the eye-centredness and head-centredness values of output neurons in the untrained and in the trained model. In particular, <xref ref-type="fig" rid="fig20">Fig 20</xref> shows that training had the effect of increasing the head-centredness value for a large proportion of output neurons. Furrthermore, while almost no output neurons were classified as head-centred before training, a significant number of output neurons were classified as head-centred after training. A comparison of the output population analysis of the model trained with the modified learning rule 19 shown in <xref ref-type="fig" rid="fig20">Fig 20</xref> with the performance of the model trained with the standard trace learning rule (6) presented in <xref ref-type="fig" rid="fig5">Fig 5</xref> shows that the modified learning rule 19 is also significantly more capable of producing head-centred output neurons than the standard trace learning rule (6) when all of the input neurons have sigmoidal gain modulation by eye-position.</p>
<fig id="fig20" position="float" fig-type="figure">
<label>Fig 20.</label>
<caption><title>Simulation results showing the output reference frame response characteristics of a model incorporating a population of sigmoidal modulated input neurons trained with the modified learning rule 19: Current Postsynaptic Trace with anti-Hebbian Learning.</title><p>The scatter plot shows the reference frame response characteristics of all output neurons before and after training. Conventions as for <xref ref-type="fig" rid="fig14">Fig 14</xref>. It can be seen that training increased the head-centredness values of most output neurons, with quite a number of head-centred output neurons present in the trained model.</p></caption>
<graphic xlink:href="327288_fig20.tif"/>
</fig>
<p>In summary, these results showed that training the model with learning rule 19 drives the development of head-centred output neurons even when the whole input population had sigmoidal eye-position gain modulation. The comparison of <xref ref-type="fig" rid="fig20">Fig 20</xref> with <xref ref-type="fig" rid="fig14">Fig 14</xref> shows that learning rule 16, which incorporated a delayed postsynaptic trace <italic>q<sub>i</sub></italic>(<italic>t</italic> &#x2212; &#x0394;<italic>T</italic>), is more effective at producing head-centred output neurons than learning rule 19, which incorporated the current trace <italic>q<sub>i</sub></italic>(<italic>t</italic>) calculated at the same time <italic>t</italic> when the weights are updated.</p>
</sec>
<sec id="s7h">
<title>Modified Learning Rule: Delayed Postsynaptic Trace Learning Rule</title>
<p>The simulation results presented in section Modified Learning Rule: Delayed Postsynaptic Trace with Anti-Hebbian Learning, section Modified Learning Rule: Delayed Postsynaptic Firing Rate with Anti-Hebbian Learning and section Modified Learning Rule: Current Postsynaptic Trace with Anti-Hebbian Learning showed, respectively, that training the model with learning rule 16, learning rule 18 and learning rule 19 successfully self-organised head-centred output responses and increased the head-centredness value of a large proportion of output neurons when all input neurons had sigmoidal eye-position gain modulation. Importantly, section Self-organisation with peaked and monotonic gain fields showed this was not the case for the standard trace learning rule (6). Training the model with the standard trace learning rule successfully self-organised head-centred output neurons when all of the input neurons had peaked eye-position gain modulation, but failed when the modulation of the input population was altered from peaked to sigmoidal. Indeed, the simple introduction of a small proportion (e.g. with <italic>p</italic> &#x003E; 0.2) of input neurons with sigmoidal eye-position gain modulation was enough to undermine the self-organisation of head-centred output responses (section Standard Trace Learning Rule with Mixed Peaked and Sigmoidal eye-position Modulation of Input Neurons).</p>
<p>The modified learning rules introduced in section Modified Learning Rule: Delayed Postsynaptic Trace with Anti-Hebbian Learning, section Modified Learning Rule: Delayed Postsynaptic Firing Rate with Anti-Hebbian Learning and in section Modified Learning Rule: Current Postsynaptic Trace with Anti-Hebbian Learning all had an anti-hebbian term as their common component. Out of these learning rules, the best performance was observed with learning rule 16, which incorporated a postsynaptic delayed-trace term. In this case, an interesting question is whether the superior efficacy of this learning rule in driving the development of head-centred output responses was primarily due to the anti-hebbian term or the postsynaptic delayed-trace term. In particular, is an anti-hebbian term actually needed in this learning rule for the self-organisation of head-centred responses in the presence of input neurons with sigmoidal eye-position gain modulation, or could head-centred output responses develop using a learning rule that only incorporated a postsynaptic delayed-trace term?</p>
<p>This section addresses the above questions by investigating the performance of the model with the following learning rule
<disp-formula id="eqn20">
<alternatives><graphic xlink:href="327288_eqn20.gif"/></alternatives>
</disp-formula>
where <italic>&#x03B1;</italic> was the learning rate, <italic>q<sub>i</sub></italic> was the trace value of the output neuron <italic>i</italic> and <italic>V<sub>j</sub></italic> was the firing rate of the input neuron <italic>j</italic>. In <xref ref-type="disp-formula" rid="eqn20">Eq 20</xref> the trace value <italic>q<sub>i</sub></italic> is calculated at time (<italic>t</italic> &#x2212; &#x0394;<italic>T</italic>)<italic>ms</italic>. This learning rule has no anti-hebbian term, and relies purely on the postsynaptic delayed-trace term <italic>q<sub>i</sub></italic>(<italic>t</italic> &#x2212; &#x0394;<italic>T</italic>) to drive the development of head-centred output neurons.</p>
<p>The simulation parameters for the model are given in <xref ref-type="table" rid="tbl1">Table 1</xref>. The time delay &#x0394;<italic>T</italic> used to compute the trace value of each output neuron <italic>i</italic> was set to 30<italic>ms</italic> for optimal learning performance.</p>
<p><xref ref-type="fig" rid="fig21">Fig 21</xref> shows how training changed the firing rate responses of output neuron &#x0023;223. Prior to training the response of output neuron &#x0023;223 had no consistent structure in head-centred space across different eye-positions (<xref ref-type="fig" rid="fig21">Fig 21 <bold>A</bold></xref>), whilst after training the same output neuron responded maximally to the same head-centred location across all four eye-positions (<xref ref-type="fig" rid="fig21">Fig 21<bold>B</bold></xref>). Thus, neuron &#x0023;223 learned to respond in a head-centred reference frame after training.</p>
<fig id="fig21" position="float" fig-type="figure">
<label>Fig 21.</label>
<caption><title>Simulation results showing the firing rate responses of a model incorporating a population of monotonic modulated input neurons trained with the modified learning rule 20: Delayed Postsynaptic Trace Learning Rule.</title><p>The figure shows the firing rate responses of output neuron &#x0023;223 before training <bold>(A)</bold> and after training <bold>(B)</bold> during testing for four different eye-positions: &#x2212;18&#x00B0;, &#x2212;6&#x00B0;, 6&#x00B0; and 18&#x00B0;. Conventions as for <xref ref-type="fig" rid="fig12">Fig 12</xref>. The comparison of subplot <bold>(A)</bold> and subplot <bold>(B)</bold> shows that the output neuron learned to respond to a specific head-centred location regardless of the eye-position after training. Output neuron &#x0023;223 is not shown in the scatter plot of subplot <bold>(A)</bold> because this neuron did not respond for every eye-position before training and was therefore excluded from further analysis (section Analysis of Network Performance). However, subplot <bold>(B)</bold> shows that the same output neuron learned to respond to a specific head-centred location regardless of the eye-position after training.</p></caption>
<graphic xlink:href="327288_fig21.tif"/>
</fig>
<p><xref ref-type="fig" rid="fig22">Fig 22</xref> shows how the synaptic weight structure of the same output neuron &#x0023;223 plotted in <xref ref-type="fig" rid="fig21">Fig 21</xref> changed due to training the model with learning rule 20. It is clear that the final synaptic weight structure after training (<xref ref-type="fig" rid="fig22">Fig 22<bold>B</bold></xref>) resembles the predicted weight structure for head-centred neurons (<xref ref-type="fig" rid="fig8">Fig 8</xref>), even though this was not the case before training (<xref ref-type="fig" rid="fig22">Fig 22<bold>A</bold></xref>).</p>
<fig id="fig22" position="float" fig-type="figure">
<label>Fig 22.</label>
<caption><title>Simulation results showing the strengths of the afferent synapses of a model incorporating a population of sigmoidal modulated input neurons trained with the modified learning rule 20: Delayed Postsynaptic Trace Learning Rule.</title><p>The figure shows the strengths of the afferent synapses from the input population to output neuron &#x0023;223 for the untrained <bold>(A)</bold> and trained <bold>(B)</bold> model. The output neuron corresponds to the one plotted in <xref ref-type="fig" rid="fig21">Fig 21</xref>. Conventions as for <xref ref-type="fig" rid="fig13">Fig 13</xref>. The synaptic weight structure for this output neuron after training shown in plot <bold>(B)</bold> has approximately the correct profile for a head-centred neuron (<xref ref-type="fig" rid="fig8">Fig 8</xref>).</p></caption>
<graphic xlink:href="327288_fig22.tif"/>
</fig>
<p><xref ref-type="fig" rid="fig23">Fig 23</xref> shows the eye-centredness and head-centredness values of all of the output neurons in the untrained model and in the trained model. After training, the head-centredness values of many output neurons had increased substantially. However, there was only a single output neuron, which was neuron &#x0023;223, with a greater head-centredness value than eye-centreredness, and which was consequently classed as responding in a head-centred reference frame. Therefore, in the simulations presented in this paper, the anti-hebbian term in learning rules (16), (18) and (19) appears to play an important and essential role in producing relatively large numbers of head-centred output neurons when all of the input neurons have sigmoidal eye-position gain fields.</p>
<fig id="fig23" position="float" fig-type="figure">
<label>Fig 23.</label>
<caption><title>Simulation results showing the output reference frame response characteristics of a model incorporating a population of sigmoidal modulated input neurons trained with the modified learning rule 20: Delayed Postsynaptic Trace Learning Rule.</title><p>The scatter plot shows the reference frame response characteristics of all output neurons before and after training. Conventions as for <xref ref-type="fig" rid="fig14">Fig 14</xref>. It is evident that training had the effect of increasing the head-centredness values of most output neurons. Although, there is only a single head-centred output neuron present in the trained model.</p></caption>
<graphic xlink:href="327288_fig23.tif"/>
</fig>
</sec>
<sec id="s7i">
<title>Delayed Postsynaptic Trace Learning Rule with Mixed Peaked and Sigmoidal eye-position Modulation of Input Neurons</title>
<p>In the previous section, it was found that an anti-hebbian term was needed in the learning rule in order to produce relatively large numbers of head-centred output neurons if the entire population of input neurons had sigmoidal gain fields. However, experimental studies have demonstrated that the primate cortex contains a mixed population of visual neurons with either peaked or monotonic eye-position gain fields [<xref ref-type="bibr" rid="c7">7</xref>&#x2013;<xref ref-type="bibr" rid="c9">9</xref>]. This raises the question of whether learning rule (20), which relies solely on a postsynaptic delayed-trace term without any anti-hebbian term, could produce a much larger number of head-centred output neurons if the input population had a 50:50 mix of peaked and sigmoidal eye-position gain fields.</p>
<p>This section presents simulation results with the modified learning rule (20) when there is a 50:50 mixture of peaked and monotonic gain fields in the visual input population. Can the model produce a large number of head-centred output neurons under such conditions without an anti-hebbian term in the learning rule? This is still a potentially challenging task for the model because in section Standard Trace Learning Rule with Mixed Peaked and Sigmoidal eye-position Modulation of Input Neurons it was shown that the standard trace learning rule (6), which lacks an anti-hebbian term, failed to produce significant numbers of head-centred output neurons when the proportion of sigmoidal gain modulated input neurons rose above just 20&#x0025; of the overall input population. The model parameters for the simulations presented in this section are given in <xref ref-type="table" rid="tbl1">Table 1</xref>.</p>
<p><xref ref-type="fig" rid="fig24">Fig 24</xref> shows how training changed the firing rate responses of output neuron &#x0023;281. In particular, <xref ref-type="fig" rid="fig24">Fig 24<bold>B</bold></xref> shows that after training output neuron &#x0023;281 responded maximally to the same head-centred location across all four eye-positions, although this was not the case prior to training (<xref ref-type="fig" rid="fig24">Fig 24<bold>A</bold></xref>). Hence neuron &#x0023;281 responded in a head-centred frame of reference after training.</p>
<fig id="fig24" position="float" fig-type="figure">
<label>Fig 24.</label>
<caption><title>Simulation results of a model incorporating a mixed population of peaked and sigmoidal modulated input neurons, with sigmoidal modulation rate p set to 0.5, trained with the modified learning rule 20: Delayed Postsynaptic Trace Learning Rule.</title><p>The figure shows the firing rate responses of output neuron &#x0023;281 before training <bold>(A)</bold> and after training <bold>(B)</bold> during testing for four different eye-positions: &#x2212;18&#x00B0;, &#x2212;6&#x00B0;, 6&#x00B0; and 18&#x00B0;. Conventions as for <xref ref-type="fig" rid="fig12">Fig 12</xref>. Plot <bold>(B)</bold> shows that the same output neuron learned to respond to a specific head-centred location regardless of the eye-position after training.</p></caption>
<graphic xlink:href="327288_fig24.tif"/>
</fig>
<p><xref ref-type="fig" rid="fig25">Fig 25</xref> shows how the synaptic weight structure of the same output neuron &#x0023;281 plotted in <xref ref-type="fig" rid="fig24">Fig 24</xref> changed due to training. <xref ref-type="fig" rid="fig25">Fig 25<bold>B</bold></xref> shows that after training, the synaptic weight structure of the output neuron was clearly similar to the predicted weight structure for head-centred neurons shown in <xref ref-type="fig" rid="fig8">Fig 8</xref>. The synaptic weight structure before training reflected the randomly assigned connection weights (<xref ref-type="fig" rid="fig25">Fig 25<bold>A</bold></xref>).</p>
<fig id="fig25" position="float" fig-type="figure">
<label>Fig 25.</label>
<caption><title>Simulation results of a model incorporating a mixed population of peaked and sigmoidal modulated input neurons, with sigmoidal modulation rate <italic>p</italic> set to 0.5, trained with the modified learning rule 20: Delayed Postsynaptic Trace Learning Rule.</title><p>The figure shows the strengths of the afferent synapses from the input population to output neuron &#x0023;281 for the untrained <bold>(A)</bold> and trained <bold>(B)</bold> model. The output neuron corresponds to the one plotted in <xref ref-type="fig" rid="fig24">Fig 24</xref>. Conventions as for <xref ref-type="fig" rid="fig13">Fig 13</xref>. The synaptic weight structure for this output neuron after training shown in plot <bold>(B)</bold> has the correct kind of profile for a head-centred neuron (<xref ref-type="fig" rid="fig8">Fig 8</xref>).</p></caption>
<graphic xlink:href="327288_fig25.tif"/>
</fig>
<p><xref ref-type="fig" rid="fig26">Fig 26</xref> shows the eye-centredness and head-centredness values of output neurons in the untrained and in the trained model. <xref ref-type="fig" rid="fig26">Fig 26</xref> shows that training the model with learning rule 20, where the input population contained a 50:50 mix of neurons with peaked or sigmoidal gain modulation, had the effect of increasing the head-centredness value of most output neurons. Moreover, a large proportion of output neurons in the trained model are head-centred (i.e. with a head-centredness value greater than eye-centredness). Indeed, more head-centred output neurons were observed in the trained model in this section than in any of the trained models presented in previous section Modified Learning Rule: Delayed Postsynaptic Trace with Anti-Hebbian Learning, section Modified Learning Rule: Delayed Postsynaptic Firing Rate with Anti-Hebbian Learning and section Modified Learning Rule: Current Postsynaptic Trace with Anti-Hebbian Learning.</p>
<fig id="fig26" position="float" fig-type="figure">
<label>Fig 26.</label>
<caption><title>Simulation results of a model incorporating a mixed population of peaked and sigmoidal modulated input neurons, with sigmoidal modulation rate <italic>p</italic> set to 0.5, trained with the modified learning rule 20: Delayed Postsynaptic Trace Learning Rule.</title><p>The scatter plot shows the reference frame response characteristics of all output neurons before and after training. Conventions as for <xref ref-type="fig" rid="fig14">Fig 14</xref>. It is evident that training had the effect of increasing the head-centredness values of most output neurons, with many more head-centred output neurons present in the trained model.</p></caption>
<graphic xlink:href="327288_fig26.tif"/>
</fig>
<p>In summary, these results demonstrated that it was possible for the model to produce large numbers of head-centred output neurons with a learning rule that relied purely on a postsynaptic delayed-trace term without any anti-hebbian term if the input population contained a 50:50 mix of neurons that were modulated by either peaked or sigmoidal gain fields. Such a mixture of different forms of eye-position gain modulation is biologically compatible with experimental findings [<xref rid="c7" ref-type="bibr">7</xref>,<xref rid="c8" ref-type="bibr">8</xref>].</p>
</sec>
</sec>
<sec id="s8">
<title>Discussion</title>
<p>The majority of previously published models of coordinate transformation from eye-centred to head-centred visual representations have relied on some form of supervised error-correction learning, in which an explicit supervisory signal is used to specify the desired head-centred output responses during training [<xref ref-type="bibr" rid="c14">14</xref>&#x2013;<xref ref-type="bibr" rid="c16">16</xref>]. The availability of such a supervisory training signal makes the self-organisation of these models robust even with monotonic (e.g. planar or sigmoidal) eye-position gain modulated input neurons. However, an immediate problem with these kinds of models is explaining exactly where such a supervisory training signal might originate from in the brain. Another major problem for models that employ a backpropagation of error supervised learning, such as the classic model of [<xref rid="c14" ref-type="bibr">14</xref>], is that this model architecture is entirely biologically implausible [<xref rid="c18" ref-type="bibr">18</xref>]. In particular, there is no plausible explanation for how the error terms needed to adjust the afferent synaptic weights in the intermediate layer of the model could be generated and implemented in the brain. One consequence of this is that the backpropagation of error learning procedure can result in individual neurons making both excitatory and inhibitory synaptic connections on different postsynaptic neurons. Although not shown here, we have verified this through replicating the model simulations of [<xref rid="c14" ref-type="bibr">14</xref>] and [<xref rid="c16" ref-type="bibr">16</xref>]. This feature of backpropagation of error models violates an accepted principle of cortical architecture, sometimes referred to as Dale&#x2019;s Law, that an individual neuron must have the same kind of excitatory or inhibitory effect at all of its synaptic connections with other neurons [<xref rid="c19" ref-type="bibr">19</xref>]. This failure of backpropagation of error models even undermines the potential relevance of their trained synaptic architecture to understanding how coordinate transformations to head-centred visual representations might be implemented in the brain. [<xref rid="c15" ref-type="bibr">15</xref>] also used error-correction learning to develop head-centred output representations. Their learning scheme employed a simpler global error signal for the entire output population, which might conceivably be implemented by some form of neuromodulator release. However, the implementation of error correction learning in their model still required complex architectural features that have not been identified in cortex.</p>
<p>To remedy the lack of biological plausibility in previously published models that use supervised learning to drive the development of visual neurons that respond in a head-centred reference frame, [<xref rid="c1" ref-type="bibr">1</xref>] demonstrated a model that self-organised using unsupervised competitive learning driven by the standard trace learning rule (6). In their model the trace learning rule was able to exploit the natural statistics of how primates tend to move their eyes and head as they explore their visual environment, with more frequent shifts in eye-position than head position. These natural eye and head movements enable trace learning to bind together different visual input patterns corresponding to visual targets located in the same head-centred location but different retinal locations, thereby driving the development of postsynaptic neurons that respond to visual targets in specific head-centred locations.</p>
<p>It was originally demonstrated by [<xref rid="c1" ref-type="bibr">1</xref>], and shown again above, that the self-organising model described in section Materials and methods using the standard trace learning rule (6) with peaked eye-position gain modulated input neurons is able to develop head-centred output representations during training. However, a key new result in this paper is that the self-organising model with purely sigmoidal eye-position gain modulated input neurons develops eye-centred output neurons when using the standard trace learning rule. The cause of this contrasting behaviour between peaked and sigmoidal gain fields is the correlations between the activity of the input neurons across all of the input patterns during training. It is well understood that in a standard competitive neural network, individual output neurons learn to respond to subsets of input neurons that tend to be most frequently co-active [<xref rid="c18" ref-type="bibr">18</xref>]. In the self-organising model with peaked gain modulation, the subsets of input neurons that are frequently co-active correspond to circular clusters that are highly localised in both the retinotopic preference and eye-position preference dimensions. With a standard hebbian learning rule with no significant memory trace of recent neuronal activity, individual output neurons will learn to respond to these localised circular clusters of input neurons. If a trace learning rule is implemented, it is a relatively easy task for individual output neurons to simply bind together these clusters of input neurons along a diagonal line in the input space corresponding to a particular head-centred location. Output neurons will then respond to particular head-centred locations regardless of eye-position or the retinal location of a visual target. However, the situation is quite different with sigmoidal gain modulated input neurons. Due to monotonic saturation of the input neuron response function in the eye-position dimension of the input space, the subsets of input neurons that are most frequently co-active are localised in the retinotopic preference dimension but elongated in the eye-position preference dimension. With a standard hebbian learning rule, individual output neurons will learn to respond to these elongated clusters of input neurons, which will give rise to eye-centred output responses. However, if a trace learning rule is implemented, the output neurons still learn to represent eye-centred rather than head-centred locations. This is because developing head-centred output responses would require the trace learning rule to disrupt and break apart output representations corresponding to the elongated clusters of input neurons with correlated activities. However, in practice the trace learning effect is not strong enough to achieve this. Consequently, even with trace learning, the elongated clusters of input neurons with correlated activities continue to drive the development of eye-centred output neurons. This finding holds for any input neuron response function with a monotonic gain in the eye-position dimension, be it purely linear [<xref rid="c14" ref-type="bibr">14</xref>] or linear rectified [<xref rid="c20" ref-type="bibr">20</xref>].</p>
<p>We next showed that a manually prewired neural network model with sigmoidal eye-position gain modulated input neurons could perform the coordinate transformation. This is achieved by elevating the weight of synaptic connections from all input neurons which responded strongly, for some eye-position, to a visual target in the head-centred location to which the postsynaptic output neuron is assigned. This result established the feasability of the coordinate transformation within the given model architecture with sigmoidal gain fields acting on the input population. However, when synaptic plasticity based on the standard trace learning rule (6) is suddenly introduced into the manually prewired model, it is found that just a single epoch of visually-guided training is sufficient to dramatically reduce the prevalence of head-centred neurons in the output population, and by the 5<sup>th</sup> epoch all neurons are eye-centred for all subsequent epochs of training. This showed that sigmoidal gain modulation would, in so far as the synapses are plastic, even undermine the functioning of a cortical circuit which is somehow prewired, perhaps by genetic hardwiring or by an initial period of supervised learning as suggested by [<xref rid="c20" ref-type="bibr">20</xref>], to produce the head-centred visual representations. Given the ubiquitous presence of associative synaptic plasticity in cortex, it is therefore a substantial challenge to explain how head-centred visual neurons might develop and persist in the brain given the relatively large proportion of precursor eye-centred visual neurons that have monotonic eye-position gain fields.</p>
<p>It has been shown that neurons in many parietal areas have a mixture of different forms of gain modulation, not all of which are planar. In fact, 41&#x0025; and 56&#x0025; are not planar in areas LIP and 7a, respectively [<xref rid="c8" ref-type="bibr">8</xref>]. Consequently, we explored how various degrees of prevalence of sigmoidal gain modulation would influence the self-organisation of the model with the standard trace learning rule (6). It is found that so long as there is no more than approximately 20&#x0025; sigmoidal gain modulation in the input population then the fraction of head-centred neurons in the trained model did not drop below &#x007E;15&#x0025;. However, the decrease in performance, and eventual collapse of the model, is very severe when the presence of sigmoidal gain modulation in the input population is increased. In particular, with a more biologically realistic 50:50 mix in peaked and sigmoidal gain modulated input neurons, the model failed to develop a significant population of head-centred output neurons with the standard trace learning rule (6).</p>
<p>In order to find a biologically plausible way in which the self-organising model could develop relatively large numbers of head-centred visual output neurons when at least half or even all of the input neurons had sigmoidal gain fields, we next explored the performance of the model architecture using a variety of more sophisticated learning rules that may incorporate temporally delayed memory traces, as well as a mixture of both trace learning and anti-Hebbian learning terms. These new, modified learning rules were previously developed by [<xref rid="c25" ref-type="bibr">25</xref>] in the context of modelling transform invariant visual object recognition. The new learning rules continue to be biologically plausible because they rely on locally available biological quantities, namely the activities of the pre&#x002D; and post-synaptic neurons, to guide the modification of synaptic weights. The different modified learning rules were found to successfully drive the self-organisation of head-centred output responses when all of the input neurons had sigmoidal eye-position modulation. The modified learning rules were able to substantially improve the temporal binding of the standard trace learning rule by incorporating a trace of previous neuronal activity with an explicit time delay &#x0394;<italic>T</italic>. This was evidenced by better performance being achieved when the learning rule incorporated a trace value of the postsynaptic neuron from an earlier time &#x0394;<italic>T</italic> in the past instead of the trace value at the current time. Furthermore, the new learning rules were able to overcome the high correlations between overlapping input patterns with sigmoidal gain fields through the additional incorporation of anti-Hebbian learning, which had been previously found to offer a significant performance enhancement by [<xref rid="c25" ref-type="bibr">25</xref>]. However, learning rule (20) incorporated a delayed postsynaptic trace without an anti-Hebbian term (section Modified Learning Rule: Delayed Postsynaptic Trace Learning Rule). Simulation results with this learning rule showed that head-centred output responses developed in the trained model. Thus, anti-Hebbian learning is not necessarily a requirement for the development of head-centred output responses. The performance of learning rule (20) was then investigated under a more biologically realistic scenario in which the input population consisted of a 50:50 mixture of neurons with either peaked or sigmoidal eye-position gain modulation. These results demonstrated that the model could produce large numbers of head-centred output neurons with a learning rule that relied purely on a postsynaptic delayed-trace term without any anti-hebbian term if the input population contained a 50:50 mix of neurons modulated by either peaked or sigmoidal gain fields. These last findings should be contrasted with the performance of
the model with the standard trace learning rule (6), which failed to develop a significant 1251 number of head-centred output neurons with such a mixed input population.</p>
</sec>
<sec id="s9">
<title>Conclusion</title>
<p>In conclusion, the work presented here has shown that the existence of a synaptic weight structure that can perform a coordinate transformation does not guarantee that it will emerge through a process of self-organisation using any one particular form of trace learning rule. In particular, the standard trace learning rule originally proposed by [<xref rid="c1" ref-type="bibr">1</xref>] failed to produce head-centred output neurons when the input neurons were modulated by a sigmoidal function of eye-position, and even failed with a biologically realistic 50:50 mix of input neurons with peaked and sigmoidal gain fields. In order to remedy this problem, we had to investigate the use of more sophisticated, yet still biologically plausible, learning rules that incorporated temporally delayed memory traces, as well as a mixture of both trace learning and anti-hebbian learning terms. These new, modified learning rules were found to produce head-centred output neurons when the input population had sigmoidal gain fields. Moreover, it was also found that the delayed postsynaptic trace in learning rule (20) was sufficient to drive the development of head-centred output neurons in the absence of anti-hebbian learning, especially if the input population had a 50:50 mix of peaked and monotonic gain fields. This work thus makes an important contribution to understanding how head-centred visual neurons may develop in the brain through an unsupervised process of visually-guided learning given visual precursor neurons with sigmoidal (monotonic) eye-position modulation. Furthermore, although we have studied one particular kind of coordinate transformation, i.e. from eye-centred to head-centred visual representations, these findings may also apply to a range of other coordinate transformations with sensorimotor integration of monotonically encoded motor variables [<xref rid="c4" ref-type="bibr">4</xref>,<xref rid="c5" ref-type="bibr">5</xref>].</p>
</sec>
</body>
<back>
<sec id="s10" sec-type="supplementary-material">
<title>Supporting information</title>
<p><bold>Appendix A. Eye-centredness Reference Frame Analysis.</bold> During testing, the visual target was located in head-centred locations
<disp-formula id="eqn21">
<alternatives><graphic xlink:href="327288_eqn21.gif"/></alternatives>
</disp-formula>
for <italic>j</italic> &#x003D; 1, &#x2026;, <italic>T</italic>, and while in each location it was observed from eye-positions
<disp-formula id="eqn22">
<alternatives><graphic xlink:href="327288_eqn22.gif"/></alternatives>
</disp-formula>
for <italic>i</italic> &#x003D; 1, &#x2026;, <italic>E</italic>. The eye-position shift &#x0394;<italic>e</italic> was set to a multiple of the head-centred target location shift &#x0394;<italic>h</italic> to cause resampling of the neuron&#x2019;s response at the same retinal location for different eye-positions, thereby providing a resampling of the response in both head-centred and eye-centred space across multiple eye-positions.</p>
<p>The set of head-centred locations {<italic>t</italic><sub>1</sub>,&#x2026;,<italic>t<sub>T</sub></italic>} corresponded to retinal locations <italic>R<sub>i</sub></italic> &#x003D; {<italic>t</italic><sub>1</sub> &#x2212; <italic>e<sub>i</sub></italic>,&#x2026;,<italic>t<sub>T</sub></italic> &#x2212; <italic>e<sub>i</sub></italic>} when the model was fixating eye-position <italic>e<sub>i</sub></italic>, and from this it is clear that among retinal locations common to all eye-positions, <italic>t<sub>1</sub></italic> &#x2212; <italic>e</italic><sub>1</sub> was the first and <italic>t<sub>T</sub></italic> &#x2212; <italic>e<sub>E</sub></italic> was the last, that is
<disp-formula id="eqn23">
<alternatives><graphic xlink:href="327288_eqn23.gif"/></alternatives>
</disp-formula>
Therefore <italic>f<sub>i</sub></italic> and <italic>l<sub>i</sub>,</italic> denoting the first and last position included from the <italic>i</italic><sup>th</sup> response vector respectively, had to correspond to these two retinal locations respectively
<disp-formula id="eqn24">
<alternatives><graphic xlink:href="327288_eqn24.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn25">
<alternatives><graphic xlink:href="327288_eqn25.gif"/></alternatives>
</disp-formula>
We can resolve each equation to find an explicit formula for <italic>f</italic><sub><italic>i</italic></sub> and <italic>l</italic><sub><italic>i</italic></sub> in terms of <italic>i</italic> as follows.</p>
<p>By substituting <xref ref-type="disp-formula" rid="eqn21">Eq 21</xref> and <xref ref-type="disp-formula" rid="eqn22">Eq 22</xref> into <xref ref-type="disp-formula" rid="eqn24">Eq 24</xref> we obtain
<disp-formula id="ueqn1">
<alternatives><graphic xlink:href="327288_ueqn1.gif"/></alternatives>
</disp-formula>
Rearranging this gives
<disp-formula id="eqn26">
<alternatives><graphic xlink:href="327288_eqn26.gif"/></alternatives>
</disp-formula></p>
<p>By substituting <xref ref-type="disp-formula" rid="eqn21">Eq 21</xref> and <xref ref-type="disp-formula" rid="eqn22">Eq 22</xref> into <xref ref-type="disp-formula" rid="eqn25">Eq 25</xref> we obtain
<disp-formula id="ueqn2">
<alternatives><graphic xlink:href="327288_ueqn2.gif"/></alternatives>
</disp-formula>
Rearranging this gives
<disp-formula id="eqn27">
<alternatives><graphic xlink:href="327288_eqn27.gif"/></alternatives>
</disp-formula></p>
<p>We can also deduce the length <italic>V</italic> of the portion of each response vector that is used in the eye-centred correlation analysis as follows. By definition, for each response vector
<disp-formula id="ueqn3">
<alternatives><graphic xlink:href="327288_ueqn3.gif"/></alternatives>
</disp-formula>
Substituting in <xref ref-type="disp-formula" rid="eqn26">Eq 26</xref> and <xref ref-type="disp-formula" rid="eqn27">Eq 27</xref> gives
<disp-formula id="ueqn4">
<alternatives><graphic xlink:href="327288_ueqn4.gif"/></alternatives>
</disp-formula>
Rearranging gives
<disp-formula id="eqn28">
<alternatives><graphic xlink:href="327288_eqn28.gif"/></alternatives>
</disp-formula></p>
</sec>
<ack>
<title>Acknowledgments</title>
<p>This work was supported by CNPq, National Council for Scientific and Technological Development - Brazil.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Mender</surname> <given-names>BM</given-names></string-name>, <string-name><surname>Stringer</surname> <given-names>SM</given-names></string-name>. <article-title>A model of self-organizing head-centered visual responses in primate parietal areas</article-title>. <source>PloS one</source>. <year>2013</year>;<volume>8</volume>(<issue>12</issue>):<fpage>e81406</fpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Andersen</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Snyder</surname> <given-names>LH</given-names></string-name>, <string-name><surname>Li</surname> <given-names>CS</given-names></string-name>, <string-name><surname>Stricanne</surname> <given-names>B</given-names></string-name>. <article-title>Coordinate transformations in the representation of spatial information</article-title>. <source>Current Opinion in Neurobiology</source>. <year>1993</year>;<volume>3</volume>(<issue>2</issue>):<fpage>171</fpage>&#x2013;<lpage>176</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Mender</surname> <given-names>BM</given-names></string-name>, <string-name><surname>Stringer</surname> <given-names>SM</given-names></string-name>. <article-title>Self-organization of head-centered visual responses under ecological training conditions</article-title>. <source>Network: Computation in Neural Systems</source>. <year>2014</year>;<volume>25</volume>(<issue>3</issue>):<fpage>116</fpage>&#x2013;<lpage>136</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Xing</surname> <given-names>J</given-names></string-name>, <string-name><surname>Andersen</surname> <given-names>RA</given-names></string-name>. <article-title>Models of the Posterior Parietal Cortex Which Perform Multimodal Integration and Represent Space in Several Coordinate Frames</article-title>. <source>Journal of Cognitive Neuroscience</source>. <year>2000</year>;<volume>12</volume>:<fpage>601</fpage>&#x2013;<lpage>614</lpage>. doi:<pub-id pub-id-type="doi">10.1162/089892900562363.</pub-id></mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Chang</surname> <given-names>SW</given-names></string-name>, <string-name><surname>Papadimitriou</surname> <given-names>C</given-names></string-name>, <string-name><surname>Snyder</surname> <given-names>LH</given-names></string-name>. <article-title>Using a compound gain field to compute a reach plan</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>64</volume>(<issue>5</issue>):<fpage>744</fpage>&#x2013;<lpage>755</lpage>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Andersen</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Mountcastle</surname> <given-names>VB</given-names></string-name>. <article-title>The influence of the angle of gaze upon the excitability of the light-sensitive neurons of the posterior parietal cortex</article-title>. <source>The Journal of Neuroscience</source>. <year>1983</year>;<volume>3</volume>(<issue>3</issue>):<fpage>532</fpage>&#x2013;<lpage>548</lpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Andersen</surname> <given-names>R</given-names></string-name>, <string-name><surname>Essick</surname> <given-names>G</given-names></string-name>, <string-name><surname>Siegel</surname> <given-names>R</given-names></string-name>. <article-title>Encoding of spatial location by posterior parietal neurons</article-title>. <source>Science</source>. <year>1985</year>;<volume>230</volume>(<issue>4724</issue>):<fpage>456</fpage>&#x2013;<lpage>458</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.4048942</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Andersen</surname> <given-names>R</given-names></string-name>, <string-name><surname>Bracewell</surname> <given-names>R</given-names></string-name>, <string-name><surname>Barash</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gnadt</surname> <given-names>J</given-names></string-name>, <string-name><surname>Fogassi</surname> <given-names>L</given-names></string-name>. <article-title>Eye position effects on visual, memory, and saccade-related activity in areas LIP and 7a of macaque</article-title>. <source>The Journal of Neuroscience</source>. <year>1990</year>;<volume>10</volume>(<issue>4</issue>):<fpage>1176</fpage>&#x2013;<lpage>1196</lpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Galletti</surname> <given-names>C</given-names></string-name>, <string-name><surname>Battaglini</surname> <given-names>P</given-names></string-name>, <string-name><surname>Fattori</surname> <given-names>P</given-names></string-name>. <article-title>Eye Position Influence on the Parieto-occipital Area PO of the Macaque Monkey</article-title>. <source>European Journal of Neuroscience</source>. <year>1995</year>;<volume>7</volume>(<issue>12</issue>):<fpage>2486</fpage>&#x2013;<lpage>2501</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Mullette-Gillman</surname> <given-names>OA</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>YE</given-names></string-name>, <string-name><surname>Groh</surname> <given-names>JM</given-names></string-name>. <article-title>Eye-Centered, Head-Centered, and Complex Coding of Visual and Auditory Targets in the Intraparietal Sulcus</article-title>. <source>Journal of Neurophysiology</source>. <year>2005</year>;<volume>94</volume>(<issue>4</issue>):<fpage>2331</fpage>&#x2013;<lpage>2352</lpage>. doi:<pub-id pub-id-type="doi">10.1152/jn.00021.2005.</pub-id></mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Galletti</surname> <given-names>C</given-names></string-name>, <string-name><surname>Battaglini</surname> <given-names>PP</given-names></string-name>, <string-name><surname>Fattori</surname> <given-names>P</given-names></string-name>. <article-title>Parietal neurons encoding spatial locations in craniotopic coordinates</article-title>. <source>Experimental Brain Research</source>. <year>1993</year>;<volume>96</volume>(<issue>2</issue>):<fpage>221</fpage>&#x2013;<lpage>229</lpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Duhamel</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Bremmer</surname> <given-names>F</given-names></string-name>, <string-name><surname>BenHamed</surname> <given-names>S</given-names></string-name>, <string-name><surname>Graf</surname> <given-names>W</given-names></string-name>. <article-title>Spatial invariance of visual receptive fields in parietal cortex neurons</article-title>. <source>Nature</source>. <year>1997</year>;<volume>389</volume>(<issue>6653</issue>):<fpage>845</fpage>&#x2013;<lpage>848</lpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="book"><string-name><surname>Breveglieri</surname> <given-names>R</given-names></string-name>, <string-name><surname>Bosco</surname> <given-names>A</given-names></string-name>, <string-name><surname>Canessa</surname> <given-names>A</given-names></string-name>, <string-name><surname>Fattori</surname> <given-names>P</given-names></string-name>, <string-name><surname>Sabatini</surname> <given-names>SP</given-names></string-name>. <chapter-title>Evidence for peak-shaped gaze fields in area V6A: implications for sensorimotor transformations in reaching tasks</chapter-title>. In: <source>Bioinspired Applications in Artificial and Natural Computation</source>. <publisher-name>Springer</publisher-name>; <year>2009</year>. p. <fpage>324</fpage>&#x2013;<lpage>333</lpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Zipser</surname> <given-names>D</given-names></string-name>, <string-name><surname>Andersen</surname> <given-names>RA</given-names></string-name>. <article-title>A back-propagation programmed network that simulates response properties of a subset of posterior parietal neurons</article-title>. <source>Nature</source>. <year>1988</year>;<volume>331</volume>(<issue>6158</issue>):<fpage>679</fpage>&#x2013;<lpage>684</lpage>. doi:<pub-id pub-id-type="doi">10.1038/331679a0</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="confproc"><string-name><surname>Mazzoni</surname> <given-names>P</given-names></string-name>, <string-name><surname>Andersen</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Jordan</surname> <given-names>MI</given-names></string-name>. <article-title>A more biologically plausible learning rule for neural networks</article-title>. <conf-name>Proceedings of the National Academy of Sciences</conf-name>. <year>1991</year>;<volume>88</volume>(<issue>10</issue>):<fpage>4433</fpage>&#x2013;<lpage>4437</lpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Pouget</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sejnowski</surname> <given-names>TJ</given-names></string-name>. <article-title>Spatial Transformations in the Parietal Cortex Using Basis Functions</article-title>. <source>Journal of Cognitive Neuroscience</source>. <year>1997</year>;<volume>9</volume>:<fpage>222</fpage>&#x2013;<lpage>237</lpage>. doi:<pub-id pub-id-type="doi">10.1162/jocn.1997.9.2.222</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Pouget</surname> <given-names>A</given-names></string-name>, <string-name><surname>Snyder</surname> <given-names>LH</given-names></string-name>. <article-title>Computational approaches to sensorimotor transformations</article-title>. <source>Nature Neuroscience</source>. <year>2000</year>;<volume>3</volume> <issue>Suppl</issue>:<fpage>1192</fpage>&#x2013;<lpage>1198</lpage>. doi:<pub-id pub-id-type="doi">10.1038/81469</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="book"><string-name><surname>Rolls</surname> <given-names>ET</given-names></string-name>, <string-name><surname>Treves</surname> <given-names>A</given-names></string-name>. <source>Neural Networks and Brain Function</source>. <publisher-name>Oxford University Press</publisher-name> <publisher-loc>Oxford</publisher-loc>; <year>1998</year>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>O&#x2019;Donohue</surname> <given-names>TL</given-names></string-name>, <string-name><surname>Millington</surname> <given-names>WR</given-names></string-name>, <string-name><surname>Handelmann</surname> <given-names>GE</given-names></string-name>, <string-name><surname>Contreras</surname> <given-names>PC</given-names></string-name>, <string-name><surname>Chronwall</surname> <given-names>BM</given-names></string-name>. <article-title>On the 50th anniversary of Dale&#x2019;s law: multiple neurotransmitter neurons</article-title>. <source>Trends in Pharmacological Sciences</source>. <year>1985</year>;<volume>6</volume>(<issue>0</issue>):<fpage>305</fpage> &#x2013; <lpage>308</lpage>. doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1016/0165-6147(85)90141-5">http://dx.doi.org/10.1016/0165-6147(85)90141-5</ext-link>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Salinas</surname> <given-names>E</given-names></string-name>, <string-name><surname>Abbott</surname> <given-names>L</given-names></string-name>. <article-title>Transfer of coded information from sensory to motor networks</article-title>. <source>The Journal of Neuroscience</source>. <year>1995</year>;<volume>15</volume>(<issue>10</issue>):<fpage>6461</fpage>&#x2013;<lpage>6474</lpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Galletti</surname> <given-names>C</given-names></string-name>, <string-name><surname>Battaglini</surname> <given-names>P</given-names></string-name>, <string-name><surname>Fattori</surname> <given-names>P</given-names></string-name>. <article-title>Functional Properties of Neurons in the Anterior Bank of the Parieto-occipital Sulcus of the Macaque Monkey</article-title>. <source>European Journal of Neuroscience</source>. <year>1991</year>;<volume>3</volume>(<issue>5</issue>):<fpage>452</fpage>&#x2013;<lpage>461</lpage>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Spratling</surname> <given-names>MW</given-names></string-name>. <article-title>Learning posture invariant spatial representations through temporal correlations</article-title>. <source>Autonomous Mental Development, IEEE Transactions on</source>. <year>2009</year>;<volume>1</volume>(<issue>4</issue>):<fpage>253</fpage>&#x2013;<lpage>263</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Merriam</surname> <given-names>EP</given-names></string-name>, <string-name><surname>Gardner</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Movshon</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name>. <article-title>Modulation of visual responses by gaze direction in human visual cortex</article-title>. <source>Journal of Neuroscience</source>. <year>2013</year>;<volume>33</volume>(<issue>24</issue>):<fpage>9879</fpage>&#x2013;<lpage>9889</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Daddaoua</surname> <given-names>N</given-names></string-name>, <string-name><surname>Dicke</surname> <given-names>P</given-names></string-name>, <string-name><surname>Thier</surname> <given-names>P</given-names></string-name>. <article-title>Eye position information is used to compensate the consequences of ocular torsion on V1 receptive fields</article-title>. <source>Nature communications</source>. <year>2014</year>;<volume>5</volume>:<fpage>3047</fpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Rolls</surname> <given-names>ET</given-names></string-name>, <string-name><surname>Stringer</surname> <given-names>SM</given-names></string-name>. <article-title>Invariant object recognition in the visual system with error correction and temporal difference learning</article-title>. <source>Network (Bristol, England)</source>. <year>2001</year>;<volume>12</volume>(<issue>2</issue>):<fpage>111</fpage>&#x2013;<lpage>129</lpage>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>F&#x00F6;ldi&#x00E1;k</surname> <given-names>P</given-names></string-name>. <article-title>Learning invariance from transformation sequences</article-title>. <source>Neural Computation</source>. <year>1991</year>;<volume>3</volume>(<issue>2</issue>):<fpage>194</fpage>&#x2013;<lpage>200</lpage>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Wallis</surname> <given-names>G</given-names></string-name>, <string-name><surname>Rolls</surname> <given-names>ET</given-names></string-name>. <article-title>Invariant face and object recognition in the visual system</article-title>. <source>Progress in neurobiology</source>. <year>1997</year>;<volume>51</volume>(<issue>2</issue>):<fpage>167</fpage>&#x2013;<lpage>194</lpage>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Freedman</surname> <given-names>EG</given-names></string-name>, <string-name><surname>Sparks</surname> <given-names>DL</given-names></string-name>. <article-title>Eye-head coordination during head-unrestrained gaze shifts in rhesus monkeys</article-title>. <source>Journal of Neurophysiology</source>. <year>1997</year>;<volume>77</volume>(<issue>5</issue>):<fpage>2328</fpage>&#x2013;<lpage>2348</lpage>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="book"><string-name><surname>Dayan</surname> <given-names>P</given-names></string-name>, <string-name><surname>Abbott</surname> <given-names>LF</given-names></string-name>. <source>Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems</source>. <publisher-name>MIT Press</publisher-name>, ISBN <isbn>0-262-04199-5</isbn>; <year>2001</year>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Rolls</surname> <given-names>ET</given-names></string-name>. <article-title>Invariant visual object and face recognition: neural and computational bases, and a model, VisNet</article-title>. <source>Frontiers in Computational Neuroscience</source>. <year>2012</year>;<volume>6</volume>(<issue>35</issue>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Royer</surname> <given-names>S</given-names></string-name>, <string-name><surname>Par&#x00E9;</surname> <given-names>D</given-names></string-name>. <article-title>Conservation of total synaptic weight through balanced synaptic depression and potentiation</article-title>. <source>Nature</source>. <year>2003</year>;<volume>422</volume>(<issue>6931</issue>):<fpage>518</fpage>&#x2013;<lpage>522</lpage>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Oja</surname> <given-names>E</given-names></string-name>. <article-title>Simplified neuron model as a principal component analyzer</article-title>. <source>Journal of mathematical biology</source>. <year>1982</year>;<volume>15</volume>(<issue>3</issue>):<fpage>267</fpage>&#x2013;<lpage>273</lpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="book"><string-name><surname>Kohonen</surname> <given-names>T</given-names></string-name>. <source>Self-organization and associative memory</source>. vol. <volume>8</volume>. <publisher-name>Springer Science &#x0026; Business Media</publisher-name>; <year>2012</year>.</mixed-citation></ref>
</ref-list>
</back>
</article>