<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/138669</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Bioinformatics</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Exploiting general independence criteria for network inference</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Verbyla</surname>
<given-names>Petras</given-names>
</name>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Desgranges</surname>
<given-names>Nina</given-names>
</name>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1998-492X</contrib-id>
<name>
<surname>Richardson</surname>
<given-names>Sylvia</given-names>
</name>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3113-7942</contrib-id>
<name>
<surname>Wernisch</surname>
<given-names>Lorenz</given-names>
</name>
</contrib>
<aff id="a1"><institution>MRC Biostatistics Unit, Cambridge Biomedical Campus, Cambridge Institute of Public Health, Forvie Site, Robinson Way</institution>, Cambridge CB2 0SR, <country>UK</country></aff>
</contrib-group>
<author-notes>
<fn><p>CONTACT Petras Verbyla Email: <email>petras.verbyla@mrc-bsu.cam.ac.uk</email></p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2017</year>
</pub-date>
<elocation-id>138669</elocation-id>
<history>
<date date-type="received">
<day>16</day>
<month>5</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>17</day>
<month>5</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="138669.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>ABSTRACT</title>
<p>Inference of networks representing dependency relationships is a key tool for understanding data derived from biological systems. It has been shown that nonlinear relationships and non-Gaussian noise aid detection of directions of functional dependencies. In this study we explore how far generalised independence criteria for statistical independence proposed in the literature are better suited to the inference of networks compared to standard independence criteria based on linear relationships and Gaussian noise. We compare such criteria within the framework of the PC algorithm, a popular network inference algorithm for directed acyclic dependency graphs. We also propose and evaluate a method to apply unconditional independence criteria to assess conditional independence and a method to simulate data with desired properties from experimental data. Our main finding is that a recently proposed criterion based on distance covariance performs well compared to other independence criteria in terms of error rates, speed of computation, and need of fine-tuning parameters when applied to experimental biological datasets.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>KEYWORDS</title>
<kwd>Independence Criteria</kwd>
<kwd>HSIC</kwd>
<kwd>Distance Covariance</kwd>
<kwd>PC algorithm</kwd>
</kwd-group>
<counts>
<page-count count="27"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<label>1.</label>
<title>Introduction</title>
<sec id="s1a">
<label>1.1.</label>
<title>Motivation</title>
<p>Biological systems are driven by complex regulatory processes. In the analysis and reconstruction of such processes graphical models play a crucial role. Using network inference algorithms it is possible to derive regulatory models from high-throughput data, for example, from gene or protein expression data. A wide variety of network inference algorithms have been designed and implemented and necessitate common platforms for assessment, for example, the DREAM network inference challenges [<xref ref-type="bibr" rid="c11">11</xref>], to provide objective means for choosing reliable inference algorithms.</p>
<p>Inference algorithms are based on a variety of statistical principles. However, most rely on some form of estimating or testing the similarity or correlation between genes, for example, GeneNet [<xref ref-type="bibr" rid="c15">15</xref>] and MutRank [<xref ref-type="bibr" rid="c14">14</xref>], or on mutual information as does CLR [<xref ref-type="bibr" rid="c3">3</xref>] and RelNet [<xref ref-type="bibr" rid="c2">2</xref>], or on regression and feature selection as does MRNET [<xref ref-type="bibr" rid="c13">13</xref>] and Genie3 [<xref ref-type="bibr" rid="c9">9</xref>].</p>
<p>Using independence criteria for network inference have been suggested in contexts outside biological networks [<xref ref-type="bibr" rid="c10">10</xref>] and form the basis of the classic PC algorithm and its many variations [<xref ref-type="bibr" rid="c16">16</xref>]. Such algorithms exploit d-separation, the equivalent of statistical independence for graph structures, for the inference of directed acyclic dependency graphs (see for example, [<xref ref-type="bibr" rid="c16">16</xref>]).</p>
<p>Linear dependencies and Gaussian noise are typically assumed for most applications of the PC algorithm to continuous data. However, such assumptions are likely to be too restrictive in the case of many experimental datasets. Moreover, as a series of studies have shown (for example, [<xref ref-type="bibr" rid="c8">8</xref>]), the simplifying assumptions of linearity and Gaussian noise make it even more difficult to establish functional dependencies and their directions. They constitute limiting cases where for two dependent variables, for example, it becomes impossible to infer the direction of their functional dependency. These arguments strongly suggest that inference algorithms based on statistical independence should exploit nonlinear dependencies and non-Gaussian noise. The idea of combining the PC algorithm with a generalised independence criterion, the Hilbert-Schmidt Independence Criterion or HSIC, as independence oracle for conditional independence was proposed in [<xref ref-type="bibr" rid="c26">26</xref>] but not made operational.</p>
<p>The contribution of our study is threefold. First, we compare the performance of several independence criteria on biological experimental data. In particular, we compare the linear-Gaussian, the HSIC, and a further criterion based on distance, the Distance Covariance Criterion or DCC [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref>], within the framework of the PC algorithm, when applied to protein expression data. Second, since not all criteria are available in a version that allows for testing conditional independence, we propose and test an approach that relies on residuals and requires only an unconditional version of an independence criterion. Third, the true network is rarely known when assessing algorithms. Hence, we also propose a simulation method that, starting from experimental data and a target network, produces simulated data according to the dependency structure of a target network but which are otherwise as close to the original data as possible in their noise characteristics and functional (possibly nonlinear) forms of dependencies. We demonstrate that such simulated dataset can be used successfully for differentiating between the performance of independence criteria for network inference. Finally, we make all algorithms and data available as a package for the R statistical environment [<xref ref-type="bibr" rid="c17">17</xref>].</p>
<p>We emphasise that the current study is not proposing a new inference algorithm and does not attempt to compare the performance of the PC algorithm to other network inference algorithms. Our aim is rather to compare and assess the relative merits of various independence criteria for network inference within the framework of a typical inference approach such as the PC algorithm and to explore problems and suggest solutions for their implementation and application to experimental data.</p>
<p>In the following sections we describe a representative dataset. We then provide some background on the inference of directed acyclic graphs using statistical independence as well as two generalised independence criteria. In the Results section their performance is compared on simulated as well as the original data.</p>
</sec>
<sec id="s1b">
<label>1.2.</label>
<title>Data</title>
<p>In order to assess general independence criteria for the inference of biological regulatory networks, we turn to a well studied dataset on protein expression from Sachs et al. [<xref ref-type="bibr" rid="c20">20</xref>]. The study comprises eight experimental datasets of single cell measurements. Each dataset reports the expression level of eleven proteins: RAF, MEK, ERK (aka P44.42), PLC<italic>&#x03B3;</italic>, PIP2, PIP3, PKC, AKT, PKA, JNK, P38. The number of observations (cells) varies from 700 to 900 cells per dataset. Each dataset is characterised by the quantitative value of protein expression response to a specific stimulatory cue or an inhibitory intervention (listed in <xref ref-type="table" rid="tblA1">Table A1</xref> in the supplementary material).</p>
<p>Protein expression levels were obtained by flow cytometry [<xref ref-type="bibr" rid="c20">20</xref>] measuring modification states of proteins, such as phosphorylation through antibodies. These are single cell measurements with each cell representing an independent observation. Only a few protein modifications are monitored. For example, PKC phosphorylates RAF at S497, S499, S259, however, only antibodies for RAF S259 were available. Consequently, some dependencies between protein states might be missed. Despite these shortcomings, some links between proteins are well established and shown in <xref ref-type="fig" rid="fig1">Figure 1</xref> according to [<xref ref-type="bibr" rid="c20">20</xref>].</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.:</label>
<caption><p>Summary of known dependencies (after [<xref ref-type="bibr" rid="c20">20</xref>]).</p></caption>
<graphic xlink:href="138669_fig1.tif"/>
</fig>
<p>The graph in <xref ref-type="fig" rid="fig1">Figure 1</xref> serves a twofold purpose. First, by exploiting the graph structure and by resampling the data, as described in more detail in the Results section, we generate datasets with characteristics close to real experimental datasets, but with known dependencies. They will serve as test sets for comparing the performance of inference algorithms. Second, using the original datasets, the graph provides a gold standard for measuring performance of algorithms on experimental data. Several limitations need to be kept in mind though. For some edges the direction of causal influence remains ambiguous. All inference algorithms considered here are based on the assumption that the dependency structure can be represented by a directed acyclic graph. Such assumption is likely to hold only approximately true in real datasets. Also the selection of proteins is by no means complete and it is likely that latent, unobserved variables induce additional dependencies. However, since we are primarily interested in a comparison of performances of independence criteria, these limitations are less problematic for the present study. There is no reason to belief that these inaccuracies in the knowledge of the true network should favor one approach unfairly over another. Conclusions about the relative merits of independence criteria based on the current datasets should generalise well to other data.</p>
<p><xref ref-type="fig" rid="figA1">Figure A1</xref> in the supplementary material shows boxplots for each variable and pairwise scatterplots between variables of dataset 8. Some dependencies are clear and close to linear, for example, RAF to MEK. Other dependencies are far less obvious, for example, RAF to PKC. This pattern of clear marginal dependencies between some of the related protein pairs but not all of them, is present in all eight datasets, as a reminder that network inference is not easily reducible to simple marginal correlation.</p>
</sec>
<sec id="s1c">
<label>1.3.</label>
<title>Probabilistic graphical models</title>
<p>We recall some terminology for probabilistic graphical models. For a full and comprehensive introduction see, for example, [<xref ref-type="bibr" rid="c23">23</xref>]. A graph <bold>G</bold> = (<bold>V, E</bold>) has vertices <bold>V</bold> and edges <bold>E</bold> &#x2286; <bold>V</bold> <italic>&#x00D7;</italic> <bold>V</bold>. An edge between two nodes <italic>V</italic><sub>1</sub> and <italic>V</italic><sub>2</sub> can be either undirected, symbolically <italic>V</italic><sub>1</sub> &#x2212; <italic>V</italic><sub>2</sub>, or directed, symbolically <italic>V</italic><sub>1</sub> &#x2192; <italic>V</italic><sub>2</sub>. For a directed edge <italic>V</italic><sub>1</sub> <italic>&#x2192; V</italic><sub>2</sub>, <italic>V</italic><sub>1</sub> is a parent of <italic>V</italic><sub>2</sub> and <italic>V</italic><sub>2</sub> is a child of <italic>V</italic><sub>1</sub>. Two vertices connected by an edge or directed edge are adjacent. For a set of vertices <bold>W</bold> the set of all parents is denoted by Pa<sub><bold>G</bold></sub>(<bold>W</bold>). The degree <italic>d</italic>(<italic>V</italic>) of a node <italic>V</italic> is the number of nodes adjacent to <italic>V</italic>. A sequence of nodes (<italic>V</italic><sub>1</sub>, &#x2026;, <italic>V</italic><sub><italic>n</italic></sub>), <italic>V</italic><sub><italic>i</italic></sub> <italic>&#x2208;</italic> <bold>V</bold>, forms a path if neighbouring nodes are connected. A directed path has all its edges directed in the same direction. A path is a cycle if (<italic>V</italic><sub>1</sub> = <italic>V</italic><sub><italic>n</italic></sub>).</p>
<p>A <italic>directed graph</italic> contains only directed edges. It is a <italic>directed acyclic graph</italic> (DAG) if it contains no directed cycles. A graph is <italic>undirected</italic> if it contains only undirected edges and <italic>mixed</italic> if it contains both types of edges. A DAG is <italic>compatible with a mixed graph</italic> if the graphs agree on the directed edges. A <italic>collider</italic> (v-structure) is a triplet &#x2329;<italic>V</italic><sub>1</sub>, <italic>V</italic><sub>2</sub>, <italic>V</italic><sub>3</sub>&#x232A; &#x2282; <bold>V</bold> such that {<italic>V</italic><sub>1</sub>, <italic>V</italic><sub>3</sub>} &#x2208; Pa<sub><bold>G</bold></sub>(<italic>V</italic><sub>2</sub>) and (<italic>V</italic><sub>1</sub>, <italic>V</italic><sub>3</sub>) <italic>&#x2209;</italic> <bold>E</bold>.</p>
<p>In a probabilistic graphical model the nodes <bold>V</bold> are associated with random variables with a joint probability distribution <bold>P</bold>. We will denote the random variables and the corresponding vertices by the same identifiers. In this study we are concerned with continuous variables and hence assume <bold>P</bold> is continuous and that it has a density <italic>f</italic>. For sets <bold>X, Y, Z</bold> of variables with conditional probability densities <italic>f</italic> (<bold>X, Y</bold> | <bold>Z</bold>), <italic>f</italic> (<bold>X</bold> | <bold>Z</bold>), and <italic>f</italic> (<bold>Y</bold> | <bold>Z</bold>), <bold>X</bold> and <bold>Y</bold> are conditionally independent given <bold>Z</bold> (denoted by <bold>X</bold> &#x22A5;<sub><italic>P</italic></sub> <bold>Y</bold> | <bold>Z</bold>), if <italic>f</italic> (<bold>X, Y</bold> | <bold>Z</bold>) = <italic>f</italic> (<bold>X</bold> | <bold>Z</bold>)<italic>f</italic> (<bold>Y</bold> | <bold>Z</bold>). A probability distribution <bold>P</bold> over the node set <bold>V</bold> is called <italic>Markov</italic> with respect to a DAG <bold>G</bold> if it permits the factorization
<disp-formula id="eqn1"><alternatives><graphic xlink:href="138669_eqn1.gif"/></alternatives></disp-formula></p>
<p>Symbolically, <bold>X</bold> &#x22A5;<sub><italic>G</italic></sub> <bold>Y</bold> | <bold>Z</bold> indicates that for a graph <bold>G</bold>, which is associated with a set of random variables <bold>V</bold>, and vertex sets <bold>X, Y, Z</bold> &#x2208; <bold>V</bold>, factorization (1) implies that <bold>X</bold> is independent of <bold>Y</bold> conditioned on <bold>Z</bold>. With this notation in place we see that <bold>P</bold> is Markov with respect to <bold>G</bold> if <bold>X</bold> &#x22A5;<sub><italic>G</italic></sub> <bold>Y</bold> | <bold>Z</bold> implies <bold>X</bold> &#x22A5;<sub><italic>P</italic></sub> <bold>Y</bold> | <bold>Z</bold> for all sets <bold>X, Y</bold>, and <bold>Z</bold>, that is, all independencies implied by <bold>G</bold> are in <bold>P</bold>. If, conversely, <bold>X</bold> &#x22A5;<sub><italic>P</italic></sub> <bold>Y</bold> | <bold>Z</bold> implies <bold>X</bold> &#x22A5;<sub><italic>G</italic></sub> <bold>Y</bold> | <bold>Z</bold>, that is, all independencies of <bold>P</bold> are implied by <bold>G, P</bold> is <italic>faithful</italic> to <bold>G</bold>. That is, if a distribution <bold>P</bold> is Markov and faithful with respect to a graph <bold>G</bold>, the graph represents all and only the independencies of <bold>P</bold> as captured in factorization (1). We will also say that <bold>G</bold> <italic>represents</italic> <bold>P</bold> <italic>faithfully</italic> in this case. Two DAGs are <italic>Markov equivalent</italic> if each distribution is Markov to either both or none of them. Consequently, in this case the two graphs are indiscernible based on probabilistic independency relationships alone.</p>
</sec>
<sec id="s1d">
<label>1.4.</label>
<title>PC algorithm</title>
<p>The PC algorithm [<xref ref-type="bibr" rid="c23">23</xref>] is a constraint based method for finding a DAG <bold>G</bold> that represents <bold>P</bold> faithfully. More precisely, if such graph exists, the algorithm returns a <italic>partially directed graph</italic> (PDAG), a mixed graph so that there exists at least one DAG which is compatible with the PDAG and represents <bold>P</bold> faithfully. The PDAG also has the property that for any undirected edge there exist two DAGs compatible with the PDAG, both representing <bold>P</bold> faithfully, but with the edge oriented in opposite directions. In this sense the PDAG is <italic>maximally oriented</italic>. If <bold>P</bold> can be represented faithfully by a DAG <bold>G</bold> and if there exists an independence oracle that returns, for each query triplet <bold>X, Y, Z</bold>, whether <bold>X</bold> &#x22A5;<sub><italic>P</italic></sub> <bold>Y</bold> | <bold>Z</bold> or not, [<xref ref-type="bibr" rid="c23">23</xref>] show that the PC algorithm returns a maximally oriented PDAG that is compatible with <bold>G</bold>.</p>
<p>The PC algorithm consists of three distinct phases. The first (skeleton) phase finds the skeleton of the PDAG, that is, it finds all adjacencies based on an independence oracle. The second (collider) phase finds all colliders (<italic>V</italic><sub><italic>i</italic></sub>, <italic>V</italic><sub><italic>k</italic></sub>, <italic>V</italic><sub><italic>j</italic></sub>) and directs edges <italic>V</italic><sub><italic>i</italic></sub> &#x2192; <italic>V</italic><sub><italic>k</italic></sub> and <italic>V</italic><sub><italic>j</italic></sub> &#x2192; <italic>V</italic><sub><italic>k</italic></sub>. The third (transitive) phase applies the Meek rules [<xref ref-type="bibr" rid="c12">12</xref>] to extend all directions found in the collider phase to the rest of the PDAG ([<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c26">26</xref>] give a succinct description of details).</p>
<p>In addition, the PDAG can be extended using background knowledge about the direction of some of its undirected edges. If the background knowledge is compatible with the underlying distribution <bold>P</bold>, iterative application of an extended set of Meek rules results in an a PDAG-K (PDAG with background <italic>knowledge</italic>) that again is maximally oriented in the sense that for each undirected edge there exist two DAGs compatible with the PDGA-K and both representing <bold>P</bold> faithfully as well as agreeing with the background knowledge, but which contain opposite directions of the edge (see [<xref ref-type="bibr" rid="c12">12</xref>] for details).</p>
<p>In practical applications with finite samples an independence oracle is usually unavailable and is replaced by a statistical test of independence. Throughout the paper we use tests (described in detail in the next section) that allow us to reject the null hypothesis of independence in terms of <italic>p</italic>-values, that is, low <italic>p</italic>-values indicate dependencies. A consequence of sampling error is that the PC algorithm might not produce the correct PDAG, but might instead produce a graph with under or over predicted edges, wrong directions of edges, doubly directed edges directed in both directions, and even cycles. In most circumstances it is appropriate for the algorithm to return such ambiguous graph and leave it to the user to decide which edges or directions to ignore.</p>
<p>A fixed cutoff level <italic>&#x03B1;</italic> is used for all decisions on independence in the PC algorithm. Since edges indicate dependencies, higher levels of <italic>&#x03B1;</italic> usually result in the acceptance of more edges and denser graphs.</p>
<sec id="s1d1">
<label>1.4.1.</label>
<title>Additive Noise model</title>
<p>Tests for conditional independence are based on certain assumptions. A popular one is that dependencies between a child and its parents can be modelled by a linear function with additive independent Gaussian noise. Unfortunately, apart from being rarely fulfilled in practise, these assumptions make it actually more difficult to identify directions of influence between variables. Several authors, for example, [<xref ref-type="bibr" rid="c8">8</xref>] or [<xref ref-type="bibr" rid="c26">26</xref>], therefore assume more general noise models. In <italic>additive noise models</italic> the dependency of a variable on its parents is modelled by a nonlinear function and additive noise (not necessarily Gaussian), that is, <italic>V</italic><sub><italic>i</italic></sub> = <italic>f</italic><sub><italic>i</italic></sub>(Pa<sub><bold>G</bold></sub>(<bold>V<sub>i</sub></bold>)) &#x002B; <italic>&#x0454;</italic><sub><italic>i</italic></sub>, with <italic>&#x0454;</italic><sub><italic>i</italic></sub> independent for each <italic>i</italic>.</p>
<p>If data are generated by such process the assumption of an additive noise model, which includes the linear Gaussian noise case, has a twofold advantage. First, independence tests used in the PC algorithm based on nonlinearity and general noise distributions are more accurate than tests based on assumptions of linearity or Gaussian noise distributions resulting in a more accurate PDAG. Second, exploiting the inherent improvement in establishing the correct direction for undirected edges, more edges can be directed than through consideration of independence relationships alone (as in the PDAG). Meek&#x2019;s rules [<xref ref-type="bibr" rid="c12">12</xref>] for background knowledge can then be applied to obtain a PDAG-K to direct additional edges of the PDAG. This is achieved in the <italic>generalised transitive phase</italic> algorithm in [<xref ref-type="bibr" rid="c26">26</xref>] who show that it results in maximally directed mixed graphs for a slightly wider class of models than just additive noise models.</p>
</sec>
</sec>
</sec>
<sec id="s2">
<label>2.</label>
<title>Methods</title>
<p>Our aim is to employ the PC algorithm and to apply a generalised transitive orientation phase to obtain a PDAG-K representing a probability distribution according to the additive noise model. For this purpose we need to specify an independence oracle that is suitable for nonlinear relationships and non-Gaussian noise. In the following we provide a summary of two criteria, the <italic>Hilbert-Schmidt Independence Criterion</italic> or HSIC and the <italic>Distance Covariance Criterion</italic> or DCC, and describe our implementations.</p>
<p>[<xref ref-type="bibr" rid="c21">21</xref>] show that the DCC is actually a version of the HSIC for a specific kernel. However, here we apply the HSIC with the widely used squared exponential kernel as explained in the following. In a way, comparing the HSIC with the DCC in this study is comparing the HSIC with two very different kernels, one motivated by a popular choice of the kernel function, the other rather indirectly via a distance correlation approach. However, the computational requirements and implementation issues of HSIC (with a squared exponential kernel) and of DCC are very different, as we discuss now.</p>
<p>We assume we have <italic>n</italic> samples <italic>v</italic><sub><italic>i</italic></sub> from a set of variables <bold>V</bold> sampled from a distribution <bold>P</bold>. We are interested in establishing independence of subsets of variables <bold>X</bold> <italic>&#x2286;</italic> <bold>V</bold> and <bold>Y</bold> <italic>&#x2286;</italic> <bold>V</bold> or their independence conditioned on variables <bold>Z</bold> <italic>&#x2286;</italic> <bold>V</bold>. We denote the measurements corresponding to <bold>X, Y</bold>, and <bold>Z</bold> for sample <italic>i</italic> by <italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>, and <italic>z</italic><sub><italic>i</italic></sub>.</p>
<p>As outlined above the PC algorithm requires an independence oracle that states whether <bold>X</bold> <italic>&#x22A5;</italic><sub><italic>P</italic></sub> <bold>Y</bold> or <bold>X</bold> <italic>&#x22A5;</italic><sub><italic>P</italic></sub> <bold>Y</bold> | <bold>Z</bold> based on samples <italic>v</italic><sub>1</sub>, &#x2026;, <italic>v</italic><sub><italic>n</italic></sub>. [<xref ref-type="bibr" rid="c26">26</xref>] suggest using the HSIC as independence oracle for the PC algorithm. In the following we will compare the performance of the two independence criteria, the <italic>Hilbert-Schmidt Independence Criterion</italic> or HSIC and the <italic>Distance Covariance Criterion</italic> or DCC. We call the PC algorithm based on HSIC, following [<xref ref-type="bibr" rid="c26">26</xref>], <italic>kernel-PC</italic> or kPC, and, in analogy, the PC algorithm based on DCC, <italic>distance-PC</italic> or dPC. We will define further variants of the kPC below.</p>
<sec id="s2a">
<label>2.1.</label>
<title>Hilbert-Schmidt Independence Criterion</title>
<p>For a comprehensive introduction to the HSIC see for example [<xref ref-type="bibr" rid="c22">22</xref>] or [<xref ref-type="bibr" rid="c4">4</xref>]. For our purposes it is sufficient to describe the calculation of the HSIC statistic for a finite sample {(<italic>x</italic><sub>1</sub>, <italic>y</italic><sub>1</sub>), &#x2026;, (<italic>x</italic><sub><italic>n</italic></sub>, <italic>y</italic><sub><italic>n</italic></sub>)}. The HSIC is based on a <italic>kernel</italic> function, a similarity function between sample points. As kernel function we use a Gaussian kernel <inline-formula><alternatives><inline-graphic xlink:href="138669_inline1.gif"/></alternatives></inline-formula>, where the kernel width <italic>&#x03BB;</italic> is a parameter that needs to be carefully selected (see the Results section). Let <italic>K</italic> and <italic>L</italic> be the Gram matrices associated with kernel functions <italic>k</italic> and <italic>l</italic>, that is <italic>K</italic><sub><italic>i,j</italic></sub> = <italic>k</italic>(<italic>x</italic><sub><italic>i</italic></sub>, <italic>x</italic><sub><italic>j</italic></sub>) and <italic>L</italic><sub><italic>i,j</italic></sub> = <italic>l</italic>(<italic>y</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>j</italic></sub>). The centred Gram matrices are <inline-formula><alternatives><inline-graphic xlink:href="138669_inline2.gif"/></alternatives></inline-formula>, where <inline-formula><alternatives><inline-graphic xlink:href="138669_inline3.gif"/></alternatives></inline-formula> (here I<sub><italic>n</italic></sub> is the <italic>n</italic>-dimensional identity matrix and <bold>1</bold><sub><italic>n</italic></sub> is a vector of ones of length <italic>n</italic>). An estimate <inline-formula><alternatives><inline-graphic xlink:href="138669_inline4.gif"/></alternatives></inline-formula> of the HSIC is then given by
<disp-formula id="eqn2"><alternatives><graphic xlink:href="138669_eqn2.gif"/></alternatives></disp-formula>
where tr(<italic>A</italic>) is the trace (sum of diagonal elements) of a matrix <italic>A</italic>. Generally, <italic>H</italic> (<italic>x, y</italic>) is close to 0 when <italic>X</italic> and <italic>Y</italic> are independent. [<xref ref-type="bibr" rid="c4">4</xref>] also provide an estimator for a <italic>conditional</italic> version <inline-formula><alternatives><inline-graphic xlink:href="138669_inline5.gif"/></alternatives></inline-formula> of the HSIC for a sample set (<italic>x</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>, <italic>z</italic><sub><italic>i</italic></sub>), <italic>i</italic> = 1, &#x2026;, <italic>n</italic> as
<disp-formula id="eqn3"><alternatives><graphic xlink:href="138669_eqn3.gif"/></alternatives></disp-formula>
where <inline-formula><alternatives><inline-graphic xlink:href="138669_inline6.gif"/></alternatives></inline-formula>, and <inline-formula><alternatives><inline-graphic xlink:href="138669_inline7.gif"/></alternatives></inline-formula> are defined as above for <italic>x</italic>, and <italic>y</italic>, and <inline-formula><alternatives><inline-graphic xlink:href="138669_inline8.gif"/></alternatives></inline-formula> is the analogous Gram matrix for z. &#x2208; is a regularization parameter that needs to be carefully selected (see the Results section). The calculation of <inline-formula><alternatives><inline-graphic xlink:href="138669_inline9.gif"/></alternatives></inline-formula> is computationally very expensive, but some simplifications are introduced in [<xref ref-type="bibr" rid="c26">26</xref>].</p>
<p>If we use the PC algorithm with the HSIC as independence oracle we obtain algorithm kPC.</p>
</sec>
<sec id="s2b">
<label>2.2.</label>
<title>Tests of (unconditional) independence for kPC</title>
<p>[<xref ref-type="bibr" rid="c5">5</xref>] suggest two ways of calculating a <italic>p</italic>-value for the HSIC statistic (2), a permutation test and a test based on an approximation using the Gamma distribution.</p>
<p><bold>Permutation test</bold>. The first test is a simple permutation test where <italic>r</italic> permutations of the form <inline-formula><alternatives><inline-graphic xlink:href="138669_inline10.gif"/></alternatives></inline-formula>, <italic>j</italic> = 1, <italic>&#x2026;, r</italic>, for permutations <italic>&#x03C1;</italic><sub><italic>j</italic></sub> of sample indices are created. The proportion of permutations <italic>&#x03C1;</italic><sub><italic>j</italic></sub> for which the HSIC estimator (2) is larger than the HSIC of the original dataset, that is <italic>H</italic> (<italic>x, y</italic><sub>(<italic>j</italic>)</sub>) <italic>&#x003E; H</italic> (<italic>x, y</italic>), is an estimate of the <italic>p</italic>-value for rejecting the null hypothesis of independence. The underlying assumption is that permuting <italic>y</italic> removes any dependency between <italic>x</italic> and <italic>y</italic>.</p>
<p>Computing <italic>H</italic> (<italic>x, y</italic>) is expensive with complexity <italic>O</italic>(<italic>n</italic><sup>3</sup>), <italic>n</italic> the sample size. [<xref ref-type="bibr" rid="c26">26</xref>] suggested an incomplete Cholesky factorization with <italic>m</italic> steps to reduce the complexity to <italic>O</italic>(<italic>nm</italic><sup>3</sup>) for a chosen <italic>m &#x003C; n</italic>. That is, <italic>K</italic> and <italic>L</italic> are approximated by <inline-formula><alternatives><inline-graphic xlink:href="138669_inline11.gif"/></alternatives></inline-formula>, and the matrix of eigenvalues <italic>D</italic><sub><italic>x</italic></sub> of size <italic>n &#x00D7; m</italic> and <italic>m &#x00D7; m</italic>, respectively. Similarly <inline-formula><alternatives><inline-graphic xlink:href="138669_inline12.gif"/></alternatives></inline-formula>. This factorization is suitable since, with quickly decaying kernel functions, Gram matrices often have a low effective rank. Instead of permuting the entries of <italic>y</italic> and recalculating the Gram matrix, we exploit the one-to-one relationship between samples <italic>y</italic><sub><italic>i</italic></sub> and the rows and columns of the Gram matrix <italic>L</italic>, since <italic>L</italic>(<italic>i, j</italic>) = <italic>l</italic>(<italic>y</italic><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>j</italic></sub>) (with a symmetric kernel function <italic>l</italic>). In the calculation of <italic>H</italic> (<italic>x, y</italic><sub>(<italic>i</italic>)</sub>) we use <inline-formula><alternatives><inline-graphic xlink:href="138669_inline13.gif"/></alternatives></inline-formula>, with a permutation matrix <italic>P</italic><sub><italic>j</italic></sub> permuting rows according to permutation <italic>&#x03C1;</italic><sub><italic>j</italic></sub>. This means that an incomplete Cholesky decomposition needs to be performed only once for all permuted datasets <italic>y</italic><sub>(<italic>i</italic>)</sub>, and consequent values <italic>H</italic> (<italic>x, y</italic><sub>(<italic>i</italic>)</sub>) can be obtained simply by permuting coordinates of the eigenvectors in <italic>U</italic><sub><italic>y</italic></sub> (<inline-formula><alternatives><inline-graphic xlink:href="138669_inline14.gif"/></alternatives></inline-formula> is kept the same).</p>
<p><bold>Gamma Test</bold>. The value of the asymptotic distribution of the empirical estimate <italic>H</italic> (<italic>x, y</italic>) of the HSIC under the null hypothesis of independence is approximated by a Gamma distribution: <italic>H</italic> (<italic>x, y</italic>) <italic>&#x223C;</italic> Gam(<italic>&#x03B1;, &#x03B8;</italic>) where <italic>&#x03B1;</italic> is the shape parameter and <italic>&#x03B8;</italic> is the scale parameter calculated as
<disp-formula><alternatives><graphic xlink:href="138669_ueqn1.gif"/></alternatives></disp-formula></p>
<p>To compute this distribution we use estimates of the mean and the variance from sample points under the null hypothesis as provided by theorems 3 and 4 in [<xref ref-type="bibr" rid="c5">5</xref>]. The <italic>p</italic>-value is then obtained as upper-tail quantile of <italic>H</italic> (<italic>x, y</italic>).</p>
</sec>
<sec id="s2c">
<label>2.3.</label>
<title>Test of conditional independence for kPC</title>
<p>In the PC algorithm an oracle for conditional independence <bold>X</bold> &#x22A5;<sub><italic>P</italic></sub> <bold>Y</bold> | <bold>Z</bold> is required. We explore two approaches. The first, <italic>permutation-cluster test</italic> suggested by [<xref ref-type="bibr" rid="c26">26</xref>], is based on a conditional version of HSIC from [<xref ref-type="bibr" rid="c4">4</xref>]. The alternative test we propose here is based on <italic>residuals</italic>. It is simpler in that it only requires an unconditional version of the HSIC and can be readily applied to other independence criteria for which there is no conditional version readily available that allows integration of a conditioning set of variables, as in the case of the DCC.</p>
<p><bold>Permutation-cluster test</bold>. As suggested in [<xref ref-type="bibr" rid="c26">26</xref>], in order to obtain a <italic>p</italic>-value for rejecting independence based on the estimator (3) for conditional HSIC criterion, the samples are clustered according to the Euclidean distance between the <italic>z</italic> coordinates of samples. Sample labels of <italic>y</italic> are only permuted within each cluster, thus ensuring that the permuted samples break dependency between <italic>x</italic> and <italic>y</italic> for an approximately fixed <italic>z</italic> but retain their dependency on <italic>z</italic>. For the clustering we use a k-means algorithm [[<xref ref-type="bibr" rid="c6">6</xref>]] (R function kmeans()). A larger number of clusters is desirable to achieve an almost constant <italic>z</italic> within each cluster. On the other hand, enough samples are required in each cluster to achieve a permutation of labels that breaks any conditional dependency between <italic>x</italic> and <italic>y</italic>. For the sample sizes considered here, good results were obtained with a constant cluster number of 10.</p>
<p><bold>Residuals test</bold>. As a simpler alternative to obtain <italic>p</italic>-values for the conditional HSIC we propose to test residuals for independence based on any unconditional test of independence. The residuals <italic>r</italic><sub><italic>x</italic></sub> and <italic>r</italic><sub><italic>y</italic></sub> are obtained by regressing <italic>x</italic> and <italic>y</italic> on <italic>z</italic> in a nonlinear fashion. The regression removes the dependencies between <italic>x</italic> and <italic>y</italic> due to <italic>z</italic> and consequently the residuals should be independent if <bold>X</bold> &#x22A5;<sub><italic>P</italic></sub> <bold>Y</bold> | <bold>Z</bold>. For regression we use a generalized additive model (GAM, see [<xref ref-type="bibr" rid="c7">7</xref>]) as implemented in the R function <monospace>gam()</monospace> in the library mgcv [<xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>] with default settings). That is, we regress <italic>y</italic> on a set of variables <inline-formula><alternatives><inline-graphic xlink:href="138669_inline15.gif"/></alternatives></inline-formula> as <inline-formula><alternatives><inline-graphic xlink:href="138669_inline16.gif"/></alternatives></inline-formula> where <italic>f</italic><sub><italic>i</italic></sub> are spline functions (selected by cross-validation) and <italic>&#x03B5;</italic> is Gaussian noise. We have now the option of using either the permutation or Gamma test of <xref ref-type="sec" rid="s2b">section 2.2</xref> on the residuals.</p>
</sec>
<sec id="s2d">
<label>2.4.</label>
<title>Distance covariance</title>
<p>The distance covariance has been suggested as an alternative measure of independence to the HSIC by [<xref ref-type="bibr" rid="c24">24</xref>] and [<xref ref-type="bibr" rid="c25">25</xref>]. An estimate of the distance covariance for a set of <italic>n</italic> samples {(<italic>x</italic><sub>1</sub>, <italic>y</italic><sub>1</sub>), &#x2026;, (<italic>x</italic><sub><italic>n</italic></sub>, <italic>y</italic><sub><italic>n</italic></sub>)} is obtained as follows. For variable <italic>X</italic> we define
<disp-formula><alternatives><graphic xlink:href="138669_ueqn2.gif"/></alternatives></disp-formula>
<disp-formula><alternatives><graphic xlink:href="138669_ueqn3.gif"/></alternatives></disp-formula>
with the power parameter &#x03BE;. Similarly, we define
<disp-formula><alternatives><graphic xlink:href="138669_ueqn4.gif"/></alternatives></disp-formula>
for the variable <italic>Y</italic>. An estimator for the distance covariance is then obtained as
<disp-formula><alternatives><graphic xlink:href="138669_ueqn5.gif"/></alternatives></disp-formula></p>
<p>If we use the PC algorithm with the DCC as independence oracle we obtain algorithm dPC.</p>
</sec>
<sec id="s2e">
<label>2.5.</label>
<title>Test of (unconditional) independence for dPC</title>
<p>The independence test is already implemented by <monospace>R</monospace> in the <monospace>energy</monospace> package [<xref ref-type="bibr" rid="c19">19</xref>] with the function <monospace>dcov.test()</monospace>. The test is implemented as a permutation test. The output <italic>p</italic>-value is calculated as <inline-formula><alternatives><inline-graphic xlink:href="138669_inline17.gif"/></alternatives></inline-formula> where <italic>m</italic> is the number of replicates which are greater than the observed value of the statistic dCov(X,Y) and <italic>p</italic> is the total number of replicates (personal communication with M.L.Rizzo).</p>
</sec>
<sec id="s2f">
<label>2.6.</label>
<title>Test of conditional independence for dPC</title>
<p>We generalize the test of the previous <xref ref-type="sec" rid="s2e">Section 2.5</xref> to a conditional version by applying the (unconditional) independence test to residuals formed as in section Residuals test of <xref ref-type="sec" rid="s2c">2.3</xref>.</p>
</sec>
</sec>
<sec id="s3">
<label>3.</label>
<title>Results</title>
<p>We first investigate the effectiveness of the independence criteria in finding dependencies in small simulated examples and explore parameter settings. We continue with a larger scale example with data simulated by permutation resampling from real data. Finally we present our results for the datasets from [<xref ref-type="bibr" rid="c20">20</xref>].</p>
<sec id="s3a">
<label>3.1.</label>
<title>Testing unconditional independence criteria</title>
<p>We expect the effectiveness of the independence criteria to depend crucially on the signal to noise ratio. We therefore tested the HSIC and DCC on 300 samples simulated from <inline-formula><alternatives><inline-graphic xlink:href="138669_inline18.gif"/></alternatives></inline-formula>, <inline-formula><alternatives><inline-graphic xlink:href="138669_inline19.gif"/></alternatives></inline-formula> for varying noise levels <italic>&#x03C3;</italic><sup>2</sup> and signal range of 2 from <italic>&#x2212;</italic>1 to 1. <italic>X</italic> and <italic>Y</italic> are dependent, hence independence should be rejected with low <italic>p</italic>-values. The simulated data are shown in the <xref ref-type="fig" rid="figA2">figure A2</xref> in the supplementary material. <xref ref-type="table" rid="tbl1">Table 1</xref> lists <italic>p</italic>-values for combinations of methods from <xref ref-type="sec" rid="s2b">Sections 2.2</xref> and <xref ref-type="sec" rid="s2e">2.5</xref> and varying noise levels <italic>&#x03C3;</italic>. All the <italic>p</italic>-values are a mean of 100 replications of the test. The size of the simulated sample is 300, as this is a reasonably typical sample size for high-throughput experiments. At <italic>&#x03C3;</italic> = 10 variables <italic>X</italic> and <italic>Y</italic> are effectively independent. As expected the <italic>p</italic>-value is less and less reliable for detecting dependency for samples with increasing noise levels. In this simple test both criteria, HSIC and DCC, behave similarly.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.:</label>
<caption><p>Testing independence criteria. All the <italic>p</italic>-value estimates are the mean of 100 <italic>p</italic>-values from repetitions for each of the three tests. The size of the simulated sample is 300. The DCC and HSIC <italic>p</italic>-values are obtained from 1000 permutations each.</p></caption>
<graphic xlink:href="138669_tbl1.tif"/>
</table-wrap>
<p>The HSIC depends on a kernel width parameter <italic>&#x03BB;</italic>. <xref ref-type="fig" rid="fig2">Figure. 2a</xref>) to c) show <italic>p</italic>-values for different HSIC tests when the kernel width <italic>&#x03BB;</italic> varies from 0.001 to 1000. We note that in order to reject independence successfully <italic>&#x03BB;</italic> needs to be chosen carefully, particularly with higher noise. If <italic>&#x03BB;</italic> is very small, then <italic>k</italic>(<italic>x, y</italic>) <italic>&#x2248;</italic> 0 for almost all <italic>x</italic> &#x2260; <italic>y</italic>, and the Gram matrix is close to the identity matrix. If <italic>&#x03BB;</italic> is too large, then <italic>k</italic>(<italic>x, y</italic>) <italic>&#x2248;</italic> 1, for all <italic>x, y</italic> and the Gram matrix is ill-conditioned. In either case any dependency variables is hard to detect. Based on these figures we choose a kernel width in the range <italic>&#x03BB;</italic> &#x2208; (0.5, 9). Furthermore we observe that the permutation and Gamma tests both with and without incomplete Cholesky decomposition always give very similar results for this range of <italic>&#x03BB;</italic>. Therefore, for further analysis we will use the Gamma test with an incomplete Cholesky decomposition since it is computationally most efficient (as seen in <xref ref-type="fig" rid="figA4">Figure A4</xref> of the supplementary material).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.:</label>
<caption><p>Dependency of <italic>p</italic>-values of HSIC and DCC tests on varying parameters, kernel width <italic>&#x03BB;</italic> for HSIC and index <italic>&#x03BE;</italic> for DCCC.</p></caption>
<graphic xlink:href="138669_fig2.tif"/>
</fig>
<p><xref ref-type="fig" rid="fig2">Figure 2d</xref>) shows the dependency of <italic>p</italic>-values on the power parameter &#x03BE; of the DCC. For simplicity we set &#x03BE; = 1, although smaller values might work even better.</p>
</sec>
<sec id="s3b">
<label>3.2.</label>
<title>Comparison of network inference algorithms</title>
<p>The performances of the algorithms are compared using ROC curves sensitivity over specificity while varying the <italic>p</italic>-value cutoff required by the oracle for statistical independence in the PC algorithm. Unless otherwise stated we will focus on the absence and presence of edges in the inferred graphs ignoring their direction when calculating specificity and sensitivity.</p>
<p>Three types of datasets are considered: data simulated from a simple known network, data obtained by permuting residuals after fitting a network to data from [<xref ref-type="bibr" rid="c20">20</xref>], and finally the original data from the same study. Parameters of the algorithms were fixed as in <xref ref-type="table" rid="tbl2">Table 2</xref>.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.:</label>
<caption><p>Free parameters for kPCs and dPC.</p></caption>
<graphic xlink:href="138669_tbl2.tif"/>
</table-wrap>
<p>For easy reference we label the algorithms as follows. The standard PC algorithm as implemented in the <monospace>R</monospace> package <monospace>pcalg</monospace> with the <monospace>gaussCItest</monospace> criterion (implementing Fisher&#x2019;s <italic>z</italic>-test for correlation) is labelled PC. The PC algorithm based on the DCC from <xref ref-type="sec" rid="s2d">Section 2.4</xref> is labelled dPC. The PC algorithm based on the HSIC from <xref ref-type="sec" rid="s2a">Section 2.1</xref> is labelled kPC. The kPC version based on the Permutation-cluster test of <xref ref-type="sec" rid="s2c">Section 2.3</xref> is labeled kPC-Clust. The kPC version based on the Residuals test of <xref ref-type="sec" rid="s2c">Section 2.3</xref> with the Gamma approximation is labelled kPC-Resid.</p>
<sec id="s3b1">
<label>3.2.1.</label>
<title>Data simulated from artificial network</title>
<p>The relationships between the nodes are described in <xref ref-type="fig" rid="fig3">Figure 3b</xref>. Since the network contains nonlinear relationships and non-Gaussian noise, as expected, the PC algorithm performed worst. Close to perfect performance was achieved by dPC, kPC-Clust and kPC-Resid. <xref ref-type="fig" rid="fig3">Figure A3a</xref> shows the spread of ROC curves when simulating data repeatedly. dPC, kPC-Resid, and kPC-Clust outperformed PC in 100, 98, and 100 out of 100 cases, respectively.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.:</label>
<caption><p>Toy simulated example on 9 nodes and 300 observations.</p></caption>
<graphic xlink:href="138669_fig3.tif"/>
</fig>
</sec>
<sec id="s3b2">
<label>3.2.2.</label>
<title>Data simulated by resampling</title>
<p>Next we compared the performance of the algorithms on samples obtained by fitting a plausible network to data in [<xref ref-type="bibr" rid="c20">20</xref>] and resampling residuals. In this way we still control the structure of the underlying network but obtain more realistic noise distributions.</p>
<p>As outlined in the introduction, the data consist of eight datasets of expression levels of eleven proteins, each dataset obtained after specific experimental interventions. Here we present only the results from the dataset 7, the rest is provided in the supplementary material in <xref ref-type="fig" rid="figA5">Figure A5</xref>. Protein PKC was inhibited for dataset 7. Since PKC was externally modified no causal arcs lead into PKC. The network is that of <xref ref-type="fig" rid="fig1">Figure 1</xref> with arcs into PKC removed.</p>
<p>For the simulation we used the causal model of <xref ref-type="fig" rid="fig4">Figure 4a</xref>. The data generation starts from parentless nodes (PKC and PKA). These variables are assigned the original values from the samples in the experimental dataset. Next, recursively iterating over nodes whose parents already have assigned values, a generalised additive model similar to <xref ref-type="sec" rid="s2c">section 2.3</xref> is fitted to obtain mean estimates and residuals for the experimental sample values of the focus node when regressing on the values previously assigned to its parents. These residuals are permuted before being added to mean estimates to obtain resampled values to assign to the focus node.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.:</label>
<caption><p>Data simulated from the Dataset 7, with non-reduced noise.</p></caption>
<graphic xlink:href="138669_fig4.tif"/>
</fig>
<p>This procedure ensures that only the assumed dependencies as captured in the nonlinear regressions on parents are maintained, while all other dependencies are removed by permuting residuals. On the other hand, the noise characteristics of the original data are maintained to some degree. In particular, some focus nodes show little functional dependence on their parents, that is, a very low signal to noise ratio. This is a characteristic of experimental data as well. In order to explore the influence of this signal to noise ratio, additional data sets are simulated with residuals scaled down by a factor <italic>k</italic> before being added to mean estimates, improving on the signal to noise ratio.</p>
<p><xref ref-type="fig" rid="fig4">Figure 4b</xref> shows that all the PC versions based on general independence criteria significantly outperform the traditional PC algorithm. dPC, kPC-Resid and kPC-Clust result in areas under the ROC curve of greater than 0.8 while that of PC is only 0.67. Performance is worse than for the toy example above. This is mainly due to a small signal to noise ratio for many relationships: on visual inspection many relationships in <xref ref-type="fig" rid="fig1">Figure 1</xref> are hardly noticeable in the data. This results in the regression step not capturing much signal. On the other hand, real data are likely to show this type of noise characteristics. The effect of varying the scaling factor <italic>k</italic> for the residual noise is shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>. Generally, as expected, with lower noise performance improves. The dPC version is responding well to lower noise.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.:</label>
<caption><p>Signal to noise ratio effect on efficiency of the algorithms</p></caption>
<graphic xlink:href="138669_fig5.tif"/>
</fig>
<p><xref ref-type="fig" rid="figA3">Figure A3b</xref> in the supplementary material shows ROC curves for repeated simulations. The dPC, kPC-Resid, and kPC-Clust outperform the PC algorithm 99, 93, and 91 out of a 100 repetitions. To provide some insight into the variability of these results for different datasets, we show results for all simulated data from all 8 datasets in the supplementary material in <xref ref-type="fig" rid="figA5">Figure A5</xref>. Qualitatively the results are similar to that for dataset 7 presented here. In particular, the standard PC algorithm is almost always performing worst (except for dataset 2) and the dPC algorithm has a slight edge. Reducing noise helps improving results as shown in <xref ref-type="fig" rid="figA6">Figures A6</xref>, <xref ref-type="fig" rid="figA7">A7</xref> and <xref ref-type="fig" rid="figA8">A8</xref> for most of the dataset and algorithm combinations.</p>
</sec>
<sec id="s3b3">
<label>3.2.3.</label>
<title>Original data</title>
<p>Finally we tested all the algorithms on the original data from [<xref ref-type="bibr" rid="c20">20</xref>]. Again we only show the results on dataset 7 here, the rest of the results are provided in the supplementary material in <xref ref-type="fig" rid="figA9">Figure A9</xref>. We expect to find the skeleton of the graph in <xref ref-type="fig" rid="fig6">Figure 6a</xref> derived from <xref ref-type="fig" rid="fig1">Figure 1</xref> as in the previous section. In <xref ref-type="fig" rid="fig6">Figure 6b</xref> we see the ROC curves for four versions of the PC algorithms. Results are very similar to the ones seen for simulated data: dPC and kPCs outperform PC and are quite similar among themselves, with dPC having a slight edge. We may conclude that independence criteria based PC versions are a significant improvement on the traditional PC algorithm on the real data as well as the simulated one.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.:</label>
<caption><p>ROC curves for dataset 7.</p></caption>
<graphic xlink:href="138669_fig6.tif"/>
</fig>
</sec>
</sec>
<sec id="s3c">
<label>3.3.</label>
<title>Combining all datasets</title>
<p>The eight datasets of [<xref ref-type="bibr" rid="c20">20</xref>] have slightly different dependence structures due to variation in the external interventions. Combining information from all datasets should improve reconstruction of the underlying graph structure. To test this intuition we combine a consensus graphical structure from networks fitted to each dataset. Two types of consensus networks are obtained. The first takes edges that appear in <italic>at least one</italic> of the individual networks (labelled <italic>union network</italic>). The second calculates the typical average occurrence of edges over all edges in the union network and over all eight networks. Then only those edges of the union network are retained which occur (across all eight networks) more often than this typical average. We label this network <italic>aboveaverage network</italic>. More sophisticated approaches are conceivable, however, here we only wanted to investigate whether there is potential improvement by combining networks at all, and the effect of the choice of an independence criterion on the consensus network. We compare the output of our algorithm to the skeleton illustrated in <xref ref-type="fig" rid="fig7">Figure 7a</xref> derived from <xref ref-type="fig" rid="fig1">Figure 1</xref> and corresponding ROC curves are shown in <xref ref-type="fig" rid="fig7">Figure 7b</xref> and <xref ref-type="fig" rid="fig7">7c</xref>.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.:</label>
<caption><p>All 8 datasets combined.</p></caption>
<graphic xlink:href="138669_fig7.tif"/>
</fig>
<p>Combining networks results in a slight improvement overall compared to <xref ref-type="fig" rid="fig6">Figure 6</xref> with a trade-off between sensitivity and specificity shifted between the two types of combinations. The general independence criteria are again superior.</p>
</sec>
<sec id="s3d">
<label>3.4.</label>
<title>Discovering directions</title>
<p>So far we have looked at performance of algorithms when inferring the skeleton of a DAG with undirected edges only. The PC algorithm adds an edge orienting phase exploiting collider patterns and transitive closure requirements as formalised in the Meek rules [<xref ref-type="bibr" rid="c12">12</xref>] in the collider phase. Exploiting nonlinear relationships and non-Gaussian noise additional edges might be oriented. This is achieved by the PC algorithm extended by the generalised transitive phase which incorporates background knowledge emerging from testing directions exploiting nonlinearity and non-Gaussian noise.</p>
<p>With an imperfect independence oracle or data that do not strictly follow modelling assumptions, ambiguities can arise when orienting edges, possibly leading to cycles and doubly oriented edges. There are no general rules how to resolve such ambiguities. In this section we ignore doubly oriented edges as undirected for the purpose of assessing algorithms.</p>
<p>Results comparing algorithms by fractions of correct out of predicted orientations at various orientation phases are presented in <xref ref-type="table" rid="tbl3">Table 3</xref>. Free parameters were fixed as in the <xref ref-type="table" rid="tbl2">Table 2</xref>. The generalized transitive phase adds many more orientations to those found in the collider phase. For simulated data, this phase actually adds all the missing orientations in the correct direction.</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3.:</label>
<caption><p>The fraction of correct among predicted orientations at different stages of the algorithms.</p></caption>
<graphic xlink:href="138669_tbl3.tif"/>
</table-wrap>
<p>We illustrate some of the results from <xref ref-type="table" rid="tbl3">Table 3</xref>. <xref ref-type="fig" rid="fig8">Figure 8</xref> shows the output graphs of the orientation phases of algorithm kPC-Resid. There is one doubly oriented edge emerging in the collider phase between AKT <italic>&#x2194;</italic> PIP3. This phase also orients one edge in the wrong direction. Since there are no more colliders no further edge can be oriented at that stage. However, exploiting nonlinearity and non-Gaussian noise it is straightforward to orient the rest of the edges in the Generalised transitive phase.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8.:</label>
<caption><p>Output of the kPC-Resid algorithm on the data simulated from dataset 8. Color coding: dashed black undirected or doubly directed edges represent correctly identified undirected edges, green directed edges represent correct, while red directed edges represent incorrect orientations. Dashed black oriented edges are from the previous phase.</p></caption>
<graphic xlink:href="138669_fig8.tif"/>
</fig>
<p>Similarly, <xref ref-type="fig" rid="fig9">Figure 9</xref> shows the output of the kPC-Resid on the real dataset 8. Unfortunately the graph contains several triangles which make it impossible to orient any edge in the collider phase. The generalised transitive phase can differentiate between the oriented structures. Finally, for comparison with the kPC algorithm, <xref ref-type="fig" rid="fig10">Figure 10</xref> shows the output of the dPC algorithm on simulated dataset 8. In contrast to the kPC-Resid algorithm there are enough colliders present to allow the algorithm to orient edges even in the collider phase. It orients four edges correctly, even though strictly speaking a collider at MEK is inconsistent with a collider at ERK. The rest of the edges gets oriented in the generalised transitive phase. Due to the two inconsistent colliders the overall result is slightly inferior to that of the kPC-Resid algorithm.</p>
<fig id="fig9" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9.:</label>
<caption><p>Output of the kPC-Resid algorithm on the dataset 8.</p></caption>
<graphic xlink:href="138669_fig9.tif"/>
</fig>
<fig id="fig10" position="float" orientation="portrait" fig-type="figure">
<label>Figure 10.:</label>
<caption><p>Output of the dPC algorithm on the data simulated from the dataset 8.</p></caption>
<graphic xlink:href="138669_fig10.tif"/>
</fig>
</sec>
</sec>
<sec id="s4">
<label>4.</label>
<title>Discussion</title>
<p>The purpose of this study was to investigate how far probabilistic independence criteria for continuous data that go beyond linear relationships and Gaussian noise can improve the identification of edges and their orientation in a causal graph when applied to experimental data and data simulated in a realistic fashion from experimental data. We analysed two different criteria proposed in the literature, the Hilbert-Schmidt Independence Criterion or HSIC, and the Distance Covariance Criterion, or DCC in the context of the popular PC algorithm that relies on measures of probabilistic independence for network inference. The distance covariance is a natural extension of the Pearson correlation parameter [<xref ref-type="bibr" rid="c25">25</xref>]. To our knowledge this is the first implementation and application of the DCC to causal or network inference. All algorithms discussed in this study are available as package for the R statistical environment [<xref ref-type="bibr" rid="c18">18</xref>].</p>
<p>Overall, our findings confirm that the performance of general independence criteria is decisively better over that based on linear relationships with Gaussian noise on simulated as well as experimental data in terms of correct undirected edges as well as of correct directions. Secondly, we find only little difference between the performance of the HSIC and DCC in general, with the DCC showing slightly better performance for some datasets.</p>
<p>In order to assess the algorithms in a realistic scenario we applied them to a wellknown experimental dataset for which the network is approximately known based on biological knowledge. Of course, this knowledge of the network might be inaccurate and we therefore propose a generic way to simulate data based on experimental data and an approximate or putative network structure that keeps much of the noise characteristics of the original data but reproduces those and only those conditional dependencies required by the network. As we demonstrate in our analysis these simulated datasets form an excellent compromise between retaining much of the nonlinear and non-Gaussian characteristics of the original data, but for an exactly known network. As we see in our study one difficulty remains: if some arcs of the assumed network are not supported by the data, for example, if there is little dependency in the data in the first place between two variables which we wish to connect in the network, our method is unable to create such dependency artificially. Nevertheless, as long as the assumed network reflects most of the dependencies in the experimental data, the simulated data are useful for comparative studies between different algorithms as shown in <xref ref-type="sec" rid="s3b2">section 3.2.2</xref>.</p>
<p>The PC algorithm requires a test for conditional independence. Independence criteria might, however, only be available in an unconditional form. We propose a simple procedure, based on fitting nonlinear regressions, to adapt such criteria to the conditional independence case. Since there is a conditional version of the HSIC available we had an opportunity to compare the conditional HSIC (in algorithm kPC-Clust) with our adaptation of the unconditional HSIC (in algorithm kPC-Resid). As can be seen throughout the study, the adapted kPC-Resid version, particularly on experimental and realistically simulated data, is performing comparably to the conditional version with the conditional HSIC having a slight advantage. It is worth noting that kPC-Clust involves calculating the empirical estimate of the conditional HSIC (3) which is computationally significantly more expensive than the unconditional HSIC (2), therefore in practice kPC-Clust can be up to 5 <italic>&#x2212;</italic> 10 times slower than kPC-Resid or dPC.</p>
<p>The empirical estimation of HSIC depends on parameters such as the kernel width <italic>&#x03BB;</italic> and the regularisation parameter <italic>E</italic>. Here we used simulations to find sensible ranges for these parameters. Another advantage of the DCC is that it is less affected by parameter choices, essentially only the power parameter <italic>&#x03BE;</italic> which can be safely set to a value between 0.1 and 1 without affecting the results too much.</p>
<p>The PC algorithm is very restrictive in its assumptions on the dependency structure. For example, cycles or unobserved variables are excluded. It would be interesting to see whether inference techniques allowing such more complex assumptions benefit from general independence criteria in the same way the PC algorithm does.</p>
<p>The PC algorithm is firmly based in a frequentist statistical framework. Bayesian inference is often strongly dependent on specific noise models through the likelihood function. It needs to be explored how to incorporate independence criteria in a Bayesian framework, possibly through a form of loss likelihood [<xref ref-type="bibr" rid="c1">1</xref>].</p>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="other"><string-name><given-names>P.</given-names> <surname>Bissiri</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Holmes</surname></string-name>, and <string-name><given-names>S.G.</given-names> <surname>Walker</surname></string-name>, <article-title>A general framework for updating belief distributions</article-title>, <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source> (<year>2016</year>).</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><string-name><given-names>A.J.</given-names> <surname>Butte</surname></string-name> and <string-name><given-names>I.S.</given-names> <surname>Kohane</surname></string-name>, <article-title>Mutual information relevance networks: functional genomic clustering using pairwise entropy measurements</article-title>, in <source>Pac Symp Biocomput</source>, Vol. <volume>5</volume>. <year>2000</year>, pp. <fpage>418</fpage>&#x2013;<lpage>429</lpage>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><given-names>J.J.</given-names> <surname>Faith</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Hayete</surname></string-name>, <string-name><given-names>J.T.</given-names> <surname>Thaden</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Mogno</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Wierzbowski</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Cottarel</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Kasif</surname></string-name>, <string-name><given-names>J.J.</given-names> <surname>Collins</surname></string-name>, and <string-name><given-names>T.S.</given-names> <surname>Gardner</surname></string-name>, <article-title>Large-scale mapping and validation of escherichia coli transcriptional regulation from a compendium of expression profiles</article-title>, <source>PLoS biol</source> <volume>5</volume> (<year>2007</year>), p. <fpage>e8</fpage>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Gretton</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Bousquet</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Smola</surname></string-name>, and <string-name><given-names>B.</given-names> <surname>Sch&#x000F6;lkopf</surname></string-name>, <article-title>Measuring statistical dependence with Hilbert-Schmidt norms</article-title>, in <source>Algorithmic learning theory</source>. <issue>Springer</issue>, <year>2005</year>, pp. <fpage>63</fpage>&#x2013;<lpage>77</lpage>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Gretton</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Fukumizu</surname></string-name>, <string-name><given-names>C.H.</given-names> <surname>Teo</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Song</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Sch&#x000F6;lkopf</surname></string-name>, and <string-name><given-names>A.J.</given-names> <surname>Smola</surname></string-name>, <article-title>A kernel statistical test of independence</article-title>, <source>NIPS20</source> (<year>2008</year>).</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><string-name><given-names>J.A.</given-names> <surname>Hartigan</surname></string-name> and <string-name><given-names>M.A.</given-names> <surname>Wong</surname></string-name>, <article-title>A k-means clustering algorithm</article-title>., <source>Applied Statistics</source> <volume>28</volume> (<year>1979</year>), pp. <fpage>100</fpage>&#x2013;<lpage>108</lpage>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="other"><string-name><given-names>T.</given-names> <surname>Hastie</surname></string-name> and <string-name><given-names>R.</given-names> <surname>Tibshirani</surname></string-name>, <article-title>Generalized additive models</article-title>, <source>Statistical science</source> (<year>1986</year>), pp. <fpage>297</fpage>&#x2013;<lpage>310</lpage>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="other"><string-name><given-names>P.O.</given-names> <surname>Hoyer</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Janzing</surname></string-name>, <string-name><given-names>J.M.</given-names> <surname>Mooij</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Peters</surname></string-name>, and <string-name><given-names>B.</given-names> <surname>Sch&#x000F6;lkopf</surname></string-name>, <article-title>Nonlinear causal discovery with additive noise models</article-title>, in <source>Advances in neural information processing systems</source>. <volume>2009</volume>, pp. <fpage>689</fpage>&#x2013;<lpage>696</lpage>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Irrthum</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Wehenkel</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Geurts</surname></string-name>, <etal>et al.</etal>, <article-title>Inferring regulatory networks from expression data using tree-based methods</article-title>, <source>PloS</source> one <volume>5</volume> (<year>2010</year>), p. <fpage>e12776</fpage>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="confproc"><string-name><given-names>C.</given-names> <surname>Lippert</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Stegle</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Ghahramani</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Borgwardt</surname></string-name>, <article-title>A kernel method for unsupervised structured network inference</article-title>, <conf-name>JMLR Workshop and Conference Proceedings</conf-name> Volume <volume>5</volume>:<fpage>368</fpage>&#x2013;<lpage>375</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><string-name><given-names>D.</given-names> <surname>Marbach</surname></string-name>, J.C.C. and <string-name><given-names>Robert</given-names> <surname>Kffner</surname></string-name>, <string-name><given-names>N.M.</given-names> <surname>Vega</surname></string-name>, <string-name><given-names>R.J.</given-names> <surname>Prill</surname></string-name>, <string-name><given-names>D.M.</given-names> <surname>Camacho</surname></string-name>, K.R.A. amd <collab>The DREAM5 Consortium</collab>, <string-name><given-names>M.</given-names> <surname>Kellis</surname></string-name>, <string-name><given-names>J.J.</given-names> <surname>Collins</surname></string-name>, and <string-name><given-names>G.</given-names> <surname>Stolovitzky</surname></string-name>, <article-title>Wisdom of crowds for robust gene network inference</article-title>, <source>Nature Methods</source> <volume>9</volume>, <fpage>796</fpage>&#x2013;<lpage>804</lpage> (<year>2012</year>) (2012).</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="confproc"><string-name><given-names>C.</given-names> <surname>Meek</surname></string-name>, <article-title>Causal inference and causal explanation with background knowledge</article-title>, in <conf-name>Proceedings of the Eleventh conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc</conf-name>., <year>1995</year>, pp. <fpage>403</fpage>&#x2013;<lpage>410</lpage>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><string-name><given-names>P.E.</given-names> <surname>Meyer</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Kontos</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Lafitte</surname></string-name>, and <string-name><given-names>G.</given-names> <surname>Bontempi</surname></string-name>, <article-title>Information-theoretic inference of large transcriptional regulatory networks</article-title>, <source>EURASIP journal on bioinformatics and systems biology</source> <volume>2007</volume> (<year>2007</year>), pp. <fpage>1</fpage>&#x2013;<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Obayashi</surname></string-name> and <string-name><given-names>K.</given-names> <surname>Kinoshita</surname></string-name>, <article-title>Rank of correlation coefficient as a comparable measure for biological significance of gene coexpression</article-title>, <source>DNA research</source> <volume>16</volume> (<year>2009</year>), pp. <fpage>249</fpage>&#x2013;<lpage>260</lpage>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><string-name><given-names>R.</given-names> <surname>Opgen-Rhein</surname></string-name> and <string-name><given-names>K.</given-names> <surname>Strimmer</surname></string-name>, <article-title>From correlation to causation networks: a simple approximate learning algorithm and its application to high-dimensional plant gene expression data</article-title>, <source>BMC systems biology</source> <volume>1</volume> (<year>2007</year>), p. <fpage>37</fpage>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="book"><string-name><given-names>J.</given-names> <surname>Pearl</surname></string-name>, <source>Causality</source>, <publisher-name>Cambridge university press</publisher-name>, <year>2009</year>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="book"><collab>R Core Team</collab>, <source>R: A Language and Environment for Statistical Computing</source>, <publisher-name>R Foundation for Statistical Computing</publisher-name>, <publisher-loc>Vienna, Austria</publisher-loc>. Available at <ext-link ext-link-type="uri" xlink:href="http://www.R-project.org">http://www.R-project.org</ext-link>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="book"><collab>R Core Team</collab>, <source>cR: A Language and Environment for Statistical Computing</source>, <publisher-name>R Foundation for Statistical Computing</publisher-name>, <publisher-loc>Vienna, Austria</publisher-loc> (<year>2014</year>). Available at <ext-link ext-link-type="uri" xlink:href="http://www.R-project.org/">http://www.R-project.org/</ext-link>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="other"><string-name><given-names>M.L.</given-names> <surname>Rizzo</surname></string-name> and <string-name><given-names>G.J.</given-names> <surname>Szekely</surname></string-name>, <article-title>E-statistics</article-title> (<year>2014</year>). Available at <ext-link ext-link-type="uri" xlink:href="http://cran.r-project.org/web/packages/energy/energy.pdf">http://cran.r-project.org/web/packages/energy/energy.pdf</ext-link>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><string-name><given-names>K.</given-names> <surname>Sachs</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Perez</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Pe&#x2019;er</surname></string-name>, <string-name><given-names>D.A.</given-names> <surname>Lauffenburger</surname></string-name>, and <string-name><given-names>G.P.</given-names> <surname>Nolan</surname></string-name>, <article-title>Causal proteinsignaling networks derived from multiparameter single-cell data</article-title>, <source>Science</source> <volume>308</volume> (<year>2005</year>), pp. <fpage>523</fpage>&#x2013;<lpage>529</lpage>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><string-name><given-names>D.</given-names> <surname>Sejdinovic</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Sriperumbudur</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Gretton</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Fukumizu</surname></string-name>, <etal>et al.</etal>, <article-title>Equivalence of distance-based and rkhs-based statistics in hypothesis testing</article-title>, <source>The Annals of Statistics</source> <volume>41</volume> (<year>2013</year>), pp. <fpage>2263</fpage>&#x2013;<lpage>2291</lpage>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="book"><string-name><given-names>J.</given-names> <surname>Shawe-Taylor</surname></string-name> and <string-name><given-names>N.</given-names> <surname>Cristianini</surname></string-name>, <source>Kernel methods for pattern analysis</source>, <publisher-name>Cambridge university press</publisher-name>, <year>2004</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="book"><string-name><given-names>P.</given-names> <surname>Spirtes</surname></string-name>, <string-name><given-names>C.N.</given-names> <surname>Glymour</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Scheines</surname></string-name>, <source>Causation, prediction, and search</source>, Vol. <volume>81</volume>, <publisher-name>MIT press</publisher-name>, <year>2000</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><string-name><given-names>G.J.</given-names> <surname>Sz&#x00E9;kely</surname></string-name>, <string-name><given-names>M.L.</given-names> <surname>Rizzo</surname></string-name>, <string-name><given-names>N.K.</given-names> <surname>Bakirov</surname></string-name>, <etal>et al.</etal>, <article-title>Measuring and testing dependence by correlation of distances</article-title>, <source>The Annals of Statistics</source> <volume>35</volume> (<year>2007</year>), pp. <fpage>2769</fpage>&#x2013;<lpage>2794</lpage>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><string-name><given-names>G.J.</given-names> <surname>Sz&#x00E9;kely</surname></string-name>, <string-name><given-names>M.L.</given-names> <surname>Rizzo</surname></string-name>, <etal>et al.</etal>, <article-title>Brownian distance covariance</article-title>, <source>The annals of applied statistics</source> <volume>3</volume> (<year>2009</year>), pp. <fpage>1236</fpage>&#x2013;<lpage>1265</lpage>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><string-name><given-names>R.E.</given-names> <surname>Tillman</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Gretton</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Spirtes</surname></string-name>, <article-title>Nonlinear directed acyclic structure learning with weakly additive noise model</article-title>, <source>NIPS</source> <volume>22</volume>, Vancouver (<year>2009</year>).</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Wood</surname></string-name>, <article-title>Stable and efficient multiple smoothing parameter estimation for generalized additive models</article-title>., <source>Journal of the American Statistical Association</source>. <volume>99</volume> (<year>2004</year>), pp. <fpage>673</fpage>&#x2013;<lpage>686</lpage>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Wood</surname></string-name>, <article-title>Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models</article-title>., <source>Journal of the Royal Statistical Society</source>. <volume>73</volume> (<year>2011</year>), pp. <fpage>3</fpage>&#x2013;<lpage>36</lpage>.</mixed-citation></ref>
</ref-list>
<sec id="s5" sec-type="supplymentary-materials">
<title>Appendix A. Supplementary Material</title>
<table-wrap id="tblA1" orientation="portrait" position="float">
<label>Table A1.:</label>
<caption><p>Table of datasets</p></caption>
<graphic xlink:href="138669_tblA1.tif"/>
</table-wrap>
<fig id="figA1" position="float" orientation="portrait" fig-type="figure">
<label>Figure A1.:</label>
<caption><p>Boxplot and pair plot of dataset 8 after log transformation.</p></caption>
<graphic xlink:href="138669_figA1.tif"/>
</fig>
<fig id="figA2" position="float" orientation="portrait" fig-type="figure">
<label>Figure A2.:</label>
<caption><p>Data simulated with nonlinear dependencies</p></caption>
<graphic xlink:href="138669_figA2.tif"/>
</fig>
<fig id="figA3" position="float" orientation="portrait" fig-type="figure">
<graphic xlink:href="138669_figA3.tif"/>
</fig>
<fig id="figA4" position="float" orientation="portrait" fig-type="figure">
<label>Figure A4.:</label>
<caption><p>Time efficiency of independence tests.</p></caption>
<graphic xlink:href="138669_figA4.tif"/>
</fig>
<fig id="figA5" position="float" orientation="portrait" fig-type="figure">
<label>Figure A5.:</label>
<caption><p>ROC curves to compare kPCs, dPC and PC algorithms on data simulated from the indicated datasets.</p></caption>
<graphic xlink:href="138669_figA5.tif"/>
</fig>
<fig id="figA6" position="float" orientation="portrait" fig-type="figure">
<label>Figure A6.:</label>
<caption><p>dPC effectiveness on data with varying noise levels, all eight datasets.</p></caption>
<graphic xlink:href="138669_figA6.tif"/>
</fig>
<fig id="figA7" position="float" orientation="portrait" fig-type="figure">
<label>Figure A7.:</label>
<caption><p>kPC-Resid effectiveness on data with varying noise levels, all eight datasets.</p></caption>
<graphic xlink:href="138669_figA7.tif"/>
</fig>
<fig id="figA8" position="float" orientation="portrait" fig-type="figure">
<label>Figure A8.:</label>
<caption><p>kPC-Clust effectiveness on data with varying noise levels, all eight datasets.</p></caption>
<graphic xlink:href="138669_figA8.tif"/>
</fig>
<fig id="figA9" position="float" orientation="portrait" fig-type="figure">
<label>Figure A9.:</label>
<caption><p>ROC curves to compare kPCs, dPC and PC algorithms on the real data.</p></caption>
<graphic xlink:href="138669_figA9.tif"/>
</fig>
</sec>
</back>
</article>