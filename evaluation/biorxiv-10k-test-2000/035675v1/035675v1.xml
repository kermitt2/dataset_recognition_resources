<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/035675</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Bioinformatics</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A regression framework for the proportion of true null hypotheses</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Boca</surname><given-names>Simina M.</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Leek</surname><given-names>Jeffrey T.</given-names></name>
</contrib>
</contrib-group>
<pub-date pub-type="epub">
<year>2015</year>
</pub-date>
<elocation-id>035675</elocation-id>
<history>
<date date-type="received">
<day>30</day>
<month>12</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>30</day>
<month>12</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2015, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2015</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="035675.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract><title>Abstract</title>
<p>The false discovery rate is one of the most commonly used error rates for measuring and controlling rates of false discoveries when performing multiple tests. Adaptive false discovery rates rely on an estimate of the proportion of null hypotheses among all the hypotheses being tested. This proportion is typically estimated once for each collection of hypotheses. Here we propose a regression framework to estimate the proportion of null hypotheses conditional on observed covariates. We provide both finite sample and asymptotic conditions under which this covariate-adjusted estimate is conservative - leading to appropriately conservative false discovery rate estimates. We demonstrate the viability of our approach through simulation and an application to estimating different prior probabilities for a range of sample sizes and allele frequencies in a GWAS meta-analysis.</p>
</abstract>
<counts>
<page-count count="14"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1"><label>1</label><title>Introduction</title>
<p>Multiple testing is a ubiquitous issue in modern scientific studies. Microarrays (<xref ref-type="bibr" rid="c5">Brown, 1995</xref>), next-generation sequencing (<xref ref-type="bibr" rid="c17">Shendure and Ji, 2008</xref>), and high-throughput metabolomics make it possible to simultaneously test the relationship between hundreds or thousands of biomarkers and an exposure or outcome of interest. These problems have a common structure consisting of a collection of variables, or features, for which measurements are obtained on multiple samples, with a hypothesis test being performed for each feature.</p>
<p>When performing thousands of hypothesis tests, the most widely used framework for controlling for multiple testing is the false discovery rate. For a fixed unknown parameter <italic>&#x00B5;</italic>, and testing a single null hypothesis <italic>H</italic><sub>0</sub>: <italic>&#x00B5;</italic> &#x003D; <italic>&#x00B5;</italic><sub>0</sub> versus some alternative hypothesis, for example, <italic>H</italic><sub>1</sub>: <italic>&#x00B5;</italic> &#x003D; <italic>&#x00B5;</italic><sub>1</sub>, the null hypothesis may either truly hold or not for each feature. Additionally, the test may lead to <italic>H</italic><sub>0</sub> either being rejected or not being rejected. Thus, when performing <italic>m</italic> hypothesis tests for <italic>m</italic> different unknown parameters, <xref ref-type="table" rid="tbl1">Table 1</xref> shows the total number of outcomes of each type, using the notation from <xref ref-type="bibr" rid="c3">Benjamini and Hochberg (1995)</xref>. We note that <italic>U</italic>, <italic>T, V</italic>, and <italic>S</italic>, and as a result, also <italic>R</italic> &#x003D; <italic>V</italic> &#x002B; <italic>S</italic>, are random variables, while <italic>m</italic><sub>0</sub>, the number of null hypotheses, is fixed and unknown.</p>
<table-wrap id="tbl1" orientation="portrait" position="float"><label>Table 1:</label>
<caption><p>Outcomes of testing multiple hypotheses.</p></caption>
<graphic xlink:href="035675_tbl1.tif"/>
</table-wrap>
<p>The false discovery rate (FDR), introduced in <xref ref-type="bibr" rid="c3">Benjamini and Hochberg (1995)</xref>, is the expected fraction of false discoveries among all discoveries. The false discovery rate depends on the overall fraction of null hypotheses, namely <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline1.gif"/></alternatives></inline-formula>. This proportion can also be interpreted as the <italic>a priori</italic> probability that a null hypothesis is true, <italic>&#x03C0;</italic><sub>0</sub>.</p>
<p>When estimating the FDR, incorporating an estimate of <italic>&#x03C0;</italic><sub>0</sub> can result in a more powerful procedure compared to the original <xref ref-type="bibr" rid="c3">Benjamini and Hochberg (1995)</xref> procedure; moreover, as <italic>m</italic> increases, the estimate of <italic>&#x03C0;</italic><sub>0</sub> improves, which means that the power of the multiple-testing approach does not necessarily decrease when more hypotheses are considered (<xref ref-type="bibr" rid="c18">Storey, 2002</xref>).</p>
<p>Most modern adaptive false discovery rate procedures rely on an estimate of &#x03C0;<sub>0</sub> using the data of all tests being performed. But additional information, in the form of meta-data, may be available to aid the decision about whether to reject the null hypothesis for a particular feature. One example is in set-level inference, including gene-set analysis, where each set has a different fraction of false discoveries. Another is in meta-analyses where each feature may have a different sample size and hence different power. Adjusting for covariates independent of the data conditional on the truth of the null hypothesis has also been shown to improve power in RNA-seq, eQTL, and proteomics studies (<xref ref-type="bibr" rid="c10">Ignatiadis et al., 2015</xref>).</p>
<p>In this paper, we build on the work of <xref ref-type="bibr" rid="c3">Benjamini and Hochberg (1995)</xref>, <xref ref-type="bibr" rid="c7">Efron et al. (2001)</xref>, and <xref ref-type="bibr" rid="c18">Storey (2002)</xref> and the more recent work of <xref ref-type="bibr" rid="c16">Scott et al. (2014)</xref>, which frames the concept of FDR <italic>regression</italic> and extends the concepts of FDR and <italic>&#x03C0;</italic><sub>0</sub> to incorporate covariates, represented by additional meta-data. Our focus will be on estimating the covariate-specific <italic>&#x03C0;</italic><sub>0</sub>. We will also show how this can be seen as an extension of our work in <xref ref-type="bibr" rid="c4">Boca et al. (2013)</xref> on set-level inference, where we developed an approach which focused on estimating the fraction of non-null variables in a set, introducing the idea of &#x201C;atoms,&#x201D; non-overlapping sets based on the original annotations, and the concept of the &#x201C;atomic FDR.&#x201D; We provide a more direct approach to estimating the covariate-specific <italic>&#x03C0;</italic><sub>0</sub> and provide a number of theoretical frequentist properties for our estimator.</p>
<p>The remainder of the paper is organized as follows. In <xref ref-type="sec" rid="s2">Section 2</xref> we provide an overview of an example application application to GWAS meta-analysis. In <xref ref-type="sec" rid="s3">Section 3</xref>, we review the definitions of FDR and <italic>&#x03C0;</italic><sub>0</sub> and extend <italic>&#x03C0;</italic><sub>0</sub> to consider conditioning on a specific covariate. In <xref ref-type="sec" rid="s4">Section 4</xref>, we discuss estimation and inference procedures for the covariate-specific <italic>&#x03C0;</italic><sub>0</sub> in the FDR regression framework. In <xref ref-type="sec" rid="s5">Section 5</xref>, we consider special cases within the FDR regression framework, including how the no covariates case and the case where the features are partitioned return us to the &#x201C;standard&#x201D; estimation procedures. In <xref ref-type="sec" rid="s6">Section 6</xref>, we explore some theoretical properties of the estimator, including showing that, under certain conditions, it is a conservative estimator of the covariate-level <italic>&#x03C0;</italic><sub>0</sub>, its variance has an upper bound which can be calculated from the given data, and it is an asymptotically conservative estimator of the covariate-level <italic>&#x03C0;</italic><sub>0</sub>. In <xref ref-type="sec" rid="s7">Section 7</xref> and <xref ref-type="sec" rid="s8">Section 8</xref>, we consider simulations and an analysis of GWAS data. Finally, <xref ref-type="sec" rid="s9">Section 9</xref> provides our statement of reproducibility and <xref ref-type="sec" rid="s10">Section 10</xref> provides the discussion.</p></sec>
<sec id="s2"><label>2</label><title>Example application: adjusting for sample size and allele frequency in GWAS meta-analysis</title>
<p>As we have described, there are a variety of situations where meta-data could be valuable for improving estimation of the prior probability a hypothesis is true or false. Here we consider an example from the meta-analysis of data from genome-wide association studies (GWAS) for body mass index (<xref ref-type="bibr" rid="c13">Locke et al., 2015</xref>).</p>
<p>In a GWAS, data are collected for a large number of genomic loci called single nucleotide polymorphisms (SNPs) <xref ref-type="bibr" rid="c8">Hirschhorn and Daly (2005)</xref>. Each person has a copy of the DNA at each SNP inherited from their mother and from their father. At each locus there are usually one of two types of DNA, called alleles, that can be inherited, denoted <italic>A</italic> and <italic>a</italic>. In general, <italic>A</italic> refers to the variant that is more common in the population being studied and <italic>a</italic> to the variant that is less common. Each person has a genotype for that SNP of the form <italic>AA</italic>, <italic>Aa</italic>, or <italic>aa</italic>. The number of copies of <italic>a</italic>, commonly called the minor allele - is assumed to follow a binomial distribution.</p>
<p>In a GWAS, each individual has the alleles for hundreds of thousands of SNPs measured along with some outcomes of interest like body mass index (BMI). Then each SNP is tested for association with the outcome in a regression model and p-values are calculated for the association. GWAS studies have grown to sample sizes of tens of thousands of individuals. But the largest studies consist of meta-analyses combining multiple studies <xref ref-type="bibr" rid="c14">Neale et al. (2010)</xref>; <xref ref-type="bibr" rid="c8">Hirschhorn and Daly (2005)</xref>. In these studies, the sample size may not be the same for each SNP, for example if different individuals are measured with different technologies which measure different SNPs. As a result, the sample size could be considered as a meta-data covariate.</p>
<p>A second covariate of interest could be the frequency of the minor allele <italic>a</italic> in the population. The power to detect associations increases with increasing minor allele frequency. This is related to the idea that logistic regression is more powerful for outcomes that occur frequently.</p>
<p>Here we consider data from the Genetic Investigation of ANthropometric Traits (GIANT) consortium, specifically the genome-wide association study for BMI (<xref ref-type="bibr" rid="c13">Locke et al., 2015</xref>). The GIANT consortium performed a meta-analysis of 329,224 individuals measuring 2,555,510 SNPs and tested each for association with BMI. Here we will consider using a regression model to estimate a prior probability for association for each SNP conditional on the SNP-specific sample size and allele frequency.</p></sec>
<sec id="s3"><label>3</label><title>Covariate-specific &#x03C0;<sub>0</sub></title>
<p>We will now review the main concepts behind the FDR and the <italic>a priori</italic> probability that a null hypothesis is true, and consider the extension to the covariate-specific FDR, and the covariate-specific <italic>a priori</italic> probability. A natural mathematical definition of the FDR would be:</p>
<disp-formula><alternatives><graphic xlink:href="035675_ueqn1.gif"/></alternatives></disp-formula>
<p>However, <italic>R</italic> is a random variable that can be equal to 0, so the definition that is generally used is:</p>
<p>
<disp-formula id="eqn1"><alternatives><graphic xlink:href="035675_eqn1.gif"/></alternatives></disp-formula>
namely the expected fraction of false discoveries among all discoveries multiplied by the probability of making at least one rejection.</p>
<p>We index the <italic>m</italic> null hypotheses being considered by 1 <italic>&#x2264; i &#x2264; m</italic>: <italic>H</italic><sub>01</sub><italic>, H</italic><sub>02</sub><italic>,&#x2026;,H</italic><sub>0</sub><italic><sub>m</sub></italic>. For each <italic>i</italic>, the corresponding null hypothesis <italic>H</italic><sub>0</sub><italic><sub>i</sub></italic> can be considered as being about a binary parameter <italic>&#x03B8;<sub>i</sub></italic>, such that:</p>
<disp-formula><alternatives><graphic xlink:href="035675_ueqn2.gif"/></alternatives></disp-formula>
<p>Thus, assuming that <italic>&#x03B8;<sub>i</sub></italic> are identically distributed, the <italic>a priori</italic> probability that a feature is null is:</p>
<disp-formula id="eqn2"><alternatives><graphic xlink:href="035675_eqn2.gif"/></alternatives></disp-formula>
<p>We now extend the definition of <italic>&#x03C0;</italic><sub>0</sub> to consider conditioning on a covariate <bold>X</bold><italic><sub>i</sub></italic>, where <bold>X</bold><italic><sub>i</sub></italic> is a column vector of length <italic>c</italic>, possibly with <italic>c</italic> &#x003D; 1:
<statement><title>Definition 1</title>
<p>
<disp-formula><alternatives><graphic xlink:href="035675_ueqn3.gif"/></alternatives></disp-formula></p>
</statement></p></sec>
<sec id="s4"><label>4</label><title>Estimation and inference for covariate-specific &#x03C0;<sub>0</sub> in the FDR regression framework</title>
<p>We will now discuss the estimation and inference procedures for <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold><italic><sub>i</sub></italic>) in a FDR regression framework. We assume that a hypothesis test is performed for each <italic>i</italic>, summarized by a p-value <italic>P<sub>i</sub></italic>. At a given threshold 0 <italic>&#x003C; &#x03BB;&#x003C;</italic> 1, we consider the random variables <italic>Y<sub>i</sub></italic>:</p>
<disp-formula id="eqn3"><alternatives><graphic xlink:href="035675_eqn3.gif"/></alternatives></disp-formula>
<p>Thus, <italic>Y<sub>i</sub></italic> is a dichotomous random variable that is 1 when the null hypothesis <italic>H</italic><sub>0</sub><italic><sub>i</sub></italic> is not rejected at an <italic>&#x03B1;</italic>-level of <italic>&#x03BB;</italic> and 0 when it is rejected. Thus, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline2.gif"/></alternatives></inline-formula> for a fixed, given <italic>&#x03BB;</italic>. The null p-values will come from a Uniform(0,1) distribution, while the p-values for the features from the alternative
<disp-formula id="eqn4"><alternatives><graphic xlink:href="035675_eqn4.gif"/></alternatives></disp-formula></p>
<p>The major assumption we make moving forward is that <italic>conditional on the null, the p-values do not depend on the covariates</italic>. In Theorem 2, we prove the major result we will use to derive the estimator for <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold><italic><sub>i</sub></italic>).
<statement><title>Theorem 2</title>
<p><italic>Suppose that m hypotheses tests are performed and that conditional on the null, the p-values do not depend on the covariates. Then:</italic>
<disp-formula><alternatives><graphic xlink:href="035675_ueqn4.gif"/></alternatives></disp-formula></p>
<p><statement><title>Proof</title>
<p>
<disp-formula><alternatives><graphic xlink:href="035675_ueqn5.gif"/></alternatives></disp-formula></p>
<p>Then, using the assumption that conditional on the null, the p-values do not depend on the covariates:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn6.gif"/></alternatives></disp-formula></p>
</statement></p>
</statement></p>
<p>In Corollary 3, we show the corresponding result for the no-covariate case. This result is easy to prove directly, but we consider it as a corrolary to Theorem 2 to show that there are no identifiability problems with the extension to covariates.
<statement><title>Corollary 3</title>
<p><italic>Suppose that m hypotheses tests are performed and that conditional on the null, the p-values do not depend on the covariates. Then:</italic>
<disp-formula><alternatives><graphic xlink:href="035675_ueqn7.gif"/></alternatives></disp-formula></p>
<p><statement><title>Proof</title>
<p>Applying the law of iterated expectations:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn8.gif"/></alternatives></disp-formula></p>
<p>We complete the proof by using:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn9.gif"/></alternatives></disp-formula>
where <italic>&#x03BD;</italic> is typically either the Lebesgue measure over a subset &#x211D; or the counting measure over a subset of &#x211A;, and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline3.gif"/></alternatives></inline-formula> is the cumulative distribution function for <bold>X</bold><italic><sub>i</sub></italic>. Here we are implicitly assuming some distribution for <bold>X</bold><italic><sub>i</sub></italic> as well. Everywhere else we are conditioning on <bold>X</bold>.</p>
<p>We first review the procedure which applies Corollary 3 to lead to the estimator of <italic>&#x03C0;</italic><sub>0</sub> for the no-covariate case, which is also used in <xref ref-type="bibr" rid="c18">Storey (2002)</xref>, then develop a procedure based on Theorem 2 to obtain an estimator of <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold>). Both of them are based on assuming reasonably powered tests and a large enough <italic>&#x03BB;</italic>, so that:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn10.gif"/></alternatives></disp-formula></p>
<p>Corollary 3 then leads to:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn11.gif"/></alternatives></disp-formula>
resulting in:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn12.gif"/></alternatives></disp-formula></p>
<p>Using a method-of-moments approach, we consider the estimator:
<disp-formula id="eqn5"><alternatives><graphic xlink:href="035675_eqn5.gif"/></alternatives></disp-formula>
which is used in <xref ref-type="bibr" rid="c18">Storey (2002)</xref>. Applying the same steps with Theorem 2, we get:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn13.gif"/></alternatives></disp-formula></p>
<p>We can use a regression framework to estimate <italic>E</italic>[<italic>Y<sub>i</sub>|</italic><bold>X</bold><italic><sub>i</sub></italic> &#x003D; <bold>x</bold><italic><sub>i</sub></italic>], then estimate <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold>) by:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn14.gif"/></alternatives></disp-formula></p>
<p>We now denote by <bold>Y</bold> the random vector of length <italic>m</italic> with the <italic>i<sub>th</sub></italic> element <italic>Y<sub>i</sub></italic> and by <bold>X</bold> the matrix of dimension <italic>m &#x00D7;</italic> (<italic>c</italic> &#x002B; 1), which has the <italic>i<sub>th</sub></italic> row consisting of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline4.gif"/></alternatives></inline-formula>. Moving forward, we will denote by <bold>x</bold> the observed values of the random matrix <bold>X</bold>.</p>
<p>We consider estimators of the form:
<disp-formula id="eqn6"><alternatives><graphic xlink:href="035675_eqn6.gif"/></alternatives></disp-formula>
where <bold>S</bold> &#x003D; <bold>Z</bold>(<bold>Z</bold><italic><sup>T</sup></italic><bold><italic>Z</italic></bold>)&#x2212;<bold><italic>Z</italic></bold><italic><sup>T</sup></italic> for some <italic>m &#x00D7; p</italic> matrix <bold>Z</bold> with <italic>p&#x003C;m</italic> and <italic>rank</italic>(<bold>Z</bold>)&#x003D; <italic>d &#x2264; p</italic> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline5.gif"/></alternatives></inline-formula> is the <italic>i<sub>th</sub></italic> row of <bold>S</bold>; in particular, we can have <bold>Z</bold> &#x003D; <bold>X</bold> for linear regression or have <bold>Z</bold> also include polynomial or spline terms. If <italic>d</italic> &#x003D; <italic>p</italic>, then <bold>Z</bold><italic><sup>T</sup></italic><bold>Z</bold> is invertible; if <italic>d &#x003C; p</italic>, one can use any pseudoinverse of <bold>Z</bold><italic><sup>T</sup></italic><bold><italic>Z</italic></bold>, since the projection matrix is unique. If we assume that the p-values are independent, we can also use bootstrap samples of them to obtain a confidence interval for <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline6.gif"/></alternatives></inline-formula>. The details for the entire estimation and inference procedure are in Algorithm 1.</p>
</statement></p>
</statement></p>
<sec id="s4a"><label>4.1</label><title>Algorithm 1: Estimation and inference for <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline7.gif"/></alternatives></inline-formula></title>
<p><list list-type="order">
<list-item><p>Obtain the p-values <italic>P</italic><sub>1</sub><italic>,P</italic><sub>2</sub><italic>,&#x2026;,P<sub>m</sub></italic>, for the <italic>m</italic> hypothesis tests.</p></list-item>
<list-item><p>For a given threshold <italic>&#x03BB;</italic>, obtain <italic>Y<sub>i</sub></italic> &#x003D; 1(<italic>P<sub>i</sub> &#x003E;&#x03BB;</italic>) for 1 <italic>&#x2264; i &#x2264; m</italic>.</p></list-item>
<list-item><p>Choose a design matrix <bold>Z</bold>, estimate <italic>E</italic>[<italic>Y<sub>i</sub>|</italic><bold>X</bold><italic><sub>i</sub></italic> &#x003D; <bold>x</bold><italic><sub>i</sub></italic>] by:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn15.gif"/></alternatives></disp-formula> where <bold>S</bold> &#x003D; <bold>Z</bold>(<bold>Z</bold><italic><sup>T</sup></italic><bold><italic>Z</italic></bold>)&#x2212;<bold><italic>Z</italic></bold><italic><sup>T</sup></italic>, and <italic>&#x03C0;</italic><sub>0</sub>(<bold>x<italic><sub>i</sub></italic></bold>) by:
<disp-formula id="eqn7"><alternatives><graphic xlink:href="035675_eqn7.gif"/></alternatives></disp-formula></p></list-item>
<list-item><p>Take <italic>B</italic> bootstrap samples of <italic>P</italic><sub>1</sub><italic>, P</italic><sub>2</sub><italic>,&#x2026;,P<sub>m</sub></italic> and calculate the bootstrap estimates <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline8.gif"/></alternatives></inline-formula> for 1 <italic>&#x2264; b &#x2264; B</italic> using the procedure described above.</p></list-item>
<list-item><p>Form a 1 <italic>&#x2212; &#x03B1;</italic> upper confidence interval for <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline9.gif"/></alternatives></inline-formula> by taking the 1 <italic>&#x2212; &#x03B1;</italic> quantile of the <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline10.gif"/></alternatives></inline-formula> as the upper confidence bound, the lower confidence bound being 0.</p></list-item></list></p></sec></sec>
<sec id="s5"><label>5</label><title>Special cases for covariate-specific &#x03C0;<sub>0</sub></title>
<sec id="s5a"><label>5.1</label><title>No covariates</title>
<p>If we do not consider any covariates, the usual estimator <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline11.gif"/></alternatives></inline-formula> from <xref ref-type="disp-formula" rid="eqn5">Equation 5</xref> can be deduced from applying Algorithm 1 by fitting a linear regression with just an intercept.</p></sec>
<sec id="s5b"><label>5.2</label><title>Partioning the features</title>
<p>Now assume that the set of features is partitioned into <italic>S</italic> sets, namely that a collection of sets <italic>S</italic> &#x003D; <italic>&#x007B;A<sub>s</sub></italic>: 1 <italic>&#x2264; s &#x2264; S&#x007D;</italic> is considered such that all sets are non-empty, pairwise disjoint, and have the set of all the features as their union. Note that the index <italic>s</italic> does not need to indicate any kind of ordering of the sets. For example, such partioning could be induced by considering all possible atoms resulting from gene-set annotations, or could consist of brain regions of interest in a functional imaging analysis, when considering only the genes or voxels that are annotated (<xref ref-type="bibr" rid="c4">Boca et al., 2013</xref>). We can consider this in the covariate framework we developed by taking <bold>x</bold><italic><sub>i</sub></italic> to be a vector of length <italic>S &#x2212;</italic> 1, which consists of 0s at all positions with the exception of a value of 1 at the index corresponding to the single set <italic>A<sub>s</sub> &#x2208; S</italic> such that <italic>i &#x2208; A<sub>s</sub></italic>, for 1 <italic>&#x2264; s &#x2264; S &#x2212;</italic> 1. Set <italic>A<sub>S</sub></italic> representing the &#x201C;baseline set,&#x201D; so that <bold>x</bold><italic><sub>i</sub></italic> is a vector of length <italic>S &#x2212;</italic> 1 consisting of just 0s if <italic>i &#x2208; A<sub>S</sub></italic>. In notation commonly used in linear algebra:</p>
<disp-formula id="eqn8"><alternatives><graphic xlink:href="035675_eqn8.gif"/></alternatives></disp-formula>
<p>Taking into account the partition, a natural way of estimating <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold><italic><sub>i</sub></italic>) is to just apply the estimator <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline12.gif"/></alternatives></inline-formula> from <xref ref-type="disp-formula" rid="eqn5">Equation 5</xref> to each of the <italic>S</italic> sets:</p>
<disp-formula><alternatives><graphic xlink:href="035675_ueqn16.gif"/></alternatives></disp-formula>
<p>A related idea has been proposed for partitioning hypotheses into sets to improve power <xref ref-type="bibr" rid="c6">Efron (2008)</xref>. These results can also be obtained by estimating <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline13.gif"/></alternatives></inline-formula> via Algorithm 1 by fitting a linear regression with an intercept and the covariates <bold>x</bold><italic><sub>i</sub></italic>.</p></sec></sec>
<sec id="s6"><label>6</label><title>Theoretical results</title>
<p>We now proceed to explore some theoretical properties of the estimator <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline14.gif"/></alternatives></inline-formula>. In what follows, <bold>1</bold> is the <italic>m &#x00D7;</italic> 1 vector consisting of just 1s. We will also use the notation:</p>
<disp-formula><alternatives><graphic xlink:href="035675_ueqn17.gif"/></alternatives></disp-formula>
<p>Lemma 4 below gives the bias of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline15.gif"/></alternatives></inline-formula>. Note that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline16.gif"/></alternatives></inline-formula>, since <italic>&#x03BB; &#x2264;</italic> 1<italic>,G</italic>(<italic>&#x03BB;</italic>) <italic>&#x2264;</italic> 1, and <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold><italic><sub>i</sub></italic>) <italic>&#x2264;</italic> 1, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline17.gif"/></alternatives></inline-formula>. The second term could, however, be negative, and depends on the level of non-linearity present in <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold><italic><sub>i</sub></italic>) and misspecification of the model as encapsulated in the design matrix <bold>Z</bold>.
<statement><title>Lemma 4</title>
<p>The bias of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline18.gif"/></alternatives></inline-formula> is:
<disp-formula id="eqn9"><alternatives><graphic xlink:href="035675_eqn9.gif"/></alternatives></disp-formula></p>
<p><statement><title>Proof</title>
<p>By <xref ref-type="disp-formula" rid="eqn7">Eq. 7</xref>:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn18.gif"/></alternatives></disp-formula></p>
<p>Using the result of Theorem 2:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn19.gif"/></alternatives></disp-formula></p>
<p>Given that <bold>S</bold> &#x003D; <bold>Z</bold>(<bold>Z</bold><italic><sup>T</sup></italic><bold><italic>Z</italic></bold>)<sup>&#x2212;</sup><bold><italic>Z</italic></bold><sup>T</sup> and that the first column of <bold>Z</bold> is <bold>1</bold>, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline19.gif"/></alternatives></inline-formula>. This is a known result used in linear regression. It can be obtained using the fact that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline20.gif"/></alternatives></inline-formula>, where <bold>Z</bold><sub>1</sub> is a matrix consisting of <italic>d</italic> linearly independent columns of <bold>Z</bold>, including the first column, then applying the formula for the inverse of a block matrix. Thus:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn20.gif"/></alternatives></disp-formula></p>
</statement></p>
</statement></p>
<p>Theorem 5 shows that, if the model is correctly specified, i.e. <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold>)&#x003D; <bold>Z</bold><italic>&#x03B2;</italic> for some vector <italic>&#x03B2;</italic> of length <italic>c</italic> &#x002B; 1, then <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline21.gif"/></alternatives></inline-formula> is a conservative estimate of <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold><italic><sub>i</sub></italic>).
<statement><title>Theorem 5</title>
<p><italic>If &#x03C0;</italic><sub>0</sub>(<bold>x</bold>)&#x003D; <bold>Z</bold><italic>&#x03B2; for some vector &#x03B2; of length c &#x002B;1, then</italic> <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline22.gif"/></alternatives></inline-formula> <italic>is a conservative estimate of &#x03C0;<sub>0</sub>(</italic><bold><italic>x</italic></bold><italic><sub>i</sub>), i.e.:</italic>
<disp-formula><alternatives><graphic xlink:href="035675_ueqn21.gif"/></alternatives></disp-formula></p>
<p><statement><title>Proof</title>
<p>In this case, using the fact that <bold>S</bold> is a projection matrix onto the space spanned by the columns of <bold>Z</bold> and therefore <bold>SZ</bold> &#x003D; <bold>Z</bold>:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn22.gif"/></alternatives></disp-formula>
so:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn23.gif"/></alternatives></disp-formula></p>
</statement></p>
</statement>
<statement><title>Remark 6</title>
<p><italic>If the same &#x03C0;<sub>0</sub> is shared by all the features, i.e. it does not change based on any covariates, then <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline23.gif"/></alternatives></inline-formula> is a conservative estimate of &#x03C0;<sub>0</sub>. This result is also described elsewhere, for example in <xref ref-type="bibr" rid="c18">Storey (2002)</xref>. We note here that it can also be obtained as a direct consequence of Theorem 5. Theorem 5 also applies to the case where the covariates concern the partitioning of the features, as in <xref ref-type="sec" rid="s5b">Section 5.2</xref></italic>.</p>
</statement></p>
<p>Lemma 7 gives a bound on <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline24.gif"/></alternatives></inline-formula> in terms of <bold>S</bold> and <italic>&#x03BB;</italic>. We note that this bound can always be calculated from the given data.
<statement><title>Lemma 7</title>
<p><italic>Assuming that Y<sub>i</sub> are independent conditional on</italic> <bold><italic>X</italic></bold> <italic>and that all the features are independent:</italic>
<disp-formula><alternatives><graphic xlink:href="035675_ueqn24.gif"/></alternatives></disp-formula>
<italic>where S<sub>ii</sub> are the diagonal elements of</italic> <bold>S</bold>.
<statement><title>Proof</title>
<p>By <xref ref-type="disp-formula" rid="eqn7">Eq. 7</xref>:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn25.gif"/></alternatives></disp-formula></p>
<p>By independence of <italic>Y<sub>i</sub></italic> conditional on <bold>X</bold> and independence of the features:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn26.gif"/></alternatives></disp-formula></p>
<p>Since <italic>Y<sub>j</sub>|</italic><bold>X</bold><italic><sub>j</sub></italic> &#x003D; <bold>x</bold><italic><sub>j</sub></italic> is a Bernoulli random variable, its variance is <italic>P</italic> [<italic>Y<sub>j</sub></italic> &#x003D; 1<italic>|</italic><bold>X</bold><italic><sub>j</sub></italic> &#x003D; <bold>x</bold><italic><sub>j</sub></italic>]&#x007B;1 <italic>&#x2212; P</italic> [<italic>Y<sub>j</sub></italic> &#x003D;1<italic>|</italic><bold>X</bold><italic><sub>j</sub></italic> &#x003D; <bold>x</bold><italic><sub>j</sub></italic>]&#x007D;, which has <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline25.gif"/></alternatives></inline-formula> as its maximal value, attained at <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline26.gif"/></alternatives></inline-formula>. This leads to:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn27.gif"/></alternatives></disp-formula>
the last equality being a direct consequence of <bold>S</bold> being a symmetric idempotent matrix.</p>
</statement></p>
</statement></p>
<p>Theorem 8 shows that, if <italic>S<sub>ii</sub> &#x2192;</italic> 0 as <italic>m &#x2192;&#x221E;</italic> holds alongside the assumptions of Lemma 7, then <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline27.gif"/></alternatives></inline-formula> is a consistent estimator of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline28.gif"/></alternatives></inline-formula>.
<statement><title>Theorem 8</title>
<p><italic>If Y<sub>i</sub> are independent conditional on</italic> <bold><italic>X</italic></bold><italic>, all the features are independent, and S<sub>ii</sub> &#x2192; 0 as m &#x2192; &#x221E;</italic>,
<disp-formula><alternatives><graphic xlink:href="035675_ueqn200.gif"/></alternatives></disp-formula></p>
<statement><title>Proof</title>
<p>By Chebyshev&#x2019;s inequality, for all <italic>&#x220A; &#x003E;</italic> 0:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn28.gif"/></alternatives></disp-formula></p>
<p>Then, by using the stated assumptions and Lemma 7, we get that
<disp-formula><alternatives><graphic xlink:href="035675_ueqn29.gif"/></alternatives></disp-formula></p>
</statement>
</statement></p>
<p>Is it likely or even possible that <italic>S<sub>ii</sub> &#x2192;</italic> 0 as <italic>m &#x2192;&#x221E;</italic>? In general this will be the case, unless there are some <bold>x</bold><italic><sub>i</sub></italic> which have very high leverage on the regression line by being far from the overall mean of the <bold>x</bold><italic><sub>i</sub></italic> vectors. The reason for this is that <bold>S</bold> being idempotent implies that <italic>Tr</italic>(<bold>S</bold>)&#x003D; <italic>rank</italic>(<bold>S</bold>), and given that <bold>S</bold> &#x003D; <bold>Z</bold>(<bold>Z</bold><italic><sup>T</sup></italic><bold><italic>Z</italic></bold>)<sup>&#x2212;</sup><bold><italic>Z</italic></bold><sup>T</sup>, <italic>Tr</italic>(<bold>S</bold>)&#x003D; <italic>rank</italic>(<bold>Z</bold>)&#x003D; <italic>d</italic>, which means that the mean value of <italic>S<sub>ii</sub></italic> is <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline29.gif"/></alternatives></inline-formula>.The diagonal elements of <bold>S</bold> are also the leverages for the individual data points, with a &#x201C;rule of thumb&#x201D; of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline30.gif"/></alternatives></inline-formula> often being used to identify high leverage points (<xref ref-type="bibr" rid="c9">Hoaglin and Welsch, 1978</xref>). It can also be shown that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline31.gif"/></alternatives></inline-formula> We first note that 0 <italic>&#x2264; S<sub>ii</sub> &#x2264;</italic> 1, by once again using the fact that <bold>S</bold> is idempotent:</p>
<disp-formula><alternatives><graphic xlink:href="035675_ueqn30.gif"/></alternatives></disp-formula>
<p>We get the improved lower bound by using the fact that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline32.gif"/></alternatives></inline-formula> and Cauchy&#x2019;s inequality:</p>
<disp-formula><alternatives><graphic xlink:href="035675_ueqn31.gif"/></alternatives></disp-formula>
<p>Furter using the fact that the mean value of <italic>S<sub>ii</sub></italic> is <italic>d/m</italic> and the inequalities between the arithmetic mean and the minimum and maximum values, we obtain:</p>
<disp-formula><alternatives><graphic xlink:href="035675_ueqn32.gif"/></alternatives></disp-formula>
<p><xref ref-type="bibr" rid="c9">Hoaglin and Welsch (1978)</xref> discuss the case where <italic>S<sub>ii</sub></italic> &#x003D; 1, which occurs when the model is fully saturated, predicting the outcome exactly.</p>
<p>Thus, by Theorems 5 and 8, under reasonable conditions, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline33.gif"/></alternatives></inline-formula> is a conservative and an asymptotically conservative estimator of <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold><italic><sub>i</sub></italic>).</p>
<p>We note that our approach to estimating <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold><italic><sub>i</sub></italic>) does not place any restrictions on its range. Thus, in practice, the values will be truncated to be between 0 and 1. In the following theorem, we show that implementing this truncation decreases the mean squared error of the estimator. The approach is similar to that taken in Theorem 2 of <xref ref-type="bibr" rid="c18">Storey (2002)</xref>.
<statement><title>Theorem 9</title>
<p>Let
<disp-formula><alternatives><graphic xlink:href="035675_ueqn33.gif"/></alternatives></disp-formula></p>
<p><italic>Then:</italic>
<disp-formula><alternatives><graphic xlink:href="035675_ueqn34.gif"/></alternatives></disp-formula></p>
<p><statement><title>Proof</title>
<p>We prove this result by showing that:
<disp-formula id="eqn10"><alternatives><graphic xlink:href="035675_eqn10.gif"/></alternatives></disp-formula>
and
<disp-formula id="eqn11"><alternatives><graphic xlink:href="035675_eqn11.gif"/></alternatives></disp-formula></p>
<p>Then, we can combine them as follows:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn35.gif"/></alternatives></disp-formula></p>
<p>In 10:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn36.gif"/></alternatives></disp-formula>
because in this region <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline34.gif"/></alternatives></inline-formula>.</p>
<p>In 11:
<disp-formula><alternatives><graphic xlink:href="035675_ueqn37.gif"/></alternatives></disp-formula>
because in this region <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline35.gif"/></alternatives></inline-formula>.</p>
</statement></p>
</statement></p></sec>
<sec id="s7"><label>7</label><title>Simulations</title>
<p>We first describe simulations which give a better idea of the usefulness of Lemma 4 and Theorem 5. We implemented a variety of scenarios, with different values of <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold><italic><sub>i</sub></italic>) and <bold>Z</bold>, representing different levels of linearity and model misspecification. In each case, there are <italic>m</italic> &#x003D;1, 000 features and <italic>&#x03BB;</italic> was taken to be 0.8. For the scenarios where <bold>x</bold><italic><sub>i</sub></italic> is a scalar, its values were taken to be evenly spaced, while for the scenarios where it is a vector, the values the first component were taken to be evenly spaced, while the second component was a step function, with the first <italic>m/</italic>2 values being equal to 1 and the remaining <italic>m/</italic>2 values being equal to 0. We then randomly generated whether each feature was from the null or alternative distributions, so that the null hypothesis was true for the features for which a success was drawn from the Bernoulli distribution with probability <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold><italic><sub>i</sub></italic>).</p>
<p>For the null features, p-values were randomly sampled from a <italic>U</italic>(0, 1) distribution, while for the alternative features, they were sampled from a <italic>&#x03B2;</italic>(<italic>a, b</italic>) distribution, with <italic>a</italic> &#x003D;1<italic>,b</italic> &#x003D; 50. Sampling the true positive p-values from a Beta distribution is justified in light of recent statistical research (<xref ref-type="bibr" rid="c2">Allison et al., 2002</xref>; <xref ref-type="bibr" rid="c15">Pounds and Morris, 2003</xref>; <xref ref-type="bibr" rid="c1">Allison et al., 2006</xref>; <xref ref-type="bibr" rid="c12">Leek and Storey, 2011</xref>). Plots of <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold><italic><sub>i</sub></italic>) versus <bold>x</bold><italic><sub>i</sub></italic> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline36.gif"/></alternatives></inline-formula> and <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold><italic><sub>i</sub></italic>) versus the index <italic>i</italic> are in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Note that, as expected, the closer we get to having a correctly specified model with a linear estimator, the better the estimation is.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure"><label>Figure 1:</label>
<caption><p>Different simulation scenarios. The true function <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold><italic><sub>i</sub></italic>) is plotted in a thick black line, while the empirical means of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline37.gif"/></alternatives></inline-formula>, assuming different modelling approaches are shown in the dashed orange line and the dashed-and-dotted blue line. In c), different terms are used in the regression for <italic>x<sub>i</sub></italic><sub>1</sub>, while the true values are used for <italic>x</italic><sub>2</sub><italic><sub>i</sub></italic>.</p></caption>
<graphic xlink:href="035675_fig1.tif"/>
</fig>
<p>Next, we use the same set of simulations to estimate the variance of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline38.gif"/></alternatives></inline-formula> and compare it to the bound from Lemma 7. Plots of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline39.gif"/></alternatives></inline-formula> and its upper bound versus the index <italic>i</italic> are presented in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure"><label>Figure 2:</label>
<caption><p>The same simulation scenarios as in <xref ref-type="fig" rid="fig1">Figure 1</xref>. The empirical variance of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline40.gif"/></alternatives></inline-formula> is in a continuous black line, while the upper bound obtained in Lemma 7 is plotted in a dotted red. Scenarios a) and b) fit the same model on the same values of <italic>x<sub>i</sub></italic>, resulting in identical plots.</p></caption>
<graphic xlink:href="035675_fig2.tif"/>
</fig>
<p>Finally, we used the same scenarios, but varied the number of features in order to see whether Theorem 8, which says that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline41.gif"/></alternatives></inline-formula> is a consistent estimator of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline42.gif"/></alternatives></inline-formula>, holds. The number of features was taken to be either <italic>m</italic> &#x003D; 10, 100, 1, 000 or 10, 000 and the components of <bold>x</bold><italic><sub>i</sub></italic> were set as before. For each value of <italic>m</italic> considered, we calculated <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline43.gif"/></alternatives></inline-formula>. The results, shown in <xref ref-type="table" rid="tbl2">Table 2</xref>, indeed justify the assumptions of Theorem 8. In general, <bold>Z</bold><italic><sup>T</sup></italic><bold>Z</bold> can be written as a matrix of the sample means of pairs of the <italic>p</italic> variables (i.e. <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline44.gif"/></alternatives></inline-formula> for the variables <italic>i<sub>th</sub></italic> and <italic>j<sub>th</sub></italic> variables) multiplied by <italic>m</italic>, therefore all the terms in <bold>S</bold> &#x003D; <bold>Z</bold>(<bold>Z</bold><italic><sup>T</sup></italic><bold><italic>Z</italic></bold>)<sup>&#x2212;1</sup><bold><italic>Z</italic></bold><sup>T</sup> include combinations of the individual variables <italic>Z<sub>ij</sub></italic> and the sample means of combinations, with number of terms depending on <italic>p</italic>, which is fixed, multiplied by 1<italic>/m</italic>, if <bold>Z</bold><italic><sup>T</sup></italic><bold>Z</bold> is invertible. Thus, as long as all the means are bounded as <italic>m &#x2192;&#x221E;</italic>, as they would be in the case of equally spaced values, then <italic>S<sub>ii</sub> &#x2192;</italic> 0 as <italic>m &#x2192;&#x221E;</italic>, fulfilling the conditions for Theorem 8.</p>
<table-wrap id="tbl2" orientation="portrait" position="float"><label>Table 2:</label>
<caption><p>Values of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="035675_inline45.gif"/></alternatives></inline-formula> for the simulation scenarios in <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2</xref>, using the panel labels in <xref ref-type="fig" rid="fig2">Figure 2</xref>, for different values for the number of features, <italic>m</italic>. Scenarios a) and b) fit the same model, therefore have the same upper bounds.</p></caption>
<graphic xlink:href="035675_tbl2.tif"/>
</table-wrap></sec>
<sec id="s8"><label>8</label><title>Data analysis</title>
<p>Here we consider data from the Genetic Investigation of ANthropometric Traits (GIANT) consortium, specifically the genome-wide association study for body mass index (BMI) (<xref ref-type="bibr" rid="c13">Locke et al., 2015</xref>). From a total of 2,555,510 SNPs, we removed the SNPs which did not have minor allele frequencies (MAFs) listed for the HapMap CEU population, leading to 2,500,573 SNPs. For each of these SNPs, we considered the p-values from the test of association with BMI and the meta-data covariates consisting of the number of individuals (N) considered for each SNP and the minor allele frequencies (MAFs) in the HapMap CEU population, since it is well-known that both sample size and MAF have an impact on p-values, with larger sample sizes and MAFs leading to more significant results.</p>
<p>The model we considered uses natural cubic splines with 5 degrees of freedom to model N and 3 discrete categories for the MAFs. <xref ref-type="fig" rid="fig3">Figure 3</xref> shows the dependence of p-values on sample sizes within this dataset. <xref ref-type="fig" rid="fig4">Figure 4</xref> shows the estimates of <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold><italic><sub>i</sub></italic>) plotted against the sample size N, stratified by the CEU MAFs for a random subset of 50,000 SNPs.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure"><label>Figure 3:</label>
<caption><p>Histograms of p-values for the SNP-BMI tests of association from the GIANT consortium. Panel a) shows the distribution for all sample sizes <italic>N</italic> (2,500,573 SNPs), while panel b) shows the subset <italic>N &#x003C;</italic>200,000 (187,114 SNPs).</p></caption>
<graphic xlink:href="035675_fig3.tif"/>
</fig>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure"><label>Figure 4:</label>
<caption><p>Plot of the estimates of <italic>&#x03C0;</italic><sub>0</sub>(<bold>x</bold><italic><sub>i</sub></italic>) against the sample size N, stratified by the MAF categories for a random subset of 50,000 SNPs.</p></caption>
<graphic xlink:href="035675_fig4.tif"/>
</fig>
<p>Our results are consistent with intuition - larger sample sizes and larger MAFs lead to a smaller fraction of SNPs estimated to be null. Applying this estimator to the false discovery rate calculation will mean increased power to detect associations for SNPs with large sample sizes and large MAFs, with potentially reduced power for SNPs with the opposite characteristics.</p></sec>
<sec id="s9"><label>9</label><title>Reproducibility</title>
<p>All analyses and simulations in this paper are fully reproducible and the code is available on Github at: <ext-link ext-link-type="uri" xlink:href="https://github.com/SiminaB/Fdr-regression">https://github.com/SiminaB/Fdr-regression</ext-link></p></sec>
<sec id="s10"><label>10</label><title>Discussion</title>
<p>Here we have introduced a regression framework for the proportion of true null hypotheses in a multiple testing framework. We have provided conditions for conservative and consistent estimation of this proportion conditional on covariates. Using simulations we have shown that while the regression estimates may be incorrect under model misspecification the upper bounds on the variance of the estimator hold even for inaccurate models.</p>
<p>Applying our estimator to GWAS data from the GIANT consortium demonstrated that, as expected, the estimate of the fraction of null hypotheses decreases with both sample size and minor allele frequency. It is a well known and problematic phenomenon that p-values for all features decrease as the sample size increases. This is because the null is rarely precisely true for any given feature. One interesting consequence of our estimates is that we can calibrate what fraction of p-values appear to be drawn from the non-null distribution as a function of sample size, potentially allowing us to quantify the effect of the&#x201C;large sample size means small p-values&#x201D; problem directly.</p>
<p>A range of other applications for our methodology are also possible by modifying our regression framework, including estimating false discovery rates for gene sets (<xref ref-type="bibr" rid="c4">Boca et al., 2013</xref>), estimating science-wise false discovery rates (<xref ref-type="bibr" rid="c11">Jager and Leek, 2013</xref>), or improving power in high-throughput biological studies (<xref ref-type="bibr" rid="c10">Ignatiadis et al., 2015</xref>).</p>
</sec>
</body>
<back>
<ref-list><title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Allison</surname>, <given-names>D. B.</given-names></string-name>, <string-name><surname>Cui</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Page</surname>, <given-names>G. P.</given-names></string-name>, and <string-name><surname>Sabripour</surname>, <given-names>M.</given-names></string-name> (<year>2006</year>). <article-title>Microarray data analysis: from disarray to consolidation and consensus</article-title>. <source>Nature Reviews Genetics</source>, <volume>7</volume>(<issue>1</issue>):<fpage>55</fpage>&#x2013;<lpage>65</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Allison</surname>, <given-names>D. B.</given-names></string-name>, <string-name><surname>Gadbury</surname>, <given-names>G. L.</given-names></string-name>, <string-name><surname>Heo</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Fern&#x00E1;ndez</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>C.-K.</given-names></string-name>, <string-name><surname>Prolla</surname>, <given-names>T. A.</given-names></string-name>, and <string-name><surname>Weindruch</surname>, <given-names>R.</given-names></string-name> (<year>2002</year>). <article-title>A mixture model approach for the analysis of microarray gene expression data</article-title>. <source>Computational Statistics &#x0026; Data Analysis</source>, <volume>39</volume>(<issue>1</issue>):<fpage>1</fpage>&#x2013;<lpage>20</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="other"><string-name><surname>Benjamini</surname>, <given-names>Y.</given-names></string-name> and <string-name><surname>Hochberg</surname>, <given-names>Y.</given-names></string-name> (<year>1995</year>). <article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title>. <source>Journal of the Royal Statistical Society. Series B (Methodological)</source>, pages <fpage>289</fpage>&#x2013;<lpage>300</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="other"><string-name><surname>Boca</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Corrada Bravo</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Caffo</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Leek</surname>, <given-names>J. T.</given-names></string-name>, and <string-name><surname>Parmigiani</surname>, <given-names>G.</given-names></string-name> (<year>2013</year>). <article-title>A decision-theory approach to interpretable set analysis for high-dimensional data</article-title>. <source>Biometrics</source>. <pub-id pub-id-type="doi">doi: 10.1111/biom.12060.</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Brown</surname>, <given-names>O. P.</given-names></string-name> (<year>1995</year>). <article-title>Quantitative monitoring of gene expression patterns with a complementary dna microarray</article-title>. <source>Science</source>, <volume>270</volume>:<fpage>467</fpage>&#x2013;<lpage>470</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="other"><string-name><surname>Efron</surname>, <given-names>B.</given-names></string-name> (<year>2008</year>). <article-title>Simultaneous inference: When should hypothesis testing problems be combined?</article-title> <source>The annals of applied statistics</source>, pages <fpage>197</fpage>&#x2013;<lpage>223</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Efron</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Tibshirani</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Storey</surname>, <given-names>J. D.</given-names></string-name>, and <string-name><surname>Tusher</surname>, <given-names>V.</given-names></string-name> (<year>2001</year>). <article-title>Empirical Bayes analysis of a microarray experiment</article-title>. <source>Journal of the American Statistical Association</source>, <volume>96</volume>(<issue>456</issue>):<fpage>1151</fpage>&#x2013;<lpage>1160</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Hirschhorn</surname>, <given-names>J. N.</given-names></string-name> and <string-name><surname>Daly</surname>, <given-names>M. J.</given-names></string-name> (<year>2005</year>). <article-title>Genome-wide association studies for common diseases and complex traits</article-title>. <source>Nature Reviews Genetics</source>, <volume>6</volume>(<issue>2</issue>):<fpage>95</fpage>&#x2013;<lpage>108</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Hoaglin</surname>, <given-names>D. C.</given-names></string-name> and <string-name><surname>Welsch</surname>, <given-names>R. E.</given-names></string-name> (<year>1978</year>). <article-title>The hat matrix in regression and anova</article-title>. <source>The American Statistician</source>, <volume>32</volume>(<issue>1</issue>):<fpage>17</fpage>&#x2013;<lpage>22</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="other"><string-name><surname>Ignatiadis</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Klaus</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Zaugg</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Huber</surname>, <given-names>W.</given-names></string-name> (<year>2015</year>). <article-title>Data-driven hypothesis weighting increases detection power in big data analytics</article-title>. <source>bioRxiv</source>, page <fpage>034330</fpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="other"><string-name><surname>Jager</surname>, <given-names>L. R.</given-names></string-name> and <string-name><surname>Leek</surname>, <given-names>J. T.</given-names></string-name> (<year>2013</year>). <article-title>Empirical estimates suggest most published medical research is true</article-title>. <source>Biostatistics</source>. <pub-id pub-id-type="doi">doi: 10.1093/biostatistics/kxt007.</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Leek</surname>, <given-names>J. T.</given-names></string-name> and <string-name><surname>Storey</surname>, <given-names>J. D.</given-names></string-name> (<year>2011</year>). <article-title>The joint null criterion for multiple hypothesis tests</article-title>. <source>Statistical Applications in Genetics and Molecular Biology</source>, <volume>10</volume>(<issue>1</issue>).</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Locke</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Kahali</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Berndt</surname>, <given-names>S. I.</given-names></string-name>, <string-name><surname>Justice</surname>, <given-names>A. E.</given-names></string-name>, <string-name><surname>Pers</surname>, <given-names>T. H.</given-names></string-name>, <string-name><surname>Day</surname>, <given-names>F. R.</given-names></string-name>, <string-name><surname>Powell</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Vedantam</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Buchkovich</surname>, <given-names>M. L.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>J.</given-names></string-name>, <etal>et al.</etal> (<year>2015</year>). <article-title>Genetic studies of body mass index yield new insights for obesity biology</article-title>. <source>Nature</source>, <volume>518</volume>(<issue>7538</issue>):<fpage>197</fpage>&#x2013;<lpage>206</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Neale</surname>, <given-names>B. M.</given-names></string-name>, <string-name><surname>Medland</surname>, <given-names>S. E.</given-names></string-name>, <string-name><surname>Ripke</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Asherson</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Franke</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Lesch</surname>, <given-names>K.-P.</given-names></string-name>, <string-name><surname>Faraone</surname>, <given-names>S. V.</given-names></string-name>, <string-name><surname>Nguyen</surname>, <given-names>T. T.</given-names></string-name>, <string-name><surname>Sch&#x00E4;fer</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Holmans</surname>, <given-names>P.</given-names></string-name>, <etal>et al.</etal> (<year>2010</year>). <article-title>Meta-analysis of genome-wide association studies of attention-deficit/hyperactivity disorder</article-title>. <source>Journal of the American Academy of Child &#x0026; Adolescent Psychiatry</source>, <volume>49</volume>(<issue>9</issue>):<fpage>884</fpage>&#x2013;<lpage>897</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Pounds</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Morris</surname>, <given-names>S. W.</given-names></string-name> (<year>2003</year>). <article-title>Estimating the occurrence of false positives and false negatives in microarray studies by approximating and partitioning the empirical distribution of p-values</article-title>. <source>Bioinformatics</source>, <volume>19</volume>(<issue>10</issue>):<fpage>1236</fpage>&#x2013;<lpage>1242</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="other"><string-name><surname>Scott</surname>, <given-names>J. G.</given-names></string-name>, <string-name><surname>Kelly</surname>, <given-names>R. C.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>P.</given-names></string-name>, and <string-name><surname>Kass</surname>, <given-names>R. E.</given-names></string-name> (<year>2014</year>). <article-title>False discovery rate regression: an application to neural synchrony detection in primary visual cortex</article-title>. <source>Journal of the American Statistical Association</source>, (just-accepted):00&#x2013;00.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Shendure</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Ji</surname>, <given-names>H.</given-names></string-name> (<year>2008</year>). <article-title>Next-generation DNA sequencing</article-title>. <source>Nature Biotechnology</source>, <volume>26</volume>(<issue>10</issue>):<fpage>1135</fpage>&#x2013;<lpage>1145</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Storey</surname>, <given-names>J. D.</given-names></string-name> (<year>2002</year>). <article-title>A direct approach to false discovery rates</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>, <volume>64</volume>(<issue>3</issue>):<fpage>479</fpage>&#x2013;<lpage>498</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>