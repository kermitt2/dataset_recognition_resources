<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/432393</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Systems Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Inverse sensitivity analysis of mathematical models avoiding the curse of dimensionality</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4274-4158</contrib-id>
<name>
<surname>Lambert</surname>
<given-names>Ben</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Gavaghan</surname>
<given-names>David J.</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9835-8606</contrib-id>
<name>
<surname>Tavener</surname>
<given-names>Simon</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Zoology, University of Oxford</institution>, Oxford, Oxfordshire, <country>U.K</country>.</aff>
<aff id="a2"><label>2</label><institution>MRC Centre for Outbreak Analysis and Modelling, Infectious Disease Epidemiology, Imperial College London</institution>, London W2 1PG, <country>UK</country>.</aff>
<aff id="a3"><label>3</label><institution>Department of Computer Science, University of Oxford</institution>, Oxford, <country>U.K</country>.</aff>
<aff id="a4"><label>4</label><institution>Department of Statistics, Colorado State University</institution>, Fort Collins, Colorado, <country>U.S.A</country>.</aff>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>&#x002A;</label><p><email>ben.c.lambert@gmail.com</email>.</p></fn>
</author-notes>
<pub-date pub-type="epub"><year>2018</year></pub-date>
<elocation-id>432393</elocation-id>
<history>
<date date-type="received">
<day>01</day>
<month>10</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>01</day>
<month>10</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>01</day>
<month>10</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="432393.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<label>1</label><title>Abstract</title>
<p>Biological systems have evolved a degree of robustness with respect to perturbations in their environment and this capability is essential for their survival. In applications ranging from therapeutics to conservation, it is important to understand not only the sensitivity of biological systems to changes in their environments, but which features of these systems are necessary to achieve a given outcome. Mathematical models are increasingly employed to understand these mechanisms. Sensitivity analyses of such mathematical models provide insight into the responsiveness of the system when experimental manipulation is difficult. One common approach is to seek the probability distribution of the outputs of the system corresponding to a known distribution of inputs. By contrast, inverse sensitivity analysis determines the probability distribution of model inputs which produces a known distribution of outputs. The computational complexity of the methods used to conduct inverse sensitivity analyses for deterministic systems has limited their application to models with relatively few parameters. Here we describe a novel Markov Chain Monte Carlo method we call &#x201C;Contour Monte Carlo&#x201D;, which can be used to invert systems with a large number of parameters. We demonstrate the utility of this method by inverting a range of frequently-used deterministic models of biological systems, including the logistic growth equation, the Michaelis-Menten equation, and an SIR model of disease transmission with nine input parameters. We argue that the simplicity of our approach means it is amenable to a large class of problems of practical significance and, more generally, provides a probabilistic framework for understanding the inversion of deterministic models.</p>
<sec>
<label>2</label><title>Author summary</title>
<p>Mathematical models of complex systems are constructed to provide insight into their underlying functioning. Statistical inversion can probe the often unobserved processes underlying biological systems, by proceeding from a given distribution of a model&#x2019;s outputs (the aggregate &#x201C;effects&#x201D;) to a distribution over input parameters (the constituent &#x201C;causes&#x201D;). The process of inversion is well-defined for systems involving randomness and can be described by Bayesian inference. The inversion of a deterministic system, however, cannot be performed by the standard Bayesian approach. We develop a conceptual framework that describes the inversion of deterministic systems with fewer outputs than input parameters. Like Bayesian inference, our approach uses probability distributions to describe the uncertainty over inputs and outputs, and requires a prior input distribution to ensure a unique &#x201C;posterior&#x201D; probability distribution over inputs. We describe a computational Monte Carlo method that allows efficient sampling from the posterior distribution even as the dimension of the input parameter space grows. This is a two-step process where we first estimate a &#x201C;contour volume density&#x201D; associated with each output value which is then used to define a sampling algorithm that yields the requisite input distribution asymptotically. Our approach is simple, broadly applicable and could be widely adopted.</p>
</sec>
</abstract>
<counts>
<page-count count="36"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1"><label>3</label><title>Introduction</title>
<p>Mathematical models have played an important role in understanding a wide range of biological phenomena, including the structure and function of the heart [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>], the growth of tumours [<xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c4">4</xref>], epidemics of infectious diseases [<xref ref-type="bibr" rid="c5">5</xref>], and population dynamics in conservation areas [<xref ref-type="bibr" rid="c6">6</xref>,<xref ref-type="bibr" rid="c7">7</xref>]. In all of these models, the variables of interest (the &#x201C;outputs&#x201D;) are mediated by input parameters, which, when varied, can produce a multitude of system behaviours. Whilst these parameters take algebraic form, they should (ideally) be relatable to biological processes. Determining the sets of inputs which maintain normal functioning or, alternatively, cause the system to produce unfavourable outcomes are crucial for a range of applications. Since biological systems are typically highly nonlinear [<xref ref-type="bibr" rid="c8">8</xref>], the sensitivity of outputs to changes in the input processes can be difficult to determine without recourse to model analysis or simulation. A forward sensitivity analysis can determine the probability distribution of the outputs of a model based on the probability distribution of model inputs. A so-called inverse sensitivity analysis determines the probability distribution of model inputs which produces a known distribution of outputs.</p>
<p>While forward and inverse sensitivity analyses are important concepts in all areas of science and technology, these issues are particularly acute in mathematical and computational biology. Consider first a simple physical system, the laminar flow of a Newtonian fluid which is governed by the Navier-Stokes equations, a classical and universally accepted system of partial differential equations based on fundamental physical principles and simplifying assumptions. The Navier-Stokes equations include a small number of parameters (e.g., viscosity, surface tension) that can be independently measured with high precision and laboratory-scale experiments can be conducted in well-controlled environments with a high degree of reproducibility. Modelling in the biological sciences by contrast, is currently in a rapid state of development, many models are highly parametrized, and their parameters cannot be independently measured but must be inferred from the output of the model. Even in situations when the mathematical model is well established, such as the Hodgkin-Huxley equations, some parameters may be well characterized from experimental observations, while others are not. A recent study of the Hodgkin-Huxley equations [<xref ref-type="bibr" rid="c9">9</xref>] indicates that sufficient membrane potential data can provide a reasonable fit to the three major conductance parameters. However, no matter how complete and accurate the membrane potential data, the time constants associated with the rate parameters in the model are practically unidentifiable. Whereas parameter estimation seeks a single value for model parameters that could produce a set of output values, the corresponding inverse sensitivity problem seeks the <italic>distribution</italic> of model parameters that is compatible with an observed output distribution. For example, in the Hodgkin-Huxley system, an inverse sensitivity analysis can determine how variable the unobserved rate parameters can be whilst having no real effect on the peak-to-trough voltage.</p>
<p>For a large class of models of practical interest in biology, including ordinary differential equations (ODEs) and partial differential equations (PDEs), analytic methods for inverse sensitivity analysis are unavailable. Further it is common for the dimension of the inputs to exceed that of the outputs. For deterministic systems, this means that there exist non-singular sets of inputs that can yield a given output and that any feasible output distribution may be caused by any member of a collection of possible input distributions. In Bayesian inference of stochastic systems, prior distributions are specified for parameters of the likelihood ensuring a unique input distribution for a given set of output data. For deterministic models, however, there is no uncertainty in the input-to-output map, meaning standard Bayesian techniques cannot be used without introducing an artificial error distribution. Here we describe a conceptual framework to understand the inversion of deterministic systems. We then describe a novel Markov Chain Monte Carlo method we call &#x201C;Contour Monte Carlo&#x201D; and use this approach to invert a number of systems, including the logistic growth equation, the Michaelis-Menten equation, and an SIR model of disease transmission. In contrast to methods that use a grid to discretize the parameter domain to determine a numerical output-to-input map [<xref ref-type="bibr" rid="c10">10</xref>], our method uses stochastic sampling to instead approximate the multiplicity of a given output value; that is, the number of inputs that map to that value. By using random sampling, the computational complexity of our approach does not grow with parameter dimensions, meaning that it can be applied to large model systems. We also argue that the simplicity of our approach means it is amenable to a large class of problems of practical significance.</p>
<p><underline>Outline of the paper</underline>: In <xref ref-type="sec" rid="s2a">&#x00A7;4.1</xref>, we present the basic CMC algorithm assuming uniform priors in <xref ref-type="sec" rid="s2a1">&#x00A7;4.1.1</xref>, and extend this approach to general prior distributions in <xref ref-type="sec" rid="s2b">&#x00A7;4.2</xref>. In <xref ref-type="sec" rid="s2c">&#x00A7;4.3</xref>, we examine several algebraic examples to illustrate how the CMC algorithm works in detail. CMC is then used to invert a range of examples drawn from mathematical biology in <xref ref-type="sec" rid="s3a">&#x00A7;5.1</xref> before a discussion in <xref ref-type="sec" rid="s4">&#x00A7;6</xref>.</p></sec>
<sec id="s2"><label>4</label><title>Methods</title>
<sec id="s2a"><label>4.1</label><title>Estimating the posterior distribution</title>
<sec id="s2a1"><label>4.1.1</label><title>Uniform prior distributions</title>
<p>Consider a deterministic function from a vector of inputs <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline1.gif"/></alternatives></inline-formula> to output quantities <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline2.gif"/></alternatives></inline-formula>, where <italic>P</italic> &#x003C; <italic>K</italic>. Since the dimensionality of the input parameters exceeds that of the output quantities, each feasible output value permits a set of possible input causes. This means that without further information it is not possible to determine a single cause for an output value, unless that output value is &#x201C;singular&#x201D;. That which is true for individual output values is also true for output distributions. For any given output distribution, there are a collection of possible input distributions that could yield it.</p>
<p>The distribution that we want to estimate is a conditional density of the form <italic>p</italic>(<bold>&#x039B;</bold>|<bold><italic>Q</italic></bold>(.), <italic>data</italic>), where <bold><italic>Q</italic></bold>(.) denotes the particular form of input-to-output function used, and <italic>data</italic> denotes a collection of output data points. In analogy to Bayesian inference, we term this conditional density over inputs the &#x201C;posterior distribution&#x201D;. In what follows we implicitly assume a dependence on <bold><italic>Q</italic></bold>(.), and omit it for brevity. To sample from this distribution efficiently, we first consider the joint distribution,
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="432393_eqn1.gif"/>
</alternatives>
</disp-formula>where the <italic>prior</italic> term does not contain any dependence on the data since it solely depends on the geometry of the input-output map, which is determined by <bold><italic>Q</italic></bold>(.). We assume that the output data distribution has been fitted with an appropriate distribution, to yield a density value for any value of output, <italic>p</italic>(<bold><italic>Q</italic></bold>|<italic>data</italic>), which we term the &#x201C;target distribution&#x201D;. An alternative way to decompose the joint distribution is,
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="432393_eqn2.gif"/>
</alternatives>
</disp-formula></p>
<p>For a deterministic model <italic>p</italic>(<bold><italic>Q</italic></bold>|<bold>&#x039B;</bold>, <italic>data</italic>) &#x003D; <italic>p</italic>(<bold><italic>Q</italic></bold>|<bold>&#x039B;</bold>), since <bold><italic>Q</italic></bold> is fully determined by the inputs <bold>&#x039B;</bold>. Because of the determinacy this term becomes,
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="432393_eqn3.gif"/>
</alternatives>
</disp-formula></p>
<p>Equating the right hand side of <xref ref-type="disp-formula" rid="eqn1">equations (1)</xref> and <xref ref-type="disp-formula" rid="eqn2">(2)</xref>, we obtain an expression for the posterior input distribution,
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="432393_eqn4.gif"/>
</alternatives>
</disp-formula></p>
<p>To sample from the posterior distribution specified by <xref ref-type="disp-formula" rid="eqn4">equation (4)</xref>, we use our estimates of the prior that are obtained by independent sampling of the inputs uniformly within their bounds (we consider unbounded non-uniform priors in <xref ref-type="sec" rid="s2b">&#x00A7;4.2</xref>) to produce an estimator of the distribution,
<disp-formula id="eqn5">
<alternatives><graphic xlink:href="432393_eqn5.gif"/>
</alternatives>
</disp-formula>where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline3.gif"/></alternatives></inline-formula> is the (potentially un-normalised) contour volume at an output value of <bold><italic>Q</italic></bold> - a measure of the size of the input set which maps to <bold><italic>Q</italic></bold>. Due to the continuous nature of the output, the distribution <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline4.gif"/></alternatives></inline-formula> actually defines a contour volume density, which can be used to estimate the posterior input distribution,
<disp-formula id="eqn6">
<alternatives><graphic xlink:href="432393_eqn6.gif"/>
</alternatives>
</disp-formula></p>
<p>Since the above probability distribution is non-standard and potentially un-normalised, we use MCMC to sample from it, and use Random Walk Metropolis sampling [<xref ref-type="bibr" rid="c11">11</xref>] for this process with an acceptance probability of,
<disp-formula id="eqn7">
<alternatives><graphic xlink:href="432393_eqn7.gif"/>
</alternatives>
</disp-formula></p>
<p>We recognise that since an approximate form of the un-normalised distribution, as well as its gradients, are known, more efficient forms of MCMC algorithms such as adaptive Metropolis [<xref ref-type="bibr" rid="c12">12</xref>], or Hamiltonian Monte Carlo (HMC) [<xref ref-type="bibr" rid="c13">13</xref>] could also be used.</p>
<p>To summarise, our &#x201C;Contour Monte Carlo&#x201D; (CMC) algorithm is composed of two distinct phases (see Algorithm 1 for more detail): first, sample the input parameters to yield estimates of the contour volume density, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline5.gif"/></alternatives></inline-formula>; second, use <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline6.gif"/></alternatives></inline-formula> to specify a prior that distributes probability mass uniformly within a contour volume which, along with a target output distribution, determines a posterior input distribution. This distribution is then sampled from using an MCMC method.</p>
<statement><label>Algorithm 1</label><p>Pseudocode for the Contour Monte Carlo algorithm for parameters with uniform probability distributions on bounded domains <bold>(</bold>&#x039B;). (Parameters that may be vectors are shown in bold.)</p>
<p><fig id="ufig1" position="float" orientation="portrait" fig-type="figure">
<graphic xlink:href="432393_ufig1.tif"/>
</fig></p>
</statement>
<p>Since we are reconstructing the posterior input distribution by MCMC sampling, it is crucial to determine when our sampling distribution has converged. To do so, we use the same convergence diagnostic criteria that are used in applied Bayesian inference. We run a number of Markov Chains in parallel and compare their within-chain variance to that between the chains by calculating <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline10.gif"/></alternatives></inline-formula> [<xref ref-type="bibr" rid="c14">14</xref>], and use a convergence threshold of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline11.gif"/></alternatives></inline-formula> on all input parameters. Once convergence is achieved, we discard an initial portion of the chains as &#x201C;warm-up&#x201D; [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c15">15</xref>] and use the rest to form our posterior distribution.</p></sec></sec>
<sec id="s2b"><label>4.2</label><title>General prior distributions</title>
<p>The type of uniform prior we have described thus far appears sensible since it allocates probability mass in exact accordance with the geometry of the input-output map. In the absence of further information it seems reasonable to suppose that all inputs which produce a given output value are equally likely causes of it, and Algorithm 1 describes how to sample from the resultant posterior input distribution. It is unclear, however, how to use our existing CMC algorithm to handle unbounded prior spaces, where uniform prior distributions cannot be specified. We now extend the framework described in <xref ref-type="sec" rid="s2a1">&#x00A7;4.1.1</xref> to handle arbitrary prior distributions. We start with the expression for the posterior distribution,
<disp-formula>
<alternatives><graphic xlink:href="432393_ueqn3.gif"/></alternatives>
</disp-formula>where we have used Bayes&#x2019; rule to rewrite the conditional prior term. We then recognise that for a deterministic model <italic>p</italic>(<bold><italic>Q</italic></bold>(<bold>&#x039B;</bold>)|<bold>&#x039B;</bold>) &#x003D; 1, resulting in the following general expression for the posterior distribution,
<disp-formula id="eqn8">
<alternatives><graphic xlink:href="432393_eqn8.gif"/>
</alternatives>
</disp-formula></p>
<p>If the parameter bounds are non-infinite and consist of a total volume <italic>V</italic> in input space, and we suppose that all values of <bold>&#x039B;</bold> are equally likely <italic>a priori</italic>, then the above becomes,
<disp-formula id="eqn9">
<alternatives><graphic xlink:href="432393_eqn9.gif"/>
</alternatives>
</disp-formula></p>
<p>However if all parameter values are equally probable then <italic>p</italic>(<bold><italic>Q</italic></bold>(<bold>&#x039B;</bold>)) is simply the proportion of input space defined by the set &#x007B;&#x039B;&#x2032;: <italic>Q</italic>(&#x039B;&#x2032;) &#x003D; <italic>Q</italic>(&#x039B;)&#x007D;. Multiplying this by the overall volume of input space yields the volume of input space occupied by this set, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline12.gif"/></alternatives></inline-formula>, and we recapitulate the expression for the posterior that we approximated in <xref ref-type="disp-formula" rid="eqn6">equation (6)</xref>. We thus see that the assumption of a uniform prior within a contour volume is equivalent to the assumption of an uniform prior throughout the input domain.</p>
<p>More generally, we can imagine setting informative priors on parameters that may or may not be finitely bounded. For parameters with infinite bounds a uniform prior is improper, and we therefore resort to using valid probability distributions that have support over an infinity of values. In general, the priors that we set will not be aligned with the shape of input contours, meaning that there can be a variation in the probability density within a given contour volume. How should we handle parameter inference in this new framework? <xref ref-type="disp-formula" rid="eqn8">Equation (8)</xref> is the expression for the posterior in this, more general, setting. Algorithm 2 describes how to account for general prior distributions in CMC, which is then used in illustrative example 3(b) to estimate the posterior input distribution.</p>
<statement><label>Algorithm 2</label><p>Pseudocode for the Contour Monte Carlo algorithm for parameters with general probability distributions on potentially unbounded domains <bold>(</bold>&#x039B;). (Parameters that may be vectors are shown in bold.)</p>
<p><fig id="ufig2" position="float" orientation="portrait" fig-type="figure">
<graphic xlink:href="432393_ufig2.tif"/>
</fig></p>
</statement>
</sec>
<sec id="s2c"><label>4.3</label><title>Illustrative examples</title>
<sec id="s2c1"><label>4.3.1</label><title>One-to-one input-to-output maps</title>
<p>Whilst our approach is intended for systems where the number of outcomes exceeds the inputs, simpler systems with one input and one output (one-to-one) can be used to build an intuitive understanding of the method. Here the usual Jacobian of a coordinate transformation plays the role of the prior term, <italic>p</italic>(<bold>&#x039B;</bold>|<bold><italic>Q</italic></bold>(<bold>&#x039B;</bold>)).</p>
<p><bold>Illustrative example &#x0023;1</bold></p>
<p>Consider an input (<italic>&#x03BB;</italic>) to output (<italic>Q</italic>) map of the form,
<disp-formula id="eqn10">
<alternatives><graphic xlink:href="432393_eqn10.gif"/>
</alternatives>
</disp-formula>where <italic>&#x03BB;</italic> &#x2208; [0, <italic>1</italic>]. Suppose that we seek a distribution over inputs <italic>p</italic>(<italic>&#x03BB;</italic>) such that the corresponding distribution over outputs follows <italic>Q</italic>(<italic>&#x03BB;</italic>) &#x007E; <italic>beta</italic>(2, 2) (see the black lines in the right hand column of <xref ref-type="fig" rid="fig1">Figure 1</xref>). A straightforward way to generate samples from <italic>p</italic>(<italic>&#x03BB;</italic>) is to first sample <italic>Q<sub>i</sub></italic> &#x007E; <italic>beta</italic>(2, 2) then use the inverse map to yield samples of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline16.gif"/></alternatives></inline-formula>. For most interesting models in mathematical biology however, including ODEs and PDEs, an inverse map cannot be calculated or does not exist. As such, we aim to generate samples from <italic>p</italic>(<italic>&#x03BB;</italic>) using only the forward map.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Illustrative example &#x0023;1: Estimated posterior input distributions (left column) and corresponding output distributions (right column) for naive Metropolis (top row), modified Metropolis using the analytic Jacobian (middle row) and sampling-based Modified Metropolis (bottom row).</title><p>Target densities are indicated by black lines. For the analytic case, the Jacobian used corresponds to <xref ref-type="disp-formula" rid="eqn12">equation (12)</xref>. In each case, 316,000 Metropolis samples across 8 Markov chains (40,000 iterations on each chain, with the first 500 samples discarded as warm-up) were used to produce the histograms. For the sampling-based Modified Metropolis, the Jacobian transformations were estimated as indicated in the caption to <xref ref-type="fig" rid="fig2">Figure 2</xref>, and are shown as the blue-dashed line in <xref ref-type="fig" rid="fig2">Figure 2B</xref>.</p></caption>
<graphic xlink:href="432393_fig1.tif"/>
</fig>
<p>It may appear that a valid approach to generate input samples from <italic>p</italic>(<italic>&#x03BB;</italic>) is to use a Markov chain Monte Carlo (MCMC) algorithm like random walk Metropolis [<xref ref-type="bibr" rid="c16">16</xref>], where proposed <italic>&#x03BB;</italic>&#x2032; are accepted as samples if,
<disp-formula id="eqn11">
<alternatives><graphic xlink:href="432393_eqn11.gif"/>
</alternatives>
</disp-formula>where <italic>p</italic>(<italic>Q</italic>(<italic>&#x03BB;</italic>)) is the target density (here a <italic>beta</italic>(2, 2) distribution) evaluated at <italic>Q</italic> &#x003D; <italic>&#x03BB;</italic><sup>2</sup>. This approach, however, results in an input distribution (top-left panel in <xref ref-type="fig" rid="fig1">Figure 1</xref>) which, when transformed, does not recapture the target density (top-right panel in <xref ref-type="fig" rid="fig1">Figure 1</xref>). The reason for this bias is that it neglects to include a Jacobian term in the density which accounts for the nonlinear change of measure in going from input to output space. If this Jacobian term is included in the Metropolis accept-reject rule,
<disp-formula id="eqn12">
<alternatives><graphic xlink:href="432393_eqn12.gif"/>
</alternatives>
</disp-formula>where, in this case, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline17.gif"/></alternatives></inline-formula>, then this results in an input distribution (middle-left panel in <xref ref-type="fig" rid="fig1">Figure 1</xref>) which, when transformed, results in an output distribution that corresponds to the target (middle-right panel in <xref ref-type="fig" rid="fig1">Figure 1</xref>).</p>
<p>Indeed, in this simple example, it is possible to exactly determine the input distribution that results in a <italic>beta</italic>(2, 2) target. This is given by a Jacobian transformation multiplied by the target density,
<disp-formula id="eqn13">
<alternatives><graphic xlink:href="432393_eqn13.gif"/>
</alternatives>
</disp-formula>which is shown by the black lines in the left hand column of <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p>
<sec id="s2c1a"><title>Estimating the Jacobian transformations by sampling</title>
<p>Whilst the Jacobian may be calculated analytically and the marginalization performed for simple input to output maps, for maps of modest difficulty, this term cannot typically be analytically derived. We now illustrate how approximate Jacobian transformations can be obtained by sampling. To do so, we first sample uniformly from the bounds of <italic>&#x03BB;</italic> (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) which, when transformed by <xref ref-type="disp-formula" rid="eqn10">equation (10)</xref>, yields a sampling distribution for <italic>Q</italic> (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). By fitting a Gaussian kernel density estimator to this sampling distribution (dashed blue lines in <xref ref-type="fig" rid="fig2">Figure 2B</xref>), we can well approximate the analytic inverse Jacobian density (grey line) with a modest number of samples. This, hence, allows us to use the Jacobian-corrected Metropolis sampler defined by the acceptance ratio of <xref ref-type="disp-formula" rid="eqn12">equation (12)</xref> to generate samples from the input distribution <italic>p</italic>(<italic>&#x03BB;</italic>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Illustrative example &#x0023;1: (A) Uniform sampling of input over [0, 1]. (B) Corresponding output under map defined by <xref ref-type="disp-formula" rid="eqn10">equation (10)</xref>.</title><p>Histograms were constructed using 100,000 random samples. The grey curves indicate the exact sampling distributions. The blue-dotted line in panel B shows the Gaussian kernel density estimate of the sampling distribution using a bandwidth of 0.01.</p></caption>
<graphic xlink:href="432393_fig2.tif"/>
</fig>
<p>Once the inverse Jacobian transformation has been estimated, we simply replace the actual Jacobian in our Metropolis-based rule given by <xref ref-type="disp-formula" rid="eqn12">equation (12)</xref>, with the sample-based estimates <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline18.gif"/></alternatives></inline-formula>,
<disp-formula id="eqn14">
<alternatives><graphic xlink:href="432393_eqn14.gif"/>
</alternatives>
</disp-formula></p>
<p>Our approach, therefore, consists of two distinct steps: first, independent sampling from the prior space (which is currently assumed to be bounded but, in <xref ref-type="sec" rid="s2b">&#x00A7;4.2</xref>, this is relaxed) and transformation of these inputs to form a sample of outputs, followed by the fitting of an empirical density estimator to the output samples; second, Jacobian-modified Metropolis sampling using the Jacobian transformations estimated from the first step. When the sample size of the first step approaches infinity, the Markov chain in the second step converges asymptotically to the target density. For a finite sample size for the first step, the sampling distribution of the Markov chain in the second step does not exactly converge to the target distribution due to Jensen&#x2019;s inequality. For even modest sample sizes, however, we have found this bias negligible compared to other sources of uncertainty.</p>
<p>In the bottom row of <xref ref-type="fig" rid="fig1">Figure 1</xref>, we illustrate how using our two step method allows us to generate an input distribution which maps to the target.</p></sec></sec>
<sec id="s2c2"><label>4.3.2</label><title>Many to one input-to-output maps</title>
<p>For many-to-one maps, no inverse map exists, meaning that a given output value (unless a singular point) corresponds to a number of input vectors. This means that a given output distribution can be produced from a variety of input distributions. Like in Bayesian inference, to reduce the set of allowable input distributions to one, we specify a prior distribution over input parameters. In contrast to Bayesian inference, this prior is specified as a conditional probability distribution, where we condition on particular output values.</p>
<p>A natural way to specify such a prior is to assume that the probability distribution is uniform within the set of possible inputs that generate a given output value. The prior distribution is then given by the probability density,
<disp-formula id="eqn15">
<alternatives><graphic xlink:href="432393_eqn15.gif"/>
</alternatives>
</disp-formula>where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline19.gif"/></alternatives></inline-formula> is the volume of input space where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline20.gif"/></alternatives></inline-formula>. We term the size of the region of input space that produces a particular output value, its &#x201C;contour volume&#x201D;. Note that we use the term &#x201C;volume&#x201D; liberally here. For a two-dimension input space and one dimensional output space as shown in <xref ref-type="fig" rid="fig3">Figure 3</xref> this corresponds to a length, although for higher dimensional examples the set can be an area, a volume or a hypervolume.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Illustrative example &#x0023;2: Plot of function given by <xref ref-type="disp-formula" rid="eqn16">equation (16)</xref> showing contours of constant <italic>Q</italic> values.</title><p>Here <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline21.gif"/></alternatives></inline-formula>.</p></caption>
<graphic xlink:href="432393_fig3.tif"/>
</fig>
<p><bold>Illustrative example &#x0023;2</bold></p>
<p>To explain how this process works we consider the case of two dimensional inputs (<italic>&#x03BB;</italic><sub>1</sub><italic>,&#x03BB;</italic><sub>2</sub>) bounded by the region <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline22.gif"/></alternatives></inline-formula>, and a uni-dimensional output,
<disp-formula id="eqn16">
<alternatives><graphic xlink:href="432393_eqn16.gif"/>
</alternatives>
</disp-formula></p>
<p>A plot of this function is shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>, where it is observed that each value of the function <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline23.gif"/></alternatives></inline-formula> maps to any point along circular &#x201C;contours&#x201D; (dashed lines) in input space (<italic>&#x03BB;</italic><sub>1</sub><italic>,&#x03BB;</italic><sub>2</sub>), with the exception of singular point <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline24.gif"/></alternatives></inline-formula>.</p>
<p>We suppose that we want to produce an input distribution which results in a target output distribution which is uniform across allowed function values <italic>Q</italic> &#x2208; [0.2, 1], since the function is bounded by its extreme values at <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline25.gif"/></alternatives></inline-formula> (where <italic>Q</italic> &#x003D; 0.2), and (<italic>&#x03BB;</italic><sub>1</sub><italic>,&#x03BB;</italic><sub>2</sub>) &#x003D; (0, 0) (where <italic>Q</italic> &#x003D; 1).</p>
<p>We seek an analytic expression for the input distribution which is consistent with this target distribution given a uniform prior distribution over inputs. The prior distribution which is uniform over inputs can be transformed from Cartesian <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline26.gif"/></alternatives></inline-formula> to polar coordinate form,
<disp-formula id="eqn17">
<alternatives><graphic xlink:href="432393_eqn17.gif"/>
</alternatives>
</disp-formula>where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline27.gif"/></alternatives></inline-formula> and <italic>&#x03B8;</italic> &#x003D; arctan(<italic>&#x03BB;</italic><sub>2</sub>/<italic>&#x03BB;</italic><sub>1</sub>). When marginalising over <italic>&#x03B8;</italic> the above expression yields <italic>p</italic>(<italic>r</italic>) &#x003D; <italic>r</italic>/2. Since we can rewrite <italic>Q</italic> &#x003D; 1/(1 &#x002B; <italic>r</italic><sup>2</sup>), a one-dimensional Jacobian transformation yields the contour volume distribution pertaining to <italic>p</italic>(<italic>r</italic>),
<disp-formula id="eqn18">
<alternatives><graphic xlink:href="432393_eqn18.gif"/>
</alternatives>
</disp-formula></p>
<p><italic>Note:</italic> The &#x201C;Jacobian&#x201D; associated a nonlinear change in measure for a mapping from <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline28.gif"/></alternatives></inline-formula> to <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline29.gif"/></alternatives></inline-formula> where <italic>m</italic> &#x003C; <italic>n</italic>, is constructed in Theorem 2.1.1 of [<xref ref-type="bibr" rid="c17">17</xref>].</p>
<p>For this example, we can analytically calculate the prior term in <xref ref-type="disp-formula" rid="eqn4">equation (4)</xref> using <xref ref-type="disp-formula" rid="eqn15">equation (15)</xref> and the normalizing constant equal to the area of the domain &#x03A9; ( &#x003D; 4<italic>&#x03C0;</italic>),
<disp-formula>
<alternatives><graphic xlink:href="432393_ueqn6.gif"/></alternatives>
</disp-formula></p>
<p>Then using <xref ref-type="disp-formula" rid="eqn4">Eq (4)</xref>, and targeting a uniform output distribution across the range of <italic>Q</italic>, <italic>p</italic>(<italic>Q</italic>|<italic>data</italic>) &#x003D; 1/(4/5) &#x003D; 5/4, we calculate an analytic form of the posterior input distribution,
<disp-formula id="eqn19">
<alternatives><graphic xlink:href="432393_eqn19.gif"/>
</alternatives>
</disp-formula></p>
<p>This can be alternatively expressed in polar coordinates as,
<disp-formula id="eqn20">
<alternatives><graphic xlink:href="432393_eqn20.gif"/>
</alternatives>
</disp-formula></p>
<p>If we marginalise <italic>&#x03B8;</italic> from <xref ref-type="disp-formula" rid="eqn20">equation (20)</xref>, we obtain the marginal posterior distribution for <italic>r</italic>,
<disp-formula id="eqn21">
<alternatives><graphic xlink:href="432393_eqn21.gif"/>
</alternatives>
</disp-formula></p>
<p>In most applied cases, analytic calculation of the contour volume distribution, as is given by <xref ref-type="disp-formula" rid="eqn19">equation (19)</xref> in this example, is not possible, and we must instead approximate it by sampling. We now illustrate this process for the current example. To start, we idealise the problem and assume the function value <italic>Q</italic> is constant within annuli of a given width and distance from the origin (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Since our function and input domain are well-behaved it is possible to analytically calculate the area of each annulus (<xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Illustrative example &#x0023;2 (A) Mean values of the function given by <xref ref-type="disp-formula" rid="eqn16">equation (16)</xref>. (B) Areas of each annulus shown in subplot A. (C) Uniform sampling of the input space. (D) Contour volumes estimated by the uniform sampling indicated in subplot C.</title><p>Subplot C was produced using 50,000 independent samples of the input. The dashed lines indicate contours of the function. The black line in subplot D shows a kernel density model fit estimated using the un-binned output data with Gaussian kernel and a bandwidth of 0.01 (using Mathematica&#x2019;s &#x201C;SmoothKernelDistribution&#x201D; function [<xref ref-type="bibr" rid="c18">18</xref>]). The orange line indicates the true contour volume distribution. Here <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline30.gif"/></alternatives></inline-formula>.</p></caption>
<graphic xlink:href="432393_fig4.tif"/>
</fig>
<p>In the above explanation, we assumed that the function value was constant within a given annulus, however we recognise that the continuous nature of the function means that the contours are actually one dimensional lines (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Given the simple nature of this output function we can actually analytically determine the length (which we call a contour volume) of each of these lines. In our example, the output space is a foliation of these one-dimensional lines, whose non-uniform spacing determines a density of contour volumes (roughly, the area of input space that maps to an infinitesimal range of output). In Algorithm 1, we estimate the contour volume distribution by sampling uniformly from the input space (<xref ref-type="fig" rid="fig4">Figure 4C</xref>), and for each set of inputs <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline31.gif"/></alternatives></inline-formula>, we calculate a corresponding <italic>Q<sub>i</sub></italic>. To approximate <italic>p</italic>(<italic>Q</italic>) in <xref ref-type="disp-formula" rid="eqn18">equation (18)</xref> we then fit a kernel density model to (<italic>Q</italic><sub>1</sub><italic>,Q</italic><sub>2</sub>, &#x2026;, <italic>Q<sub>N</sub></italic>), where <italic>N</italic> is the number of input samples (the black line in <xref ref-type="fig" rid="fig4">Figure 4D</xref>). After relatively few input samples, the estimated contour volume distribution well approximates the true distribution of <xref ref-type="disp-formula" rid="eqn18">equation (18)</xref> (the orange line in <xref ref-type="fig" rid="fig4">Figure 4D</xref>).</p>
<p>Note that the contour volume distribution <italic>p</italic>(<italic>Q</italic>) is equivalent to the two-to-one inverse Jacobian transformation for this problem. Specifically, <xref ref-type="fig" rid="fig4">Figure 4D</xref> is analogous to <xref ref-type="fig" rid="fig2">Figure 2B</xref>, except for a two-dimensional input example. In all cases where the input spaces are bounded and the input priors are uniform, what we call the &#x201C;contour volume distribution&#x201D; is equivalent to the density defined by the inverse Jacobian transformation.</p>
<p>We compare the estimated marginal posterior distribution for <italic>r</italic> that results from the CMC algorithm with the analytic result of <xref ref-type="disp-formula" rid="eqn21">equation (21)</xref>. In the first phase of CMC, we estimate the contour volume for any output value using a kernel density estimator fit to the function evaluations from uniformly sampled inputs (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), and use this in a second phase targeting a uniform output space (within bounds). In this case, our estimated posterior density is of the simple form,
<disp-formula id="eqn22">
<alternatives><graphic xlink:href="432393_eqn22.gif"/>
</alternatives>
</disp-formula>in other words we step in inverse proportion to the estimated contour volumes. This results in a marginal posterior density for inputs which is concentrated towards the centre of the input domain (<xref ref-type="fig" rid="fig5">Figure 5A</xref>), which results in a uni-dimensional distribution for <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline32.gif"/></alternatives></inline-formula> which is in agreement with the analytic result of <xref ref-type="disp-formula" rid="eqn21">equation (21)</xref> (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). A benefit of our algorithm is that we can validate it by comparing the marginal output posterior with the target. Since in this case our output distribution looks approximately uniform (<xref ref-type="fig" rid="fig5">Figure 5C</xref>), it appears that the algorithm is working correctly.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Illustrative example &#x0023;2: (A) Joint posterior distributions over inputs (<italic>&#x03BB;</italic><sub>1</sub>,<italic>&#x03BB;</italic><sub>2</sub>). (B) Posterior distribution in terms of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline33.gif"/></alternatives></inline-formula> versus the analytic result (solid line). (C) Posterior distribution over outputs.</title><p>The analytic result is given by <xref ref-type="disp-formula" rid="eqn21">equation (21)</xref>. The kernel density estimator for the volume of contours is shown in <xref ref-type="fig" rid="fig4">Figure 4D</xref> and resulted from 50,000 independent samples from a uniform distribution over input space. The MCMC was performed using the Metropolis algorithm with toroidal boundaries and consisted of 100,000 samples on each of 12 chains. Here <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline34.gif"/></alternatives></inline-formula>.</p></caption>
<graphic xlink:href="432393_fig5.tif"/>
</fig>
<p>In Illustrative example 2, a sampler using a <italic>Q</italic>(&#x039B;)|<italic>data</italic> &#x007E; <italic>u</italic>(0.2, 1) distribution over output values will accept all input samples, and hence samples uniformly in input space (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). The result is a sampling distribution of inputs with more points towards the boundary of the input domain. Those boundary values of the domain have a lower output value and hence the output distribution is skewed towards those function values (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). To correct for the over-representation of points with low function values, we must correct for their longer lengths, which we do by estimating <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline35.gif"/></alternatives></inline-formula> and using the inverse of this as a prior.</p>
<p>A more complex test problem comprised of a pair of nonlinear maps (two outputs) with three parameters that was introduced in [<xref ref-type="bibr" rid="c10">10</xref>] is analysed in S3&#x2013;A nonlinear map.</p></sec>
<sec id="s2c3"><label>4.3.3</label><title>Informative prior distributions</title>
<p>We consider the output generated by a non-uniform distribution of parameters and a nonlinear map which we seek to recover assuming uniform and nonuniform prior distributions.</p>
<p><bold>Illustrative example &#x0023;3</bold></p>
<p>Consider a bivariate normal as an input distribution with mean and covariance matrix (<xref ref-type="fig" rid="fig6">Figure 6A</xref>),
<disp-formula id="eqn23">
<alternatives><graphic xlink:href="432393_eqn23.gif"/>
</alternatives>
</disp-formula></p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Illustrative example &#x0023;3(a): (A) The cross indicates the mean and standard deviations of a bivariate normal distribution. (B) Output distribution generated by the input distribution indicated in subplot A. The blue curve indicates the fit with a normal distribution. (C) The resultant MCMC samples from the posterior. (D) The output distribution corresponding to samples in subplot C (orange histogram) and target distribution (blue curve).</title><p>The kernel density estimator for the volume of contours is shown in <xref ref-type="fig" rid="fig4">Figure 4D</xref> and resulted from 50,000 independent samples from a uniform distribution over input space. The MCMC was performed using the Metropolis algorithm with toroidal boundaries and consisted of 10,000 samples on each of 10 chains (with the first 500 observations discarded as warm-up). Here <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline36.gif"/></alternatives></inline-formula>.</p></caption>
<graphic xlink:href="432393_fig6.tif"/>
</fig>
<p>We apply the function specified by <xref ref-type="disp-formula" rid="eqn16">equation (16)</xref> to a large number of independent samples from the above distribution, and we fit a normal distribution to the output distribution (<xref ref-type="fig" rid="fig6">Figure 6B</xref>).</p>
<sec id="s2c3a"><label>(a)</label><title>Uniform prior distribution</title>
<p>We then use the output distribution in <xref ref-type="fig" rid="fig6">Figure 6B</xref> as a target for CMC. The resultant posterior distribution is concentrated among points that lie within the same annulus in which the bulk of the probability mass for the original input distribution lies (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). This is because our prior distribution gives equal weight to all points within a given contour. Since the function values in the annulus are similar to those obtained from using the input distribution, we obtain a posterior probability mass that is distributed uniformly around the annulus.</p>
<p>Note that the posterior distribution is <italic>not</italic> the same as the input distribution. This example demonstrates an important characteristic of deterministic models where the input dimension exceeds that of the outputs: a given input distribution induces an output distribution which, when inverted, produces an input distribution that is not necessarily the same as its cause. Information is lost when transforming from the inputs to the outputs that cannot necessarily be regained by inverting the output map.</p></sec>
<sec id="s2c3b"><label>(b)</label><title>Normal prior distribution</title>
<p>We now change the priors such that they coincide with the input distribution given in <xref ref-type="disp-formula" rid="eqn23">equation (23)</xref>. We sample from this prior distribution (<xref ref-type="fig" rid="fig7">Figure 7A</xref>) and use each (<italic>&#x03BB;</italic><sub>1</sub><italic><sub>i</sub>,&#x03BB;</italic><sub>2</sub><italic><sub>i</sub></italic>) pair as inputs to <italic>Q</italic>(<italic>&#x03BB;</italic><sub>1</sub><italic><sub>i</sub>,&#x03BB;</italic><sub>2</sub><italic><sub>i</sub></italic>). We then obtain an approximate output distribution by fitting a kernel density estimator to the resultant output values (<xref ref-type="fig" rid="fig7">Figure 7B</xref>). Note that because our prior distribution has changed, our prior output distribution <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline37.gif"/></alternatives></inline-formula> changes (compare <xref ref-type="fig" rid="fig7">Figure 7B</xref> with <xref ref-type="fig" rid="fig4">Figure 4B</xref>). We use this estimated output distribution as an input to the Metropolis stage of CMC (see Algorithm 2), where we accept an input proposal with probability,
<disp-formula id="eqn24">
<alternatives><graphic xlink:href="432393_eqn24.gif"/>
</alternatives>
</disp-formula></p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><title>Illustrative example &#x0023;3(b): (A) Samples from the prior distribution specified by <xref ref-type="disp-formula" rid="eqn23">equation (23)</xref>. (B) Output prior distribution (coloured bars) and kernel density estimates (black line) resulting from samples in subplot A. (C) Posterior distribution over inputs. (D) Output distribution (orange histogram) and target density (blue curve).</title><p>A Gaussian kernel density estimator with bandwidth 0.05 was fit to the univariate output distribution resulting from 50,000 samples from the bivariate normal prior (using Mathematica&#x2019;s &#x201C;SmoothKernelDistribution&#x201D; function [<xref ref-type="bibr" rid="c18">18</xref>]). The posterior distribution was obtained using MCMC implementing the Metropolis algorithm with toroidal boundaries and consisted of 50,000 samples on each of 12 chains. Here <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline38.gif"/></alternatives></inline-formula>.</p></caption>
<graphic xlink:href="432393_fig7.tif"/>
</fig>
<p>The posterior distribution that results is located near the bulk of probability mass of the prior distribution (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). Comparing this with the distribution that resulted from using a uniform prior (<xref ref-type="fig" rid="fig6">Figure 6C</xref>), we see that more informative priors allow us to identify the input parameter distribution. However in both cases the output distribution matched the target (<xref ref-type="fig" rid="fig6">Figure 6D</xref> and <xref ref-type="fig" rid="fig7">Figure 7D</xref>).</p>
<p>When uniform priors are specified over inputs, the estimated output distribution <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline39.gif"/></alternatives></inline-formula> represents estimates of the proportion of total input volume that map to a value of <bold><italic>Q</italic></bold>. However with non-uniform priors this is no longer true. In this case, we interpret this estimated distribution as representing our <italic>a priori</italic> output distribution that results from our prior weighting over inputs. Here <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="432393_inline40.gif"/></alternatives></inline-formula> is the estimated probability of obtaining an output <bold><italic>Q</italic></bold> given our input priors.</p></sec></sec></sec></sec>
<sec id="s3"><label>5</label><title>Results</title>
<sec id="s3a"><label>5.1</label><title>Inverting classic models of mathematical biology</title>
<p>In order to observe the performance of the CMC algorithm we consider three biological models of increasing complexity.</p>
<sec id="s3a1"><label>5.1.1</label><title>Logistic growth</title>
<p>We first consider the logistic growth equation,
<disp-formula id="eqn25">
<alternatives><graphic xlink:href="432393_eqn25.gif"/>
</alternatives>
</disp-formula>
with initial condition
<disp-formula id="eqn26">
<alternatives><graphic xlink:href="432393_eqn26.gif"/>
</alternatives>
</disp-formula>where <italic>y</italic>(<italic>t</italic>) represents the density of individuals at time <italic>t</italic> in a population that experiences competition for resources, for example, a bacterial colony confined to a Petri dish. Here <italic>r</italic> represents the initial (exponential) growth rate of the colony and <italic>&#x03BA;</italic> is the carrying capacity. An output distribution was generated by calculating <italic>Q</italic><sub>1</sub> &#x003D; <italic>y</italic>(8) for each pair of samples from the following independent input distributions,
<disp-formula id="eqn27">
<alternatives><graphic xlink:href="432393_eqn27.gif"/>
</alternatives>
</disp-formula>where we assumed that the initial population density was fixed at <italic>y</italic><sub>0</sub> &#x003D; 0.1. The resultant output distribution was fit using a normal distribution and specified as the target. We assumed that the priors for the input parameters were of the form, <italic>r</italic> &#x007E; <italic>U</italic>(0, 2) and <italic>&#x03BA;</italic> &#x007E; <italic>U</italic>(5, 15), and used CMC to estimate the posterior input distribution. Contour plots of this distribution (<xref ref-type="fig" rid="fig8">Figure 8A</xref>) indicate that the growth rate <italic>r</italic> was well identified whereas the carrying capacity was not.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8.</label>
<caption><title>Example <xref ref-type="sec" rid="s3a1">&#x00A7;5.1.1</xref>: (A) The joint density of growth rate (<italic>r</italic>) and carrying capacity (<italic>&#x03BA;</italic>) when sampling the population size at <italic>t</italic> &#x003D; 8. (B) The joint density of growth rate and carrying capacity when sampling the population size at <italic>t</italic> &#x003D; 29. (C) The joint density of growth rate and carrying capacity when sampling the population size at <italic>t</italic> &#x003D; 8 and <italic>t</italic> &#x003D; 29.</title><p>The contour volumes were estimated using a two-dimensional Gaussian kernel density estimator with default bandwidth fit to 100,000 independent samples from the prior distributions (using Matlab&#x2019;s &#x201C;mvksdensity&#x201D; function [<xref ref-type="bibr" rid="c19">19</xref>]), and the posteriors were estimated using 1.2m MCMC iterations (100,000 samples on each of 12 chains).</p></caption>
<graphic xlink:href="432393_fig8.tif"/>
</fig>
<p>To explain this pattern of identification, the elasticity of <italic>Q</italic><sub>1</sub> with respect to each parameter was calculated using the method described in [<xref ref-type="bibr" rid="c20">20</xref>] and is presented in <xref ref-type="fig" rid="fig9">Figure 9</xref>. At early times (e.g., <italic>t</italic> &#x003D; 8), the population density is sensitive to the growth rate parameter <italic>r</italic> but is relatively insensitive to the carrying capacity <italic>&#x03BA;</italic> since competition for resources has yet to predominate and we are unable to identify this parameter.</p>
<fig id="fig9" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9.</label>
<caption><title>Example <xref ref-type="sec" rid="s3a1">&#x00A7;5.1.1</xref>: The elasticities of the solution to the logistic equation. (A) Elasticity with respect to growth rate <italic>r</italic>. (B) Elasticity with respect to carrying capacity <italic>&#x03BA;</italic>.</title><p>The dashed orange and black lines indicate the elasticities for <italic>Q</italic><sub>1</sub> &#x003D; <italic>y</italic>(8) and <italic>Q</italic><sub>2</sub> &#x003D; <italic>y</italic>(29) respectively. These curves were generated by assuming <italic>y</italic><sub>0</sub> &#x003D; 0.1, <italic>r</italic> &#x003D; 0.5 and <italic>&#x03BA;</italic> &#x003D; 10.</p></caption>
<graphic xlink:href="432393_fig9.tif"/>
</fig>
<p>We next chose <italic>Q</italic><sub>2</sub> &#x003D; <italic>y</italic>(29) as the output statistic and used the same input distribution to generate a target output distribution. The posterior input distribution generated by CMC (<xref ref-type="fig" rid="fig8">Figure 8B</xref>) identifies the carrying capacity but not the growth rate. At later times (e.g., <italic>t</italic> &#x003D; 29) the elasticities with respect to the parameters <italic>r</italic> and <italic>&#x03BA;</italic> are reversed (<xref ref-type="fig" rid="fig9">Figure 9</xref>). For <italic>t&#x003E;</italic> 20, the population density is essentially determined by the (equilibrium) carrying capacity and we are unable to identify the intrinsic growth rate other than to determine it must exceed some minimum value in order for the population to grow to near its equilibrium value by <italic>t</italic> &#x003D; 29.</p>
<p>Finally we used a target distribution of a bivariate normal distribution of (<italic>Q</italic><sub>1</sub><italic>,Q</italic><sub>2</sub>). The prior distributions were unchanged. The posterior density (<xref ref-type="fig" rid="fig8">Figure 8C</xref>) indicates that in this case both input parameters were well-identified. The <italic>combined</italic> distribution of the two previous output statistics is sensitive with respect to each of the parameters. Note that in this case the posterior distribution ascribes probability mass to a region of parameter space that is similar to the original (causative) input distribution. As discussed in &#x00A7;4.1 this is a special case, not typical of underdetermined input-output maps. This is because in the univariate output cases the estimates of the individual parameters are not correlated with one another, and lead to unbiased identification of <italic>r</italic> (for <italic>Q</italic><sub>1</sub>) and <italic>&#x03BA;</italic> (for <italic>Q</italic><sub>2</sub>) respectively. These constraints ensure that using the joint output distribution produces an unbiased estimation of both parameters.</p>
<p>As might be expected, informative priors affect the recovered posterior distribution. To illustrate, we use <italic>Q</italic><sub>2</sub> &#x003D; <italic>y</italic>(29) as our univariate output summary statistic. Given uniform priors, the posterior distribution (<xref ref-type="fig" rid="fig10">Figure 10A</xref> and <xref ref-type="fig" rid="fig8">Figure 8B</xref>) indicates that, although we can identify the carrying capacity, we cannot identify the growth rate. If instead of a uniform prior, we use <italic>r</italic> &#x007E; &#x0393;(2.5, 0.2) as the prior distribution which allocates probability mass around <italic>r</italic> &#x003D; 0.5, the resultant posterior narrows with respect to this parameter (<xref ref-type="fig" rid="fig10">Figure 10B</xref>). Using a prior distribution that is narrower still, <italic>r</italic> &#x007E; &#x0393;(40, 0.0125), we strongly identify both parameters (<xref ref-type="fig" rid="fig10">Figure 10C</xref>).</p>
<fig id="fig10" position="float" orientation="portrait" fig-type="figure">
<label>Figure 10.</label>
<caption><title>Example <xref ref-type="sec" rid="s3a1">&#x00A7;5.1.1</xref>: The effect of reducing the variance of the prior distribution for the growth rate on parameter identification.</title><p><bold>(A)</bold> <italic>r</italic> &#x007E; <italic>U</italic>(0, 2)<bold>. (B)</bold> <italic>r</italic> &#x007E; &#x0393;(2.5, 0.2)<bold>. (C)</bold> <italic>r</italic> &#x007E; &#x0393;(40, 0.0125). The output distributions were chosen to correspond to a mean of (<italic>r</italic>, <italic>&#x03BA;</italic>) &#x003D; (0.5, 10). In all cases the prior specified on the carrying capacity was <italic>&#x03BA;</italic> &#x007E; <italic>U</italic>(5, 15) and the population size was sampled at time step <italic>t</italic> &#x003D; 29. The contour volumes were estimated using a two-dimensional Gaussian kernel density estimator with bandwidth 0.05 fit to 100,000 independent samples from the prior distributions (using Matlab&#x2019;s &#x201C;mvksdensity&#x201D; function [<xref ref-type="bibr" rid="c19">19</xref>]), and the posteriors were estimated using 1.2m iterations of the Metropolis-Hastings algorithm.</p></caption>
<graphic xlink:href="432393_fig10.tif"/>
</fig>
<p>As in Bayesian statistics, there are two ways to identify a parameter: either we collect more data (here by specifying another summary statistic), or we use prior information. In Bayesian statistics, it is the stochasticity of the input-to-output map that generates the uncertainty, meaning that the output distribution (the &#x201C;data&#x201D;) can be generated by a collection of possible parameter distributions. In deterministic input-output maps, where the dimension of the inputs exceeds the outputs, it is the multiplicity of the mapping that introduces uncertainty. In both cases, a prior distribution must be specified to ensure that the resultant posterior is unique and a valid probability distribution.</p></sec>
<sec id="s3a2"><label>5.1.2</label><title>Michaelis-Menten kinetics</title>
<p>The Michaelis-Menten model of enzyme kinetics (see, for example, [<xref ref-type="bibr" rid="c8">8</xref>]), describes the dynamics of concentrations of an enzyme (<italic>E</italic>), a substrate (<italic>S</italic>), an enzyme-substrate complex (<italic>ES</italic>), and a product (<italic>P</italic>). Specifically,
<disp-formula id="eqn28">
<alternatives><graphic xlink:href="432393_eqn28.gif"/>
</alternatives>
</disp-formula>with initial conditions
<disp-formula id="eqn29">
<alternatives><graphic xlink:href="432393_eqn29.gif"/>
</alternatives>
</disp-formula>where <italic>k<sub>f</sub></italic> is the rate constant for the forward reaction <italic>E</italic> &#x002B; <italic>S</italic> &#x2192; <italic>ES</italic>, <italic>k<sub>r</sub></italic> is the rate of the reverse reaction <italic>ES</italic> &#x2192; <italic>E</italic> &#x002B; <italic>S</italic>, and <italic>k<sub>cat</sub></italic> is the catalytic rate at which the product is formed by the reaction <italic>ES</italic> &#x2192; <italic>E</italic> &#x002B; <italic>P</italic>. We assume independent uniform priors on each of the parameters (<italic>k<sub>f</sub>,k<sub>r</sub>,k<sub>cat</sub></italic>), with upper and lower bounds indicated by the axes in <xref ref-type="fig" rid="fig11">Figure 11</xref>. We target the bivariate output distribution given by,
<disp-formula id="eqn30">
<alternatives><graphic xlink:href="432393_eqn30.gif"/>
</alternatives>
</disp-formula></p>
<fig id="fig11" position="float" orientation="portrait" fig-type="figure">
<label>Figure 11.</label>
<caption><title>Example &#x00A7;5.1.2: The marginal densities of parameters for the Michaelis-Menten model.</title><p>The parameters were assigned uniform priors over the ranges shown in the axes. The contour volumes were estimated using a Gaussian kernel density estimator with default bandwidth fit to 120,000 iterations (using the &#x201C;kde&#x201D; function in the &#x201C;ks&#x201D; R [<xref ref-type="bibr" rid="c21">21</xref>] package with grid sizes of 300 and maxima of 4 in each dimension [<xref ref-type="bibr" rid="c22">22</xref>]), and the posteriors were estimated using c.300,000 MCMC iterations.</p></caption>
<graphic xlink:href="432393_fig11.tif"/>
</fig>
<p>Running CMC resulted in output distributions which matched the target (see S1&#x2013;Michaelis Menten). The posterior input distribution indicates that <italic>k<sub>cat</sub></italic> is strongly identified by this choice of output distribution and <italic>k<sub>f</sub></italic> is restricted to a subset of its prior bounds (<xref ref-type="fig" rid="fig11">Figures 11</xref> &#x0026; <xref ref-type="fig" rid="fig12">12</xref>). The rate of the backward reaction <italic>k<sub>r</sub></italic>, however, cannot be recovered from this output target.</p>
<fig id="fig12" position="float" orientation="portrait" fig-type="figure">
<label>Figure 12.</label>
<caption><title>Example &#x00A7;5.1.2: The joint distribution of pairs of parameters for the Michaelis-Menten model.</title><p>The parameters were assigned uniform priors over the ranges shown in the axes. The contour volumes were estimated using a Gaussian kernel density estimator with default bandwidth fit to 120,000 iterations (using the &#x201C;kde&#x201D; function in the &#x201C;ks&#x201D; R [<xref ref-type="bibr" rid="c21">21</xref>] package with grid sizes of 300 and maxima of 4 in each dimension [<xref ref-type="bibr" rid="c22">22</xref>]), and the posteriors were estimated using c.300,000 MCMC iterations.</p></caption>
<graphic xlink:href="432393_fig12.tif"/>
</fig>
<p>Some insight in to this pattern of identifiability can be gained by examining the elasticities of <italic>E an</italic>d <italic>S t</italic>o each of the input parameters (<xref ref-type="fig" rid="fig13">Figure 13</xref>). The enzyme and substrate concentrations at times <italic>t</italic> &#x003D; 2 and <italic>t</italic> &#x003D; 1, respectively, are most sensitive to changes in <italic>k<sub>cat</sub></italic>, and this parameter is the most clearly identified. The enzyme and substrate concentrations at our sampling times are less sensitive to the forward rate of reaction, k<italic><sub>f</sub></italic>, but these values are apparently sufficient to localise this parameter to a subregion of its prior bounds (<xref ref-type="fig" rid="fig11">Figure 11</xref>). The rate of the backward reaction, however, does not strongly influence either the sampled values of the enzyme or substrate concentrations at our sampling times, and this parameter is practically unidentifiable.</p>
<fig id="fig13" position="float" orientation="portrait" fig-type="figure">
<label>Figure 13.</label>
<caption><title>Example &#x00A7;5.1.2: The elasticities of the enzyme (top row) and substrate (bottom row) concentrations with respect to the three input parameters.</title><p>The dashed orange lines indicate the times at which each output is sampled in our example. These curves were generated by assuming <italic>k<sub>f</sub></italic> &#x003D; 2, <italic>k<sub>r</sub></italic> &#x003D; 1, <italic>k<sub>cat</sub></italic> &#x003D; 1.5, <italic>E</italic>(0) &#x003D; 4, <italic>S</italic>(0) &#x003D; 8, <italic>ES</italic>(0) &#x003D; 0, and <italic>P</italic> (0) &#x003D; 0.</p></caption>
<graphic xlink:href="432393_fig13.tif"/>
</fig></sec>
<sec id="s3a3"><label>5.1.3</label><title>SIR model</title>
<p>An SIR model of disease transmission incorporating a carrying capacity for the population is described by the following system of ODEs [<xref ref-type="bibr" rid="c23">23</xref>],
<disp-formula id="eqn31">
<alternatives><graphic xlink:href="432393_eqn31.gif"/>
</alternatives>
</disp-formula>with initial conditions
<disp-formula id="eqn32">
<alternatives><graphic xlink:href="432393_eqn32.gif"/>
</alternatives>
</disp-formula>where <italic>N</italic>(<italic>t</italic>) &#x003D; <italic>S</italic>(<italic>t</italic>)&#x002B; <italic>I</italic>(<italic>t</italic>)&#x002B; <italic>R</italic>(<italic>t</italic>) is the total population density at time <italic>t</italic>.</p>
<p>Including the initial conditions (<italic>S</italic><sub>0</sub><italic>,I</italic><sub>0</sub><italic>,R</italic><sub>0</sub>), we have nine uncertain parameters, each of which is assumed to have uniform prior distributions. The upper and lower bounds of these distributions are provided by the axes in <xref ref-type="fig" rid="fig14">Figure 14</xref>. We use a three dimensional output statistic in which the elements consist of sampling the infected population at <italic>t</italic> &#x003D; 10 and the recovered population at times <italic>t</italic> &#x003D; 10 and <italic>t</italic> &#x003D; 80. For our target distribution, we choose the following multivariate normal,
<disp-formula id="eqn33">
<alternatives><graphic xlink:href="432393_eqn33.gif"/>
</alternatives>
</disp-formula></p>
<fig id="fig14" position="float" orientation="portrait" fig-type="figure">
<label>Figure 14.</label>
<caption><title>Example &#x00A7;5.1.3: The marginal densities of parameters for the SIR model.</title>
<p>The parameters were assigned uniform priors over the ranges shown in the axes. The contour volumes were estimated using a Gaussian kernel density estimator with default bandwidth fit to 100,000 iterations (using the &#x201C;kde&#x201D; function in the &#x201C;ks&#x201D; R [<xref ref-type="bibr" rid="c21">21</xref>] package with grid sizes of 300 and maxima of 3000 in each dimension [<xref ref-type="bibr" rid="c22">22</xref>]), and the posteriors were estimated using 1.2m MCMC iterations (100,000 samples on each of 12 chains).</p></caption>
<graphic xlink:href="432393_fig14.tif"/>
</fig>
<p>We restrict the dimension of the output space in order to examine a highly underdetermined system and to avoid issues associated with kernel density estimation in higher dimensional output spaces. A further discussion of this issue appears in &#x00A7;6.</p>
<p>The posterior distribution over inputs indicates modest identification of the parameter <italic>r</italic>, which dictates the rate of initial exponential growth for the susceptible population (<xref ref-type="fig" rid="fig14">Figure 14<italic>r</italic></xref>) and the carrying capacity term <italic>&#x03BA;</italic>. The posterior distribution of the death rate <italic>&#x00B5;</italic> and of the recovery rate <italic>&#x03B3;</italic> was also constrained to a subset of input space (<xref ref-type="fig" rid="fig14">Figure 14<italic>&#x03BA;</italic>, 14<italic>&#x00B5;</italic>, 14<italic>&#x03B3;</italic></xref>). The remaining parameters of the model were unidentified using these output statistics (<xref ref-type="fig" rid="fig14">Figures 14</xref>). For examples of joint distributions of selected pairs of parameters see S2&#x2013;SIR model. For the posterior input distribution that resulted, we calculated the corresponding output distribution, and compared this with the marginal target distributions, indicating good agreement (see S2&#x2013;SIR model).</p></sec></sec></sec>
<sec id="s4"><label>6</label><title>Discussion</title>
<p>Natural selection has dictated that organisms have often evolved redundancies for systems essential for life. By building mathematical models we aim to mimic these systems, meaning that these models should embody similar redundancies as their biological counterparts. Assessing the sensitivity of key characteristics of model outputs to perturbations in input parameters provides insight into the sensitivity of the system to each of its constituent elements. These so-called sensitivity analyses of mathematical models allow us to probe the biological system even when biological experiments are infeasible. Inverse sensitivity analysis inverts this process and instead of determining how model outputs vary in response to changes to the input parameter values, estimates a distribution over inputs which achieves a given distribution over outputs.</p>
<p>In this paper, we introduce an approach to inverse sensitivity analysis which can be applied to systems with many input parameters, mitigating the curse of dimensionality that limits the scope of methods which rely on grid-based approaches to build an explicit output-to-input map [<xref ref-type="bibr" rid="c10">10</xref>]. We have demonstrated that our algorithms can perform inverse sensitivity analysis on mathematical models of biological systems across a range of complexities, including the logistic growth model (2 inputs &#x0026; 1 output target), Michaelis-Menten kinetics (3 inputs &#x0026; 2 output targets), and the SIR model with uncertain initial population sizes (9 inputs &#x0026; 3 output targets). As well as detailing our algorithms, we provide a probabilistic framework for understanding inverse sensitivity analysis, in which &#x201C;prior&#x201D; probability distributions are set on the inputs. These prior beliefs over input values are consistent with a &#x201C;posterior&#x201D; input distribution which, when transformed through the input-to-output map, results in a &#x201C;target&#x201D; output distribution. To sample from these posterior input distributions, we introduce a two-step sampling algorithm. In the first of these steps, input parameters are independently sampled from their prior distributions and, by fitting a kernel density estimator to the output values, this provides an approximate Jacobian transform (which we interchangeably term a &#x201C;contour volume distribution&#x201D;), which is used in the second step involving Markov chain Monte Carlo. A similar algorithm for inverse sensitivity analysis has recently been derived from a measure theoretic perspective by [<xref ref-type="bibr" rid="c24">24</xref>]. These authors also investigate stability of the posterior distribution with respect to the observed output distribution, the assumed prior distribution and the approximation of the contours of the forward map. We believe that the different path we take to the shared goal offers complementary insight into the algorithm&#x2019;s mechanism and provides an intuitive way to understand inverse sensitivity analysis, more generally.</p>
<p>There are several subtleties in the first steps of the processes described in Algorithms 1 and 2) which must be understood in order to ensure a valid input distribution is obtained. Indeed, these intricacies complicated our own efforts in testing the algorithms. Provided the output is well-behaved over the space of possible input values, a univariate output distribution can be approximated given a relatively modest number of samples from the input priors using standard kernel density estimation (KDE). Here, for the univariate output target distributions we found that KDE with a Gaussian kernel using default bandwidths from each software package used (Matlab, Mathematica and R [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c21">21</xref>]) was able to represent the output distribution with sufficient fidelity to ensure the input posterior recaptured the output target. The number of input samples necessary to ensure convergence to the true posterior input distribution, however, depends on the exact output distribution being targeted. If the bulk of probability mass for the target output distribution lies at a location in output space where the contour volume is rapidly varying, then the input distribution obtained will be sensitive to errors in kernel density estimates of the contour volume distribution, and many samples will be required. Similarly, if a region of low contour volume is targeted, then kernel density estimates with few samples will be relatively noisy and more samples will be necessary. Here we have assumed numerical errors in solving the map are negligible and independent of the parameters. Neither assumption is likely to be true for sophisticated partial differential equation models and the interaction between numerical and sampling errors is the subject of ongoing analysis. KDE introduces a further source of error, which must be carefully managed to ensure reasonable results are obtained.</p>
<p>Our algorithms avoid the curse of dimensionality of the input space which plagues grid-based approaches to inverse sensitivity analysis. The necessity of having to fit a probability distribution to the output samples resultant from sampling the prior input distributions means that, at present, our approach is limited to problems with relatively few outputs. In &#x00A7;5.1.3, the output target was a three-dimensional distribution, and we expended considerable effort finding a KDE method that adequately approximated the three-dimensional contour volume distribution. We ultimately found that the most effective approach was obtained by using the &#x201C;kde&#x201D; function within the &#x201C;ks&#x201D; R package [<xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c22">22</xref>], which uses the data to estimate unconstrained bandwidth matrices, which are then used to fit kernel density estimates to data with up to six dimensions. Density estimation, however, is currently an active area of research and software packages exist implementing many different variants of KDE (see [<xref ref-type="bibr" rid="c25">25</xref>] for a review of the R packages already available in 2011). Vine copulas have recently been suggested as an approach which avoids the curse of dimensionality in density estimation [<xref ref-type="bibr" rid="c26">26</xref>]. If this promise is realised, then our algorithm will be applicable to output target distributions of higher dimensions.</p>
<p>Mathematical models have proved indispensable tools for elucidating understanding of biological systems, which are frequently not amenable to direct experimentation. Biological systems are often robust to perturbations to particular constituent processes, and we can use mathematical models to explore these sensitivities. Inverse sensitivity analyses are a relatively recent addition to a modeller&#x2019;s toolbox, which allows one to determine an input distribution - consistent with prior beliefs - that can generate a given distribution of outputs. Here we introduce a Monte Carlo method which extends the range of models for which inverse sensitivity analysis can be performed, and illustrate its utility for several problems of interest to computational biology. It is our hope that, by publishing this method, others are encouraged to undertake inverse sensitivity analysis, which we have found is insightful for building and analysing mathematical models.</p></sec>
<sec id="s5"><label>7</label><title>Author contributions</title>
<p>BL, DJG and SJT conceived the study. BL and SJT carried out the analysis. All authors helped to write and edit the manuscript.</p></sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Nielsen</surname> <given-names>PM</given-names></string-name>, <string-name><surname>Le Grice</surname> <given-names>IJ</given-names></string-name>, <string-name><surname>Smaill</surname> <given-names>BH</given-names></string-name>, <string-name><surname>Hunter</surname> <given-names>PJ</given-names></string-name>. <article-title>Mathematical model of geometry and fibrous structure of the heart</article-title>. <source>American Journal of Physiology-Heart and Circulatory Physiology</source>. <year>1991</year>;<volume>260</volume>(<issue>4</issue>):<fpage>H1365</fpage>&#x2013;<lpage>H1378</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Noble</surname> <given-names>D</given-names></string-name>. <article-title>Modeling the heart</article-title>. <source>Physiology</source>. <year>2004</year>;<volume>19</volume>(<issue>4</issue>):<fpage>191</fpage>&#x2013;<lpage>197</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Byrne</surname> <given-names>HM</given-names></string-name>, <string-name><surname>Alarcon</surname> <given-names>T</given-names></string-name>, <string-name><surname>Owen</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Webb</surname> <given-names>SD</given-names></string-name>, <string-name><surname>Maini</surname> <given-names>PK</given-names></string-name>. <article-title>Modelling aspects of cancer dynamics: a review. Philosophical Transactions of the Royal Society of London A: Mathematical</article-title>, <source>Physical and Engineering Sciences</source>. <year>2006</year>;<volume>364</volume>(<issue>1843</issue>):<fpage>1563</fpage>&#x2013;<lpage>1578</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Byrne</surname> <given-names>HM</given-names></string-name>. <article-title>Dissecting cancer through mathematics: from the cell to the animal model</article-title>. <source>Nature Reviews Cancer</source>. <year>2010</year>;<volume>10</volume>(<issue>3</issue>):<fpage>221</fpage>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="book"><string-name><surname>Anderson</surname> <given-names>RM</given-names></string-name>, <string-name><surname>May</surname> <given-names>RM</given-names></string-name>. <source>Infectious diseases of humans: dynamics and control</source>. <publisher-name>Oxford University Press</publisher-name>; <year>1992</year>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="book"><string-name><surname>Morris</surname> <given-names>WF</given-names></string-name>, <string-name><surname>Doak</surname> <given-names>DF</given-names></string-name>, <etal>et al.</etal> <source>Quantitative conservation biology</source>. <publisher-loc>Sinauer, Sunderland</publisher-loc>, <publisher-name>Massachusetts, USA</publisher-name>. <year>2002</year>;.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Hanski</surname> <given-names>I</given-names></string-name>. <article-title>Spatially realistic theory of metapopulation ecology</article-title>. <source>Naturwissenschaften</source>. <year>2001</year>;<volume>88</volume>(<issue>9</issue>):<fpage>372</fpage>&#x2013;<lpage>381</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="book"><string-name><surname>Murray</surname> <given-names>JD</given-names></string-name>. <chapter-title>Mathematical biology: I</chapter-title>. <source>An Introduction (interdisciplinary applied mathematics)(Pt. 1)</source>. <publisher-loc>New York</publisher-loc>, <publisher-name>Springer</publisher-name>; <year>2007</year>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Daly</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Gavaghan</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Holmes</surname> <given-names>C</given-names></string-name>, <string-name><surname>Cooper</surname> <given-names>J</given-names></string-name>. <article-title>Hodgkin-Huxley revisited: reparametrization and identifiability analysis of the classic action potential model with approximate Bayesian methods</article-title>. <source>Royal Society Open Science</source>. <year>2018</year>;<volume>2</volume>:<fpage>150499</fpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Butler</surname> <given-names>T</given-names></string-name>, <string-name><surname>Estep</surname> <given-names>D</given-names></string-name>, <string-name><surname>Tavener</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Dawson</surname> <given-names>C</given-names></string-name>, <string-name><surname>Westerink</surname> <given-names>JJ</given-names></string-name>. <article-title>A measure-theoretic computational method for inverse sensitivity problems III: Multiple quantities of interest</article-title>. <source>SIAM/ASA Journal on Uncertainty Quantification</source>. <year>2014</year>;<volume>2</volume>(<issue>1</issue>):<fpage>174</fpage>&#x2013;<lpage>202</lpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="book"><string-name><surname>Lambert</surname> <given-names>B</given-names></string-name>. <source>A Student&#x2019;s Guide to Bayesian Statistics</source>. <publisher-name>Sage Publications Ltd</publisher-name>.; <year>2018</year>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Haario</surname> <given-names>H</given-names></string-name>, <string-name><surname>Saksman</surname> <given-names>E</given-names></string-name>, <string-name><surname>Tamminen</surname> <given-names>J</given-names></string-name>, <etal>et al.</etal> <article-title>An adaptive Metropolis algorithm</article-title>. <source>Bernoulli</source>. <year>2001</year>;<volume>7</volume>(<issue>2</issue>):<fpage>223</fpage>&#x2013;<lpage>242</lpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Neal</surname> <given-names>RM</given-names></string-name>, <etal>et al.</etal> <article-title>MCMC using Hamiltonian dynamics</article-title>. <source>Handbook of Markov Chain Monte Carlo</source>. <year>2011</year>;<volume>2</volume>:<fpage>113</fpage>&#x2013;<lpage>162</lpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Gelman</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rubin</surname> <given-names>DB</given-names></string-name>. <article-title>Inference from iterative simulation using multiple sequences</article-title>. <source>Statistical Science</source>. <year>1992</year>; p. <fpage>457</fpage>&#x2013;<lpage>472</lpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="book"><string-name><surname>Gelman</surname> <given-names>A</given-names></string-name>, <string-name><surname>Carlin</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Stern</surname> <given-names>HS</given-names></string-name>, <string-name><surname>Dunson</surname> <given-names>DB</given-names></string-name>, <string-name><surname>Vehtari</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rubin</surname> <given-names>DB</given-names></string-name>. <source>Bayesian data analysis</source>. <publisher-name>CRC press</publisher-name>; <year>2013</year>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Metropolis</surname> <given-names>N</given-names></string-name>, <string-name><surname>Rosenbluth</surname> <given-names>AW</given-names></string-name>, <string-name><surname>Rosenbluth</surname> <given-names>MN</given-names></string-name>, <string-name><surname>Teller</surname> <given-names>AH</given-names></string-name>, <string-name><surname>Teller</surname> <given-names>E</given-names></string-name>. <article-title>Equation of state calculations by fast computing machines</article-title>. <source>The Journal of Chemical Physics</source>. <year>1953</year>;<volume>21</volume>(<issue>6</issue>):<fpage>1087</fpage>&#x2013;<lpage>1092</lpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="book"><string-name><surname>Yang</surname> <given-names>L</given-names></string-name>. <source>Infinite dimensional stochastic inverse problems</source>. <publisher-name>Colorado State University</publisher-name>; <year>2018</year>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Wolfram Research</surname> <given-names>I</given-names></string-name>. <source>Mathematica 8.0</source>;. <ext-link ext-link-type="uri" xlink:href="https://www.wolfram.com">https://www.wolfram.com</ext-link>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="website"><collab>MATLAB version 9.0.0.341360 (R2016a)</collab>; <year>2016</year>. <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/">https://www.mathworks.com/</ext-link>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Daly</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Gavaghan</surname> <given-names>D</given-names></string-name>, <string-name><surname>Cooper</surname> <given-names>J</given-names></string-name>, <string-name><surname>Tavener</surname> <given-names>SJ</given-names></string-name>. <article-title>Inference-based assessment of parameter identifiability in nonlinear biological models</article-title>. <source>Journal of the Royal Society Interface</source>. <year>2018</year>;<fpage>15</fpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="website"><collab>R Core Team</collab>. <source>R: A Language and Environment for Statistical Computing</source>; <year>2014</year>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.R-project.org/">http://www.R-project.org/</ext-link>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="website"><string-name><surname>Duong</surname> <given-names>T</given-names></string-name>, <string-name><surname>Duong</surname> <given-names>MT</given-names></string-name>. <article-title>Package ks</article-title>; <year>2018</year>. <ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/web/packages/ks/ks.pdf">https://cran.r-project.org/web/packages/ks/ks.pdf</ext-link>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Tavener</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Mikucki</surname> <given-names>M</given-names></string-name>, <string-name><surname>Field</surname> <given-names>SG</given-names></string-name>, <string-name><surname>Antolin</surname> <given-names>MF</given-names></string-name>. <article-title>Transient sensitivity analysis for nonlinear population models</article-title>. <source>Methods in Ecology and Evolution</source>. <year>2011</year>;<volume>2</volume>(<issue>5</issue>):<fpage>560</fpage>&#x2013;<lpage>575</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Butler</surname> <given-names>T</given-names></string-name>, <string-name><surname>Jakeman</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wildey</surname> <given-names>T</given-names></string-name>. <article-title>Combining push forward measures and Baye&#x2019;s rule to construct consistent solutions to stochastic inverse problems</article-title>. <source>SIAM J Sci Comput</source>. <year>2018</year>;<volume>40</volume>(<issue>2</issue>):<fpage>A984</fpage>&#x2013;<lpage>A1011</lpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="website"><string-name><surname>Deng</surname> <given-names>H</given-names></string-name>, <string-name><surname>Wickham</surname> <given-names>H</given-names></string-name>. <article-title>Density estimation in R</article-title>; <year>2011</year>. <ext-link ext-link-type="uri" xlink:href="https://vita.had.co.nz/papers/density-estimation.pdf">https://vita.had.co.nz/papers/density-estimation.pdf</ext-link>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Nagler</surname> <given-names>T</given-names></string-name>, <string-name><surname>Czado</surname> <given-names>C</given-names></string-name>. <article-title>Evading the curse of dimensionality in nonparametric density estimation with simplified vine copulas</article-title>. <source>Journal of Multivariate Analysis</source>. <year>2016</year>;<volume>151</volume>:<fpage>69</fpage>&#x2013;<lpage>89</lpage>.</mixed-citation></ref>
</ref-list>
<sec id="s6"><title>Supporting information captions</title>
<list list-type="bullet">
<list-item><p>S1&#x2013;Michaelis-Menten kinetics</p></list-item>
<list-item><p>S2&#x2013;SIR model</p></list-item>
<list-item><p>S3&#x2013;A nonlinear map</p></list-item></list>
</sec>
</back>
</article>