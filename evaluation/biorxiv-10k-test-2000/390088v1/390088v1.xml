<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/390088</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Human pointing errors suggest a flattened, task-dependent representation of space</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8674-2763</contrib-id>
<name><surname>Vuong</surname><given-names>Jenny</given-names></name>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
<xref ref-type="aff" rid="a1">&#x2020;</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9839-660X</contrib-id>
<name><surname>Fitzgibbon</surname><given-names>Andrew W.</given-names></name>
<xref ref-type="aff" rid="a2">&#x2021;</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8674-2763</contrib-id>
<name><surname>Glennerster</surname><given-names>Andrew</given-names></name>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
<xref ref-type="aff" rid="a1">&#x2020;</xref>
</contrib>
<aff id="a1"><label>&#x2020;</label><institution>School of Psychology and Clinical Language Sciences, University of Reading</institution>, <country>United Kingdom</country></aff>
<aff id="a2"><label>&#x2021;</label><institution>Microsoft</institution>, Cambridge, <country>United Kingdom</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label>Corresponding authors</corresp>
</author-notes>
<pub-date pub-type="epub"><year>2018</year></pub-date>
<elocation-id>390088</elocation-id>
<history>
<date date-type="received"><day>11</day><month>8</month><year>2018</year></date>
<date date-type="rev-recd"><day>11</day><month>8</month><year>2018</year></date>
<date date-type="accepted"><day>13</day><month>8</month><year>2018</year></date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="390088.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>People are able to keep track of objects as they navigate through space, even when objects are out of sight. This requires some kind of representation of the scene and of the observer&#x2019;s location. We tested the accuracy and reliability of observers&#x2019; estimates of the visual direction of previously-viewed targets. Participants viewed 4 objects from one location, with binocular vision and small head movements giving information about the 3D locations of the objects. Without any further sight of the targets, participants walked to another location and pointed towards them. All the conditions were tested in an immersive virtual environment and some were also carried out in a real scene. Participants made large, consistent pointing errors that are poorly explained by any single 3D representation. Instead, a flattened representation of space that is dependent on the structure of the environment at the time of pointing provides a good account of participants&#x2019; errors. This suggests that the mechanisms for updating visual direction of unseen targets are not based on a stable 3D model of the scene, even a distorted one.</p>
</abstract>
<counts>
<page-count count="41"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<label>1.</label>
<title>Introduction</title>
<p>If a moving observer is to keep track of the location of objects they have seen earlier but which are currently out of view, they must store some kind of representation of the scene and update their location and orientation within that representation. There is no consensus on how this might be done in humans. One possibility is that the representation avoids using 3D coordinates and instead relies on a series of stored sensory states connected by actions (&#x2019;view-based&#x2019;), as has been proposed in simple animals such as bees or wasps [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c3">3</xref>], humans [<xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c6">6</xref>] and deep neural networks [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c8">8</xref>]. Alternatively, the representation might be based on 3D coordinate frames that are stable in world-, body- or head-centered frames [<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c10">10</xref>], possibly based on &#x2018;grid&#x2019; cells in entorhinal cortex [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c12">12</xref>] or &#x2018;place&#x2019; cells in the hippocampus [<xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c14">14</xref>].</p>
<p>One key difference between these approaches is the extent to which the observer&#x2019;s task is incorporated in the representation. For 3D coordinate-based representations of a scene, task is irrelevant and, by definition, the underlying representation remains constant however it is interrogated. Other representations do not have this constraint. Interestingly, in new approaches to visual scene representation using reinforcement learning and deep neural networks, the task and the environment are inextricably linked in the learned representation [<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c7">7</xref>]. This task-dependency is one of the key determinants we explore in our experiment on spatial updating.</p>
<p>If the brain has access to a 3D model of the scene and the observer&#x2019;s location in the same coordinate frame then, in theory, spatial updating is a straight-forward matter of geometry. It is harder to see how it could be done in a view-based framework. People can imagine what will happen when they move [<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c18">18</xref>] although they often do so with very large errors [<xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c24">24</xref>]. In this paper, we examine the accuracy and precision of pointing to targets that were viewed from one location and then not seen again as the observer walked to a new location to point in order to test the hypothesis that a single 3D reconstruction of the scene, built up when the observer was initially inspecting the scene, can explain observers&#x2019; pointing directions. The task is similar to that described in many experiments on spatial updating such as indirect walking to a target [<xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>], a triangle completion task [<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref>], drawing a map of a studied environment including previously viewed objects&#x2019; location [<xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c32">32</xref>] or viewing a set of objects on a table and then indicating the remembered location of the objects after walking round it or after the table has been rotated [<xref ref-type="bibr" rid="c33">33</xref>, <xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c36">36</xref>]. However, none of these studies have compared directly the predictions of a 3D reconstruction model with one that varies according to the location of the observer when they point, as we do here.</p>
<p>Spatial updating has been discussed in relation to both &#x2018;egocentric&#x2019; and &#x2018;allocentric&#x2019; representations of a scene [<xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c40">40</xref>] and, in theory, either or both of these representations could be used in order to point at a target. An &#x2018;egocentric&#x2019; model is assumed to encode local orientations and distances of objects relative to the observer [<xref ref-type="bibr" rid="c41">41</xref>, <xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c33">33</xref>, <xref ref-type="bibr" rid="c38">38</xref>] while an allocentric model is world-based reflecting the fact that the relative orientation of objects in the representation would not be affected by the observer walking from one location to another [<xref ref-type="bibr" rid="c40">40</xref>]. People might use both [<xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c39">39</xref>]. Wang and Spelke [<xref ref-type="bibr" rid="c38">38</xref>] have emphasised consistency as a useful discriminator between the models. So, for example, disorientating a participant by spinning them on a chair should affect pointing errors to all objects by adding a constant bias if participants use an allocentric representation. The argument that Wang and Spelke [<xref ref-type="bibr" rid="c42">42</xref>] make about disorientation conflates two separate issues, one about the origin and axes of a representation (such and &#x2018;allocentric&#x2019; or &#x2018;egocentric&#x2019;) and the other about the internal consistency of a representation. In this paper, we focus on the latter issue. We ask whether a single consistent, but possibly distorted, 3D reconstruction of the scene could explain the way that people point to previously-viewed targets.</p>
</sec>
<sec id="s2">
<label>2.</label>
<title>Results</title>
<p>Our stimulus was designed so that participants could get a &#x2018;normal&#x2019; view of the spatial layout of 4 target objects whose position they would have to remember i.e., in either the real world or in a head mounted display, they had binocular view of the scene and could move their head freely (typically, they moved &#x00B1;25cm). The targets consisted of four different colored boxes that were laid out on one side of the room at about eye height (see <xref ref-type="fig" rid="fig1">Fig. 1a</xref>) while, on the other side of the room, there were partitions (referred to from now on as &#x2018;walls&#x2019;) that obscured the target objects from view once the participant had left the original viewing zone (&#x2018;Start zone&#x2019;), see <xref ref-type="fig" rid="fig1">Fig. 1d and e</xref>. For the first experiment, the scene was set up in a real room (<xref ref-type="fig" rid="fig1">Fig. 1a</xref>) and replicated in a virtual scene (<xref ref-type="fig" rid="fig1">Fig. 1b</xref>). After viewing the scene, participants walked to one of 3 pointing zones where they pointed multiple times to each of the boxes in a specified order (randomized per trial, see Methods for details).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><p><italic>(a) Photo of experimental setup in real world (b) screen shot of experimental setup in virtual reality (c) schematic plan view of one testing layout. 4 boxes are arranged such that the blue and the pink, and the red and the yellow boxes lie along two separate visual directions as seen from the start zone (white diamond). From the start zone, the blue and the red box are always closer than the pink and the yellow box. The two visual directions subtend an angle of 25&#x00B0;. This angle is preserved for all box layouts (though distances varied, see Methods). Pointing to targets was tested at 3 different pointing zones (amber, black and cyan diamond). Black lines indicate positions of walls (in the real world, these were made from temporary walls). The icon in the corner shows nominal &#x2018;North&#x2019; direction. (d) We defined pointing direction as the direction indicated by the pointing device, labeled here as &#x03D5;. (e) We did</italic> not <italic>use direction subtended at the cyclopean point (midpoint between the left and the right eye along the interocular axis) and the tip of the pointing device, labeled here as &#x03B8;</italic>.</p></caption>
<graphic xlink:href="390088_fig1.tif"/>
</fig>
<p><xref ref-type="fig" rid="fig1">Fig. 1</xref> shows the layout of the boxes in a real scene (<xref ref-type="fig" rid="fig1">Fig. 1a</xref>), a virtual scene (<xref ref-type="fig" rid="fig1">Fig. 1b</xref>) and in plan view (<xref ref-type="fig" rid="fig1">Fig. 1c</xref>, shown here for Experiment 1). The obscuring walls are shown on the left with a participant pointing in the real scene. The measure of pointing error used was the signed angle between the target and the &#x2018;straight ahead&#x2019; indicated by the pointing device subtended at the tracked center of the pointing device, i.e. participants were asked to &#x2018;shoot&#x2019; at the target boxes, yielding the pointing direction illustrated in <xref ref-type="fig" rid="fig1">Fig. 1d</xref>. Although not all participants had experience of shooting in video games or similar, the instruction was understood by all and this definition of pointing error gave rise to an unbiased distribution of errors for shots at a visible target (<xref ref-type="fig" rid="figS6">Figure S6</xref>) which was not the case for a considered alternative, namely the direction of the pointing device relative to the cyclopean eye (<xref ref-type="fig" rid="fig1">Fig. 1e</xref>).</p>
<p><xref ref-type="fig" rid="fig2">Fig. 2a&#x2013;2c</xref> illustrate pointing directions made by one participant recorded in one trial. The participant pointed to each of the target boxes 8 times in a pseudo-random order (specified by the experimenter) from one of 3 pointing zones (shown in a, b and c respectively). These plots demonstrate a pattern of biases that was typical across participants: they tended to point too far &#x2018;north&#x2019; relative to the true target positions when they were at the north end of the room (see <xref ref-type="fig" rid="fig1">Fig. 1c</xref> for the nominal directions in the room) and too far south when they were in the south end of the room. The magnitude of the errors can be seen in <xref ref-type="fig" rid="fig2">Fig. 2d</xref> - <xref ref-type="fig" rid="fig2">Fig. 2g</xref>. These plot data for 9 different box layouts, with the colors indicating the color of the box, the symbol type indicating the pointing zone and each symbol showing the mean pointing error for 20 participants. Data for the individual participants are shown in <xref ref-type="fig" rid="figS2">Figs. S2</xref> and <xref ref-type="fig" rid="figS3">S3</xref>.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><p><italic>(a-c) Examples of pointing directions of one participant tested in Experiment 1. The white diamond depicts the center of the start zone, (a) the amber diamond shows the center of pointing zone A, (b) the black diamond shows the center of pointing zone B (c) and the cyan diamond depicts the center of pointing zone C. The blue line shows the walking path from the start zone to the pointing zones. (d) Experiment 1: pointing errors for direct and indirect walking paths to the pointing zones compared. Each symbol shows mean pointing error for 20 participants and for a given configuration of target boxes. Colors indicate the boxes, symbol shape indicates pointing zone. (e) Experiment 1: Pointing errors from real world and virtual scene compared. (f) Experiment 2: Pointing errors compared for different initial facing directions. (g) Experiment 1 and 2: pointing errors for Zone C compared to pointing errors for Zone A and B</italic>.</p></caption>
<graphic xlink:href="390088_fig2.tif"/>
</fig>
<p>The pointing errors participants made were often large (e.g. &#x00B1;30&#x00B0;) and were even larger on the very first trial when (for this trial only) participants were asked to remember the box locations but not told what their task would be (<xref ref-type="fig" rid="figS5">Fig. S5</xref>). Pointing directions were highly consistent across conditions as can be seen from the pointing errors for different versions of the experiment shown in <xref ref-type="fig" rid="fig2">Fig. 2d, e and f</xref>. <xref ref-type="fig" rid="fig2">Fig. 2d</xref>, for example, shows that the pattern of pointing errors was very similar whether participants walked around a set of walls to arrive at the pointing location or walked direct (correlation coefficients 0.92, <italic>p</italic> &#x003C; 0.001, slope 0.98). There was also a high correlation between pointing errors when participants repeated exactly the same conditions but looked either north or south in order to view the image that told them which was the next pointing target. In these two versions of the experiment, participants&#x2019; rotation to point at the target was quite different, so the egocentric direction of the target was quite different at the moment they discovered which target they had to point to, but this had no systematic effect on the pointing errors (<xref ref-type="fig" rid="fig2">Fig. 2f</xref>, correlation coefficients 0.93, <italic>p</italic> &#x003C; 0.001, slope 0.91). Finally, there is a high correlation between pointing errors when the experiment was repeated in a real or a virtual environment (<xref ref-type="fig" rid="fig2">Fig. 2e</xref>, correlation coefficients 0.88, <italic>p</italic> &#x003C; 0.001), which supports the findings of Waller and colleagues[<xref ref-type="bibr" rid="c43">43</xref>], although the range of pointing errors was greater in the virtual room (slope 1.42). The first-trial data and the data plotted on the ordinate in <xref ref-type="fig" rid="fig2">Fig. 2e</xref> were collected in a real environment (<xref ref-type="fig" rid="fig1">Fig. 1a</xref>); all the rest of the data were collected in virtual reality. In all these results, the participants&#x2019; pointing directions were somewhere between the true direction of the target and a direction orthogonal to the obscuring wall (visible in <xref ref-type="fig" rid="fig1">Fig. 1a</xref> and shown as a vertical black line in <xref ref-type="fig" rid="fig2">Fig. 2a&#x2013;2c</xref>). This means that pointing errors when participants were in zones A and B (<xref ref-type="fig" rid="fig2">Fig. 2a&#x2013;2b</xref>) were reliably anticlockwise relative to the true target location (positive in <xref ref-type="fig" rid="fig2">Fig. 2g</xref>, <italic>M</italic> &#x003D; 12.1, <italic>SD</italic> &#x003D; 20.0682, t(949) &#x003D; 18.7, <italic>p</italic> &#x003C; 0.001 in a two-tailed t-test) while when the participant is in zone C (<xref ref-type="fig" rid="fig2">Fig. 2c</xref>) the errors were reliably clockwise (negative in <xref ref-type="fig" rid="fig2">Fig. 2g</xref>, <italic>M</italic> &#x003D; &#x2212;12.9, <italic>SD</italic> &#x003D; 14.7, t(479) &#x003D; &#x2212;19.2, <italic>p</italic> &#x003C; 0.001).</p>
<p>In order to investigate the origin of these zone-based biases, we set up an experiment in which everything remained constant other than the orientation of the obscuring wall. The scene layout is shown in <xref ref-type="fig" rid="fig3">Fig. 3</xref>. As the participant left the start zone, a wall appeared, obscuring the target boxes; it could appear in front of the observer, shown in <xref ref-type="fig" rid="fig3">Fig. 3a and 3d</xref>, or slanted as shown in <xref ref-type="fig" rid="fig3">Fig. 3e and 3h</xref>. The results are shown in <xref ref-type="fig" rid="fig3">Fig. 3b and 3c</xref>. Unlike the manipulations shown in <xref ref-type="fig" rid="fig2">Fig. 2a-2c</xref>, the orientation of the wall had a dramatic effect on participant pointing directions: the data are no longer close to the line of unity as they were in <xref ref-type="fig" rid="fig2">Fig. 2</xref> and have no significant correlation (pointing errors for the scenes shown in <xref ref-type="fig" rid="fig3">Fig. 3a</xref> and <xref ref-type="fig" rid="fig3">Fig. 3e</xref> are compared in <xref ref-type="fig" rid="fig3">Fig. 3b</xref> and have a correlation coefficient of 0.02, <italic>p</italic> &#x003D; 0.921; equivalently, for the scenes illustrated in <xref ref-type="fig" rid="fig3">Fig. 3d</xref> and <xref ref-type="fig" rid="fig3">Fig. 3h</xref>, pointing errors are compared in <xref ref-type="fig" rid="fig3">Fig. 3c</xref> and have a correlation coefficient of &#x2212;0.02, <italic>p</italic> &#x003D; 0.915). So, the obscuring wall changes pointing. The way it does so can be seen in <xref ref-type="fig" rid="fig3">Fig. 3f</xref> and <xref ref-type="fig" rid="fig3">Fig. 3g</xref> which show the distribution of differences between matched conditions (matched by participant, pointing zone, box layout and target box) where only the orientation of the obscuring wall changes. The means of these distributions are both significantly different from zero (one-tailed t-test, <italic>p</italic> &#x003C; 0.001) and shifted in opposite directions. This is what one would expect if participants tended to point somewhere between the true target direction and a direction orthogonal to the obscuring wall, just as they did in Experiment 1 (<xref ref-type="fig" rid="fig2">Fig. 2g</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><p><italic>(a,d,e,h) Plan view of a box layout tested in different configurations: The same box positions have been tested at the same pointing zones, but differed in the wall orientation. (b&#x002B;c) The tight correlations found in <xref ref-type="fig" rid="fig2">Fig. 2</xref> no longer apply here, despite same pointing zones and box layouts. Histogram of actual pointing differences between two wall orientations recorded at (f) zone C and at (g) zone E. Using a one-tailed</italic> t<italic>-test, the hypotheses could be rejected that the errors come from a distribution centered around 0 (for zone C: M</italic> &#x003D; 23.5, <italic>SD</italic> &#x003D; 26.2, <italic>t</italic>(319) &#x003D; 16.0, <italic>p</italic> &#x003C; 0.001; <italic>for zone E: M</italic> &#x003D; &#x2212;27.1, <italic>SD</italic> &#x003D; 23.8, <italic>t</italic>(319) &#x003D; &#x2212;20.3, <italic>p</italic> &#x003C; 0.001). <italic>Histogram of predicted pointing directions at (i) zone C (one-tailed</italic> t-<italic>test, hypothesis could not be rejected (M</italic> &#x003D; &#x2212;0.304, <italic>SD</italic> &#x003D; 25.4, <italic>t</italic>(319) &#x003D; &#x2212;0.214, <italic>p</italic> &#x003D; 0.831)) <italic>and (i) zone E (hypothesis could not be rejected (M</italic> &#x003D; &#x2212;1.8156, <italic>SD</italic> &#x003D; 21.9614, <italic>t</italic>(319) &#x003D; &#x2212;1.48, <italic>p</italic> &#x003D; 0.140)). <italic>For more details about the different configurations, see <xref ref-type="fig" rid="figS1">Fig. S1</xref> in Supplementary Information</italic>.</p></caption>
<graphic xlink:href="390088_fig3.tif"/>
</fig>
<sec id="s2a">
<label>2.1.</label>
<title>Models</title>
<p>The pointing task requires the observer to update an internal representation of their location and orientation. One possible model of our data is that a cumulative error in this updating process is the critical factor explaining the pointing biases. However, as we have already seen, <xref ref-type="fig" rid="fig2">Fig. 2d</xref> shows a very high correlation between biases obtained when participants walked directly (i.e. a short distance) or indirectly (a much longer distance) to a pointing zone which would not be expected if pointing biases were caused by cumulative path integration errors. This conclusion is supported by modeling. The model assumes that participants maintain an accurate representation of the target location but mis-estimate their stride length and the rotation of their body on each step. If this noise has zero mean, then the distribution of pointing errors from the final location is also centered on zero (<xref ref-type="fig" rid="figS11">Fig. S11</xref>) but if there is a systematic bias in the estimate of stride length or rotation at each step, then systematic biases in predicted location, and hence in pointing direction, can develop. <xref ref-type="fig" rid="fig4">Fig. 4c</xref> and <xref ref-type="fig" rid="fig4">Fig. 4d</xref> illustrate this effect for two actual trajectories to zone A (in orange) and zone C (cyan) and, as dotted lines, the trajectories resulting from the cumulative effect of a systematic mis-estimate at each step. These trajectories are the ones that best explain the participants&#x2019; pointing data (see Supplementary Information for details). The errors predicted by the model are larger for an indirect path to than for a direct path to zone C, as one would expect for a path integration model (<xref ref-type="fig" rid="figS10">Fig. S10</xref>) but, as we have seen, this is not the case for human observers (<xref ref-type="fig" rid="fig2">Fig. 2d</xref>) and a path integration model does not make any predictions about the effect of changing the orientation of the obscuring-wall which is one of the strongest determinants of pointing errors in our data (<xref ref-type="fig" rid="fig3">Fig. 3</xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><p><italic>Experiment 1 (a) Schematic explanation of the &#x2018;abathic distance&#x2019; model. (b) Predictions of target box locations under the abathic distance model (opaque rectangles show true box positions). (c) Schematic explanation of the dead-reckoning model. The opaque head shows the true translation; the faded head shows the model translation. The model gives a best fitting translation bias and orientation bias assuming these are the same on every step. (d) Solid lines show the walking paths of a participant, dotted lines show the misestimated walking path using the dead-reckoning model</italic>.</p></caption>
<graphic xlink:href="390088_fig4.tif"/>
</fig>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><p><italic>(a-d) &#x2018;Retrofit&#x2019; model using the estimated pointing directions of all participants to create a map of where they thought the boxes were, based on their pointing responses. (e) Plan view of the calculated positions of all boxes in each layout using the &#x2018;retrofit&#x2019; model. It seems as if all the boxes are shifted towards a plane parallel to the wall that is obscuring the boxes from where the observer is standing. (f) The projection model assumes that the projection plane is parallel to the wall, with a set distance</italic>.</p></caption>
<graphic xlink:href="390088_fig5.tif"/>
</fig>
<p>Another possible cause for systematic errors is that the observer builds a distorted 3D representation of space when they are at the start zone and uses it to guide their pointing. For example, many models of the representation of space built up from binocular disparity assume that there is a distorted mapping between true space and represented space, often with an &#x2018;abathic&#x2019; distance at which objects are judged to be at the correct distance and a compression of visual space towards this plane [<xref ref-type="bibr" rid="c44">44</xref>, <xref ref-type="bibr" rid="c45">45</xref>] as illustrated in <xref ref-type="fig" rid="fig4">Fig. 4a</xref>. Applying this 2-parameter model to our pointing data, the best-fitting abathic distance is actually about 5m <italic>behind</italic> the observer (see Supplementary Information, <xref ref-type="sec" rid="s5c7">Section 4.3.7</xref>). and, just like the path integration model, the abathic model cannot account for the effect of the slant of the obscuring-wall.</p>
<p>The abathic distance model is only one case of a model in which the observer uses a distorted 3D model of the scene as the basis of his or her pointing responses. An extreme version of this type of model is to allowing the location of the target boxes to vary freely in order to find a scene layout that maximizes the likelihood of the pointing data, i.e. &#x2018;retrofitting&#x2019; the scene to match the data. The result of doing this for experiment 1 is shown in <xref ref-type="fig" rid="fig5">Fig. 5</xref>. We used the pointing directions of all 20 participants from three pointing zones per target box in each condition to calculate the most likely location of the box that would explain their pointing responses. <xref ref-type="fig" rid="fig5">Fig. 5a</xref> shows the result of this calculation for the blue box in one condition. It is clear that the derived box location is far from the true location. <xref ref-type="fig" rid="fig5">Figs. 5b, 5c, and 5d</xref> show the same for the pink, red and yellow target boxes respectively (see <xref ref-type="sec" rid="s5c1">Section 4.3.1</xref> for details and <xref ref-type="fig" rid="figS8">Fig. S8</xref> which shows the same retrofit process applied to the results of experiment 4).</p>
<p><xref ref-type="fig" rid="fig5">Fig. 5e</xref> marks the location of all the peaks of the likelihood distributions illustrated in <xref ref-type="fig" rid="fig5">Fig. 5a-d</xref> and all other conditions in experiment 1. These points tend to cluster around a plane that is parallel to the plane of the obscuring wall whereas the true locations of the target boxes (shown in translucent colors in <xref ref-type="fig" rid="fig5">Fig. 5f</xref>) are nothing like this. It seems that participants tend to point in a direction that is more orthogonal to the obscuring wall than the true target direction but, in addition, the inferred target locations all have a similar depth relative to the obscuring walls, as if the depth structure of the scene has been &#x2018;squashed&#x2019;. In reality, the pink and blue boxes were very close to the obscuring wall in the center of the room while red and yellow boxes were more distant. Yet, according to participants&#x2019; pointing data, the <italic>apparent</italic> location of the targets were all at a similar distance relative to the wall. So, as a <italic>post hoc</italic> &#x2018;model&#x2019; of this apparent distortion of the scene structure, we assume that the observer points to notional targets that are on a plane that is (i) parallel to the obscuring wall and (ii) at a distance determined by the best fit to the points shown in <xref ref-type="fig" rid="fig5">Fig. 5e</xref>. For each trial, we project the true box location onto this plane orthographically, as shown in <xref ref-type="fig" rid="fig5">Fig. 5f</xref> and then use this notional target location as a predictor of pointing from all zones. In this case, the distance of the projection plane is 1.77<italic>m</italic> from the central obscuring wall. We fix this parameter and use is to determine the distance of the &#x2018;model&#x2019; projection plane in other experiments.</p>
<p>We can now compare the best distorted-world model (&#x2018;retrofitted&#x2019; to the data) with this projection-plane model. It might seem counter-intuitive that the &#x2018;retrofit&#x2019; model is not, by definition, the best model in every case since it is fitted to the data but of course, that model is constrained to use the same scene layout independent of pointing zone and wall orientation while the projected plane model is not. <xref ref-type="fig" rid="fig7">Fig. 7</xref> shows the predicted pointing error according to the &#x2018;retrofit&#x2019; model for the data from experiments 1, 2, 3 and 4. Specifically, the most likely layout of the boxes is chosen to explain the pointing data per experiment, i.e. the scene configuration per box layout is fixed per box layout per experiment but allowed to vary between experiments. <xref ref-type="fig" rid="fig8">Fig. 8</xref> shows the same but now using the &#x2018;projected plane&#x2019; model. It is immediately clear that the slopes relating predicted and actual pointing error are steeper in <xref ref-type="fig" rid="fig8">Fig. 8</xref> than in <xref ref-type="fig" rid="fig7">Fig. 7</xref> (correlations 0.86, 0.73 and 0.81 in <xref ref-type="fig" rid="fig8">Fig. 8b, c and d</xref> for the &#x2018;projected plane&#x2019; model and 0.54, 0.32 and 0.40 in panels <xref ref-type="fig" rid="fig7">Fig. 7b, c and d</xref> for the &#x2018;retrofit&#x2019; model). Note that it is not sensible to compare the models for Experiment 1 (panel <xref ref-type="fig" rid="fig7">Fig. 7a</xref> and <xref ref-type="fig" rid="fig8">Fig. 8a</xref>) because the &#x2018;projected plane&#x2019; model was generated from the data collected in Experiment 1. As expected given the better correlations, there is a reduction in RMS error due to the projected plane model for Experiment 2 and 4 relative to the retrofit model and, when the number of free parameters in each model is taken into account using Bayesian Information Criterion (BIC) the projected plane model is superior in all three experiments (see <xref ref-type="table" rid="tblS2">Table S2</xref>).</p>
<p>Finally, returning to <xref ref-type="fig" rid="fig3">Fig. 3</xref>, we can see how the projected plane model explains the bias that is caused by the slanted or East-West wall in that experiment. In <xref ref-type="fig" rid="fig3">Fig. 3j and 3i</xref> we have taken paired trials (i.e. with everything else the same except for the wall orientation) and plotted the distribution of pointing &#x2018;errors&#x2019; caused by altering the wall orientation where &#x2018;errors&#x2019; are now defined relative to the <italic>predicted</italic> direction of the target. Unlike <xref ref-type="fig" rid="fig3">Fig. 3g and 3f</xref>, where pointing errors were defined relative to the ground truth target direction and the distributions of errors were systematically biased, in both <xref ref-type="fig" rid="fig3">Fig. 3j and 3i</xref> zero now indicates the prediction of the projection-plane model and now the mean of the distribution of errors is no longer significantly different from zero.</p>
</sec>
</sec>
<sec id="s3">
<label>3.</label>
<title>Discussion</title>
<p>For a spatial representation to be generated in one location and used in another, there must be some transformation of the representation, or its read-out, that takes account of the observer&#x2019;s movement. The experiments described here show that in humans this process is highly inaccurate and consistently so.</p>
<p>The data appear to rule out several important hypotheses. Crucially, any hypothesis that seeks to explain the errors in terms of a distorted internal model of the scene at the initial encoding stage will fail to capture the strong influence that we have shown of the obscuring wall (see <xref ref-type="fig" rid="fig3">Fig. 3</xref>). We examined this hypothesis in relation to the standard models of a compression of visual space around an &#x2018;abathic&#x2019; distance [<xref ref-type="bibr" rid="c44">44</xref>, <xref ref-type="bibr" rid="c45">45</xref>] (details in <xref ref-type="sec" rid="s5c7">Section 4.3.7</xref> and <xref ref-type="fig" rid="figS12">Fig. S12</xref>) but the point is more general. We have allowed <italic>any</italic> type of distortion of the scene that the observer sees from the starting zone to explain the data, provided that the <italic>same</italic> distortion is used to explain a participant&#x2019;s pointing direction from all pointing zones and any wall orientation. The fact that this still provides a worse fit to the data than our projection model (see <xref ref-type="fig" rid="fig7">Fig. 7</xref>, <xref ref-type="fig" rid="fig8">8</xref> and <xref ref-type="table" rid="tblS2">Table S2</xref>) is strong evidence against all models of this type.</p>
<p>The data also rule out any of the hypotheses that we have examined based on noisy path integration. One strong piece of evidence in relation to such hypotheses is the high correlation between the magnitude of errors participants make when they walk to a pointing location either via a long or via a short route (<xref ref-type="fig" rid="fig2">Fig. 2d</xref> and <xref ref-type="fig" rid="figS2">Fig. S2a</xref>). These data suggest that it is the location of the pointing zone, not the route to it, that matters. Also, modeling the data from all 4 experiments, a noisy integration model does a poor job of predicting participant data compared to the other models we tested (<xref ref-type="table" rid="tblS1">Table S1</xref>). Instead, our data suggest that people point to a memory of the target boxes that is nothing like where they really are. Our best &#x2018;model&#x2019; of the data is that participants&#x2019; pointing responses reflect a process that behaves as if the target boxes were further away than they really are and that their apparent locations are compressed towards a plane that is parallel to the wall that obscures the target boxes from the view of the participant (see <xref ref-type="fig" rid="fig6">Fig. 6</xref>).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><p><italic>Schematic explanation of projection model using different wall orientations of Experiment 4</italic></p></caption>
<graphic xlink:href="390088_fig6.tif"/>
</fig>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7:</label>
<caption><p><italic>Retrofit Model Experiment 1, 2, 4: Using the retrofit model and the data for Experiment 1, 2, and 4 combined, we can now predict the box positions, see <xref ref-type="fig" rid="figS7">Fig. S7</xref> in the Supplementary material. (a) Experiment 1, R</italic> &#x003D; 0.52 <italic>and p</italic> &#x003C; 0.001, <italic>RMSE &#x003D; 24.1&#x00B0;(b) Experiment 2, R</italic> &#x003D; 0.54 <italic>and p</italic> &#x003C; 0.001, <italic>RMSE &#x003D; 29.3&#x00B0;(c) for the predicted box positions, R</italic> &#x003D; 0.32 <italic>and p</italic> &#x003D; 0.0014, <italic>RMSE &#x003D; 34.06&#x00B0;(d) Experiment 4, R</italic> &#x003D; 0.40 <italic>and p</italic> &#x003C; 0.001, <italic>RMSE &#x003D; 25.7&#x00B0;(d) Experiment 3: Retrofit model calculated using all 6 zones to predict box locations, see <xref ref-type="fig" rid="figS8">Fig. S8</xref></italic></p></caption>
<graphic xlink:href="390088_fig7.tif"/>
</fig>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8:</label>
<caption><p><italic>Projection model: (a) Experiment 1, R</italic> &#x003D; 0.86 <italic>and p</italic> &#x003C; 0.001 <italic>(b) Experiment 2, R</italic> &#x003D; 0.86 <italic>and p</italic> &#x003C; 0.001 <italic>(c) Experiment 4, R</italic> &#x003D; 0.73 <italic>and p</italic> &#x003C; 0.001 <italic>(d) Experiment 3, R</italic> &#x003D; 0.81 <italic>and p</italic> &#x003C; 0.001</p></caption>
<graphic xlink:href="390088_fig8.tif"/>
</fig>
<p>One paper using a similar paradigm to ours also shows consistent pointing biases [<xref ref-type="bibr" rid="c26">26</xref>]. When the pointing zone (in their experiment, called an &#x2018;indirect waypoint&#x2019;) was displaced from the location at which the scene layout was learned, pointing was shifted consistently in the direction of the waypoint. The participant was blindfolded during the test phase and guided to the waypoint so there was no equivalent of our wall and only two waypoints were tested so it is hard to make a direct comparison with our data. Nevertheless, their data suggest that the location of the pointing zone with respect to the start zone has a systematic biasing effect on the direction that participants point to learned targets.</p>
<p>It has often been shown that the observer&#x2019;s orientation can have a significant effect on pointing performance, for example when the orientation of the participant in the test phase differs from their orientation during learning of the scene layout whether the test phase requires real walking [<xref ref-type="bibr" rid="c46">46</xref>] or imaginary movement [<xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c47">47</xref>]. Meilinger et al. [<xref ref-type="bibr" rid="c48">48</xref>] investigated the effect of adding walls to an environment and shown they have a significant effect. However, they do not examine the biasing effects of moving to different pointing locations nor can they be used to deduce any systematic biases in pointing directions since they report &#x2018;absolute errors&#x2019; in pointing, as do many other papers in the field, a measure that conflates variable errors and systematic biases [<xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c50">50</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c51">51</xref>, <xref ref-type="bibr" rid="c19">19</xref>]. R&#x00F6;hrich et al. [<xref ref-type="bibr" rid="c32">32</xref>] showed that a participant&#x2019;s location (in a town) with respect to target (a well-known market square) was important in determining the orientation at which they drew a plan-view sketch of the square but they did not predict biases in pointing. The fact that we found biases that were large and consistent, whether participants began a trial facing in one direction or the opposite direction (<xref ref-type="fig" rid="fig2">Fig. 2f</xref>) is the most relevant evidence from this experiment that egocentric factors did not play a part in causing the biases [<xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c39">39</xref>].</p>
<p>The fact that the best 3D reconstruction model does not do well in explaining our data raises the question of what participants do instead. The &#x2018;projection plane&#x2019; model is essentially a redescription of the data in one experiment, rather than an explanation, albeit one that then extends successfully to other situations. Whatever heuristic the visual system uses, it seems to ignore critical aspects of geometry, such as relative depths, so that objects are treated in a more simplistic way (equivalent to assuming that the target objects all lie on a plane) than would be the case if the internal model behaved like a real 3D model. Recent findings using &#x2018;Generative Query Networks&#x2019; [<xref ref-type="bibr" rid="c8">8</xref>] suggest that our understanding of the process of &#x2018;imagining&#x2019; novel views of an observed scene, without using a 3D reconstruction, is likely to improve rapidly.</p>
<p>Finally, it is worth considering what effect such large biases might have in ordinary life. The most relevant data from our experiments in relation to this question are, arguably, the pointing biases that we recorded from the very first trial for each participant. We made sure that for this trial that the participant was unaware of the task they were about to be asked to do. Biases in this case were even larger than for the rest of the data (<xref ref-type="fig" rid="figS5">Fig. S5</xref>). If these data reflect performance in daily living one might ask why we so rarely encounter catastrophic consequences but, in fact, the task is an unusual one and under most circumstances it is likely that visual landmarks will help to refine direction judgments <italic>en route</italic> to a target.</p>
</sec>
<sec id="s4">
<title>Conclusion</title>
<p>Our conclusions are twofold. First, although human observers <italic>can</italic> point to remembered objects, and hence must update some form of internal representation while the objects are out of view, we have shown that they make highly repeatable errors when doing so. Second, the best explanation of our data is not consistent with a single stable 3D model (even a distorted one) of the target locations. This means that whatever the rules are for spatial updating in human observers, they must involve more than the structure of the remembered scene and geometric integration (even with errors) of the path taken by the observer.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>This research was funded by a <italic>Microsoft Research</italic> PhD Scholarship to JV and by <italic>EPSRC</italic> grant EP/K011766/1 and <italic>Dstl/EPSRC</italic> grant EP/N019423/1 to AG. We thank Bruce Cumming for discussions that led to Experiment 4 and Lyndsey Pickup for help with modeling.</p>
</ack>
<sec>
<title>Author contributions</title>
<p>Conceived project: A.G. and A.W.F.; contributed ideas to experiment: A.G., J.V. and A.W.F.; development of testing platform and environments: J.V.; performed experiments and analysis: J.V.; contributed ideas to modeling: A.W.F., A.G. and J.V.; wrote paper: A.G. and J.V.</p>
</sec>
<sec>
<title>Competing interests</title>
<p>The authors declare no competing interests.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><surname>Cartwright</surname>, <given-names>B.</given-names></string-name> &#x0026; <string-name><surname>Collett</surname>, <given-names>T.</given-names></string-name> <article-title>Landmark Learning in Bees - Experiments and Models</article-title>. <source>Journal of Comparative Physiology</source> <volume>151</volume>, <fpage>521</fpage>&#x2013;<lpage>543</lpage> (<year>1983</year>). URL <ext-link ext-link-type="uri" xlink:href="http://scholar.google.com/scholar?hl&#x003D;en&#x0026;btnG&#x003D;Search&#x0026;q&#x003D;intitle:Landmark&#x002B;Learning&#x002B;in&#x002B;Bees&#x002B;Experiments&#x002B;and&#x002B;Models#5">http://scholar.google.com/scholar?hl&#x003D;en&#x0026;btnG&#x003D;Search&#x0026;q&#x003D;intitle:Landmark&#x002B;Learning&#x002B;in&#x002B;Bees&#x002B;Experiments&#x002B;and&#x002B;Models#5</ext-link>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><string-name><surname>Wehner</surname>, <given-names>R.</given-names></string-name> <article-title>Desert ant navigation: how miniature brains solve complex tasks</article-title>. <source>Journal of comparative physiology. A, Neuroethology, sensory, neural, and behavioral physiology</source> <volume>189</volume>, <fpage>579</fpage>&#x2013;<lpage>88</lpage> (<year>2003</year>). URL <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/12879352">http://www.ncbi.nlm.nih.gov/pubmed/12879352</ext-link>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><surname>Collett</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>Collett</surname>, <given-names>T. S.</given-names></string-name> <article-title>Local and global navigational coordinate systems in desert ants</article-title>. <source>The Journal of Experimental Biology</source> <volume>212</volume>, <fpage>901</fpage>&#x2013;<lpage>905</lpage> (<year>2009</year>). URL <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/19282486">http://www.ncbi.nlm.nih.gov/pubmed/19282486</ext-link>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><string-name><surname>Gillner</surname>, <given-names>S.</given-names></string-name> &#x0026; <string-name><surname>Mallot</surname>, <given-names>H. A.</given-names></string-name> <article-title>Navigation and acquisition of spatial knowledge in a virtual maze</article-title>. <source>Journal of Cognitive Neuroscience</source> <volume>10</volume>, <fpage>445</fpage>&#x2013;<lpage>463</lpage> (<year>1998</year>). URL <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/9712675">http://www.ncbi.nlm.nih.gov/pubmed/9712675</ext-link>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><string-name><surname>Gootjes-Dreesbach</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Pickup</surname>, <given-names>L. C.</given-names></string-name>, <string-name><surname>Fitzgibbon</surname>, <given-names>A. W.</given-names></string-name> &#x0026; <string-name><surname>Glennerster</surname>, <given-names>A.</given-names></string-name> <article-title>Comparison of view-based and reconstruction-based models of human navigational strategy</article-title>. <source>Journal of Vision</source> <volume>17</volume>, <fpage>11</fpage>&#x2013;<lpage>11</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="other"><string-name><surname>Mallot</surname>, <given-names>H. A.</given-names></string-name> &#x0026; <string-name><surname>Lancier</surname>, <given-names>S.</given-names></string-name> <article-title>Place recognition from distant landmarks: human performance and maximum likelihood model</article-title>. <source>Biological Cybernetics</source> <fpage>1</fpage>&#x2013;<lpage>13</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="confproc"><string-name><surname>Zhu</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal> <article-title>Target-driven visual navigation in indoor scenes using deep reinforcement learning</article-title>. In <conf-name>Robotics and Automation (ICRA), 2017 IEEE International Conference on</conf-name>, <fpage>3357</fpage>&#x2013;<lpage>3364</lpage> (<conf-loc>IEEE</conf-loc>, <year>2017</year>).</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><string-name><surname>Eslami</surname>, <given-names>S. A.</given-names></string-name> <etal>et al.</etal> <article-title>Neural scene representation and rendering</article-title>. <source>Science</source> <volume>360</volume>, <fpage>1204</fpage>&#x2013;<lpage>1210</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="other"><string-name><surname>Andersen</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>L. H.</given-names></string-name>, <string-name><surname>Bradley</surname>, <given-names>D. C.</given-names></string-name> &#x0026; <string-name><surname>Xing</surname>, <given-names>J.</given-names></string-name> <article-title>Multi-modal representation of space in the posterior parietal cortex and its use in planning movements</article-title> <volume>20</volume>, <fpage>303</fpage>&#x2013;<lpage>330</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><string-name><surname>Snyder</surname>, <given-names>L. H.</given-names></string-name>, <string-name><surname>Grieve</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Brotchie</surname>, <given-names>P.</given-names></string-name> &#x0026; <string-name><surname>Andersen</surname>, <given-names>R. A.</given-names></string-name> <article-title>Separate body- and world-referenced representations of visual space in parietal cortex</article-title>. <source>Nature</source> <volume>394</volume>, <fpage>887</fpage>&#x2013;<lpage>891</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><string-name><surname>Hafting</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Fyhn</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Molden</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Moser</surname>, <given-names>M.-B.</given-names></string-name> &#x0026; <string-name><surname>Moser</surname>, <given-names>E. I.</given-names></string-name> <article-title>Microstructure of a spatial map in the entorhinal cortex</article-title>. <source>Nature</source> <volume>436</volume>, <fpage>801</fpage>&#x2013;<lpage>6</lpage> (<year>2005</year>). URL <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/15965463">http://www.ncbi.nlm.nih.gov/pubmed/15965463</ext-link>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><string-name><surname>Killian</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Jutras</surname>, <given-names>M. J.</given-names></string-name> &#x0026; <string-name><surname>Buffalo</surname>, <given-names>E. A.</given-names></string-name> <article-title>A map of visual space in the primate entorhinal cortex</article-title>. <source>Nature</source> <volume>491</volume>, <fpage>761</fpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><string-name><surname>O&#x2019;Keefe</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Dostrovsky</surname>, <given-names>J.</given-names></string-name> <article-title>The hippocampus as a spatial map. Preliminary evidence from unit activity in the freely-moving rat</article-title>. <source>Brain Research</source> <volume>34</volume>, <fpage>171</fpage>&#x2013;<lpage>175</lpage> (<year>1971</year>).</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><surname>Moser</surname>, <given-names>E. I.</given-names></string-name>, <string-name><surname>Kropff</surname>, <given-names>E.</given-names></string-name> &#x0026; <string-name><surname>Moser</surname>, <given-names>M.-B.</given-names></string-name> <article-title>Place cells, grid cells, and the brain&#x2019;s spatial representation system</article-title>. <source>Annual review of neuroscience</source> <volume>31</volume> (<year>2008</year>).</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="other"><string-name><surname>Mirowski</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal> <article-title>Learning to navigate in complex environments</article-title>. <source>arXiv preprint arXiv:1611.03673</source> (<year>2016</year>).</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><string-name><surname>Simons</surname>, <given-names>D. J.</given-names></string-name> &#x0026; <string-name><surname>Wang</surname>, <given-names>R. F.</given-names></string-name> <article-title>Perceiving real-world viewpoint changes</article-title>. <source>Psychological Science</source> <volume>9</volume>, <fpage>315</fpage>&#x2013;<lpage>320</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><string-name><surname>Mou</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>McNamara</surname>, <given-names>T. P.</given-names></string-name>, <string-name><surname>Valiquette</surname>, <given-names>C. M.</given-names></string-name> &#x0026; <string-name><surname>Rump</surname>, <given-names>B.</given-names></string-name> <article-title>Allocentric and egocentric updating of spatial memories</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>30</volume>, <fpage>142</fpage>&#x2013;<lpage>57</lpage> (<year>2004</year>). URL <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/14736303">http://www.ncbi.nlm.nih.gov/pubmed/14736303</ext-link>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><string-name><surname>Byrne</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Becker</surname>, <given-names>S.</given-names></string-name> &#x0026; <string-name><surname>Burgess</surname>, <given-names>N.</given-names></string-name> <article-title>Remembering the past and imagining the future: a neural model of spatial memory and imagery</article-title>. <source>Psychological review</source> <volume>114</volume>, <fpage>340</fpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><string-name><surname>Thorndyke</surname>, <given-names>P. W.</given-names></string-name> &#x0026; <string-name><surname>Hayes-Roth</surname>, <given-names>B.</given-names></string-name> <article-title>Differences in spatial knowledge acquired and navigation from maps</article-title>. <source>Cognitive Psychology</source> <volume>589</volume>, <fpage>560</fpage>&#x2013;<lpage>589</lpage> (<year>1982</year>).</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><string-name><surname>Rieser</surname>, <given-names>J. J.</given-names></string-name> <article-title>Access to knowledge of spatial structure at novel points of observation</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>15</volume>, <fpage>1157</fpage>&#x2013;<lpage>1165</lpage> (<year>1989</year>). URL <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/2530309">http://www.ncbi.nlm.nih.gov/pubmed/2530309</ext-link>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><string-name><surname>Presson</surname>, <given-names>C. C.</given-names></string-name> &#x0026; <string-name><surname>Montello</surname>, <given-names>D. R.</given-names></string-name> <article-title>Updating after rotational and translational body movements: Coordinate structure of perspective space</article-title>. <source>Perception</source> <volume>23</volume>, <fpage>1447</fpage>&#x2013;<lpage>1455</lpage> (<year>1994</year>).</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><string-name><surname>Shelton</surname>, <given-names>A. L.</given-names></string-name> &#x0026; <string-name><surname>McNamara</surname>, <given-names>T. P. M. C.</given-names></string-name> <article-title>Multiple views of spatial memory</article-title>. <source>Psychonomic Bulletin &#x0026; Review</source> <volume>4</volume>, <fpage>102</fpage>&#x2013;<lpage>106</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><string-name><surname>Wraga</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Creem</surname>, <given-names>S. H.</given-names></string-name> &#x0026; <string-name><surname>Proffitt</surname>, <given-names>D. R.</given-names></string-name> <article-title>Updating displays after imagined object and viewer rotations</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>26</volume>, <fpage>151</fpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><string-name><surname>May</surname>, <given-names>M.</given-names></string-name> <article-title>Imaginal perspective switches in remembered environments: Transformation versus interference accounts</article-title>. <source>Cognitive Psychology</source> <volume>48</volume>, <fpage>163</fpage>&#x2013;<lpage>206</lpage> (<year>2004</year>). URL <ext-link ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S0010028503001270">http://linkinghub.elsevier.com/retrieve/pii/S0010028503001270</ext-link>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><string-name><surname>Rieser</surname>, <given-names>J. J.</given-names></string-name> &#x0026; <string-name><surname>Rider</surname>, <given-names>E. A.</given-names></string-name> <article-title>Young children&#x2019;s spatial orientation with respect to multiple targets when walking without vision</article-title>. <source>Developmental Psychology</source> <volume>27</volume>, <fpage>97</fpage> (<year>1991</year>).</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><string-name><surname>Klatzky</surname>, <given-names>R. L.</given-names></string-name>, <string-name><surname>Lippa</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Loomis</surname>, <given-names>J. M.</given-names></string-name> &#x0026; <string-name><surname>Golledge</surname>, <given-names>R. G.</given-names></string-name> <article-title>Encoding, learning, and spatial updating of multiple object locations specified by 3-D sound, spatial language, and vision</article-title>. <source>Experimental Brain Research. Experimentelle Hirnforschung. Exp&#x00E9;rimentation c&#x00E9;r&#x00E9;brale</source> <volume>149</volume>, <fpage>48</fpage>&#x2013;<lpage>61</lpage> (<year>2003</year>). URL <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/12592503">http://www.ncbi.nlm.nih.gov/pubmed/12592503</ext-link>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><string-name><surname>Medendorp</surname>, <given-names>W. P.</given-names></string-name> <article-title>Spatial constancy mechanisms in motor control</article-title>. <source>Philosophical Transactions of the Royal Society of London B: Biological Sciences</source> <volume>366</volume>, <fpage>476</fpage>&#x2013;<lpage>491</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><string-name><surname>Bennett</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Loomis</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Klatzky</surname>, <given-names>R. L.</given-names></string-name> &#x0026; <string-name><surname>Giudice</surname>, <given-names>N. A.</given-names></string-name> <article-title>Spatial updating of multiple targets: Comparison of younger and older adults</article-title>. <source>Memory &#x0026; cognition</source> <volume>45</volume>, <fpage>1240</fpage>&#x2013;<lpage>1251</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><string-name><surname>Loomis</surname>, <given-names>J. M.</given-names></string-name> <etal>et al.</etal> <article-title>Nonvisual navigation by blind and sighted: assessment of path integration ability</article-title>. <source>Journal of Experimental Psychology: General</source> <volume>122</volume>, <fpage>73</fpage> (<year>1993</year>).</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><string-name><surname>Foo</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Warren</surname>, <given-names>W. H.</given-names></string-name>, <string-name><surname>Duchon</surname>, <given-names>A.</given-names></string-name> &#x0026; <string-name><surname>Tarr</surname>, <given-names>M. J.</given-names></string-name> <article-title>Do humans integrate routes into a cognitive map? map-versus landmark-based navigation of novel shortcuts</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>31</volume>, <fpage>195</fpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><string-name><surname>Kozlowski</surname>, <given-names>L. T.</given-names></string-name> &#x0026; <string-name><surname>Bryant</surname>, <given-names>K. J.</given-names></string-name> <article-title>Sense of direction, spatial orientation, and cognitive maps</article-title>. <source>Journal of Experimental Psychology: Human, Perception, and Performance</source> <volume>3</volume>, <fpage>590</fpage>&#x2013;<lpage>598</lpage> (<year>1977</year>).</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><string-name><surname>R&#x00F6;hrich</surname>, <given-names>W. G.</given-names></string-name>, <string-name><surname>Hardiess</surname>, <given-names>G.</given-names></string-name> &#x0026; <string-name><surname>Mallot</surname>, <given-names>H. A.</given-names></string-name> <article-title>View-based organization and interplay of spatial working and long-term memories</article-title>. <source>PloS one</source> <volume>9</volume> (<year>2014</year>).</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>R. F.</given-names></string-name> &#x0026; <string-name><surname>Simons</surname>, <given-names>D. J.</given-names></string-name> <article-title>Active and passive scene recognition across views</article-title>. <source>Cognition</source> <volume>70</volume>, <fpage>191</fpage>&#x2013;<lpage>210</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><string-name><surname>Burgess</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Spiers</surname>, <given-names>H. J.</given-names></string-name> &#x0026; <string-name><surname>Paleologou</surname>, <given-names>E.</given-names></string-name> <article-title>Orientational manoeuvres in the dark: dissociating allocentric and egocentric influences on spatial memory</article-title>. <source>Cognition</source> <volume>94</volume>, <fpage>149</fpage>&#x2013;<lpage>166</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><string-name><surname>Newell</surname>, <given-names>F. N.</given-names></string-name>, <string-name><surname>Woods</surname>, <given-names>A. T.</given-names></string-name>, <string-name><surname>Mernagh</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>B&#x00FC;lthoff</surname>, <given-names>H. H.</given-names></string-name> <article-title>Visual, haptic and crossmodal recognition of scenes</article-title>. <source>Experimental Brain Research</source> <volume>161</volume>, <fpage>233</fpage>&#x2013;<lpage>242</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><string-name><surname>Mou</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>McNamara</surname>, <given-names>T. P.</given-names></string-name>, <string-name><surname>Rump</surname>, <given-names>B.</given-names></string-name> &#x0026; <string-name><surname>Xiao</surname>, <given-names>C.</given-names></string-name> <article-title>Roles of egocentric and allocentric spatial representations in locomotion and reorientation</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>32</volume>, <fpage>1274</fpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="book"><string-name><surname>Klatzky</surname>, <given-names>R. L.</given-names></string-name> <chapter-title>Allocentric and egocentric spatial representations: Definitions, distinctions, and interconnections</chapter-title>. In <source>Spatial cognition</source>, <fpage>1</fpage>&#x2013;<lpage>17</lpage> (<publisher-name>Springer</publisher-name>, <year>1998</year>).</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>R. F.</given-names></string-name> &#x0026; <string-name><surname>Spelke</surname>, <given-names>E. S.</given-names></string-name> <article-title>Updating egocentric representations in human navigation</article-title>. <source>Cognition</source> <volume>77</volume>, <fpage>215</fpage>&#x2013;<lpage>250</lpage> (<year>2000</year>). URL <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/11018510">http://www.ncbi.nlm.nih.gov/pubmed/11018510</ext-link>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="other"><string-name><surname>Burgess</surname>, <given-names>N.</given-names></string-name> <article-title>Spatial memory: How egocentric and allocentric combine</article-title> <volume>10</volume>, <fpage>551</fpage>&#x2013;<lpage>557</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="confproc"><string-name><surname>Meilinger</surname>, <given-names>T.</given-names></string-name> &#x0026; <string-name><surname>Vosgerau</surname>, <given-names>G.</given-names></string-name> <article-title>Putting egocentric and allocentric into perspective</article-title>. In <conf-name>International Conference on Spatial Cognition</conf-name>, <fpage>207</fpage>&#x2013;<lpage>221</lpage> (<conf-loc>Springer</conf-loc>, <year>2010</year>).</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><string-name><surname>Fukusima</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Loomis</surname>, <given-names>J. M.</given-names></string-name> &#x0026; <string-name><surname>Da Silva</surname>, <given-names>J. A.</given-names></string-name> <article-title>Visual perception of egocentric distance as assessed by triangulation</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source> <volume>23</volume>, <fpage>86</fpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="other"><string-name><surname>Wang</surname>, <given-names>R. F.</given-names></string-name> &#x0026; <string-name><surname>Spelke</surname>, <given-names>E. S.</given-names></string-name> <article-title>Human spatial representation: insights from animals</article-title> <volume>6</volume>, <fpage>376</fpage>&#x2013;<lpage>382</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><string-name><surname>Waller</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Beall</surname>, <given-names>A. C.</given-names></string-name> &#x0026; <string-name><surname>Loomis</surname>, <given-names>J. M.</given-names></string-name> <article-title>Using virtual environments to assess directional knowledge</article-title>. <source>Journal of Environmental Psychology</source> <volume>24</volume>, <fpage>105</fpage>&#x2013;<lpage>116</lpage> (<year>2004</year>). URL <ext-link ext-link-type="uri" xlink:href="http://linkinghub.elsevier.com/retrieve/pii/S0272494403000513">http://linkinghub.elsevier.com/retrieve/pii/S0272494403000513</ext-link>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><string-name><surname>Johnston</surname>, <given-names>E. B.</given-names></string-name> <article-title>Systematic distortions of shape from steoreposis</article-title>. <source>Vision Research</source> <volume>31</volume>, <fpage>1351</fpage>&#x2013;<lpage>1360</lpage> (<year>1991</year>).</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="other"><string-name><surname>Foley</surname>, <given-names>J. M.</given-names></string-name> <article-title>Binocular distance perception</article-title> <volume>87</volume>, <fpage>411</fpage>&#x2013;<lpage>433</lpage> (<year>1980</year>).</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><string-name><surname>Meilinger</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Riecke</surname>, <given-names>B. E.</given-names></string-name> &#x0026; <string-name><surname>B&#x00FC;lthoff</surname>, <given-names>H. H.</given-names></string-name> <article-title>Local and global reference frames for environmental spaces</article-title>. <source>The Quarterly Journal of Experimental Psychology</source> <fpage>1</fpage>&#x2013;<lpage>28</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><string-name><surname>Waller</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Montello</surname>, <given-names>D. R.</given-names></string-name>, <string-name><surname>Richardson</surname>, <given-names>A. E.</given-names></string-name> &#x0026; <string-name><surname>Hegarty</surname>, <given-names>M.</given-names></string-name> <article-title>Orientation specificity and spatial updating of memories for layouts</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>28</volume>, <fpage>1051</fpage>&#x2013;<lpage>1063</lpage> (<year>2002</year>). URL <ext-link ext-link-type="uri" xlink:href="http://doi.apa.org/getdoi.cfm?doi&#x003D;10.1037/0278-7393.28.6.1051">http://doi.apa.org/getdoi.cfm?doi&#x003D;10.1037/0278-7393.28.6.1051</ext-link>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><string-name><surname>Meilinger</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Strickrodt</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>B&#x00FC;lthoff</surname>, <given-names>H. H.</given-names></string-name> <article-title>Qualitative differences in memory for vista and environmental spaces are caused by opaque borders, not movement or successive presentation</article-title>. <source>Cognition</source> <volume>155</volume>, <fpage>77</fpage>&#x2013;<lpage>95</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><string-name><surname>Farrell</surname>, <given-names>M. J.</given-names></string-name> &#x0026; <string-name><surname>Robertson</surname>, <given-names>I. H.</given-names></string-name> <article-title>Mental rotation and automatic updating of body-centered spatial relationships</article-title>. <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source> <volume>24</volume>, <fpage>227</fpage>&#x2013;<lpage>233</lpage> (<year>1998</year>). URL <ext-link ext-link-type="uri" xlink:href="http://doi.apa.org/getdoi.cfm?doi&#x003D;10.1037/0278-7393.24.1.227">http://doi.apa.org/getdoi.cfm?doi&#x003D;10.1037/0278-7393.24.1.227</ext-link>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>R. F.</given-names></string-name> <article-title>Between reality and imagination: When is spatial updating automatic?</article-title> <source>Perception &#x0026; Psychophysics</source> <volume>66</volume>, <fpage>68</fpage>&#x2013;<lpage>76</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><string-name><surname>Shelton</surname>, <given-names>A. L.</given-names></string-name> &#x0026; <string-name><surname>McNamara</surname>, <given-names>T. P.</given-names></string-name> <article-title>Systems of spatial reference in human memory</article-title>. <source>Cognitive Psychology</source> <volume>43</volume>, <fpage>274</fpage>&#x2013;<lpage>310</lpage> (<year>2001</year>). URL <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/11741344">http://www.ncbi.nlm.nih.gov/pubmed/11741344</ext-link>.</mixed-citation></ref>
</ref-list>
<sec sec-type="supplementary-material">
<sec id="s5">
<label>4.</label>
<title>Supplementary Material</title>
<sec id="s5a">
<label>4.1.</label>
<title>General Methods</title>
<p>In virtual reality, participants viewed the stimuli through a binocular head-mounted display NVIS SX111 with horizontal view of 102&#x00B0;, vertical view of 64&#x00B0; and binocular overlap of 50&#x00B0;. Each display for the right and the left eye had a resolution of 1280 &#x00D7; 1024 pixels refreshed at a rate of 60 Hz and an end-to-end latency of 2 frames. The tracking system consisted of 14 MX3/T20/T20-S Vicon tracking cameras, a tracking computer (Quad Core Intel Xeon 3.6GHz processor, NVidia Quadro K2000 graphics card, 8GB RAM) running Windows and a graphics computer (8 core AMD Opteron 6212 processor, dual NVidia GeForce GTX 590 graphics cards, 16GB RAM) running Linux that generated the experimental stimulus. The head-mounted display was tracked at a rate of 240 Hz.</p>
<p>In the real environment, participants wore a tracked hardhat with blinkers on the side to imitate the horizontal field of vision of the head-mounted display. The participants were blindfolded when guided to the start zone by the experimenter. The target boxes had markers stuck to them so that they could be tracked and their position recorded during trials. In both virtual and real environment, participants used a hand-held, fully-tracked pointer resembling a gun, to indicate a pointing direction with the direction of the pointer and with a button press.</p>
<p>All participants had normal or corrected-to-normal vision and could distinguish the colors of the target boxes. Their acuity and stereoacuity were tested before the start of the experiment: all had 6/6 vision or better and normal stereopsis with 60 arc seconds or better.</p>
<sec id="s5a1">
<label>4.1.1</label>
<title>Ethical Approval</title>
<p>The experiments described in this study received the approval of the University of Reading Research Ethics Committee. Informed consent was obtained from all participants.</p>
</sec>
<sec id="s5a2">
<label>4.1.2</label>
<title>Experiment 1</title>
<p>The first pointing experiment was conducted in both a real-world scene and in virtual reality. Participants were first tested in a real-world block, followed by a block in virtual reality and another real-world block at the end.</p>
<sec id="s5a2a">
<title>Participants</title>
<p>We recruited 22 participants (aged 19&#x2013;46) who were paid &#x00A3;10.00 per hour. Data from 2 participants had to be discarded as one failed to understand the task and one felt too uncomfortable wearing the head-mounted display. 19 out of the 20 were na&#x00EF;ve to the experiment, and 1 was a researcher in our lab who had prior knowledge about the task.</p>
</sec>
<sec id="s5a2b">
<title>Procedure</title>
<p>In virtual reality, in order to start a trial participants walked towards a large green cube with an arrow pointing towards it in an otherwise black space then, as soon as they stepped inside the cube, the stimulus appeared. In the real world, participants wore a blindfold and were guided to the start zone by the experimenter. As soon as they were in the center of the start zone, the experimenter removed the blindfold. In both real and virtual experiments, the participants then viewed 4 boxes from this start zone. They were told that from where they were standing, the blue box was always closer than the pink box and both were in the same visual direction. Likewise, the red box was always closer than the yellow box and also both in the same visual direction as viewed from the start zone, see the example plan view in <xref ref-type="fig" rid="fig1">Fig. 1a&#x2013;c</xref>. Participants took as much time as they needed to view and memorize the box positions, although this was typically between 10 and 20 seconds. If they attempted to leave the start zone to walk closer to the target boxes, the whole scene disappeared in virtual reality or were inhibited by the experimenter in the real world. When they had finished memorizing the box layout, they walked behind a &#x2018;wall&#x2019; (partition) towards a pointing zone. In virtual reality, the pointing zone was indicated by a colored poster (colored according to the color of the box to which they should point) and this poster only appeared after they left the start zone similarly, in the real-world task, there were three white posters at the three pointing zones (see <xref ref-type="fig" rid="fig1">Fig. 1c</xref>) and the participant was only told which one to stop at after they had left the start zone. In virtual reality, the poster changed its color after each shot to indicate the next target box to point at. In the real world, the experimenter told the participant which box to point at. Following these instructions, the participant had to point 8 times to each of the 4 boxes in a pseudo-random order, i.e. 32 pointing directions in all. The end of each trial was indicated by the whole scene disappearing in virtual reality (replaced by the large green box) and in the real world the experimenter told the participant to wait to be guided back blindfolded to the start. The participant received a score ranging from 0&#x2013;100&#x0025; reflecting the accuracy of all the shots but this information could not be used to infer the direction or magnitude of their pointing error on any given shot.</p>
</sec>
<sec id="s5a2c">
<title>Box layouts and stimulus</title>
<p>8 participants were tested on 9 box layouts (27 trials in total &#x2212; 9 layouts &#x00D7; 3 pointing zones), whereas the remaining 12 participants were tested on 4 layouts (12 trials in total &#x2212; 4 layouts &#x00D7; 3 pointing zones). All layouts are shown in online Supplementary Data. The virtual and the real stimulus were designed to be as similar as possible, with a similar scale, texture mapping taken from photographs in the laboratory and target boxes using the same colors and icons, see <xref ref-type="fig" rid="fig1">Fig. 1a&#x2013;b</xref>.</p>
<p>The box positions in the 9 box layouts were chosen such that the following criteria were satisfied, see <xref ref-type="fig" rid="fig1">Fig. 1(c)</xref>:
<list list-type="bullet">
<list-item><p>The blue and the pink boxes were positioned along one line (as seen in plan view) while the red and the yellow box were positioned along a second line. The two lines intersected inside the start zone.</p></list-item>
<list-item><p>All box layouts preserved the box order: this meant that the blue box was always in front of the pink box, and the red box was in front of the yellow box, but the distances to each box varied from layout to layout.</p></list-item>
<list-item><p>The 2 lines were 25&#x00B0; apart from one another. This number allowed a range of target distances along these line while the boxes all remained within the real room.</p></list-item>
</list></p>
</sec>
</sec>
<sec id="s5a3">
<label>4.1.3</label>
<title>Experiment 2</title>
<p>This experiment aimed to identify the influence of the facing orientation at the pointing zones. 7 participants, all of whom had been participants in the first experiment, viewed the same 6 layouts as in Experiment 1 but in virtual reality only. They then walked to either zone A or zone C, stopped and faced a poster located either to the &#x2018;North&#x2019;, &#x2018;South&#x2019;, or &#x2018;West&#x2019; of the pointing zone (unlike Experiment 1 when there was only one poster position per pointing zone, see <xref ref-type="fig" rid="figS1">Fig. S1a</xref>. Otherwise, the procedure was identical to Experiment 1, with the exception that the participant pointed 6 times (rather than 8) to each of the boxes in a random order at the pointing zones. Data are shown in <xref ref-type="fig" rid="figS3">Figure S3</xref> and <xref ref-type="fig" rid="fig2">Figure 2f</xref>.</p>
</sec>
<sec id="s5a4">
<label>4.1.4</label>
<title>Experiment 3</title>
<p>The third experiment was also carried out only in virtual reality and here the testing room was scaled (in the x-y plane) by a factor of 1.5, i.e. the height of the room did not change. The pointing zones A, B and C were mirrored along the center line of the room to create an additional 3 pointing zones D, E and F, see <xref ref-type="fig" rid="figS1">Fig. S1b</xref> but the physical size of the laboratory dictated that participants could only go to one side of the scene or the other (either zones A, B, C or zones D, E, F). The participants did not know whether they would walk to the left or to the right from the start zone until the moment that they pressed the button (when the boxes disappeared). There were 6 participants, 3 of whom had taken part in the previous experiments while 3 were na&#x00EF;ve.</p>
</sec>
<sec id="s5a5">
<label>4.1.5</label>
<title>Experiment 4</title>
<p>The scene layout for this experiment is shown in <xref ref-type="fig" rid="figS1">Fig. S1c&#x2013;j</xref>. There were 4 pointing zones B, C, D and E and only 4 box layouts. All the data was collected in virtual reality. For every condition (box layout and pointing zone), participants carried out 2 trials (12 shots per target) with the wall in one orientation and the same number of trials with the wall in a different orientation.</p>
<fig id="figS1" position="float" orientation="portrait" fig-type="figure">
<label>Figure S1:</label>
<caption><p><italic>Plan view of (a) Experiment 2, (b), Experiment 3 and (c&#x2013;j) Experiment 4 with 8 different configurations. Black solid line indicates walls that were always present. In (c&#x2013;j), the gray dotted line shows the wall that appeared when the participant pressed a button at the start zone (white diamond). The black dotted line shows the wall that appeared after participant left the start zone. In (g) and (h) the wall remained in the same place</italic>.</p></caption>
<graphic xlink:href="390088_figS1.tif"/>
</fig>
<p>4 na&#x00EF;ve and 6 non-na&#x00EF;ve were recruited for this study. The procedure of this experiment was similar to Experiments 1&#x2013;3 with the following differences: Participants were standing at the start zone and viewed the target box positions. After pressing a button, a 1.8m tall wall appeared in front of them, see gray dotted line in <xref ref-type="fig" rid="figS1">Fig. S1c&#x2013;j</xref>. A white poster at eye height and a green square on the floor appeared in the room indicating the location of the pointing zone and the orientation. When the participant left the start zone, another wall appeared, see black solid line in <xref ref-type="fig" rid="figS1">Fig. S1c&#x2013;j</xref>. The rest of the trial was as described in Experiment 1&#x2013;3.</p>
</sec>
</sec>
<sec id="s5b">
<label>4.2.</label>
<title>Results</title>
<sec id="s5b1">
<label>4.2.1</label>
<title>Individual participant data</title>
<fig id="figS2" position="float" orientation="portrait" fig-type="figure">
<label>Figure S2:</label>
<caption><p><italic>Experiment 1</italic> &#x2014; <italic>Like <xref ref-type="fig" rid="fig2">Figure 2</xref> but showing data for individual participants. (a) Pointing error collected in a real and virtual environments; (b) comparison of pointing errors after direct and indirect walking. Symbol colors indicate different participants</italic>.</p></caption>
<graphic xlink:href="390088_figS2.tif"/>
</fig>
<fig id="figS3" position="float" orientation="portrait" fig-type="figure">
<label>Figure S3:</label>
<caption><p><italic>Experiment 2</italic> &#x2014; <italic>Pointing errors collected with different facing orientation, color-coded by participants. (a) Participants&#x2019; errors when their initial orientation was &#x2018;North&#x2019; plotted against errors for &#x2018;South&#x2019; orientation</italic>, (b) <italic>&#x2018;North&#x2019; compared to &#x2018;West&#x2019; and (c) &#x2018;West&#x2019; compared to &#x2018;South&#x2019;. See <xref ref-type="fig" rid="fig1">Figure 1c</xref> for icon showing nominal &#x2018;North&#x2019; in the scene</italic>.</p></caption>
<graphic xlink:href="390088_figS3.tif"/>
</fig>
<fig id="figS4" position="float" orientation="portrait" fig-type="figure">
<label>Figure S4:</label>
<caption><p><italic>Experiment 4</italic> &#x2014; <italic>test/re-test reliability. In this experiment, the same conditions were repeated. Errors on the second run are plotted against errors on the first run. The slope of the correlation is</italic> 0.61, <italic>indicating that participants had a smaller range of pointing errors in the second run of the experiment. As in previous figures, each color indicates a different participant</italic>.</p></caption>
<graphic xlink:href="390088_figS4.tif"/>
</fig>
</sec>
<sec id="s5b2">
<label>4.2.2</label>
<title>First Trial Data</title>
<fig id="figS5" position="float" orientation="portrait" fig-type="figure">
<label>Figure S5:</label>
<caption><p><italic>Data from participants&#x2019; very first trial (in Experiment 1). Participants viewed the stimuli in the real world without further instructions. After walking to the pointing zone, they were asked to point to the four boxes in a random order, eight times each (i.e. 32 shots per participant). Colors indicate different participants as in <xref ref-type="fig" rid="figS2">Fig. S2</xref></italic></p></caption>
<graphic xlink:href="390088_figS5.tif"/>
</fig>
</sec>
<sec id="s5b3">
<label>4.2.3</label>
<title>Alternative definitions of pointing direction</title>
<fig id="figS6" position="float" orientation="portrait" fig-type="figure">
<label>Figure S6:</label>
<caption><p><italic>Evidence supporting the choice of &#x2018;shooting&#x2019; angle (&#x03D5;) rather than visual direction (&#x03B8;) as the most appropriate definition of pointing direction. In a control condition, participants pointed at target boxes from the start zone so the target was visible (18 participants tested on 2 box layouts with 4 target boxes in each layout, 144 shots in total). Their instruction was to &#x2018;shoot at the box&#x2019;, just as it was in the spatial updating experiments. (a) Distribution of pointing errors when target direction is defined relative to the pointing device and shooting direction is defined by the orientation of the device (see &#x03D5; in <xref ref-type="fig" rid="fig1">Fig. 1d</xref>). (b) As for (a) but with pointing errors defined as the difference in visual direction of the target and the pointing device as measured from the cyclopean point (see &#x03B8; in <xref ref-type="fig" rid="fig1">Fig. 1e</xref>). The mean of the distribution is significantly biased for &#x03B8; (t-test, p &#x003C; 0.001) but not for &#x03D5; (p &#x003D; 0.605) suggesting &#x03D5; reflects participants&#x2019; intentions when pointing. (c) There is a significant correlation between the &#x03D5; and &#x03B8; measures in the spatial updating experiments (R &#x003D; 0.91). These data are from Experiment 1. When &#x03D5; &#x003D; 0, &#x03B8; is about</italic> &#x2212;20&#x00B0;. <italic>This is the direction of bias that would be created if a right-handed participant held the pointing device slightly out to their right and pointed to a target directly ahead of them</italic>.</p></caption>
<graphic xlink:href="390088_figS6.tif"/>
</fig>
</sec>
</sec>
<sec id="s5c">
<label>4.3.</label>
<title>Visual Models</title>
<sec id="s5c1">
<label>4.3.1</label>
<title>Retrofit model</title>
<p>With <italic>&#x03BC;</italic> &#x003D; 0 and an arbitrarily chosen <italic>&#x03C3;</italic> &#x003D; 15, it is now possible to calculate for each possible box position in <bold><italic>B</italic></bold> the likelihood of the participant pointing to that possible box position using the probability density function:
<disp-formula id="eqn1">
<alternatives>
<graphic xlink:href="390088_eqn1.gif"/>
</alternatives>
</disp-formula>
with <italic>p</italic> &#x003D; [1,&#x0226; ,20] for 20 participants, and <italic>b</italic> &#x003D; [1,&#x2026;,4] for all 4 boxes in <italic>l</italic> &#x003D; [1,&#x2026;,9] 9 box layouts, and <italic>m</italic> &#x003D; [1,&#x2026;,3] for all 3 testing zones, and <italic>k</italic> &#x003D; [1,&#x2026;,4000] for all possible box positions defined in <bold><italic>B</italic></bold>, The maximum likelihood is then calculated for each box in each layout across all participants across all zones:
<disp-formula id="eqn2">
<alternatives>
<graphic xlink:href="390088_eqn2.gif"/>
</alternatives>
</disp-formula>
with <italic>P</italic> &#x003D; 20 for the total number of participants and <italic>M</italic> &#x003D; 3 for the total number of shooting zones.</p>
</sec>
<sec id="s5c2">
<label>4.3.2</label>
<title>Retrofit target locations for Experiment 1, 2 and 4 combined</title>
<fig id="figS7" position="float" orientation="portrait" fig-type="figure">
<label>Figure S7:</label>
<caption><p><italic>Box layouts tested in Experiment 1, 2, and 4: (a) Real box positions of 3 box layouts plotted in the same plan view. (b) Predicted box layouts calculated using the retrofit model, based on combined data from all studies</italic>.</p></caption>
<graphic xlink:href="390088_figS7.tif"/>
</fig>
</sec>
<sec id="s5c3">
<label>4.3.3</label>
<title>Retrofit Model Experiment 3</title>
<fig id="figS8" position="float" orientation="portrait" fig-type="figure">
<label>Figure S8:</label>
<caption><p><italic>Experiment 3, &#x2018;retrofit&#x2019; model using the estimated directions of one participant pointing to the blue box in one layout (a) at zones A&#x2013;C alone or (b) at zones D&#x2013;F alone or (c) A&#x2013;F all together. (d) Similar to (c) but now showing the maximum likelihood location for all the boxes in all layouts. In both (c) and (d), the predicted box locations are shifted towards a north-south plane in the center of the room</italic>.</p></caption>
<graphic xlink:href="390088_figS8.tif"/>
</fig>
</sec>
<sec id="s5c4">
<label>4.3.4.</label>
<title>Dead-Reckoning Model with Calibration Errors</title>
<p>The dead-reckoning model simulates a moving observer storing an egocentric map of the box positions by constantly updating the heading direction with respect to north (<italic>&#x221D;</italic>) and estimating the distance traveled on each step (<italic>d</italic>). Here, we assume that the observer misestimates <italic>&#x221D;</italic> and <italic>d</italic> with a constant error (multiplicative calibration errors <italic>&#x03C9;<sub>&#x221D;</sub></italic> and <italic>&#x03C9;<sub>d</sub></italic> respectively). This leads to a cumulative error in the estimate of the participant&#x2019;s location. The box locations are assumed to be known correctly.</p>
<p>Initially, the position of the boxes is given, by definition, as follows, where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="390088_inline1.gif"/></alternatives></inline-formula> is the starting distance of box <italic>b</italic> and angle <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="390088_inline2.gif"/></alternatives></inline-formula> is its visual direction with respect to &#x2019;north&#x2019; <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="390088_inline3.gif"/></alternatives></inline-formula> as viewed from the start position (boxes are still visible here):
<disp-formula>
<alternatives>
<graphic xlink:href="390088_ueqn1.gif"/>
</alternatives>
</disp-formula>
with <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="390088_inline4.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="390088_inline5.gif"/></alternatives></inline-formula> being the <italic>x</italic>- and <italic>y</italic>-coordinates of the <italic>b<sup>th</sup></italic> box, <italic>pos</italic><sub>0,<italic>x</italic></sub> and <italic>pos</italic><sub>0,<italic>y</italic></sub> the Cyclopean point of the observer at the start position. At subsequent steps, when the boxes become obscured by the wall, the polar coordinates are calculated using the following equations:
<disp-formula>
<alternatives>
<graphic xlink:href="390088_ueqn2.gif"/>
</alternatives>
</disp-formula>
with
<disp-formula>
<alternatives>
<graphic xlink:href="390088_ueqn3.gif"/>
</alternatives>
</disp-formula>
</p>
<p>We fitted this model to the data from Experiment 1 by varying the two free parameters, <italic>&#x03C9;<sub>&#x221D;</sub></italic> and <italic>&#x03C9;<sub>d</sub></italic>, to give the minimum root-mean-square-error. (see <xref ref-type="fig" rid="figS9">Fig. S9</xref>).</p>
<fig id="figS9" position="float" orientation="portrait" fig-type="figure">
<label>Figure S9:</label>
<caption><p><italic>Parameters in the dead-reckoning model. Using the data from all participants, this plots shows the different values of the root-mean-square error (RMSE) between data and model measured in degrees using different combinations of the model parameters &#x03C9;<sub>d</sub> (distance bias) and &#x03C9;<sub>&#x221D;</sub> (angular bias). Any combination with an RMSE larger than twice that of the ground truth model for the indirect condition of Experiment 1 (RMSE<sub>lim</sub></italic> &#x003D; 2 &#x00B7; 16.6&#x00B0;) <italic>has been set to that value (&#x2018;Max&#x2019;). Dark blue squares show the calibration errors with lower RMSE values, the lowest is at</italic> (0.9,0.9) <italic>with an RMSE value of</italic> 13.2&#x00B0; <italic>(&#x2018;Min&#x2019;)</italic>.</p></caption>
<graphic xlink:href="390088_figS9.tif"/>
</fig>
</sec>
<sec id="s5c5">
<label>4.3.5</label>
<title>Comparison between direct and indirect walking using dead-reckoning models</title>
<fig id="figS10" position="float" orientation="portrait" fig-type="figure">
<label>Figure S10:</label>
<caption><p><italic>Comparison of dead-reckoning predictions for direct and indirect walking. Error predicted by the dead-reckoning model for the indirect walking condition plotted against this value for direct walking trails</italic>. <italic>As in <xref ref-type="fig" rid="fig2">Figure 2d</xref>, each symbol is based on the mean data for 20 participants. The data for Zone C (triangles) is most informative as the length difference between the direct and indirect paths is most extreme in this case. Here, the errors for indirect walking are significantly more positive than the errors for direct walking (direct walking M</italic> &#x003D; 6.06, <italic>SD</italic> &#x003D; 3.19, <italic>indirect walking M</italic> &#x003D; 0.764, <italic>SD</italic> &#x003D; 2.76, <italic>t</italic>(35) &#x003D; 18.5, <italic>p</italic> &#x003C; 0.001) <italic>whereas the experimental data for these two conditions were not significantly different</italic>.</p></caption>
<graphic xlink:href="390088_figS10.tif"/>
</fig>
</sec>
<sec id="s5c6">
<label>4.3.6</label>
<title>Zero-mean noise</title>
<p>If, instead, we assume that the dead-reckoning noise has zero mean then there is no systematic effect on pointing, which we demonstrate as follows for an estimate of orientation. We added a normally distributed random error to the estimate of visual direction with respect to north on each step, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="390088_inline6.gif"/></alternatives></inline-formula>:
<disp-formula id="eqn3">
<alternatives>
<graphic xlink:href="390088_eqn3.gif"/>
</alternatives>
</disp-formula>
with the function <italic>randn</italic>(<italic>&#x03BC;</italic>, <italic>&#x221D;</italic>) returning a random number drawn from a distribution with a standard deviation <italic>&#x221D;</italic>, and a mean <italic>&#x03BC;</italic>. <italic>E</italic> is a random error added to the estimate of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="390088_inline7.gif"/></alternatives></inline-formula>, drawn from a distribution with a standard deviation of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="390088_inline8.gif"/></alternatives></inline-formula> radians, and a mean of <italic>&#x03BC;</italic> &#x003D; 0.</p>
<fig id="figS11" position="float" orientation="portrait" fig-type="figure">
<label>Figure S11:</label>
<caption><p><italic>Histogram of prediction errors calculated a 100 times for each box in each layout, using the walking trajectory of every participant tested in the indirect walking condition of Experiment 1 at pointing zones A, B, and C with a normally distributed random noise on the estimate of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="390088_inline9.gif"/></alternatives></inline-formula>. The mean of the pointing errors is not significantly different from zero</italic>.</p></caption>
<graphic xlink:href="390088_figS11.tif"/>
</fig>
<p>Using Eq. (3), predictions of pointing directions were calculated with a random additive noise on the direction of &#x2018;North&#x2019;. Calculating the directions 100 times for each box in each layout, using the walking trajectory of every participant tested in the indirect walking condition of Experiment 1 at pointing zones A&#x2013;C, a histogram of errors is plotted in <xref ref-type="fig" rid="figS11">Fig. S11</xref>.</p>
</sec>
<sec id="s5c7">
<label>4.3.7</label>
<title>Abathic Model</title>
<p><xref ref-type="fig" rid="figS12">Fig. S12</xref> shows psychophysical data from Johnston [<xref ref-type="bibr" rid="c44">44</xref>] that determine an abathic distance (shown by the cross, 82cm) and slope (0.23) relating estimated (or &#x2018;scaling&#x2019;) distance to physical viewing distance. In general, we can fit these two parameters to our pointing data (<xref ref-type="fig" rid="fig4">Figure 4a</xref>).</p>
<fig id="figS12" position="float" orientation="portrait" fig-type="figure">
<label>Figure S12:</label>
<caption><p><italic>Data re-plotted from Johnston&#x2019;s original paper [<xref ref-type="bibr" rid="c44">44</xref>] showing the relationship between true and estimated perceived egocentric distances. Note, that because the slope is less than 1 there is an underestimation of distances larger, and an overestimation of distances smaller than the abathic distance</italic>.</p></caption>
<graphic xlink:href="390088_figS12.tif"/>
</fig>
<p>The best fitting values are a slope of 1.03 and an intercept of 0.17 which corresponds to an abathic distance of &#x2212;5.66. Note that the negative abathic distance is due to the slope being very close to 1. Thus, the misestimated egocentric distances of the boxes, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="390088_inline10.gif"/></alternatives></inline-formula> is:
<disp-formula id="eqn4">
<alternatives>
<graphic xlink:href="390088_eqn4.gif"/>
</alternatives>
</disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="390088_inline11.gif"/></alternatives></inline-formula> is the true egocentric distance and <italic>b</italic> &#x003D; [1,&#x2026;,4] the index for each box.</p>
</sec>
<sec id="s5c8">
<label>4.3.8</label>
<title>Comparison between abathic and dead-reckoning model predictions</title>
<fig id="figS13" position="float" orientation="portrait" fig-type="figure">
<label>Figure S13:</label>
<caption><p><italic>Abathic and dead-reckoning models applied to the indirect walking condition in Experiment 1. (a) and (c) show pointing directions and errors for the abathic model. (b) and (d) show the same for the dead-reckoning model</italic>.</p></caption>
<graphic xlink:href="390088_figS13.tif"/>
</fig>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>Table S1:</label>
<caption><p><italic>Abathic, dead-reckoning and projection plane models compared. Gray-colored cells show lowest root-mean-square error amongst the 3 different models. ANOVA to test null hypothesis, that all the means across the 3 different models are drawn from the same distribution:null hypothesis cannot be rejected, with &#x221D;</italic> &#x003D; 0.05, [F<sub>2,57</sub> &#x003D; 3.00, <italic>p</italic> &#x003D; 0.0579<italic>]; right-tailed</italic> t<italic>-test to test whether the mean of errors of the abathic model (M</italic> &#x003D; 14.4, <italic>SD</italic> &#x003D; 4.57) <italic>is higher than the mean of the projection model (M</italic> &#x003D; 12.91, <italic>SD</italic> &#x003D; 3.47) <italic>(null hypothesis: the mean is not higher, alternative hypothesis: it is higher): [t</italic>(19) &#x003D; 1.78, <italic>p</italic> &#x003D; 0.0457]; <italic>right-tailed t-test to test whether mean errors of dead-reckoning model (M</italic> &#x003D; 16.2, <italic>SD</italic> &#x003D; 4.58) <italic>is higher than the mean error of projection model: [t</italic>(19) &#x003D; 4.69, <italic>p</italic> &#x003C; 0.001]</p></caption>
<graphic xlink:href="390088_tblS1.tif"/>
</table-wrap>
</sec>
<sec id="s5c9">
<label>4.3.9</label>
<title>Comparison between ground truth and projection model pointing directions</title>
<fig id="figS14" position="float" orientation="portrait" fig-type="figure">
<label>Figure S14:</label>
<caption><p><italic>Experiment 1 - actual and predicted pointing directions according to the projection plane model</italic></p></caption>
<graphic xlink:href="390088_figS14.tif"/>
</fig>
<fig id="figS15" position="float" orientation="portrait" fig-type="figure">
<label>Figure S15:</label>
<caption><p><italic>Experiment 2 - actual and predicted pointing directions according to the projection plane model</italic></p></caption>
<graphic xlink:href="390088_figS15.tif"/>
</fig>
<fig id="figS16" position="float" orientation="portrait" fig-type="figure">
<label>Figure S16:</label>
<caption><p><italic>Experiment 3 - actual and predicted pointing directions according to the projection plane model</italic></p></caption>
<graphic xlink:href="390088_figS16.tif"/>
</fig>
<fig id="figS17" position="float" orientation="portrait" fig-type="figure">
<label>Figure S17:</label>
<caption><p><italic>Experiment 4 - actual and predicted pointing directions according to the projection plane model</italic></p></caption>
<graphic xlink:href="390088_figS17.tif"/>
</fig>
</sec>
<sec id="s5c10">
<label>4.3.10</label>
<title>Model comparisons using two model selection criteria, AIC and BIC</title>
<table-wrap id="tblS2" orientation="portrait" position="float">
<label>Table S2:</label>
<caption><p><italic>A comparison of the fits of the projection plane model and the retrofit model using Akiake (AIC) and Bayesian (BIC) Information Criteria which penalize a model according to the number of free parameters it has</italic>.</p></caption>
<graphic xlink:href="390088_tblS2.tif"/>
</table-wrap>
</sec>
</sec>
<sec id="s5d">
<label>4.4.</label>
<title>Raw data</title>
<p>Raw data and code to reproduce an example figure (<xref ref-type="fig" rid="fig8">Figure 8</xref>) are online: <monospace><ext-link ext-link-type="uri" xlink:href="https://github.com/jnyvng/PointingData">https://github.com/jnyvng/PointingData</ext-link></monospace>.</p>
<p>Also, there is an interactive website showing the raw pointing data: <monospace><ext-link ext-link-type="uri" xlink:href="http://www.jennyvuong.net/dataWebsite/">http://www.jennyvuong.net/dataWebsite/</ext-link></monospace></p>
</sec>
</sec>
</sec>
</back>
</article>