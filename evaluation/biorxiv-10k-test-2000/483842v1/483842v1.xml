<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/483842</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Human online adaptation to changes in prior probability</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7645-7141</contrib-id>
<name><surname>Norton</surname><given-names>Elyse H.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7471-7336</contrib-id>
<name><surname>Acerbi</surname><given-names>Luigi</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">&#x2020;</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Ma</surname><given-names>Wei Ji</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2079-4552</contrib-id>
<name><surname>Landy</surname><given-names>Michael S.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Psychology Department, New York University</institution>, New York, NY, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution>Center for Neural Science, New York University</institution>, New York, NY, <country>USA</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label>Corresponding author, email: <email>landy@nyu.edu</email></corresp>
<fn fn-type="present-address" id="n1"><label>&#x2020;</label><p>Current Address: D&#x00E9;partement des neurosciences fondamentales, Universit&#x00E9; de Gen&#x00E8;ve, CMU, 1 rue Michel-Servet, 1206 Gen&#x00E8;ve, Switzerland</p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2018</year>
</pub-date>
<elocation-id>483842</elocation-id>
<history>
<date date-type="received">
<day>30</day>
<month>11</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>30</day>
<month>11</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>30</day>
<month>11</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="483842.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Optimal sensory decision-making requires the combination of uncertain sensory signals with prior expectations. The effect of prior probability is often described as a shift in the decision criterion. Can observers track sudden changes in probability? To answer this question, we used a change-point detection paradigm that is frequently used to examine behavior in changing environments. In a pair of orientation-categorization tasks, we investigated the effects of changing probabilities on decision-making. In both tasks, category probability was updated using a sample-and-hold procedure. We developed an ideal Bayesian change-point detection model in which the observer marginalizes over both the current run length (i.e., time since last change) and the current category probability. We compared this model to various alternative models that correspond to different strategies &#x2013; from approximately Bayesian to simple heuristics &#x2013; that the observers may have adopted to update their beliefs about probabilities. We find that probability is estimated following an exponential averaging model with a bias towards equal priors, consistent with a conservative bias. The mechanism underlying change of decision criterion is a combination of on-line estimation of prior probability and a stable, long-term equal-probability prior, thus operating at two very different timescales.</p>
<sec>
<title>Author summary</title>
<p>We demonstrate how people learn and adapt to changes to the probability of occurrence of one of two categories on decision-making under uncertainty. The study combined psychophysical behavioral tasks with computational modeling. We used two behavioral tasks: a typical forced-choice categorization task as well as one in which the observer specified the decision criterion to use on each trial before the stimulus was displayed. We formulated an ideal Bayesian change-point detection model and compared it to several alternative models. We found that the data are best fit by a model that estimates category probability based on recently observed exemplars with a bias towards equal probability. Our results suggest that the brain takes multiple relevant time scales into account when setting category expectations.</p>
</sec>
</abstract>
<counts>
<page-count count="24"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Sensory decision-making involves making decisions under uncertainty. Furthermore, optimal sensory decision-making requires the combination of uncertain sensory signals with prior expectations. Perceptual models of decision-making often incorporate prior expectations to describe human behavior. In Bayesian models, priors are combined with likelihoods to compute a posterior [<xref rid="c1" ref-type="bibr">1</xref>]. In signal detection theory, the effect of unequal probabilities (signal present vs. absent) is a shift of the decision criterion [<xref rid="c2" ref-type="bibr">2</xref>].</p>
<p>The effects of prior probability on the decision criterion have been observed in detection [<xref rid="c2" ref-type="bibr">2</xref>&#x2013;<xref rid="c4" ref-type="bibr">4</xref>], line tilt [<xref rid="c5" ref-type="bibr">5</xref>], numerosity estimation [<xref rid="c6" ref-type="bibr">6</xref>, <xref rid="c7" ref-type="bibr">7</xref>], recognition memory [<xref rid="c8" ref-type="bibr">8</xref>], and perceptual categorization [<xref rid="c9" ref-type="bibr">9</xref>] tasks, among others. These studies generally use explicit priors, assume a fixed effect, and treat learning as additional noise. However, there are many everyday tasks in which the probability of a set of alternatives needs to be assessed based on one&#x2019;s past experience with the outcomes of the task. The importance of experience has been demonstrated in studies examining differences between experience-based and description-based decisions [<xref rid="c10" ref-type="bibr">10</xref>, <xref rid="c11" ref-type="bibr">11</xref>] and in perceptual-categorization tasks with unequal probability, in which response feedback leads to performance that is closer to optimal than observational feedback [<xref rid="c12" ref-type="bibr">12</xref>, <xref rid="c13" ref-type="bibr">13</xref>]. While these studies demonstrate the importance of experience on decision-making behavior, they do not describe how experience influences expectation formation. The influence of experience on the formation of expectations has been demonstrated for learning stimulus mean [<xref rid="c14" ref-type="bibr">14</xref>&#x2013;<xref rid="c17" ref-type="bibr">17</xref>], variance [<xref rid="c14" ref-type="bibr">14</xref>, <xref rid="c18" ref-type="bibr">18</xref>], higher-order statistics [<xref rid="c19" ref-type="bibr">19</xref>], likelihood distributions [<xref rid="c20" ref-type="bibr">20</xref>], the rate of change of the environment [<xref rid="c15" ref-type="bibr">15</xref>&#x2013;<xref rid="c17" ref-type="bibr">17</xref>, <xref rid="c21" ref-type="bibr">21</xref>&#x2013;<xref rid="c23" ref-type="bibr">23</xref>], and prior probability [<xref rid="c24" ref-type="bibr">24</xref>, <xref rid="c25" ref-type="bibr">25</xref>]. Here, we add to previous work by investigating how one&#x2019;s previous experience influences probability learning in a changing environment.</p>
<p>In the previous work on probability learning by Behrens et al. [<xref rid="c24" ref-type="bibr">24</xref>], participants tracked the probability of a reward to learn the value of two alternatives. This is a classic decision-making task that involves combining prior probability and reward. In contrast, we are interested in perceptual decision-making under uncertainty, in which prior probability is combined with uncertain sensory signals. We might expect differences in strategy between cognitive and perceptual tasks, as cognitive tasks can be susceptible to additional biases. For example, participants often exhibit base rate neglect (i.e., they ignore prior probability when evaluating evidence) in cognitive tasks [<xref rid="c26" ref-type="bibr">26</xref>] but not in perceptual tasks [<xref rid="c2" ref-type="bibr">2</xref>]. On the other hand, Behrens et al. [<xref rid="c24" ref-type="bibr">24</xref>] found that participants&#x2019; behavior was well described by an optimal Bayesian model, in that observed learning rates quantitatively matched those of a Bayesian decision-maker carrying out the same task. Although it is important to note that behavior was not compared to alternative models. A more recent study by Zylberberg et al. [<xref rid="c25" ref-type="bibr">25</xref>] examined probability learning under uncertainty in a motion-discrimination task. In this study, probability was estimated from a confidence signal rather than explicit feedback. Additionally, probability was changed across blocks, allowing participants to infer a change had occurred. Here, we examine probability learning when feedback is explicit and changes are sudden.</p>
<p>To investigate probability learning in uncertain and changing conditions, we designed a perceptual-categorization task in which observers need to learn category probability through experience. To prevent the use of external cues (e.g., the start of a new block indicating a change in probability) probabilities were changed using a sample-and-hold procedure. This approach has been used in decision-making [<xref rid="c21" ref-type="bibr">21</xref>, <xref rid="c22" ref-type="bibr">22</xref>, <xref rid="c24" ref-type="bibr">24</xref>] and motor domains [<xref rid="c18" ref-type="bibr">18</xref>] to examine behavior in dynamic contexts. Observers completed both a covert-and overt-criterion task, in which the decision criterion was implicit or explicit, respectively. The overt task, previously developed by Norton et al. [<xref rid="c16" ref-type="bibr">16</xref>], provided a richer dataset upon which to compare models of human behavior. We determined how observers tracked category probability in a changing environment by comparing human behavior to both Bayesian and alternative models. We find that a number of models qualitatively describe human behavior, but that, quantitatively, all models are outperformed by an exponential averaging model with a bias towards equal priors. Our results suggest that changes in the decision criterion are the result of both probability estimates computed on-line and a more stable, long-term prior.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Experiment</title>
<p>During each session, observers completed one of two orientation-categorization tasks. On each trial in the <italic>covert-criterion</italic> task, observers categorized an ellipse as belonging to category A or B by key press (<xref rid="fig1" ref-type="fig">Fig 1A</xref>). On each trial in the <italic>overt-criterion</italic> task, observers used the mouse to rotate a line to indicate their criterion prior to the presentation of an ellipse (<xref rid="fig1" ref-type="fig">Fig 1B</xref>). The observer was correct if the ellipse belonged to category A and was clockwise of the criterion line or if the ellipse belonged to category B and was counter-clockwise of the criterion line. The overt-criterion task is an explicit version of the covert-criterion task developed by Norton et al. [<xref rid="c16" ref-type="bibr">16</xref>]. The overt-criterion task provides a richer dataset than the covert-criterion task in that it affords a continuous measure and allows us to see trial by trial changes in the reported decision criterion, at the expense of being a more cognitive task. In both tasks, the categories were defined by normal distributions on orientation with different means (<italic>&#x00B5;</italic><sub>B</sub> = <italic>&#x00B5;</italic><sub>A</sub> &#x002B; &#x2206;&#x03B8;) and equal standard deviation (<italic>&#x03C3;</italic><sub><italic>C</italic></sub>); the mean of category A was set clockwise of the the mean of category B and a neutral criterion would be located halfway between the category means (<xref rid="fig1" ref-type="fig">Fig 1C</xref>). The difficulty of the task was equated across participants by setting &#x2206;<italic>&#x03B8;</italic> to a value that predicted a <italic>d</italic><sup>&#x2032;</sup> value of 1.5 based on the data from the initial measurement session (see Methods). All data are reported relative to the neutral criterion, <italic>z</italic><sub>neutral</sub> = (<italic>&#x00B5;</italic><sub>A</sub> &#x002B; <italic>&#x00B5;</italic><sub>B</sub>)<italic>/</italic>2.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig 1.</label>
<caption><title>Experimental design.</title>
<p>A: Trial sequence in the covert-criterion task. After stimulus offset, observers reported the category by key press and received auditory feedback indicating the correctness of their response. In addition, the fixation cross was displayed in the color of the generating category. B: Trial sequence in the overt-criterion task. Prior to stimulus onset, observers rotated a line to indicate their criterion by sliding the mouse side-to-side. When the observer clicked the mouse, a stimulus was displayed under the criterion line and auditory feedback was provided. C: Example stimulus distributions for category A (green) and category B (red). Note that numbers on the <italic>x</italic>-axis are relative to the neutral criterion (i.e., 0 is the neutral criterion). D: Example of random stepwise changes in probability across a block of trials. Change points occurred every 80-120 trials and are depicted above by the black arrows. Category A probabilities <italic>&#x03C0;</italic><sub><italic>t</italic></sub> were sampled from a discrete set, <italic>S</italic><sub><italic>&#x03C0;</italic></sub>= &#x007B;0.2, 0.35, 0.5, 0.65, 0.8&#x007D;. E: Generative model for the task in which category probability <italic>&#x03C0;</italic> is updated following a sample-and-hold procedure, a category <italic>C</italic> is selected based on the category probability, a stimulus <italic>s</italic> is drawn from the category distribution and is corrupted by visual noise resulting in the noisy measurement <italic>x</italic>. Note that this diagram omits the dependency that leads to change points every 80-120 trials.</p></caption>
<graphic xlink:href="483842_fig1.tif"/>
</fig>
<p>Prior to testing, observers were trained on the categories. Only the covert-criterion task was used for training (see Section 6 in S1 Appendix). During training, category probability was equal (<italic>&#x03C0;</italic> = 0.5) and observers received feedback on every trial that indicated both the correctness of the response (tone) and the generating category (visual). As a measure of category learning, we compute the probability of being correct in the training block and averaged across sessions. All observers learned the categories to the expected level of accuracy (<italic>p</italic>(correct) = 0.74 &#x00B1; 0.01; mean &#x00B1; SEM across observers), given that the expected fraction of correct responses for an ideal observer with <italic>d</italic>&#x2019; = 1.5 and equal priors over categories is 0.77. As an additional test of category learning, immediately following training observers estimated the mean orientation of each category by rotating an ellipse. Each category mean was estimated once and no feedback was provided. There was a significant correlation between the true and estimated means for each category (category A: <italic>r</italic> = 0.82, <italic>p &#x003C;</italic> 0.0001; category B: <italic>r</italic> = 0.97, <italic>p &#x003C;</italic> 0.0001), suggesting that categories were learned. However, on average mean estimates were repelled from the category boundary (average category A error of 11.3&#x00B0; &#x00B1; 6.3&#x00B0; and average category B error of &#x2212;8.0&#x00B0; &#x00B1; 2.6&#x00B0;; mean &#x00B1; SEM across observers) suggesting a systematic repulsive bias.</p>
<p>To determine how category probability affects decision-making, during testing category A probability <italic>&#x03C0;</italic><sub><italic>t</italic></sub> was determined using a sample-and-hold procedure (<xref rid="fig1" ref-type="fig">Fig 1D</xref>; category B probability was 1 &#x2212; <italic>&#x03C0;</italic><sub><italic>t</italic></sub>). For <italic>t</italic> = 1, category A probability was randomly chosen from a set of five probabilities <italic>S</italic><sub><italic>&#x03C0;</italic></sub> = &#x007B;0.2, 0.35, 0.5, 0.65, 0.8&#x007D;. On most trials, no change occurred (&#x2206;<sub><italic>t</italic></sub> = 0), so that <italic>&#x03C0;</italic><sub><italic>t</italic>&#x002B;1</sub> = <italic>&#x03C0;</italic><sub><italic>t</italic></sub>. Every 80-120 trials there was a change point (&#x2206;<sub><italic>t</italic></sub> = 1), with change point sampled uniformly. At each change point, category probability was randomly selected from the <italic>S</italic><sub><italic>&#x03C0;</italic></sub> excluding the current probability. On each trial <italic>t</italic>, a category <italic>C</italic><sub><italic>t</italic></sub> was randomly selected (with <italic>P</italic> (category A) = <italic>&#x03C0;</italic><sub><italic>t</italic></sub>) and a stimulus <italic>s</italic><sub><italic>t</italic></sub> was drawn from the stimulus distribution corresponding to the selected category. We assume that the observer&#x2019;s internal representation of the stimulus is a noisy measurement <italic>x</italic><sub><italic>t</italic></sub> drawn from a Gaussian distribution with mean <italic>s</italic><sub><italic>t</italic></sub> and standard deviation <italic>&#x03C3;</italic><sub>v</sub>, which represents visual noise (v). The generative model of the task is summarized in <xref rid="fig1" ref-type="fig">Fig 1E</xref>.</p>
</sec>
</sec>
<sec id="s3">
<title>Bayesian models</title>
<p>To understand how decision-making behavior is affected by changes in category probability, we compared observer performance to several Bayesian models. To compute the behavior of a Bayesian observer, we developed a Bayesian change point-detection algorithm, based on Adams and MacKay [<xref rid="c27" ref-type="bibr">27</xref>], but which also accounts for Markov dependencies in the transition distribution after a change. Specifically, the Bayesian observer estimates the posterior distribution over the current run length (time since the last change point), and the state (category probability) before the last change point, given the data so far observed (category labels until trial <italic>t</italic>, C<sub><italic>t</italic></sub> = (<italic>C</italic><sub>1</sub>, <italic>&#x2026;, C</italic><sub><italic>t</italic></sub>)). We denote the current run length at the end of trial <italic>t</italic> by <italic>r</italic><sub><italic>t</italic></sub>, the current state by <italic>&#x03C0;</italic><sub><italic>t</italic></sub>, and the state before the last change point by <italic>&#x03BE;</italic><sub><italic>t</italic></sub>, where both <italic>&#x03C0;</italic><sub><italic>t</italic></sub>, <italic>&#x03BE;</italic><sub><italic>t</italic></sub> &#x2208; <italic>S</italic><sub><italic>&#x03C0;</italic></sub>. That is, if a changepoint occurs after trial <italic>t</italic> (i.e., <italic>r</italic><sub><italic>t</italic></sub> = 0), then the new category A probability will be <italic>&#x03C0;</italic><sub><italic>t</italic></sub> and the previous run&#x2019;s category probability <italic>&#x03BE;</italic><sub><italic>t</italic></sub> = <italic>&#x03C0;</italic><sub><italic>t</italic>-1</sub>. If no changepoint occurs, both <italic>&#x03C0;</italic> and <italic>&#x03BE;</italic> remain unchanged. We use the notation <inline-formula><alternatives><inline-graphic xlink:href="483842_inline1.gif"/></alternatives></inline-formula> to indicate the set of observations (category labels) associated with the run <italic>r</italic><sub><italic>t</italic></sub>, which is <inline-formula><alternatives><inline-graphic xlink:href="483842_inline2.gif"/></alternatives></inline-formula> for <italic>r</italic><sub><italic>t</italic></sub> <italic>></italic> 0, and &#x2205; for <italic>r</italic><sub><italic>t</italic></sub> = 0. The range of times with a colon, e.g., <inline-formula><alternatives><inline-graphic xlink:href="483842_inline3.gif"/></alternatives></inline-formula>, indicates the sub-vector of <italic>C</italic> with elements from <italic>t</italic>&#x2032; to <italic>t</italic> included.</p>
<p>Both of our tasks provide category feedback, so that at the end of trial <italic>t</italic> the observer has been informed of C<sub>1:<italic>t</italic></sub>. In S1 Appendix we derive the iterative Bayesian ideal-observer model. After each trial, the model calculates a posterior distribution over possible run lengths and previous probability states, <italic>P</italic> (<italic>r</italic><sub><italic>t</italic></sub>, <italic>&#x03BE;</italic><sub><italic>t</italic></sub>&#x007C;C<sub>1:<italic>t</italic></sub>). The generative model makes it easy to calculate the conditional probability of the current state for a given run length and previous state, <italic>P</italic> (<italic>&#x03C0;</italic><sub><italic>t</italic></sub>&#x007C;<italic>r</italic><sub><italic>t</italic></sub>, <italic>&#x03BE;</italic><sub><italic>t</italic></sub>, C<sub>1:<italic>t</italic></sub>). These two distributions may be combined, marginalizing (summing) across the unknown run length and previous states to yield the predictive probability distribution of the current state, <italic>P</italic> (<italic>&#x03C0;</italic><sub><italic>t</italic></sub>&#x007C;C<sub>1:<italic>t</italic></sub>). Given this distribution over states, in both tasks the observer needs to determine the probability of each category. In particular,
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="483842_eqn1.gif"/></alternatives>
</disp-formula>
</p>
<p>In the overt task, the ideal observer sets the current criterion to the optimal value <inline-formula><alternatives><inline-graphic xlink:href="483842_inline4.gif"/></alternatives></inline-formula> based on the known category orientation distributions and the current estimate of category probabilities. Further, in the ideal and all subsequent models of the overt task, in addition to early sensory noise (<italic>&#x03C3;</italic><sub>v</sub>) we assume the actual setting is perturbed by adjustment noise <inline-formula><alternatives><inline-graphic xlink:href="483842_inline5.gif"/></alternatives></inline-formula> where <inline-formula><alternatives><inline-graphic xlink:href="483842_inline6.gif"/></alternatives></inline-formula></p>
<p>In the covert task, the observer views a stimulus and makes a noisy measurement <italic>x</italic><sub><italic>t</italic></sub> of its true orientation <italic>s</italic><sub><italic>t</italic></sub> with noise variance <inline-formula><alternatives><inline-graphic xlink:href="483842_inline6a.gif"/></alternatives></inline-formula>. The prior category probability is combined with the noisy measurement to compute category A&#x2019;s posterior probability <italic>P</italic> (C<sub><italic>t</italic>&#x002B;1</sub> = A&#x007C;<italic>x</italic><sub><italic>t</italic>&#x002B;1</sub>, C<sub>1:<italic>t</italic></sub>). The observer responds &#x201C;A&#x201D; if that probability is greater than 0.5.</p>
<p>We consider the ideal-observer model (Bayes<sub>ideal</sub>) and three (suboptimal) variants thereof, which deviate from the ideal observer in terms of their beliefs about specific features of the experiment (Bayes<sub><italic>r</italic></sub>, Bayes<sub><italic>&#x03C0;</italic></sub>, and Bayes<sub><italic>&#x03B2;</italic></sub>). Two further variants of the Bayesian model (Bayes<sub>bias</sub> and Bayes<sub><italic>r,&#x03B2;</italic></sub>) are described in S1 Appendix. Crucially, all these models are &#x201C;Bayesian&#x201D; in that they compute a posterior over run length and probability state, but they differ with respect to the observer&#x2019;s assumptions about the generative model.</p>
<sec id="s3a">
<title><bold>Bayes</bold><sub><bold>ideal</bold></sub></title>
<p>The ideal Bayesian observer uses knowledge of the generative model to maximize expected gain. This model assumes knowledge of sensory noise, the category distributions, the set of potential probability states, and the run length distribution. Because observers were trained on the categories prior to completing each categorization task, assuming knowledge of the category distributions seems reasonable. Further, it is reasonable to assume knowledge of sensory noise as this is a characteristic of the observer. However, since observers were not told how often probability would change, nor were they told the set of potential probability states, observers may have had incorrect beliefs about the generative model. Thus, we developed additional Bayesian models (described below), in which observers could have potentially incorrect beliefs about different aspects of the generative model.</p>
</sec>
<sec id="s3b">
<title><bold>Bayes</bold><sub><italic><bold>r</bold></italic></sub></title>
<p>The Bayes<sub><italic>r</italic></sub> model allows for an observer to have an incorrect belief about the run length distribution. For a given discrete <italic>r</italic>, the observer believes that the run length distribution is drawn from a discrete uniform distribution, &#x223C;Unif <inline-formula><alternatives><inline-graphic xlink:href="483842_inline7.gif"/></alternatives></inline-formula> We chose this particular interval rather than a more general one, because it is simple and includes the true generative distribution. For the ideal observer, <italic>r</italic> = 120.</p>
</sec>
<sec id="s3c">
<title><bold>Bayes<sub><italic>&#x03C0;</italic></sub></bold></title>
<p>The Bayes<sub><italic>&#x03C0;</italic></sub> model allows for an observer to have an incorrect belief about the set of probability states. The veridical set of experimental states is five values of <italic>&#x03C0;</italic> ranging from 0.2 to 0.8. The Bayes<sub><italic>&#x03C0;</italic></sub> model observer also assumes five possible values of <italic>&#x03C0;</italic> evenly spaced, but ranging from <italic>&#x03C0;</italic><sub>min</sub> to <italic>&#x03C0;</italic><sub>max</sub> = 1 &#x2212; <italic>&#x03C0;</italic><sub>min</sub>, where <italic>&#x03C0;</italic><sub>min</sub> is a free parameter.</p>
</sec>
<sec id="s3d">
<title><bold>Bayes<sub><italic>&#x03B2;</italic></sub></bold></title>
<p>While incorrect assumptions about parameters of the generative model result in suboptimal behavior, suboptimality can also arise from prior biases (that is, incorrect hyper-parameters). The Bayes<sub><italic>&#x03B2;</italic></sub> model, like the Bayes<sub>ideal</sub> model, assumes knowledge of the generative model, but also includes a hyperprior Beta(<italic>&#x03B2;, &#x03B2;</italic>) that is applied after a change-point. Thus, as <italic>&#x03B2;</italic> increases, the posterior belief over category probabilities is biased toward equal probability. For the ideal observer, <italic>&#x03B2;</italic> = 1 (a uniform distribution).</p>
</sec>
<sec id="s3e">
<title>Heuristic models</title>
<p>In addition to the Bayesian models described above, we tested the following heuristic models that do not require the observer to compute a posterior over run length and probability state. In each of the following models, assumptions vary about whether and how probability is estimated. In the Fixed Criterion (Fixed) model the observer assumes fixed probabilities. In the Exponential-Averaging (Exp), Exponential-Averaging with Prior Bias (Exp<sub>bias</sub>), and the <xref ref-type="bibr" rid="c29">Wilson et al. (2013)</xref> models, probability is estimated based on the recent history of categories. Finally, in the Reinforcement-Learning (RL) model, the decision criterion is updated following an error-driven learning rule with no assumptions about probability. We also tested three additional heuristic models that are described in the Supplementary material (see S1 Appendix).</p>
<p>Each of the following models, other than the RL model, computes an estimate of category probability <inline-formula><alternatives><inline-graphic xlink:href="483842_inline8.gif"/></alternatives></inline-formula> on each trial and the estimated probability of the alternative is <inline-formula><alternatives><inline-graphic xlink:href="483842_inline9.gif"/></alternatives></inline-formula> On each trial, the optimal criterion <italic>z</italic><sub>opt</sub> is computed based on these estimated probabilities in the identical manner as for the Bayesian models. To make a categorization decision in the covert-criterion task, the criterion is applied to the noisy observation of the stimulus. In the overt-criterion task, the observer reports the criterion, which we again assume is corrupted by adjustment noise.</p>
<sec id="s3e1">
<title>Fixed</title>
<p>While incorporating category probability into the decision process maximizes expected gain, an alternative strategy is to ignore changes in probability and fix the decision criterion throughout the block. In the fixed-criterion model, we assume equal category probability and the criterion is set to the neutral criterion:
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="483842_eqn2.gif"/></alternatives>
</disp-formula>
</p>
<p>This model is equivalent to a model in which the likelihood ratio, <inline-formula><alternatives><inline-graphic xlink:href="483842_inline10.gif"/></alternatives></inline-formula> is used to make categorization decisions rather than the posterior odds ratio in the covert-criterion task. This is a reasonable strategy for an observer who wants to make an informed decision, but is unsure about the current probability state and its rate of change.</p>
</sec>
<sec id="s3e2">
<title>Exp</title>
<p>The exponential-averaging model computes smoothed estimates of category probability by taking a weighted average of previously experienced category labels, giving more weight to recently experienced labels:
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="483842_eqn3.gif"/></alternatives>
</disp-formula>
where <italic>&#x03B1;</italic><sub>Exp</sub> is the smoothing factor, 0 <italic>&#x003C; &#x03B1;</italic><sub>Exp</sub> <italic>&#x003C;</italic> 1, and <italic>C</italic><sub><italic>t</italic></sub> = 1 if category A is observed on trial <italic>t</italic> and <italic>C</italic><sub><italic>t</italic></sub> = 0 otherwise. The time constant of memory decay for this model is <inline-formula><alternatives><inline-graphic xlink:href="483842_inline11.gif"/></alternatives></inline-formula>. Mathematically this model is equivalent to a delta-rule [<xref rid="c28" ref-type="bibr">28</xref>] model based on an &#x201C;error&#x201D; that is the difference between the current category and the current probability estimate: <inline-formula><alternatives><inline-graphic xlink:href="483842_inline12.gif"/></alternatives></inline-formula> The criterion <italic>z</italic> is then set to the optimal value based on this category-probability estimate.</p>
</sec>
<sec id="s3e3">
<title><bold>Exp</bold><sub><bold>bias</bold></sub></title>
<p>The Exp<sub>bias</sub> model is identical to the Exp model, while also incorporating a common finding in the literature [<xref rid="c2" ref-type="bibr">2</xref>] known as conservatism (i.e., observers are biased towards the neutral criterion). On each trial an estimate of probability is computed as described in (<xref ref-type="disp-formula" rid="eqn3">Eq 3</xref>), and averaged with a prior probability of 0.5:
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="483842_eqn4.gif"/></alternatives>
</disp-formula>
where <italic>w</italic> is the weight given to the probability estimate <inline-formula><alternatives><inline-graphic xlink:href="483842_inline13.gif"/></alternatives></inline-formula> and (1 - <italic>w</italic>) is the weight given to <italic>&#x03C0;</italic><sub>A</sub> = 0.5. The criterion <italic>z</italic> is also set to the optimal value based on this category-probability estimate.</p>
</sec>
<sec id="s3e4">
<title><xref ref-type="bibr" rid="c29">Wilson et al. (2013)</xref></title>
<p>Due to the complexity of the full Bayesian change-point detection model, Wilson et al. [<xref rid="c29" ref-type="bibr">29</xref>] developed an approximation to the full model using a mixture of delta rules. In their reduced model, the full run-length distribution is approximated by maintaining a subset of all possible run lengths. These are referred to as nodes &#x007B;<italic>l</italic><sub>1</sub>, <italic>&#x2026;, l</italic><sub><italic>i</italic></sub>&#x007D;. On each trial, an estimate of probability is computed for each node,
<disp-formula id="eqn5">
<alternatives><graphic xlink:href="483842_eqn5.gif"/></alternatives>
</disp-formula>
where <italic>&#x03BD;</italic><sub>p</sub> is a model parameter that represents the strength of the observer&#x2019;s prior towards equal category probability (pseudocounts of a Beta prior; larger <italic>&#x03BD;</italic><sub>p</sub> corresponds to stronger conservatism) and <italic>C</italic><sub><italic>t</italic></sub> is the category label. The learning rate for each node is thus <inline-formula><alternatives><inline-graphic xlink:href="483842_inline14.gif"/></alternatives></inline-formula> For a single-node model, this is identical to the Exp model described above (<xref ref-type="disp-formula" rid="eqn3">Eq 3</xref>). To obtain an overall estimate of probability in a multiple-node model, estimates (<xref ref-type="disp-formula" rid="eqn5">Eq 5</xref>) are combined by taking a weighted average. The weights, <italic>p</italic>(<italic>l</italic><sub><italic>i</italic></sub>&#x007C;<italic>C</italic><sub>1:<italic>t</italic></sub>), are updated on every trial. The update is dependent on an approximation to the change-point prior, which in turn depends on the hazard rate <italic>h</italic> (for details see Wilson et al. [<xref rid="c29" ref-type="bibr">29</xref>], Eqs 25-31, along with the later correction [<xref rid="c30" ref-type="bibr">30</xref>]). In other words, when the probability of a change is high, more weight is given to <italic>l</italic><sub>1</sub>, which results in a greater change in the overall probability estimate. But when the probability of a change is low, more weight is given to nodes greater than <italic>l</italic><sub>1</sub> and the probability estimate is more stable. For a three-node model probability is estimated as
<disp-formula id="eqn6">
<alternatives><graphic xlink:href="483842_eqn6.gif"/></alternatives>
</disp-formula>
</p>
<p>For the purpose of the current experiment, we used a three-node model in which the first node was fixed (<italic>l</italic><sub>1</sub> = 1) and <italic>l</italic><sub>2</sub> and <italic>l</italic><sub>3</sub> were allowed to vary. In addition, the prior strength parameter, <italic>&#x03BD;</italic><sub>p</sub>, which modulates the learning rate, was also free. By allowing <italic>&#x03BD;</italic><sub>p</sub> to vary, this model is equivalent to the three-node model described by Wilson and colleagues [<xref rid="c29" ref-type="bibr">29</xref>] in which all nodes were free and <italic>&#x03BD;</italic><sub>p</sub> = 2 was fixed. The hazard rate was set to 0.01 and we assumed a change occurred at <italic>t</italic> = 1, so that all the weight was given to <italic>l</italic><sub>1</sub>. We also tested an alternative model with a fixed value of <italic>&#x03BD;</italic><sub>p</sub> = 2, but it resulted in a worse fit.</p>
</sec>
<sec id="s3e5">
<title>RL</title>
<p>While the models described above make assumptions about how probability is estimated, it is also possible that observers simply update the decision criterion without estimating the current probability state. Reinforcement-learning is a model-free approach that assumes the observer does not use knowledge of the environmental structure. Instead, the decision criterion, rather than an estimate of category probability, is updated. The observer updates the internal criterion (<italic>z</italic>) on each trial according to the following delta rule:
<disp-formula id="eqn7">
<alternatives><graphic xlink:href="483842_eqn7.gif"/></alternatives>
</disp-formula>
</p>
<p>Thus, the criterion is updated when negative feedback is received by taking a small step in the direction of the difference between the noisy observation (<italic>x</italic><sub><italic>t</italic></sub>) and current criterion (<italic>z</italic><sub><italic>t</italic></sub>), where the step size is scaled by the learning rate <italic>&#x03B1;</italic><sub>RL</sub>. Nothing is changed after a correct response. Assuming effective training, we initialize the RL model by setting <inline-formula><alternatives><inline-graphic xlink:href="483842_inline15.gif"/></alternatives></inline-formula>.</p>
</sec>
</sec>
<sec id="s3f">
<title>Raw data</title>
<p><xref rid="fig2" ref-type="fig">Fig 2</xref> shows raw data for single observers in the covert (<xref rid="fig2" ref-type="fig">Fig 2A</xref>) and overt (<xref rid="fig2" ref-type="fig">Fig 2C</xref>) tasks. For visualization in the covert task, the &#x2018;excess&#x2019; number of A responses is plotted as a function of trial (gray line in <xref rid="fig2" ref-type="fig">Fig 2A</xref>). To compute the &#x2018;excess&#x2019; number of A responses, we subtracted <inline-formula><alternatives><inline-graphic xlink:href="483842_inline16.gif"/></alternatives></inline-formula> from the cumulative number of A responses. Thus, &#x2018;excess&#x2019; A responses are constant for an observer who reported A and B equally often, increase when A is reported more, and decrease when A is reported less. To get a sense of how well the observer performed in the covert task, the number of &#x2018;excess&#x2019; A trials (based on the actual category on each trial rather than the observer&#x2019;s response) is shown in black (<xref rid="fig2" ref-type="fig">Fig 2A</xref>, top). For reference, <italic>&#x03C0;</italic><sub>A</sub> is shown as a function of trial (<xref rid="fig2" ref-type="fig">Fig 2A</xref>, bottom). From visual inspection, the observer reported A more often when <italic>&#x03C0;</italic><sub>A,t</sub> <italic>></italic> 0.5 and B more often when <italic>&#x03C0;</italic><sub>A,t</sub> <italic>&#x003C;</italic> 0.5.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig 2.</label>
<caption><title>Behavioral data and model fits.</title>
<p>A: Behavioral data from a representative observer (CWG) in the covert-criterion task. Top: The number of &#x2018;excess&#x2019; A responses (i.e., the cumulative number of A responses minus <inline-formula><alternatives><inline-graphic xlink:href="483842_inline17.gif"/></alternatives></inline-formula>) across trials. Gray line: Observer data. Black line: The true number of &#x2018;excess&#x2019; A&#x2019;s. Bottom: The corresponding category probability. B: Model fits (colored lines) for observer CWG in the covert-criterion task. Shaded regions: 68&#x0025; CI for model fits. Gray: Observer data. C: Behavioral data from a representative observer (GK) in the overt-criterion task. Top: The orientation of the criterion line relative to the neutral criterion as a function of trial number. Gray circles: raw settings.vGray line: running average over a 5-trial moving window. Black line: ideal criterion for an observer with perfect knowledge of the experimental parameters. Bottom: Category probability across trials. D: Models fits (colored lines) for observer GK in the overt-criterion task. A running average was computed over a 5-trial window for visualization. Shaded regions: 68&#x0025; CI on model fits. These are generally smaller than the model-fit line. Gray line: the running average computed from the observer&#x2019;s data.</p></caption>
<graphic xlink:href="483842_fig2.tif"/>
</fig>
<p>In the overt task, the orientation of the observer&#x2019;s criterion setting, relative to the neutral criterion, is plotted as a function of trial (gray circles). For visualization, a running average was computed over a five-trial moving window (gray line). Here (<xref rid="fig2" ref-type="fig">Fig 2C</xref>, top), the black line represents the criterion on each trial, given perfect knowledge of the categories, sensory uncertainty, and category probability. While this is impossible for an observer to attain, we can see that the observer&#x2019;s criterion follows the general trend. This suggests that observers update their criterion appropriately in response to changes in probability. That is, the criterion is set counter-clockwise from the neutral criterion when <italic>&#x03C0;</italic><sub>A,t</sub> <italic>></italic> 0.5, and clockwise of neutral when <italic>&#x03C0;</italic><sub>A,t</sub> <italic>&#x003C;</italic> 0.5.<xref rid="fig2" ref-type="fig">Fig 2C</xref> (bottom) shows <italic>&#x03C0;</italic><sub>A</sub> as a function of trial.</p>
</sec>
</sec>
<sec id="s4">
<title>Modeling results</title>
<sec id="s4a">
<title>Model predictions</title>
<p>A qualitative comparison of the behavioral data to the ground truth suggests that observers updated their criterion in response to changes in probability. However, it does not tell us how. To explore the mechanism underlying these changes, we compared the observers&#x2019; data to multiple models. For each task and model, the mean model response for a representative subject is plotted in <xref rid="fig2" ref-type="fig">Fig 2B</xref> (covert task) and 2D (overt task). Shaded regions indicate 68&#x0025; CIs computed from the posterior over model parameters. Specifically, we sampled parameters from the posterior over model parameters and computed the model response for the given set of parameters. We then computed the standard deviation across model responses for a large sample (see Model visualization). Shaded regions computed on overt fits are generally narrower than the data line. Qualitatively, most models captured observers&#x2019; changing criteria, with the fixed model being much worse. Differences across models are especially pronounced in the overt task. Specifically, we see that the Exp, Exp<sub>bias</sub>, and <xref ref-type="bibr" rid="c29">Wilson et al. (2013)</xref> models capture changes in criterion that occur between change points that the Bayesian models fail to capture.</p>
</sec>
</sec>
<sec id="s5">
<title>Model comparison</title>
<p>We quantitatively compared models by computing the log marginal likelihood (LML), also known as log Bayes factor, for each subject, model, and task. The marginal likelihood is the probability of the data given the model, marginalized over model parameters (see Methods). Here, we report differences in log marginal likelihood (&#x2206;LML) from the best model, so that larger &#x2206;LML correspond to worse models. We compare models both by looking at average performance across subjects (a fixed-effects analysis), and also via a Bayesian Model Selection approach (BMS; [<xref rid="c31" ref-type="bibr">31</xref>]) in which subjects are treated as random variables. With BMS, we estimate for each model its posterior frequency <italic>f</italic> in the population and its protected exceedance probability <italic>&#x03D5;</italic>, which is the probability that a given model is the most frequent model in the population, above and beyond chance [<xref rid="c32" ref-type="bibr">32</xref>].</p>
<p>Model comparison (<xref rid="fig3" ref-type="fig">Fig 3</xref>) favored the Exp<sub>bias</sub>model, which outperformed the second best model Bayes<sub><italic>r</italic></sub> (covert task: &#x2206;LML = 9.27 &#x00B1; 2.86; overt task: &#x2206;LML = 8.96 &#x00B1; 4.01; mean and SEM across observers) in the two tasks (covert task: <italic>t</italic>(10) = 3.99, <italic>p</italic> = 0.003; overt task: <italic>t</italic>(10) = 2.37, <italic>p</italic> = 0.04). Similarly, Bayesian model comparison performed at the group level also favored the Exp<sub>bias</sub> model (covert task: <italic>f</italic> = 0.46 and <italic>&#x03D5;</italic> = 0.96; overt task: <italic>f</italic> = 0.38 and <italic>&#x03D5;</italic> = 0.84). These results suggests that observers estimate probability by taking a weighted average of recently experienced categories with a bias towards <italic>&#x03C0;</italic> = 0.5.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig 3.</label>
<caption><title>Model comparison.</title>
<p>A: Average log marginal likelihood (LML) scores relative to the winning model, Exp<sub>bias</sub> (top: covert task; bottom: overt task). Lower scores indicate a better fit. Error bars: 95&#x0025; CI (bootstrapped). B: Bayesian model selection at the group level. The protected exceedance probability (<italic>&#x03D5;</italic>) is plotted for each model. Models are stacked in order of decreasing probability.</p></caption>
<graphic xlink:href="483842_fig3.tif"/>
</fig>
<p><xref rid="fig4" ref-type="fig">Fig 4A</xref> shows the number of observers that were best fit by each model for each task. To compare LML scores across tasks, and for the purpose of this analysis only, we standardized model scores for each observer and task. Standardized LML scores in the overt task are plotted as a function of standardized LML scores in the covert task in <xref rid="fig4" ref-type="fig">Fig 4B</xref>. We found a significant positive correlation, <italic>r</italic> = 0.62, <italic>p &#x003C;</italic> 0.01, indicating that models with higher LML scores in the covert task were also higher in the overt task. This result suggests that strategy was fairly consistent across tasks. In addition, there was more variance in model scores for worse-fitting models.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig 4.</label>
<caption><title>Modeling results for individual observers.</title><p>A: Model frequency. The number of observers best fit by each model plotted for each task. Models are stacked in order of decreasing frequency. B: Comparison of LML scores across tasks. LML scores were standardized for each observer and task. Standardized LML scores in the overt task are plotted as a function of standardized LML scores in the covert task (colored data points). Black dashed line: identity line. models are shown in <xref rid="tbl1" ref-type="table">Tables 1</xref> and <xref rid="tbl2" ref-type="table">2</xref>.</p></caption>
<graphic xlink:href="483842_fig4.tif"/>
</fig>
</sec>
<sec id="s6">
<title>Model parameters</title>
<p>We examine here the parameter estimates of the best-fitting model, Exp<sub>bias</sub>, recalling that the two model parameters <italic>&#x03B1;</italic><sub>Exp</sub> and <italic>w</italic> represent, respectively, the exponential smoothing factor and the degree to which observers exhibit conservatism. The <italic>maximum a posteriori</italic> (MAP) parameter estimates are plotted in <xref rid="fig5" ref-type="fig">Fig 5</xref>. Converting the smoothing factor to a time constant, we found that the time constant in both tasks was well below the true rate of change (covert:<italic>&#x03C4;</italic> = [4.24, 7.18]; overt: <italic>&#x03C4;</italic> = [3.48, 4.75]). We conducted paired-sample <italic>t</italic>-tests to compare the raw parameter estimates in the covert and overt tasks. We found a significant difference in <italic>w</italic> (<italic>t</italic>(10) = &#x2212;2.55, <italic>p</italic> = 0.03), suggesting that observers were more conservative in the covert than the overt task. No significant difference was found for <italic>&#x03B1;</italic><sub>Exp</sub> (<italic>t</italic>(10) = &#x2212;0.98, <italic>p</italic> = 0.35).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><p><bold>Maximum a posteriori parameter estimates &#x00B1;S.E. in the covert-criterion task.</bold></p></caption>
<graphic xlink:href="483842_tbl1.tif"/>
</table-wrap>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><p><bold>Maximum a posteriori parameter estimates &#x00B1;S.E. in the overt-criterion task.</bold></p></caption>
<graphic xlink:href="483842_tbl2.tif"/>
</table-wrap>
<fig id="fig5" position="float" fig-type="figure">
<label>Fig 5.</label>
<caption><title>Parameter estimates for the best fitting model.</title>
<p>A: Mean <italic>&#x03B1;</italic><sub>exp</sub> (top) and <italic>w</italic> (bottom) MAP values across observers. The &#x002A; denotes significance at the <italic>p &#x003C;</italic> 0.05 level. Error bars: &#x00B1;SEM. B: Individual <italic>&#x03B1;</italic><sub>exp</sub> (top) and <italic>w</italic> (bottom) MAP values from fits in the overt task as a function of MAP values from fits in the covert task. Black dashed lines: identity line.</p></caption>
<graphic xlink:href="483842_fig5.tif"/>
</fig>
<p>To investigate whether there was bias in the parameter-estimation procedure when fitting the Exp<sub>bias</sub> model, we also conducted a parameter-recovery analysis. Most parameters could be recovered correctly, except for adjustment variability (<italic>&#x03C3;</italic><sub>a</sub>) in the overt task, which we found to be overestimated on average (see Section 4 in S1 Appendix for details). Note that this bias in estimating <italic>&#x03C3;</italic><sub>a</sub> in the overt task does not affect our model comparison, which is based on LMLs and not on point estimates.</p>
<p>While we might expect performance to be similar across tasks and observers (i.e., a correlation between the parameter fits in each task), no significant correlations were found (<italic>&#x03B1;</italic><sub>Exp</sub>: <italic>r</italic> = &#x2212;0.14, <italic>p</italic> = 0.67; <italic>w</italic>: <italic>r</italic> = 0.16, <italic>p</italic> = 0.64). Parameter estimates for all.</p>
</sec>
<sec id="s7">
<title>Discussion</title>
<p>Although we know that people update decision criteria in response to explicit changes in prior probability, the effects of implicit changes in prior probability on decision-making behavior are less well known. In the present study, we used model comparison to investigate the mechanisms underlying decision-making behavior in an orientation-categorization task as prior probability changed. We tested a set of models that varied in both computational and memory demands. Models were tested on data from both a covert- and overt-criterion task. A comprehensive approach, consisting of both qualitative and quantitative analysis, was performed to determine the best fitting model. We found that observers updated their decision criterion following changes in probability. Additionally, we observed systematic changes in the decision criterion during periods of stability, which was clearly evident in the overt-criterion data. Model comparison favored an exponential-averaging model with a bias towards equal probability. This suggests that observers update the decision criterion by combining on-line estimation of probability with an equal-probability prior. Ultimately, our results help explain decision-making behavior in situations in which people need to assess the probability of an outcome based on previous experience.</p>
<sec id="s7a">
<title>Criterion updates in response to implicit changes in category probability</title>
<p>To determine the influence of prior probability on decision-making behavior, we examined changes in the decision criterion. First, we found that no participant was best fit by a fixed-criterion model. This finding suggests that observers update decision criteria in response to implicit changes in probability. This result is consistent with previous studies in which prior probability was explicit [<xref rid="c2" ref-type="bibr">2</xref>&#x2013;<xref rid="c9" ref-type="bibr">9</xref>]. Further, this finding complements recent studies suggesting that individuals can learn and adapt to statistical regularities in changing environments [<xref rid="c14" ref-type="bibr">14</xref>&#x2013;<xref rid="c17" ref-type="bibr">17</xref>, <xref rid="c24" ref-type="bibr">24</xref>, <xref rid="c25" ref-type="bibr">25</xref>, <xref rid="c33" ref-type="bibr">33</xref>, <xref rid="c34" ref-type="bibr">34</xref>]. Although this finding suggests that observers dynamically adjust decision criteria in response to changes in prior probability, it does not tell us how they do this (e.g., do observers compute on-line estimates of probability?). To uncover the mechanisms underlying changes in decision-making behavior, we compared multiple models ranging from the full Bayesian change-point detection model to a model-free reinforcement-learning (RL) model.</p>
</sec>
<sec id="s7b">
<title>Systematic criterion fluctuations</title>
<p>How is the decision criterion set? Qualitatively, most models appear to fit the data reasonably well in the covert task. However, when we look at data from the overt task, while Bayesian models captured the overall trend, they failed to capture local fluctuations in the decision criterion observed during periods of stability (i.e., time intervals between change points). In other words, the criterion predicted by the Bayesian models stabilized whereas the observers&#x2019; behavior did not. In contrast to the Bayesian models, the exponential models continually update the observer&#x2019;s estimate of probability based on recently experienced categories. How quickly observers updated this estimate is determined in the model by the decay-rate parameter. From our model fits to the data, we found that observers had an average decay rate that was substantially smaller than the true run length distribution (4.5 vs. 100 trials, respectively), leading to frequent, systematic fluctuations in decision criteria. Although we cannot directly observe these fluctuations in the covert task, because the estimated decay rate was not significantly different across tasks we can assume the fluctuations occurred in a similar manner. Like the exponential models, the RL model was also able to capture local fluctuations in the decision criterion. However, the amplitude of the changes in criterion predicted by the RL model was generally too low compared to the data. This discrepancy was especially clear in the overt task; no participant was best fit by the RL model. These results have two important implications. (1) It is important to test alternatives to Bayesian models: observers&#x2019; behavior might be explained without requiring an internal representation of probability. (2) Using multiple tasks can provide additional insight into behavior. Here, the fluctuations in decision criteria between change points led to suboptimal behavior. Overall, our findings suggest that suboptimality arose from an incorrect, possibly heuristic inference process, that goes beyond mere sensory noise [<xref rid="c35" ref-type="bibr">35</xref>&#x2013;<xref rid="c37" ref-type="bibr">37</xref>].</p>
</sec>
<sec id="s7c">
<title>A dual mechanism for criterion updating</title>
<p>While a number of the models captured local fluctuations in the decision criterion, we found that the Exp<sub>bias</sub> model fit the data best according to a quantitative model comparison. This result suggests that observers compute on-line estimates of category probability based on recent experience. Further, the bias component of the model suggests that observers are conservative, as reflected in a long-term prior that categories are equally likely. The degree to which observers weight this prior varied across individuals and tasks. Taken together, these results suggest a dual mechanism for learning and incorporating prior probability into decisions. That is, there are (at least) two components to decision making that are acquired and updated at very different timescales.</p>
<p>Multiple-mechanism models have been used to describe behavior in decision-making [<xref rid="c29" ref-type="bibr">29</xref>] and motor behavior [<xref rid="c38" ref-type="bibr">38</xref>]. A model that combines delta rules predicts motor behavior better than either delta rule alone [<xref rid="c38" ref-type="bibr">38</xref>]. Using a combination of delta rules [<xref rid="c29" ref-type="bibr">29</xref>], we were able to capture the local fluctuations in criterion that the full Bayesian model missed. However, we found that a constant weight on <italic>&#x03C0;</italic> = 0.5 fit better than the multiple-node model described by Wilson and colleagues [<xref rid="c29" ref-type="bibr">29</xref>]. Temporal differences between their task and ours might explain some of the differences we observed, as changes occurred much more slowly in our experiment. Additionally, while fitting Wilson et al.&#x2019;s model we set the hazard rate to 0.01 (the average rate of change), but observers had to learn this value throughout the experiment and may have had incorrect assumptions about the rate of change [<xref rid="c21" ref-type="bibr">21</xref>, <xref rid="c22" ref-type="bibr">22</xref>].</p>
</sec>
<sec id="s7d">
<title>Explanations of conservatism</title>
<p>While we observed conservatism in both the covert- and overt-criterion tasks, we found that, on average, observers were significantly more conservative in the covert task. To understand why conservatism differs across tasks, we need to understand the differences between the tasks. While the generative model was identical across tasks, the observer&#x2019;s response differed. In the covert task, observers chose between two alternatives. In the overt task, observers selected a decision criterion. This is an important difference because it allows us to potentially rule out previous explanations of conservatism, such as the use of subjective probability [<xref rid="c4" ref-type="bibr">4</xref>], misestimation of the relative frequency of events [<xref rid="c39" ref-type="bibr">39</xref>, <xref rid="c40" ref-type="bibr">40</xref>], and incorrect assumptions about the sensory distributions [<xref rid="c41" ref-type="bibr">41</xref>, <xref rid="c42" ref-type="bibr">42</xref>]; these explanations predict similar levels of conservatism across tasks. On the other hand, conservatism may be due to the use of suboptimal decision rules. Probability matching is a strategy in which participants select alternatives proportional to their probability, and has been used to explain suboptimal behavior in forced-choice tasks in which observers choose between two or more alternatives [<xref rid="c6" ref-type="bibr">6</xref>, <xref rid="c43" ref-type="bibr">43</xref>&#x2013;<xref rid="c46" ref-type="bibr">46</xref>]. Thus, the higher levels of conservatism in the covert task may have been due to the use of a suboptimal decision rule like probability matching, which would effectively smooth the observer&#x2019;s response probability across trials. Probability matching is not applicable to responses in the overt task. Thus, the use of different decision rules may result in different levels of conservatism. These differences may also arise from an increase in uncertainty in the covert task due to less explicit feedback. An observer with greater uncertainty will rely more on the prior. Thus, conservatism may be the result of having a prior over criteria that interacts with task uncertainty. This can be tested by manipulating uncertainty over the generative model and measuring changes in conservatism. It is also possible that conservatism is the result of both the use of suboptimal decision rules and one or more of the previously proposed explanations.</p>
</sec>
<sec id="s7e">
<title>Incorrect assumptions about the generative model</title>
<p>While we tested a number of Bayesian models that explored an array of assumptions about the generative model, clearly one could propose even more variants. In particular, we only analyzed one such assumption at a time. A simple way to expand the model space is via a factorial comparison [<xref rid="c36" ref-type="bibr">36</xref>, <xref rid="c47" ref-type="bibr">47</xref>], which we did not consider here due to computational intractability and the combinatorial explosion of models. Most notably, for all except the RL model we assumed knowledge of the category distributions. However, Norton et al. [<xref rid="c16" ref-type="bibr">16</xref>] found that for the same orientation-categorization task, category means were estimated dynamically, even after prolonged training. Similarly, Gifford et al. [<xref rid="c48" ref-type="bibr">48</xref>] observed suboptimality in an auditory-categorization task and found that the data were best explained by a model with non-stationary categories and prior probability that was updated using the recent history of category exemplars. This occurred despite holding categories and probability constant within a block. In fact, similar effects of non-stationarity have been observed in several other studies [<xref rid="c49" ref-type="bibr">49</xref>&#x2013;<xref rid="c51" ref-type="bibr">51</xref>]. In addition to non-stationary category means, observers may also have misestimated category variance [<xref rid="c52" ref-type="bibr">52</xref>], especially since learning category variance takes longer than learning category means [<xref rid="c14" ref-type="bibr">14</xref>].</p>
</sec>
</sec>
<sec id="s8">
<title>Conclusion</title>
<p>In sum, our results provide a computational model for how decision-making behavior changes in response to implicit changes in prior probability. Specifically, they suggest a dual mechanism for learning and incorporating prior probability that operate at different timescales. Importantly, this helps explain behavior in situations in which assessment of probability is learned through experience. Further, our results demonstrate the need to compare multiple models and the benefit of using tasks that provide a richer, more informative dataset.</p>
</sec>
<sec id="s9">
<title>Materials and methods</title>
<sec id="s9a">
<title>Participants</title>
<p>Eleven observers participated in the experiment (mean age 26.6, range 20-31, 8 females). All observers had normal or corrected-to-normal vision. One of the observers (EHN) was also an author. The Institutional Review Board at New York University approved the experimental procedure and observers gave written informed consent prior to participation.</p>
</sec>
<sec id="s9b">
<title>Apparatus and stimuli</title>
<p>Stimuli were presented on a gamma-corrected Dell Trinitron P780 CRT monitor with a 31.3 x 23.8&#x00B0; display, a resolution of 1024 x 768 pixels, a refresh rate of 85 Hz, and a mean luminance of 40 cd<italic>/</italic>m<sup>2</sup>. Observers viewed the display from a distance of 54.6 cm. The experiment was programmed in MATLAB [<xref rid="c53" ref-type="bibr">53</xref>] using the Psychophysics Toolbox [<xref rid="c54" ref-type="bibr">54</xref>, <xref rid="c55" ref-type="bibr">55</xref>].</p>
<p>Stimuli were 4.0 &#x00D7; 1.0&#x00B0; ellipses presented at the center of the display on a mid-gray background. In both the orientation-discrimination and covert-criterion tasks, trials began with a central white fixation cross (1.2&#x00B0;). In the overt-criterion task, a yellow line with random orientation was presented at the center of the display (5.0 &#x00D7; 0.5&#x00B0;).</p>
</sec>
<sec id="s9c">
<title>Procedure</title>
<sec id="s9c1">
<title>Categories</title>
<p>In the &#x2018;categorization&#x2019; sessions described below, stimulus orientations were drawn from one of two categories (A or B). Category distributions were Gaussian with different means (<italic>&#x00B5;</italic><sub>B</sub> <italic>> &#x00B5;</italic><sub>A</sub>) and equal variance <inline-formula><alternatives><inline-graphic xlink:href="483842_inline18.gif"/></alternatives></inline-formula>. The mean of category A was chosen randomly from all possible orientations at the beginning of each session and the mean of category B was set so that <italic>d</italic>&#x2019; was approximately 1.5, which was determined for each observer based on the estimates of sensory uncertainty obtained during a &#x2018;measurement session&#x2019; (<xref rid="fig1" ref-type="fig">Fig 1C</xref>).</p>
</sec>
<sec id="s9c2">
<title>Sessions</title>
<p>All observers participated in three 1-hour sessions. Observers completed a &#x2018;measurement&#x2019; session first followed by two &#x2018;categorization&#x2019; sessions. Observers completed the covert-criterion task in the first &#x2018;categorization&#x2019; session followed by the overt-criterion task in the second, or vice versa (chosen randomly). At the beginning of each &#x2018;categorization&#x2019; session, observers completed 200 trials of category training followed by 800 experimental trials. Prior to training, observers were provided with a detailed explanation of the category distributions and training task. After training, observers were provided with additional instructions about the subsequent &#x2018;categorization&#x2019; task and told that the categories would remain constant for the remainder of the session but that category probability may change. Observers were not told how often category probability would change or the range of probability states</p>
</sec>
</sec>
<sec id="s9d">
<title>Measurement task</title>
<p>During the &#x2018;measurement&#x2019; session, sensory uncertainty (<italic>&#x03C3;</italic><sub>v</sub>) was estimated using a two-interval, forced-choice, orientation-discrimination task in which two black ellipses were presented sequentially on a mid-gray background. The observer reported the interval containing the ellipse that was more clockwise by keypress. Once the response was recorded, auditory feedback was provided and the next trial began. An example trial sequence is shown in Fig S3A in S1 Appendix.</p>
<p>The orientation of the ellipse in the first interval was chosen randomly on every trial from a uniform distribution ranging from &#x2212;90 to 90&#x00B0;. The orientation of the second ellipse was randomly oriented clockwise or counter-clockwise of the first. The difference in orientation between the two ellipses was selected using an adaptive staircase procedure. The minimum step-size was 1&#x00B0; and the maximum step-size was 32&#x00B0;. Each observer ran two blocks. In each block, four staircases (65 trials each) were interleaved (two 1-up, 2-down and two 1-up, 3-down staircases) and randomly selected on each trial. For analyses and results see S1 Appendix.</p>
</sec>
<sec id="s9e">
<title>Category training</title>
<p>Each training trial was identical to a covert-criterion trial (<xref rid="fig1" ref-type="fig">Fig 1A</xref>). During training there was an equal chance that a stimulus was drawn from either category. To assess learning of category distributions, observers were asked to estimate the mean orientation of each category following training. The mean of each category was estimated exactly once. The order in which category means were estimated was randomized. For estimation, a black ellipse with random orientation was displayed in the center of the display. Observers slid the mouse to the right and left to rotate the ellipse clockwise and counterclockwise, respectively and clicked the mouse to indicate they were satisfied with the setting. No feedback was provided. We computed the proportion correct for each observer to ensure category learning by comparing it to the expected proportion correct (<italic>p</italic>(correct) = 0.77) for <italic>d</italic><sup>&#x2032;</sup> = 1.5. Mean estimates are plotted in Fig S4C in S1 Appendix as a function of the true category means. We computed the average estimation error for each category and observer by subtracting the estimate from the true mean. From visual inspection, it appears that training was effective with the exception of one outlier, which we assume was a lapse.</p>
</sec>
<sec id="s9f">
<title>Categorization tasks</title>
<sec id="s9f1">
<title>Covert-criterion task</title>
<p>In the covert-criterion task, observers categorized ellipses based on their orientation. The start of each trial (<italic>N</italic><sub>trials</sub> = 800) was signaled by the appearance of a central white fixation cross (500 ms). A black oriented ellipse was then displayed at the center of the screen (300 ms). Observers categorized the ellipse as A or B by keypress. Observers received feedback as to whether they were correct on every trial. Observers received a point for every correct response and aggregate points were displayed at the top of the screen to motivate observers. In addition, the fixation cross was displayed at the center of the screen in the color corresponding to the true category (category A: green; category B: red). The next trial began immediately. An example trial sequence is depicted in <xref rid="fig1" ref-type="fig">Fig 1A</xref>.</p>
</sec>
<sec id="s9f2">
<title>Overt-criterion</title>
<p>task In the overt-criterion task, observers completed an explicit version of the categorization task described above that was developed by Norton et al. [<xref rid="c16" ref-type="bibr">16</xref>]. At the beginning of each trial (<italic>N</italic><sub>trials</sub> = 800), a line was displayed at the center of the screen. The orientation of the line was randomly selected from a uniform distribution ranging from &#x2212;90 to 90&#x00B0;. The observers&#x2019; task was to rotate the line to indicate the criterion for that trial. Observers rotated the line clockwise or counterclockwise by sliding the mouse to the right or left and clicked the mouse to indicate their setting. Next, an ellipse was displayed under the criterion line in the color corresponding to the true category for 300 ms. Auditory feedback indicated whether the set criterion correctly categorized the ellipse. That is, observers were correct when a category A stimulus was clockwise of the criterion line or a category B stimulus was counterclockwise of the line. Observers received a point for a correct response and aggregate points were displayed at the top of the screen. The next trial began immediately. An example trial sequence is depicted in <xref rid="fig1" ref-type="fig">Fig 1B</xref>.</p>
</sec>
</sec>
<sec id="s9g">
<title>Model fitting</title>
<p>For fitting, all models had one free noise parameter. In the covert-criterion task, this was sensory noise (<bold><italic>&#x03C3;</italic></bold><sub>v</sub>). In the overt-criterion task, sensory noise was fixed and set to the value obtained in the &#x2018;measurement&#x2019; session, but we included a noise parameter for the adjustment of the criterion line (<italic>&#x03C3;</italic><sub>a</sub>). Fixing one noise parameter in the overt-criterion task ameliorated potential issues of lack of parameter identifiability [<xref rid="c56" ref-type="bibr">56</xref>], and ensured that models had the same complexity across tasks. The Bayes<sub>ideal</sub> and Fixed models had no additional parameters. Each suboptimal Bayesian model had one additional parameter: Bayes<sub><italic>r</italic></sub> (<italic>r</italic>); Bayes<sub><italic>&#x03C0;</italic></sub> (<italic>&#x03C0;</italic><sub>min</sub>); Bayes<sub><italic>&#x03B2;</italic></sub> (<italic>&#x03B2;</italic>). The Exp and RL models also only had one additional parameter (<italic>&#x03B1;</italic>). The Exp<sub>bias</sub> had two additional parameters (<italic>&#x03B1;</italic><sub>exp</sub> and <italic>w</italic>), and the <xref ref-type="bibr" rid="c29">Wilson et al. (2013)</xref> model had three (<italic>l</italic><sub>2</sub>, <italic>l</italic><sub>3</sub>, and <italic>&#x03BD;</italic><sub>p</sub>).</p>
<p>To fit each model, for each subject and task we computed the logarithm of the <italic>unnormalized</italic> posterior probability of the parameters,
<disp-formula id="eqn8">
<alternatives><graphic xlink:href="483842_eqn8.gif"/></alternatives>
</disp-formula>
where data are category decisions in the covert-criterion task and criterion orientation in the overt-criterion task, M is a specific model, and &#x03B8; represents the model parameters (generally, a vector). The first term of <xref ref-type="disp-formula" rid="eqn8">Eq 8</xref> is the log likelihood, while the second term is the prior over parameters (see below).</p>
<p>We evaluated <xref ref-type="disp-formula" rid="eqn8">Eq 8</xref> on a cubic grid, with bounds chosen to contain almost all posterior probability mass. The grid for all models, except <xref ref-type="bibr" rid="c29">Wilson et al. (2013)</xref>, consisted of 100 equally spaced values for each parameter. Due to the computational demands of the <xref ref-type="bibr" rid="c29">Wilson et al. (2013)</xref> model, we reduced the grid to 50 equally spaced values. The grid allowed us to approximate the full posterior distribution over parameters, and also to evaluate the normalization constant for the posterior, which corresponds to the evidence or marginal likelihood, used as a metric of model comparison (see Model comparison). We reported as parameter estimates the best-fitting model parameters on the grid, that is the maximum-a-posteriori (MAP) values (see <xref rid="tbl1" ref-type="table">Tables 1</xref> and <xref rid="tbl2" ref-type="table">2</xref>). We used the full posterior distributions to compute posterior predictive distributions, that is, model predictions for visualization (see Model visualization), and to generate plausible parameter values for our model-recovery analysis.</p>
<sec id="s9g1">
<title>Priors over parameters</title>
<p>We chose all priors to be uninformative. For noise parameters, inspired by the Jeffreys prior for scale parameters, we used uniform priors in log space over a reasonably large range ([0&#x00B0; 3.4<italic>&#x00B0;</italic>]) [<xref rid="c57" ref-type="bibr">57</xref>]. For <italic>&#x03B1;</italic><sub>exp</sub>, <italic>&#x03B1;</italic><sub>RL</sub>, and <italic>w</italic> we used a uniform prior from 0 to 1. For <italic>&#x03BD;</italic> we used a uniform prior from 2 to 200 trials. For <italic>&#x03C0;</italic><sub>min</sub> we used a uniform prior from 0 to 0.5. For <italic>&#x03B2;</italic>, we used a uniform prior on the square root of the parameter value, ranging from 0 to 10. Instead of fitting the individual nodes in the <xref ref-type="bibr" rid="c29">Wilson et al. (2013)</xref> model, we fit the difference between nodes, i.e., <italic>&#x03B4;</italic><sub>1</sub> = <italic>l</italic><sub>2</sub> &#x2212; <italic>l</italic><sub>1</sub> and <italic>&#x03B4;</italic><sub>2</sub> = <italic>l</italic><sub>3</sub> &#x2212; <italic>l</italic><sub>2</sub>. We used a uniform prior on the square root of <italic>&#x03B4;</italic><sub>1</sub> ranging from 1.01 to 5 and on the square root of <italic>&#x03B4;</italic><sub>2</sub> ranging from 1.01 to 14. Finally, for <italic>&#x03BD;</italic><sub>p</sub> we used a uniform prior in log space from 0 to 5.</p>
</sec>
</sec>
<sec id="s9h">
<title>Response probability</title>
<sec id="s9h1">
<title>Covert-criterion task</title>
<p>For each model, parameter combination, observer, and trial in the covert-criterion task, we computed the probability of choosing category A on each trial given a stimulus, <italic>s</italic><sub><italic>t</italic></sub>, and all previously experienced categories, C<sub>1:<italic>t</italic>-1</sub>. In all models, the observer&#x2019;s current decision depends on the noisy measurement, <italic>x</italic><sub><italic>t</italic></sub>, so the probability of responding A for a given stimulus <italic>s</italic><sub><italic>t</italic></sub> is
<disp-formula id="eqn9">
<alternatives><graphic xlink:href="483842_eqn9.gif"/></alternatives>
</disp-formula>
</p>
<p>Because the current criterion setting in the RL model depends on the vector of all previous stimulus measurements, x<sub>1:<italic>t</italic></sub>, the probability could not be computed analytically for this model. As an approximation, we used Monte Carlo simulations with 5000 sample measurement vectors. For each measurement vector, we applied the model&#x2019;s decision rule and approximated the probability by computing the proportion of times the model chose A out of all the simulations. For all models, we included a fixed lapse rate, <italic>&#x03BB;</italic> = 10<sup>-4</sup>, that is the probability of a completely random response. The probability of choosing category A, <inline-formula><alternatives><inline-graphic xlink:href="483842_inline19.gif"/></alternatives></inline-formula>, in the presence of lapses was then
<disp-formula id="eqn10">
<alternatives><graphic xlink:href="483842_eqn10.gif"/></alternatives>
</disp-formula>
</p>
<p>Effectively, the lapse rate acts as a regularization term that avoids excessive penalties to the likelihood of a model for outlier trials.</p>
<p>Next, assuming conditional independence between trials, we computed the log likelihood across all of the observer&#x2019;s choices, given each model and parameter combination
<disp-formula id="eqn11">
<alternatives><graphic xlink:href="483842_eqn11.gif"/></alternatives>
</disp-formula>
where <italic>t</italic> is the trial index and <italic>N</italic><sub>trials</sub> is the total number of trials.</p>
</sec>
<sec id="s9h2">
<title>Overt-criterion task</title>
<p>For each model, parameter combination, observer, and trial in the overt-criterion task, we computed the decision criterion on each trial. For all models except the RL model, the criterion was computed as in S1 Appendix. For the RL model, the criterion was computed as in <xref ref-type="disp-formula" rid="eqn7">Eq 7</xref> for 5000 sample measurement vectors. For all models in the overt-criterion task, the criterion was corrupted by adjustment noise with variance <inline-formula><alternatives><inline-graphic xlink:href="483842_inline20.gif"/></alternatives></inline-formula>, so that <inline-formula><alternatives><inline-graphic xlink:href="483842_inline21.gif"/></alternatives></inline-formula> where <italic>z</italic><sub><italic>t</italic></sub> was the observer&#x2019;s chosen criterion at trial <italic>t</italic>, and <inline-formula><alternatives><inline-graphic xlink:href="483842_inline22.gif"/></alternatives></inline-formula> was the actual reported criterion after adjustment noise. In addition, the observer had a chance of lapsing (e.g., a misclick), in which case the response was uniformly distributed in the range. Therefore, the probability that the observer reports the criterion <inline-formula><alternatives><inline-graphic xlink:href="483842_inline23.gif"/></alternatives></inline-formula> was
<disp-formula id="eqn12">
<alternatives><graphic xlink:href="483842_eqn12.gif"/></alternatives>
</disp-formula>
with <italic>&#x03BB;</italic> = 5 <italic>&#x00D7;</italic> 10<sup>-5</sup>. As in the covert-criterion task, we computed the log likelihood across all trials by summing the log probability
<disp-formula id="eqn13">
<alternatives><graphic xlink:href="483842_eqn13.gif"/></alternatives>
</disp-formula>
</p>
</sec>
<sec id="s9h3">
<title>Model comparison</title>
<p>To obtain a quantitative measure of model fit, for each observer, model, and task we computed the log marginal likelihood (LML) by integrating over the parameters in Eq 8,
<disp-formula id="eqn14">
<alternatives><graphic xlink:href="483842_eqn14.gif"/></alternatives>
</disp-formula>
</p>
<p>To approximate the integral in <xref ref-type="disp-formula" rid="eqn14">Eq 14</xref>, we marginalized across each parameter dimension using the trapezoidal method. Assuming equal probability across models, the marginal likelihood was proportional to the posterior probability of a given model and thus represents a principled metric for comparison that automatically accounts for both goodness of fit and model complexity via Bayesian Occam&#x2019;s razor [<xref rid="c58" ref-type="bibr">58</xref>].</p>
<p>Penalizing for model complexity is a desirable feature of a model-comparison metric to reduce overfitting.</p>
<p>In addition to the Bayesian model-comparison metric described above, we computed the Akaike Information Criterion (AIC) [<xref rid="c59" ref-type="bibr">59</xref>] for each of our models. AIC is one of many information criteria that penalize the maximum log likelihood by a term that increases with the number of parameters. LML and AIC results were consistent (see S1 Appendix for model comparison using AIC scores).</p>
<p>For comparison purposes, we report relative model-comparison scores, 2206LML and &#x2206;AIC. We used bootstrapping to compute confidence intervals on the mean difference scores. Specifically, we simulated 10,000 sample data sets. For each simulated dataset we sampled, with replacement, 11 difference scores (the same number of difference scores as observers) and calculated the mean. To determine the 95&#x0025; CI, we sorted the mean difference scores and determined the scores that corresponded to the 2.5 and percentiles.</p>
<p>For an additional analysis at the group level, we used the random-effects Bayesian model selection analysis (BMS) developed by Stephan et al. [<xref rid="c31" ref-type="bibr">31</xref>] and expanded on by Rigoux et al. [<xref rid="c32" ref-type="bibr">32</xref>]. Specifically, using observers&#x2019; LML scores we computed the protected exceedance probability <italic>&#x03D5;</italic> and the posterior model frequency for each model. Exceedance probability represents the probability that one model is the most frequent decision-making strategy in the population, given the group data, above and beyond chance. This analysis was conducted using the open-source software package Statistical Parametric Mapping (SPM12; <ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</ext-link>).</p>
</sec>
<sec id="s9h4">
<title>Model visualization</title>
<p>For each model, observer, and task, we randomly sampled 1000 parameter combinations from the joint posterior distribution with replacement. For each parameter combination, we simulated model responses using the same stimuli that were presented to the observer. Because the model output in the covert task was the probability of reporting category A, for each trial in a simulated dataset we simulated 10,000 model responses (i.e., category decisions), calculated the cumulative number of A&#x2019;s for each simulated dataset, and averaged the results. The mean and standard deviation were computed across all simulated datasets in both tasks. Model fit plots show the mean response (colored line) with shaded regions representing one standard deviation from the mean. Thus, shaded regions represent a 68&#x0025; confidence interval on model fits.</p>
</sec>
<sec id="s9h5">
<title>Model recovery</title>
<p>To ensure that our models were discriminable, we performed a model-recovery analysis, details of which can be found in S1 Appendix. In addition to the model-recovery analysis, we also performed a parameter-recovery analysis for the winning model (Exp<sub>bias</sub>). This was done to determine whether our parameter estimation procedure was biased for each parameter and task.</p>
</sec>
</sec>
</sec>
<sec id="s11" sec-type="supplementary-material">
<title>Supporting information</title>
<p>S1 Appendix. Supplementary Information. Ideal observer model derivation; additional models; model comparison with AIC; model recovery analysis; measurement task; category training.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We would like to thank Chris Grimmick for helping with data collection. This work utilized the NYU IT High Performance Computing resources and services.</p>
</ack>
<ref-list>
<title>Reference</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="book"><string-name><surname>Bernardo</surname> <given-names>J</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>A.</given-names></string-name> <source>Bayesian theory</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>; <year>1994</year>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="book"><string-name><surname>Green</surname> <given-names>D</given-names></string-name>, <string-name><surname>Swets</surname> <given-names>JA</given-names></string-name>. <source>Signal detection theory and psychophysics</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Wiley</publisher-name>; <year>1966</year>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Tanner Jr</surname> <given-names>WP</given-names></string-name>. <article-title>Theory of recognition</article-title>. <source>J Acoust Soc Am</source>. <year>1956</year>;<volume>28</volume>:<fpage>882</fpage>&#x2013;<lpage>888</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Ackermann</surname> <given-names>JF</given-names></string-name>, <string-name><surname>Landy</surname> <given-names>MS</given-names></string-name>. <article-title>Suboptimal decision criteria are predicted by subjectively weighted probabilities and rewards</article-title>. <source>Atten Percept Psychophys</source>. <year>2015</year>;<volume>77</volume>:<fpage>638</fpage>&#x2013;<lpage>658</lpage>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Ulehla</surname> <given-names>ZJ</given-names></string-name>. <article-title>Optimality of perceptual decision criteria</article-title>. <source>J Exp Psychol</source>. <year>1966</year>;<volume>71</volume>:<fpage>564</fpage>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Healy</surname> <given-names>AF</given-names></string-name>, <string-name><surname>Kubovy</surname> <given-names>M.</given-names></string-name> <article-title>Probability matching and the formation of conservative decision rules in a numerical analog of signal detection</article-title>. <source>J Exp Psychol Hum Learn</source>. <year>1981</year>;<volume>7</volume>:<fpage>344</fpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Kubovy</surname> <given-names>M</given-names></string-name>, <string-name><surname>Healy</surname> <given-names>AF</given-names></string-name>. <article-title>The decision rule in probabilistic categorization: What it is and how it is learned</article-title>. <source>J Exp Psychol Gen</source>. <year>1977</year>;<volume>106</volume>:<fpage>427</fpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Healy</surname> <given-names>AF</given-names></string-name>, <string-name><surname>Kubovy</surname> <given-names>M.</given-names></string-name> <article-title>The effects of payoffs and prior probabilities on indices of performance and cutoff location in recognition memory</article-title>. <source>Mem Cognit</source>. <year>1978</year>;<volume>6</volume>:<fpage>544</fpage>&#x2013;<lpage>553</lpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Maddox</surname> <given-names>WT</given-names></string-name>. <article-title>Toward a unified theory of decision criterion learning in perceptual categorization</article-title>. <source>J Exp Anal Behav</source>. <year>2002</year>;<volume>78</volume>:<fpage>567</fpage>&#x2013;<lpage>595</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Barron</surname> <given-names>G</given-names></string-name>, <string-name><surname>Erev</surname> <given-names>I.</given-names></string-name> <article-title>Small feedback-based decisions and their limited correspondence to description-based decisions</article-title>. <source>J Behav Decis Mak</source>. <year>2003</year>;<volume>16</volume>:<fpage>215</fpage>&#x2013;<lpage>233</lpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Hertwig</surname> <given-names>R</given-names></string-name>, <string-name><surname>Barron</surname> <given-names>G</given-names></string-name>, <string-name><surname>Weber</surname> <given-names>EU</given-names></string-name>, <string-name><surname>Erev</surname> <given-names>I.</given-names></string-name> <article-title>Decisions from experience and the effect of rare events in risky choice</article-title>. <source>Psychol Sci</source>. <year>2004</year>;<volume>15</volume>:<fpage>534</fpage>&#x2013;<lpage>539</lpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Bohil</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Wismer</surname> <given-names>AJ</given-names></string-name>. <article-title>Implicit learning mediates base rate acquisition in perceptual categorization</article-title>. <source>Psychon Bull Rev</source>. <year>2015</year>;<volume>22</volume>:<fpage>586</fpage>&#x2013;<lpage>593</lpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Wismer</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Bohil</surname> <given-names>CJ</given-names></string-name>. <article-title>Base-rate sensitivity through implicit learning</article-title>. <source>PLoS ONE</source>. <year>2017</year>;<volume>12</volume>(<issue>6</issue>):<fpage>e0179256</fpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Berniker</surname> <given-names>M</given-names></string-name>, <string-name><surname>Voss</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kording</surname> <given-names>K.</given-names></string-name> <article-title>Learning priors for Bayesian computations in the nervous system</article-title>. <source>PLoS ONE</source>. <year>2010</year>;<volume>5</volume>(<issue>9</issue>):<fpage>e12686</fpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Summerfield</surname> <given-names>C</given-names></string-name>, <string-name><surname>Behrens</surname> <given-names>TE</given-names></string-name>, <string-name><surname>Koechlin</surname> <given-names>E.</given-names></string-name> <article-title>Perceptual classification in a rapidly changing environment</article-title>. <source>Neuron</source>. <year>2011</year>;<volume>71</volume>:<fpage>725</fpage>&#x2013;<lpage>736</lpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Norton</surname> <given-names>EH</given-names></string-name>, <string-name><surname>Fleming</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name><surname>Landy</surname> <given-names>MS</given-names></string-name>. <article-title>Suboptimal criterion learning in static and dynamic environments</article-title>. <source>PLoS Comput Biol</source>. <year>2017</year>;<volume>13</volume>(<issue>1</issue>):<fpage>e1005304</fpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Nassar</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Heasly</surname> <given-names>B</given-names></string-name>, <string-name><surname>Gold</surname> <given-names>JI</given-names></string-name>. <article-title>An approximately Bayesian delta-rule model explains the dynamics of belief updating in a changing environment</article-title>. <source>J Neurosci</source>. <year>2010</year>;<volume>30</volume>:<fpage>12366</fpage>&#x2013;<lpage>12378</lpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Landy</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Trommersha&#x00FC;ser</surname> <given-names>J</given-names></string-name>, <string-name><surname>Daw</surname> <given-names>ND</given-names></string-name>. <article-title>Dynamic estimation of task-relevant variance in movement under risk</article-title>. <source>J Neurosci</source>. <year>2012</year>;<volume>32</volume>:<fpage>12702</fpage>&#x2013;<lpage>12711</lpage>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Acerbi</surname> <given-names>L</given-names></string-name>, <string-name><surname>Wolpert</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Vijayakumar</surname> <given-names>S.</given-names></string-name> <article-title>Internal representations of temporal statistics and feedback calibrate motor-sensory interval timing</article-title>. <source>PLoS Comput Biol</source>. <year>2012</year>;<volume>8</volume>(<issue>11</issue>):<fpage>e1002771</fpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Sato</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Kording</surname> <given-names>KP</given-names></string-name>. <article-title>How much to trust the senses: Likelihood learning</article-title>. <source>J Vis</source>. <year>2014</year>;<volume>14</volume>(<issue>13</issue>):<fpage>13</fpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Glaze</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Kable</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Gold</surname> <given-names>JI</given-names></string-name>. <article-title>Normative evidence accumulation in unpredictable environments</article-title>. <source>Elife</source>. <year>2015</year>;<fpage>4</fpage>. DOI:<pub-id pub-id-type="doi">10.7554/eLife.08825</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Wilson</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Nassar</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Gold</surname> <given-names>JI</given-names></string-name>. <article-title>Bayesian online learning of the hazard rate in change-point problems</article-title>. <source>Neural Comput</source>. <year>2010</year>;<volume>22</volume>:<fpage>2452</fpage>&#x2013;<lpage>2476</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Meyniel</surname> <given-names>F</given-names></string-name>, <string-name><surname>Schlunegger</surname> <given-names>D</given-names></string-name>, <string-name><surname>Dehaene</surname> <given-names>S.</given-names></string-name> <article-title>The sense of confidence during probabilistic learning: A normative account</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year>;<volume>11</volume>(<issue>6</issue>):<fpage>e1004305</fpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Behrens</surname> <given-names>TE</given-names></string-name>, <string-name><surname>Woolrich</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Walton</surname> <given-names>ME</given-names></string-name>, <string-name><surname>Rushworth</surname> <given-names>MF</given-names></string-name>. <article-title>Learning the value of information in an uncertain world</article-title>. <source>Nat Neurosci</source>. <year>2007</year>;<volume>10</volume>:<fpage>1214</fpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Zylberberg</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wolpert</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Shadlen</surname> <given-names>MN</given-names></string-name>. <article-title>Counterfactual reasoning underlies the learning of priors in decision making</article-title>. <source>Neuron</source>. <year>2018</year>;<volume>99</volume>(<issue>5</issue>):<fpage>1083</fpage>&#x2013;<lpage>1097</lpage>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Tversky</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kahneman</surname> <given-names>D.</given-names></string-name> <article-title>Judgment under uncertainty: Heuristics and biases</article-title>. <source>Science</source>. <year>1974</year>;<volume>185</volume>:<fpage>1124</fpage>&#x2013;<lpage>1131</lpage>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Adams</surname> <given-names>RP</given-names></string-name>, <string-name><surname>MacKay</surname> <given-names>DJ</given-names></string-name>. <article-title>Bayesian online changepoint detection</article-title>. <source>Arxiv preprint arXiv:07103742</source>. <year>2007</year>;.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="book"><string-name><surname>Rescorla</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Wagner</surname> <given-names>AR</given-names></string-name>. <chapter-title>A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Black</surname> <given-names>AH</given-names></string-name>, <string-name><surname>Prokasy</surname> <given-names>WF</given-names></string-name></person-group>, editors. <source>Classical conditioning II: Current research and theory</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Appleton-Century-Crofts</publisher-name>; <year>1972</year>. p. <fpage>64</fpage>&#x2013;<lpage>99</lpage>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Wilson</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Nassar</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Gold</surname> <given-names>JI</given-names></string-name>. <article-title>Correction: A mixture of delta-rules approximation to Bayesian inference in change-point problems</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year>;<volume>9</volume>(<issue>7</issue>):<fpage>e1003150</fpage>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Wilson</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Nassar</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Gold</surname> <given-names>JI</given-names></string-name>. <article-title>A mixture of delta-rules approximation to Bayesian inference in change-point problems</article-title>. <source>PLoS Comput Biol</source>. <year>2018</year>;<volume>14</volume>(<issue>6</issue>):<fpage>e1006210</fpage>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Stephan</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Penny</surname> <given-names>WD</given-names></string-name>, <string-name><surname>Daunizeau</surname> <given-names>J</given-names></string-name>, <string-name><surname>Moran</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Friston</surname> <given-names>KJ</given-names></string-name>. <article-title>Bayesian model selection for group studies</article-title>. <source>Neuroimage</source>. <year>2009</year>;<volume>46</volume>:<fpage>1004</fpage>&#x2013;<lpage>1017</lpage>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Rigoux</surname> <given-names>L</given-names></string-name>, <string-name><surname>Stephan</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Friston</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Daunizeau</surname> <given-names>J.</given-names></string-name> <article-title>Bayesian model selection for group studies&#x2014;revisited</article-title>. <source>Neuroimage</source>. <year>2014</year>;<volume>84</volume>:<fpage>971</fpage>&#x2013;<lpage>985</lpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Burge</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ernst</surname> <given-names>MO</given-names></string-name>, <string-name><surname>Banks</surname> <given-names>MS</given-names></string-name>. <article-title>The statistical determinants of adaptation rate in human reaching</article-title>. <source>J Vis</source>. <year>2008</year>;<volume>8</volume>(<issue>4</issue>):<fpage>20</fpage>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Qamar</surname> <given-names>AT</given-names></string-name>, <string-name><surname>Cotton</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>George</surname> <given-names>RG</given-names></string-name>, <string-name><surname>Beck</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Prezhdo</surname> <given-names>E</given-names></string-name>, <string-name><surname>Laudano</surname> <given-names>A</given-names></string-name>, <etal>et al.</etal> <article-title>Trial-to-trial, uncertainty-based adjustment of decision boundaries in visual categorization</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2013</year>;<volume>110</volume>:<fpage>20332</fpage>&#x2013;<lpage>20337</lpage>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Beck</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>WJ</given-names></string-name>, <string-name><surname>Pitkow</surname> <given-names>X</given-names></string-name>, <string-name><surname>Latham</surname> <given-names>PE</given-names></string-name>, <string-name><surname>Pouget</surname> <given-names>A.</given-names></string-name> <article-title>Not noisy, just wrong: the role of suboptimal inference in behavioral variability</article-title>. <source>Neuron</source>. <year>2012</year>;<volume>74</volume>(<issue>1</issue>):<fpage>30</fpage>&#x2013;<lpage>39</lpage>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Acerbi</surname> <given-names>L</given-names></string-name>, <string-name><surname>Vijayakumar</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wolpert</surname> <given-names>DM</given-names></string-name>. <article-title>On the origins of suboptimality in human probabilistic inference</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>(<issue>6</issue>):<fpage>e1003661</fpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Drugowitsch</surname> <given-names>J</given-names></string-name>, <string-name><surname>DeAngelis</surname> <given-names>GC</given-names></string-name>, <string-name><surname>Angelaki</surname> <given-names>DE</given-names></string-name>, <string-name><surname>Pouget</surname> <given-names>A.</given-names></string-name> <article-title>Tuning the speed-accuracy trade-off to maximize reward rate in multisensory decision-making</article-title>. <source>Elife</source>. <year>2015</year>;<volume>4</volume>:<fpage>e06678</fpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Izawa</surname> <given-names>J</given-names></string-name>, <string-name><surname>Shadmehr</surname> <given-names>R.</given-names></string-name> <article-title>Learning from sensory and reward prediction errors during motor adaptation</article-title>. <source>PLoS Comput Biol</source>. <year>2011</year>;<volume>7</volume>(<issue>3</issue>):<fpage>e1002012</fpage>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Attneave</surname> <given-names>F.</given-names></string-name> <article-title>Psychological probability as a function of experienced frequency</article-title>. <source>J Exp Psychol</source>. <year>1953</year>;<volume>46</volume>:<fpage>81</fpage>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Varey</surname> <given-names>CA</given-names></string-name>, <string-name><surname>Mellers</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Birnbaum</surname> <given-names>MH</given-names></string-name>. <article-title>Judgments of proportions</article-title>. <source>Exp Psychol Hum Percept Perform</source>. <year>1990</year>;<volume>16</volume>:<fpage>613</fpage>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Maloney</surname> <given-names>LT</given-names></string-name>, <string-name><surname>Thomas</surname> <given-names>EA</given-names></string-name>. <article-title>Distributional assumptions and observed conservatism in the theory of signal detectability</article-title>. <source>J Math Psychol</source>. <year>1991</year>;<volume>35</volume>:<fpage>443</fpage>&#x2013;<lpage>470</lpage>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Kubovy</surname> <given-names>M.</given-names></string-name> <article-title>A possible basis for conservatism in signal detection and probabilistic categorization tasks</article-title>. <source>Percept Psychophys</source>. <year>1977</year>;<volume>22</volume>:<fpage>277</fpage>&#x2013;<lpage>281</lpage>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><surname>Lee</surname> <given-names>W</given-names></string-name>, <string-name><surname>Janke</surname> <given-names>M.</given-names></string-name> <article-title>Categorizing externally distributed stimulus samples for unequal molar probabilities</article-title>. <source>Psychol Rep</source>. <year>1965</year>;<volume>17</volume>:<fpage>79</fpage>&#x2013;<lpage>90</lpage>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>Murray</surname> <given-names>RF</given-names></string-name>, <string-name><surname>Patel</surname> <given-names>K</given-names></string-name>, <string-name><surname>Yee</surname> <given-names>A.</given-names></string-name> <article-title>Posterior probability matching and human perceptual decision making</article-title>. <source>PLoS Comp Biol</source>. <year>2015</year>;<volume>11</volume>(<issue>6</issue>):<fpage>e1004342</fpage>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>Thomas</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Legge</surname> <given-names>D.</given-names></string-name> <article-title>Probability matching as a basis for detection and recognition decisions</article-title>. <source>Psychol Rev</source>. <year>1970</year>;<volume>77</volume>:<fpage>65</fpage>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><surname>Wozny</surname> <given-names>DR</given-names></string-name>, <string-name><surname>Beierholm</surname> <given-names>UR</given-names></string-name>, <string-name><surname>Shams</surname> <given-names>L.</given-names></string-name> <article-title>Probability matching as a computational strategy used in perception</article-title>. <source>PLoS Comp Biol</source>. <year>2010</year>;<volume>6</volume>(<issue>8</issue>):<fpage>e1000871</fpage>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>van den Berg</surname> <given-names>R</given-names></string-name>, <string-name><surname>Awh</surname> <given-names>E</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>WJ</given-names></string-name>. <article-title>Factorial comparison of working memory models</article-title>. <source>Psychol Rev</source>. <year>2014</year>;<volume>121</volume>:<fpage>124</fpage>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><surname>Gifford</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Cohen</surname> <given-names>YE</given-names></string-name>, <string-name><surname>Stocker</surname> <given-names>AA</given-names></string-name>. <article-title>Characterizing the impact of category uncertainty on human auditory categorization behavior</article-title>. <source>PLoS Comp Biol</source>. <year>2014</year>;<volume>10</volume>(<issue>7</issue>):<fpage>e1003715</fpage>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><surname>Petzschner</surname> <given-names>FH</given-names></string-name>, <string-name><surname>Glasauer</surname> <given-names>S.</given-names></string-name> <article-title>Iterative Bayesian estimation as an explanation for range and regression effects: a study on human path integration</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>:<fpage>17220</fpage>&#x2013;<lpage>17229</lpage>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Raviv</surname> <given-names>O</given-names></string-name>, <string-name><surname>Ahissar</surname> <given-names>M</given-names></string-name>, <string-name><surname>Loewenstein</surname> <given-names>Y.</given-names></string-name> <article-title>How recent history affects perception: the normative approach and its heuristic approximation</article-title>. <source>PLoS Comp Biol</source>. <year>2012</year>;<volume>8</volume>(<issue>10</issue>):<fpage>e1002731</fpage>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><surname>Fischer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Whitney</surname> <given-names>D.</given-names></string-name> <article-title>Serial dependence in visual perception</article-title>. <source>Nat Neurosci</source>. <year>2014</year>;<volume>17</volume>:<fpage>738</fpage>&#x2013;<lpage>743</lpage>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><string-name><surname>Zylberberg</surname> <given-names>A</given-names></string-name>, <string-name><surname>Roelfsema</surname> <given-names>PR</given-names></string-name>, <string-name><surname>Sigman</surname> <given-names>M.</given-names></string-name> <article-title>Variance misperception explains illusions of confidence in simple perceptual decisions</article-title>. <source>Conscious Cogn</source>. <year>2014</year>;<volume>27</volume>:<fpage>246</fpage>&#x2013;<lpage>253</lpage>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="other"><collab>MATLAB. version 7.10.0 (R2010a). Natick, Massachusetts: The MathWorks Inc.; 2010</collab>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><string-name><surname>Brainard</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Vision</surname> <given-names>S.</given-names></string-name> <article-title>The psychophysics toolbox</article-title>. <source>Spat Vis</source>. <year>1997</year>;<volume>10</volume>:<fpage>433</fpage>&#x2013;<lpage>436</lpage>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><string-name><surname>Pelli</surname> <given-names>DG</given-names></string-name>. <article-title>The VideoToolbox software for visual psychophysics: Transforming numbers into movies</article-title>. <source>Spat Vis</source>. <year>1997</year>;<volume>10</volume>:<fpage>437</fpage>&#x2013;<lpage>442</lpage>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><string-name><surname>Acerbi</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>WJ</given-names></string-name>, <string-name><surname>Vijayakumar</surname> <given-names>S.</given-names></string-name> <article-title>A framework for testing identifiability of bayesian models of perception</article-title>. <source>In: Advances in neural information processing systems</source>; <year>2014</year>. p. <fpage>1026</fpage>&#x2013;<lpage>1034</lpage>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="book"><string-name><surname>Jaynes</surname> <given-names>ET</given-names></string-name>. <source>Probability theory: the logic of science</source>. <publisher-name>Cambridge, England: Cambridge University Press</publisher-name>; <year>2003</year>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="book"><string-name><surname>MacKay</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Mac Kay</surname> <given-names>DJ</given-names></string-name>. <source>Information theory, inference and learning algorithms</source>. <publisher-loc>Cambridge, England</publisher-loc>: <publisher-name>Cambridge university press</publisher-name>; <year>2003</year>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><string-name><surname>Akaike</surname> <given-names>H.</given-names></string-name> <article-title>Information theory and an extension of the maximum likelihood principle</article-title>. <source>In: Proceedings of the 2nd international symposium on information</source>; <year>1973</year>. p. <fpage>267</fpage>&#x2013;<lpage>281</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>