<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/254961</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Visual and auditory brain areas share a neural code for perceived emotion</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Sievers</surname>
<given-names>Beau</given-names>
</name>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wheatley</surname>
<given-names>Thalia</given-names>
</name>
</contrib>
<aff id="a1"><institution>Department of Psychological and Brain Sciences, Dartmouth College</institution>, Hanover, NH 03755</aff>
</contrib-group>
<author-notes>
<corresp id="cor1">Correspondence to: Beau Sievers, 6207 Moore Hall, Dartmouth College, Hanover, NH 03755. <email>beau.r.sievers.gr@dartmouth.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2018</year>
</pub-date>
<elocation-id>254961</elocation-id>
<history>
<date date-type="received">
<day>26</day>
<month>1</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>26</day>
<month>1</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>29</day>
<month>1</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="254961.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Emotion communication must be robust to interference from a noisy environment. One safeguard against interference is crossmodal redundancy&#x2014;for example, conveying the same information using both sound and movement. Emotion perceivers should therefore be adapted to efficiently detect crossmodal correspondences, increasing the likelihood that emotion signals will be understood. One possible such adaptation is the use of a single neural code for both auditory and visual information. To investigate this, we tested two hypotheses: (1) that distinct auditory and visual brain areas represent emotion expressions using the same parameters, and (2) that auditory and visual expressions of emotion are represented together in one brain area using a supramodal neural code. We presented emotion expressions during functional magnetic resonance imaging (<italic>N</italic>&#x003D;20,3 scan hrs/participant) and tested these hypotheses using representational similarity analysis (<xref ref-type="bibr" rid="c23">Kriegeskorte &#x0026; Kievit, 2013</xref>). A single model of stimulus features and emotion content fit brain activity in both auditory and visual areas, supporting hypothesis (1), and posterior superior temporal gyrus represented both auditory and visual emotion expressions, supporting hypothesis (2). These results hold for both discrete and mixed (e.g., Happy-Sad) emotional expressions. Surprisingly, further exploratory analysis showed auditory and visual areas represent stimulus features and emotion content even when stimuli are presented in each area&#x2019;s non-preferred modality.</p>
</abstract>
<counts>
<page-count count="21"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Across the animal kingdom, communicative signals are linked in sight and sound: the rattlesnake&#x2019;s threat is telegraphed by the simultaneous shaking of its tail and its distinctive rattle. The perceptiual systems of signal receivers, co-evolved alongside signal senders, should exploit such crossmodal redundancies. Here we test this directly by examining neural processing during the perception of auditory and visual displays of emotion. We show that human auditory and visual brain areas represent emotion using the same neural code. Further, this code is used to represent emotion presented in each area&#x2019;s non-preferred modality. We suggest this shared neural code facilitates successful detection and understanding of evolutionarily relevant signals.</p>
<p>From Shakespeare&#x2019;s <italic>Hamlet</italic>, to Jane Austen&#x2019;s <italic>Emma</italic>, to Disney&#x2019;s <italic>Frozen</italic>, communicative misunderstanding is the mainspring of human drama. This may be rooted in humanity&#x2019;s evolutionary history. As a radically social species, our survival depends on the ability to quickly understand others&#x2019; thoughts and feelings (<xref ref-type="bibr" rid="c3">Allport, 1924</xref>; <xref ref-type="bibr" rid="c39">Tooby &#x0026;Cosmides, 1990</xref>). This is no easy task, as communication transpires across a <italic>noisy channel</italic>&#x2014;<italic>imprecise</italic> gestures, sounds, and speech, must pierce through a chaotic environment to maximize their chances of perception by distracted and inattentive observers. Effective communication requires expressive signals that can survive the noisy channel, and brains adapted to perceive them (<xref ref-type="bibr" rid="c9">Dezecache, Mercier, &#x0026; Scott-Phillips, 2013</xref>; <xref ref-type="bibr" rid="c16">Huron, 2012</xref>; <xref ref-type="bibr" rid="c28">Lorenz, 1970</xref>). Consistent with this <italic>adaptive signaling</italic> account of emotion expression (<xref ref-type="bibr" rid="c15">Hebets et al., 2016</xref>; <xref ref-type="bibr" rid="c16">Huron, 2012</xref>), previous research has revealed that emotion expressions are strikingly similar across music and movement (<xref ref-type="bibr" rid="c37">Sievers, Polansky, Casey, &#x0026; Wheatley, 2013</xref>). If this crossmodal redundancy is exploited by perceivers (<xref ref-type="bibr" rid="c15">Hebets et al., 2016</xref>; <xref ref-type="bibr" rid="c19">Johnstone, 1996</xref>,<xref ref-type="bibr" rid="c20">1997</xref>), we should observe a tight fit between the structure of emotion expressions and their representation in perceving brains.</p>
<p>We tested two hypotheses: (1) that both auditory and visual areas encode emotion expressions using the same parameters&#x2014;i.e., they share a representational geometry (<xref ref-type="bibr" rid="c23">Kriegeskorte &#x0026; Kievit, 2013</xref>)&#x2014;and (2) that auditory and visual expressions of emotion are represented together in one brain area using a supramodal neural code.</p>
<p>A model capturing both dynamic (i.e., time-varying) stimulus features and emotional meaning fit activity in both auditory and visual areas, supporting hypothesis (1). The same model fit activity in posterior superior temporal gyrus (pSTG) during both auditory and visual emotion expressions, supporting hypothesis (2). Additional exploratory analysis showed that auditory and visual areas represent stimulus features and emotion content even when stimuli are presented in each area&#x2019;s non-preferred modality. These results support an adaptive signaling account of emotion perception, where the structure of emotional signals and the brains of receivers have adapted to tightly fit one another, facilitating efficient and reliable signal perception.</p>
<sec id="s1a">
<title>Previous research on neural representation of emotion</title>
<p>Emotion-related neural processes are distributed across a wide range of brain areas, with each area implicated in the production and/or perception of a range of emotions (<xref ref-type="bibr" rid="c27">Lindquist, Wager, Kober, Bliss-Moreau, &#x0026; Barrett, 2012</xref>). However, certain aspects of emotion processing are tightly localized. Lesion studies have demonstrated that some brain areas play emotion-specific roles; for example, the amygdala is critical for recognizing fearful stimuli (<xref ref-type="bibr" rid="c2">Adolphs, Tranel, Damasio, &#x0026; Damasio, 1994</xref>), and the insula for recognizing disgust (<xref ref-type="bibr" rid="c6">Calder, Lawrence, &#x0026; Young, 2001</xref>).</p>
<p>Our hypotheses ask not only <italic>where</italic> in the brain emotions are represented, but <italic>how</italic> those representations are structured. For example, a single brain area may distinguish between emotions using different spatial patterns of activity that all have the same mean. To characterize the representational properties of these areas, it is necessary to use techniques that are sensitive to such spatially distributed patterns; e.g., multivariate pattern classification (<xref ref-type="bibr" rid="c30">Norman, Polyn, Detre, &#x0026; Haxby, 2006</xref>) or representational similarity analysis (RSA; <xref ref-type="bibr" rid="c23">Kriegeskorte &#x0026; Kievit, 2013</xref>). Below, we summarize previous research taking a multivariate approach.</p>
<p><xref ref-type="bibr" rid="c32">Peelen et al. (2010</xref>) found that patterns of activation in the medial prefrontal cortex (mPFC) and posterior superior temporal sulcus (pSTS) had greater within-emotion similarity than between-emotion similarity across modalities, indicating these areas supramodally represent emotion identity. <xref ref-type="bibr" rid="c7">Chikazoe et al. (2014)</xref> found supramodal directional valence (i.e., positive vs. neutral vs. negative) representations in medial and lateral orbitofrontal cortex (OFC), alongside modality-specific directional valence representations for visual scenes in ventral temporal cortex, and for tastes in anterior insular cortex. <xref ref-type="bibr" rid="c38">Skerry &#x0026; Saxe (2015</xref>) presented written stories depicting characters experiencing many different emotions. They found a model fitting 38 appraisal features (e.g., &#x201C;Did someone cause this situation intentionally, or did it occur by accident?&#x201D;) fit activity in dorsal and middle medial prefrontal cortex, the temporoparietal junction, and a network of regions identified by a theory of mind localization task. <xref ref-type="bibr" rid="c21">Kim et al. (2017)</xref> presented emotional movie clips and orchestral music, finding a range of supramodal representations: valence direction in the precuneus, valence magnitude in mPFC, STS, and middle frontal gyrus (MFG), and both valence direction and magnitude in the STS, MFG, and thalamus.</p>
</sec>
<sec id="s1b">
<title>Experimental paradigm</title>
<p>The present work builds on the foundation of previous research in several ways. Our stimuli consisted of short clips of music and animation in which the depicted object&#x2014;a piano or a bouncing ball&#x2014;was held constant, and emotion was communicated solely by varying stimulus features. This ensured emotion processing requirements were uniform across the stimulus set. By contrast, collections of images or movies depicting emotionally charged scenes (e.g., the International Affective Picture System; Lang, <xref ref-type="bibr" rid="c26">Bradley, &#x0026; Cuthbert, 2008</xref>) may require a wide variety of processes for emotion evaluation, including moral judgment, memory, and so on.</p>
<p>Stimuli were created by participants in a previously documented experiment (<xref ref-type="bibr" rid="c37">Sievers et al., 2013</xref>), who manipulated five stimulus features (speed, irregularity, consonance/spikiness, ratio of big-to-small movements, ratio of upward-to-downward movements) to generate five emotions (Angry, Happy, Peaceful, Sad, Scared). This approach distinguishes between emotions with similar valence, such as Angry and Sad or Happy and Peaceful. The stimulus set was augmented by linearly mixing the features of each emotion pair, creating mixed emotions (e.g., Happy-Sad). Emotions were mixed at 25&#x0025;, 50&#x0025;, and 75&#x0025;. Three additional, &#x201C;neutral&#x201D; emotions were identified by searching for points in the stimulus feature possibility space that were distant from all emotions. Music and animation were matched, such that for each musical stimulus there was an animation stimulus with analogous features. This process yielded 76 total stimulus classes, including both music and animation. All stimuli are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/kvbqm/">https://osf.io/kvbqm/</ext-link>. A separate set of participants judged how well each stimulus fit all five emotion labels, and a subset of these participants viewed many music and animation stimuli while undergoing fMRI scanning (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><p>Experimental paradigm. <italic>A</italic>. Participants in <xref ref-type="bibr" rid="c37">Sievers et al. (2013)</xref> manipulated stimulus features to generate music and animation expressing five prototypical emotions: Angry, Happy, Peaceful, Sad, and Scared. <italic>B</italic>. Mixed emotions were generated by linear interpolation between the stimulus features of prototypical emotions. <italic>C</italic>. Participants judged the emotion content of many prototypical and mixed emotions in music and animation. <italic>D</italic>. A subset of participants viewed many prototypical and mixed emotions in music and animation while undergoing jittered event-related fMRI scanning. <italic>E</italic>. Results were analyzed using searchlight representational similarity analysis (<xref ref-type="bibr" rid="c23">Kriegeskorte &#x0026; Kievit, 2013</xref>; <xref ref-type="bibr" rid="c24">Kriegeskorte et al., 2006</xref>, <xref ref-type="bibr" rid="c25">2008</xref>). For each searchlight sphere, the structure of the neural representational dissimilarity matrix (RDM) was predicted using a linear combination of stimulus feature and emotion judgment RDMs.</p></caption>
<graphic xlink:href="254961_fig1.tif"/>
</fig>
<p>The approach described above enabled the use of an exhaustively complete model, including both stimulus features and participants&#x2019; judgments of emotion content. All inter-stimulus differences were dependent upon parameters explicitly represented in this model. The fitness of the model to activity across the brain during vision and audition was evaluated using searchlight representational similarity analysis (<xref ref-type="bibr" rid="c23">Kriegeskorte &#x0026; Kievit, 2013</xref>; <xref ref-type="bibr" rid="c24">Kriegeskorte, Goebel, &#x0026; Bandettini, 2006</xref>; <xref ref-type="bibr" rid="c25">Kriegeskorte, Mur, &#x0026; Bandettini,2008</xref>).</p>
</sec></sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Representational Similarity Analysis</title>
<p>We created 10 model representational dissimilarity matrices (RDMs): five based on the parameter settings used to create the stimuli (speed, irregularity, consonance/spikiness, ratio of big-to-small movements, ratio of upward-to-downward movements), and five based on the emotion judgments of our behavioral participants (Angry, Happy, Peaceful, Sad, and Scared). Each RDM captured the distance between every pair of stimuli in terms of a single stimulus feature or emotion judgment parameter (Supplementary Figure 1). RDMs were constructed such that that our model was not sensitive to differences in the mean level of BOLD activity between music and animation trials. This was achieved by using the same stimulus feature parameter settings to create both music and animation stimuli, and by averaging emotion judgments across music and animation. This ensured that the modeled distance between any two music stimuli was always equal to the modeled distance between the corresponding animation stimuli, and that the mean distance between music stimuli was equal to the mean distance between animation stimuli.</p>
<p>To test hypotheses (1) and (2), we performed a searchlight representational similarity analysis (<xref ref-type="bibr" rid="c24">Kriegeskorte et al., 2006</xref>, <xref ref-type="bibr" rid="c25">2008</xref>). Within each searchlight sphere we calculated the Spearman correlation distance between each pair of stimulus-dependent patterns of BOLD activity to create a neural RDM. To assess how the neural RDM could be expressed as a linear combination of our model RDMs, we fit a multiple regression model using our 10 model RDMs as predictors and the neural RDM as the target. RDMs were ranked before regression. We ran this analysis twice&#x2014;first, using only music trials to create the neural RDM, then using only animation trials.</p>
<p>Our model explained variance in a range of visual and auditory brain regions, providing strong support for hypothesis (1), that these regions share a common representational geometry (<xref ref-type="fig" rid="fig3">Figure 3</xref>; <xref ref-type="table" rid="tbl1">Table 1</xref>). The peak of the average model fit across participants was in the medial lingual gyrus for animation trials (M&#x003D;.16; 95&#x0025; CI: .1-.23; t(19)&#x003D;5.13; p &#x003C; .001; all p-values corrected at FWER&#x003D;.05) and in bilateral anterior superior temporal gyrus for music trials (M&#x003D;.16; 95&#x0025; CI: .11-.21; t(19)&#x003D;6.65; p &#x003C; .001). The magnitude and anatomical location of the peak model fit were consistent across participants (Supplementary Figures 2 and 3). For per-parameter beta weights, see Supplementary Figures 5 and 6.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><p>Highlighted brain areas fit a model including stimulus features and emotion judgments during animation trials (blue), music trials (green), and both trial types (yellow). Neural dissimilarity matrices show pairwise similarity of activity patterns evoked by each stimulus at the locations of best model fit (circled)&#x2014;medial lingual gyrus (animation) and lateral superior temporal gyrus (music). Fully-labled verions of these matrices are shown in Supplementary Figure 7. Multidimensional scaling flattens these dissimilarity matrices to two dimensions, so the distance between dots reflects the similarity of patterns of neural activity. Dots are colored by mixing the legend colors based on participants&#x2019; judgments of the emotion content of each stimulus.</p></caption>
<graphic xlink:href="254961_fig2.tif"/>
</fig>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><p>Maps of the mean coefficient of determination (R<sup>2</sup>) across participants. The model included 5 stimulus feature parameters and 5 emotion judgment parameters, and was separately fitto animation and music trials. Maps thresholded at voxelwise FWER&#x003D;.05. R<sup>2</sup> values &#x003C; .02 hidden for visual clarity. Box plots show per-participant R<sup>2</sup> values at the location of best model fit at the group level. For per-parameter beta weights, see Supplementary Figures 5 and 6.</p></caption>
<graphic xlink:href="254961_fig3.tif"/>
</fig>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><p>Brain regions fitting the stimulus feature and emotion judgment model during animation trials.</p></caption>
<graphic xlink:href="254961_tbl1.tif"/>
</table-wrap>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><p>Brain regions fitting the stimulus feature and emotion judgment model during music trials.</p></caption>
<graphic xlink:href="254961_tbl2.tif"/>
</table-wrap>
<p>To locate brain regions representing emotion supramodally, we created binary overlap masks per-subject, selecting voxels where our model explained a meaningful amount of variance (R<sup>2</sup>&#x003E;.02) for both music and animation trials. These masks were averaged to map the proportion of participants with supramodal representations in each voxel. Supramodal representations were found in bilateral posterior superior temporal gyrus (pSTG) in 65&#x0025; of participants (p &#x003C; .001), providing support for hypothesis (2) (<xref ref-type="fig" rid="fig4">Fig 4</xref>). Group level model fits in each unimodal analysis were also significant at this location (animation mean R<sup>2</sup>&#x003D;.04, 95&#x0025; CI: .02-.07, t(19)&#x003D;4.25, p&#x003C;.001; music mean R<sup>2</sup>&#x003D;.07, 95&#x0025; CI: .05-.1, t(19)&#x003D;5.3, p&#x003C;.001). Due to individual differences in functional anatomy, this procedure underestimates the proportion of participants with supramodal emotion representations. Manual inspection of the overlap masks showed supramodal emotion representations in pSTG were consistent across participants, and that some participants showed additional supramodal representations in other areas, including the right inferior frontal gyrus (Supplementary Figure 4).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><p>A. Binary overlap masks were created per participant, selecting voxels voxels that were significant at the group level for both music and animation trials. Maps show the voxelwise average of these overlap masks, expressing the proportion of participants representing emotion in music and animation in the same brain areas. Maps thresholded at voxelwise FWER&#x003D;.05. R<sup>2</sup> values &#x003C; .02 hidden for visual clarity. B. Box plots show R<sup>2</sup> for music and animation trials at the location where most participants exhibited supramodal emotion representations.</p></caption>
<graphic xlink:href="254961_fig4.tif"/>
</fig>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3:</label>
<caption><p>Brain regions fitting the stimulus feature and emotion judgment model during both music and animation trials.</p></caption>
<graphic xlink:href="254961_tbl3.tif"/>
<graphic xlink:href="254961_tbl3a.tif"/>
</table-wrap>
</sec>
<sec id="s2b">
<title>Exploratory intermodal RSA</title>
<p>To find brain areas representing emotion even when stimuli are presented in the non-preferred modality, we performed an exploratory intermodal RSA that used RDMs containing only between-modality distances. To build the neural target RDM, we took the rank correlation between patterns of activity elicited when each emotion was presented as music and when each emotion was presented as animation (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Model RDMs were built using an analogous procedure, and were rank-ordered before analysis. Note that because within-modality pairs were excluded, all intermodal RDMs were square, corresponding to the lower-left square region of the larger triangular RDM created using stimuli from both modalities. If a brain area is inactive when stimuli are presented in its non-preferred modality, then the intermodal neural RDM should be uncorrelated with the intermodal model RDMs. If a brain area is active, even weakly, and representing emotion content, its intermodal neural RDM should be correlated with the intermodal model RDMs.</p>
<p>The intermodal RSA revealed a bilateral set of areas across occiptal, superior parietal, temporal, cingulate, and frontal cortex that represented emotions presented in the non-preferred modality (<xref ref-type="fig" rid="fig5">Figure 5B</xref>; <xref ref-type="table" rid="tbl4">Table 4</xref>). Note that some of these areas did not show significant unimodal model fit. Peak intermodal model fit was in right lingual gyrus (M&#x003D;.29; 95&#x0025; CI: .21&#x2013;.38; t(19)&#x003D;7.07; p &#x003C; .001). Notably, the peak intermodal model fit exceeded the peak within-modality model fit for both music and animation.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><p>Intermodal model fit. We created behavioral and neural intermodal RDMs to locate brain areas representing emotion even when stimuli were presented in that area&#x2019;s non-preferred modality. A. Intermodal RDMs capture the stimulus feature, emotion judgment, or neural pattern distances between music and animation expressing the same emotion. The intermodal RDM is the lower-left square of the the full RDM created using both music and animation. B. Intermodal model fit, thresholded at FWER&#x003D;.05. R<sup>2</sup> values &#x003C; .04 hidden for visual clarity. Box plot shows per-participant R<sup>2</sup> values at the location of best model fit at the group level.</p></caption>
<graphic xlink:href="254961_fig5.tif"/>
</fig>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>Table 4:</label>
<caption><p>Brain regions fitting intermodal model; i.e., regions which fit the stimulus feature and emotion judgment model even when the stimulus is presented in the non-preferred modality.</p></caption>
<graphic xlink:href="254961_tbl4.tif"/>
</table-wrap>
</sec>
<sec id="s2c">
<title>Unthresholded statistical maps</title>
<p>All unthresholded statistical maps are available at <ext-link ext-link-type="uri" xlink:href="https://neurovault.org/collections/3399/">https://neurovault.org/collections/3399/</ext-link>.</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>On the adaptive signaling account of emotion perception, the human brain should show adaptations specific to the crossmodally redundant structure of emotion expression. To investigate this, we tested two hypotheses: (1) that auditory and visual brain areas encode emotion expressions using the same underlying parameters, and (2) that in some brain areas, auditory and visual expressions of emotion are represented using a single, supramodal neural code. Visual and auditory sensory areas both fit a model including stimulus features and emotion judgments, indicating these regions use the same neural code for emotion, supporting hypothesis (1). The same model fit activity in pSTG during both animation and music trials, indicating the presence of a supramodal emotion representation, supporting hypothesis (2). Exploratory intermodal representational similarity analysis showed that low-level visual and auditory areas represent stimulus features and emotion content even when presented in their non-preferred modality.</p>
<p>Tuning of sensory representations to evolutionarily revelant signals&#x2014;in this case, emotion expressions&#x2014;shows that the need to identify such signals has exerted a profound shaping force on low-level perceptual processes. Such tuning is predicted by the adaptive signaling account of emotion perception (<xref ref-type="bibr" rid="c9">Dezecache et al., 2013</xref>; <xref ref-type="bibr" rid="c15">Hebets et al., 2016</xref>; <xref ref-type="bibr" rid="c16">Huron, 2012</xref>; <xref ref-type="bibr" rid="c28">Lorenz, 1970</xref>). We do not see or hear the actions of others as raw sense impressions first, and later encode them as communicating emotion after a chain of intermediary processing steps occuring in encapsulated cognitive modules (<xref ref-type="bibr" rid="c12">Firestone &#x0026; Scholl, 2016</xref>; <xref ref-type="bibr" rid="c13">Fodor, 1985</xref>). Rather, we begin accumulating evidence for an emotional intepretation from the lowest levels of sensory processing.</p>
<sec id="s5">
<title>Supramodal representation in pSTG/pSTS</title>
<p>Our findings in pSTG overlap with previously reported pSTS activation during action understanding (<xref ref-type="bibr" rid="c4">M. S. Beauchamp, Lee, Argall, &#x0026; Martin, 2004</xref>; Wyk, Hudac, Carter, <xref ref-type="bibr" rid="c43">Sobel, &#x0026; Pelphrey, 2009</xref>) and emotion perception tasks (<xref ref-type="bibr" rid="c22">Kreifelts, Ethofer, Grodd, Erb, &#x0026; Wildgruber, 2007</xref>; <xref ref-type="bibr" rid="c34">Robins, Hunyadi, &#x0026; Schultz, 2009</xref>; <xref ref-type="bibr" rid="c40">Watson et al., 2014</xref>). The pSTG/pSTS may act as a general-purpose hub for transforming unimodal inputs into a common supramodal representation, and then comparing them to check for a match. Supporting this account, the pSTS shows greater activation for combined audio-visual presentation than for either modality alone (<xref ref-type="bibr" rid="c4">M. S. Beauchamp et al., 2004</xref>; <xref ref-type="bibr" rid="c42">Wright, Pelphrey, Allison, McKeown, &#x0026; McCarthy, 2003</xref>). The amplitude of these responses, when controlled for noise distorting the stimulus, predicts object categorization performance (<xref ref-type="bibr" rid="c41">Werner &#x0026; Noppeney, 2010</xref>). Interestingly, visual and auditory selectivity in pSTS are linked, with areas sensitive to moving mouths responding strongly to voices, but not non-vocal sounds (<xref ref-type="bibr" rid="c45">Zhu &#x0026; Beauchamp, 2017</xref>). This suggests crossmodal selectivity in pSTS may be shaped by co-occurence statistics in the environment.</p>
</sec>
<sec id="s6">
<title>Reading emotion from semantic content vs. stimulus features</title>
<p>Recent studies of emotion perception have emphasized reading emotions from semantic content (<xref ref-type="bibr" rid="c7">Chikazoe et al., 2014</xref>; <xref ref-type="bibr" rid="c21">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="c38">Skerry &#x0026; Saxe, 2015</xref>). The emotional meaning of stimuli used in these studies (e.g., detailed written stories; images from the International Affective Picture System, <xref ref-type="bibr" rid="c26">Lang et al., 2008</xref>) depends on semantic processing: recognizing <italic>what</italic> is depicted and <italic>why</italic> it is emotionally relevant. While important, this type of emotion perception is different in kind from reading emotional meaning conveyed by stimulus features, such as movement or prosody. By contrast, our experiment used music and animation in which the depicted object was held constant, and relatively low-level stimulus features were manipulated to express a wide range of emotions. These different approaches likely impose different neural processing demands. We anticipate that advances in automatic feature extraction (<xref ref-type="bibr" rid="c29">McNamara, Vega, &#x0026; Yarkoni, 2017</xref>) will enable the use of stimuli and models spanning not only the stimulus feature and emotion spaces examined here, but also additional dimensions of semantic meaning, context dependence, self- and other-relevance, appraisal features, and so on. Such future experiments will be the best of both (or many) worlds, allowing researchers to disentangle the many possible underlying mechanisms supporting emotion perception.</p>
</sec>
<sec id="s7">
<title>Adaptive signaling vs. &#x201C;peg fits hole&#x201D;</title>
<p>One possible reading of these results is that humans have evolved neural detectors specific to the structure of emotion expressions, and that these are present from birth. On this &#x201C;peg fits hole&#x201D; interpretation, any sensory input with the right structure should be detected and interpreted as an emotion expression. While this may be true in some basic cases, such as infants&#x2019; reactions to shouting or motherese, cross-cultural variation in emotion expressions places a limit on the &#x201C;peg fits hole&#x201D; interpretation. Although emotion expressions across cultures share structural features supporting mutual intelligibility (<xref ref-type="bibr" rid="c11">Ekman, 1992</xref>; <xref ref-type="bibr" rid="c18">Jack, Sun, Delis, Garrod, &#x0026; Schyns, 2016</xref>; <xref ref-type="bibr" rid="c37">Sievers et al., 2013</xref>), there are also substantial cross-cultural differences (<xref ref-type="bibr" rid="c17">Jack, Caldara, &#x0026; Schyns, 2012</xref>; <xref ref-type="bibr" rid="c18">Jack et al., 2016</xref>; <xref ref-type="bibr" rid="c44">Yuki, Maddux, &#x0026; Masuda, 2007</xref>). The neural mechanisms supporting emotion perception must therefore flexibly accommodate culture-specific emotion dialects and display rules. These mechanisms need not be present from birth, and need not be specific to emotion. Rather, emotion perception may exploit statistical learning and predictive coding processes (<xref ref-type="bibr" rid="c8">Clark, 2013</xref>; <xref ref-type="bibr" rid="c36">Saffran, Aslin, &#x0026; Newport, 1996</xref>), or may arise later in development, emerging from cognitive strategies for coping with a complex social world (<xref ref-type="bibr" rid="c5">Blakemore, 2008</xref>). On this account, the structure of emotion expressions, the brains of emotion perceivers, and their cultural-environmental niche are interlinked and evolve together. The cross-cultural intelligibility of emotion expressions can be explained by globally shared contextual factors, including the evolutionary inheritance of the human body, the challenge of cooperating with others in a dangerous, unpredictable, resource-limited world, and the related need to estimate others&#x2019; internal states. Cross-cultural differences can be understood as path-dependent adaptations specific to a regional cultural-environmental niche.</p>
</sec>
</sec>
<sec id="s8">
<title>Conclusion</title>
<p>The structure of emotion expressions is shared across music and movement and is tightly coupled to meaning. This is reflected in the organization of the brain: the same neural code is used to represent emotion in auditory, visual, and supramodal areas. Surprisingly, unimodal auditory and visual areas represent stimuli shown in their non-preferred modality. Such efficient organization is consistent with the adaptive signaling account of emotion perception. This theory predicts both that emotion signals be crossmodally redundant in order to survive communication across a noisy channel, and that receivers be specifically adapted to the crossmodal nature of the signal&#x2019;s structure. In other words, human emotion perception is optimized &#x201C;end-to-end&#x201D;&#x2014;all levels of the processing hierarchy are tuned to support the social goal of understanding the emotional states that predict others&#x2019; behavior.</p>
</sec>
<sec id="s9">
<title>Materials and Methods</title>
<sec id="s9a">
<title>Participants</title>
<p>79 participants (47 female) were recruited from the Dartmouth College student community to participate in the emotion evaluation task (experiment 1). 20 of these participants (11 female) also participated in the fMRI of emotion viewing task (experiment 2). All fMRI participants were right-handed and had normal or corrected-to-normal vision. All participants provided written informed consent, and the study was approved by the Dartmouth College Committee for the Protection of Human Subjects.</p>
</sec>
<sec id="s9b">
<title>Stimuli</title>
<p>Emotion stimuli were generated using an amodal dynamic model of movement across a number line with five parameters: speed, irregularity, consonance/spikiness, ratio of big-to-small movements, and ratio of upward-to-downward movements. Model output was mapped to either simple piano melodies or the movement of an animated bouncing ball. Each time the model was run, it probabilistically generated a new stimulus based on the current parameter settings. Participants in (<xref ref-type="bibr" rid="c37">Sievers et al., 2013</xref>) (music <italic>N</italic>&#x003D;25, movement <italic>N</italic>&#x003D;25, total <italic>N</italic>&#x003D;50) used this model to express five emotions: Angry, Happy, Peaceful, Sad, and Scared. For each emotion, parameter settings were similar for both music and movement. Details of the model are described in <xref ref-type="bibr" rid="c37">Sievers et al. (2013)</xref>. All stimuli are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/kvbqm/">https://osf.io/kvbqm/</ext-link>.</p>
<p>To reduce the influence of outliers, the median parameter settings across music and movement were used to generate stimuli for the present experiments. In addition to the five prototypical emotions listed above, we created mixed emotion stimuli by interpolating linearly between the parameter settings for each emotion pair; 25&#x0025;, 50&#x0025;, and 75&#x0025; mixes were used. We also added three putatively &#x201C;neutral&#x201D; or &#x201C;non-emotional&#x201D; parameter settings selected to be distant from all other stimuli. &#x201C;Search One&#x201D; and &#x201C;Search Four&#x201D; were selected by a Monte Carlo search algorithm, and consisted of extreme values for all five parameters. &#x201C;Biggest Gap&#x201D; was created by selecting the midpoint of the largest gap between the five prototypical emotions emotions and the parameter endpoints.</p>
<p>For each prototypical, mixed, and &#x201C;non-emotional&#x201D; parameter setting in each modality, we generated 20 exemplars, fora total of 1,520 stimuli (38 emotions x 2 modalities x 20 exemplars). Because stimuli were created using a probabilistic method, all exemplars were compared to a larger, separate sample of 5000 same-emotion examples to ensure no stimulus was further than one standard deviation from the category mean along any parameter.</p>
</sec>
<sec id="s9c">
<title>Experiment 1 (emotion evaluation)</title>
<p>Participants (<italic>N</italic>&#x003D;79,47 female) evaluated the emotion content of the stimuli. Stimuli were presented using a computer program that displayed five slider bars, one for each emotion prototype (Angry, Happy, Peaceful, Sad, and Scared). The on-screen order of slider bars and emotion stimuli were randomized across participants. Participants viewed or listened to each stimulus at least three times, and were asked to use the slider bars to evaluate what emotion or mix of emotions the stimulus expressed.</p>
</sec>
<sec id="s9d">
<title>Experiment 2 (fMRI of emotion viewing)</title>
<p>During each fMRI run, participants (<italic>N</italic>&#x003D;20,11 female) viewed 18 randomly selected exemplars from each of the 76 stimulus classes described above. Each stimulus class was shown once per run, and participants completed 18 runs across 3 separate scanning sessions (&#x223C;3 hours of scan time, 1,368 stimulus impressions). Each scan session was scheduled for approximately the same time of day, and no more than one week elapsed between scan sessions.</p>
<p>Stimuli were truncated to 3s in duration and followed by fixation periods of randomly varying duration (range: 0.5s&#x2013;20s). The ratio of simulus presentation to fixation was 1:1. A Monte Carlo procedure was used to select separate, optimized stimulus presentation orderings and timings for each participant.</p>
<p>This procedure used AFNI make_random_timing.py to generate thousands of possible stimulus timings, and AFNI 3dDeconvolve to select the timings that best supported deconvolving unique patterns of brain activity for each stimulus. Stimuli were presented using PsychoPy (<xref ref-type="bibr" rid="c33">Peirce, 2007</xref>). Participants were instructed to attend to the emotion content of the stimuli. During randomly interspersed catch trials (10 per run), participants used a button box to rate on a four-point scale whether the most recently presented stimulus had emotion content that was &#x201C;more mixed&#x201D; or &#x201C;more pure.&#x201D; To ensure familiarity with the stimuli, all fMRI participants had previously completed the emotion evaluation task.</p>
</sec>
<sec id="s9e">
<title>fMRI acquisition</title>
<p>Participants were scanned at the Dartmouth Brain Imaging Center using a 3T Phillips Achieva Intera scanner with a 32-channel head coil. Functional images were acquired using an echo-planar sequence (35ms TE, 3000ms TR; 90&#x00B0; flip angle; 3x3x3mm resolution) with 192 dynamic scans per run. A high resolution T1-weighted anatomical scan (3.7 ms TE; 8200ms TR; .938x.938x1mm resolution) was acquired at the end of each scanning session. Sound was delivered using an over-ear headphone system. Foam padding was placed around participants&#x2019; heads to minimize motion.</p>
</sec>
<sec id="s9f">
<title>fMRI preprocessing</title>
<p>Anatomical images were skull-stripped and aligned to the last TR of the last EPI image using AFNI align_epi_anat.py. EPI images were aligned to the last TR of the last EPI image using AFNI 3dvolreg. Rigid body transformations for aligning participants&#x2019; anatomical and EPI images to the AFNI version of the MNI 152 ICBM template were calculated using AFNI @auto_tlrc. Alignment transformations were concatenated and applied in a single step using AFNI 3dAllineate. EPI images were scaled to show percent signal change and concatenated. EPI images were not smoothed. The general linear model was used to estimate BOLD-responses evoked by each of the 76 emotional stimulus classes using AFNI 3dREMLfit.</p>
</sec>
<sec id="s9g">
<title>Representational similarity analysis</title>
<p>Representational similarity analysis (RSA) (<xref ref-type="bibr" rid="c24">Kriegeskorte et al., 2006</xref>, <xref ref-type="bibr" rid="c25">2008</xref>) was conducted using PyMVPA (<xref ref-type="bibr" rid="c14">Hanke et al., 2009</xref>) and Scikit-Learn (<xref ref-type="bibr" rid="c31">Pedregosa et al., 2012</xref>). Stimulus feature representational distance matrices (RDMs) for each of the parameters described in (<xref ref-type="bibr" rid="c37">Sievers et al., 2013</xref>) (speed, irregularity, consonance/spikiness, ratio of big-to-small movements, ratio of upward-to-downward movements) were created by calculating the Euclidean distances between the slider bar settings for each pair of emotions. Emotions in music and animation were created using the same slider bar settings, making it unecessary to create modality-specific feature RDMs. Emotion RDMs were created by calculating the Euclidean distance between the mean of each emotion judgment parameter in experiment 1 (Angry, Happy, Peaceful, Sad, and Scared) for each pair of stimuli. Emotion judgments were averaged across music and animation, making it unnecessary to create modality-specific emotion judgment RDMs. Intermodal RDMs were built by calculating the full multi-modality RDM including both music and movement stimuli and selecting its lower-left square region. Because the music and animation stimuli were created using the same slider bar settings, and because emotion judgments were averaged across modality, the mean distance between music stimuli was equal to the mean distance between animation stimuli. This ensured our analyses would not be sensitive to mean differences in BOLD activity between music and animation.</p>
<p>Representational similarity analysis was seperately conducted for music trials, animation trials, and (for the intermodal analysis) music and animation trials together. Each analysis used a spherical searchlight with a 3-voxel (9mm) radius. For music and animation trials, we calculated a neural RDM in each searchlight sphere by measuring the correlation distance between each estimated stimulus-evoked pattern of activation within modality. Intermodal neural RDMs were created by calculating the full multi-modality RDM including both music and movement stimuli and selecting its lower-left square region, containing only inter-modality distances (<xref ref-type="fig" rid="fig5">Figure 5A</xref>).</p>
<p>Multiple regression using least squares was used to assess how the neural RDM in each seachlight sphere could be expressed as a linear combination of our stimulus feature and emotion judgment RDMs. RDMs were rank-ordered before model fitting. This procedure generated beta weight and coefficient of determination (R<sup>2</sup>) maps for each participant, for each analysis. To locate areas fitting our model during both music and animation trials, per-participant overlap maps were created by identifying voxels where both music and animation model fit exceeded .02 and where the group level model fit was significant at FWER&#x003D;.05. Group level maps were calculated and corrected for multiple comparisons at voxelwise FWER&#x003D;.05 using permutation testing with BROCCOLI (<xref ref-type="bibr" rid="c10">Eklund, Dufort, Villani, &#x0026; Laconte, 2014</xref>). Maps were visualized using Nilearn (<xref ref-type="bibr" rid="c1">Abraham et al., 2014</xref>) and AFNI SUMA (<xref ref-type="bibr" rid="c35">Saad, Reynolds, Argall, Japee, &#x0026; Cox, 2004</xref>). All unthresholded statistical maps are available at <ext-link ext-link-type="uri" xlink:href="https://neurovault.org/collections/3399/">https://neurovault.org/collections/3399/</ext-link>.</p>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>Bibliography</title>
<ref id="c1"><mixed-citation publication-type="website"><string-name><surname>Abraham</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Pedregosa</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Eickenberg</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Gervais</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Muller</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kossaifi</surname>, <given-names>J.</given-names></string-name>,&#x2026; <string-name><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name> (<year>2014</year>). <article-title>Machine Learning for Neuroimaging with Scikit-Learn</article-title>, <volume>8</volume>(February), <fpage>1</fpage>&#x2013;<lpage>10</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fninf.2014.00014">https://doi.org/10.3389/fninf.2014.00014</ext-link></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="website"><string-name><surname>Adolphs</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Tranel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Damasio</surname>, <given-names>H.</given-names></string-name>, &#x0026; <string-name><surname>Damasio</surname>, <given-names>A.</given-names></string-name> (<year>1994</year>). <article-title>Impaired recognition of emotion in facial expressions following bilateral damage to the human amygdala</article-title>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/372669a0">https://doi.org/10.1038/372669a0</ext-link></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="book"><string-name><surname>Allport</surname>, <given-names>F. H.</given-names></string-name> (<year>1924</year>). <source>Social Psychology</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Houghton Mifflin</publisher-name>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="website"><string-name><surname>Beauchamp</surname>, <given-names>M. S.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Argall</surname>, <given-names>B.</given-names></string-name>, &#x0026; <string-name><surname>Martin</surname>, <given-names>A.</given-names></string-name> (<year>2004</year>). <article-title>Integration of auditory and visual information about objects in superior temporal sulcus</article-title>. <source>Neuron</source>, <volume>41</volume>, <fpage>809</fpage>&#x2013;<lpage>823</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0896-6273(04)00070-4">https://doi.org/10.1016/S0896-6273(04)00070-4</ext-link></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="website"><string-name><surname>Blakemore</surname>, <given-names>S.-J.</given-names></string-name> (<year>2008</year>). <article-title>The social brain in adolescence</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>9</volume>(<issue>4</issue>), <fpage>267</fpage>&#x2013;<lpage>277</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nrn2353">https://doi.org/10.1038/nrn2353</ext-link></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="website"><string-name><surname>Calder</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Lawrence</surname>, <given-names>A. D.</given-names></string-name>, &#x0026; <string-name><surname>Young</surname>, <given-names>A. W.</given-names></string-name> (<year>2001</year>). <article-title>Neuropsychology of Fear and Loathing</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>2</volume>(<issue>5</issue>), <fpage>352</fpage>&#x2013;<lpage>363</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/35072584">https://doi.org/10.1038/35072584</ext-link></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="website"><string-name><surname>Chikazoe</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>D. H.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, &#x0026; <string-name><surname>Anderson</surname>, <given-names>A. K.</given-names></string-name> (<year>2014</year>). <article-title>Population coding of affect across stimuli, modalities and individuals</article-title>. <source>Nature Neuroscience</source>, <volume>17</volume>(<issue>8</issue>), <fpage>1114</fpage>&#x2013;<lpage>1122</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.3749">https://doi.org/10.1038/nn.3749</ext-link></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="website"><string-name><surname>Clark</surname>, <given-names>A.</given-names></string-name> (<year>2013</year>). <article-title>Whatever next&#x003F; Predictive brains, situated agents, and the future of cognitive science</article-title>. <source>The Behavioral and Brain Sciences</source>, <volume>36</volume>(<issue>3</issue>), <fpage>181</fpage>&#x2013;<lpage>204</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0140525X12000477">https://doi.org/10.1017/S0140525X12000477</ext-link></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="website"><string-name><surname>Dezecache</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Mercier</surname>, <given-names>H.</given-names></string-name>, &#x0026; <string-name><surname>Scott-Phillips</surname>, <given-names>T. C.</given-names></string-name> (<year>2013</year>). <article-title>An evolutionary approach to emotional communication</article-title>. <source>Journal of Pragmatics</source>, <volume>59</volume>,<fpage>221</fpage>&#x2013;<lpage>233</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.pragma.2013.06.007">https://doi.org/10.1016/j.pragma.2013.06.007</ext-link></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="website"><string-name><surname>Eklund</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Dufort</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Villani</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Laconte</surname>, <given-names>S.</given-names></string-name> (<year>2014</year>). <article-title>BROCCOLI: Software for fast fMRI analysis on many-core CPUs and GPUs</article-title>. <source>Frontiers in Neuroinformatics</source>, <volume>8</volume>(March), <fpage>24</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fninf.2014.00024">https://doi.org/10.3389/fninf.2014.00024</ext-link></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="website"><string-name><surname>Ekman</surname>, <given-names>P.</given-names></string-name> (<year>1992</year>). <article-title>An argument for basic emotions</article-title>. <source>Cognition &#x0026; Emotion</source>, <volume>6</volume>(<issue>3</issue>), <fpage>169</fpage>&#x2013;<lpage>200</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1080/02699939208411068">https://doi.org/10.1080/02699939208411068</ext-link></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="website"><string-name><surname>Firestone</surname>, <given-names>C.</given-names></string-name>, &#x0026; <string-name><surname>Scholl</surname>, <given-names>B. J.</given-names></string-name> (<year>2016</year>). <article-title>Cognition does not affect perception: Evaluating the evidence for &#x201C;top-down&#x201D; effects</article-title>. <source>Behavioral and Brain Sciences</source>, <volume>39</volume>, <fpage>e229</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0140525X15000965">https://doi.org/10.1017/S0140525X15000965</ext-link></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Fodor</surname>, <given-names>J.A.</given-names></string-name> (<year>1985</year>). <article-title>Pr&#x00E9;cis ofThe Modularity of Mind</article-title>. <source>Behavioral and Brain Sciences</source>, <volume>8</volume>,<fpage>1</fpage>&#x2013;<lpage>42</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="website"><string-name><surname>Hanke</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Halchenko</surname>, <given-names>Y. O.</given-names></string-name>, <string-name><surname>Sederberg</surname>, <given-names>P. B.</given-names></string-name>, <string-name><surname>Hanson</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Haxby</surname>, <given-names>J. V.</given-names></string-name>, &#x0026; <string-name><surname>Pollmann</surname>, <given-names>S.</given-names></string-name> (<year>2009</year>). <article-title>PyMVPA: A python toolbox for multivariate pattern analysis of fMRI data</article-title>. <source>Neuroinformatics</source>, <volume>7</volume>(<issue>1</issue>), <fpage>37</fpage>&#x2013;<lpage>53</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s12021-008-9041-y">https://doi.org/10.1007/s12021-008-9041-y</ext-link></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="website"><string-name><surname>Hebets</surname>, <given-names>E. A.</given-names></string-name>, <string-name><surname>Barron</surname>, <given-names>A. B.</given-names></string-name>, <string-name><surname>Balakrishnan</surname>, <given-names>C. N.</given-names></string-name>, <string-name><surname>Hauber</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Mason</surname>, <given-names>P. H.</given-names></string-name>, &#x0026; <string-name><surname>Hoke</surname>, <given-names>K. L.</given-names></string-name> (<year>2016</year>). <article-title>A systems approach to animal communication</article-title>. <conf-name>Proceedings of the Royal Society B: Biological Sciences</conf-name>, <volume>203</volume>(<issue>1826</issue>), <fpage>20152889</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rspb.2015.2889">https://doi.org/10.1098/rspb.2015.2889</ext-link></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="website"><string-name><surname>Huron</surname>, <given-names>D.</given-names></string-name> (<year>2012</year>). <article-title>Understanding Music-Related Emotion: Lessons from Ethology</article-title>. <conf-name>Proceedings of the 12th International Conference on Music Perception and Cognition</conf-name>, <fpage>473</fpage>&#x2013;<lpage>481</lpage>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://icmpcescom2012.web.auth.gr/sites/default/files/papers/473&#x007B;&#x005C;_&#x007D;Proc.pdf">http://icmpc-escom2012.web.auth.gr/sites/default/files/papers/473&#x007B;&#x005C;_&#x007D;Proc.pdf</ext-link></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="website"><string-name><surname>Jack</surname>, <given-names>R. E.</given-names></string-name>, <string-name><surname>Caldara</surname>, <given-names>R.</given-names></string-name>, &#x0026; <string-name><surname>Schyns</surname>, <given-names>P. G.</given-names></string-name> (<year>2012</year>). <article-title>Internal representations reveal cultural diversity in expectations of facial expressions of emotion</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>141</volume>(<issue>1</issue>), <fpage>19</fpage>&#x2013;<lpage>25</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/a0023463">https://doi.org/10.1037/a0023463</ext-link></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="website"><string-name><surname>Jack</surname>, <given-names>R. E.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Delis</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Garrod</surname>, <given-names>O. G. B.</given-names></string-name>, &#x0026; <string-name><surname>Schyns</surname>, <given-names>P. G.</given-names></string-name> (<year>2016</year>). <article-title>Four not six: Revealing culturally common facial expressions of emotion</article-title>. <source>Journal of Experimental Psychology: General</source>, <volume>145</volume>(<issue>6</issue>), <fpage>708</fpage>&#x2013;<lpage>730</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037/xge0000162">https://doi.org/10.1037/xge0000162</ext-link></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="confproc"><string-name><surname>Johnstone</surname>, <given-names>R. A.</given-names></string-name> (<year>1996</year>). <article-title>Multiple displays in animal communication: &#x2018;backup signals&#x2019; and &#x2018;multiple messages&#x2019;</article-title>. <conf-name>Proceedings of the Royal Society B: Biological Sciences</conf-name>, <volume>351</volume>,<fpage>329</fpage>&#x2013;<lpage>338</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="book"><string-name><surname>Johnstone</surname>, <given-names>R. A.</given-names></string-name> (<year>1997</year>). <chapter-title>The evolution of animal signals</chapter-title>. In <string-name><given-names>J.</given-names> <surname>Krebs</surname></string-name> &#x0026; <string-name><given-names>N.</given-names> <surname>Davies</surname></string-name> (Eds.), <source>Behavioral ecology</source> (pp. <fpage>155</fpage>&#x2013;<lpage>178</lpage>). <publisher-loc>Oxford</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="website"><string-name><surname>Kim</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Shinkareva</surname>, <given-names>S. V.</given-names></string-name>, &#x0026; <string-name><surname>Wedell</surname>, <given-names>D. H.</given-names></string-name> (<year>2017</year>). <article-title>Representations of modality-general valence for videos and music derived from fMRI data</article-title>. <source>Neurolmage</source>, <volume>148</volume>(January), <fpage>42</fpage>&#x2013;<lpage>54</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2017.01.002">https://doi.org/10.1016/j.neuroimage.2017.01.002</ext-link></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="website"><string-name><surname>Kreifelts</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Ethofer</surname>,<given-names>T.</given-names></string-name>,<string-name><surname>Grodd</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Erb</surname>, <given-names>M.</given-names></string-name>,&#x0026;<string-name><surname>Wildgruber</surname>, <given-names>D.</given-names></string-name> (<year>2007</year>). <article-title>Audiovisual integration of emotional signals in voice and face: An event-related fMRI study</article-title>. <source>Neurolmage</source>,<volume>37</volume>(<issue>4</issue>), <fpage>1445</fpage>&#x2013;<lpage>1456</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2007.06.020">https://doi.org/10.1016/j.neuroimage.2007.06.020</ext-link></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="website"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, &#x0026; <string-name><surname>Kievit</surname>, <given-names>R. a.</given-names></string-name> (<year>2013</year>). <article-title>Representational geometry: integrating cognition, computation, and the brain</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>17</volume>(<issue>8</issue>), <fpage>401</fpage>&#x2013;<lpage>12</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2013.06.007">https://doi.org/10.1016/j.tics.2013.06.007</ext-link></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="website"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Goebel</surname>, <given-names>R.</given-names></string-name>, &#x0026; <string-name><surname>Bandettini</surname>, <given-names>P.</given-names></string-name> (<year>2006</year>). <article-title>Information-based functional brain mapping</article-title>. <conf-name>Proceedings of the National Academy of Sciences of the United States of America</conf-name>, <volume>103</volume>(<issue>10</issue>), <fpage>3863</fpage>&#x2013;<lpage>8</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.0600244103">https://doi.org/10.1073/pnas.0600244103</ext-link></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="website"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mur</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Bandettini</surname>, <given-names>P.</given-names></string-name> (<year>2008</year>). <article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title>. <source>Frontiers in Systems Neuroscience</source>, <volume>2</volume>(November), <fpage>4</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/neuro.06.004.2008">https://doi.org/10.3389/neuro.06.004.2008</ext-link></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="book"><string-name><surname>Lang</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Bradley</surname>, <given-names>M.</given-names></string-name>,&#x0026;<string-name><surname>Cuthbert</surname>, <given-names>B.</given-names></string-name> (<year>2008</year>). <chapter-title>International affective picture system (IAPS): Affective ratings of pictures and instruction manual</chapter-title>. <source>Technical Report A-8</source>. <publisher-loc>Gainesville, FL</publisher-loc>: <publisher-name>University of Florida</publisher-name>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="website"><string-name><surname>Lindquist</surname>, <given-names>K. A.</given-names></string-name>, <string-name><surname>Wager</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Kober</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Bliss-Moreau</surname>, <given-names>E.</given-names></string-name>, &#x0026; <string-name><surname>Barrett</surname>, <given-names>L. F.</given-names></string-name> (<year>2012</year>). <article-title>The brain basis of emotion: A meta-analytic review</article-title>. <source>Behavioral and Brain Sciences</source>, <volume>35</volume>(<issue>03</issue>), <fpage>121</fpage>&#x2013;<lpage>143</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1017/S0140525X11000446">https://doi.org/10.1017/S0140525X11000446</ext-link></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="book"><string-name><surname>Lorenz</surname>, <given-names>K.</given-names></string-name> (<year>1970</year>). <source>Studies in Animal and Human Behavior</source>, Volume <volume>1</volume>. <publisher-loc>London</publisher-loc>: <publisher-name>Methuen</publisher-name>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="website"><string-name><surname>McNamara</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Vega</surname>, <given-names>A.</given-names></string-name> <string-name><surname>de la</surname></string-name>, &#x0026; <string-name><surname>Yarkoni</surname>, <given-names>T.</given-names></string-name> (<year>2017</year>). <article-title>Developing a comprehensive framework for multimodal feature extraction</article-title>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1702.06151">http://arxiv.org/abs/1702.06151</ext-link></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="website"><string-name><surname>Norman</surname>, <given-names>K. a</given-names></string-name>, <string-name><surname>Polyn</surname>, <given-names>S. M.</given-names></string-name>, <string-name><surname>Detre</surname>, <given-names>G. J.</given-names></string-name>, &#x0026; <string-name><surname>Haxby</surname>, <given-names>J. V.</given-names></string-name> (<year>2006</year>). <article-title>Beyond mind-reading: multi-voxel pattern analysis of fMRI data</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>10</volume>(<issue>9</issue>), <fpage>424</fpage>&#x2013;<lpage>30</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.tics.2006.07.005">https://doi.org/10.1016/j.tics.2006.07.005</ext-link></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="website"><string-name><surname>Pedregosa</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Varoquaux</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Michel</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Thirion</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Grisel</surname>, <given-names>O.</given-names></string-name>,&#x2026; <string-name><surname>Duchesnay</surname>, <given-names>&#x00C9;</given-names></string-name>. (<year>2012</year>). <article-title>Scikit-learn: Machine Learning in Python</article-title>, <volume>12</volume>,<fpage>2825</fpage>&#x2013;<lpage>2830</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1007/s13398-014-0173-7.2">https://doi.org/10.1007/s13398-014-0173-7.2</ext-link></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="website"><string-name><surname>Peelen</surname>, <given-names>M. V.</given-names></string-name>, <string-name><surname>Atkinson</surname>, <given-names>A. P.</given-names></string-name>, &#x0026; <string-name><surname>Vuilleumier</surname>, <given-names>P.</given-names></string-name> (<year>2010</year>). <article-title>Supramodal Representations of Perceived Emotions in the Human Brain</article-title>. <source>Journal of Neuroscience</source>, <volume>30</volume>(<issue>30</issue>), <fpage>10127</fpage>&#x2013;<lpage>10134</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2161-10.2010">https://doi.org/10.1523/JNEUROSCI.2161-10.2010</ext-link></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="website"><string-name><surname>Peirce</surname>, <given-names>J. W.</given-names></string-name> (<year>2007</year>). <article-title>PsychoPy-Psychophysics software in Python</article-title>. <source>Journal of Neuroscience Methods</source>, <volume>162</volume>(<issue>1-2</issue>), <fpage>8</fpage>&#x2013;<lpage>13</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jneumeth.2006.11.017">https://doi.org/10.1016/j.jneumeth.2006.11.017</ext-link></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="website"><string-name><surname>Robins</surname>, <given-names>D. L.</given-names></string-name>, <string-name><surname>Hunyadi</surname>, <given-names>E.</given-names></string-name>, &#x0026; <string-name><surname>Schultz</surname>, <given-names>R. T.</given-names></string-name> (<year>2009</year>). <article-title>Superior temporal activation in response to dynamic audio-visual emotional cues</article-title>. <source>Brain and Cognition</source>, <volume>69</volume>(<issue>2</issue>), <fpage>269</fpage>&#x2013;<lpage>278</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.bandc.2008.08.007">https://doi.org/10.1016/j.bandc.2008.08.007</ext-link></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="website"><string-name><surname>Saad</surname>,<given-names>Z.S.</given-names></string-name>, <string-name><surname>Reynolds</surname>, <given-names>R. C.</given-names></string-name>,<string-name><surname>Argall</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Japee</surname>, <given-names>S.</given-names></string-name>,&#x0026;<string-name><surname>Cox</surname>, <given-names>R. W.</given-names></string-name> (<year>2004</year>). <article-title>SUMA: an interface for surface-based intra- and inter-subject analysis with AFNI</article-title>. <source>Biomedical Imaging: Nano to Macro, 2004. IEEE International Symposium on</source>, (October 2015), <fpage>1510</fpage>&#x2013;<lpage>1513</lpage> Vol. <volume>2</volume>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1109/ISBI.2004.1398837">https://doi.org/10.1109/ISBI.2004.1398837</ext-link></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="website"><string-name><surname>Saffran</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Aslin</surname>, <given-names>R. N.</given-names></string-name>, &#x0026; <string-name><surname>Newport</surname>, <given-names>E. L.</given-names></string-name> (<year>1996</year>). <article-title>Statistical learning by 8-month-old infants</article-title>. <source>Science</source> (<publisher-loc>New York</publisher-loc>, <publisher-name>N.Y</publisher-name>.), <volume>274</volume>(<issue>5294</issue>), <fpage>1926</fpage>&#x2013;<lpage>1928</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.274.5294.1926">https://doi.org/10.1126/science.274.5294.1926</ext-link></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="website"><string-name><surname>Sievers</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Polansky</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Casey</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Wheatley</surname>, <given-names>T.</given-names></string-name> (<year>2013</year>). <article-title>Music and movement share a dynamic structure that supports universal expressions of emotion</article-title>. <conf-name>Proceedings of the National Academy of Sciences of the United States of America</conf-name>, <volume>110</volume>(<issue>1</issue>), <fpage>70</fpage>&#x2013;<lpage>5</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1073/pnas.1209023110">https://doi.org/10.1073/pnas.1209023110</ext-link></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="website"><string-name><surname>Skerry</surname>, <given-names>A. E.</given-names></string-name>, &#x0026; <string-name><surname>Saxe</surname>, <given-names>R.</given-names></string-name> (<year>2015</year>). <article-title>Neural Representations of Emotion Are Organized around Abstract Event Features</article-title>. <source>Current Biology: CB</source>, <volume>25</volume>(<issue>15</issue>), <fpage>1945</fpage>&#x2013;<lpage>54</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/jxub.2015.06.009">https://doi.org/10.1016/jxub.2015.06.009</ext-link></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="website"><string-name><surname>Tooby</surname>, <given-names>J.</given-names></string-name>,&#x0026;<string-name><surname>Cosmides</surname>, <given-names>L.</given-names></string-name> (<year>1990</year>). <article-title>The past explains the present: Emotional adaptations and the structure of ancestral environments</article-title>. <source>Ethology andSociobiology</source>, <volume>11</volume>(<issue>4</issue>), <fpage>375</fpage>&#x2013;<lpage>424</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/0162-3095(90)90017-Z">https://doi.org/10.1016/0162-3095(90)90017-Z</ext-link></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="website"><string-name><surname>Watson</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Latinus</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Noguchi</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Garrod</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Crabbe</surname>, <given-names>F.</given-names></string-name>, &#x0026; <string-name><surname>Belin</surname>, <given-names>P.</given-names></string-name> (<year>2014</year>). <article-title>Crossmodal Adaptation in Right Posterior Superior Temporal Sulcus during Face-Voice Emotional Integration</article-title>. <source>Journal of Neuroscience</source>,<volume>34</volume>(<issue>20</issue>), <fpage>6813</fpage>&#x2013;<lpage>6821</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.4478-13.2014">https://doi.org/10.1523/JNEUROSCI.4478-13.2014</ext-link></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="website"><string-name><surname>Werner</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Noppeney</surname>, <given-names>U.</given-names></string-name> (<year>2010</year>). <article-title>Superadditive responses in superior temporal sulcus predict audiovisual benefits in object categorization</article-title>. <source>Cerebral Cortex</source>, <volume>20</volume>(<issue>8</issue>), <fpage>1829</fpage>&#x2013;<lpage>1842</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhp248">https://doi.org/10.1093/cercor/bhp248</ext-link></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="website"><string-name><surname>Wright</surname>, <given-names>T. M.</given-names></string-name>, <string-name><surname>Pelphrey</surname>, <given-names>K. A.</given-names></string-name>, <string-name><surname>Allison</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>McKeown</surname>, <given-names>M. J.</given-names></string-name>, &#x0026; <string-name><surname>McCarthy</surname>, <given-names>G.</given-names></string-name> (<year>2003</year>). <article-title>Polysensory interactions along lateral temporal regions evoked by audiovisual speech</article-title>. <source>Cerebral Cortex</source>, <volume>13</volume>(<issue>10</issue>), <fpage>1034</fpage>&#x2013;<lpage>1043</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/13.10.1034">https://doi.org/10.1093/cercor/13.10.1034</ext-link></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="website"><string-name><surname>Wyk</surname>, <given-names>B. C. V.</given-names></string-name>, <string-name><surname>Hudac</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Carter</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Sobel</surname>, <given-names>D. M.</given-names></string-name>, &#x0026; <string-name><surname>Pelphrey</surname>, <given-names>K. a.</given-names></string-name> (<year>2009</year>). <article-title>Action understanding in the superior temporal sulcus region</article-title>. <source>Psychological Science : A Journal of the American Psychological Society/APS</source>, <volume>20</volume>(<issue>6</issue>), <fpage>771</fpage>&#x2013;<lpage>7</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1111/j.1467-9280.2009.02359.x">https://doi.org/10.1111/j.1467-9280.2009.02359.x</ext-link></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="website"><string-name><surname>Yuki</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Maddux</surname>, <given-names>W. W.</given-names></string-name>, &#x0026; <string-name><surname>Masuda</surname>, <given-names>T.</given-names></string-name> (<year>2007</year>). <article-title>Are the windows to the soul the same in the East and West&#x003F; Cultural differences in using the eyes and mouth as cues to recognize emotions in Japan and the United States</article-title>. <source>Journal of Experimental Social Psychology</source>, <volume>43</volume>(<issue>2</issue>), <fpage>303</fpage>&#x2013;<lpage>311</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.jesp.2006.02.004">https://doi.org/10.1016/j.jesp.2006.02.004</ext-link></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="website"><string-name><surname>Zhu</surname>, <given-names>L. L.</given-names></string-name>, &#x0026; <string-name><surname>Beauchamp</surname>, <given-names>M. S.</given-names></string-name> (<year>2017</year>). <article-title>Mouth and Voice: A Relationship between Visual and Auditory Preference in the Human Superior Temporal Sulcus</article-title>. <source>The Journal of Neuroscience</source>, <volume>37</volume>(<issue>10</issue>), <fpage>2697</fpage>&#x2013;<lpage>2708</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.2914-16.2017">https://doi.org/10.1523/JNEUROSCI.2914-16.2017</ext-link></mixed-citation></ref>
</ref-list>
</back>
</article>