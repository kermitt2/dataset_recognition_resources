<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/248385</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Sampling Neuron Morphologies</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3374-4041</contrib-id>
<name>
<surname>Farhoodi</surname>
<given-names>Roozbeh</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8408-4499</contrib-id>
<name>
<surname>Kording</surname>
<given-names>Konrad Paul</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Sharif University of Technology</institution></aff>
<aff id="a2"><label>2</label><institution>University of Pennsylvania</institution></aff>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="other"><p><email>roozbehfarhoudi@gmail.com</email>, <email>koerding@gmail.coml</email></p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2018</year>
</pub-date>
<elocation-id>248385</elocation-id>
<history>
<date date-type="received">
<day>15</day>
<month>1</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>15</day>
<month>1</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>15</day>
<month>1</month>
<year>2018</year>
</date>
</history><permissions><copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license></permissions>
<self-uri xlink:href="248385.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>The intricate morphology of neurons has fascinated since the dawn of neuroscience, and yet, it is hard to synthesize them. Current algorithms typically define a growth process with parameters that allow matching aspects of the morphologies. However, such algorithmic growth processes are far simpler than the biological ones. What is needed is an algorithm that, given a database of morphologies, produces more of those. Here, we introduce a generator for neuron morphologies that is based on a statistical sampling process. Our Reversible Jump Markov chain Monte Carlo (RJMCMC) method starts with a trivial neuron and iteratively perturbs the morphology bringing the features close to those of the database. By quantifying the statistics of the generated neurons, we find that it outperforms growth-based models for many features. Good generative models for neuron morphologies promise to be important both for neural simulations and for morphology reconstructions from imaging data.</p></abstract>
<counts>
<page-count count="30"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>The morphology of neurons is beautiful and exhibits many regularities [<xref ref-type="bibr" rid="c1">1</xref>]. We can see the morphology as being the result of a process that optimizes aspects of wiring cost [<xref ref-type="bibr" rid="c2">2</xref>] [<xref ref-type="bibr" rid="c3">3</xref>]. Alternatively, we can see morphology as being the result of a growth process [<xref ref-type="bibr" rid="c4">4</xref>]. There are also multiple other rule-like aspects. For example, Pierrets rule tells that there is a correlation between the neuron size and length of its axonal size[<xref ref-type="bibr" rid="c1">1</xref>] and Larkman&#x2019;s rule tells us that diameter of segments in the neuron are reversely correlated with their length [<xref ref-type="bibr" rid="c5">5</xref>]. There are also important environmental influences on cellular morphologies [<xref ref-type="bibr" rid="c6">6</xref>] suggesting that neither simple optimization nor simple growth-rule approaches can be sufficient. There are many mechanisms at play in the process that generates neuron morphologies suggesting that a meaningful generator for neuron morphologies needs to be multifaceted.</p>
<p>The morphology of neurons is important as it affects the way neurons compute. Different parts of the dendritic tree can produce different kinds of signals. For example, the apical dendrite of layer 5 pyramidal neurons can produced Calcium spikes [<xref ref-type="bibr" rid="c7">7</xref>]. Similarly basal dendrites are often able to produce NMDA spikes [<xref ref-type="bibr" rid="c8">8</xref>]. Moreover, the travel of signals along the dendrites changes the signal transmission from a synapse to the soma [<xref ref-type="bibr" rid="c9">9</xref>]. The morphology of neurons is important for any kind of precise neuron simulation [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref>]. As such, morphologies are an important aspect of neuroscience.</p>
<p>Driven by recent interests in brain simulation as well as neuronal reconstruction, there is renewed interest in generative models for morphologies. Scientists may want to simulate more neurons than morphologies characterized by anatomists. This produces a need for generating morphologies that are like those that have been characterized[<xref ref-type="bibr" rid="c12">12</xref>]. Moreover, when trying to reconstruct neurons from 3D imaging data [<xref ref-type="bibr" rid="c13">13</xref>][<xref ref-type="bibr" rid="c14">14</xref>] [<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c16">16</xref>] we could also benefit from good generative models as a deviation may indicate a reconstruction mistake. Both of these fields could benefit considerably from the existence of good generative models for neuron morphologies.</p>
<p>Today&#x2019;s generators use multiple different intuitions [<xref ref-type="bibr" rid="c17">17</xref>]. One set of approaches uses a simple growth process. For example, they may start at the soma and, at every potential branching point sample from statistical descriptors[<xref ref-type="bibr" rid="c18">18</xref>] an idea which is used in popular tools [<xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>]. In that approach, all decisions about growth are strictly local at each subsequent branching point. A second set of approaches tries to follow the growth process and utilizes knowledge about the simultaneously developing segments [<xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c24">24</xref>]. These approaches effectively bootstrap off the idea of optimization [<xref ref-type="bibr" rid="c2">2</xref>]; however decisions are still unaffected by geometry. A third set of approaches includes knowledge of the geometry, e.g. by simulating neurotrophic particles [<xref ref-type="bibr" rid="c25">25</xref>]. This approach allows fast generation but ignores all aspects that are directly related to the growths process. All these approaches are based on the idea of producing neurons based on insights into the way neuron generation works, are conceptually beautiful, and successfully describe important aspects of morphologies.</p>
<p>Neuron morphologies have many characterized properties. They have certain shapes, certain distributions of branch angles, densities etc. Hence, it is hard to know how well a given generator characterizes real neurons. Each method may be good, or even provably optimal, at generating certain features. But if we acknowledge that the real generative process is more complicated than these generators we are faced with the problem of how we could generate neurons that obey many of the aspects of real neuron morphologies. We want to have a generator that can match many features from the dataset.</p>
<p>If we have a feature set that allows us to ask how probable a hypothesized morphology is based on a model derived from a database of real morphologies then we could generate neurons through a sampling process. Such an approach could try lots of neuron morphologies and basically find out which ones are more like those of real neurons and thus probable. In [<xref ref-type="bibr" rid="c26">26</xref>] such a process is used for generating textures. Generalizing this idea to morphologies is hard because the topological structure of the ambient space is nested. In fact, Markov chain Monte Carlo (MCMC) is a set of generally applicable methods that can be used to sample if we have a function that approximates the probability. In the case of morphologies that could be the probability of the morphology induced from a model fit to the feature set. What makes the problem complicated is the fact that different morphologies are not just different in the settings of parameters but about their number. In such cases Reversible jump MCMC is appropriate for sampling [<xref ref-type="bibr" rid="c27">27</xref>]. Such an approach would come with the promise of satisfying any number of features of the neuron morphology.</p>
<p>Here we present an approach based on Markov chain Monte Carlo methods for generating morphologies. By iterating changes, the chain becomes closer and closer to the generative distribution until it samples from a meaningful distribution. The advantage of our approach is that it generates morphologies based on a dataset of neurons, compared to other generative models which we can only optimize a small number of generator parameters. In this setup by adding more features or generally by making the generative model better, the resulting morphologies gradually become more realistic. We study the convergence and mixing time of the method and compare it with a recent model for generating morphologies.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<p>Morphologies are beautiful and important, raising the question of how they can be synthesized. We thus introduce an approach that generates new samples for a class of neurons by analyzing the morphologies in an existing database. We first introduce a rich set of features for quantifying the morphologies of a neuron class. Although the values of the features vary across the neurons, we can extract their salient statistics and join all features into one vector. This, along with a na&#x00EF;ve Bayes assumption, allows us to build a probabilistic model that represents a neuron class. Having probability distribution over the space of possible neurons enables us to use popular methods for extracting samples such as Reversible Jump Markov Chain Monte Carlo (RJMCMC). We will thus be able to construct morphologies that are statistically matched to those of the database.</p>
<sec id="s2a">
<label>2.1</label>
<title>Generative model</title>
<p>The similarity of neurons can be readily defined in a relevant feature space. Such a feature set could be learned from a large database [<xref ref-type="bibr" rid="c28">28</xref>], but here we use a hand-engineered feature set. Relevant features may be a real number, e.g. the total length of neuritis. They may be counts in a histogram, e.g. all the angles at the branching nodes within a certain interval (<xref ref-type="fig" rid="fig1">figure 1.a</xref>). They may also be a vector, e.g. the density of neuritis in cylindrical coordinates. While some real valued features reveal the global structure of a neuron, histograms and densities can describe the arborizations and branching patterns. By concatenating all the features, we represent each morphology by a long feature vector (<xref ref-type="fig" rid="fig1">figure 1.b</xref>). The difference in the features and hence distance of feature vectors can be used to compare two neurons.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><p>Representing a neuron morphology by its geometrical and morphological features. a) Morphology of a neuron is shown in the center, surrounding by its extracted features. b) By putting all the features together we can construt a long feature vector that describes is <xref ref-type="sec" rid="s4b">section 4.2</xref></p></caption>
<graphic xlink:href="248385_fig1.tif"/>
</fig>
<p>To attain a generative model for a neuron type, we want to be able to ask how representative a morphology is relative to those in a database. The morphologies in the database will all be different from one another because of randomness in the growth process, environmental conditions and, maybe disturbingly, details of the imaging techniques. To obtain a simple generative model we can characterize the database by the univariate means and variances of each feature (<xref ref-type="fig" rid="fig2">figure 2.a</xref>). To calculate the probability of a neuron under this generative model, we simply assume a normal model where each feature is assumed conditionally independent on the others, an assumption that is called naive Bayes. This gives a direct way of calculating the log probability of a morphology under the Gaussian model (<xref ref-type="fig" rid="fig2">figure 2.d</xref>) for each possible feature vector.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><p>Features can be used to define a generative model of a given database of neuron: a) 3 samples of pyramidal neurons are shown on the left. On the right side the mean and the variance of features across all the samples are shown. b) An arbitrary neuron and its features are shown. c) By taking the standard difference between the arbitrary neuron and the database the residual of features for two neuron are computed. d) the square sum of all residuals (scaled by their inverse std) defines a metric for the distance from database. Moreover by taking the exponential of distance we can define a Gaussian probability distribution over the space of all possible neurons. (for details of generative model look at 4.3).</p></caption>
<graphic xlink:href="248385_fig2.tif"/>
</fig>
<p>The number of possible neurons grows rapidly with the number of nodes. The degrees of freedom are three times the number of nodes. This makes it hard to know if the distribution of trees is meaningfully defined by the above defined distribution of features. One possible approach is to fix the dimension of the tree, e.g. by giving each neuron 1000 nodes. The other approach that we take in this paper is to correct the probabilities by normalizing them with a factor derived from the dimensionality of the tree (see Methods for details). With this assumption we can obtain a meaningful distribution over trees.</p>
<p>Once we have a generative model of trees based on feature similarity and dimensionality, we need a method for sampling morphologies. Here we used Monte Carlo Markov chain (MCMC) for sampling. MCMC is an efficient way to sample from a potentially nontrivial space. MCMC starts with an initial state and in each step, the current state will be perturbed to produce a proposal state (<xref ref-type="fig" rid="fig3">figure 3</xref>). We choose the Metropolis Hastings algorithm. <italic>x</italic> and <italic>x&#x2032;</italic> are the current state and the proposal state, respectively, and &#x2119;(<italic>x</italic>|<italic>y</italic>) is the probability of jumping from state <italic>x</italic> to state <italic>y</italic>. The probability of acceptance of the proposal is equal to:
<disp-formula id="eqn1"><alternatives><graphic xlink:href="248385_eqn1.gif"/></alternatives></disp-formula></p>
<p>This allows sampling from a problem with fixed dimensionality.</p>
<p>To generate a diverse set of trees, the number of nodes needs to change during sampling. Reversible jump MCMC (RJMCMC) can deal with this problem by including the Jacobian matrix and correcting the acceptance probability ([<xref ref-type="bibr" rid="c29">29</xref>]). In this case the second term in the expression 1 should be multiplied with the determinant of the Jacobian of the mapping from the domain of the current state to the proposed state. By iterating this process, the features of the current neuron gradually becomes closer to those from the database during the so-called burn-in. It then subsequently samples morphologies from the generative model.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><p>Schematic illustrating of MCMC algorithm for sampling neuron morphologies. When the morphology of a class of neuron is modeled by a probability distribution over the space of all possible morphologies, MCMC can sample. It starts with an initial neuron; possibly a trivial neuron (only soma) or an element of database. Iteratively, the current neuron is perturbed to make a proposal neuron. If the proposal neuron is closer to the database compare to the current neuron, it will replace it, otherwise it may be rejected. After many iterations the neuron gradually move toward the database.</p></caption>
<graphic xlink:href="248385_fig3.tif"/>
</fig>
<p>Since the space of all possible neurons is huge and the features are different, mixing time, the time it takes for the chain to converge to the meaningful probability distribution, matters. To address mixing we tested different perturbation method to change the neuron in each iterations and selected those that allowed faster mixing. Our perturbations change the morphological structure (see <xref ref-type="fig" rid="fig4">figure 4</xref>) and doing so move the morphology towards the high probability region. Some of our steps propose to add or remove nodes. Since the number of nodes changes across iterations, this perturbation jumps between different dimensional space and hence we have to implement RJMCMC. Other steps propose rotations of part of the neuron. This facilitates convergence of the geometrical picture of the neuron in addition to the statistics of its segments. And yet further steps propose sliding parts. This enhances the convergence of morphological features. The combination of all of these proposals converges reasonably fast.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><p>Proposal distributions. Some perturbation change the topological structure of the neuron and some of them change it geometrically (e.g. location). In the center an initial morphology is represented and around it are some possible proposals. These proposals usually change only parts of the morphology (red). In Sliding a part of the neuron moves over the morphology. This part is a connected component that attaches to random point on the neuron. In Extension a new node is added to an end point of the neuron. In Rotation one of the connected component of the morphology is rotated in 3d space.</p></caption>
<graphic xlink:href="248385_fig4.tif"/>
</fig>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Simple objects</title>
<p>To sanity-check our algorithm we first used it to generate simple objects. Since each neuron consists of segments we first test the algorithm to make a segment that is slightly curved. To measure this, we define bending as the fraction of the whole length of the object to the Euclidean distance between its endpoints. The goal is to generate segments with a bending ratio in the vicinity of 1.15. The algorithm starts with a straight line and in each step a random node of the segments will be selected and all the nodes on one side of it rotate. We find that the algorithm rapidly converges (<xref ref-type="fig" rid="fig5">Fig. 5.a</xref>). The algorithm thus seems to work well for simple problems of fixed dimensionality.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><p>Generating simple morphologies. Neurons are assembled by connecting segments therefore generating them can be reduce to generating segments and trees. a) To generate a segment, we started with straight line and each time rotate a part of it using MCMC algorithm. The objection is to have the bending ratio close to <italic>1.15</italic>. The middle diagram shows the convergence during the iterations. Nine samples are plotted on the right. b) To generate trees with segments, we allowed changing the number of nodes during iterations. The objection here is to generate a geometrical graph with around 50 nodes and 4 branching nodes such that its segments have bending ratio around <italic>1.15</italic>. Convergence of number of nodes (red) and bending ratio (blue) for generating one object is plotted. Nine samples are plotted on the right.</p></caption>
<graphic xlink:href="248385_fig5.tif"/>
</fig>
<p>To further check our algorithm we ran it on a case where the number of nodes does change. The objective is to both match the number of branching points, the number of nodes and also match the bending ratio. During this process the number of nodes can change. The algorithm readily achieves the defined objectives(<xref ref-type="fig" rid="fig5">figure 5.b</xref>). Asking for several features to be fit slows down convergence. Both the Metropolis Hastings and the RJMCMC parts by themselves seem to work well for simple, understandable, problems.</p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Generating Neurons</title>
<p>We now turn to our real objective, generating a morphology from a database of actual neuron morphologies. We start by analyzing layer 5 pyramidal cells in the neuromorpho database (<xref ref-type="fig" rid="fig6">6.a</xref>). Every neuron is different from one another but they share the morphological features of this neuron class.</p>
<p>To understand how our algorithm can replicate such a neuron, we analyze the evolution of the neuron over subsequent samples (<xref ref-type="fig" rid="fig6">6.b</xref>). We start with a trivial neuron, a soma with just seven equally spaced neurites. It takes the algorithm roughly 1000 samples to produce a morphology that roughly looks like a pyramidal neuron (upper). After another 25k iterations we obtain a morphology that looks quite real (lower right). Quantifying the features of the neuron reveals that some features converge quickly, e.g. distance (<xref ref-type="fig" rid="fig6">6.b</xref> upper) and others that converge quite slowly, e.g. the number of branching nodes (<xref ref-type="fig" rid="fig6">6.b</xref> lower). Importantly, when added across all features convergence happens, but very slowly. Indeed, even after 25k iterations they are probably not perfectly converged, which we share with most other real-world applications of MCMC. We thus see how the accumulation of small changes can give rise to complex tree morphologies.</p>
<p>We then test our algorithm on relatively distinct classes of neurons. Pyramidal, Tripolar, Purkinje and Stellate, in the database are considered (<xref ref-type="fig" rid="fig8">figure8</xref> right). For each class there are about 1000 morphologies. As we see in the figure the generated morphologies look similar to the real ones (<xref ref-type="fig" rid="fig2">figure2</xref> left). Some features are visually traceable in the generated neuron, like the density in different locations of space, yet other features are hard to inspect visually, e.g. the density of length of segments. Overall, the method works well across distinct classes of neuron morphologies.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><p>Generating a Pyramidal neuron: a) 3 real morphologies from a database of layer 5 pyramidal neuron are shown on left and a generated morphologies with our algorithm is shown on the right. The algorithm is run for 25k iterations and to depict how it converges a few samples during the iteration is plotted in (b). Above each neuron, the iteration's number is written. Since the general imagery of neuron did not change dramatically after 1000 iteration we plotted more neurons for the initial running. To explore the convergence we looked at the overall distance (left) as well a two features during the iterations (left and middle). The onset is the convergence up to 1000 iteration.</p></caption>
<graphic xlink:href="248385_fig6.tif"/>
</fig>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7:</label>
<caption><p>Convergence of features. We continue generating pyramidal neuron in <xref ref-type="fig" rid="fig6">figure 6</xref> by plot four histogram during the iterations. a) the database and the mean of each feature are shown. b) the histogram are shown during the a few iteration number. c) the distance of feature from database if shown.</p></caption>
<graphic xlink:href="248385_fig7.tif"/>
</fig>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8:</label>
<caption><p>Generated neurons from each of four classes in the database. For each classes, three typical samples are plotted to the left and three generated, morphologies with our method are shown in the right. See supplement for additional information</p></caption>
<graphic xlink:href="248385_fig8.tif"/>
</fig>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>Comparison with NeuGen</title>
<p>To conclude that our method is useful we need to compare to the current state of the art. We thus compare with the NeuGen package. NeuGen is a growth based model of generating morphologies that was developed in 2006 [<xref ref-type="bibr" rid="c19">19</xref>]. Although the generated neuron for both method looks good but when we look at a variety features RJMCMC shows its advantage (9). Moreover, it looks like RJMCMC produces a broader set of neurons. NeuGen also simulates the actual neural growth process, which is interesting for developmental applications, but our approach aims at just producing a matching morphology. RJMCMC produces morphologies that match the real ones in many ways and, in its focus on morphology quality vs process it offers a new tool to generate morphologies.</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<p>We have introduced a method for generating dendrite morphologies. It extracts human-chosen morphological features from a database. It then uses the means and variances of these features to construct a simple generative model using the naive Bayes assumption. Sampling from the resulting distribution is challenging because morphologies may present with different number of nodes. Using the neuromorpho.org for dataset and RJMCMC for sampling, we test our algorithm and find that it meaningfully matches the statistical distribution of the training dataset and produces morphologies that look strikingly similar to us. We find that our method properly matches the full feature set where a traditional generator for morphologies only matched a subset of feature statistics.</p>
<p>Countless processes contribute to the generation of neuron morphologies but we only use a limited number of features in our algorithm. Regulation happens through countless molecular cascades [<xref ref-type="bibr" rid="c30">30</xref>], mechanical factors [<xref ref-type="bibr" rid="c31">31</xref>, <xref ref-type="bibr" rid="c32">32</xref>], electrical activation [<xref ref-type="bibr" rid="c33">33</xref>] and geometrical limitations [<xref ref-type="bibr" rid="c34">34</xref>]. Each of these factors will be different, even across neurons of the same class, producing a highly structured multi-dimensional distribution[<xref ref-type="bibr" rid="c17">17</xref>]. And yet, neurons in one class typically share common characteristics, e.g. pyramidal neurons can be characterized by severally short basal dendrites and a large apical dendrite joined to an arborization in the tuft [<xref ref-type="bibr" rid="c35">35</xref>]. We have used a long feature vector (around 600 features). We can never be sure that there are no important features that we are missing. Future work, could use better features vectors to generate more meaningful morphologies.</p>
<p>An alternative to the human choice of features is to have algorithms choose those features. The recently popular framework of Generative Adversarial Networks (GANs) allows doing just that [<xref ref-type="bibr" rid="c28">28</xref>]. In that framework one network would figure out how realistic a neuron is by comparing the real and the simulated morphologies, while another system generates simulated morphologies. In a way, the system automatically finds the right features without human guidance. Using this method, GANs could successfully be trained to produce images of simple objects[<xref ref-type="bibr" rid="c36">36</xref>] or draw new paintings with the style of a given artist[<xref ref-type="bibr" rid="c37">37</xref>]. While exciting, it has been shown to be hard to generate a good statistical distribution using a GANs [<xref ref-type="bibr" rid="c38">38</xref>]. GANs may be a new hope for better generative models but they may suffer from statistical issues.</p>
<p>To convert our set of features into a generative model, we implicitly modeled them by an independent Gaussian distribution, which could not be further from the truth. First, we expect correlations of the features within a group, e.g. two close bins in the angular histogram (<xref ref-type="fig" rid="fig1">figure 1</xref>), or between groups; bending ratio (degree of straightness of the segments of a neuron) and the curvature. One could model the joint distribution in a more meaningful way, but that would be a hard statistical problem. Second, the assumption of Gaussian distribution might be violated for some features, e.g. those that represent counts. To overcome these issues, one can modify the generative model by using empirical distribution of the features from a big database. The generated morphologies from our method show that while assumption of independent distribution for features are far from real it can still produce satisfactory results.</p>
<p>While in theory RJMCMC samples evenly from the distribution, in practice we should expect it to have a large mixing time. Therefore, we should never expect chains to mix. To check that the neuron are meaningfully generated, we analyzed the convergence on the features presented (<xref ref-type="fig" rid="fig6">figure 6</xref>). However, this casual analysis cannot ensure that the chain properly mixes and generally there is no real solution to the convergence of MCMC[<xref ref-type="bibr" rid="c39">39</xref>]. Using a wide set of meaningful proposals enabled our technique to have a good acceptance rate and relatively quickly converge to high probability solutions. There are many ways how convergence behavior could be improved. For example, in the Ising model, the Wolff algorithm collectively flips the spin of a cluster of units and this can accelerate convergence by allowing the state to move non-locally [<xref ref-type="bibr" rid="c40">40</xref>].</p>
<p>Moreover, by using Hamiltonian MCMC we can potentially avoid random walk behavior in our sampling method [<xref ref-type="bibr" rid="c41">41</xref>]. Running multiple chains at different temperatures may also help improve mixing[<xref ref-type="bibr" rid="c42">42</xref>, <xref ref-type="bibr" rid="c43">43</xref>]. Importantly, we do not claim that we evenly sample from the generative model but the fact that we are good at matching the features should be enough for many applications.</p>
<p>While the proposals are inspired by neuronal development, our method can not speak to the issue of developmental neurobiology. Our approach is fundamentally different from previous approaches which they use an explicit growth process [<xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c24">24</xref>]. The biological inspiration of our proposals helped us speed up convergence. Moreover, the approach presented in this paper and previous growth-based works a may not be mutually exclusive. We could use their growth rules as part of our feature set. Or alternatively, we could initialize our algorithm with the results of their growth process, allowing a fine tuning of the results. Here we focused on de-novo generation of morphologies.</p>
<p>A good morphology generator could be useful for simulating a realistic neural network. After all, there is heterogeneity in morphology across neurons, and without a meaningful generator it is impossible to simulate networks of heterogeneous neurons. Sampled neurons could be used to find the link between the function of a neuron and its morphology [<xref ref-type="bibr" rid="c44">44</xref>] or its connectivity with other neurons [<xref ref-type="bibr" rid="c45">45</xref>]. With our methods its easy to generate samples of the morphology of any neuron type, which can facilitate realistic simulations.</p>
<p>One could also foresee that a good generative model could be useful for the segmentation of images. For example in electron macroscopy (EM) data a huge dataset of images is produced but there extracting the skeleton of each neuron is hard. Combining good generative models of neurons with current approaches promises better segmentations.</p>
</sec>
<sec id="s4">
<label>4</label>
<title>Materials and Methods</title>
<p>Neurons have a variety of morphological structures; Dendritic trees come in all shapes and sizes. They range from a total length of a few tens of micrometers to a few millimeters and vary significantly even within one neuronal class. Because of this wide spectrum of morphological shapes, building a growth model is usually difficult. On the other hand, the morphological structure can be characterized by a feature set. In this section, we start by a quantitative representation of neuron morphologies, a feature set. Based on these set, we construct a generative model and, using RJMCMC, sample from this generative model. This allows us to sample new dendritic trees that are statistically similar to those in the database.</p>
<sec id="s4a">
<label>4.1</label>
<title>Representation of a neuron&#x2019;s morphology</title>
<p>The geometry of neuronal arborizations can be stored in swc format [<xref ref-type="bibr" rid="c46">46</xref>]. In this format the morphology of a neuron is modeled by geometric graph where the nodes and edges represent the point on the morphology and links between the points, respectively (<xref ref-type="fig" rid="fig4">figure4.1.a</xref>). For each node three pieces of information is provided: its three-dimensional location, the radius of the biggest sphere contained in the morphology and the neurite type at the location of the node (soma, axon and dendrite). Computationally, it is easier to extract the features of the morphologies and perturb them using this compact representation.</p>
<sec id="s4a1">
<label>4.1.1</label>
<title>Sub-sampling of Nodes</title>
<p>Some morphological features depend on the number of nodes in the swc file. To make them comparable across all neurons in a database, we use a subsampling method. Our method preserves the terminals and branching nodes and meanwhile selecting the nodes on a segment such that the distance between two consecutive nodes is within a bound. More precisely, it starts from one end node and greedy pick the maximum number of nodes that distance between the consecutive nodes is bigger than the threshold (<xref ref-type="fig" rid="fig4">figure 4.1.b</xref>). While the lower bound increases the number of nodes in the sub-sampling method decreases and the neuron is approximated poorer (<xref ref-type="fig" rid="fig4">figure 4.1.c</xref>). By choosing a fixed bound for the sub-sampling method, we can approximate all the neurons in a database with the same quality, which facilitates feature extraction.</p>
</sec>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>Features Extraction</title>
<p>Graphical representation of the morphologies of a neuron alongside with the uniform sub-sampling of the nodes enables us to define their features. Some features we used are previously described as L-measure[<xref ref-type="bibr" rid="c47">47</xref>] and some of them are novel. Generally speaking, the features can be classified into 3 kinds: scalar, density and histogram. For the two later cases, we divide the whole range to bins and calculate the values in each bins. The type of each of the features and the size of feature (for the histogram the number of bin size) are written in the table 4.2. Here we shortly describe them:</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><p>Features of the neuron: the list of all the features</p></caption>
<graphic xlink:href="248385_tbl1.tif"/>
</table-wrap>
<sec id="s4b1">
<label>4.2.1</label>
<title>Number of nodes, branching, end nodes and initial segments</title>
<p>A neuron is built up by a binary tree except for the Soma. Four basic scalar features of this graph are the number of nodes, number of branching nodes, the number of end nodes and the number of nodes attached directly to the soma i.e. initial segments.</p>
</sec>
<sec id="s4b2">
<label>4.2.2</label>
<title>Global Angles</title>
<p>Global angles measure how straight the segments of the neuron are grown away from the soma. It is computed for each node by the angle between the direction that neurite has grown with respect to position of the soma as the origin and are defined by measuring the vector connecting the node to its parent and the vector connecting it to the Soma. Since the neurons have the tendency to explore the surrounding space, it is expected that in many nodes this angle is obtuse.</p>
</sec>
<sec id="s4b3">
<label>4.2.3</label>
<title>Local Angles</title>
<p>Local angles measure the straightness of the neurites by calculating the angles between two vectors: the vector connecting the node to its parent and the vector connecting the node to its child. Notice that the node has only one child to define the local angle. If the segment of the neuron is a flat line locally at the node, this value would be 180 degree. Lower values indicates the curvature of the neurite.</p>
</sec>
<sec id="s4b4">
<label>4.2.4</label>
<title>Branching Angles</title>
<p>At each branching node, we can be calculate the branching angle by computing the angle between the vectors connecting the branching node to its children (<xref ref-type="fig" rid="fig4">figure 4.2.12</xref>). Usually this is an acute angle.</p>
</sec>
<sec id="s4b5">
<label>4.2.5</label>
<title>Side Branching Angles</title>
<p>Since the neuron are 3d object, the branching angle itself can not describe the local shape of neuron around the branching node and we need to look at the side angles; the angle between the vector connecting a branching node to its children, and the angle connecting it to its parent(<xref ref-type="fig" rid="fig4">figure 4.2.12</xref>). Similar to the branching angle, we can approximate the side angle locally or segmentally.</p>
</sec>
<sec id="s4b6">
<label>4.2.6</label>
<title>Curvature</title>
<p>Local angles describe the directional changing in the segments in each intermediate node, however these changes are correlated for close-by immediate nodes. The curvature measures between the difference in two consecutive local angles. In[<xref ref-type="bibr" rid="c48">48</xref>] the author used the same measure for analyzing the curvature of the segments of the neurons of drosophila.</p>
</sec>
<sec id="s4b7">
<label>4.2.7</label>
<title>Spatial Distribution</title>
<p>Although neurons vary in the way they distribute in space, neurons from the same class usually share a similar spatial distribution. To quantify the spatial distribution, we rescale the neuron to put it in the unit 3d box. Then by meshing this box with a regular latices, the number of nodes that occupy a in every mesh can be counted. A schematic way of meshing is plotted in the <xref ref-type="fig" rid="fig4">figure 4.2.12</xref>. This feature is containing a rough approximation of Sholl analysis in particular[<xref ref-type="bibr" rid="c49">49</xref>] which first used to distinguish the visual and motor cortices of cats. In Sholl analysis the number of crossing for a circle of given radius is studied. We use a polar coordinate system to take into account symmetry orthogonal to the surface of many structures.</p>
</sec>
<sec id="s4b8">
<label>4.2.8</label>
<title>Self-Avoidances</title>
<p>Neurites spread in space but usually avoid self-intersections. Similar to the spatial distribution we can measure self avoidances of the neurites by makeing a regular lattice and counting the number of nodes in each box. We count the number of boxes that only contains one node.</p>
</sec>
<sec id="s4b9">
<label>4.2.9</label>
<title>Fractal growth</title>
<p>Similarly to the previous feature, we count the number of boxes that cover the neuron. This measure represent the fractal geometry of the the neuron [<xref ref-type="bibr" rid="c50">50</xref>].</p>
</sec>
<sec id="s4b10">
<label>4.2.10</label>
<title>Length of Neural Segments</title>
<p>The length of each segment is an important property of a class of neurons.</p>
</sec>
<sec id="s4b11">
<label>4.2.11</label>
<title>Bending</title>
<p>For each node the shortest neural path that connect it to the soma is usually close to a straight line. To make it concrete, for each node the ratio of its shortest path through the neuron to soma divided by the Euclidean distance between the node and Soma are calculated. By subtracting one and taking mean square of this ratio for all node we get the Neuronal/Euclidean ratio.</p>
</sec>
<sec id="s4b12">
<label>4.2.12</label>
<title>Segmental bending</title>
<p>Similar to the previous feature, we can measure the flatness of each segment of neuron by dividing the neural distance and its Euclidean distance.</p>
</sec>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>Generative model of the morphology</title>
<p>In this section, we define a probability distribution on the set of all possible morphologies given a database. In the previous section we defined a set of features for one neuron. We calculate the mean (<italic>&#x03BC;</italic><sub><italic>i</italic></sub>) and variance (<italic>&#x03C3;</italic><sub><italic>i</italic></sub>) of the features. Here we want to define a probability distribution over the space of all possible neurons such it concentrates around the points with the features of closing to those of the database. To fix translation invariant, we set the location of root (soma) to be at the origin. Suppose a neuron is made of <italic>n</italic> nodes and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="248385_inline1.gif"/></alternatives></inline-formula> is the vector in &#x211D;<sup>3</sup> that connect jth node to its parent (1 &#x2264; <italic>j</italic> &#x2264; <italic>n</italic> &#x2013; 1). Also, <italic>a</italic><sub><italic>i</italic></sub>, is the <italic>i</italic>th feature of the neuron. The probability distribution of the neuron is then proportional to:
<disp-formula id="eqn2"><alternatives><graphic xlink:href="248385_eqn2.gif"/></alternatives></disp-formula>
</p>
<p>Where <italic>m</italic>, <italic>v</italic> are two predefined values and:
<disp-formula id="eqn3"><alternatives><graphic xlink:href="248385_eqn3.gif"/></alternatives></disp-formula>
</p>
<p>Notice that the probability distribution is made of two products; the first factor is independent of the features and is defined such that if no feature is defined, then integration of <xref ref-type="disp-formula" rid="eqn2">equation 2</xref> over all possible neuron is finite (and is equal to one) and therefore the proposed probability distribution is well-defined. Keep in mind that once the length <italic>l</italic><sub><italic>i</italic></sub>&#x2019;s are fixed, then there are <italic>n</italic>! possibilities for connecting them to make a graph of tree. The second factor ensures that a neuron with the features close to the database has high probability value and therefore it has higher chance to be sampled.</p>
<p>Finally we have to define <italic>&#x03C3;</italic><sub><italic>i</italic></sub>&#x2019;s. Naively, we would like to choose it to be the standard deviation of feature <italic>i</italic> in the database. However, if we do so, we obtain a rather bizarre problem. The effective measure of models is not independent of the <italic>&#x03C3;</italic><sub><italic>i</italic></sub> for two reasons. First, features are not actually independent of one another which could be ameliorated by introducing extra parameters. Second, the number of possible trees co-varies with the <italic>&#x03C3;</italic><sub><italic>i</italic></sub>. There could be potential mathematically beautiful solutions to this problem. However, here we chose a simple pragmatic solution. We multiplied each <italic>&#x03C3;</italic><sub><italic>i</italic></sub> with 5 and each histogram feature we also divided by the number of bins. We find that this ad-hoc strategy reasonably corrects for the two mentioned biases.</p>
<sec id="s4c1">
<label>4.3.1</label>
<title>Markov chain Monte Carlo</title>
<p>To use the generative model it is necessary to have a way to draw samples form the distribution. Here we use Markov chain Mote Carlo method to generate samples of neuron morphology. It start from an initial neuron and in each iteration one of the perturbations is selected randomly (the perturbations are explained in the next section) to obtain a proposal morphology. In Metropolis Hasting, if the probability value of the proposal neuron is higher, it will replace the current neuron otherwise with the probability of their ratio (proposal to ratio) the proposal will be accepted. When the Markov chain is not symmetric, the acceptance probability should be modified to:
<disp-formula id="eqn4"><alternatives><graphic xlink:href="248385_eqn4.gif"/></alternatives></disp-formula>
or in other words:
<disp-formula id="eqn5"><alternatives><graphic xlink:href="248385_eqn5.gif"/></alternatives></disp-formula></p>
<p>The first ratio indeed compare two states and is called Bayesian posterior. The second ratio, <italic>symmetric ratio</italic>, is added to make the Markov chain symmetric and it should be calculated for each perturbation individually. For many of perturbation this ratio is 1 but especially for the perturbation that changes the dimension of the neuron (increase or decrease the number of compartments) it should be elaborately calculated.</p>
</sec>
<sec id="s4c2">
<label>4.3.2</label>
<title>Reversible jump Markov chain Monte Carlo</title>
<p>When the dimension of current and proposal neuron are different, neither of the ratios in <xref ref-type="disp-formula" rid="eqn5">equation 5</xref> are well defined. Reversible jump Markov chain Monte Carlo is a way to get around this problem by replacing the first ratio with the ratio of the probability density function of two probabilities and the second ratio with the Jacobian of the mapping between the two space([<xref ref-type="bibr" rid="c29">29</xref>] and [<xref ref-type="bibr" rid="c27">27</xref>]). In the list of proposals below, the first one needs this modification and we calculate and simplify the ratio there.</p>
</sec>
<sec id="s4c3">
<label>4.3.3</label>
<title>Initialization</title>
<p>An initial neuron is required to run the algorithm. We need a simple neuron with one node as soma and a chain of connecting nodes. For certain questions, generating neurons de-novo is not necessary and initializing with real neurons could leads to better results. To validate the method, we tried both ways of initialization.</p>
</sec>
</sec>
<sec id="s4d">
<label>4.4</label>
<title>list of proposals</title>
<p>To preform MCMC on neurons, we need a set of actions to perturb the neurons. When they act on a neuron, the shape of a neuron changes and therefore the features of the resulted neuron are different. Based on the probability density of the two neurons and the RJMCMC factor, the new neuron may be replaced with the initial one. Here we present the list of all perturbations on a neuron. Some of the perturbations change the neuron geometrically, for example rotating a part of neuron, some of them also change the graphical structure of neuron, for example by perturbing the connectivity between nodes. The perturbations used in this paper can be classified into three categories.</p>
<sec id="s4d1">
<label>4.4.1</label>
<title>Extending or Reducing the neuron</title>
<p>The neuron is made of a set of nodes and in this perturbation a node is added or removed. There are different ways to do this perturbation. When one of these sub perturbations are chosen, the node in selected uniformly from all possible nodes. The probability of selecting one of these perturbations and the possible way of choosing one node is shown schematically in the <xref ref-type="fig" rid="fig4">figure4.2.12.a</xref>. When the new node is created its location is drawn from a 3 dimensional Gaussian (with <italic>m</italic> and <italic>v</italic> as the mean and variance). When the created/removed node is an intermediate node, all the nodes after the created node are shifted by the location of the new node. Notice that the jumping from one space to another is made by projection onto multiple Euclidean spaces and therefore the RJM-CMC ratio for each of these perturbations are computed based on the number of possibilities and the reverse perturbations; and their ratio is generally not one. We define the number of possibilities of each neuron to be the number of potential jumps in <xref ref-type="fig" rid="fig4">figure4.2.12.a</xref>:
<disp-formula id="eqn6"><alternatives><graphic xlink:href="248385_eqn6.gif"/></alternatives></disp-formula></p>
<p>Then by using the <xref ref-type="disp-formula" rid="eqn2">equation 2</xref>, the probability of acceptance for extension is equal to:
<disp-formula><alternatives><graphic xlink:href="248385_ueqn1.gif"/></alternatives></disp-formula>
</p>
<p>Where <italic>l</italic><sup>new</sup>is the new vector added to the current neuron. Hence for example if the perturbation &#x201C;Removing one end-node from the set of nodes&#x201D; is selected, the RJMCMC ratio is equal to number of possibility for proposal neuron divided by the number of possibility for the current neuron.</p></sec>
<sec id="s4d2">
<label>4.4.2</label>
<title>Sliding a part of the neuron over itself</title>
<p>In this perturbation, the neuron is detached from a node to turn to two separate parts, then these two parts parallel transport and reattach in the another node of the neuron(<xref ref-type="fig" rid="fig4">figure4.4.b</xref>). As the result the topology of the neuron changes. The detached node can be any non-soma node and the reattached node should be a non-branch node. These two nodes can randomly be chosen from all the possible nodes of the neuron but to boost the MCMC, we used two ideas. First, the distance between the these two nodes are less than a certain limit. Second, in many neuron classes it gives a better result when the neuron is detached from one of the two outgoing segments of a branching node of the morphology. Because of that we put a different probability for choosing the detached parts of the morphology among the branching nodes verses non-branching nodes. Notice that this perturbation is symmetric and hence the relevant ratio is one.</p>
</sec>
<sec id="s4d3">
<label>4.4.3</label>
<title>Rotation</title>
<p>In this perturbation a part of neuron will be rotated along one node (<xref ref-type="fig" rid="fig4">figure 4.4.c</xref>). Specifically, a nodes will be selected and the part of the connected component which is not containing the soma would be rotated with a random unitary matrix in 3d. Similar to the previous perturbation, to boost MCMC we put different probability for selecting a uniformly random nodes of a neuron or a branching node.</p>
<p>The unitary random 3d matrix should be selected randomly from all unitary matrices. For boosting MCMC, it is better that the selected rotation be close to identity matrix and the probability distribution on all unitary matrix being symmetric. For doing that, we put a symmetric distribution on the space of 3d rotations which concentrate mostly around the identity. Since every rotation can be expressed as
<disp-formula id="eqn7"><alternatives><graphic xlink:href="248385_eqn7.gif"/></alternatives></disp-formula>
<disp-formula id="eqn8"><alternatives><graphic xlink:href="248385_eqn8.gif"/></alternatives></disp-formula>
we can set a probability distribution on the <italic>&#x03B8;</italic><sub><italic>i</italic></sub>&#x2019;s and hence make a probability distribution on the space of 3d rotations. The probability distributions on the <italic>Oi&#x2019;s</italic> are coming from one of the Von Mieses distributions which are symmetric distribution on the one dimensional circle. To symmetrize the 3d rotation, we use a simple trick of multiplying them in the form of:
<disp-formula id="eqn9"><alternatives><graphic xlink:href="248385_eqn9.gif"/></alternatives></disp-formula></p>
<p>Notice that by doing that the probability density at the matrix <italic>R</italic> is the same as <italic>R</italic><sup>-1</sup>.</p>
</sec>
</sec>
<sec id="s4e">
<label>4.5</label>
<title>Ergodicity</title>
<p>At the end, it should be taken into account that MCMC works only when the Markov chain is transitive, and here it can be checked easily it is the case. For example by removing the nodes from a given neuron we can produce a single node neuron (with <italic>p</italic> > 0) and by adding nodes we can go from single-node neuron to any neuron (again with <italic>p</italic> > 0).</p>
</sec>
<sec id="s4f">
<label>4.6</label>
<title>Implementation Details</title>
<p>The algorithm implemented in python with three main classes. Neuron class gives a neuron object containing nodes. Each node has geometrical attributes (location and type) in addition to topological ones (parent and children). Moreover it calculates the features indicated above once the object is called. The second class take a database of neurons and extracts the parameters for the algorithm. The third class does MCMC sampling starting from a simple neuron and by applying the proposals perturbations on the initial neuron in each iteration shapes the neuron closer to the database. The code is available at: Neuron Generator</p>
</sec>
</sec>

</body>
<back>
<ack>
<label>5</label>
<title>Acknowledgments</title>
<p>The authors thank Pavan Ramkumar and Hugo Fernandes for their significant discussion. Roozbeh Farhoodi was supported by Cognitive Sciences and Technologies Council (COGC) and National Foundation of Elites of Iran. Konrad Kording and Roozbeh Farhoodi were supported by NIH (U01MH109100, R01MH103910). KK initiated the first idea. KK and RF the mathematical foundations of idea. RF developed the software. KK and RF wrote the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="book"><string-name><given-names>S. R.</given-names> <surname>y Cajal</surname></string-name>, <source>Histology of the nervous system of man and vertebrates</source>, vol. <volume>1</volume>. <publisher-name>Oxford University Press</publisher-name>, <publisher-loc>USA</publisher-loc>, <year>1995</year>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><string-name><given-names>H.</given-names> <surname>Cuntz</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Forstner</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Borst</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Hausser</surname></string-name>, &#x201C;<article-title>One rule to grow them all: a general theory of neuronal branching and its practical application</article-title>,&#x201D; <source>PLoS Comput Biol</source>, vol. <volume>6</volume>, no. <issue>8</issue>, p. <fpage>e1000877</fpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><given-names>G. M.</given-names> <surname>Shepherd</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Stepanyants</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Bureau</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Chklovskii</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Svoboda</surname></string-name>, &#x201C;<article-title>Geometric and functional organization of cortical circuits</article-title>,&#x201D; <source>Nature neuroscience</source>, vol. <volume>8</volume>, no. <issue>6</issue>, pp. <fpage>782</fpage>&#x2013;<lpage>790</lpage>, <year>2005</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><string-name><given-names>Q.</given-names> <surname>Wen</surname></string-name> and <string-name><given-names>D. B.</given-names> <surname>Chklovskii</surname></string-name>, &#x201C;<article-title>A cost-benefit analysis of neuronal morphology</article-title>,&#x201D; <source>Journal of neurophysiology</source>, vol. <volume>99</volume>, no. <issue>5</issue>, pp. <fpage>2320</fpage>&#x2013;<lpage>2328</lpage>, <year>2008</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><string-name><given-names>A. U.</given-names> <surname>Larkman</surname></string-name>, &#x201C;<article-title>Dendritic morphology of pyramidal neurones of the visual cortex of the rat: Iii. spine distributions</article-title>,&#x201D; <source>Journal of comparative neurology</source>, vol. <volume>306</volume>, no. <issue>2</issue>, pp. <fpage>332</fpage>&#x2013;<lpage>343</lpage>, <year>1991</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Tallinen</surname></string-name>, <string-name><given-names>J. Y.</given-names> <surname>Chung</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Rousseau</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Girard</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Lefevre</surname></string-name>, and <string-name><given-names>L.</given-names> <surname>Mahadevan</surname></string-name>, &#x201C;<article-title>On the growth and form of cortical convolutions</article-title>,&#x201D; <source>Nature Physics</source>, <year>2016</year>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><string-name><given-names>M. E.</given-names> <surname>Larkum</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Kaiser</surname></string-name>, and <string-name><given-names>B.</given-names> <surname>Sakmann</surname></string-name>, &#x201C;<article-title>Calcium electrogenesis in distal apical dendrites of layer 5 pyramidal cells at a critical frequency of back-propagating action potentials</article-title>,&#x201D; <source>Proceedings of the National Academy of Sciences</source>, vol. <volume>96</volume>, no. <issue>25</issue>, pp. <fpage>14600</fpage>&#x2013;<lpage>14604</lpage>, <year>1999</year>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Schiller</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Major</surname></string-name>, <string-name><given-names>H. J.</given-names> <surname>Koester</surname></string-name>, and <string-name><given-names>Y.</given-names> <surname>Schiller</surname></string-name>, &#x201C;<article-title>Nmda spikes in basal dendrites of cortical pyramidal neurons</article-title>,&#x201D; <source>Nature</source>, vol. <volume>404</volume>, no. <issue>6775</issue>, pp. <fpage>285</fpage>&#x2013;<lpage>289</lpage>, <year>2000</year>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><given-names>D. A.</given-names> <surname>Henze</surname></string-name>, <string-name><given-names>W. E.</given-names> <surname>Cameron</surname></string-name>, and <string-name><given-names>G.</given-names> <surname>Barrionuevo</surname></string-name>, &#x201C;<article-title>Dendritic morphology and its effects on the amplitude and rise-time of synaptic signals in hippocampal ca3 pyramidal cells</article-title>,&#x201D; <source>The Journal of comparative neurology</source>, vol. <volume>369</volume>, no. <issue>3</issue>, pp. <fpage>331</fpage>&#x2013;<lpage>334</lpage>, <year>1996</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><string-name><given-names>M. L.</given-names> <surname>Hines</surname></string-name> and <string-name><given-names>N. T.</given-names> <surname>Carnevale</surname></string-name>, &#x201C;<article-title>The neuron simulation environment</article-title>,&#x201D; <source>NEURON</source>, vol. <volume>9</volume>, no. <issue>6</issue>, <year>2006</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="other"><string-name><given-names>J. M.</given-names> <surname>Bower</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Beeman</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Hucka</surname></string-name>, &#x201C;<article-title>The genesis simulation system</article-title>,&#x201D; <year>2003</year>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><string-name><given-names>H.</given-names> <surname>Markram</surname></string-name>, &#x201C;<article-title>The blue brain project</article-title>,&#x201D; <source>Nature Reviews Neuroscience</source>, vol. <volume>7</volume>, no. <issue>2</issue>, pp. <fpage>153</fpage>&#x2013;<lpage>160</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Helmstaedter</surname></string-name>, <string-name><given-names>K. L.</given-names> <surname>Briggman</surname></string-name>, <string-name><given-names>S. C.</given-names> <surname>Turaga</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Jain</surname></string-name>, <string-name><given-names>H. S.</given-names> <surname>Seung</surname></string-name>, and <string-name><given-names>W.</given-names> <surname>Denk</surname></string-name>, &#x201C;<article-title>Connectomic reconstruction of the inner plexiform layer in the mouse retina</article-title>,&#x201D; <source>Nature</source>, vol. <volume>500</volume>, no. <issue>7461</issue>, pp. <fpage>168</fpage>&#x2013;<lpage>174</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><given-names>Z.</given-names> <surname>Zheng</surname></string-name>, <string-name><given-names>J. S.</given-names> <surname>Lauritzen</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Perlman</surname></string-name>, <string-name><given-names>C. G.</given-names> <surname>Robinson</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Nichols</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Milkie</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Torrens</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Price</surname></string-name>, <string-name><given-names>C. B.</given-names> <surname>Fisher</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Sharifi</surname></string-name>, <etal>et al.</etal>, &#x201C;<article-title>A complete electron microscopy volume of the brain of adult drosophila melanogaster</article-title>,&#x201D; <source>bioRxiv</source>, p. <fpage>140905</fpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><string-name><given-names>H.</given-names> <surname>Peng</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Ruan</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Long</surname></string-name>, <string-name><given-names>J. H.</given-names> <surname>Simpson</surname></string-name>, and <string-name><given-names>E. W.</given-names> <surname>Myers</surname></string-name>, &#x201C;<article-title>V3d enables real-time 3d visualization and quantitative analysis of large-scale biological image data sets</article-title>,&#x201D; <source>Nature biotechnology</source>, vol. <volume>28</volume>, no. <issue>4</issue>, pp. <fpage>348</fpage>&#x2013;<lpage>353</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><string-name><given-names>A. R.</given-names> <surname>Jones</surname></string-name>, <string-name><given-names>C. C.</given-names> <surname>Overly</surname></string-name>, and <string-name><given-names>S. M.</given-names> <surname>Sunkin</surname></string-name>, &#x201C;<article-title>The allen brain atlas: 5 years and beyond</article-title>,&#x201D; <source>Nature reviews. Neuroscience</source>, vol. <volume>10</volume>, no. <issue>11</issue>, p. <fpage>821</fpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="book"><string-name><given-names>B.</given-names> <surname>Torben-Nielsen</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Cuntz</surname></string-name>, &#x201C;<chapter-title>Introduction to dendritic morphology</chapter-title>,&#x201D; in <source>The Computing Dendrite</source>, pp. <fpage>3</fpage>&#x2013;<lpage>22</lpage>, <publisher-name>Springer</publisher-name>, <year>2014</year>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><string-name><given-names>R.</given-names> <surname>Burke</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Marks</surname></string-name>, and <string-name><given-names>B.</given-names> <surname>Ulfhake</surname></string-name>, &#x201C;<article-title>A parsimonious description of motoneuron dendritic morphology using computer simulation</article-title>,&#x201D; <source>Journal of Neuroscience</source>, vol. <volume>12</volume>, no. <issue>6</issue>, pp. <fpage>2403</fpage>&#x2013;<lpage>2416</lpage>, <year>1992</year>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><string-name><given-names>J. P.</given-names> <surname>Eberhard</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Wanner</surname></string-name>, and <string-name><given-names>G.</given-names> <surname>Wittum</surname></string-name>, &#x201C;<article-title>Neugen: a tool for the generation of realistic morphology of cortical neurons and neural networks in 3d</article-title>,&#x201D; <source>Neurocomputing</source>, vol. <volume>70</volume>, no. <issue>1</issue>, pp. <fpage>327</fpage>&#x2013;<lpage>342</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><string-name><given-names>R. A.</given-names> <surname>Koene</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Tijms</surname></string-name>, <string-name><given-names>P.</given-names> <surname>van Hees</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Postma</surname></string-name>, <string-name><given-names>A.</given-names> <surname>de Ridder</surname></string-name>, <string-name><given-names>G. J.</given-names> <surname>Ramakers</surname></string-name>, <string-name><given-names>J.</given-names> <surname>van Pelt</surname></string-name>, and <string-name><given-names>A.</given-names> <surname>van Ooyen</surname></string-name>, &#x201C;<article-title>Netmorph: a framework for the stochastic generation of large scale neuronal networks with realistic neuron morphologies</article-title>,&#x201D; <source>Neuroinformatics</source>, vol. <volume>7</volume>, no. <issue>3</issue>, pp. <fpage>195</fpage>&#x2013;<lpage>210</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><string-name><given-names>G. A.</given-names> <surname>Ascoli</surname></string-name> and <string-name><given-names>J. L.</given-names> <surname>Krichmar</surname></string-name>, &#x201C;<article-title>L-neuron: a modeling tool for the efficient generation and parsimonious description of dendritic morphology</article-title>,&#x201D; <source>Neurocomputing</source>, vol. <volume>32</volume>, pp. <fpage>1003</fpage>&#x2013;<lpage>1011</lpage>, <year>2000</year>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Torben-Nielsen</surname></string-name> and <string-name><given-names>E.</given-names> <surname>De Schutter</surname></string-name>, &#x201C;<article-title>Context-aware modeling of neuronal morphologies</article-title>,&#x201D; <source>Frontiers in neuroanatomy</source>, vol. <volume>8</volume>, p. <fpage>92</fpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><string-name><given-names>F.</given-names> <surname>Zubler</surname></string-name> and <string-name><given-names>R.</given-names> <surname>Douglas</surname></string-name>, &#x201C;<article-title>A framework for modeling the growth and development of neurons and networks</article-title>,&#x201D; <source>Frontiers in computational neuroscience</source>, vol. <volume>3</volume>, p. <fpage>25</fpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><string-name><given-names>P.</given-names> <surname>Gleeson</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Steuber</surname></string-name>, and <string-name><given-names>R. A.</given-names> <surname>Silver</surname></string-name>, &#x201C;<article-title>neuroconstruct: a tool for modeling networks of neurons in 3d space</article-title>,&#x201D; <source>Neuron</source>, vol. <volume>54</volume>, no. <issue>2</issue>, pp. <fpage>219</fpage>&#x2013;<lpage>235</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Luczak</surname></string-name>, &#x201C;<article-title>Measuring neuronal branching patterns using model-based approach</article-title>,&#x201D; <source>Frontiers in computational neuroscience</source>, vol. <volume>4</volume>, <year>2010</year>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Portilla</surname></string-name> and <string-name><given-names>E. P.</given-names> <surname>Simoncelli</surname></string-name>, &#x201C;<article-title>A parametric texture model based on joint statistics of complex wavelet coefficients</article-title>,&#x201D; <source>International journal of computer vision</source>, vol. <volume>40</volume>, no. <issue>1</issue>, pp. <fpage>49</fpage>&#x2013;<lpage>70</lpage>, <year>2000</year>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><string-name><given-names>P. J.</given-names> <surname>Green</surname></string-name> and <string-name><given-names>D. I.</given-names> <surname>Hastie</surname></string-name>, &#x201C;<article-title>Reversible jump mcmc</article-title>,&#x201D; <source>Genetics</source>, vol. <volume>155</volume>, no. <issue>3</issue>, pp. <fpage>1391</fpage>&#x2013;<lpage>1403</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><string-name><given-names>I.</given-names> <surname>Goodfellow</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Pouget-Abadie</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Mirza</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Xu</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Warde-Farley</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Ozair</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Courville</surname></string-name>, and <string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name>, &#x201C;<article-title>Generative adversarial nets</article-title>,&#x201D; in <source>Advances in neural information processing systems</source>, pp. <fpage>2672</fpage>&#x2013;<lpage>2680</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><string-name><given-names>P. J.</given-names> <surname>Green</surname></string-name>, &#x201C;<article-title>Reversible jump markov chain monte carlo computation and bayesian model determination</article-title>,&#x201D; <source>Biometrika</source>, vol. <volume>82</volume>, no. <issue>4</issue>, pp. <fpage>711</fpage>&#x2013;<lpage>732</lpage>, <year>1995</year>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Henley</surname></string-name> and <string-name><given-names>M.-m.</given-names> <surname>Poo</surname></string-name>, &#x201C;<article-title>Guiding neuronal growth cones using ca 2&#x002B; signals</article-title>,&#x201D; <source>Trends in cell biology</source>, vol. <volume>14</volume>, no. <issue>6</issue>, pp. <fpage>320</fpage>&#x2013;<lpage>330</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><string-name><given-names>E. V.</given-names> <surname>Romanova</surname></string-name>, <string-name><given-names>K. A.</given-names> <surname>Fosser</surname></string-name>, <string-name><given-names>S. S.</given-names> <surname>Rubakhin</surname></string-name>, <string-name><given-names>R. G.</given-names> <surname>Nuzzo</surname></string-name>, and <string-name><given-names>J. V.</given-names> <surname>Sweedler</surname></string-name>, &#x201C;<article-title>Engineering the morphology and electrophysiological parameters of cultured neurons by microfluidic surface patterning</article-title>,&#x201D; <source>The FASEB journal</source>, vol. <volume>18</volume>, no. <issue>11</issue>, pp. <fpage>1267</fpage>&#x2013;<lpage>1269</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Gotz</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Hartfuss</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Malatesta</surname></string-name>, &#x201C;<article-title>Radial glial cells as neuronal precursors: a new perspective on the correlation of morphology and lineage restriction in the developing cerebral cortex of mice</article-title>,&#x201D; <source>Brain research bulletin</source>, vol. <volume>57</volume>, no. <issue>6</issue>, pp. <fpage>777</fpage>&#x2013;<lpage>788</lpage>, <year>2002</year>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Kaehr</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Allen</surname></string-name>, <string-name><given-names>D. J.</given-names> <surname>Javier</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Currie</surname></string-name>, and <string-name><given-names>J. B.</given-names> <surname>Shear</surname></string-name>, &#x201C;<article-title>Guiding neuronal development with in situ microfabrication</article-title>,&#x201D; <source>Proceedings of the National Academy of Sciences of the United States of America</source>, vol. <volume>101</volume>, no. <issue>46</issue>, pp. <fpage>16104</fpage>&#x2013;<lpage>16108</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><string-name><given-names>P.</given-names> <surname>Clark</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Britland</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Connolly</surname></string-name>, &#x201C;<article-title>Growth cone guidance and neuron morphology on micropatterned laminin surfaces</article-title>,&#x201D; <source>Journal of cell science</source>, vol. <volume>105</volume>, no. <issue>1</issue>, pp. <fpage>203</fpage>&#x2013;<lpage>212</lpage>, <year>1993</year>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><string-name><given-names>N.</given-names> <surname>Spruston</surname></string-name>, &#x201C;<article-title>Pyramidal neurons: dendritic structure and synaptic integration</article-title>,&#x201D; <source>Nature Reviews Neuroscience</source>, vol. <volume>9</volume>, no. <issue>3</issue>, pp. <fpage>206</fpage>&#x2013;<lpage>221</lpage>, <year>2008</year>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="other"><string-name><given-names>A.</given-names> <surname>Radford</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Metz</surname></string-name>, and <string-name><given-names>S.</given-names> <surname>Chintala</surname></string-name>, &#x201C;<article-title>Unsupervised representation learning with deep convolutional generative adversarial networks</article-title>,&#x201D; <source>arXiv preprint</source> <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1511.06434">arXiv:1511.06434</ext-link>, <year>2015</year>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="other"><string-name><given-names>L. A.</given-names> <surname>Gatys</surname></string-name>, <string-name><given-names>A. S.</given-names> <surname>Ecker</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Bethge</surname></string-name>, &#x201C;<article-title>A neural algorithm of artistic style</article-title>,&#x201D; <source>arXiv preprint</source> <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1508.06576">arXiv:1508.06576</ext-link>, <year>2015</year>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="other"><string-name><given-names>M.</given-names> <surname>Arjovsky</surname></string-name> and <string-name><given-names>L.</given-names> <surname>Bottou</surname></string-name>, &#x201C;<article-title>Towards principled methods for training generative adversarial networks</article-title>,&#x201D; <source>arXiv preprint</source> <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1701.04862">arXiv:1701.04862</ext-link>, <year>2017</year>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><string-name><given-names>K. L.</given-names> <surname>Mengersen</surname></string-name>, <string-name><given-names>C. P.</given-names> <surname>Robert</surname></string-name>, and <string-name><given-names>C.</given-names> <surname>Guihenneuc-Jouyaux</surname></string-name>, &#x201C;<article-title>Mcmc convergence diagnostics: a reviewww</article-title>,&#x201D; <source>Bayesian statistics</source>, vol. <volume>6</volume>, pp. <fpage>415</fpage>&#x2013;<lpage>440</lpage>, <year>1999</year>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><string-name><given-names>U.</given-names> <surname>Wolff</surname></string-name>, &#x201C;<article-title>Collective monte carlo updating for spin systems</article-title>,&#x201D; <source>Physical Review Letters</source>, vol. <volume>62</volume>, no. <issue>4</issue>, p. <fpage>361</fpage>, <year>1989</year>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><string-name><given-names>R. M.</given-names> <surname>Neal</surname></string-name> <etal>et al.</etal>, &#x201C;<article-title>Mcmc using hamiltonian dynamics</article-title>,&#x201D; <source>Handbook of Markov Chain Monte Carlo</source>, vol. <volume>2</volume>, no. <issue>11</issue>, <year>2011</year>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><string-name><given-names>R. H.</given-names> <surname>Swendsen</surname></string-name> and <string-name><given-names>J.-S.</given-names> <surname>Wang</surname></string-name>, &#x201C;<article-title>Replica monte carlo simulation of spinglasses</article-title>,&#x201D; <source>Physical Review Letters</source>, vol. <volume>57</volume>, no. <issue>21</issue>, p. <fpage>2607</fpage>, <year>1986</year>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><string-name><given-names>D. J.</given-names> <surname>Earl</surname></string-name> and <string-name><given-names>M. W.</given-names> <surname>Deem</surname></string-name>, &#x201C;<article-title>Parallel tempering: Theory, applications, and new perspectives</article-title>,&#x201D; <source>Physical Chemistry Chemical Physics</source>, vol. <volume>7</volume>, no. <issue>23</issue>, pp. <fpage>3910</fpage>&#x2013;<lpage>3916</lpage>, <year>2005</year>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><string-name><given-names>K. M.</given-names> <surname>Stiefel</surname></string-name> and <string-name><given-names>T. J.</given-names> <surname>Sejnowski</surname></string-name>, &#x201C;<article-title>Mapping function onto neuronal morphology</article-title>,&#x201D; <source>Journal of neurophysiology</source>, vol. <volume>98</volume>, no. <issue>1</issue>, pp. <fpage>513</fpage>&#x2013;<lpage>526</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><string-name><given-names>S. L.</given-names> <surname>Hill</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Riachi</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Sch&#x00FC;rmann</surname></string-name>, and <string-name><given-names>H.</given-names> <surname>Markram</surname></string-name>, &#x201C;<article-title>Statistical connectivity provides a sufficient foundation for specific functional connectivity in neocortical neural microcircuits</article-title>,&#x201D; <source>Proceedings of the National Academy of Sciences</source>, vol. <volume>109</volume>, no. <issue>42</issue>, pp. <fpage>E2885</fpage>&#x2013;<lpage>E2894</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><string-name><given-names>E.</given-names> <surname>Stockley</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Cole</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Brown</surname></string-name>, and <string-name><given-names>H.</given-names> <surname>Wheal</surname></string-name>, &#x201C;<article-title>A system for quantitative morphological measurement and electrotonic modelling of neurons: threedimensional reconstruction</article-title>,&#x201D; <source>Journal of neuroscience methods</source>, vol. <volume>47</volume>, no. <issue>1</issue>, pp. <fpage>39</fpage>&#x2013;<lpage>51</lpage>, <year>1993</year>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><string-name><given-names>R.</given-names> <surname>Scorcioni</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Polavaram</surname></string-name>, and <string-name><given-names>G. A.</given-names> <surname>Ascoli</surname></string-name>, &#x201C;<article-title>L-measure: a web-accessible tool for the analysis, comparison and search of digital reconstructions of neuronal morphologies</article-title>,&#x201D; <source>Nature protocols</source>, vol. <volume>3</volume>, no. <issue>5</issue>, pp. <fpage>866</fpage>&#x2013;<lpage>876</lpage>, <year>2008</year>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><string-name><given-names>R.</given-names> <surname>Kraft</surname></string-name>, <string-name><given-names>M. M.</given-names> <surname>Escobar</surname></string-name>, <string-name><given-names>M. L.</given-names> <surname>Narro</surname></string-name>, <string-name><given-names>J. L.</given-names> <surname>Kurtis</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Efrat</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Barnard</surname></string-name>, and <string-name><given-names>L. L.</given-names> <surname>Restifo</surname></string-name>, &#x201C;<article-title>Phenotypes of drosophila brain neurons in primary culture reveal a role for fascin in neurite shape and trajectory</article-title>,&#x201D; <source>Journal of Neuroscience</source>, vol. <volume>26</volume>, no. <issue>34</issue>, pp. <fpage>8734</fpage>&#x2013;<lpage>8747</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><string-name><given-names>D. A.</given-names> <surname>Sholl</surname></string-name>, &#x201C;<article-title>Dendritic organization in the neurons of the visual and motor cortices of the cat</article-title>,&#x201D; <source>Journal of anatomy</source>, vol. <volume>87</volume>, no. Pt <issue>4</issue>, p. <fpage>387</fpage>, <year>1953</year>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Di Ieva</surname></string-name>, &#x201C;<article-title>The fractal geometry of the brain</article-title>,&#x201D; <source>Springer series in computational neuroscience</source> (, <year>2016</year>.</mixed-citation></ref>
</ref-list>
<sec id="s5" sec-type="supplementary-material">
<label>6</label>
<title>Supporting Information</title>
<p>Here we will give more details on the generation of various aspects of the simulations. <xref ref-type="fig" rid="fig1">Figure 1</xref>: The neuron was chosen from the Chen contribution (pyramidal neocortex) with neuromorpho id of 32114. It was then sub-sampled such that the distance between two consecutive nodes is around 20<italic>&#x03BC;m</italic>. The histogram figures are normalized such that the summation of densities over all the bins is equal to one. For each histogram related to the angles we chose 18 bins. For others the bin length is selected such that the number of bins is between 20 to 30 (see code-base for details).</p>
<p><xref ref-type="fig" rid="fig2">Figure 2</xref>: 3 samples from Chen contribution (pyramidal neocortex) is shown. The mean and deviation is computed over the whole database. The features that presented here are the same as the feature in fig 1.</p>
<p><xref ref-type="fig" rid="fig5">Figure 5</xref>: The objective function is <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="248385_inline2.gif"/></alternatives></inline-formula> where <italic>E</italic>, <italic>N</italic> and <italic>B</italic> are bending ratio for segment (see the method section for definition), number of nodes and number of branching, respectively, and <italic>&#x03BC;</italic><sub><italic>x</italic></sub> and <italic>&#x03C3;</italic><sub><italic>x</italic></sub> are constant values representing the mean and deviation of the feature <italic>x</italic> (<italic>&#x03BC;</italic><sub><italic>E</italic></sub> = 1.15, <italic>&#x03C3;</italic><sub><italic>E</italic></sub> = 0.05, <italic>&#x03BC;</italic><sub><italic>N</italic></sub> = 50, <italic>&#x03C3;</italic><sub><italic>N</italic></sub> = 5, <italic>&#x03BC;</italic><sub><italic>B</italic></sub> = 4, <italic>&#x03C3;</italic><sub><italic>B</italic></sub> = 0.5). The proposal for first simulation (a) is the rotation around random node and for the second simulation (b) is the general sliding and the rotation around random node (see the method section for definition of proposals). The initial state for the first simulation (a) is a straight line made of 50 nodes with equal distant from parent. The initial state for the first simulation (a) is one node. Both simulations run for 2000 iterations.</p>
<p><xref ref-type="fig" rid="fig6">Figure 6</xref> and <xref ref-type="fig" rid="fig7">7</xref>: pyramidal neocortex from Chen&#x2019;s lab was selected and subsampled such that the distance between two consecutive nodes is around 20&#x03BC;m. The initial neuron of the algorithm is a 2D star shape neuron with 7 wings and 70 nodes on each wing and one node for soma. The probably distribution for selecting the preturbations is: &#x2018;rotation for any node&#x2019;: 3/13, &#x2018;rotation for branching&#x2019;: 4/13, &#x2018;sliding general&#x2019;: .5/13, &#x2018;sliding certain in distance&#x2019;: 1/13, &#x2018;sliding for branching node&#x2019;: .5/13, &#x2018;sliding for branching node certain distance&#x2019;: 1/13, &#x2018;sliding for end nodes&#x2019;: 2/13. The kappa for rotations are set to 400. the interval for sliding has the length 100. The MCMC is run for 25000 iterations.</p>
<p><xref ref-type="fig" rid="fig8">Figure 8</xref>: Four different class of neuron morphology are selected: 1) Pyramidal cells of rat hippocampus from Chen database, 2) Tripolar cell of rat neocortex from Brown archive, 3) Purkinje cells of mouse cerebellum from Kengaku archive and 4) Stellate cells of mouse neocortex from Ballester-Rosado archive. For each database the features are extracted based on the table 4.2. The initial neurons are star-like neuron with 7 wings. all the samples run for 25k iteration. The detail of selecting perturbation is the same as 6.</p>
<p><xref ref-type="fig" rid="fig9">Figure 9</xref>: The neurons that generated from NeuGen are pyramidal L5. To generate them, we changed the seed number to a range of different values. The output of the software are .hoc files. We converted them it to .swc file by finding the location of the mean of each component in the &#x002A;.hoc file.</p>
<fig id="fig9" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9:</label>
<caption><p>Comparing with growth based algorithm. We compare the algorithm that presented here with a state-of-art algorithm, NeuGen. It is designed for specific classes of neurons and among them we generated layer 5 pyramidal neurons. a few samples is shown on top left. A few samples from our algorithm is shown on right. In middle a histogram of global angle is shown for NeuGen, out method and Pyramidal cells. Global angles is one of the parameters that is not explicitly used in the growth process of NeuGen and the figure shows that the generated neuron failed to have it as their histogram is far from pyramidal cell. One of the aspect of our method compare to other package (including NeuGen) is that is fits better on the features and also it gives a degree of freedom to the generated samples. On bottom we chose a few features and extract the mean and variance from generated samples (for the angles, the mean over bins is computed). The error bar shows that the samples generated by our method have higher degree of freedom.</p></caption>
<graphic xlink:href="248385_fig9.tif"/>
</fig>
<fig id="fig10" position="float" orientation="portrait" fig-type="figure">
<label>Figure 10:</label>
<caption><p>Representation of a neuron. a) neurons are constructed by a set of cylinders and spheres which have locations and diameters with an underlying tree structure (swc format). b) In this representation the distances between two consecutive nodes is arbitrary. However, for calculating many features of neuron, these distances should be roughly equal. As such we do sub sampling the neuron to straighten it. In this process on each segments of the neuron, the nodes that are closer to each other less that a certain threshold will be removed. c) as the threshold for distance increases, the sub sampling approximation become coarser.</p></caption>
<graphic xlink:href="248385_fig10.tif"/>
</fig>
<fig id="fig11" position="float" orientation="portrait" fig-type="figure">
<label>Figure 11:</label>
<caption><p>Features of neuron.</p></caption>
<graphic xlink:href="248385_fig11.tif"/>
</fig>
<fig id="fig12" position="float" orientation="portrait" fig-type="figure">
<label>Figure 12:</label>
<caption><p>The list of all proposals a) flowchart of Extension/Reduction of a neuron. Starting from an initial tree, here we used two forms of perturbations. On the left side, an end node is added or removed while on the right side, an intermediate node is added or removed. The new or removed nodes are chosen randomly among all the possibilities which is written on the last line. The notations: N<sub>end</sub> number of end nodes, N<sub>inter</sub> intermediate nodes and N<sub>branch</sub> number of branching nodes.Hence the number of all nodes of the neuron is equal to: N<sub>node</sub> = 1 &#x002B; N<sub>inter</sub> &#x002B; N<sub>end</sub> &#x002B; N<sub>end</sub>. Notice that there is not any limitation on the number of nodes that attach to the soma, but other nodes can at most have 2 children. b) The flowchart of sliding perturbation of a neuron. To slide the neuron over itself we need to select a non-soma node for detaching and nonbranch node for reattaching. When the non-soma node is selected, neuron is detached from the parent of this node and this two sections parallel transport and would be reattached in non-branch node. In the figure the gray part is the old position of one of the section and red one is the new position.The non-soma node can be the children of a branching node (left neuron) or any other non-soma node (right). Also the sliding distance (I in the figure) is forced to be less than a threshold. c) rotation along a random node. To rotate a part of neuron we have to select a non-soma node and rotate the part of neuron which is connected to it and does not contain the soma. The unitary matrix for rotation is coming from a symmetric distribution on the set of all unitary matrix. The selected node for rotation can be a node in general (left) or branching node (right)</p></caption>
<graphic xlink:href="248385_fig12.tif"/>
</fig>
</sec>
</back>
</article>