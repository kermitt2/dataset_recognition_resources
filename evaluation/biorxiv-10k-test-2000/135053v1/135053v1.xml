<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/135053</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Attention is required for knowledge-based sequential grouping of syllables into words</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Ding</surname><given-names>Nai</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Pan</surname><given-names>Xunyi</given-names></name>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Luo</surname><given-names>Cheng</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Su</surname><given-names>Naifei</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Zhang</surname><given-names>Wen</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Zhang</surname><given-names>Jianfeng</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a7">7</xref>
</contrib>
<aff id="a1"><label>1</label><institution>College of Biomedical Engineering and Instrument Sciences, Zhejiang Univ.</institution>, <country>China</country></aff>
<aff id="a2"><label>2</label><institution>Key Labratory for Biomedical Engineering of Ministry of Education, Zhejiang Univ.</institution>, <country>China</country></aff>
<aff id="a3"><label>3</label><institution>State Key Laboratory of Industrial Control Technology, Zhejiang Univ.</institution>, <country>China</country></aff>
<aff id="a4"><label>4</label><institution>Interdisciplinary Center for Social Sciences, Zhejiang Univ.</institution>, <country>China</country></aff>
<aff id="a5"><label>5</label><institution>Neuro and Behavior EconLab, Zhejiang Univ. of Finance and Economics</institution>, <country>China</country></aff>
<aff id="a6"><label>6</label><institution>School of International Studies, Zhejiang Univ.</institution>, <country>China</country></aff>
<aff id="a7"><label>7</label><institution>Mental Health Center, School of Medicine, Zhejiang Univ.</institution>, <country>China</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1">Corresponding Author: Nai Ding, College of Biomedical Engineering and Instrument Sciences, Zhejiang University, China 310027, Email: <email>ding_nai@zju.edu.cn</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2017</year>
</pub-date>
<elocation-id>135053</elocation-id>
<history>
<date date-type="received">
<day>06</day>
<month>5</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>08</day>
<month>5</month>
<year>2017</year>
</date>
</history><permissions><copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="135053.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract><title>Abstract</title>
<p>How the brain sequentially groups sensory events into temporal chunks and how this process is modulated by attention are fundamental questions in cognitive neuroscience. Sequential grouping includes bottom-up primitive grouping and top-down knowledge-based grouping. In speech perception, grouping acoustic features into syllables can rely on bottom-up acoustic continuity cues but grouping syllables into words critically relies on the listener&#x2019;s lexical knowledge. This study investigates whether top-down attention is required to apply lexical knowledge to group syllables into words, by concurrently monitoring neural entrainment to syllables and words using electroencephalography (EEG). When attention is directed to a competing speech stream or cross-modally to a silent movie, neural entrainment to syllables is weakened but neural entrainment to words largely diminishes. These results strongly suggest that knowledge-based grouping of syllables into words requires top-down attention and is a bottleneck for the neural processing of unattended speech.</p>
</abstract>
<counts>
<page-count count="33"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1"><title>Introduction</title>
<p>Sequentially grouping events into temporal chunks is a fundamental function of the brain (<xref ref-type="bibr" rid="c32">Lashley, 1951</xref>, <xref ref-type="bibr" rid="c23">Gavornik and Bear, 2014</xref>). During speech comprehension, for example, sequential grouping occurs hierarchically, with syllables being grouped into words and words being grouped into phrases, sentences, and discourses. Similarly, during music perception, musical notes are hierarchically grouped into meters and phrases. Neurophysiological studies show that slow changes in neural activity can follow the time course of a temporal sequence. Within a temporal chunk, neural activity may show a sustained deviation from baseline (<xref ref-type="bibr" rid="c2">Barascud et al., 2016</xref>, Pe&#x00F1;a and Melloni, 2012) or a monotonic change in phase or power (O&#x2019;Connell et al., 2012, <xref ref-type="bibr" rid="c49">Pallier et al., 2011</xref>, <xref ref-type="bibr" rid="c7">Brosch et al., 2011</xref>). At the end of a temporal sequence, an offset response is often observed (<xref ref-type="bibr" rid="c16">Ding et al., 2016</xref>, <xref ref-type="bibr" rid="c45">Nelson et al., 2017</xref>, <xref ref-type="bibr" rid="c6">Brennan et al., 2016</xref>). Furthermore, the sensory responses to individual events within a temporal chunk are significantly altered by learning (<xref ref-type="bibr" rid="c23">Gavornik and Bear, 2014</xref>, <xref ref-type="bibr" rid="c53">Sanders et al., 2002</xref>, <xref ref-type="bibr" rid="c67">Yin et al., 2008</xref>, <xref ref-type="bibr" rid="c69">Zhou et al., 2010</xref>, <xref ref-type="bibr" rid="c19">Farthouat et al., 2016</xref>, <xref ref-type="bibr" rid="c8">Buiatti et al., 2009</xref>), demonstrating that prior knowledge strongly influences sensory processing of sequences.</p>
<p>Whether sequential grouping requires attention is under debate (<xref ref-type="bibr" rid="c59">Snyder et al., 2006</xref>, <xref ref-type="bibr" rid="c58">Shinn-Cunningham, 2008</xref>, <xref ref-type="bibr" rid="c57">Shinn-Cunningham et al., 2017</xref>). On the one hand, it has been hypothesized that top-down attention is required for sequential grouping, especially for complex scenes consisting of multiple sequences. Evidence has been provided that attention can strongly affect neural and behavioral responses to sound sequences (<xref ref-type="bibr" rid="c11">Carlyon et al., 2001</xref>, <xref ref-type="bibr" rid="c56">Shamma et al., 2011</xref>, <xref ref-type="bibr" rid="c35">Lu et al., 2017</xref>, <xref ref-type="bibr" rid="c22">Fritz et al., 2007</xref>). Research on visual object recognition has also suggested that top-down attention is required for the binding of simultaneously presented features, e.g., color and shape information (<xref ref-type="bibr" rid="c63">Treisman and Gelade, 1980</xref>). On the other hand, a large number of neurophysiological studies have shown that the brain is highly sensitive to temporal regularities in sound when when the sound is not attended (<xref ref-type="bibr" rid="c2">Barascud et al., 2016</xref>, N&#x00E4;&#x00E4;t&#x00E4;nen et al., 2007, <xref ref-type="bibr" rid="c62">Sussman et al., 2007</xref>), suggesting that primitive analyses of temporal sequences may occur as a preattentative automatic process (<xref ref-type="bibr" rid="c20">Fodor, 1983</xref>).</p>
<p>Sequential grouping is not a single computational module, which further complicates the discussion about how attention influences sequential grouping. Sequential grouping can depend on multiple mechanisms, including bottom-up primitive grouping and top-down schema-based grouping (<xref ref-type="bibr" rid="c5">Bregman, 1990</xref>). Bottom-up grouping depends on the similarity between sensory features (<xref ref-type="bibr" rid="c42">Micheyl et al., 2005</xref>, <xref ref-type="bibr" rid="c38">McDermott et al., 2011</xref>, <xref ref-type="bibr" rid="c66">Woods and McDermott, 2015</xref>) while top-down schema-based grouping relies on prior knowledge (<xref ref-type="bibr" rid="c4">Billig et al., 2013</xref>, <xref ref-type="bibr" rid="c27">Hannemann et al., 2007</xref>, <xref ref-type="bibr" rid="c28">Jones and Freyman, 2012</xref>). Both grouping mechanisms play important roles in auditory perception. For example, in spoken word recognition, integrating acoustic features into phonemes and syllables can rely on acoustic continuity cues within a syllable (<xref ref-type="bibr" rid="c57">Shinn-Cunningham et al., 2017</xref>) while integrating syllables into words crucially relies on lexical knowledge, i.e., the knowledge about which syllable combinations constitute valid words (<xref ref-type="bibr" rid="c14">Cutler, 2012</xref>). Most previous studies focus on how attention modulates primitive sequential grouping while relatively little is known about how schema-based grouping is modulated by attention. The current study fills this gap by studying how the brain groups syllables into words based on lexical knowledge.</p>
<p>Behavioral evidence has suggested that cognitive processing of unattended spoken words is limited. Without paying attention, listeners cannot recall the spoken words they heard and cannot even notice a change in the language being played (<xref ref-type="bibr" rid="c12">Cherry, 1953</xref>). There is also evidence, however, for some low-level perceptual analysis for the unattended speech stream. For example, listeners can recall the gender of an unattended speaker (<xref ref-type="bibr" rid="c12">Cherry, 1953</xref>) and some listeners can notice their names in the unattended speech stream (<xref ref-type="bibr" rid="c13">Conway et al., 2001</xref>, <xref ref-type="bibr" rid="c65">Wood and Cowan, 1995</xref>). These results suggest that different speech processing stages could be differentially influenced by attention. Basic acoustic features can be recalled, very salient words such as one&#x2019;s name can sometimes be recalled, while ordinary words cannot be recalled.</p>
<p>In this study, we used spoken word processing as a paradigm to test how attention may differentially modulate neural processing of basic sensory events, i.e., syllables, and temporal chunks constructed based on prior knowledge, i.e., multisyllabic words. Recent human neurophysiological results showed that cortical activity could concurrently follow hierarchical linguistic units of different sizes (<xref ref-type="bibr" rid="c16">Ding et al., 2016</xref>). In this study, we employed an isochronous syllable sequences as the speech stimulus, in which neighboring syllables combined into bisyllabic words (<xref rid="fig1" ref-type="fig">Fig. 1A</xref>). The stimulus was made in Chinese and all the syllables are monosyllabic morphemes. We first tested whether neural entrainment at the word rate could be observed, without any acoustic cue between word boundaries, and then tested whether attention differentially modulated neural entrainment to syllables (acoustic events) and neural entrainment to bisyllabic words (temporal chunks). The listener&#x2019;s attentional focus was differently manipulated in three experiments. Experiment one and two presented competing sensory stimuli, e.g., a spoken passage or a silent movie, together with the isochronous syllable sequence, and the listeners had to attend to different stimuli in different experimental blocks. Experiment three, in contrast, directed the listener&#x2019;s attentional focus to specific cued time intervals.</p>
<fig id="fig1" position="float" fig-type="figure"><label>Figure 1.</label>
<caption><p>Experiment design. (A) Structure of the isochronous syllable sequence, which alternates between word states and random states. The syllables are presented at a constant rate of 4 Hz and therefore the bisyllabic words are presented at 2 Hz. English syllables are shown in the figure for illustrative purposes and Chinese syllables and words are used in the experiment. (B) In experiment one, the isochronous syllable sequence and a competing spoken passage are simultaneously presented to different ears. (C) In experiment two, the listeners either attend to the isochronous syllable sequence (presented to both ears) or watch a movie while passively listening to the syllable sequence.</p></caption>
<graphic xlink:href="135053_fig1.tif"/>
</fig>
</sec>
<sec id="s2"><title>Results</title>
<p>In the first experiment, listeners were exposed to two concurrent speech streams, one to each ear (i.e., dichotically). One speech stream was an isochronous syllable sequence that alternates between word states and random states (<xref rid="fig1" ref-type="fig">Fig. 1</xref>). In the word states neighoring two words constructed a bisyllabic words and in the random state the order between syllables was randomized. The other speech stream was a spoken passage that was time compressed, i.e. fastened, by a factor of 2.5 to increase task difficulty.</p>
<p>During the time intervals when the bisyllabic words are played, the EEG power spectrum averaged over subjects and channels is shown in <xref rid="fig2" ref-type="fig">Fig. 2A</xref>. When the word sequence is attended, two peaks are observed in the power spectrum, one at the syllabic rate (P = 10<sup>&#x2212;4</sup>, bootstrap) and the other at the word rate (P = 10<sup>&#x2212;4</sup>, bootstrap). The topographic distribution of EEG power is centered near channel FCz. When attention is directed to the competing speech stream, a single response peak is observed at the syllabic rate (P = 10<sup>&#x2212;4</sup>, bootstrap) while the neural response at the word-rate is no longer significantly stronger than the power in the neighboring frequency bins (P = 0.58, bootstrap). Comparing the conditions when the word lists are attended to or not, the difference in normalized word-rate response amplitude (i.e., the difference between the filled red and black bars in <xref rid="fig2" ref-type="fig">Fig. 2B</xref>) is significantly larger than the difference in normalized syllable-rate response amplitude (i.e., the difference between the hollow red and black bars in <xref rid="fig2" ref-type="fig">Fig. 2B</xref>, P = 0.01, bootstrap). The change in normalized word-rate response amplitude is more than 21.7 dB larger than the change in normalized syllable-rate response amplitude (27 dB vs. 5.3 dB). These results demonstrate that selective attention has a much stronger influence on the neural representation of linguistically defined temporal chunks, i.e., words, than the neural representation of acoustic events, i.e., syllables.</p>
<fig id="fig2" position="float" fig-type="figure"><label>Figure 2.</label>
<caption><p>Attention differentially modulates neural entrainment to syllables and bisyllabic words. EEG response spectra averaged over subjects and channels are shown in panel A and C for experiment one and two respectively. Stars indicate frequency bins that show significantly stronger power than the power averaged over a 1-Hz wide neighboring frequency region (&#x002A; P &#x003C; 0.05, &#x002A;&#x002A; P &#x003C; 0.005, bootstrap). Response peak at the syllabic rate is observed in both attended and unattended conditions. Response peak at the word rate however, is only observed for the attended condition. The topographic plots of the EEG response at the syllable and word rates are shown above the spectrum (a: attended; u: unattended), which generally shows a central-frontal distribution. In the topographic plots, the 5 black dots show the position of FCz (middle), Fz (upper), Cz (lower), FC3 (left), and FC4 (right). (B,D) Normalized power at the syllable and word rates. Power at each target frequency is normalized by subtracting the power averaged over a 1-Hz wide neighboring frequency region (excluding the target frequency), which reduces the influence of background broadband neural activity. Red bars represent the attended condition and black bars represent the unattended condition. The attention-related amplitude change relative to the response amplitude in the attended condition, i.e. (attended-unattended)/ attended, is shown in percentage near each response peak. Stars indicate whether the attention-related change in response amplitude is significantly larger than 0. Attention modulates both the syllable-rate response and the word-rate response but the effect is much stronger at the word rate.</p></caption>
<graphic xlink:href="135053_fig2.tif"/>
</fig>
<p>Spoken passage comprehension involves almost all neural computations required for spoken word recognition. Therefore, it remains unclear whether the strong modulation of word-rate processing is due to the lack of top-down attention or a competition in other neural resources required for spoken word recognition. To address this issue, experiment 2 utilizes visual input to divert top-down attention. In this experiment, the isochronous syllable sequence is presented to both ears diotically and listeners either listen to speech or watch a silent movie with subtitles. The EEG power spectrum during the time intervals when the word states are presented is shown in <xref rid="fig2" ref-type="fig">Fig. 2C</xref>. The results largely mirror the results in experiment 1, except that the word-rate response is marginally significant when attention is directed to the visual input (P = 0.07, bootstrap). The attention related change in response amplitude is stronger at the word rate than at the syllable rate (i.e., the amplitude difference between the filled red and black bars in <xref rid="fig2" ref-type="fig">Fig. 2D</xref> is larger than the amplitude difference between the hollow red and black bars, P = 0.002, bootstrap). These results show that without any competing auditory input, the word-level neural representation still strongly relies on top-down attention.</p>
<p>The frequency-domain analysis in <xref rid="fig2" ref-type="fig">Fig. 2</xref> reveals steady-state properties of the neural tracking of syllables and words. To further reveal how the neural response evolves over time, the waveform of the EEG signals averaged over channels is shown in <xref rid="fig3" ref-type="fig">Fig. 3</xref>. EEG responses show clear syllabic-rate oscillations when listening to random syllables. When bisyllabic words appear, EEG activity becomes dominated by word-rate oscillations, as is revealed by the intervals between response peaks (<xref rid="fig3" ref-type="fig">Fig. 3AB</xref>). The neural response power near the word and the syllable rates is further illustrated in <xref rid="fig3" ref-type="fig">Fig. 3CD</xref>. In the attended conditions, the word-rate neural response starts to increase about 500 ms after the word state onset when speech is presented in quiet (<xref rid="fig3" ref-type="fig">Fig. 3D</xref>) and this latency elongates to about 1 s when there is a competing speech stream (<xref rid="fig3" ref-type="fig">Fig. 3C</xref>). Furthermore, the syllabic-rate neural response shows a decrease in power about 1 s after the word state onset (<xref rid="fig3" ref-type="fig">Fig. 3CD</xref>).</p>
<fig id="fig3" position="float" fig-type="figure"><label>Figure 3.</label>
<caption><p>Temporal dynamics of the EEG response to words. (AB) The EEG waveforms for experiment one (E1) and experiment two (E2) are shown in panel A and B respectively (bandpass filtered between 1.5 and 4.5 Hz). The EEG waveform is grand averaged over subjects and channels. The word state starts from time 0. Each response peak, i.e., local maximum, is marked by a dotted line. Before the onset of the word state, regular neural oscillations are observed showing a peak every &#x223C;250 ms, corresponding to a 4-Hz syllable-rate rhythm. About 500-1000 ms after the word onset, in attended conditions, a slow oscillation emerges showing a peak every 500 ms, corresponding to a 2-Hz word-rate rhythm. (CD) Instantaneous amplitude of the EEG response filtered around the syllable rate (1.75-2.25 Hz) or the word rate (3.75-4.25 Hz) for experiment one and two. The EEG instantaneous amplitude is baseline corrected by subtracting the mean amplitude in a 1-second duration pre-stimulus interval. The shaded areas above/below the horizontal dotted line at 0 &#x03BC;V indicate time intervals when the word-/syllable-rate response amplitude significantly differs from the pre-stimulus baseline (dark gray: P &#x003C; 0.01, light gray: P&#x003C;0.05; bootstrap, FDR corrected). The word-rate response shows a significant increase in power about 500-1000 ms after the first word appears, in all conditions except for the unattended condition in experiment one. For the attended conditions, a decrease in the syllable-rate response is also seen during the word state. The instantaneous amplitude is the magnitude of the Hilbert transform of the filtered EEG responses.</p></caption>
<graphic xlink:href="135053_fig3.tif"/>
</fig>
<p>Experiments 1 and 2 show that neural tracking of words is severely attenuated when attention is directed to a competing sensory stimulus. We then ask if attention can modulate the word-rate neural response dynamically over time, in the absence of any competing stimulus. In a 3<sup>rd</sup> experiment, the listeners hear a single speech stream and have to attend to some words while ignoring others. The onset of each word state is verbally cued and the listeners have to focus on either the first word or the last word in a word state (<xref rid="fig4" ref-type="fig">Fig. 4A</xref>).</p>
<fig id="fig4" position="float" fig-type="figure"><label>Figure 4.</label>
<caption><p>Temporal attention quickly modulates the neural tracking of words. (A) Illustration of the two tasks. The subjects have to judge the animacy of either the first word or the last word in a word state. The onset of the word state is cued by the preceding 3 syllables, which are one, two, and three. (B) The grand-averaged EEG response in the first-word condition and the last-word condition (using the convention in <xref rid="fig3" ref-type="fig">Fig. 3AB</xref>). The red and purple bars on the x-axis show the time intervals in which the first word and the last word in the word state are presented. (C) Instantaneous amplitude of the EEG response filtered around the syllable rate (1.75-2.25 Hz) and the word rate (3.75-4.25 Hz). The shaded areas indicate time intervals when the response amplitude significantly differs from the pre-stimulus baseline (dark gray: P &#x003C; 0.01, light gray: P&#x003C;0.05; bootstrap, FDR corrected). The word-rate response is strongly modulated by temporal attention and shows stronger activation near the attended word (i.e., stronger activation in an earlier window for the first-word condition compared with the last-word condition). The syllable-rate response is less strongly modulated by temporal attention. The instantaneous amplitude is extracted using the same method used in <xref rid="fig3" ref-type="fig">Fig. 3CD</xref>. (D) Phase and amplitude of the word-rate response in each 500-ms time bin. The red and purple arrows indicate complex-valued Fourier coefficient at the word rate in each time bin, for the first- and last-word conditions respectively. The response shows a phase change between the first bin and the second bin in the word state. The black arrows show the mean response averaged over the word state response in experiment one (E1) and experiment two (E2).</p></caption>
<graphic xlink:href="135053_fig4.tif"/>
</fig>
<p>In experiment 3, the listeners have to judge the animacy of the first word in each word state in one block (called the first-word condition) and judge the animacy of the last word in each word state in another block (called the last-word condition). Timing is critical for these tasks since the listeners have to judge the animacy of the right words and not confuse them with the neighboring words. The two tasks force the listeners to attend to words at different positions of a sequence and therefore dissociate their attentional focus in time.</p>
<p>The results of experiment 3 are shown in <xref rid="fig4" ref-type="fig">Fig. 4</xref>. The time course of the word-rate neural response is significantly modulated by temporal attention. The neural response shows a stronger word-rate response near the beginning/end of a word state in the first/last word condition (<xref rid="fig4" ref-type="fig">Fig. 4B</xref>). In other words, the word-rate response is significantly stronger during the time intervals being attended to. Although the onset of the word state is cued, the phase of the word-rate response still takes about 500 ms to stabilize after the word state onset (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>). In other words, temporal prediction cannot greatly fasten the stabilization of the neural response phase.</p>
</sec>
<sec id="s3"><title>Discussion</title>
<p>The current study investigates how attention differentially modulates the neural entrainment to acoustic events, i.e., syllables, and temporal chunks, i.e., words. Here, the grouping of syllables into words is purely based on top-down lexical knowledge, i.e., the mental dictionary, rather than bottom-up acoustic cues. It is shown that top-down attention more strongly modulates the word-rate neural response compared with the syllable-rate neural response (up to 20 dB differences in attention-related changes in response power), which strongly suggests that attention is crucial for knowledge-based sequential grouping.</p>
<sec id="s3a"><title>Neural processing of unattended auditory streams</title>
<p>The brain can detect statistical regularities in sounds even without top-down attentional modulation (N&#x00E4;&#x00E4;t&#x00E4;nen et al., 2007). For example, neural activity can entrain to intensity fluctuations in sound even when the listeners do not pay attention (<xref ref-type="bibr" rid="c33">Linden et al., 1987</xref>). Similarly, in the current study, the syllabic rhythm is reflected in the EEG response whether the listeners pay attention or not. Previous studies have shown that when a random tone cloud turns into a fixed multi-tone sequence repeating in time, the brain can quickly detect such a transition even when attention is directed to other sensory stimuli (<xref ref-type="bibr" rid="c2">Barascud et al., 2016</xref>). Furthermore, the brain can also detect violations in multi-tone sequences that repeat in time (<xref ref-type="bibr" rid="c62">Sussman et al., 2007</xref>). Therefore, although attention can strongly modulate primitive auditory grouping, i.e., bottom-up feature-based grouping of acoustic events into auditory streams (<xref ref-type="bibr" rid="c11">Carlyon et al., 2001</xref>, <xref ref-type="bibr" rid="c56">Shamma et al., 2011</xref>, <xref ref-type="bibr" rid="c57">Shinn-Cunningham et al., 2017</xref>), it is clear that the brain can detect basic statistical regularities in sounds preattentatively.</p>
<p>Statistical regularities in sound can be extracted by bottom-up analysis of auditory features. In the current study, however, the grouping of syllables into words can only rely on top-down knowledge about which syllables can possibly construct a valid multisyllabic word. The word boundaries can only be determined by comparing the auditory input with word templates stored in the long-term memory. The current results show that neural entrainment to bisyllablic words is much more strongly influenced by top-down attention, compared with the neural entrainment to syllables. Therefore, although bottom-up grouping of basic auditory features into a sound stream may occur preattentatively, top-down schema-based grouping of syllables into words critically relies on attention.</p>
</sec>
<sec id="s3b"><title>Attention modulation of neural processing of speech</title>
<p>This study uses Chinese as the testing language. In Chinese, generally speaking, each syllable corresponds to a morpheme but there is no one-to-one mapping between syllables and morphemes due to the existence of homophones. For example, the syllable <inline-formula><alternatives><inline-graphic xlink:href="135053_inline1.gif"/></alternatives></inline-formula> could correspond to an adjective (e.g., green <inline-formula><alternatives><inline-graphic xlink:href="135053_inline2.gif"/></alternatives></inline-formula>), a noun (e.g., law <inline-formula><alternatives><inline-graphic xlink:href="135053_inline3.gif"/></alternatives></inline-formula>), or a verb (e.g., filter <inline-formula><alternatives><inline-graphic xlink:href="135053_inline4.gif"/></alternatives></inline-formula>). Since the mapping between syllables and morphemes is highly ambiguous, a random syllable sequence cannot be reliably mapped into a sequence of morphemes and is generally heard as a meaningless syllable sequence. The bisyllabic words used in this study, however, are common unambiguous words that can be precisely decoded when listening to the syllable sequences. Therefore, the study probes the process of grouping syllables (ambiguous morphemes) into multisyllabic (multimorphemic) words.</p>
<p>Speech comprehension involves multiple processing stages, e.g., encoding acoustic speech features (<xref ref-type="bibr" rid="c55">Shamma, 2001</xref>), decoding phonemic information based on acoustic features (<xref ref-type="bibr" rid="c40">Mesgarani et al., 2014</xref>, <xref ref-type="bibr" rid="c15">Di Liberto et al., 2015</xref>), grouping syllables into words (<xref ref-type="bibr" rid="c14">Cutler, 2012</xref>), and grouping words into higher level linguistic structures such as phrases and sentences (<xref ref-type="bibr" rid="c21">Friederici, 2002</xref>). Previous studies have shown that attention can modulate neural entrainment to the intensity fluctuations in the speech, i.e., the speech envelope that corresponds to the syllabic rhythm (<xref ref-type="bibr" rid="c29">Kerlin et al., 2010</xref>, <xref ref-type="bibr" rid="c39">Mesgarani and Chang, 2012</xref>, O&#x2019;Sullivan et al., 2014, <xref ref-type="bibr" rid="c50">Park et al., 2016</xref>). The envelope-following response is stronger for the attended speech but remains observable for the unattended speech (<xref ref-type="bibr" rid="c17">Ding and Simon, 2012</xref>, <xref ref-type="bibr" rid="c61">Steinschneider et al., 2013</xref>), especially when there is no competing auditory input (<xref ref-type="bibr" rid="c30">Kong et al., 2014</xref>). In terms of the spatial distribution of neural activity, neural entrainment to the unattended speech is stronger near sensory areas around the superior temporal gyrus and attenuates in higher-order cortical areas (<xref ref-type="bibr" rid="c25">Golumbic et al., 2013</xref>). The current study extends previous studies by showing that neural entrainment to linguistic units, such as words, is more strongly modulated by attention than neural entrainment to the speech envelope. When attention is directed to a competing speech stream, word-rate neural entrainment is no longer observed. These results show that attention strongly modulates the lexical segmentation process, which creates a bottleneck for the neural processing of unattended speech streams.</p>
<p>Previous studies on attention modulation of lexical processing mostly focus on semantic processing of words that have clear physical boundaries. It is found that the N400 ERP response disappears for unattended auditory or visual words (<xref ref-type="bibr" rid="c46">Nobre and Mccarthy, 1995</xref>, <xref ref-type="bibr" rid="c3">Bentin et al., 1995</xref>). On the other hand, visual experiments have shown that semantic processing can occur for words presented at the attended location even when these words are not consciously perceived (<xref ref-type="bibr" rid="c36">Luck et al., 1996</xref>, <xref ref-type="bibr" rid="c44">Naccache et al., 2002</xref>). Therefore, semantic processing of isolated words could be a subconscious process but requires attention. The current study extends these previous studies by showing the phonological construction of words, i.e., the grouping of syllables into words, also requires attention. Here, the grouping of syllables into words can only be achieved by comparing the input speech stream with phonological templates of words that are stored in long-term memory. Therefore, the current results strongly suggest that phonological grouping process crucially relies on attention.</p>
</sec>
<sec id="s3c"><title>Low-frequency neural oscillations and temporal information processing</title>
<p>The current data and previous studies (<xref ref-type="bibr" rid="c16">Ding et al., 2016</xref>, <xref ref-type="bibr" rid="c8">Buiatti et al., 2009</xref>, <xref ref-type="bibr" rid="c60">Steinhauer et al., 1999</xref>, <xref ref-type="bibr" rid="c19">Farthouat et al., 2016</xref>, <xref ref-type="bibr" rid="c41">Meyer et al., 2016</xref>, <xref ref-type="bibr" rid="c52">Peelle et al., 2013</xref>) show that, during speech listening, cortical activity is concurrently entrained to hierarchical linguistic units, including syllables, words, phrases, and sentences. Neural entrainment to hierarchical linguistic units provides a plausible mechanism to map hierarchical linguistic units into coupled dynamic neural processes that allow interactions between different linguistic levels (<xref ref-type="bibr" rid="c37">Martin and Doumas, 2017</xref>, <xref ref-type="bibr" rid="c26">Goswami and Leong, 2013</xref>, <xref ref-type="bibr" rid="c24">Giraud and Poeppel, 2012</xref>, <xref ref-type="bibr" rid="c64">Wassenhove et al., 2003</xref>). Neural entrainment to words stabilizes &#x223C;0.5-1 s after the word state onset. Similarly, previous studies have shown that neural entrainment to phrases and sentences also stabilizes within &#x223C;1 ms (<xref ref-type="bibr" rid="c68">Zhang and Ding, 2017</xref>). When the onset time of a word state is precisely cued, the neural response phase still takes about &#x223C;0.5 s to stabilize (<xref rid="fig4" ref-type="fig">Fig. 4C</xref>), suggesting that neural entrainment to words is not purely a predictive process and requires feedfowrd syllabic input.</p>
<p>The current data and previous results (<xref ref-type="bibr" rid="c16">Ding et al., 2016</xref>) suggest that low-frequency neural entrainment is closely related to the binding of syllables into temporal chunks such as words and phrases. Previous studies have also suggested slow changes in neural activity may indicate information integration over time during word by word reading (<xref ref-type="bibr" rid="c49">Pallier et al., 2011</xref>) and during decision making (O&#x2019;Connell et al., 2012). Therefore, low-frequency neural entrainment provides a plausible neural signature for the mental construction of temporal chunks.</p>
<p>Low-frequency neural entrainment to sensory stimuli is a widely observed phenomenon. Neurophysiological evidence has been provided that the phase of low-frequency neural oscillations can modulate neuronal firing (<xref ref-type="bibr" rid="c31">Lakatos et al., 2005</xref>, <xref ref-type="bibr" rid="c10">Canolty et al., 2006</xref>) and can serve as a mechanism for temporal attention and temporal prediction(<xref ref-type="bibr" rid="c1">Arnal and Giraud, 2012</xref>, <xref ref-type="bibr" rid="c54">Schroeder and Lakatos, 2009</xref>). Furthermore, slow neural oscillations may also provide a neural context for the integration of faster neural activity falling into the same cycle of a slow neural oscillation (Buzs&#x00E1;ki, 2010, <xref ref-type="bibr" rid="c34">Lisman and Jensen, 2013</xref>). Therefore low-frequency neural entrainment to temporal chunks may naturally provide a mechanism to put neural representations of sensory events into a context and allow information integration across sensory events.</p>
</sec>
</sec>
<sec id="s4"><title>Methods</title>
<sec id="s4a"><title>Subjects</title>
<p>Fourteen subjects participated in each experiment (18-28 years old; mean age: 22; 50&#x0025; female). All subjects were graduate or undergraduate students at Zhejiang University, with no self-reported hearing loss or neurological disorders. The experimental procedures were approved by the Institutional Review Board of Zhejiang University Interdisciplinary Center for Social Sciences. The subjects provided written consent and were paid for the experiment.</p>
</sec>
<sec id="s4b"><title>Word Materials</title>
<p>The study employed 160 animate bisyllabic words and 160 inanimate bisyllabic words. Animate words included animals (N = 40, e.g., monkey, dolphin), plants (N = 40, e.g. lemon, carrot), humans (N = 48, e.g., doctor, doorman), and names of well known people in history (N = 32, e.g., Bai Li, a famous poet in Tang dynasty). Inanimate words include objects (N = 80, e.g., teacup, pencil) and places (N = 80, e.g., Beijing, Zhejiang).</p>
</sec>
<sec id="s4c"><title>Stimuli</title>
<p>The stimulus consisted of an isochronous syllable sequence. All syllables were independently synthesized using the Neospeech synthesizer (<ext-link ext-link-type="uri" xlink:href="http://www.neospeech.com/">http://www.neospeech.com/</ext-link>, the male voice, Liang). All syllables were adjusted to the same intensity and the same duration, i.e., 250 ms (see <xref ref-type="bibr" rid="c16">Ding et al., 2016</xref> for details). The syllable sequence alternated between a word state and a random state (<xref rid="fig5" ref-type="fig">Fig. 5A</xref>). The number of syllables in each state and the number of word states in each stimulus, i.e., <italic>M</italic>, were shown in <xref rid="fig5" ref-type="fig">Fig 5B</xref>. Each sequence started and ended with a random state to reduce the probability that words might pop out at the beginning and end of each stimulus, even when the syllable sequence was not attended.</p>
<fig id="fig5" position="float" fig-type="figure"><label>Figure 5.</label>
<caption><p>Structure of the isochronous syllable sequence in each experiment. (A) The sequence alternates between random states and word states M times in each trial. At the beginning and end of each trial, NH and NT random syllables are presented. (B) Statistical distribution of the number of syllables in each state.</p></caption>
<graphic xlink:href="135053_fig5.tif"/>
</fig>
<p>In experiment one, an isochronous syllable sequence and a competing spoken passage were dichotically presented, and the ear each stimulus was presented to was counterbalanced across subjects. The competing spoken passages (chosen from the <italic>Syllabus for Mandarin Proficiency Tests</italic>) were time compressed by a factor of 2.5 and gaps longer than 30 ms were shortened to 30 ms. Long acoustic pauses were removed in case the listeners might shift their attentional focus during the pauses. In each trial, 19 seconds of spoken passages were presented and the duration of each syllable sequence was set to 18 seconds, i.e., 72 syllables. The competing spoken passage started 1 second before the syllable sequence so that the syllable sequence was less likely to be noticed when the listeners focused on the spoken passage. The number of syllables in the word and random states was randomized using a uniform distribution so that the alternation between states was not completely regular while the total duration could be easily controlled.</p>
<p>In experiment two, an isochronous syllable sequence was identically, i.e., diotically, presented to both ears. The number of syllables in the word and random states was subject to a geometric distribution so that the subjects could not predict when state transitions would occur.</p>
<p>In experiment three, each random state always consisted of 6 syllables and the last 3 syllables were always &#x201C;yi, er, san&#x201D; which means &#x201C;one two three&#x201D; in mandarin Chinese. These 3 syllables served as cues for the onset time of a word state.</p>
<p>In all experiments, no word appeared twice in a trial and there was no immediate repetition of any syllable. In experiment one and two, words in the same word state belonged to the same category, i.e., animate or inanimate. In experiment three, however, the words in each word state were randomly chosen from all possible words. The subjects were never told how many word states might appear in a trial.</p>
</sec>
<sec id="s4d"><title>Procedures</title>
<p>The study consisted of three experiments. Each experiment contained two blocks, differing in the subject&#x2019;s attentional focus.</p>
<p><bold>Experiment one:</bold> In the first block, listeners had to focus on the time-compressed spoken passage and answer comprehension questions after each trial. The comprehension questions were presented 1 s after the spoken passage and the listeners had to give a verbal answer (correct rate: 84 &#x00B1; 2&#x0025;, mean &#x00B1; standard error throughout the paper). After the experimenter recorded the answer they pressed a key to continue the experiment. The next trial was played after an interval randomized between 1 and 2 seconds (uniform distribution) after the key press. In the second block, subjects had to focus on the syllable sequences and judge if an additional word presented 1 s after the sequence offset appeared in the sequence by a key press (correct rate: 77 &#x00B1; 2&#x0025;). The next trial started after an interval randomized between 1 and 2 seconds (uniform distribution) after the key press. The same set of 50 trials (50 distinct spoken passages paired with 50 distinct syllable sequences) were presented in each block with a random order. The subjects had their eyes closed when listening to the stimuli and had a break every 25 trials. The listeners always attended to the spoken passages in the first block to reduce the possibility that they may spontaneously shift their attentional focus to the isochronous syllable sequence after knowing that there were words embedded in the sequence.</p>
<p><bold>Experiment two</bold>: A word listening block and a movie watching block were presented, the order of which was counterbalanced across subjects. In the word listening block, after each trial, the subjects had to judge if they heard more animate words or more inanimate words by pressing different keys (correct rate: 81&#x00B1;3&#x0025;). The subjects were told that all words within the same word state belonged to the same category, i.e., animate or inanimate. Sixty trials were presented and the subjects had a break after every 15 trials. Before the word listening condition, the subjects went through a practice section, in which they listened to two example sequences and did the same task. They received feedback during the practice session but not during the main experiment. The neural responses showed the same pattern whichever block was presented first and therefore the responses were averaged over all subjects regardless of the presentation order.</p>
<p>In the movie watching block, the subjects watched a silent movie (the Little Prince) with Chinese subtitles. The syllable sequences were presented about 3 minutes after the movie started to make sure that the subjects had already engaged in the movie watching task. Sixty syllable sequences were presented in a randomized order, with the inter-stimulus-interval randomized between 1 and 2 seconds. The movie was stopped after all the 60 sequences were presented. The subjects had their eyes open in both blocks although no visual stimulus was presented in the word listening block.</p>
<p><bold>Experiment three:</bold> The experiment was divided into a first-word condition block and a last-word condition block, the order of which were counterbalanced across subjects. The subjects had to judge whether they heard more animate words or inanimate words by pressing different keys. In the first-word/last-word condition, they should only count the first-word or the last word in each word state. Five word states appeared in each trial and therefore if, e.g., 3 word states started with animate words the subjects should judge that the trial had more animate words in the first-word condition. They were not told how many word states might appear in each sequence. The subjects had a break every 15 trials. Before each condition, the subjects went through a practice session, in which they listened to two example sequences and made judgments. They received feedback during the practice session but not during the main experiment. In the main experiment, the subjects gave correct answers in 80 &#x00B1; 4&#x0025; and 63 &#x00B1; 3&#x0025; trials in the first-word and last-word conditions respectively. The correct rate was significantly higher in the first-word condition (P &#x003C; 0.0001, bootstrap), in which the timing of the target word was more predictable. The correct rate, however, remained above the 50&#x0025; chance level in the last-word condition (P &#x003C; 0.0001, bootstrap).</p>
</sec>
<sec id="s4e"><title>EEG recording and analysis</title>
<p>EEG responses were recorded using a 64-channel Biosemi ActiveTwo system. Additionally, four electrodes were used to record horizontal and vertical EOG and two reference electrodes were placed at the left and right mastoids. The EEG recordings were low-pass filtered below 400 Hz and sampled at 2048 Hz. The EEG recordings were referenced to the average mastoid recording offline and the horizontal and vertical EOG signals were regressed out. Since the study focused on word-rate and syllable-rate neural responses (2 Hz and 4 Hz respectively), the EEG recordings were high-pass filtered above 0.7 Hz. The EEG recordings were epoched based on the onset of each word state (9 s epochs starting 2 s before the word state onset) and averaged over all epochs.</p>
<p>In the frequency domain analysis, a Discrete Fourier Transform was applied to each EEG channel and each subject. The analysis window was 2 s in duration, corresponding to a frequency resolution of 0.5 Hz. In experiment two, a single analysis window was used, which started from the word state onset. In experiment one, since the word state is longer, two successive analysis windows were applied, with the first one starting from the word state onset and the second starting from the offset of the first analysis window. The EEG spectrum is averaged over EEG channels and subjects (and also analysis windows in experiment one) by calculating the root-mean-square value.</p>
<p>In the time domain analysis, to visualize the response waveform (<xref rid="fig3" ref-type="fig">Fig. 3A</xref>), the EEG responses were filtered between 1.5 and 4.5 Hz using a linear phase finite impulse response (FIR) filter (impulse response duration: 1 s). The linear delay caused by the FIR filter is compensated by shifting the filtered signal back in time. When separately analyzing the instantaneous amplitude of the word-rate or syllable-rate response (<xref rid="fig3" ref-type="fig">Fig. 3CD</xref> and <xref rid="fig4" ref-type="fig">4C</xref>), the EEG responses were bandpass filtered using a 1-s duration FIR filter with the lower and higher cutoff frequencies set to 0.25 Hz below and above the word or syllable rate. The instantaneous amplitude of the word-rate and syllable-rate EEG responses were extracted using the Hilbert transform.</p>
</sec>
<sec id="s4f"><title>Statistical test</title>
<p>This study used bias-corrected and accelerated bootstrap for all significance tests (<xref ref-type="bibr" rid="c18">Efron and Tibshirani, 1993</xref>). In the bootstrap procedure, all the subjects were resampled with replacement 10<sup>4</sup> times. For the significance test for peaks in the response spectrum (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>), the response amplitude at the peak frequency is compared with the mean amplitude of the neighboring 2 frequency bins (corresponding to a 1-Hz width). For the significance test for time intervals showing response amplitude differences (<xref rid="fig3" ref-type="fig">Fig. 3CD</xref> and <xref rid="fig4" ref-type="fig">4C</xref>), the EEG waveform was averaged over all sampled subjects and the instantaneous amplitude was then extracted using the Hilbert transform.</p>
</sec>
</sec>
</body>
<back>
<ack><title>Acknowledgement</title>
<p>We thank Huan Luo, Lucia Melloni, David Poeppel, Jonathan Simon, and Elana Zion-Golumbic for helpful comments on earlier versions of this manuscript.</p>
<sec><title>Funding</title>
<p>Work supported by National Natural Science Foundation of China 31500873 (ND), Zhejiang Provincial Natural Science Foundation of China LR16C090002 (ND), Fundamental Research Funds for the Central Universities (ND), and research funding from the State Key Laboratory of Industrial Control Technology, Zhejiang University (ND). The funding sources were not involved in study design, data collection and interpretation, or the decision to submit the work for publication.</p>
</sec>
</ack>
<ref-list><title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Arnal</surname>, <given-names>L. H.</given-names></string-name> &#x0026; <string-name><surname>Giraud</surname>, <given-names>A.-L.</given-names></string-name> <year>2012</year>. <article-title>Cortical oscillations and sensory predictions</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>16</volume>, <fpage>390</fpage>&#x2013;<lpage>398</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Barascud</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Pearce</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Griffiths</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name> &#x0026; <string-name><surname>Chait</surname>, <given-names>M.</given-names></string-name> <year>2016</year>. <article-title>Brain responses in humans reveal ideal observer-like sensitivity to complex acoustic patterns</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>113</volume>, <fpage>E616</fpage>&#x2013;<lpage>E625</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Bentin</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kutas</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>Hillyard</surname>, <given-names>S. A.</given-names></string-name> <year>1995</year>. <article-title>Semantic processing and memory for attended and unattended words in dichotic listening: behavioral and electrophysiological evidence</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>21</volume>, <fpage>54</fpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Billig</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Davis</surname>, <given-names>M. H.</given-names></string-name>, <string-name><surname>Deeks</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Monstrey</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Carlyon</surname>, <given-names>R. P.</given-names></string-name> <year>2013</year>. <article-title>Lexical influences on auditory streaming</article-title>. <source>Current Biology</source>, <volume>23</volume>, <fpage>1585</fpage>&#x2013;<lpage>1589</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="book"><string-name><surname>Bregman</surname>, <given-names>A. S.</given-names></string-name> <year>1990</year>. <source>Auditory scene analysis: the perceptual organization of sound</source>, <publisher-loc>Cambridge</publisher-loc>, <publisher-name>The MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Brennan</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Stabler</surname>, <given-names>E. P.</given-names></string-name>, <string-name><surname>Van Wagenen</surname>, <given-names>S. E.</given-names></string-name>, <string-name><surname>Luh</surname>, <given-names>W. M.</given-names></string-name> &#x0026; <string-name><surname>Hale</surname>, <given-names>J. T.</given-names></string-name> <year>2016</year>. <article-title>Abstract linguistic structure correlates with temporal activity during naturalistic comprehension</article-title>. <source>Brain and language</source>, <volume>157</volume>, <fpage>81</fpage>&#x2013;<lpage>94</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Brosch</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Selezneva</surname>, <given-names>E.</given-names></string-name> &#x0026; <string-name><surname>Scheich</surname>, <given-names>H.</given-names></string-name> <year>2011</year>. <article-title>Formation of associations in auditory cortex by slow changes of tonic firing</article-title>. <source>Hearing research</source>, <volume>271</volume>, <fpage>66</fpage>&#x2013;<lpage>73</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Buiatti</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pe</surname> <given-names>A M.</given-names></string-name> &#x0026; <string-name><surname>Dehaene-Lambertz</surname>, <given-names>G.</given-names></string-name> <year>2009</year>. <article-title>Investigating the neural correlates of continuous speech computation with frequency-tagged neuroelectric responses</article-title>. <source>Neuroimage</source>, <volume>44</volume>, <fpage>509</fpage>&#x2013;<lpage>51</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Buzsaki</surname>, <given-names>G.</given-names></string-name> <year>2010</year>. <article-title>Neural syntax: cell assemblies, synapsembles, and readers</article-title>. <source>Neuron</source>, <volume>68</volume>, <fpage>362</fpage>&#x2013;<lpage>385</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Canolty</surname>, <given-names>R. T.</given-names></string-name>, <string-name><surname>Edwards</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Dalal</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Soltani</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nagarajan</surname>, <given-names>S. S.</given-names></string-name>, <string-name><surname>Kirsch</surname>, <given-names>H. E.</given-names></string-name>, <string-name><surname>Berger</surname>, <given-names>M. S.</given-names></string-name>, <string-name><surname>Barbaro</surname>, <given-names>N. M.</given-names></string-name> &#x0026; <string-name><surname>Knight</surname>, <given-names>R. T.</given-names></string-name> <year>2006</year>. <article-title>High Gamma Power Is Phase-Locked to Theta Oscillations in Human Neocortex</article-title>. <source>Science</source>, <volume>313</volume>, <fpage>1626</fpage> &#x2013; <lpage>1628</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Carlyon</surname>, <given-names>R. P.</given-names></string-name>, <string-name><surname>Cusack</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Foxton</surname>, <given-names>J. M.</given-names></string-name> &#x0026; <string-name><surname>Robertson</surname>, <given-names>I. H.</given-names></string-name> <year>2001</year>. <article-title>Effects of attention and unilateral neglect on auditory stream segregation</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>27</volume>, <fpage>115</fpage>&#x2013;<lpage>127</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Cherry</surname>, <given-names>E. C.</given-names></string-name> <year>1953</year>. <article-title>Some experiments on the recognition of speech, with one and with two ears</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>25</volume>, <fpage>975</fpage>&#x2013;<lpage>979</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Conway</surname>, <given-names>A. R. A.</given-names></string-name>, <string-name><surname>Cowan</surname>, <given-names>N.</given-names></string-name> &#x0026; <string-name><surname>Bunting</surname>, <given-names>M. F.</given-names></string-name> <year>2001</year>. <article-title>The cocktail party phenomenon revisited: The importance of working memory capacity</article-title>. <source>Psychonomic Bulletin &#x0026; Review</source>, <volume>8</volume>, <fpage>331</fpage>&#x2013;<lpage>335</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="book"><string-name><surname>Cutler</surname>, <given-names>A.</given-names></string-name> <year>2012</year>. <source>Native listening: Language experience and the recognition of spoken words</source>, <publisher-name>Mit Press</publisher-name>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Di Liberto</surname>, <given-names>G. M.</given-names></string-name>, <string-name><surname>O&#x0027;Sullivan</surname>, <given-names>J. A.</given-names></string-name> &#x0026; <string-name><surname>Lalor</surname>, <given-names>E. C.</given-names></string-name> <year>2015</year>. <article-title>Low-frequency cortical entrainment to speech reflects phoneme-level processing</article-title>. <source>Current Biology</source>, <volume>25</volume>, <fpage>2457</fpage>&#x2013;<lpage>2465</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Ding</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Melloni</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Tian</surname>, <given-names>X.</given-names></string-name> &#x0026; <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name> <year>2016</year>. <article-title>Cortical tracking of hierarchical linguistic structures in connected speech</article-title>. <source>Nature Neuroscience</source>, <volume>19</volume>, <fpage>158</fpage>&#x2013;<lpage>164</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Ding</surname>, <given-names>N.</given-names></string-name> &#x0026; <string-name><surname>Simon</surname>, <given-names>J. Z.</given-names></string-name> <year>2012</year>. <article-title>Emergence of neural encoding of auditory objects while listening to competing speakers</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>109</volume>, <fpage>11854</fpage>&#x2013;<lpage>11859</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="book"><string-name><surname>Efron</surname>, <given-names>B.</given-names></string-name> &#x0026; <string-name><surname>Tibshirani</surname>, <given-names>R.</given-names></string-name> <year>1993</year>. <source>An introduction to the bootstrap</source>, <publisher-name>CRC press</publisher-name>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Farthouat</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Franco</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Mary</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Delpouve</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wens</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>De Beeck</surname>, <given-names>M. O.</given-names></string-name>, <string-name><given-names>DE TI</given-names> <surname>Ge, X</surname></string-name>. &#x0026; <string-name><surname>Peigneux</surname>, <given-names>P.</given-names></string-name> <year>2016</year>. <article-title>Auditory Magnetoencephalographic Frequency-Tagged Responses Mirror the Ongoing Segmentation Processes Underlying Statistical Learning</article-title>. <source>Brain Topography</source>, <fpage>1</fpage>&#x2013;<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="book"><string-name><surname>Fodor</surname>, <given-names>J. A.</given-names></string-name> <year>1983</year>. <source>The modularity of mind: An essay on faculty psychology</source>, <publisher-name>MIT press</publisher-name>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Friederici</surname>, <given-names>A. D.</given-names></string-name> <year>2002</year>. <article-title>Towards a neural basis of auditory sentence processing</article-title>. <source>Trends in cognitive sciences</source>, <volume>6</volume>, <fpage>78</fpage>&#x2013;<lpage>84</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Fritz</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Elhilali</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>David</surname>, <given-names>S. V.</given-names></string-name> &#x0026; <string-name><surname>Shamma</surname>, <given-names>S. A.</given-names></string-name> <year>2007</year>. <article-title>Does attention play a role in dynamic receptive field adaptation to changing acoustic salience in A1?</article-title> <source>Hearing Research</source>, <volume>229</volume>, <fpage>186</fpage>-<lpage>203</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Gavornik</surname>, <given-names>J. P.</given-names></string-name> &#x0026; <string-name><surname>Bear</surname>, <given-names>M. F.</given-names></string-name> <year>2014</year>. <article-title>Learned spatiotemporal sequence recognition and prediction in primary visual cortex</article-title>. <source>Nature neuroscience</source>, <volume>17</volume>, <fpage>732</fpage>-<lpage>737</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Giraud</surname>, <given-names>A.-L.</given-names></string-name> &#x0026; <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name> <year>2012</year>. <article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title>. <source>Nature neuroscience</source>, <volume>15</volume>, <fpage>511</fpage>&#x2013;<lpage>517</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Golumbic</surname>, <given-names>E. M. Z.</given-names></string-name>, <string-name><surname>Ding</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Bickel</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Lakatos</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Schevon</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Mckhann</surname>, <given-names>G. M.</given-names></string-name>, <string-name><surname>Goodman</surname>, <given-names>R. R.</given-names></string-name>, <string-name><surname>Emerson</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Mehta</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Simon</surname>, <given-names>J. Z.</given-names></string-name>, <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name> &#x0026; <string-name><surname>Schroeder</surname>, <given-names>C. E.</given-names></string-name> <year>2013</year>. <article-title>Mechanisms Underlying Selective Neuronal Tracking of Attended Speech at a &#x201C;Cocktail Party"</article-title>. <source>Neuron</source>, <volume>77</volume>, <fpage>980</fpage>&#x2013;<lpage>991</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Goswami</surname>, <given-names>U.</given-names></string-name> &#x0026; <string-name><surname>Leong</surname>, <given-names>V.</given-names></string-name> <year>2013</year>. <article-title>Speech rhythm and temporal structure: Converging perspectives?</article-title> <source>Laboratory Phonology</source>, <volume>4</volume>, <fpage>67</fpage>&#x2013;<lpage>92</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Hannemann</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Obleser</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Eulitz</surname>, <given-names>C.</given-names></string-name> <year>2007</year>. <article-title>Top-down knowledge supports the retrieval of lexical information from degraded speech</article-title>. <source>Brain research</source>, <volume>1153</volume>, <fpage>134</fpage>&#x2013;<lpage>143</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Jones</surname>, <given-names>J. A.</given-names></string-name> &#x0026; <string-name><surname>Freyman</surname>, <given-names>R. L.</given-names></string-name> <year>2012</year>. <article-title>Effect of priming on energetic and informational masking in a same-different task</article-title>. <source>Ear and hearing</source>, <volume>33</volume>, <fpage>124</fpage>&#x2013;<lpage>133</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Kerlin</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Shahin</surname>, <given-names>A. J.</given-names></string-name> &#x0026; <string-name><surname>Miller</surname>, <given-names>L. M.</given-names></string-name> <year>2010</year>. <article-title>Attentional Gain Control of Ongoing Cortical Speech Representations in a &#x201C;Cocktail Party&#x201D;</article-title>. <source>Journal of Neuroscience</source>, <volume>30</volume>, <fpage>620</fpage>&#x2013;<lpage>628</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Kong</surname>, <given-names>Y.-Y.</given-names></string-name>, <string-name><surname>Mullangi</surname>, <given-names>A.</given-names></string-name> &#x0026; <string-name><surname>Ding</surname>, <given-names>N.</given-names></string-name> <year>2014</year>. <article-title>Differential Modulation of Auditory Responses to Attended and Unattended Speech in Different Listening Conditions</article-title>. <source>Hearing Research</source>, <volume>316</volume>, <fpage>73</fpage>&#x2013;<lpage>81</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Lakatos</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Shah</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Knuth</surname>, <given-names>K. H.</given-names></string-name>, <string-name><surname>Ulbert</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Karmos</surname>, <given-names>G.</given-names></string-name> &#x0026; <string-name><surname>Schroeder</surname>, <given-names>C. E.</given-names></string-name> <year>2005</year>. <article-title>An oscillatory hierarchy controlling neuronal excitability and stimulus processing in the auditory cortex</article-title>. <source>Journal of neurophysiology</source>, <volume>94</volume>, <fpage>1904</fpage>&#x2013;<lpage>1911</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="book"><string-name><surname>Lashley</surname>, <given-names>K. S.</given-names></string-name> <year>1951</year>. <chapter-title>the problem of serial order in behavior</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Jeffress</surname>, <given-names>L. A</given-names></string-name></person-group>. (ed.) <source>Cerebral Mechanisms in Behavior, The Hixon Symposium</source>. <publisher-name>New York: Wiley</publisher-name>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Linden</surname>, <given-names>R. D.</given-names></string-name>, <string-name><surname>Picton</surname>, <given-names>T. W.</given-names></string-name>, <string-name><surname>Hamel</surname>, <given-names>G.</given-names></string-name> &#x0026; <string-name><surname>Campbell</surname>, <given-names>K. B.</given-names></string-name> <year>1987</year>. <article-title>Human auditory steady-state evoked potentials during selective attention</article-title>. <source>Electroencephalography and Clinical Neurophysiology</source>, <volume>66</volume>, <fpage>145</fpage>&#x2013;<lpage>159</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Lisman</surname>, <given-names>J. E.</given-names></string-name> &#x0026; <string-name><surname>Jensen</surname>, <given-names>O.</given-names></string-name> <year>2013</year>. <article-title>The theta-gamma neural code</article-title>. <source>Neuron</source>, <volume>77</volume>, <fpage>1002</fpage>&#x2013;<lpage>1016</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Lu</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Yin</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Oxenham</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Fritz</surname>, <given-names>J. B.</given-names></string-name> &#x0026; <string-name><surname>Shamma</surname>, <given-names>S. A.</given-names></string-name> <year>2017</year>. <article-title>Temporal coherence structure rapidly shapes neuronal interactions</article-title>. <source>Nature Communications</source>, <volume>8</volume>, <fpage>13900</fpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Luck</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Vogel</surname>, <given-names>E. K.</given-names></string-name> &#x0026; <string-name><surname>Shapiro</surname>, <given-names>K. L.</given-names></string-name> <year>1996</year>. <article-title>Word meanings can be accessed but not reported during the attentional blink</article-title>. <source>Nature</source>, <volume>383</volume>, <fpage>616</fpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Martin</surname>, <given-names>A. E.</given-names></string-name> &#x0026; <string-name><surname>Doumas</surname>, <given-names>L. A.</given-names></string-name> <year>2017</year>. <article-title>A mechanism for the cortical computation of hierarchical linguistic structure</article-title>. <source>PLoS Biology</source>, <volume>15</volume>, <fpage>e200066</fpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Mcdermott</surname>, <given-names>J. H.</given-names></string-name>, <string-name><surname>Wrobleski</surname>, <given-names>D.</given-names></string-name> &#x0026; <string-name><surname>Oxenham</surname>, <given-names>A. J.</given-names></string-name> <year>2011</year>. <article-title>Recovering sound sources from embedded repetition</article-title>. <source>proceedings of the National Academy of Sciences</source>, <volume>108</volume>, <fpage>1188</fpage>&#x2013;<lpage>1193</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Mesgarani</surname>, <given-names>N.</given-names></string-name> &#x0026; <string-name><surname>Chang</surname>, <given-names>E. F.</given-names></string-name> <year>2012</year>. <article-title>Selective cortical representation of attended speaker in multi-talker speech perception</article-title>. <source>Nature</source>, <volume>485</volume>, <fpage>233</fpage>&#x2013;<lpage>236</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Mesgarani</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Cheung</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Johnson</surname>, <given-names>K.</given-names></string-name> &#x0026; <string-name><surname>Chang</surname>, <given-names>E. F.</given-names></string-name> <year>2014</year>. <article-title>Phonetic feature encoding in human superior temporal gyrus</article-title>. <source>Science</source>, <volume>343</volume>, <fpage>1006</fpage>&#x2013;<lpage>1010</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Meyer</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Henry</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Gaston</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Schmuck</surname>, <given-names>N.</given-names></string-name> &#x0026; <string-name><surname>Friederici</surname>, <given-names>A. D.</given-names></string-name> <year>2016</year>. <article-title>Linguistic bias modulates interpretation of speech via neural delta-band oscillations</article-title>. <source>Cerebral Cortex</source>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Micheyl</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Tian</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Carlyon</surname>, <given-names>R. P.</given-names></string-name> &#x0026; <string-name><surname>Rauschecker</surname>, <given-names>J. P.</given-names></string-name> <year>2005</year>. <article-title>Perceptual organization of tone sequences in the auditory cortex of awake macaques</article-title>. <source>Neuron</source>, <volume>48</volume>, <fpage>139</fpage>&#x2013;<lpage>148</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Naatanen</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Paavilainen</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Rinne</surname>, <given-names>T.</given-names></string-name> &#x0026; <string-name><surname>Alho</surname>, <given-names>K.</given-names></string-name> <year>2007</year>. <article-title>The mismatch negativity (MMN) in basic research of central auditory processing: a review</article-title>. <source>Clinical Neurophysiology</source>, <volume>118, 2544-2590</volume>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Naccache</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Blandin</surname>, <given-names>E.</given-names></string-name> &#x0026; <string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name> <year>2002</year>. <article-title>Unconscious masked priming depends on temporal attention</article-title>. <source>Psychological science</source>, <volume>13</volume>, <fpage>416</fpage>-<lpage>424</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Nelson</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>El Karoui</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Giber</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Cohen</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Koopman</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name> &#x0026;. <article-title>PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES</article-title> <year>2017</year>. <source>Neurophysiological dynamics of phrase-structure building during sentence processing. Proceedings of the National Academy of Sciences</source>, <volume>114</volume>, <fpage>E3669</fpage>&#x2013;<lpage>E3678</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Nobre</surname>, <given-names>A. C.</given-names></string-name> &#x0026; <string-name><surname>Mccarthy</surname>, <given-names>G.</given-names></string-name> <year>1995</year>. <article-title>Language-related field potentials in the anterior-medial temporal lobe: II. Effects of word type and semantic priming</article-title>. <source>Journal of neuroscience</source>, <volume>15</volume>, <fpage>1090</fpage>&#x2013;<lpage>1098</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>O&#x0027;Connell</surname>, <given-names>R. G.</given-names></string-name>, <string-name><surname>Dockree</surname>, <given-names>P. M.</given-names></string-name> &#x0026; <string-name><surname>Kelly</surname>, <given-names>S. P.</given-names></string-name> <year>2012</year>. <article-title>A supramodal accumulation-to-bound signal that determines perceptual decisions in humans</article-title>. <source>Nature neuroscience</source>, <volume>15</volume>, <fpage>1729</fpage>&#x2013;<lpage>1735</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="other"><string-name><surname>O&#x0027;Sullivan</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Power</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Mesgarani</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Rajaram</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Foxe</surname>, <given-names>J. J.</given-names></string-name>, <string-name><surname>Shinn-Cunningham</surname>, <given-names>B. G.</given-names></string-name>, <string-name><surname>Slaney</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Shamma</surname>, <given-names>S. A.</given-names></string-name> &#x0026; <string-name><surname>Lalor</surname>, <given-names>E. C.</given-names></string-name> <year>2014</year>. <article-title>Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG</article-title>. <source>Cerebral Cortex</source>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Pallier</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Devauchelle</surname>, <given-names>A.-D</given-names></string-name>. &#x0026; <string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name> <year>2011</year>. <article-title>Cortical representation of the constituent structure of sentences</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>108</volume>, <fpage>2522</fpage>&#x2013;<lpage>2527</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Park</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Kayser</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Thut</surname>, <given-names>G.</given-names></string-name> &#x0026; <string-name><surname>Gross</surname>, <given-names>J.</given-names></string-name> <year>2016</year>. <article-title>Lip movements entrain the observers&#x0027; low-frequency brain oscillations to facilitate speech intelligibility</article-title>. <source>eLife</source>, <volume>5</volume>, <fpage>e14521</fpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Pena</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>Melloni</surname>, <given-names>L.</given-names></string-name> <year>2012</year>. <article-title>Brain oscillations during spoken sentence processing</article-title>. <source>Journal of cognitive neuroscience</source>, <volume>24, 1149-1164</volume>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Peelle</surname>, <given-names>J. E.</given-names></string-name>, <string-name><surname>Gross</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Davis</surname>, <given-names>M. H.</given-names></string-name> <year>2013</year>. <article-title>Phase-Locked Responses to Speech in Human Auditory Cortex are Enhanced During Comprehension</article-title> <source>Cerebral Cortex</source>, <volume>23</volume>, <fpage>1378</fpage>&#x2013;<lpage>1387</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Sanders</surname>, <given-names>L. D.</given-names></string-name>, <string-name><surname>Newport</surname>, <given-names>E. L.</given-names></string-name> &#x0026; <string-name><surname>Neville</surname>, <given-names>H. J.</given-names></string-name> <year>2002</year>. <article-title>Segmenting nonsense: an event-related potential index of perceived onsets in continuous speech</article-title>. <source>Nature neuroscience</source>, <volume>5</volume>, <fpage>700</fpage>&#x2013;<lpage>703</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Schroeder</surname>, <given-names>C. E.</given-names></string-name> &#x0026; <string-name><surname>Lakatos</surname>, <given-names>P.</given-names></string-name> <year>2009</year>. <article-title>Low-frequency neuronal oscillations as instruments of sensory selection</article-title>. <source>Trends in Neurosciences</source>, <volume>32</volume>, <fpage>9</fpage>&#x2013;<lpage>18</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Shamma</surname>, <given-names>S.</given-names></string-name> <year>2001</year>. <article-title>On the role of space and time in auditory processing</article-title>. <source>Trends in cognitive sciences</source>, <volume>5</volume>, <fpage>340</fpage>&#x2013;<lpage>348</lpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Shamma</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Elhilali</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>Micheyl</surname>, <given-names>C.</given-names></string-name> <year>2011</year>. <article-title>Temporal coherence and attention in auditory scene analysis</article-title>. <source>Trends in Neurosciences</source>, <volume>34</volume>, <fpage>114</fpage>&#x2013;<lpage>123</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="book"><string-name><surname>Shinn-Cunningham</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Best</surname>, <given-names>V.</given-names></string-name> &#x0026; <string-name><surname>Lee</surname>, <given-names>A. K.</given-names></string-name> <year>2017</year>. <chapter-title>Auditory Object Formation and Selection</chapter-title>. <italic>In:</italic> <person-group person-group-type="editor"><string-name><surname>Middlebrooks</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Simon</surname>, <given-names>J. Z.</given-names></string-name>, <string-name><surname>Popper</surname>, <given-names>A. N.</given-names></string-name> &#x0026; <string-name><surname>Fay, R.</surname> <given-names>R</given-names></string-name></person-group>. (eds.) <source>The Auditory System at the Cocktail Party</source>. <publisher-name>Springer International Publishing</publisher-name>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><surname>Shinn-Cunningham</surname>, <given-names>B. G.</given-names></string-name> <year>2008</year>. <article-title>Object-based auditory and visual attention</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>12</volume>, <fpage>182</fpage>&#x2013;<lpage>186</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><string-name><surname>Snyder</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Alain</surname>, <given-names>C.</given-names></string-name> &#x0026; <string-name><surname>Picton</surname>, <given-names>T. W.</given-names></string-name> <year>2006</year>. <article-title>Effects of attention on neuroelectric correlates of auditory stream segregation</article-title>. <source>Journal of cognitive neuroscience</source>, <volume>18</volume>, <fpage>1</fpage>&#x2013;<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><string-name><surname>Steinhauer</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Alter</surname>, <given-names>K.</given-names></string-name> &#x0026; <string-name><surname>Friederici</surname>, <given-names>A. D.</given-names></string-name> <year>1999</year>. <article-title>Brain potentials indicate immediate use of prosodic cues in natural speech processing</article-title>. <source>Nature neuroscience</source>, <volume>2</volume>, <fpage>191</fpage>&#x2013;<lpage>196</lpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><string-name><surname>Steinschneider</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nourski</surname>, <given-names>K. V.</given-names></string-name> &#x0026; <string-name><surname>Fishman</surname>, <given-names>Y. I.</given-names></string-name> <year>2013</year>. <article-title>Representation of speech in human auditory cortex: Is it special?</article-title> <source>Hearing research</source>, <fpage>57</fpage>&#x2013;<lpage>73</lpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><string-name><surname>Sussman</surname>, <given-names>E. S.</given-names></string-name>, <string-name><given-names>HORV</given-names> <surname>Th, J.</surname></string-name>, <string-name><surname>Winkler</surname>, <given-names>I.</given-names></string-name> &#x0026; <string-name><surname>Orr</surname>, <given-names>M.</given-names></string-name> <year>2007</year>. <article-title>The role of attention in the formation of auditory streams</article-title>. <source>Attention, Perception, &#x0026; Psychophysics</source>, <volume>69</volume>, <fpage>136</fpage>&#x2013;<lpage>152</lpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><string-name><surname>Treisman</surname>, <given-names>A. M.</given-names></string-name> &#x0026; <string-name><surname>Gelade</surname>, <given-names>G.</given-names></string-name> <year>1980</year>. <article-title>A feature-integration theory of attention</article-title>. <source>Cognitive psychology</source>, <volume>12</volume>, <fpage>97</fpage>&#x2013;<lpage>136</lpage>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><string-name><surname>Wassenhove</surname>, <given-names>V. V.</given-names></string-name>, <string-name><surname>Grant</surname>, <given-names>K. W.</given-names></string-name> &#x0026; <string-name><surname>Poeppel</surname>, <given-names>D.</given-names></string-name> <year>2003</year>. <article-title>Visual speech speeds up the neural processing of auditory speech</article-title>. <source>proceedings of the National Academy of Sciences of the United States of America</source>, <volume>102</volume>, <fpage>1181</fpage>&#x2013;<lpage>1186</lpage>.</mixed-citation></ref>
<ref id="c65"><mixed-citation publication-type="journal"><string-name><surname>Wood</surname>, <given-names>N.</given-names></string-name> &#x0026; <string-name><surname>Cowan</surname>, <given-names>N.</given-names></string-name> <year>1995</year>. <article-title>The cocktail party phenomenon revisited: how frequent are attention shifts to one&#x0027;s name in an irrelevant auditory channel?</article-title> <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>, <volume>21</volume>, <fpage>255</fpage>.</mixed-citation></ref>
<ref id="c66"><mixed-citation publication-type="journal"><string-name><surname>Woods</surname>, <given-names>K. J.</given-names></string-name> &#x0026; <string-name><surname>Mcdermott</surname>, <given-names>J. H.</given-names></string-name> <year>2015</year>. <article-title>Attentive tracking of sound sources</article-title>. <source>Current Biology</source>, <volume>25</volume>, <fpage>2238</fpage>&#x2013;<lpage>2246</lpage>.</mixed-citation></ref>
<ref id="c67"><mixed-citation publication-type="journal"><string-name><surname>Yin</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mishkin</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sutter</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>Fritz</surname>, <given-names>J. B.</given-names></string-name> <year>2008</year>. <article-title>Early stages of melody processing: stimulus-sequence and task-dependent neuronal activity in monkey auditory cortical fields A1 and R</article-title>. <source>Journal of neurophysiology</source>, <volume>100</volume>, <fpage>3009</fpage>&#x2013;<lpage>3029</lpage>.</mixed-citation></ref>
<ref id="c68"><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname>, <given-names>W.</given-names></string-name> &#x0026; <string-name><surname>Ding</surname>, <given-names>N.</given-names></string-name> <year>2017</year>. <article-title>Time-domain analysis of neural tracking of hierarchical linguistic structures</article-title>. <source>NeuroImage</source>, <volume>146</volume>, <fpage>333</fpage>&#x2013;<lpage>340</lpage>.</mixed-citation></ref>
<ref id="c69"><mixed-citation publication-type="journal"><string-name><surname>Zhou</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>De Villers-Sidani,</surname> <given-names>&#x00C9;.</given-names></string-name>, <string-name><surname>Panizzutti</surname>, <given-names>R.</given-names></string-name> &#x0026; <string-name><surname>Merzenich</surname>, <given-names>M. M.</given-names></string-name> <year>2010</year>. <article-title>Successive-signal biasing for a learned sound sequence</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>107</volume>, <fpage>14839</fpage>&#x2013;<lpage>14844</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>