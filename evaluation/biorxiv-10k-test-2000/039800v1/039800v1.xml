<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/039800</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Bioinformatics</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Semi-Supervised Learning of the Electronic Health Record with Denoising Autoencoders for Phenotype Stratification</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6700-1468</contrib-id>
<name>
<surname>Beaulieu-Jones</surname>
<given-names>Brett K.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8713-9213</contrib-id>
<name>
<surname>Greene</surname>
<given-names>Casey S.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Graduate Group in Genomics and Computational Biology, Computational Genetics Lab, Institute for Biomedical Informatics, University of Pennsylvania</institution></aff>
<aff id="a2"><label>2</label><institution>Department of Systems Pharmacology and Translational Therapeutics, Institute for Biomedical Informatics, Institute for Translational Medicine and Therapeutics, University of Pennsylvania</institution></aff>
</contrib-group>
<pub-date pub-type="epub">
<year>2016</year>
</pub-date>
<elocation-id>039800</elocation-id>
<history>
<date date-type="received">
<day>17</day>
<month>2</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>18</day>
<month>2</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2016, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2016</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="039800.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Patient interactions with health care providers result in entries to electronic health records (EHRs). EHRs were built for clinical and billing purposes but contain many data points about an individual. Mining these records provides opportunities to extract electronic phenotypes that can be paired with genetic data to identify genes underlying common human diseases. This task remains challenging: high quality phenotyping is costly and requires physician review; many fields in the records are sparsely filled; and our definitions of diseases are continuing to improve over time. Here we develop and evaluate a semi-supervised learning method for EHR phenotype extraction using denoising autoencoders for phenotype stratification. By combining denoising autoencoders with random forests we find classification improvements across simulation models, particularly in cases where only a small number of patients have high quality phenotype. This situation is commonly encountered in research with EHRs. Denoising autoencoders perform dimensionality reduction allowing visualization and clustering for the discovery of new subtypes of disease. This method represents a promising approach to clarify disease subtypes and improve genotype-phenotype association studies that leverage EHRs.</p>
<p><fig id="ufig1" position="float" fig-type="figure">
<graphic xlink:href="039800_ufig1.tif"/>
</fig></p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Electronic Health Record</kwd>
<kwd>Denoising Autoencoder</kwd>
<kwd>Unsupervised</kwd>
<kwd>Electronic Phenotyping</kwd>
<kwd>Patient Stratification</kwd>
</kwd-group>
<counts>
<page-count count="32"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>INTRODUCTION</title>
<p>Human diseases are complex and not perfectly understood. This means that while diseases are often considered as fixed phenotypes, many have evolving definitions and are difficult to classify. The electronic health record (EHR) is a popular source for electronic phenotyping to augment traditional genetic association studies, but there is a relative scarcity of research quality annotated patients [<xref ref-type="bibr" rid="c1">1</xref>]. Electronic phenotyping relies on either codes designed for billing or time intensive clinician review. This is an ideal environment for semi-supervised algorithms, particular those that perform unsupervised learning on many patients followed by supervised learning on a smaller, annotated, subset.</p>
<p>Denoising autoencoders (DAs) are a powerful tool to perform unsupervised learning [<xref ref-type="bibr" rid="c2">2</xref>]. DAs are trained similarly to artificial neural networks but taught to reconstruct an original input from an intentionally corrupted input. Through this training they find higher-level representations modeling the structure of the underlying data. By applying DAs in the EHR we sought to determine whether they could reduce the number of labeled samples required, construct non-billing code based phenotypes and elucidate disease subtypes for fine-tuned genetic association. Unsupervised learning can also help to recognize misdiagnosed patients in the form of outliers and is robust to changing disease definitions over time.</p>
<p>EHRs were designed for billing and clinical usage and not optimized for research. Despite this, EHRs have already proven an effective source of phenotypes in genetic association studies [<xref ref-type="bibr" rid="c3">3</xref>,<xref ref-type="bibr" rid="c4">4</xref>]. Initially, phenotypes were hand designed based on manual clinician review of patient records. These studies were limited by the time and cost inherent in manual review, but DAs can make use of unlabeled data [<xref ref-type="bibr" rid="c5">5</xref>]. Semi-supervised learning can be performed using DAs to perform unsupervised pre-training. After learning the structure of the data, the DA&#x2019;s hidden layer can be used as input to a traditional. This allows the DA to learn from all samples, even those without labels, and requires only a small subset to be annotated. Today, phenome-wide association studies (PheWAS) are the most prevalent example of EHR phenotyping, proving particularly effective at identifying pleiotropic genetic variants [<xref ref-type="bibr" rid="c6">6</xref>]. PheWASs often use algorithms based on the International Classification of Disease (ICD) codes to construct a phenotype. This coding system was designed for billing, not to capture human phenotypes. DA constructed features are combinations of all clinical data and may provide a more holistic view of a patient than billing codes alone.</p>
<p>Through extensive study, disease diagnoses are becoming more precise over time [<xref ref-type="bibr" rid="c7">7</xref>&#x2013;<xref ref-type="bibr" rid="c11">11</xref>]. Cancers, for example, were historically typed by occurrence location and the efficacy of different treatment types. As the mechanisms of cancer are better understood, they are further categorized by their physiological nature. The progression of subtypes in lung cancer illustrates the change over time for a previously poorly defined disease [<xref ref-type="bibr" rid="c7">7</xref>]. Beginning with a single diagnosis based on occurrence in the lung, it was later differentiated as small cell lung cancer and non-small cell lung cancer [<xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c9">9</xref>]. Non-small cell lung cancer was then broken up into squamous cell carcinoma, adenocarcinoma, and large cell carcinoma. Today these subtypes continue to be broken up based on the genetic locations and pathways of associated risk variants [<xref ref-type="bibr" rid="c10">10</xref>]. Higher-level subtypes have been found to differ in cell size, cell shape, tumor origin site and chemical properties and it is becoming clear that genome-phenotype associations to these subtypes are a many-to-one relationship [<xref ref-type="bibr" rid="c11">11</xref>]. The unsupervised nature of DAs means that even as the definitions of a disease change, they would not need to be retrained. Such refinements are not limited to cancer subtyping. The ability to produce more homogenous phenotypes increased genotype to phenotype linkage in schizophrenia, bipolar disease [<xref ref-type="bibr" rid="c12">12</xref>], and Rett Syndrome [<xref ref-type="bibr" rid="c13">13</xref>&#x2013;<xref ref-type="bibr" rid="c16">16</xref>]. Furthermore, type 2 diabetes subtypes have been discovered using topological analysis of EHR patient similarity [<xref ref-type="bibr" rid="c17">17</xref>]. The dimensionality reduction possible with a DA makes clustering and visualization more feasible. Subtyping exposes disease heterogeneity and will contribute to physiological understanding of complex diseases.</p>
<p>DAs were initially introduced as a component in constructing the deep networks used in deep learning [<xref ref-type="bibr" rid="c18">18</xref>]. Deep learning algorithms have become the dominant performers in many classification problems including image recognition, speech recognition and natural language processing [<xref ref-type="bibr" rid="c19">19</xref>&#x2013;<xref ref-type="bibr" rid="c24">24</xref>]. Deep learning techniques have recently been used with increasing popularity to solve biological problems including tumor classification, predicting chromatin structure and protein binding [<xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c25">25</xref>,<xref ref-type="bibr" rid="c26">26</xref>]. DAs showed strong performance early in the deep learning revolution but have been surpassed in most domains by convolutional neural networks or recurrent neural networks [<xref ref-type="bibr" rid="c18">18</xref>]. While complex deep networks such as convolutional neural networks have surpassed the performance of DAs in these areas, they rely on strictly structured relationships such as the relative positions of pixels within an image [<xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c27">27</xref>]. This structure is unlikely to exist in the EHR. In addition, convolutional neural networks and other deep networks (Deep belief networks, Recurrent Neural Networks etc.) are notoriously hard to interpret. DAs are easily generalizable, benefit from both linear and nonlinear correlation structure in the data, and contain accessible, interpretable, internal nodes [<xref ref-type="bibr" rid="c2">2</xref>]. Oftentimes the hidden layer is a &#x201C;bottle-neck&#x201D;, a much smaller size than the input layer, in order to force the autoencoder to learn the most important patterns in the data [<xref ref-type="bibr" rid="c27">27</xref>]. In patients diagnosed with the same disease, these important patterns may represent subtypes or other important patient clustering.</p>
<p>We evaluate DAs for phenotype construction using four simulation models of EHR data for complex phenotypes, modify DAs to effectively handle missingness in data and use the DA to create cluster visualizations that can aid in the discovery of subtypes of complex diseases.</p>
</sec>
<sec id="s2">
<title>METHODS</title>
<p>We developed a denoising autoencoder approach that constructs phenotypes through unsupervised learning. To evaluate the DA, we created a simulation framework that represents multiple hidden factors that affect numerous potentially overlapping observed variables. We evaluated the reduced DA models against feature-complete representations with popular supervised learning algorithms. These evaluations covered both complete datasets, as well as the more realistic cases of incompletely labeled and missing data. Finally, we developed a technique that uses the reduced feature-space of the DA to visualize potential subtypes. Each of these is fully described in its own section below, full parameters included sweeps are available in the supplementary materials. Source code to reproduce each analysis is included in our repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/greenelab/DAPS">https://github.com/greenelab/DAPS</ext-link>) [<xref ref-type="bibr" rid="c28">28</xref>] and is provided under a permissive open source license (3-clause BSD). A docker build is included with the repository to provide a common environment to easily reproduce results without installing dependencies [<xref ref-type="bibr" rid="c29">29</xref>]. In addition, Shippable, a continuous integration platform, is used to reanalyze results and generate figures after each commit [<xref ref-type="bibr" rid="c30">30</xref>].</p>
<sec id="s2a">
<title>Unsupervised Training</title>
<p>We used the Theano library [<xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c32">32</xref>] to construct a DA consisting of three layers, an input layer <italic>x</italic>, a single hidden layer <italic>y</italic>, and a reconstructed layer <italic>z</italic> [<xref ref-type="bibr" rid="c18">18</xref>] (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Noise was added to the input layer through a stochastic corruption process, which masks 20&#x0025; of the input values, selected at random, to zero.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig. 1.</label>
<caption><p><bold>A)</bold> Network diagram of DAs used for unsupervised pre-training. <bold>B)</bold> Supervised classification occurs using the pre-trained DA hidden nodes as input to a traditional classifier. <bold>C)</bold> Simulation model with example cases and controls under each rule set.</p></caption>
<graphic xlink:href="039800_fig1.tif"/>
</fig>
<p>The hidden layer y was calculated by multiplying the input layer by a weight vector <italic>W</italic>, adding a bias vector <italic>b</italic> and computing the sigmoid (<xref ref-type="disp-formula" rid="eqn1">Formula 1</xref>). The reconstructed layer <italic>z</italic> was similarly computed using tied weights, the transpose of <italic>W</italic> and <italic>b</italic> (<xref ref-type="disp-formula" rid="eqn2">Formula 2</xref>). The cost function is the cross-entropy of the reconstruction, a measure of distance between the reconstructed layer and the input layer (<xref ref-type="disp-formula" rid="eqn3">Formula 3</xref>).
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="039800_eqn1.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="039800_eqn2.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="039800_eqn3.gif"/></alternatives>
</disp-formula></p>
<p>Stochastic gradient descent was performed for 1000 training epochs, at a learning rate of 0.1. Hidden layers of two, four, eight and sixteen hidden nodes were included in the parameter sweep with a 20&#x0025; input corruption level. Vincent et al. [<xref ref-type="bibr" rid="c18">18</xref>] provide a through explanation of training for DAs without missing data.</p>
<p>In the event of missing data, the cost calculation was modified to exclude missing data from contributing to the reconstruction cost. A missingness vector <italic>m</italic> was created for each input vector, with a value of 1 where the data is present and 0 when the data is missing. Both the input sample <italic>x</italic> and reconstruction <italic>z</italic> were multiplied by <italic>m</italic> and the cross entropy error was divided by the sum of the <italic>m</italic>, the number of non-missing features to get the average cost per feature present (<xref ref-type="disp-formula" rid="eqn4">Formula 4</xref>). This allowed the DA to learn the structure of the data from present features rather than imputation.
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="039800_eqn4.gif"/></alternatives>
</disp-formula></p>
<p>Full implementation and training details are available in the supplementary materials.</p>
</sec>
<sec id="s2b">
<title>Supervised Denoising Autoencoder Classifier</title>
<p>To convert the DA to a supervised classifier, we first trained the DA in an unsupervised fashion (pre-training) (<xref ref-type="fig" rid="fig1">Fig 1A</xref>). We then applied a variety of traditional machine learning classifiers including, decision trees, random forests, logistic regression, nearest neighbors and support vector machines to the pre-trained unsupervised hidden layer values, <italic>y</italic>, of the DA (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Random forests applied to DA hidden nodes (DA&#x002B;RF) were shown for all comparisons. Predictive performance was measured by comparing the AUROC using stratified 10-fold cross validation. The Scikit-learn library was used for the traditional classifiers [<xref ref-type="bibr" rid="c33">33</xref>]. The Support Vector Machine uses a radial basis function kernel.</p>
</sec>
<sec id="s2c">
<title>Simulation Framework</title>
<p>Patients were simulated from a set of abstract hidden input effects with a status of 1 or 0. These input effects shift the mean of 1 to N observed clinical variables chosen at random with replacement (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). An example of a hypothetical condition a hidden input effect could represent is the familial hypercholesterolemia genotype. For a patient with the familial hypercholesterolemia genotype, the simulated clinical observations could represent increases in levels of total and low-density lipoprotein cholesterol, the deposition of cholesterol in extravascular tissues, corneal arcus and elevated triglyceride levels [<xref ref-type="bibr" rid="c34">34</xref>]. Some factors such as elevated triglyceride levels are not solely the result of the genetic predisposition and are related to environmental factors. Hypothetically additional hidden input effects on the same observed variable would represent these other factors. Because our goal is to evaluate methods for their ability to broadly capture these types of patterns, we generate randomized relationships between hidden and observed variables. This avoids overfitting our evaluation to specific phenotypes.</p>
<p>Next, a confounding systematic bias was added to a random subset (33&#x0025;) of the patients as a source of additional noise to simulate the variance accompanying data created by physicians, labs, hospitals or other spurious effects.</p>
<p>Within the simulation combinations of hidden input effects determine case-control status under four models:</p>
<list list-type="order">
<list-item><p><bold>All together/all relevant.</bold> Individuals have the same value (0 or 1) for all hidden input effects. Controls have all hidden effects set to 0. Cases have all hidden effects set to 1. A model capturing any hidden input will be able to predict case/control status in this scenario.</p></list-item>
<list-item><p><bold>All independent /single effect relevant.</bold> Individuals have 0 to N (specified per simulation) hidden input effects chosen at random. One arbitrary effect (the last one) is used to determine case-control status. In controls, this is 0. In cases, this is 1. A model capturing the relevant hidden input will be able to predict case/control status in this scenario.</p></list-item>
<list-item><p><bold>All independent/percentage based.</bold> Individuals have 0 to N (specified per simulation) of hidden input effects chosen at random set to 1. The percentage of hidden input effects on represents the probability of the patient being a case. A model capturing more hidden effects will be able to more accurately predict case/control in this scenario.</p></list-item>
<list-item><p><bold>All independent/complex rule based.</bold> Individuals have 0 to N (specified per simulation) of hidden inputs chosen at random set to 1. The sum of hidden effects determines case-control status (cases are even, controls are odd). A model must capture all hidden effects to successfully predict case/control in this scenario.</p></list-item>
</list>
</sec>
<sec id="s2d">
<title>Supervised Classification Comparison</title>
<p>If successfully trained, the hidden layer of a DA, <italic>y</italic>, captures the first <italic>n</italic> factors of variation in the data, where <italic>n</italic> is the number of nodes in the hidden layer. To test whether the DA constructed useful features by learning the main factors of variation in the data we used the trained hidden layer as an input to a shallow classifier.</p>
<p>To do this, we first completed unsupervised pre-training of the DA with all of the simulated samples. The hidden layer values, <italic>y</italic>, were calculated for all samples using the trained DA without any corruption and fed in as the features to a random forest to form a supervised classifier.</p>
<p>Classification performance between DAs plus random forests (DA&#x002B;RF) were compared against decision trees, random forests, nearest neighbors and support vector machines in a parameter sweep under each model (<xref ref-type="table" rid="tbl1">Table 1</xref>). Additional model parameters included in sweeps are included in the supplementary materials. All traditional classifiers were implemented with Scikit-learn [<xref ref-type="bibr" rid="c33">33</xref>]. Classification performance was compared using the AUROC.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><p>Simulation Model 1 Parameter Sweep Specifications.</p></caption>
<graphic xlink:href="039800_tbl1.tif"/>
</table-wrap>
</sec>
<sec id="s2e">
<title>Semi-Supervised Classification Comparison</title>
<p>The supervised classification comparison was repeated but with additional patients simulated and utilized during the unsupervised pre-training of the DA. The additional patients were simulated at the same 50&#x0025; case, 50&#x0025; control ratio but their labels were discarded after simulation. These additional patients were mixed with the original labeled patients and included in the unsupervised pre-training of the DA. The unlabeled samples were then discarded and the DA&#x002B;RF was then provided the same, labeled, patient groups as the traditional classifiers. The labeled patient samples were run through the trained DA in the same manner as the unsupervised pre-training but without any corruption added to the data. The DA&#x002B;RF and traditional classifiers were evaluated in a parameter sweep under each model using 10-fold cross validation.</p>
</sec>
<sec id="s2f">
<title>Missing Data Comparison</title>
<p>The semi-supervised classification comparison was repeated five times with, 0&#x0025;, 10&#x0025;, 20&#x0025;, 30&#x0025; and 40&#x0025; of the data missing. Missing data was added at random per sample, depending on the specified percentage missing.</p>
<p>Throughout these trials, the cost calculation was modified to exclude missing data from the cost and allow the DA to learn without imputing values (<xref ref-type="disp-formula" rid="eqn4">Formula 4</xref>). The traditional classifiers were trained using mean imputation for missing data. Mean imputation is particularly well suited for the simulation models because the observations were drawn from normal distributions, potentially giving an advantage to the non-DA algorithms that would not be available in many real datasets.</p>
<p>As in the semi-supervised classification comparison trial, the DA&#x002B;RF and traditional classifiers were evaluated under each model using 10-fold cross validation.</p>
</sec>
<sec id="s2g">
<title>Clustering and Visualization</title>
<p>To interpret and visualize results, patient populations were clustered using principal components analysis (PCA) and t-Stochastic Neighbor Embedding (t-SNE) of the trained DA&#x2019;s hidden nodes [<xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c36">36</xref>]. PCA and t-SNE were implemented with the Sci-kit learn library [<xref ref-type="bibr" rid="c33">33</xref>].</p>
<p>Ten thousand patients (5,000 cases, 5,000 controls) with four hidden effects were simulated under model 1. PCA followed by t-SNE was performed initially on the raw input for comparison and then on the hidden nodes of the DA after every 10 training epochs.</p>
<p>To test the ability to identify subtypes, we simulated 15,000 patients, 5,000 cases under model 1, 5,000 cases under model 2, and 5,000 controls. Input observations were compared to two, three and four-node DAs using PCA followed by t-SNE.</p>
</sec>
</sec>
<sec id="s3">
<title>RESULTS</title>
<sec id="s3a">
<title>Case-Control DA Training Visualization</title>
<p>We trained a DA and visualized the training process using PCA and t-SNE. These visualization techniques offer intuition and the ability to examine the sub-clusters. Given 5,000 cases and 5,000 controls under simulation model 1, PCA and t-SNE alone did not yield defined clusters (<xref ref-type="fig" rid="fig2">Fig 2A</xref>). <xref ref-type="fig" rid="fig2">Figures 2B-F</xref> show the separation of cases from controls as the DA is trained. One thousand epochs of training via stochastic gradient descent were found to be sufficient for the convergence of reconstruction cost and stabilization of visualizations within simulated data (<xref ref-type="fig" rid="fig2">Figure 2 E and F</xref>, <xref ref-type="fig" rid="figS1">Supplemental Figure 1</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig. 2.</label>
<caption><p>Case vs. Control clustering via principal components analysis and t-distributed stochastic neighbor embedding throughout the training of the DA. <bold>A.)</bold> Raw input <bold>B.)</bold> 0 training epochs <bold>C.)</bold> 10 training epochs <bold>D.)</bold> 100 training epochs <bold>E.)</bold> 1,000 training epochs <bold>F.)</bold> 10,000 training epochs.</p></caption>
<graphic xlink:href="039800_fig2.tif"/>
</fig>
</sec>
<sec id="s3b">
<title>Fully Supervised Comparison</title>
<p>To examine the ability of DAs to learn the structure of the data we compare the predictive ability of classification algorithms applied to the DA constructed through unsupervised training. Random forests demonstrated a strong balance of performance and stability, and were used for all comparisons (<xref ref-type="fig" rid="figS2">Supplemental Figure 2</xref>). We then compare the DA plus a random forest classifier (DA&#x002B;RF) to the top performing classifiers on raw input data (<xref ref-type="table" rid="tbl2">Table 2</xref>).</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><p>Mean Receiver Operating Curve Area Under Curve by method under simulation model 1.</p></caption>
<graphic xlink:href="039800_tbl2.tif"/>
</table-wrap>
<p>Key trends emerged under each model; with few patients SVMs had AUCs indistinguishable from those expected from a random classifier. As one would expect, SVMs were top performers at when the number of patients was high. Random forest classification performance scaled steadily with patient count. The DA&#x002B;RF performed similarly to the random forest, showing that a 2-node DA is able to capture at least one of the input hidden effects. Capturing any signal is sufficient to accurately classify simulation model 1.</p>
</sec>
<sec id="s3c">
<title>Semi-Supervised Comparison</title>
<p>The full potential of the DA&#x002B;RF is reflected in semi-supervised parameter sweep comparison for simulation model 1 (<xref ref-type="fig" rid="fig3">Fig 3A</xref>). The DA method&#x2019;s performance is high, even with very few labeled examples, when the sufficient unlabeled examples are available. Because of the extreme feature reduction, the traditional classifier on top of the DA is able to reach its learning capacity with very few labeled patients (<xref ref-type="fig" rid="fig3">Fig 3A</xref>). Efficient learning from labeled examples is critical in practical use cases because there are often few well-annotated cases due to the expense of clinician manual review. The 2-node DA plus random forest also showed strong performance in relation to an SVM when there were many observed clinical variables (<xref ref-type="fig" rid="fig3">Fig 3B</xref>) and when there were many hidden effects. The SVM again showed the highest performance at very high numbers (1000 or more) of labeled patients.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig. 3.</label>
<caption><p><bold>A)</bold> Classification AUC in relation to the number of labeled patients under simulation model 1 (RF &#x2013; Random Forest, NN &#x2013; Nearest Neighbors, DA &#x2013; 2-node DA &#x002B; Random Forest, SVM &#x2013; Support vector machine). Unsupervised pre-training of the 2-node DA was performed with 10,000 patients. <bold>B)</bold> Heat Map showing the difference of DA&#x002B;RFC and SVM methods in relation to the number of labeled patients and observed variables under simulation model 1.</p></caption>
<graphic xlink:href="039800_fig3.tif"/>
</fig>
<p>These patterns repeat across the other simulation models, with more complex models requiring more hidden nodes to adequately model the structure of the data. In simulation model 2 (<xref ref-type="fig" rid="fig4">Fig 4A</xref>), both 4 and 8 node DAs outperform the 2-node DA. In simulation model 3, the probabilistic manner of simulation means that even a perfect classifier would be unable to classify all samples, so methods were compared on this model using expected maximum predictive accuracy of a perfect classifier. In this model, case control odds were equal to the percentage of hidden input effects on. If there are 4 hidden input effects and 2 are on, the patient has a 50&#x0025; chance of being a case and a 50&#x0025; chance of being a control. A classifier cannot model this uncertainty and the maximum expected accuracy was calculated from a binomial distribution multiplied by the minority percentage as the best a classifier could do is learn the majority class. For example, in the case of 4 hidden effects the maximum expected accuracy is 68.75&#x0025;. Under Model 3, the 4-node DA is the strongest performing, with median performance 5&#x0025; better than the next best traditional classifier. Model 4 (<xref ref-type="fig" rid="fig4">Fig 4C</xref>, <xref ref-type="fig" rid="fig4">4D</xref>) was the most difficult to classify as the classifier had to capture all of the hidden effects to be accurate. In several cases, no classifier did better than the expected performance of a random classifier. In fact, the SVM&#x2019;s average AUC over the entire sweep was indistinguishable from random performance. As expected, the 2-node performs worse than the 4 and 8-node DAs on model 4. The 2-node DA lacks sufficient dimensionality to capture more than 4 hidden input effects.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig. 4.</label>
<caption><p><bold>A)</bold> Classification Accuracy of model 2 (1, 2, 4 and 8 effects). <bold>B)</bold> Classification AUC normalized to simulation model 2 expected max predictive accuracy (1, 2, 4 and 8 effects). <bold>C)</bold> Classification AUC of model 4 (1, 2, 4 and 8 effects). <bold>D.)</bold> Classification AUC of model 4 (parameter sweep results for 1, 2, 4 and 8 effects using only the parameter sets with 2,000 labeled patients)</p></caption>
<graphic xlink:href="039800_fig4.tif"/>
</fig>
</sec>
<sec id="s3d">
<title>Semi-Supervised Missing Data Comparison</title>
<p>Clinical records often have empty fields, so algorithms must be robust to missing data. We evaluated the DA&#x2019;s robustness in this situation. The DA is robust to missing data maintaining near-max classification performance across the missingness proportions tested (<xref ref-type="fig" rid="fig5">Fig5</xref>, Supp. <xref ref-type="fig" rid="figS3">Fig 3</xref>). For these simulation models, the mean imputation used for non-DA approaches is an ideal strategy. <xref ref-type="fig" rid="fig5">Figure 5B</xref> shows consistent performance between the DA and SVM even as the percent of data missing increases, suggesting that the DA is at least as robust as the ideal imputation method.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Fig. 5.</label>
<caption><p><bold>A)</bold> Classification AUC in relation to the amount of missing data under simulation model 1. <bold>B)</bold> Heatmap showing difference of DA and SVM in relation to supervised patient count and percent of missing data.</p></caption>
<graphic xlink:href="039800_fig5.tif"/>
</fig>
</sec>
<sec id="s3e">
<title>Simulated Subtype Clustering Visualization</title>
<p>We evaluated the DAPS&#x2019; ability to perform patient stratification. To perform this analysis, we simulated 5,000 cases from each of two different models (1 and 2) to represent a disease with two subtypes. An additional 5,000 controls were simulated. We then visualized the DA constructed from this set of patients using PCA followed by tSNE. In the input data, the subtypes are relatively overlapping (<xref ref-type="fig" rid="fig6">Fig 6A</xref>). A DA with two nodes was also unable to separate this number of subtypes (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Visualizations constructed from DAs with three (<xref ref-type="fig" rid="fig6">Figure 6C</xref>) or four (<xref ref-type="fig" rid="fig6">Figure 6D</xref>) were able to effectively separate both subtypes of cases from each other and from controls.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Fig. 6.</label>
<caption><p>Case vs. Control clustering via principal components analysis and t-distributed stochastic neighbor embedding after training the DA for controls and cases generated from a combination of models 1 and 3. <bold>A)</bold>. Raw input. <bold>B.)</bold> 1,000 training epochs with 2 hidden nodes. <bold>C.)</bold> 1,000 training epochs with 3 hidden nodes. <bold>D.)</bold> 1,000 training epochs with 4 hidden nodes.</p></caption>
<graphic xlink:href="039800_fig6.tif"/>
</fig>
</sec>
</sec>
<sec id="s4">
<title>DISCUSSION</title>
<p>In this study, we presented a semi-supervised learning approach using DAs to model patients in the EHR. Competitive supervised classification accuracy with a large degree of feature reduction indicates the DA successfully learned the structure of the high-dimensional EHR data. DAs are particularly well suited to the EHR because their unsupervised nature allows the formation of a semi-supervised classifier and the ability to utilize large un-annotated patient populations to improve classification accuracy. The dimensionality reduction of DAs allows clustering of the reduced feature set for the visualization and determination of subtypes. These clusters may reveal disease subtypes, fine-tuned targets for genotype-phenotype association. The DA models are easily deconstructible because they use a simple model for the traditional classifier with transparent node compositions that can be traced back to inputs. In addition, our method proposes a straightforward modification to the DA to enable it to process missing data without imputation.</p>
<p>PheWASs are a powerful tool to leverage the vast clinical data contained in the electronic health record but currently suffer from the reliance on billing codes or manual clinician annotation. Denny et al. [<xref ref-type="bibr" rid="c1">1</xref>] call out the need for increased accuracy in phenotype definition in the original PheWAS publication, particularly for rare phenotypes or phenotypes that do not directly correspond with a billing code. In addition, several studies have found increased genetic linkage via subtyping [<xref ref-type="bibr" rid="c12">12</xref>&#x2013;<xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c37">37</xref>]. Li et al. [<xref ref-type="bibr" rid="c17">17</xref>] presented a powerful example of EHR subtyping of patients with type 2 diabetes using a similar methodology, but they utilized Ayasdi, a commercial, closed source topology data analysis software tool. Our method is built on free, open source libraries that will continue to be improved and our software is accessible for the research community.</p>
<p>DA nodes and clusters of nodes provide composite variables that may better approximate and represent the condition of the subject. These additional phenotype targets may provide more homogeneous targets for genotype associations. Beyond genotype to phenotype association, these visualizations may also help clinicians to understand the level of heterogeneity for a specific disease and to make treatment associations among sub-clusters of patients.</p>
<p>Our work provides an important contribution but challenges remain. Most importantly, the transition from simulated data to real world clinical data requires an additional patient selection step, in order to find suitable patients to perform the unsupervised pre-training step. In addition, we assume a preprocessing step has already been performed to handle the compound structure present in the EHR. This step is necessary to transform categorical, free text, images and temporal data to suitable input for the DA.</p>
<p>Future work will focus on developing tools to examine and interpret constructed phenotypes (hidden nodes) and clusters. We anticipate high weights indicate important contributors to node construction revealing relevant combinations of input features. In addition, we will develop a framework for evaluating the significance of constructed clusters for genotype to phenotype association. Finally we will construct a scheme for determining optimal hyper parameter (i.e. hidden node count) selection.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>This work is supported by the Commonwealth Universal Research Enhancement (CURE) Program grant from the Pennsylvania Department of Health as well as the Gordon and Betty Moore Foundation&#x2019;s Data-Driven Discovery Initiative through Grant GBMF4552 to C.S.G. The authors would like to thank Dr. Jason H Moore for helpful discussions. The authors acknowledge the support of the NVIDIA Corporation for the donation of a TitanX GPU used for this research.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><given-names>J.C.</given-names> <surname>Denny</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Bastarache</surname></string-name>, <string-name><given-names>M.D.</given-names> <surname>Ritchie</surname></string-name>, <string-name><given-names>R.J.</given-names> <surname>Carroll</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Zink</surname></string-name>, <string-name><given-names>J.D.</given-names> <surname>Mosley</surname></string-name>, <etal>et al.</etal>, <article-title>Systematic comparison of phenome-wide association study of electronic medical record data and genome-wide association study data</article-title>, <source>Nat.Biotechnol</source>. <volume>31</volume> (<year>2013</year>) <fpage>1102</fpage>&#x2013;<lpage>1111</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nbt.2749</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><string-name><given-names>J</given-names>. <surname>Tan</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Ung</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Cheng</surname></string-name>, <string-name><given-names>C.S.</given-names> <surname>Greene</surname></string-name>, <article-title>Unsupervised feature construction and knowledge extraction from genome-wide assays of breast cancer with denoising autoencoders</article-title>., <source>Pacific Symp. Biocomput</source>. <volume>20</volume> (<year>2015</year>) <fpage>132</fpage>&#x2013;<lpage>43</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/25592575">http://www.ncbi.nlm.nih.gov/pubmed/25592575</ext-link>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><given-names>S.J.</given-names> <surname>Hebbring</surname></string-name>, <string-name><given-names>S.J.</given-names> <surname>Schrodi</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Ye</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Page</surname></string-name>, <string-name><given-names>M.H.</given-names> <surname>Brilliant</surname></string-name>, <article-title>A PheWAS approach in studying HLA-DRB1&#x002A;1501</article-title>., <source>Genes Immun</source>. <volume>14</volume> (<year>2013</year>) <fpage>187</fpage>&#x2013;<lpage>91</lpage>. doi:<pub-id pub-id-type="doi">10.1038/gene.2013.2</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><string-name><given-names>J.C.</given-names> <surname>Denny</surname></string-name>, <string-name><given-names>D.C.</given-names> <surname>Crawford</surname></string-name>, <string-name><given-names>M.D.</given-names> <surname>Ritchie</surname></string-name>, <string-name><given-names>S.J.</given-names> <surname>Bielinski</surname></string-name>, <string-name><given-names>M.a.</given-names> <surname>Basford</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Bradford</surname></string-name>, <etal>et al.</etal>, <article-title>Variants near FOXE1 are associated with hypothyroidism and other thyroid conditions: Using electronic medical records for genome- and phenome-wide studies</article-title>, <source>Am. J. Hum. Genet</source>. <volume>89</volume> (<year>2011</year>) <fpage>529</fpage>&#x2013;<lpage>542</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.ajhg.2011.09.008</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><string-name><given-names>J.S.</given-names> <surname>Elkins</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Friedman</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Boden-Albala</surname></string-name>, <string-name><given-names>R.L.</given-names> <surname>Sacco</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Hripcsak</surname></string-name>, <article-title>Coding neuroradiology reports for the Northern Manhattan Stroke Study: a comparison of natural language processing and manual review</article-title>., <source>Comput. Biomed. Res</source>. <volume>33</volume> (<year>2000</year>) <fpage>1</fpage>&#x2013;<lpage>10</lpage>. doi:<pub-id pub-id-type="doi">10.1006/cbmr.1999.1535</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><string-name><given-names>A.L.</given-names> <surname>Tyler</surname></string-name>, <string-name><given-names>D.C.</given-names> <surname>Crawford</surname></string-name>, <string-name><given-names>S. a</given-names> <surname>Pendergrass</surname></string-name>, <article-title>The detection and characterization of pleiotropy: discovery, progress, and promise</article-title>., <source>Brief. Bioinform</source>. (<year>2015</year>) <fpage>bbv050</fpage>&#x2013;. doi:<pub-id pub-id-type="doi">10.1093/bib/bbv050</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><string-name><given-names>W.D.</given-names> <surname>Travis</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Brambilla</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Noguchi</surname></string-name>, <string-name><given-names>A.G.</given-names> <surname>Nicholson</surname></string-name>, <string-name><given-names>K.R.</given-names> <surname>Geisinger</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Yatabe</surname></string-name>, <etal>et al.</etal>, <article-title>International association for the study of lung cancer/american thoracic society/european respiratory society international multidisciplinary classification of lung adenocarcinoma</article-title>, <source>J Thorac Oncol</source>. <volume>6</volume> (<year>2011</year>) <fpage>244</fpage>&#x2013;<lpage>285</lpage>. doi:<pub-id pub-id-type="doi">10.1513/pats.201107-042ST</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><string-name><surname>Kreyberg</surname> <given-names>L.</given-names></string-name>, <article-title>Histological lung cancer types. A morphological and biological correlation</article-title>., <source>Acta Pathol Microbiol Scand Suppl</source>. (<year>1962</year>) <volume>157</volume>:<fpage>1</fpage>&#x2013;<lpage>92</lpage>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><given-names>C.F.</given-names> <surname>Mountain</surname></string-name>, <article-title>Revisions in the International System for Staging Lung Cancer</article-title>., <source>Chest</source>. <volume>111</volume> (<year>1997</year>) <fpage>1710</fpage>&#x2013;<lpage>7</lpage>. doi:<pub-id pub-id-type="doi">10.1378/chest.111.6.1710</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><string-name><given-names>L.</given-names> <surname>West</surname></string-name>, <string-name><given-names>S.J.</given-names> <surname>Vidwans</surname></string-name>, <string-name><given-names>N.P.</given-names> <surname>Campbell</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Shrager</surname></string-name>, <string-name><given-names>G.R.</given-names> <surname>Simon</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Bueno</surname></string-name>, <etal>et al.</etal>, <article-title>A novel classification of lung cancer into molecular subtypes</article-title>, <source>PLoS One</source>. <volume>7</volume> (<year>2012</year>) <fpage>1</fpage>&#x2013;<lpage>11</lpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0031906</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><string-name><given-names>A.S.</given-names> <surname>Crystal</surname></string-name>, <string-name><given-names>A.T.</given-names> <surname>Shaw</surname></string-name>, <string-name><given-names>L. V</given-names> <surname>Sequist</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Friboulet</surname></string-name>, <string-name><given-names>M.J.</given-names> <surname>Niederst</surname></string-name>, <string-name><given-names>E.L.</given-names> <surname>Lockerman</surname></string-name>, <etal>et al.</etal>, <article-title>Patient-derived models of acquired resistance can identify effective drug combinations for cancer</article-title>., <source>Science</source>. <volume>346</volume> (<year>2014</year>) <fpage>1480</fpage>&#x2013;<lpage>6</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1254721</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Labbe</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Bureau</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Moreau</surname></string-name>, <string-name><given-names>M.&#x2010;A.</given-names> <surname>Roy</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Chagnon</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Maziade</surname></string-name>, <etal>et al.</etal>, <article-title>Symptom dimensions as alternative phenotypes to address genetic heterogeneity in schizophrenia and bipolar disorder</article-title>, <source>Eur. J. Hum. Genet</source>. <volume>20</volume> (<year>2012</year>) <fpage>1182</fpage>&#x2013;<lpage>1188</lpage>. doi:<pub-id pub-id-type="doi">10.1038/ejhg.2012.67</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><string-name><given-names>P.</given-names> <surname>Chaste</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Klei</surname></string-name>, <string-name><given-names>S.J.</given-names> <surname>Sanders</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Hus</surname></string-name>, <string-name><given-names>M.T.</given-names> <surname>Murtha</surname></string-name>, <string-name><given-names>J.K.</given-names> <surname>Lowe</surname></string-name>, <etal>et al.</etal>, <article-title>A Genome-wide Association Study of Autism Using the Simons Simplex Collection: Does Reducing Phenotypic Heterogeneity in Autism Increase Genetic Homogeneity?</article-title>, <source>Biol. Psychiatry</source>. <volume>77</volume> (<year>2015</year>) <fpage>775</fpage>&#x2013;<lpage>784</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.biopsych.2014.09.017</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><given-names>F.</given-names> <surname>Ariani</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Hayek</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Rondinella</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Artuso</surname></string-name>, <string-name><given-names>M.A.</given-names> <surname>Mencarelli</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Spanhol-Rosseto</surname></string-name>, <etal>et al.</etal>, <article-title>FOXG1 Is Responsible for the Congenital Variant of Rett Syndrome</article-title>, <source>Am. J. Hum. Genet</source>. <volume>83</volume> (<year>2008</year>) <fpage>89</fpage>&#x2013;<lpage>93</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.ajhg.2008.05.015</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Bureau</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Labbe</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Croteau,</surname></string-name>, <string-name><given-names>C.</given-names> <surname>M&#x00E9;rette</surname></string-name>, <article-title>Using disease symptoms to improve detection of linkage under genetic heterogeneity</article-title>, <source>Genet. Epidemiol</source>. <volume>32</volume> (<year>2008</year>) <fpage>476</fpage>&#x2013;<lpage>486</lpage>. doi:<pub-id pub-id-type="doi">10.1002/gepi.20320</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Maziade</surname></string-name>, <string-name><given-names>M.A.</given-names> <surname>Roy</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Martinez</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Cliche</surname></string-name>, <string-name><given-names>J.P.</given-names> <surname>Fournier</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Garneau</surname></string-name>, <etal>et al.</etal>, <article-title>Negative, psychoticism, and disorganized dimensions in patients with familial schizophrenia or bipolar disorder: continuity and discontinuity between the major psychoses</article-title>, <source>Am J Psychiatry</source>. <volume>152</volume> (<year>1995</year>) <fpage>1458</fpage>&#x2013;<lpage>1463</lpage>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><string-name><given-names>L.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Cheng</surname></string-name>, <string-name><given-names>B.S.</given-names> <surname>Glicksberg</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Gottesman</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Tamler</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Chen</surname></string-name>, <etal>et al.</etal>, <source>Identification of type 2 diabetes subgroups through topological analysis of patient similarity</source>, <volume>7</volume> (<year>2015</year>) <fpage>1</fpage>&#x2013;<lpage>16</lpage>. doi:<pub-id pub-id-type="doi">10.1126/scitranslmed.aaa9364</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><string-name><given-names>P.</given-names> <surname>Vincent</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Larochelle</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name>, <string-name><given-names>P.-A.</given-names> <surname>Manzagol</surname></string-name>, <article-title>Extracting and composing robust features with denoising autoencoders</article-title>, <source>Proc. 25th Int. Conf. Mach. Learn. - ICML &#x2019;08</source>. (<year>2008</year>) <fpage>1096</fpage>&#x2013;<lpage>1103</lpage>. doi:<pub-id pub-id-type="doi">10.1145/1390156.1390294</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name>, <article-title>Learning Deep Architectures for AI</article-title>, <source>Found. Trends<sup>&#x00AE;</sup> Mach. Learn</source>. <volume>2</volume> (<year>2009</year>) <fpage>1</fpage>&#x2013;<lpage>127</lpage>. doi:<pub-id pub-id-type="doi">10.1561/2200000006</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Szegedy</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Jia</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Sermanet</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Reed</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Anguelov</surname></string-name>, <etal>et al.</etal>, <article-title>Going Deeper with Convolutions</article-title>, <source>arXiv Prepr. arXiv1409.4842</source>. (<year>2014</year>) <fpage>1</fpage>&#x2013;<lpage>12</lpage>. doi:<pub-id pub-id-type="doi">10.1109/ICCV.2011.6126456</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="website"><string-name><given-names>C.</given-names> <surname>Szegedy</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Vanhoucke</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Ioffe</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Shlens</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Wojna</surname></string-name>, <source>Rethinking the Inception Architecture for Computer Vision</source>, (<year>2015</year>). <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1512.00567">http://arxiv.org/abs/1512.00567</ext-link>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><string-name><given-names>G.</given-names> <surname>Hinton</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Deng</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Yu</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Dahl</surname></string-name>, <string-name><given-names>A.R.</given-names> <surname>Mohamed</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Jaitly</surname></string-name>, <etal>et al.</etal>, <article-title>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</article-title>, <source>IEEE Signal Process. Mag</source>. <volume>29</volume> (<year>2012</year>) <fpage>82</fpage>&#x2013;<lpage>97</lpage>. doi:<pub-id pub-id-type="doi">10.1109/MSP.2012.2205597</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Ducharme</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Vincent</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Janvin</surname></string-name>, <article-title>A Neural Probabilistic Language Model</article-title>, <source>J. Mach. Learn. Res</source>. <volume>3</volume> (<year>2003</year>) <fpage>1137</fpage>&#x2013;<lpage>1155</lpage>. doi:<pub-id pub-id-type="doi">10.1162/153244303322533223</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Graves</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Mohamed</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Hinton</surname></string-name>, <article-title>Speech Recognition With Deep Recurrent Neural Networks</article-title>, <source>Icassp</source>. (<year>2013</year>) <fpage>6645</fpage>&#x2013;<lpage>6649</lpage>. doi:<pub-id pub-id-type="doi">10.1109/ICASSP.2013.6638947</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>O.G.</given-names> <surname>Troyanskaya</surname></string-name>, <article-title>Predicting effects of noncoding variants with deep learning&#x2013;based sequence model</article-title>, <source>Nat. Methods</source>. <volume>12</volume> (<year>2015</year>) <fpage>931</fpage>&#x2013;<lpage>934</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nmeth.3547</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Park</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Kellis</surname></string-name>, <article-title>news and views Deep learning for regulatory genomics</article-title>, <source>Nat. Publ. Gr</source>. <volume>33</volume> (<year>2015</year>) <fpage>825</fpage>&#x2013;<lpage>826</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nbt.3313</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><string-name><given-names>P.</given-names> <surname>Vincent</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Larochelle</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Lajoie</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name>, <string-name><given-names>P.&#x2010;A.</given-names> <surname>Manzagol</surname></string-name>, <article-title>Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</article-title>, <source>J. Mach. Learn. Res</source>. <volume>11</volume> (<year>2010</year>) <fpage>3371</fpage>&#x2013;<lpage>3408</lpage>. doi:<pub-id pub-id-type="doi">10.1111/1467-8535.00290</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><string-name><given-names>B.K.</given-names> <surname>Beaulieu&#x2010;Jones</surname></string-name>, <article-title>Denoising Autoencoders for Phenotype Stratification (DAPS): Preprint Release</article-title>. <source>Zenodo</source>, (<year>2016</year>). doi:<pub-id pub-id-type="doi">10.5281/zenodo.46165</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="website"><string-name><surname>Docker</surname></string-name>, <string-name><surname>Docker</surname></string-name>, (<year>2016</year>). <ext-link ext-link-type="uri" xlink:href="https://www.docker.com/">https://www.docker.com/</ext-link>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="website"><string-name><surname>Shippable</surname></string-name>, <string-name><surname>Shippable</surname></string-name>, (<year>2016</year>). <ext-link ext-link-type="uri" xlink:href="https://www.shippable.com">https://www.shippable.com</ext-link>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="website"><string-name><given-names>F.</given-names> <surname>Bastien</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Lamblin</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Pascanu</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Bergstra</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Goodfellow</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Bergeron</surname></string-name>, <etal>et al.</etal>, <article-title>Theano: new features and speed improvements</article-title>, <source>arXiv Prepr. arXiv &#x2026;</source>. (<year>2012</year>) <fpage>1</fpage>&#x2013;<lpage>10</lpage>. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1211.5590">http://arxiv.org/abs/1211.5590</ext-link>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="website"><string-name><given-names>J.</given-names> <surname>Bergstra</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Breuleux</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Bastien</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Lamblin</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Pascanu</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Desjardins</surname></string-name>, <etal>et al.</etal>, <article-title>Theano: a CPU and GPU math compiler in Python</article-title>, <source>in: 9th Python Sci. Conf</source>., <year>2010</year>: pp. <fpage>1</fpage>&#x2013;<lpage>7</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www-etud.iro.umontreal.ca/&#x223C;wardefar/publications/theano_scipy2010.pdf">http://www-etud.iro.umontreal.ca/&#x223C;wardefar/publications/theano_scipy2010.pdf</ext-link>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><string-name><given-names>F.</given-names> <surname>Pedregosa</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Varoquaux</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Gramfort</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Michel</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Thirion</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Grisel</surname></string-name>, <etal>et al.</etal>, <article-title>Scikitlearn: Machine Learning in Python</article-title>, <source>&#x2026; Mach. Learn. &#x2026;</source>. <volume>12</volume> (<year>2012</year>) <fpage>2825</fpage>&#x2013;<lpage>2830</lpage>. <ext-link ext-link-type="uri" xlink:href="http://dl.acm.org/citation.cfm?id=2078195\nhttp://arxiv.org/abs/1201.0490">http://dl.acm.org/citation.cfm?id=2078195\nhttp://arxiv.org/abs/1201.0490</ext-link>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><string-name><given-names>F.</given-names> <surname>Civeira</surname></string-name>, <article-title>Guidelines for the diagnosis and management of heterozygous familial hypercholesterolemia</article-title>., <source>Atherosclerosis</source>. <volume>173</volume> (<year>2004</year>) <fpage>55</fpage>&#x2013;<lpage>68</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.atherosclerosis.2003.11.010</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><string-name><given-names>L.</given-names> <surname>Van Der Maaten</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Hinton</surname></string-name>, <article-title>Visualizing Data using t-SNE</article-title>, <source>J. Mach. Learn. Res</source>. <volume>9</volume> (<year>2008</year>) <fpage>2579</fpage>&#x2013;<lpage>2605</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s10479-011-0841-3</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="website"><string-name><given-names>G.E.</given-names> <surname>Hinton</surname></string-name>, <string-name><given-names>S.T.</given-names> <surname>Roweis</surname></string-name>, <article-title>Stochastic neighbor embedding</article-title>, <source>Adv. Neural Inf. Process. Syst</source>. (<year>2002</year>) <fpage>833</fpage>&#x2013;<lpage>840</lpage>. doi:<ext-link ext-link-type="uri" xlink:href="http://books.nips.cc/papers/files/nips15/AA45.pdf">http://books.nips.cc/papers/files/nips15/AA45.pdf</ext-link>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Manchia</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Cullis</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Turecki</surname></string-name>, <string-name><given-names>G.A.</given-names> <surname>Rouleau</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Uher</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Alda</surname></string-name>, <article-title>The Impact of Phenotypic and Genetic Heterogeneity on Results of Genome Wide Association Studies of Complex Diseases</article-title>, <source>PLoS One</source>. <volume>8</volume> (<year>2013</year>) <fpage>e76295</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0076295</pub-id>.</mixed-citation></ref>
</ref-list>
<sec id="s5" sec-type="supplementary-material">
<title>SUPPLEMENTARY MATERIALS</title>
<sec id="s5a">
<title>Parameter Sweep Specifications</title>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>Supp. Table 1.</label>
<caption><p>Simulation Model 2 Parameter Sweep Specifications.</p></caption>
<graphic xlink:href="039800_tblS1.tif"/>
</table-wrap>
<table-wrap id="tblS2" orientation="portrait" position="float">
<label>Supp. Table 2.</label>
<caption><p>Simulation Model 3 Parameter Sweep Specifications.</p></caption>
<graphic xlink:href="039800_tblS2.tif"/>
</table-wrap>
<table-wrap id="tblS3" orientation="portrait" position="float">
<label>Supp. Table 3.</label>
<caption><p>Simulation Model 4 Parameter Sweep Specifications.</p></caption>
<graphic xlink:href="039800_tblS3.tif"/>
</table-wrap>
<table-wrap id="tblS4" orientation="portrait" position="float">
<label>Supp. Table 4.</label>
<caption><p>Missing Data, Simulation Model 1 Parameter Sweep Specifications.</p></caption>
<graphic xlink:href="039800_tblS4.tif"/>
</table-wrap>
<fig id="figS1" position="float" fig-type="figure">
<label>Supp. Figure 1.</label>
<caption><p>Denoising Autoencoder Reconstruction Cost vs. Training Epochs</p></caption>
<graphic xlink:href="039800_figS1.tif"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Supp. Figure 2.</label>
<caption><p>ROC AUC comparisons for traditional classifiers across model 1 with DA hidden nodes as inputs.</p></caption>
<graphic xlink:href="039800_figS2.tif"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Supp. Figure 3.</label>
<caption><p><bold>A)</bold> Classification AUC in relation to the amount of missing data under simulation model 3. <bold>B)</bold> Heatmap showing difference of DA and SVM in relation to supervised patient count and percent of missing data.</p></caption>
<graphic xlink:href="039800_figS3.tif"/>
</fig>
</sec>
</sec>
</back>
</article>