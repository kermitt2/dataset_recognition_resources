<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/267211</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Evolutionary Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A Likelihood-Free Inference Framework for Population Genetic Data using Exchangeable Neural Networks</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Chan</surname><given-names>Jeffrey</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Perrone</surname><given-names>Valerio</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Spence</surname><given-names>Jeffrey P.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Jenkins</surname><given-names>Paul A.</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Mathieson</surname><given-names>Sara</given-names></name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Song</surname><given-names>Yun S.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<aff id="a1"><label>1</label><institution>University of California</institution>, Berkeley
</aff>
<aff id="a2"><label>2</label><institution>University of Warwick</institution>
</aff>
<aff id="a3"><label>3</label><institution>Swarthmore College</institution>
</aff>
<aff id="a4"><label>4</label><institution>Chan Zuckerberg Biohub.</institution>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor1">Correspondence to: Yun S. Song &#x003C;<email>yss@berkeley.edu</email>&#x003E;, Sara Mathieson &#x003C;<email>smathieson@cs.swarthmore.edu</email>&#x003E;</corresp>
</author-notes>
<pub-date pub-type="epub"><year>2018</year></pub-date>
<elocation-id>267211</elocation-id>
<history>
<date date-type="received">
<day>16</day>
<month>2</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>16</day>
<month>2</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>18</day>
<month>2</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="267211.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Inference for population genetics models is hindered by computationally intractable likelihoods. While this issue is tackled by likelihood-free methods, these approaches typically rely on handcrafted summary statistics of the data. In complex settings, designing and selecting suitable summary statistics is problematic and results are very sensitive to such choices. In this paper, we learn the first exchangeable feature representation for population genetic data to work directly with genotype data. This is achieved by means of a novel Bayesian likelihood-free inference framework, where a permutation-invariant convolutional neural network learns the inverse functional relationship from the data to the posterior. We leverage access to scientific simulators to learn such likelihood-free function mappings, and establish a general framework for inference in a variety of simulation-based tasks. We demonstrate the power of our method on the recombination hotspot testing problem, outperforming the state-of-the-art.</p>
</abstract>
<counts>
<page-count count="14"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<label>1.</label>
<title>Introduction</title>
<p>Statistical inference in complex population genetics models is challenging, as the likelihood is often both analytically and computationally intractable. These models are usually based on the coalescent (<xref ref-type="bibr" rid="c26">Kingman, 1982</xref>), a stochastic process describing the distribution over genealogies of a random sample of chromosomes from a large population. Unfortunately, standard coalescent-based likelihoods require integrating over a large set of correlated high-dimensional combinatorial objects, rendering classical inferential techniques inapplicable. This limitation can be overcome by likelihood-free methods such as Approximate Bayesian Computation (ABC) (<xref ref-type="bibr" rid="c3">Beaumont et al., 2002</xref>) and deep learning (<xref ref-type="bibr" rid="c36">Sheehan &#x0026; Song, 2016</xref>). These approaches leverage scientific simulators to draw samples from the generative model, and reduce population genetic data to a suite of summary statistics prior to performing inference. However, hand-engineered feature representations typically are not statistically sufficient for the parameter of interest, leading to loss in accuracy. In addition, these statistics are often based on the intuition of the user, need to be modified for each new task, and, in the case of ABC, are not amenable to hyperparameter optimization strategies since the quality of the approximation is unknown.</p>
<p>Deep learning offers the possibility to avoid the need for hand-designed summary statistics in population genetic inference and work directly with genotype data. The goal of this work is to develop a scalable general-purpose inference framework for raw genetic data, without the need for summary statistics. We achieve this by designing a neural network which exploits the exchangeability in the underlying data to learn feature representations that can approximate the posterior accurately.</p>
<p>As a concrete example, we focus on the problem of recombination hotspot testing. Recombination is a biological process of fundamental importance, in which the reciprocal exchange of DNA during cell division creates new combinations of genetic variants. Experiments have shown that some species exhibit <italic>recombination hotspots</italic>, that is, short segments of the genome with high intensity recombination rates (<xref ref-type="bibr" rid="c33">Petes, 2001</xref>). The task of recombination hotspot testing is to predict the location of recombination hotspots given genetic polymorphism data. Accurately localizing recombination hotspots would illuminate the biological mechanism that underlies recombination, and could help geneticists map the alleles causing genetic diseases (<xref ref-type="bibr" rid="c18">Hey, 2004</xref>). We demonstrate in experiments that we achieve state-of-the-art performance on the hotspot detection problem.</p>
<p>Our contributions focus on addressing major inferential challenges of complex population genetic inference. In <xref ref-type="sec" rid="s2">Section 2</xref> we review relevant lines of work in both the fields of machine learning and population genetics. In <xref ref-type="sec" rid="s3">Section 3</xref> we propose a scalable Bayesian likelihood-free inference framework for exchangeable data, which may be broadly applicable to many population genetic problems as well as more general simulator-based machine learning tasks. The application to population genetics is detailed in <xref ref-type="sec" rid="s4">Section 4</xref>. In particular, we show how this allows for direct inference on the raw population genetic data, bypassing the need for <italic>ad hoc</italic> summary statistics. In <xref ref-type="sec" rid="s5">Section 5</xref> we run experiments to validate our method and demonstrate state-of-the-art performance in the hotspot detection problem.</p>
</sec>
<sec id="s2">
<label>2.</label>
<title>Related Work</title>
<p>Likelihood-free methods like ABC have been widely employed in population genetics (<xref ref-type="bibr" rid="c3">Beaumont et al., 2002</xref>; <xref ref-type="bibr" rid="c6">Boitard et al., 2016</xref>; <xref ref-type="bibr" rid="c44">Wegmann et al., 2009</xref>; <xref ref-type="bibr" rid="c40">Sousa et al., 2009</xref>). In ABC the parameter of interest is simulated from its prior distribution, and data are subsequently simulated from the generative model and reduced to a pre-chosen set of summary statistics. These statistics are compared to the summary statistics of the real data, and the simulated parameter is weighted according to the similarity of the statistics to derive an empirical estimate of the posterior distribution. However, choosing summary statistics for ABC is challenging because there is a trade-off between loss of sufficiency and computational tractability. In addition, there is no direct way to evaluate the accuracy of the approximation.</p>
<p>Other likelihood-free approaches have emerged from the machine learning community and have been applied to population genetics, such as support vector machines (SVMs) (<xref ref-type="bibr" rid="c35">Schrider &#x0026; Kern, 2015</xref>; <xref ref-type="bibr" rid="c32">Pavlidis et al., 2010</xref>), single-layer neural networks (<xref ref-type="bibr" rid="c4">Blum &#x0026; Fran&#x00E7;ois, 2010</xref>), and deep learning (<xref ref-type="bibr" rid="c36">Sheehan &#x0026; Song, 2016</xref>). The connection between likelihood-free Bayesian inference and neural networks has also been studied previously by <xref ref-type="bibr" rid="c21">Jiang et al. (2015)</xref> and <xref ref-type="bibr" rid="c31">Papamakarios &#x0026; Murray (2016)</xref>. An attractive property of these methods is that, unlike ABC, they can be applied to multiple datasets without repeating the training process, which is commonly referred to as amortized inference. However, current practice in population genetics collapses the data to a set of summary statistics before passing it through the machine learning models. Therefore, the performance still rests on the ability to laboriously hand-engineer informative statistics, and must be repeated from scratch for each new problem setting.</p>
<p>The inferential accuracy and scalability of these methods can be improved by exploiting symmetries in the input data. Permutation-invariant models have been previously studied in machine learning for SVMs (<xref ref-type="bibr" rid="c37">Shivaswamy &#x0026; Jebara, 2006</xref>) and, recently, gained a surge of interest in the deep learning literature. Recent work on designing architectures for exchangeable data include <xref ref-type="bibr" rid="c34">Ravanbakhsh et al. (2016)</xref>, <xref ref-type="bibr" rid="c16">Guttenberg et al. (2016)</xref>, and <xref ref-type="bibr" rid="c45">Zaheer et al. (2017)</xref>, which exploit parameter sharing to encode invariances. To our knowledge, no prior work has been done on learning feature representations for exchangeable population genetic data.</p>
<p>We demonstrate these ideas on the problem of recombination hotspot testing. To this end, several methods have been developed (<xref ref-type="bibr" rid="c8">Fearnhead, 2006</xref>; <xref ref-type="bibr" rid="c29">Li et al., 2006</xref>; <xref ref-type="bibr" rid="c43">Wang &#x0026; Rannala, 2009</xref>). However, none of these are scalable to the whole genome, with the exception of <monospace>LDhot</monospace> (<xref ref-type="bibr" rid="c2">Auton et al., 2014</xref>; <xref ref-type="bibr" rid="c42">Wall &#x0026; Stevison, 2016</xref>), so we limit our comparison to this latter method. <monospace>LDhot</monospace> relies on a composite likelihood, which can be seen as an approximate likelihood for summaries of the data. It can be computed only for a restricted set of models (i.e., an unstructured population with piecewise constant population size), is unable to capture dependencies beyond those summaries, and scales at least cubically with the number of DNA sequences. The method we propose in this paper scales linearly in the number of sequences while using raw genetic data directly.</p>
</sec>
<sec id="s3">
<label>3.</label>
<title>Methodology</title>
<p>In this section we propose a flexible framework to address the shortcomings of current likelihood-free methods. Although motivated by population genetics, we first lay out the ideas that generalize beyond this application. We describe the exchangeable representation in <xref ref-type="sec" rid="s3a">Section 3.1</xref> and the training algorithm in <xref ref-type="sec" rid="s3b">Section 3.2</xref>, which are combined into a general likelihood-free inference framework in <xref ref-type="sec" rid="s3c">Section 3.3</xref>. The statistical properties of the method are studied in <xref ref-type="sec" rid="s3d">Section 3.4</xref>.</p>
<sec id="s3a">
<label>3.1.</label>
<title>Feature Representation for Exchangeable Data</title>
<p>Population genetic datapoints x<sup>(<italic>i</italic>)</sup> typically take the form of a binary matrix, where rows correspond to individuals and columns indicate the presence of a Single Nucleotide Polymorphism (SNP), namely a nucleotide variation at a given location of the DNA. For unstructured populations the order of individuals carries no information, hence the rows are exchangeable. More generally, given data <bold>X</bold> &#x003D; (x<sup>(1)</sup>,&#x2026;,x<sup>(<italic>N</italic>)</sup>) where x<sup>(<italic>i</italic>)</sup> &#x2208; &#x211D;<sup><italic>n</italic>&#x00D7;<italic>d</italic></sup> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline1.gif"/></alternatives></inline-formula>, we call <bold>X</bold> <italic>exchangeably-structured</italic> if, for every <italic>i</italic>,
<disp-formula id="ueqn1"><alternatives><graphic xlink:href="267211_ueqn1.gif"/></alternatives>
</disp-formula>,
for all permutations <italic>&#x03C3;</italic> of the indices {1,&#x2026;, <italic>n</italic>}.</p>
<p>To obtain an exchangeable feature representation of genotype data, we proceed as follows. Let <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline2.gif"/></alternatives></inline-formula> be a feature mapping. We apply a symmetric function <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline3.gif"/></alternatives></inline-formula> to the feature mapped datapoint to obtain <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline4.gif"/></alternatives></inline-formula>, a feature representation of the exchangeably-structured data. This representation is very general and can be adapted to various machine learning settings. For example, &#x03A6; could be some <italic>a priori</italic> fixed feature mapping (e.g. a kernel or summary statistics) in which case <italic>g</italic> should be chosen such that the resulting feature representation remains informative. More commonly, the mapping &#x03A6; needs to be learned (such as in kernel logistic regression or a deep neural network), hence we choose some fixed <italic>g</italic> such that subgradients can be backpropagated through <italic>g</italic> to &#x03A6;. Some examples of such a function <italic>g</italic> include the element-wise sum, element-wise max, lexicographical sort, or higher-order moments. Throughout the paper, we choose to parameterize &#x03A6; with a neural network and choose <italic>g</italic> to be the element-wise max function, such that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline5.gif"/></alternatives></inline-formula>. A variant of this representation is proposed by <xref ref-type="bibr" rid="c34">Ravanbakhsh et al. (2016)</xref> and <xref ref-type="bibr" rid="c45">Zaheer et al. (2017)</xref>.</p>
<p>This embedding of exchangeably-structured data into a vector space is suitable for many tasks such as regression or clustering. We focus on inference in which the objective is to learn the function <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline6.gif"/></alternatives></inline-formula>, where &#x0398; is the space of all parameters <italic>&#x03B8;</italic> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline7.gif"/></alternatives></inline-formula> is the space of all probability distributions on &#x0398;. Endowed with our exchangeable feature representation, a function <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline8.gif"/></alternatives></inline-formula> can be composed with our symmetric mapping to get <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline9.gif"/></alternatives></inline-formula>. For simplicity, throughout the rest of the paper we focus on binary classification where <italic>&#x03B8;</italic> &#x2208; {0,1}, so that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline10.gif"/></alternatives></inline-formula> can be parameterized by &#x2119;(&#x03B8; &#x003D; 1 &#x007C; x<sup>(i)</sup>, <italic>&#x03D5;</italic>), where <italic>&#x03D5;</italic> are nuisance parameters and <italic>h</italic> is parameterized as a neural network such that both <italic>h</italic> and &#x03A6; can be learned via backpropagation with a cross entropy loss. Specifically, we will apply this construction to infer the presence of recombination hotspots, indicated by the parameter <italic>&#x03B8;</italic>. The posterior &#x2119;(<italic>&#x03B8;</italic> &#x003D;1 &#x007C; x<sup>(i)</sup>, <italic>&#x03D5;</italic>) is estimated by a soft max application so that the output is defined on [0,1]. This exchangeable representation has many advantages. While it could be argued that flexible machine learning models could learn the structured exchangeability of the data, encoding exchangeability explicitly allows for faster per-iteration computation and improved learning efficiency, since data augmentation for exchangeability scales as <italic>O</italic>(<italic>n</italic>!). Enforcing exchangeability implicitly reduces the size of the input space from &#x211D;<sup><italic>n</italic>&#x00D7;<italic>d</italic></sup> to the quotient space &#x211D;<sup><italic>n</italic>&#x00D7;<italic>d</italic></sup>/<italic>S<sub>n</sub></italic>, where <italic>S<sub>n</sub></italic> is the symmetric group on <italic>n</italic> elements. A factorial reduction in input size leads to much more tractable inference for large <italic>n</italic>. In addition, choices of <italic>g</italic> where <italic>d</italic><sub>2</sub> is independent of <italic>n</italic> (e.g., element-wise operations with output dimension independent of <italic>n</italic>) allows for a representation which is robust to differing number of exchangeable variables between train and test time. This property is particularly desirable to construct feature representations of fixed dimension even with missing data.</p>
</sec>
<sec id="s3b">
<label>3.2.</label>
<title>Simulation-on-the-fly</title>
<p>In statistical decision theory, the Bayes risk for prior <italic>&#x03C0;</italic>(<italic>&#x03B8;</italic>) is defined as <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline11.gif"/></alternatives></inline-formula>, with <italic>l</italic> being the loss function and <italic>T</italic> an estimator. The excess risk over the Bayes risk resulting from an algorithm <italic>A</italic> with model class <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline12.gif"/></alternatives></inline-formula> can be decomposed as
<disp-formula id="ueqn2"><alternatives><graphic xlink:href="267211_ueqn2.gif"/></alternatives>
</disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline13.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline14.gif"/></alternatives></inline-formula> are the function obtained via algorithm A and the empirical risk minimizer, respectively. The terms on the right hand side are referred to as the optimization, estimation, and approximation errors, respectively. Often the goal of statistical decision theory is to minimize the excess risk, motivating algorithmic choices to control the three sources of error. For example, with supervised learning, overfitting is a result of large estimation error. Typically, for a sufficiently expressive neural network optimized via stochastic optimization techniques, the excess risk is dominated by optimization and estimation errors.</p>
<p>When we have access to scientific simulators, the amount of training data available is limited only by the amount of computational time available for simulation, so we propose simulating each training datapoint afresh such that there is exactly one epoch over the training data. We refer to this as <italic>simulation-on-the-fly</italic>. A similar setting is commonly used in the reinforcement learning literature, a key to recent success of deep reinforcement learning in applications such as games (<xref ref-type="bibr" rid="c38">Silver et al., 2016</xref>; <xref ref-type="bibr" rid="c39">2017</xref>), though it is rarely utilized in supervised learning since access to simulators is usually unavailable. In this setting, the algorithm is run for many iterations until the estimation error is sufficiently small, eliminating the pitfalls of overfitting. With fixed training data, additional iterations after the first epoch are not guaranteed to further minimize the estimation error. Furthermore, simulation-on-the-fly guarantees <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline15.gif"/></alternatives></inline-formula> <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline16.gif"/></alternatives></inline-formula>, and given that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline17.gif"/></alternatives></inline-formula> by the Universal Approximation Theorem (<xref ref-type="bibr" rid="c7">Cybenko, 1989</xref>), we can conclude that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline18.gif"/></alternatives></inline-formula>. The risk surface is much smoother than that for fixed training sets (shown empirically in <xref ref-type="sec" rid="s5">Section 5</xref>). This reduces the number of poor local minima and, consequently, the optimization error.</p>
<p>An alternative viewpoint of the simulation-on-the-fly paradigm from the lens of stochastic optimization is to compare the gradients of the two training procedures when <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline19.gif"/></alternatives></inline-formula> is restricted to first-order stochastic approximation algorithms. In order to make explicit the optimization algorithm at hand, we parameterize the model class <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline20.gif"/></alternatives></inline-formula> by <italic>w</italic> &#x2208; <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline21.gif"/></alternatives></inline-formula> such that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline22.gif"/></alternatives></inline-formula> if and only if <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline23.gif"/></alternatives></inline-formula> where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline24.gif"/></alternatives></inline-formula> is the space of all possible neural network weights for a fixed architecture. Denote the empirical risk with respect to the prior <italic>&#x03C0;</italic>(<italic>&#x03B8;</italic>) as <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline25.gif"/></alternatives></inline-formula>. In the simulation-on-the-fly regime, the <italic>t</italic>-th iteration approximates the gradient of the population risk <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline26.gif"/></alternatives></inline-formula> at <italic>w<sub>t</sub></italic> by the unbiased random vector
<disp-formula id="eqn1"><alternatives><graphic xlink:href="267211_eqn1.gif"/></alternatives>
</disp-formula>
where <italic>&#x03BE;<sub>t</sub></italic> is a random vector such that &#x1D53C;(<italic>&#x03BE;<sub>t</sub></italic>) &#x003D; 0 and &#x1D53C;(<italic>&#x03BE;<sub>t</sub></italic> | <italic>g</italic><sub>sim</sub>(<italic>w</italic><sub>1</sub>),&#x2026;, <italic>g</italic><sub>sim</sub>(<italic>w</italic><sub><italic>t</italic>&#x2212;1</sub>)) &#x003D; 0. On the other hand, for the fixed training set regime, the <italic>t</italic>-th iteration of the algorithm approximates the empirical risk gradient <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline27.gif"/></alternatives></inline-formula> at <italic>w<sub>t</sub></italic> by the unbiased random vector
<disp-formula id="eqn2"><alternatives><graphic xlink:href="267211_eqn2.gif"/></alternatives>
</disp-formula>
where once again &#x1D53C;(&#x03BE;<sub>t</sub>) &#x003D; 0 and &#x1D53C;(&#x03BE;<sub>t</sub> | <italic>g</italic><sub>fixed</sub>(<italic>w</italic><sub>1</sub>),&#x2026;, <italic>g</italic><sub>fixed</sub>(<italic>w</italic><sub><italic>t</italic>&#x2212;1</sub>)) &#x003D; 0. A key point is that while <italic>g</italic><sub>fixed</sub>(<italic>w<sub>t</sub></italic>) is unbiased with respect to <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline28.gif"/></alternatives></inline-formula>, it is biased with respect to <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline29.gif"/></alternatives></inline-formula>. Using the
formulation in <xref ref-type="disp-formula" rid="eqn2">(2)</xref>, the fixed training data setting performs stochastic optimization on the empirical risk <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline30.gif"/></alternatives></inline-formula> and converges to the empirical Bayes risk <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline31.gif"/></alternatives></inline-formula> for decaying learning rate and suitably expressive <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline32.gif"/></alternatives></inline-formula>. On the other hand, simulation-on-the-fly in <xref ref-type="disp-formula" rid="eqn1">(1)</xref> performs stochastic optimization directly on the population Bayes risk <italic>R<sub>&#x03C0;</sub></italic>, circumventing the bias incurred from using the empirical Bayes risk as a proxy for the population Bayes risk.</p>
</sec>
<sec id="s3c">
<label>3.3.</label>
<title>Likelihood-Free Inference Framework</title>
<p>With an exchangeable feature representation and an optimization procedure in hand, we can now combine these ingredients into an inference scheme. Let x, <italic>&#x03B8;</italic>, <italic>&#x03D5;</italic>, and <italic>&#x03B3;</italic> be the observed data, the latent parameter of interest, the nuisance parameters, and the prior hyperparameters, respectively. The latent parameter <italic>&#x03B8;</italic> can be inferred by drawing samples from the prior distribution <italic>&#x03B8;</italic><sup>(<italic>i</italic>)</sup>, <italic>&#x03D5;</italic><sup>(<italic>i</italic>)</sup> &#x007E; <italic>&#x03C0;</italic>(<italic>&#x03B8;</italic>, <italic>&#x03D5;</italic> | <italic>&#x03B3;</italic>) and from the density x<sup>(<italic>i</italic>)</sup> &#x007E; <italic>P</italic>(x | <italic>&#x03B8;</italic><sup>(<italic>i</italic>)</sup>, <italic>&#x03B3;</italic>, <italic>&#x03D5;</italic><sup>(<italic>i</italic>)</sup>), while stochastic optimization under the simulation-on-the-fly paradigm fits <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline33.gif"/></alternatives></inline-formula> to <italic>&#x03B8;</italic><sup>(<italic>i</italic>)</sup> in an online manner.</p>
<p>This Bayesian inference framework marginalizes over the uncertainty of the nuisance parameters. As neural networks have been empirically shown to interpolate well between examples, we recommend choosing a diffuse prior, which makes our trained model robust to model misspecification.</p>
<p>Another question about utilizing machine learning models for Bayesian inference is the calibration of the posteriors, since neural networks have been empirically shown to be overconfident in their predictions. <xref ref-type="bibr" rid="c15">Guo et al. (2017)</xref> showed that common deep learning practices cause neural networks to poorly represent aleatoric uncertainty, namely the uncertainty due to the noise inherent in the observations. These calibration issues are a byproduct of the fixed training set regime but do not apply to simulation-on-the-fly. The soft-max probabilities are calibrated for a correctly specified model under simulation-on-the-fly, since for a sufficiently expressive neural network the minimizer approximates the true posterior. However, under large model misspecification, softmax probabilities should not directly be used as posteriors since they do not properly quantify epistemic uncertainty (uncertainty in the model) as they may overconfidently classify outliers dissimilar to the training set. For recombination hotspot testing, we found that the summary statistics from the 1000 Genomes dataset (<xref ref-type="bibr" rid="c1">1000 Genomes Project Consortium, 2015</xref>) were similar to the summary statistics of the simulated data, so for simplicity we use the softmax probabilities as the posterior.</p>
</sec>
<sec id="s3d">
<label>3.4.</label>
<title>Statistical Properties</title>
<p>Our deep learning method exhibits similar asymptotic properties to those of ABC, with additional guarantees for nonobserved values of x.</p>
<p>In the simulation-on-the-fly setting, convergence to a global minimum implies that a sufficiently large neural network architecture represents the true posterior within <italic>&#x03F5;</italic>-error in the following sense: for any fixed error <italic>&#x03F5;</italic>, there exist <italic>H</italic><sub>0</sub> and <italic>N</italic><sub>0</sub> such that the trained neural network produces a posterior which satisfies
<disp-formula id="eqn3"><alternatives><graphic xlink:href="267211_eqn3.gif"/></alternatives>
</disp-formula>
for all <italic>H</italic> &#x003E; <italic>H</italic><sub>0</sub> and <italic>N</italic> &#x003E; <italic>N</italic><sub>0</sub>, where <italic>H</italic> is the minimum number of hidden units across all neural network layers, w the weights parameterizing the network, and <italic>KL</italic> the Kullback-Leibler divergence between the population risk and the neural network. Then the following proposition holds.</p>
<statement>
<label>Proposition 1.</label>
<p><italic>For all x, H &#x003E; H<sub>0</sub>, and N &#x003E; N<sub>0</sub> and for any fixed error &#x03B4; &#x003E;</italic> 0,
<disp-formula id="eqn4"><alternatives><graphic xlink:href="267211_eqn4.gif"/></alternatives>
</disp-formula>
<italic>with probability at least</italic> <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline34.gif"/></alternatives></inline-formula>, <italic>where w&#x002A; is the minimizer of</italic> <xref ref-type="disp-formula" rid="eqn3">(3)</xref>.</p>
<p>We can get stronger guarantees in the discrete setting common to population genetic data.</p>
</statement>
<statement>
<label>Corollary 1.</label>
<p><italic>Under the same conditions, if x is discrete and</italic> &#x2119;(x) &#x003E; 0 <italic>for all x, the KL divergence appearing in</italic> <xref ref-type="disp-formula" rid="eqn4">(4)</xref> <italic>converges to 0 as H,N &#x2192; &#x221E; uniformly for all</italic> x.</p>
<p>The proofs are given in the Appendix. Note that it is computationally infeasible to train a neural network such that <italic>H</italic> &#x2192; &#x221E;. Instead, we restrict the number of units to some fixed constant <italic>H</italic> inducing a model class over learnable functions <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline35.gif"/></alternatives></inline-formula>. Our training procedure in the asymptotic regime for fixed <italic>H</italic> minimizes the objective function in <xref ref-type="disp-formula" rid="eqn3">(3)</xref> as <italic>N</italic> &#x2192; &#x221E;. Similarly, under the finite sample regime, the training procedure directly minimizes the projected population risk for the restricted model class. An important property of neural networks with a finite number of hidden units is that this restricted model class is quite large and has been empirically shown to approximate many functions well, so finite <italic>H</italic> only introduces minimal error. Furthermore, deep learning has been empirically shown to converge in only a few thousand iterations for many real-world highdimensional datasets (<xref ref-type="bibr" rid="c46">Zhang et al., 2016</xref>), hence <italic>N</italic> need not approach infinity to obtain a good approximation of the posterior. We later confirm this finding experimentally. Hyperparameter optimization in neural networks can be performed by comparing the relative loss of the neural network under a variety of optimization and architecture settings. On the other hand, ABC has no such theoretical or empirical results in the finite sample regime outside of toy examples in which the likelihood can be approximated, and it has been shown empirically that the iteration complexity scales exponentially in the dimension of the summary statistics due to the curse of dimensionality.</p>
<p>We now show that our neural network learns statistics which are asymptotically sufficient. While several variants of sufficiency in the Bayesian context have been defined in the literature (<xref ref-type="bibr" rid="c27">Kolmogoroff, 1942</xref>; <xref ref-type="bibr" rid="c11">Furma&#x0144;czyki &#x0026; Niemiro, 1998</xref>) we focus on the following.</p>
</statement>
<statement>
<label>Definition 1.</label>
<p>A statistic <italic>T</italic>(<italic>x</italic>) is called <italic>prior-dependent Bayes sufficient</italic> if for a parameter <italic>&#x03B8;</italic> and fixed prior <italic>&#x03C0;</italic>(<italic>&#x03B8;</italic>), the posterior satisfies, for all <italic>&#x03B8;</italic> and <italic>x</italic>,
<disp-formula id="ueqn3"><alternatives><graphic xlink:href="267211_ueqn3.gif"/></alternatives>
</disp-formula>.</p></statement>
<statement>
<label>Proposition 2.</label>
<p><italic>Each layer of the neural network trained via the likelihood-free framework is prior-dependent Bayes sufficient with respect to &#x03C0;(&#x03B8;) as H &#x2192; &#x221E; and assuming the optimization for each H converges to the global minimum.</italic></p>
<p>This is proved in the Appendix. The sufficiency of the exchangeable feature representation ensures that no inferential accuracy has been sacrificed while reducing the data to an exchangeable feature representation. Each layer of the neural network is sufficient, allowing this representation to be used for other tasks. While this notion of sufficiency does not cover a finite architecture, it allows us to compare against the asymptotic results of ABC. More details on the properties of ABC are given in the Appendix.</p>
<p>Another desirable property is having unbiased uncertainty estimates, namely posterior calibration. <xref ref-type="bibr" rid="c9">Fearnhead &#x0026; Prangle (2012)</xref> note that ABC is asymptotically calibrated as its kernel bandwidth goes to 0, but not calibrated in general. Similarly, our deep learning procedure is calibrated as the number of hidden units <italic>H</italic> &#x2192; &#x221E;. While neural networks are difficult to analyze in fixed architecture settings with nonconvex loss surfaces, we empirically find that our neural network is calibrated in <xref ref-type="sec" rid="s5">Section 5</xref>.</p>
</statement>
</sec>
</sec>
<sec id="s4">
<label>4.</label>
<title>Population Genetics Application</title>
<p>The framework we established overcomes many challenges posed by population genetic inference. In this setting, each observation x is encoded as follows. Let x<sub><italic>S</italic></sub> be the binary <italic>n</italic> &#x00D7; <italic>d</italic> allele matrix with 0 and 1 as the major and minor alleles respectively, where <italic>n</italic> is the number of individuals and <italic>d</italic> is the number of SNPs. Let x<sub><italic>D</italic></sub> be the <italic>n</italic> &#x00D7; <italic>d</italic> matrix storing the distances between neighboring SNPs, so each row of x<sub><italic>D</italic></sub> is identical and the rightmost distance is set to 0. Define x as the <italic>n</italic> &#x00D7; <italic>d</italic> &#x00D7; <italic>2</italic> tensor obtained by stacking x<sub><italic>S</italic></sub> and x<sub><italic>D</italic></sub>. To improve the conditioning of the optimization problem, the distances are normalized such that they are on the order of [0,1]. As mentioned in <xref ref-type="sec" rid="s3a">Section 3.1</xref>, this is an instance of exchangeably-structure data.</p>
<p>The standard generative model for such data is the coales-cent, a stochastic process describing the distribution over genealogies relating samples from a population of individuals. The coalescent with recombination (<xref ref-type="bibr" rid="c14">Griffiths, 1981</xref>; <xref ref-type="bibr" rid="c19">Hudson, 1983</xref>) extends this model to describe the joint distribution of genealogies along the chromosome. The recombination rate between two DNA locations tunes the correlation between their corresponding genealogies. Population genetic data derived from the coalescent obeys translation invariance along a sequence conditioned on local recombination and mutation rates also obeying translation invariance. In order to take full advantage of parameter sharing, our chosen architecture is given by a convolutional neural network with tied weights for each row preceding the exchangeable layer, which is in turn followed by a fully connected neural network. We choose <italic>g</italic> as the element-wise max, and the architecture is depicted in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p>
<sec id="s4a">
<label>4.1.</label>
<title>Recombination Hotspot Testing</title>
<p>Recombination hotspots are short regions of the genome (&#x2248; 2 kb in humans) with high recombination rate relative to the background recombination rate. To apply our framework to the hotspot detection problem, we define the overall graphical model in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Denote <italic>w</italic> as a small window (typically &#x003C; 25 kb) of the genome such that <italic>X<sub>w</sub></italic> is the population genetic data in that window, and <italic>X</italic><sub>&#x2212;<italic>w</italic></sub> is the rest. Similarly, let <italic>&#x03C1;<sub>w</sub></italic> and <italic>&#x03C1;</italic><sub>&#x2212;<italic>w</italic></sub> be the recombination map in the window and outside of the window, respectively. Let <italic>q</italic> be the the relative proportion of the sample possessing each mutation, <italic>&#x03B7;</italic> the population size function 0 the mutation rate, and h the indicator function for whether the window defines a hotspot. While <italic>&#x03C1;<sub>w</sub></italic> and <italic>&#x03C1;</italic><sub>&#x2212;<italic>w</italic></sub> have a weak dependence (dashed line) on <italic>X<sub>&#x2212;w</sub></italic> and <italic>X<sub>w</sub></italic> respectively, this dependence decreases rapidly and is ignored for simplicity. Similarly, conditioned on <italic>q</italic>, <italic>&#x03B7;</italic> is only weakly dependent on <italic>X<sub>w</sub></italic>. The shaded nodes represent the observed variables.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p>A cartoon schematic of the exchangeable architecture for population genetics.</p></caption>
<graphic xlink:href="267211_fig1.tif"/>
</fig>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Graphical model of recombination hotspot inference: <italic>&#x03B8;</italic> is the mutation rate, <italic>&#x03B7;</italic> the population size function, <italic>q</italic> the relative proportion of the sample possessing each mutation, <italic>&#x03C1;</italic><sub>&#x2212;<italic>w</italic></sub> the recombination rate function outside of the window, <italic>&#x03C1;</italic><sub><italic>w</italic></sub> the recombination rate function inside the window, <italic>h</italic> whether the window is a hotspot, <italic>X</italic><sub>&#x2212;<italic>w</italic></sub> the population genetic data outside of the window, and <italic>X</italic><sub><italic>w</italic></sub> the data inside the window. The dashed line signifies that, conditioned on <italic>q</italic>, <italic>&#x03B7;</italic> is weakly dependent on <italic>X</italic><sub><italic>w</italic></sub> for suitably small <italic>w</italic>, and <italic>&#x03C1;</italic><sub>&#x2212;<italic>w</italic></sub> and <italic>&#x03C1;</italic><sub><italic>w</italic></sub> are only weakly dependent on <italic>X</italic><sub><italic>w</italic></sub> and <italic>X</italic><sub>&#x2212;<italic>w</italic></sub>.</p></caption>
<graphic xlink:href="267211_fig2.tif"/>
</fig>
<p>We define our prior as follows. We sample the hotspot indicator variable <italic>h</italic> &#x007E; Bernoulli(0.5) and the local recombination maps <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline36.gif"/></alternatives></inline-formula> from the released fine-scale recombination maps of HapMap (<xref ref-type="bibr" rid="c13">Gibbs et al., 2003</xref>). In addition, the demography is inferred via <monospace>SMC</monospace>&#x002B;&#x002B; (<xref ref-type="bibr" rid="c41">Terhorst et al., 2017</xref>) and fixed in an empirical Bayes style throughout training for simplicity. The human mutation rate is fixed to that experimentally found in <xref ref-type="bibr" rid="c28">Kong et al. (2012)</xref>. Since <monospace>SMC</monospace>&#x002B;&#x002B; is robust to changes in any small fixed window, inferring <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline37.gif"/></alternatives></inline-formula> from <italic>X</italic> has minimal dependence on <italic>&#x03C1;</italic><sub><italic>w</italic></sub>.</p>
<p>To test for recombination hotspots, first simulate a batch of <italic>h</italic> and <italic>&#x03C1;</italic><sub><italic>w</italic></sub>. from the prior, and <italic>X</italic><sub><italic>w</italic></sub> from <monospace>msprime</monospace> (<xref ref-type="bibr" rid="c23">Kelleher et al., 2016</xref>). Then, feed a batch of training examples into the network. Repeat until convergence or for a fixed number of iterations. At test time, slide along the genome to infer posteriors over <italic>h</italic>.</p>
</sec>
<sec id="s5">
<label>5.</label>
<title>Experiments</title>
<p>In this section, we study the accuracy of our framework to test for recombination hotspots. As very few hotspots have been experimentally validated, we primarily evaluate our method on simulated data, with parameters set to match a human-like setting. The presence of ground truth allows us to benchmark our method and compare against <monospace>LDhot</monospace>. Unless otherwise specified, for all experiments we use the mutation rate, <italic>&#x03BC;</italic> &#x003D; 1.1 &#x00D7; 10<sup>&#x2212;8</sup> per generation per nucleotide, convolution patch length of 5 SNPs, 32 and 64 convolution filters for the first two convolution layers, 128 hidden units for both fully connected layers, and 20-SNP length windows. The experiments comparing against <monospace>LDhot</monospace> used sample size <italic>n</italic> &#x003D; 64 to construct lookup tables for <monospace>LDhot</monospace> quickly. All other experiments use <italic>n</italic> &#x003D; 198, matching the size of the CEU population (i.e., Utah Residents with Northern and Western European ancestry) in the 1000 Genomes dataset. All simulations were performed using <monospace>msprime</monospace> (<xref ref-type="bibr" rid="c23">Kelleher et al., 2016</xref>). Gradient updates were performed using Adam (<xref ref-type="bibr" rid="c24">Kingma &#x0026; Ba, 2014</xref>) with learning rate 1 &#x00D7; 10<sup>&#x2212;3</sup> &#x00D7; 0.9<sup><italic>b</italic>/10000</sup>, <italic>b</italic> being the batch count.</p>
<sec id="s5a">
<label>5.1.</label>
<title>Evaluation of Exchangeable Representation</title>
<p>We compare the behavior of an explicitly exchangeable architecture to a nonexchangeable architecture that takes 2D convolutions with varying patch heights. The accuracy under human-like population genetic parameters with varying 2D patch heights is shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>. Since each training point is simulated on-the-fly, data augmentation is performed implicitly in the nonexchangeable version without having to explicitly permute the rows of each training point. As expected, directly encoding the permutation invariance leads to more efficient training and higher accuracy while also benefiting from a faster per-batch computation time. Furthermore, the slight accuracy decrease when increasing the patch height confirms the difficulty of learning permutation invariance as <italic>n</italic> grows. Another advantage of exchangeable architectures is the robustness to the number of individuals at test time. As shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>, the accuracy remains robust during test time for sample sizes roughly 0.5&#x2013;4&#x00D7; the train sample size.</p>
</sec>
<sec id="s5b">
<label>5.2.</label>
<title>Evaluation of Simulation-on-the-fly</title>
<p>Next, we analyze the effect of simulation-on-the-fly in comparison to the standard fixed training set. A fixed training set size of 10000 was used and run for 20000 training batches and a test set of size 5000. For a network using simulation-on-the-fly, 20000 training batches were run and evaluated on the same test set. The weights were initialized with a fixed random seed in both settings with 20 replicates. <xref ref-type="fig" rid="fig5">Figure 5</xref> shows that the fixed training set setting has both a higher bias and higher variance than simulation-on-the-fly. The bias can be attributed to the estimation error of a fixed training set in which the empirical risk surface is not a good approximation of the population risk surface. The variance can be attributed to an increase in the number of poor quality local optima in the fixed training set case.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Accuracy comparison between exchangeable vs nonexchangeable architectures.</p></caption>
<graphic xlink:href="267211_fig3.tif"/>
</fig>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Performance of changing the number of individuals at test time for varying training sample sizes.</p></caption>
<graphic xlink:href="267211_fig4.tif"/>
</fig>
<p>We next investigated posterior calibration. This gives us a measure for whether there is any bias in the uncertainty estimates output by the neural network. We evaluated the calibration of simulation-on-the-fly against using a fixed training set of 10000 datapoints. The calibration curves were generated by evaluating 25000 datapoints at test time and binning their posteriors, computing the fraction of true labels for each bin. A perfectly calibrated curve is the dashed black line shown in <xref ref-type="fig" rid="fig6">Figure 6</xref>. In accordance with the theory in <xref ref-type="sec" rid="s3b">Section 3.2</xref>, the simulation-on-the-fly is much better calibrated with an increasing number of training examples leading to a more well calibrated function. On the other hand, the fixed training procedure is poorly calibrated.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Comparison between the test cross entropy of a fixed training set of size 10000 and simulation-on-the-fly.</p></caption>
<graphic xlink:href="267211_fig5.tif"/>
</fig>
</sec>
<sec id="s5c">
<label>5.3.</label>
<title>Comparison to <monospace>LDhot</monospace></title>
<p>We compared our method against <monospace>LDhot</monospace> in two settings: (i) sampling empirical recombination rates from the HapMap recombination map for CEU and YRI (i.e., Yoruba in Ibadan, Nigera) (<xref ref-type="bibr" rid="c13">Gibbs et al., 2003</xref>) to set the background recombination rate, and then using this background to simulate a flat recombination map with 10 &#x2013; 100&#x00D7; relative hotspot intensity, and (ii) sampling segments of the HapMap recombination map for CEU and YRI and classifying them as hotspot according to our definition, then simulating from the drawn variable map.</p>
<p>The ROC curves for both settings are shown in <xref ref-type="fig" rid="fig7">Figure 7</xref>. Under the bivariate empirical background prior regime where there is a flat background rate and flat hotspot, both methods performed quite well as shown on the top panel of <xref ref-type="fig" rid="fig7">Figure 7</xref>. We note that the slight performance decrease for YRI when using <monospace>LDhot</monospace> is likely due to hyperparameters that require tuning for each demography. This bivariate setting is the precise likelihood ratio test for which <monospace>LDhot</monospace> tests. However, as flat background rates and hotspots are not realistic, we sample windows from the HapMap recombination map and label them according to a more suitable hotspot definition that ensures locality and rules out neglectable recombination spikes (the details are given in the Appendix). The bottom panel of <xref ref-type="fig" rid="fig7">Figure 7</xref> uses the same hotspot definition in the training and test regimes, and is strongly favorable towards the deep learning method. Under a sensible definition of recombination hotspots and realistic recombination maps, our method still performs well while <monospace>LDhot</monospace> performs almost randomly. We believe that the true performance of <monospace>LDhot</monospace> is somewhere between the first and second settings, with performance dominated by the deep learning method. Importantly, this improvement is achieved without access to any problem-specific summary statistics.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><p>Posterior calibration. The black line is a perfectly calibrated curve. The red and purple lines are calibration curves for simulation-on-the-fly after 20000 and 60000 iterations, while the blue and green lines for a fixed training set of 10000 points, for 20000 and 60000 training iterations.</p></caption>
<graphic xlink:href="267211_fig6.tif"/>
</fig>
<p>Our approach reached 90&#x0025; accuracy in fewer than 2000 iterations, taking approximately 0.5 hours on a 64 core machine with the computational bottleneck due to the <monospace>msprime</monospace> simulation (<xref ref-type="bibr" rid="c23">Kelleher et al., 2016</xref>). For <monospace>LDhot</monospace>, the two-locus lookup table for variable demography using the <monospace>LDpop</monospace> fast approximation (<xref ref-type="bibr" rid="c22">Kamm et al., 2016</xref>) took 9.5 hours on a 64 core machine (downsampling <italic>n</italic> &#x003D; 198 from <italic>N</italic> &#x003D; 256). The lookup table has a computational complexity of <italic>O</italic>(<italic>N</italic><sup>3</sup>) while per-iteration training of the neural network scales as <italic>O</italic>(<italic>n</italic>), allowing for much larger sample sizes.</p>
</sec>
</sec>
<sec id="s6">
<label>6.</label>
<title>Discussion</title>
<p>We developed the first likelihood-free inference method for population genetics that does not rely on handcrafted summary statistics. To achieve this, we designed a family of neural networks that learn an exchangeable representation of genotype data, which is in turn mapped to the posterior distribution over the parameter of interest. State-of-the-art accuracy was demonstrated on the challenging problem of recombination hotspot testing. Furthermore, we analyzed and developed general-purpose machine learning methods that can leverage scientific simulators to improve over preexisting likelihood-free inference schemes.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><p>The black line represents a random classifier. (Top) ROC curve in the CEU and YRI setting for the deep learning and <monospace>LDhot</monospace> method. (Bottom) Windows of the HapMap recombination map drawn based on whether they matched up with our hotspot definition. The blue and green line coincide almost exactly.</p></caption>
<graphic xlink:href="267211_fig7.tif"/>
</fig>
<p>The theoretical and empirical results of simulation-on-the-fly illustrate the attractiveness of fields with model simulators as a testbed for new neural network methods. For instance, this approach allows the researcher to diagnose if regularization or convergence to poor local minima is affecting performance. We believe the simulator paradigm has a lot to offer to further understanding theoretical aspects of neural networks.</p>
<p>Quantifying uncertainty over a continuous parameter could be of interest in many other population genetic tasks, in which case softmax probabilities are inapplicable. Future work could adapt our method with ideas from the Bayesian neural networks literature to obtain posterior distributions over continuous parameters (<xref ref-type="bibr" rid="c17">Hern&#x00E1;ndez-Lobato &#x0026; Adams, 2015</xref>; <xref ref-type="bibr" rid="c5">Blundell et al., 2015</xref>; <xref ref-type="bibr" rid="c12">Gal &#x0026; Ghahramani, 2016</xref>; <xref ref-type="bibr" rid="c25">Kingma et al., 2015</xref>).</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>ACKNOWLEDGEMENTS</title>
<p>We thank Ben Graham for helpful discussions. This research is supported in part by an NSF Graduate Research Fellowship (JC); EPSRC grants EP/L016710/1 (VP) and EP/L018497/1 (PJ); an NIH grant R01-GM094402 (JC, JPS, SM, and YSS); and a Packard Fellowship for Science and Engineering (YSS). YSS is a Chan Zuckerberg Biohub investigator.</p>
</ack>
<ref-list>
<title>Reference</title>
<ref id="c1"><mixed-citation publication-type="journal"><collab>1000 Genomes Project Consortium</collab>. <article-title>A global reference for human genetic variation</article-title>. <source>Nature</source>, <volume>526</volume>(<issue>7571</issue>):<fpage>68</fpage>&#x2013;<lpage>74</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="website"><string-name><surname>Auton</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Myers</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>McVean</surname>, <given-names>G.</given-names></string-name> <article-title>Identifying recombination hotspots using population genetic data</article-title>. <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1403.4264">arXiv:1403.4264</ext-link>, <year>2014</year>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Beaumont</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>W.</given-names></string-name>, and <string-name><surname>Balding</surname>, <given-names>D. J.</given-names></string-name> <article-title>Approximate Bayesian computation in population genetics</article-title>. <source>Genetics</source>, <volume>162</volume>(<issue>4</issue>):<fpage>2025</fpage>&#x2013;<lpage>2035</lpage>, <year>2002</year>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Blum</surname>, <given-names>MGB</given-names></string-name> and <string-name><surname>Fran&#x00E7;ois</surname>, <given-names>O.</given-names></string-name> <article-title>Non-linear regression models for Approximate Bayesian Computation</article-title>. <source>Statistics and Computing</source>, <volume>20</volume>(<issue>1</issue>):<fpage>63</fpage>&#x2013;<lpage>73</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="confproc"><string-name><surname>Blundell</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Cornebise</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kavukcuoglu</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Wierstra</surname>, <given-names>D.</given-names></string-name> <article-title>Weight uncertainty in neural networks</article-title>. In <conf-name>International Conference on Machine Learning</conf-name>, volume <volume>37</volume>, pp. <fpage>1613</fpage>&#x2013;<lpage>1622</lpage>, <conf-loc>Lille, France</conf-loc>, <year>2015</year>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Boitard</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Rodr&#x00ED;guez</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Jay</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Mona</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Austerlitz</surname>, <given-names>F.</given-names></string-name> <article-title>Inferring population size history from large samples of genome-wide molecular data-an approximate bayesian computation approach</article-title>. <source>PLoS genetics</source>, <volume>12</volume>(<issue>3</issue>):<fpage>e1005877</fpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Cybenko</surname>, <given-names>G.</given-names></string-name> <article-title>Approximation by superpositions of a sigmoidal function</article-title>. <source>Mathematics of Control, Signals, and Systems (MCSS)</source>, <volume>2</volume>(<issue>4</issue>):<fpage>303</fpage>&#x2013;<lpage>314</lpage>, <year>1989</year>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Fearnhead</surname>, <given-names>P.</given-names></string-name> <article-title>SequenceLDhot: detecting recombination hotspots</article-title>. <source>Bioinformatics</source>, <volume>22</volume>:<fpage>3061</fpage>&#x2013;<lpage>3066</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Fearnhead</surname>, <given-names>P.</given-names></string-name> and <string-name><surname>Prangle</surname>, <given-names>D.</given-names></string-name> <article-title>Constructing summary statistics for approximate Bayesian computation: semiautomatic approximate Bayesian computation</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>, <volume>74</volume>(<issue>3</issue>):<fpage>419</fpage>&#x2013;<lpage>474</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="website"><string-name><surname>Frazier</surname>, <given-names>D. T.</given-names></string-name>, <string-name><surname>Martin</surname>, <given-names>G. M.</given-names></string-name>, <string-name><surname>Robert</surname>, <given-names>C. P.</given-names></string-name>, and <string-name><surname>Rousseau</surname>, <given-names>J.</given-names></string-name> <article-title>Asymptotic properties of approximate bayesian computation</article-title>. <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1607.06903">arXiv:1607.06903</ext-link>, <year>2016</year>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Furma&#x0144;czyki</surname>, <given-names>K.</given-names></string-name> and <string-name><surname>Niemiro</surname>, <given-names>W.</given-names></string-name> <article-title>Sufficiency in bayesian models</article-title>. <source>Applicationes Mathematicae</source>, <volume>25</volume>(<issue>1</issue>):<fpage>113</fpage>&#x2013;<lpage>120</lpage>, <year>1998</year>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="confproc"><string-name><surname>Gal</surname>, <given-names>Y.</given-names></string-name> and <string-name><surname>Ghahramani</surname>, <given-names>Z.</given-names></string-name> <article-title>Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</article-title>. In <conf-name>International Conference on Machine Learning</conf-name>, volume <volume>48</volume>, pp. <fpage>1050</fpage>&#x2013;<lpage>1059</lpage>, <conf-loc>New York, New York, USA</conf-loc>, <year>2016</year>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Gibbs</surname>, <given-names>R. A.</given-names></string-name>, <string-name><surname>Belmont</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Hardenbol</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Willis</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Yang</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ch&#x2019;ang</surname>, <given-names>L.-Y.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Shen</surname>, <given-names>Y.</given-names></string-name>, <etal>et al.</etal> <article-title>The international hapmap project</article-title>. <source>Nature</source>, <volume>426</volume> (<issue>6968</issue>):<fpage>789</fpage>&#x2013;<lpage>796</lpage>, <year>2003</year>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Griffiths</surname>, <given-names>R. C.</given-names></string-name> <article-title>Neutral two-locus multiple allele models with recombination</article-title>. <source>Theoretical Population Biology</source>, <volume>19</volume> (<issue>2</issue>):<fpage>169</fpage>&#x2013;<lpage>186</lpage>, <year>1981</year>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="website"><string-name><surname>Guo</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Pleiss</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>Y.</given-names></string-name>, and <string-name><surname>Weinberger</surname>, <given-names>K. Q.</given-names></string-name> <article-title>On calibration of modern neural networks</article-title>. <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1706.04599">arXiv:1706.04599</ext-link>, <year>2017</year>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="website"><string-name><surname>Guttenberg</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Virgo</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Witkowski</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Aoki</surname>, <given-names>H.</given-names></string-name>, and <string-name><surname>Kanai</surname>, <given-names>R.</given-names></string-name> <article-title>Permutation-equivariant neural networks applied to dynamics prediction</article-title>. <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1612.04530">arXiv:1612.04530</ext-link>, <year>2016</year>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Hern&#x00E1;ndez-Lobato</surname>, <given-names>J. M.</given-names></string-name> and <string-name><surname>Adams</surname>, <given-names>R.</given-names></string-name> <article-title>Probabilistic back-propagation for scalable learning of Bayesian neural networks</article-title>. In <source>International Conference on Machine Learning</source>, pp. <fpage>1861</fpage>&#x2013;<lpage>1869</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Hey</surname>, <given-names>J.</given-names></string-name> <article-title>What&#x2019;s so hot about recombination hotspots&#x003F;</article-title> <source>PLoS Biol</source>, <volume>2</volume>(<issue>6</issue>):<fpage>e190</fpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Hudson</surname>, <given-names>R. R.</given-names></string-name> <article-title>Properties of a neutral allele model with intragenic recombination</article-title>. <source>Theoretical population biology</source>, <volume>23</volume>(<issue>2</issue>):<fpage>183</fpage>&#x2013;<lpage>201</lpage>, <year>1983</year>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Hudson</surname>, <given-names>R. R.</given-names></string-name> <article-title>Two-locus sampling distributions and their application</article-title>. <source>Genetics</source>, <volume>159</volume>(<issue>4</issue>):<fpage>1805</fpage>&#x2013;<lpage>1817</lpage>, <year>2001</year>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="website"><string-name><surname>Jiang</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Wu</surname>, <given-names>T.-y.</given-names></string-name>, <string-name><surname>Zheng</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Wong</surname>, <given-names>W.H.</given-names></string-name> <article-title>Learning summary statistic for approximate Bayesian computation via deep neural network</article-title>. <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1510.02175">arXiv:1510.02175</ext-link>, <year>2015</year>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Kamm</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Spence</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Chan</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Song</surname>, <given-names>Y. S.</given-names></string-name> <article-title>Two-locus likelihoods under variable population size and fine-scale recombination rate estimation</article-title>. <source>Genetics</source>, <volume>203</volume>(<issue>3</issue>): <fpage>1381</fpage>&#x2013;<lpage>1399</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Kelleher</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Etheridge</surname>, <given-names>A. M.</given-names></string-name>, and <string-name><surname>McVean</surname>, <given-names>G.</given-names></string-name> <article-title>Efficient coalescent simulation and genealogical analysis for large sample sizes</article-title>. <source>PLoS computational biology</source>, <volume>12</volume>(<issue>5</issue>): <fpage>e1004842</fpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="website"><string-name><surname>Kingma</surname>, <given-names>D.</given-names></string-name> and <string-name><surname>Ba</surname>, <given-names>J.</given-names></string-name> <article-title>Adam: A method for stochastic optimization</article-title>. <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1412.6980">arXiv:1412.6980</ext-link> <year>2014</year>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Kingma</surname>, <given-names>D. P.</given-names></string-name>, <string-name><surname>Salimans</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>Welling</surname>, <given-names>M.</given-names></string-name> <article-title>Variational dropout and the local reparameterization trick</article-title>. In <source>Neural Information Processing Systems</source>, pp. <fpage>2575</fpage>&#x2013;<lpage>2583</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Kingman</surname>, <given-names>J. F. C.</given-names></string-name> <article-title>The coalescent</article-title>. <source>Stochastic processes and their applications</source>, <volume>13</volume>(<issue>3</issue>):<fpage>235</fpage>&#x2013;<lpage>248</lpage>, <year>1982</year>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Kolmogoroff</surname>, <given-names>A.</given-names></string-name> <article-title>Sur l&#x2019;&#x00E9;stimation statistique des param&#x00E9;tres be la loi de gauss</article-title>. <source>Izvestiya Rossiiskoi Akademii Nauk. Seriya Matematicheskaya</source>, <volume>6</volume>(<issue>1</issue>):<fpage>3</fpage>&#x2013;<lpage>32</lpage>, <year>1942</year>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Kong</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Frigge</surname>, <given-names>M. L.</given-names></string-name>, <string-name><surname>Masson</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Besenbacher</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sulem</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Magnusson</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Gudjonsson</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Sigurds-son</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Jonasdottir</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Jonasdottir</surname>, <given-names>A.</given-names></string-name>, <etal>et al.</etal> <article-title>Rate of de novo mutations and the importance of father&#x2019;s age to disease risk</article-title>. <source>Nature</source>, <volume>488</volume>(<issue>7412</issue>):<fpage>471</fpage>&#x2013;<lpage>475</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Li</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>M. Q.</given-names></string-name>, and <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name> <article-title>A new method for detecting human recombination hotspots and its applications to the hapmap encode data</article-title>. <source>The American Journal of Human Genetics</source>, <volume>79</volume>(<issue>4</issue>):<fpage>628</fpage>&#x2013;<lpage>639</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>McVean</surname>, <given-names>G. A. T.</given-names></string-name>, <string-name><surname>Myers</surname>, <given-names>S. R.</given-names></string-name>, <string-name><surname>Hunt</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Deloukas</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Bentley</surname>, <given-names>D. R.</given-names></string-name>, and <string-name><surname>Donnelly</surname>, <given-names>P.</given-names></string-name> <article-title>The fine-scale structure of recombination rate variation in the human genome</article-title>. <source>Science</source>, <volume>304</volume>:<fpage>581</fpage>&#x2013;<lpage>584</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="website"><string-name><surname>Papamakarios</surname>, <given-names>G.</given-names></string-name> and <string-name><surname>Murray</surname>, <given-names>I.</given-names></string-name> <article-title>Fast &#x03F5;-free inference of simulation models with Bayesian conditional density estimation</article-title>. <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1605.06376">arXiv:1605.06376</ext-link> <year>2016</year>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Pavlidis</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Jensen</surname>, <given-names>J. D.</given-names></string-name>, and <string-name><surname>Stephan</surname>, <given-names>W.</given-names></string-name> <article-title>Searching for footprints of positive selection in whole-genome snp data from nonequilibrium populations</article-title>. <source>Genetics</source>, <volume>185</volume>(<issue>3</issue>):<fpage>907</fpage>&#x2013; <lpage>922</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Petes</surname>, <given-names>T. D.</given-names></string-name> <article-title>Meiotic recombination hot spots and cold spots</article-title>. <source>Nature Reviews Genetics</source>, <volume>2</volume>(<issue>5</issue>):<fpage>360</fpage>&#x2013;<lpage>369</lpage>, <year>2001</year>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="website"><string-name><surname>Ravanbakhsh</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Schneider</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Poczos</surname>, <given-names>B.</given-names></string-name> <article-title>Deep learning with sets and point clouds</article-title>. <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1611.04500">arXiv:1611.04500</ext-link>, <year>2016</year>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Schrider</surname>, <given-names>D. R.</given-names></string-name> and <string-name><surname>Kern</surname>, <given-names>A. D.</given-names></string-name> <article-title>Inferring selective constraint from population genomic data suggests recent regulatory turnover in the human brain</article-title>. <source>Genome biology and evolution</source>, <volume>7</volume>(<issue>12</issue>):<fpage>3511</fpage>&#x2013;<lpage>3528</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Sheehan</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Song</surname>, <given-names>Y. S.</given-names></string-name> <article-title>Deep learning for population genetic inference</article-title>. <source>PLoS Computational Biology</source>, <volume>12</volume>(<issue>3</issue>): <fpage>e1004845</fpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="confproc"><string-name><surname>Shivaswamy</surname>, <given-names>P. K.</given-names></string-name> and <string-name><surname>Jebara</surname>, <given-names>T.</given-names></string-name> <article-title>Permutation invariant svms</article-title>. <conf-name>In International Conference on Machine Learning</conf-name>, pp. <fpage>817</fpage>&#x2013;<lpage>824</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Silver</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Huang</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Maddison</surname>, <given-names>C. J.</given-names></string-name>, <string-name><surname>Guez</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sifre</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Van Den Driessche</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Schrittwieser</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Antonoglou</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Panneershelvam</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Lanctot</surname>, <given-names>M.</given-names></string-name>, <etal>et al.</etal> <article-title>Mastering the game of go with deep neural networks and tree search</article-title>. <source>Nature</source>, <volume>529</volume>(<issue>7587</issue>):<fpage>484</fpage>&#x2013;<lpage>489</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="website"><string-name><surname>Silver</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hubert</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Schrittwieser</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Antonoglou</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Lai</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Guez</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Lanctot</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Sifre</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kumaran</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Graepel</surname>, <given-names>T.</given-names></string-name>, <etal>et al.</etal> <article-title>Mastering chess and shogi by selfplay with a general reinforcement learning algorithm</article-title>. <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1712.01815">arXiv:1712.01815</ext-link> <year>2017</year>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Sousa</surname>, <given-names>V. C.</given-names></string-name>, <string-name><surname>Fritz</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Beaumont</surname>, <given-names>M. A.</given-names></string-name>, and <string-name><surname>Chikhi</surname>, <given-names>L.</given-names></string-name> <article-title>Approximate Bayesian computation without summary statistics: The case of admixture</article-title>. <source>Genetics</source>, <volume>181</volume>(<issue>4</issue>):<fpage>1507</fpage>&#x2013; <lpage>1519</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Terhorst</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kamm</surname>, <given-names>J. A.</given-names></string-name>, and <string-name><surname>Song</surname>, <given-names>Y. S.</given-names></string-name> <article-title>Robust and scalable inference of population history from hundreds of unphased whole genomes</article-title>. <source>Nature genetics</source>, <volume>49</volume>(<issue>2</issue>): <fpage>303</fpage>&#x2013;<lpage>309</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Wall</surname>, <given-names>J. D.</given-names></string-name> and <string-name><surname>Stevison</surname>, <given-names>L. S.</given-names></string-name> <article-title>Detecting recombination hotspots from patterns of linkage disequilibrium</article-title>. <source>G3:Genes, Genomes, Genetics</source>, <year>2016</year>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="confproc"><string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name> and <string-name><surname>Rannala</surname>, <given-names>B.</given-names></string-name> <article-title>Population genomic inference of recombination rates and hotspots</article-title>. <conf-name>Proceedings of the National Academy of Sciences</conf-name>, <volume>106</volume>(<issue>15</issue>):<fpage>6215</fpage>&#x2013;<lpage>6219</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Wegmann</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Leuenberger</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Excoffier</surname>, <given-names>L.</given-names></string-name> <article-title>Efficient Approximate Bayesian Computation coupled with Markov chain Monte Carlo without likelihood</article-title>. <source>Genetics</source>, <volume>182</volume>(<issue>4</issue>):<fpage>1207</fpage>&#x2013;<lpage>1218</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Zaheer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kottur</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ravanbakhsh</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Poczos</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Salakhutdinov</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Smola</surname>, <given-names>A.</given-names></string-name> <article-title>Deep sets</article-title>. <source>Neural Information Processing Systems</source>, <year>2017</year>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="website"><string-name><surname>Zhang</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hardt</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Recht</surname>, <given-names>B.</given-names></string-name>, and <string-name><surname>Vinyals</surname>, <given-names>O.</given-names></string-name> <article-title>Understanding deep learning requires rethinking generalization</article-title>. <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1611.03530">arXiv:1611.03530</ext-link> <year>2016</year>.</mixed-citation></ref>
</ref-list>
<app-group>
<app id="app1">
<title>Appendix</title></app></app-group>
<app-group>
<app id="app1a">
<label>A.</label>
<title>Statistical Properties of ABC</title>
<p>Understanding the statistical properties of ABC enables us to highlight the theoretical benefits of our approach. Variants of ABC are among the most widely-used likelihood-free inference techniques in the scientific literature. ABC simulates N draws of the parameter <italic>&#x03B8;</italic><sup>(<italic>i</italic>)</sup> &#x007E; <italic>&#x03C0;</italic>(<italic>&#x03B8;</italic>) and data x<sup>(<italic>i</italic>)</sup> &#x007E; &#x2119;(x | <italic>&#x03B8;</italic><sup>(<italic>i</italic>)</sup>) for <italic>i</italic> &#x003D; 1,&#x2026;, <italic>N</italic>, then approximates the posterior conditioned on the observed summary statistics <italic>s</italic><sub><italic>obs</italic></sub> &#x003D; <italic>S</italic>(x<sub><italic>obs</italic></sub>) by
<disp-formula id="ueqn4"><alternatives><graphic xlink:href="267211_ueqn4.gif"/></alternatives>
</disp-formula>
where <italic>S</italic>: <italic>&#x1D4B3;</italic> &#x2192; <italic>&#x1D4B1;</italic> is a summary statistic of the data and <italic>K</italic>: &#x1D4B1; &#x2192; &#x211D; is a density kernel that integrates to 1 with bandwidth <italic>u</italic> &#x003E; 0. Denote a as the <italic>N</italic>-dimensional vector corresponding to the kernel weight <italic>K</italic>(&#x00B7;) for each simulated parameter <italic>&#x03B8;</italic><sup>(<italic>i</italic>)</sup>. A common choice of <italic>K</italic> is the uniform kernel, though many variants exist. Intuitively, ABC can be interpreted as locally smoothing the empirical likelihood estimates of points near the observed data in the summary statistic space.</p>
<p>The ABC posterior asymptotically converges to the true posterior conditioned on the observed data x<sub><italic>obs</italic></sub> for sample size <italic>n</italic> under suitable regularity conditions, so that
<disp-formula id="ueqn5"><alternatives><graphic xlink:href="267211_ueqn5.gif"/></alternatives>
</disp-formula>
for any choice of sufficient statistic <italic>S</italic>. See <xref ref-type="bibr" rid="c10">Frazier et al.(2016)</xref> for a formal treatment. However, in the finite-sample regime for fixed <italic>N</italic>, the hyperparameters of the ABC algorithm should be chosen such that
<disp-formula id="eqn5"><alternatives><graphic xlink:href="267211_eqn5.gif"/></alternatives>
</disp-formula>
is minimized. Based on this formulation, the computational and statistical tradeoffs based on the choice of <italic>u</italic> and <italic>S</italic> as a function of the computational budget <italic>N</italic> are made explicit. Unfortunately, hyperparameter optimization on <italic>u</italic> and <italic>S</italic> cannot be performed since the expected KL in <xref ref-type="disp-formula" rid="eqn5">(5)</xref> cannot be compared without access to the true posterior. Instead, practitioners often optimize a surrogate objective similar to
<disp-formula id="ueqn6"><alternatives><graphic xlink:href="267211_ueqn6.gif"/></alternatives>
</disp-formula>
where &#x03C4; is a sampling threshold and <italic>m</italic>(a) is a user-defined function of the kernel weights, such as number of accepted samples or effective sample size. <italic>J</italic>(<italic>u</italic>, <italic>S</italic>) is a user-defined positive function that is decreasing in <italic>u</italic>. <italic>J</italic>(<italic>u</italic>, <italic>S</italic>) also satisfies <italic>J</italic>(<italic>u</italic>, <italic>S</italic><sub>1</sub>) &#x2264; <italic>J</italic>(<italic>u</italic>, <italic>S</italic><sub>2</sub>) for all <italic>S</italic><sub>1</sub>, <italic>S</italic><sub>2</sub> where <italic>&#x1D4B1;</italic><sub>1</sub> &#x2286; <italic>&#x1D4B1;</italic><sub>2</sub>. Note that ABC practitioners are typically not explicit about their surrogate objective function when tuning ABC; however, for clarity, we specify the general surrogate objective above, and remark that many modifications do not affect the underlying tradeoffs stated below.</p>
<p>Intuitively, the surrogate objective function encourages practitioners to choose values of <italic>u</italic> and <italic>S</italic> such that <italic>u</italic> is close to 0, and <italic>S</italic> is close to sufficient while generating enough large kernel weights within the computational budget to obtain an empirical posterior. This procedure ignores the posterior completely and could result in arbitrarily poor approximations of the posterior. Poor approximations of the posterior can result for many reasons, including lack of information in <italic>S</italic>, large <italic>u</italic>, insufficient number of samples generated, insufficient computational budget, or incorrect choice of kernel <italic>K</italic> or norm &#x2016; &#x00B7; &#x2016; for the geometry of the posterior. Furthermore, this procedure must be re-run for each new dataset x<sub><italic>obs</italic></sub> allowing for a smaller computational budget <italic>N</italic> when dealing with multiple datasets. There are no guarantees that the previous values of <italic>S</italic>] and <italic>u</italic> remain good choices for a new dataset since the parameters depend on x<sub><italic>obs</italic></sub>.</p>
</app>
</app-group>
<app-group>
<app id="app1b">
<label>B.</label>
<title>Statistical Properties of Our Method: Proofs</title>
<statement>
<label>Proof of Proposition 1</label>
<p>By the Universal Approximation Theorem and the interpretation of simulation-on-the-fly as minimizing the expected KL divergence between the population risk and the neural network, the training procedure minimizes the objective function for every &#x03F5; &#x003E; 0, <italic>H</italic> &#x003E; <italic>H</italic><sub>0</sub>, and <italic>N</italic> &#x003E; <italic>N</italic><sub>0</sub>,
<disp-formula id="ueqn7"><alternatives><graphic xlink:href="267211_ueqn7.gif"/></alternatives>
</disp-formula></p>
<p>Let w&#x002A; be a minimizer of the above expectation. By Markov&#x2019;s inequality, we get for every x and &#x03B4; &#x003E; 0 such that for all <italic>H</italic> &#x003E; <italic>H</italic><sub>0</sub> and <italic>N</italic> &#x003E; <italic>N</italic><sub>0</sub>
<disp-formula id="ueqn8"><alternatives><graphic xlink:href="267211_ueqn8.gif"/></alternatives>
</disp-formula>
with probability at least <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline38.gif"/></alternatives></inline-formula>.</p>
</statement>
<statement>
<label>Proof of Corollary 1</label>
<p>As above, we have
<disp-formula id="ueqn9"><alternatives><graphic xlink:href="267211_ueqn9.gif"/></alternatives>
</disp-formula>
for all &#x03F5; &#x003E; 0, <italic>H</italic> &#x003E; <italic>H</italic><sub>0</sub>, and <italic>N</italic> &#x003E; <italic>N</italic><sub>0</sub>. Furthermore, for all x, the KL is bounded at the minimizer since &#x2119;(x) &#x003E; 0 for all x resulting in the following bound
<disp-formula id="ueqn10"><alternatives><graphic xlink:href="267211_ueqn10.gif"/></alternatives>
</disp-formula>
independent of x. Thus, the training procedure results in a function mapping that uniformly converges to the posterior &#x2119;(<italic>&#x03B8;</italic> &#x007C; x).</p>
</statement>
<statement>
<label>Proof of Proposition 2</label>
<p>For each <italic>H</italic> and <italic>N</italic>, the neural network is trained to find the w that minimizes
<disp-formula id="ueqn11"><alternatives><graphic xlink:href="267211_ueqn11.gif"/></alternatives>
</disp-formula></p>
<p>As <italic>H</italic> &#x2192; &#x221E; and <italic>N</italic> &#x2192; &#x221E;, this quantity converges to a global minimum. By the Universal Approximation Theorem this is achieved when the function learned by the neural network is the posterior <italic>P</italic>(<italic>&#x03B8;</italic> &#x007C; x). Thus, each layer of the neural network can be viewed as a statistic <italic>T</italic>(x) of the input data x. In other words each layer of the trained neural network is prior-dependent Bayes sufficient, &#x2119;(<italic>&#x03B8;</italic> | x) &#x003D; &#x2119;(<italic>&#x03B8;</italic> | T(x)) for our chosen prior &#x03C0;(&#x03B8;).</p>
</statement>
</app>
</app-group>
<app-group>
<app id="app1c">
<label>C.</label>
<title>Recombination Hotspot Details</title>
<p>Recombination hotspots are short regions of the genome with high recombination rate relative to the background. In order to develop accurate methodology, a precise mathematical definition of a hotspot needs to be specified in accordance with the signatures of biological interest. We use the following.</p>
<statement>
<label>Definition 2</label>
<p>(Recombination Hotspot). Let a window over the genome be subdivided into three subwindows <italic>w</italic> &#x003D; (<italic>w</italic><sub><italic>l</italic></sub>, <italic>w</italic><sub><italic>h</italic></sub>, <italic>w</italic><sub><italic>r</italic></sub>) with physical distances <italic>&#x03B1;</italic><sub><italic>l</italic></sub>, <italic>&#x03B1;</italic><sub><italic>h</italic></sub>, and <italic>&#x03B1;</italic><sub><italic>r</italic></sub>, respectively, where <italic>w</italic><sub><italic>l</italic></sub>, <italic>w</italic><sub><italic>h</italic></sub>, <italic>w</italic><sub><italic>r</italic></sub> &#x2208; &#x1D4A2; where &#x1D4A2; is the space over all possible subwindows of the genome. Let a mean recombination map <italic>R</italic>: &#x1D4A2; &#x2192; &#x211D;<sub>&#x002B;</sub> be a function that maps from a subwindow of the genome to the mean recombination rate per base pair in the subwindow. A recombination hotspot for a given mean recombination map <italic>R</italic> is a window <italic>w</italic> which satisfies the following properties:
<list list-type="order">
<list-item><p>Elevated local recombination rate: <italic>R</italic>(<italic>w</italic><sub><italic>h</italic></sub>) &#x003E; <italic>k</italic> &#x00B7; max (<italic>R</italic>(<italic>w</italic><sub><italic>l</italic></sub>), <italic>R</italic>(<italic>w</italic><sub><italic>r</italic></sub>))</p></list-item>
<list-item><p>Large absolute recombination rate: <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline39.gif"/></alternatives></inline-formula></p></list-item>
</list>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="267211_inline40.gif"/></alternatives></inline-formula> is the median (at a per base pair level) genome-wide recombination rate, and k is the relative hotspot intensity.</p>
<p>The first property is necessary to enforce the locality of hotspots and rule out large regions of high recombination rate, which are typically not considered hotspots by biologists. The second property rules out regions of minuscule background recombination rate in which sharp relative spikes in recombination still remain too small to be biologically interesting. The median is chosen here to be robust to the right skew of the distribution of recombination rates. Typically, for the human genome we use <italic>&#x03B1;</italic><sub><italic>l</italic></sub> &#x003D; <italic>&#x03B1;</italic><sub><italic>r</italic></sub> &#x003D; 13 kb, <italic>&#x03B1;</italic><sub><italic>h</italic></sub> &#x003D; 2 kb, and <italic>k</italic> &#x003D; 10 based on experimental findings.</p>
<p>The most widely-used technique for recombination hotspot testing is <monospace>LDhot</monospace> as described in (<xref ref-type="bibr" rid="c2">Auton et al., 2014</xref>). The method performs a generalized composite likelihood ratio test using the two-locus composite likelihood based on (<xref ref-type="bibr" rid="c20">Hudson, 2001</xref>) and (<xref ref-type="bibr" rid="c30">McVean et al., 2004</xref>). The composite two-locus likelihood approximates the joint likelihood of a window of SNPs <italic>w</italic> by a product of pairwise likelihoods
<disp-formula id="ueqn12"><alternatives><graphic xlink:href="267211_ueqn12.gif"/></alternatives>
</disp-formula>
where <italic>X</italic><sub><italic>ij</italic></sub> denotes the data restricted only to SNPs <italic>i</italic> and <italic>j</italic>, and <italic>&#x03C1;</italic><sub><italic>ij</italic></sub> denotes the recombination rate between those sites. Only SNPs within some distance, say <italic>z</italic> &#x003D; 50, are considered.</p>
<p>Two-locus likelihoods are computed via an importance sampling scheme under a constant demography (<italic>&#x03B7;</italic> &#x003D; 1) as in (<xref ref-type="bibr" rid="c30">McVean et al., 2004</xref>). The likelihood ratio test uses a null model of a constant recombination rate and an alternative model of a differing recombination rate in the center of the window under consideration:
<disp-formula id="ueqn13"><alternatives><graphic xlink:href="267211_ueqn13.gif"/></alternatives>
</disp-formula></p>
<p>The two-locus likelihood can only be applied to single pan-mictic populations with constant demography, constant mutation rate, and without natural selection. Furthermore, the two-locus likelihood is an uncalibrated approximation of the true joint likelihood. In addition, the experiments in <xref ref-type="bibr" rid="c42">Wall &#x0026; Stevison (2016)</xref> and <xref ref-type="bibr" rid="c2">Auton et al. (2014)</xref> do not demonstrate the efficacy of <monospace>LDhot</monospace> against a realistic variable background recombination rate as its null hypothesis leads to a comparison against a biologically unrealistic flat background rate. In order to fairly compare our likelihood-free approach against the composite likelihood-based method in realistic human settings, we extended the LDhot methodology to apply to a piecewise constant demography using two-locus likelihoods computed by the software <monospace>LDpop</monospace> (<xref ref-type="bibr" rid="c22">Kamm et al., 2016</xref>). Unlike the method described in <xref ref-type="bibr" rid="c42">Wall &#x0026; Stevison (2016)</xref>, our implementation of <monospace>LDhot</monospace> uses windows defined in terms of SNPs rather than physical distance in order to measure accuracy via ROC curves, since the likelihood ratio test is a function of number of SNPs. Note that computing the approximate two-locus likelihoods for a grid of recombination values is at least <italic>O</italic>(<italic>n</italic><sup>3</sup>), which could be prohibitive for large sample sizes.</p>
</statement>
</app>
</app-group>
<app-group>
<app id="app1d">
<label>D.</label>
<title>Additional Experiments</title>
<p><bold>Regularization</bold> The simulation-on-the-fly paradigm obviates the need for modern regularization techniques such as dropout. This is due to the fact that there is no notion of overfitting since each training point is used only once and a large number of examples are drawn from the population distribution. As shown in <xref ref-type="fig" rid="fig8">Figure 8</xref>, dropout does not help improve the accuracy of our method and, in fact, leads to a minor decrease in performance. As expected, directly optimizing the population risk minimizer circumvents the problem of overfitting.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8.</label>
<caption><p>A comparison of different dropout rates. Dropout has a minimal (or slightly negative) effect on test accuracy under the simulation-on-the-fly regime.</p></caption>
<graphic xlink:href="267211_fig8.tif"/>
</fig>
<fig id="fig9" position="float" orientation="portrait" fig-type="figure">
<label>Figure 9.</label>
<caption><p>Accuracy comparison between haplotype and genotype data.</p></caption>
<graphic xlink:href="267211_fig9.tif"/>
</fig>
<p><bold>Phasing</bold> Deconvolving two haplotypes from genotype data is a challenging statistical problem, commonly referred to as phasing. Phasing without a high quality reference panel introduces significant bias into downstream inference. Our approach can flexibly perform inference directly on haplotype or genotype data, the latter being a challenge for model-based approaches. Inference directly on genotype data allows us to implicitly integrate over possible phasings, reducing the bias introduced by fixing the data to a single phasing. In the case of recombination hotspots, we have found only a minor decrease in accuracy for smallsample sizes corresponding to the reduction in statistical signal when inference is performed on genotype data. We quantified the effect of having accurately phased data in comparison to genotype data. Specifically, inference was run by simulating haplotype data and randomly pairing them to construct genotype data such that the height of the genotype image is half that of the haplotype image. We ran the experiment for <italic>n</italic> &#x003D; 16, 32, 64 as shown in <xref ref-type="fig" rid="fig9">Figure 9</xref> and found that the our method is robust, remaining highly accurate for unphased data.</p>
<p><bold>Missing Data</bold> Biological data typically contain significant amounts of missing data. The missingness results from a number of factors such as repetitive regions of the chromosome which are difficult to map, or low read coverage. Fortunately, haplotype data in population genetics is mostly missing completely at random; that is, the locations of missingness are independent of the data values. However, there is a strong correlation structure between the missingness of spatially close SNPs. To improve the robustness of our methods to missing data, we sample the missingness patterns from empirical data during training time.</p>
</app>
</app-group>
</back>
</article>