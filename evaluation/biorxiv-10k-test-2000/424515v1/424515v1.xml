<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/424515</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Stability of working memory in continuous attractor networks under the control of short-term plasticity</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1541-4906</contrib-id>
<name>
<surname>Seeholzer</surname>
<given-names>Alexander</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2775-2611</contrib-id>
<name>
<surname>Deger</surname>
<given-names>Moritz</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4344-2189</contrib-id>
<name>
<surname>Gerstner</surname>
<given-names>Wulfram</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
</contrib>
<aff id="a1"><label>1</label><institution>School of Computer and Communication Sciences and School of Life Sciences, Brain Mind Institute, &#x00C9;cole Polytechnique F&#x00C9;d&#x00C9;rale de Lausanne</institution>, Lausanne, <country>Switzerland</country></aff>
<aff id="a2"><label>2</label><institution>Institute for Zoology, Faculty of Mathematics and Natural Sciences, University of Cologne</institution>, Cologne, <country>Germany</country></aff>
</contrib-group>
<author-notes>
<fn id="n1"><label>&#x002A;</label><p><email>wulfram.gerstner@epfl.ch</email></p></fn>
</author-notes>
<pub-date pub-type="epub"><year>2018</year></pub-date>
<elocation-id>424515</elocation-id>
<history>
<date date-type="received">
<day>21</day>
<month>9</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>21</day>
<month>9</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>23</day>
<month>9</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="424515.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Continuous attractor models of working-memory store continuous-valued information in continuous state-spaces, but are sensitive to noise processes that degrade memory retention. Short-term synaptic plasticity of recurrent synapses has previously been shown to affect continuous attractor systems: short-term facilitation can stabilize memory retention, while short-term depression possibly increases continuous attractor volatility. However, it currently remains unclear to which degree these two short-term plasticity mechanisms interact, what their combined quantitative effect on working memory stability is, and whether these effects persist in neuronal networks with spike-based transmission. Here, we present a comprehensive description of the effects of short-term plasticity on noise-induced memory degradation in one-dimensional continuous attractor models. Our theoretical description, applicable to spiking and rate-based models alike, accurately describes the slow dynamics of stored memory positions in separate processes of diffusion due to spiking variability and drift due to sparse connectivity and neuronal heterogeneity. We find that facilitation decreases both diffusion and directed drifts, while short-term depression tends to increase both. Using mutual information, we evaluate the combined impact of short-term facilitation and depression on the ability of networks to retain stable working memory. Finally, our theory establishes links to experiments: we are able to predict the sensitivity of continuous working memory to distractor inputs and place constraints on network and synapse properties necessary to implement stable working memory.</p>
<sec>
<title>Author summary</title>
<p>The ability to transiently memorize positions in the visual field is crucial for behavior. Models and experiments have shown that such memories can be maintained in networks of cortical neurons with a continuum of possible activity states, that reflects the continuum of positions in the environment. However, the accuracy of positions stored in such networks will degrade over time due to the noisiness of neuronal signaling and imperfections of the biological substrate. Previous work in simplified models has shown that synaptic short-term plasticity could stabilize this degradation by dynamically up- or down-regulating the strength of synaptic connections, thereby &#x201C;pinning down&#x201D; memorized positions. Here, we present a general theory that accurately predicts the extent of this &#x201C;pinning down&#x201D; by short-term plasticity in a broad class of biologically plausible models, thereby untangling the interplay of varying biological sources of noise with short-term plasticity. Importantly, our work provides a direct and novel theoretical link from the microscopic substrate of working memory &#x2013; neurons and synaptic connections &#x2013; to observable behavioral correlates. This allows us to constrain properties of cortical networks that are currently hard to assess experimentally, which we hope will help guide future theoretical and experimental work.</p>
</sec>
</abstract>
<counts>
<page-count count="52"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Information about past environmental stimuli can be stored and retrieved seconds later from working memory [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>]. Strikingly, this transient storage is achieved for timescales of seconds with neurons and synapse transmission operating mostly on time scales of tens of milliseconds and shorter [<xref ref-type="bibr" rid="c3">3</xref>]. An influential hypothesis of neuroscience is that working memory emerges from recurrently connected cortical neuronal networks: memories are retained by self-generating cortical activity through positive feedback [<xref ref-type="bibr" rid="c4">4</xref>&#x2013;<xref ref-type="bibr" rid="c7">7</xref>], thereby bridging the time scales from milliseconds (neuronal dynamics) to seconds (behavior).</p>
<p>Sensory stimuli are often embedded in a physical continuum: for example, positions of objects in the visual field, frequencies of auditory stimuli, or the intensity and position of somatosensory stimuli on the body, all have continuously varying values. Ideally, the organization of cortical working memory circuits should reflect the continuous nature of sensory information [<xref ref-type="bibr" rid="c3">3</xref>]. A class of cortical working memory models able to store continuously structured information is that of <italic>continuous attractors</italic>, characterized by a continuum of meta-stable states, which can be used to retain memories over delay periods much longer than those of the single network constituents [<xref ref-type="bibr" rid="c8">8</xref>]. Continuous attractors were originally proposed as theoretical models for cortical working memory [<xref ref-type="bibr" rid="c9">9</xref>&#x2013;<xref ref-type="bibr" rid="c11">11</xref>], path integration [<xref ref-type="bibr" rid="c12">12</xref>&#x2013;<xref ref-type="bibr" rid="c14">14</xref>], and other cortical functions [<xref ref-type="bibr" rid="c15">15</xref>&#x2013;<xref ref-type="bibr" rid="c17">17</xref>] (see e.g. [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c18">18</xref>&#x2013;<xref ref-type="bibr" rid="c21">21</xref>] for recent reviews), but recent experimental evidence for continuous attractor dynamics was indeed found in cortical networks [<xref ref-type="bibr" rid="c22">22</xref>], the limbic system [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c23">23</xref>], and one-dimensional ring-attractors in the fly responsible for path integration and self-orientation [<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref>].</p>
<p>Continuous attractor models have been successfully employed in the context of visuospatial working memory to explain behavioral performance [<xref ref-type="bibr" rid="c26">26</xref>&#x2013;<xref ref-type="bibr" rid="c29">29</xref>], to predict the effects of neuromodulation [<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c31">31</xref>], or the implications of cognitive impairment [<xref ref-type="bibr" rid="c32">32</xref>, <xref ref-type="bibr" rid="c33">33</xref>]. However, when embedded in more realistic scenarios, the continuum of states quickly breaks down, since encoded memories are vulnerable to noise and heterogeneities that break, transiently or permanently, the crucial symmetry necessary for continuous attractors [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c34">34</xref>&#x2013;<xref ref-type="bibr" rid="c40">40</xref>]. For example, the stochasticity of neuronal spiking (&#x201C;fast noise&#x201D;) leads to transient asymmetries that randomly displace encoded memories along the continuum of states [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c35">35</xref>, <xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c40">40</xref>], leading, averaged over many trials, to <italic>diffusion</italic> of encoded information. More drastically, introducing fixed asymmetries (&#x201C;frozen noise&#x201D;) due to network heterogeneities causes a <italic>directed drift</italic> of memories and a collapse of the continuum of attractive states to a set of discrete states. Examples of heterogeneities in biological scenarios include the sparsity of recurrent connections [<xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c36">36</xref>], or randomness in neuronal parameters [<xref ref-type="bibr" rid="c36">36</xref>] and values of recurrent weights [<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c38">38</xref>]. Since both (fast) noise and heterogeneities are expected in cortical settings, the feasibility of continuous attractors as computational systems of the brain has been called into question [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c41">41</xref>].</p>
<p>Short-term plasticity of recurrent synaptic connections has been shown to influence the susceptibility of continuous attractor networks to both noise and heterogeneities. In particular, short-term depression has been observed to have strong effects on directed displacement of attractor states in rate models [<xref ref-type="bibr" rid="c42">42</xref>, <xref ref-type="bibr" rid="c43">43</xref>], although no such effect was apparent in a spiking network implementation [<xref ref-type="bibr" rid="c44">44</xref>]. Short-term facilitation, on the other hand, has been proposed as a stabilizing mechanisms that could increase the retention time of memories in continuous attractor networks with noise-free rate neurons [<xref ref-type="bibr" rid="c38">38</xref>]. In simulations of a continuous attractor implemented with spiking neurons, facilitation with a fixed set of parameters was reported to cause slow drift [<xref ref-type="bibr" rid="c45">45</xref>, <xref ref-type="bibr" rid="c46">46</xref>] and a reduced amount of diffusion [<xref ref-type="bibr" rid="c46">46</xref>]. However, despite the large number of existing studies, several fundamental questions remain unanswered. What are the quantitative effects of short-term facilitation in more complex neuronal models and across facilitation parameters? Does short-term depression influence the strength of diffusion and drift, and how does it interplay with facilitation? Do phenomena reported in rate networks persist in spiking networks? Finally, can a single theory be used to predict all of the effects observed in simulations?</p>
<p>Here, we present a comprehensive description of the effects of short-term facilitation and depression on noise-induced displacement of one-dimensional continuous attractor models. Extending earlier theories for diffusion [<xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c40">40</xref>] and drift [<xref ref-type="bibr" rid="c38">38</xref>], we derive predictions of the amount of diffusion and drift in ring-attractor models with short-term plasticity. Our theory generalizes to a large class of neuron models, given their input-output relation, and to various biologically plausible sources of heterogeneity. The theoretical predictions are validated against a set of ring-attractor networks realized with spiking neurons for varying parameters of short-term facilitation and depression. We find that facilitation and depression play antagonistic roles: facilitation can decrease <italic>both diffusion and drift</italic> while depression <italic>increases both</italic>, which we show to have profound impact on the retention of memories, as measured by mutual information. Importantly, our theory is, to a large degree, independent of the microscopic network configurations, which allows relating it to experimentally observable quantities. We apply this insight to show that our theory can predict the sensitivity of networks with short-term plasticity to distractor stimuli. Finally, we demonstrate a second possible link to experiments by placing theoretical bounds on combinations of network and synapse properties that will yield stable working memory (as predicted by our model) under the presence of realistic biological variability.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>We investigated, in theory and simulations, the effects of short-term synaptic plasticity (STP) on the dynamics of ring-attractor models consisting of <italic>N</italic> excitatory neurons with distance-dependent and symmetric excitation, and global (uniform) inhibition provided by a population of inhibitory neurons (<xref ref-type="fig" rid="fig1">Fig. 1A</xref>). For simplicity, we describe neurons in terms of firing rates, but our theory can be mapped to more complex neurons with spiking dynamics. An excitatory neuron <italic>i</italic> with 0 &#x2264; <italic>i</italic> &#x003C; <italic>N</italic> is assigned an angular position <inline-formula><alternatives><inline-graphic xlink:href="424515_inline1.gif"/></alternatives></inline-formula>, where we identify the bounds of the interval to form a ring topology (<xref ref-type="fig" rid="fig1">Fig. 1A</xref>). The firing rate <italic>&#x03D5;<sub>i</sub></italic> (in units of Hz) for each excitatory neuron <italic>i</italic> (0 &#x2264; <italic>i</italic> &#x003C; <italic>N</italic> &#x2212; 1) is given as a function of the neuronal input:
<disp-formula id="eqn1">
<alternatives>
<graphic xlink:href="424515_eqn1.gif"/>
</alternatives>
</disp-formula></p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig 1.</label>
<caption><title>Drift and diffusion in ring-attractor models with short-term plasticity.</title>
<p><bold>A</bold> Excitatory (E) neurons (red circles) are distributed on a ring with coordinates in [&#x2212;<italic>&#x03C0;,&#x03C0;</italic>]. Excitatory-to-excitatory (E-E) connections (red lines) are distance-dependent, symmetric, and subject to short-term plasticity (facilitation and depression, see <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>). Inhibitory (I) neurons (blue circles) project to all E and I neurons (blue lines) and receive connection from all E neurons (gray lines). Only outgoing connections from shaded neurons are displayed. In spiking simulations, neurons additionally receive excitatory input with spikes generated by homogeneous Poisson processes. <bold>B1</bold> Example simulation: E neurons fire asynchronously and irregularly at low rates until (dotted line) a subgroup of E neurons is stimulated (external cue), causing them to spike at elevated rates (red dots, input was centered at 0, starting at <italic>t</italic> &#x003D; 2<italic>s</italic> for 1<italic>s</italic>). During and after (dashed line) the stimulus, a bump state of elevated activity forms and sustains itself after the external cue is turned off. The spatial center of the population activity is estimated from the momentary firing rates (red line, plotted from <italic>t</italic> &#x003D; 2.5<italic>s</italic> onward). Inset: Activity profile in the bump state, centered at 0. <bold>B2</bold> Center positions of 20 repeated spiking simulations for 10 different initial cue positions each for a network with short-term depression (<italic>U</italic> &#x003D; 1, <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 150<italic>ms</italic>). Random E-E connections (with connection probability <italic>p</italic> &#x003D; 0.5) lead to directed drift in addition to diffusion. Right: Normalized histogram (200 bins) of final positions at time <italic>t</italic> &#x003D; 13.5. <bold>C</bold> Illustration of quantities used in theoretic calculations. Neurons in the bump fire at rates <italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub> (dashed black line, compare to B1, inset) due to the steady-state synaptic input <italic>J</italic><sub>0,<italic>i</italic></sub> (blue line). Movement of the bump center causes a change of the synaptic input <inline-formula><alternatives><inline-graphic xlink:href="424515_inline2.gif"/></alternatives></inline-formula> (orange line). <bold>D</bold> Diffusion along the attractor manifold is calculated (see <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>) as a weighted sum of the neuronal firing rates in the bump state (dashed black line). Spiking noise (red line) is illustrated as a random deviation from the mean rate with variance proportional to the rate. The symmetric weighting factors (blue lines show <inline-formula><alternatives><inline-graphic xlink:href="424515_inline3.gif"/></alternatives></inline-formula> for varying <italic>U</italic>) are non-zero at the flanks of the firing rate profile. Stronger short-term depression and weaker facilitation increase the magnitude of weighting factors. <bold>E</bold> Deterministic drift is calculated as a weighted sum (see <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>) of systematic deviations of firing rates from the bump state (frozen noise): a large positive firing rate deviation in the left flank (red line) will cause movement of the center position to the left (red arrow) because the weighting factors (blue lines show <inline-formula><alternatives><inline-graphic xlink:href="424515_inline4.gif"/></alternatives></inline-formula> for varying <italic>U</italic>) are asymmetric.</p></caption>
<graphic xlink:href="424515_fig1.tif"/>
</fig>
<p>Here, the input-output relation <italic>F</italic> relates the dimensionless excitatory <italic>J<sub>i</sub></italic> and inhibitory <italic>J</italic><sub>inh</sub> inputs of neuron <italic>i</italic> to its firing rate. This represents a rate-based simplification of the possibly complex underlying neuronal dynamics [<xref ref-type="bibr" rid="c47">47</xref>]. We assume that the excitatory input <italic>J<sub>i</sub></italic>(<italic>t</italic>) to neuron <italic>i</italic> at time <italic>t</italic> is given by a sum over all presynaptic neurons
<disp-formula id="eqn2">
<alternatives>
<graphic xlink:href="424515_eqn2.gif"/>
</alternatives>
</disp-formula>
where <italic>w<sub>ij</sub>s<sub>j</sub></italic>(<italic>t</italic>) describes the total activation of synaptic input from the presynaptic neuron <italic>j</italic> onto neurons <italic>i</italic>. The maximal strength <italic>w<sub>ij</sub></italic> of recurrent excitatory-to-excitatory connections is chosen to be local in the angular arrangement of neurons, such that connections are strongest to nearby excitatory neurons (<xref ref-type="fig" rid="fig1">Fig. 1A</xref>, red lines). The momentary input depends also on the synaptic activation variables <italic>s<sub>j</sub></italic>, to be defined below. Finally, connections to and from inhibitory neurons are assumed to be uniform and global (all-to-all) (<xref ref-type="fig" rid="fig1">Fig. 1A</xref>, blue lines), thereby providing non-selective inhibitory input <italic>J</italic><sub>inh</sub> to excitatory neurons.</p>
<p>As a model of STP, we assume that excitatory-to-excitatory connections are subject to short-term facilitation and depression, which we implemented using a widely adopted model of short-term synaptic plasticity [<xref ref-type="bibr" rid="c48">48</xref>]. The <italic>outgoing synaptic activations s<sub>j</sub></italic> of neuron <italic>j</italic> are modeled by the following system of ordinary differential equations:
<disp-formula id="eqn3">
<alternatives>
<graphic xlink:href="424515_eqn3.gif"/>
</alternatives>
</disp-formula></p>
<p>The timescale of recovery <italic>&#x03C4;<sub>x</sub></italic> is the main parameter of depression. While the recovery from facilitation is controlled by the timescale <italic>&#x03C4;<sub>u</sub></italic>, the parameter 0 &#x003C; <italic>U</italic> &#x2264; 1 controls the baseline strength of unfacilitated synapses as well as the timescale of their strengthening. For fixed <italic>&#x03C4;<sub>u</sub></italic>, we consider smaller values of <italic>U</italic> to lead to a &#x201C;stronger&#x201D; effect of facilitation, and take <italic>U</italic> &#x003D; 1 as the limit of non-facilitating synapses.</p>
<p>As a reference implementation of this model, we simulated networks of spiking conductance-based leaky-integrate-and-fire (LIF) neurons with (spike-based) short-term plastic synaptic transmission (<xref ref-type="fig" rid="fig1">Fig. 1B1</xref>, see <italic><xref ref-type="sec" rid="s4b">Spiking network model</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details). For these networks, under the assumption that neurons fire with Poisson statistics and the network is in a stationary state, neuronal firing can be approximated by the input-output relation <italic>F</italic> of <xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref> [<xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c50">50</xref>] (see <italic><xref ref-type="sec" rid="s4b4">Firing rate approximation</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref>), which allows us to map the network into the general framework of <xref ref-type="disp-formula" rid="eqn1">Eqs. (1)</xref>-<xref ref-type="disp-formula" rid="eqn2">(2)</xref>. In the stationary state, synaptic depression will lead to a saturation of the synaptic activation variables <italic>s<sub>j</sub></italic> at a constant value as firing rates increase. This nonlinear behavior enables spiking networks to implement bi-stable attractor dynamics with relatively low firing rates [<xref ref-type="bibr" rid="c45">45</xref>, <xref ref-type="bibr" rid="c51">51</xref>] similar to saturating NMDA synapses [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c46">46</xref>]. Since we found that without depression (for <italic>&#x03C4;<sub>x</sub></italic> &#x2192; 0) the bump state was not stable at low firing rates (in agreement with [<xref ref-type="bibr" rid="c51">51</xref>]), we always keep the depression timescale <italic>&#x03C4;<sub>x</sub></italic> at positive values.</p>
<p>Particular care was taken to ensure that networks display nearly identical bump shapes (similar to <xref ref-type="fig" rid="fig1">Fig. 1B1</xref>, inset; see also S1 Fig), which required the re-tuning of network parameters (recurrent conductance parameters and the width of distance-dependent connections; see <italic><xref ref-type="sec" rid="s4b6">Optimization of network parameters</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref>) for each combination of the STP parameters above.</p>
<p>Spiking simulations generally show a bi-stability between a <italic>non-selective</italic> state and a <italic>bump</italic> state. In the non-selective state, all excitatory neurons emit action potentials asynchronously and irregularly at roughly identical and low firing rates (<xref ref-type="fig" rid="fig1">Fig. 1B1</xref>, left of dotted line). The bump state can be evoked by stimulating excitatory neurons localized around a given position by additional external input (<xref ref-type="fig" rid="fig1">Fig. 1B1</xref>, red dots). After the external cue is turned off, a self-sustained firing rate profile (&#x201C;bump&#x201D;) emerges (<xref ref-type="fig" rid="fig1">Fig. 1B1</xref>, right of dashed line, and inset) that persists until the network state is again changed by external input. For example, a short and strong uniform excitatory input to all excitatory neurons causes a transient increase in inhibitory feedback that is strong enough to return the network to the uniform state [<xref ref-type="bibr" rid="c11">11</xref>].</p>
<p>During the bump state, fast fluctuations in the firing of single neurons transiently break the perfect symmetry of the firing rate profile and introduce small random displacements along the attractor manifold, which become apparent as a random walk of the center position. If the simulation is repeated for several trials, the bump has the same shape in each trial, but information on the center position is lost in a <italic>diffusion</italic>-like process. We additionally included varying levels of biologically plausible sources of heterogeneity (frozen noise) in our networks: <italic>random connectivity</italic> between excitatory neurons (E-E) and heterogeneity of the single neuron properties of the excitatory population [<xref ref-type="bibr" rid="c36">36</xref>], realized as a random distribution of leak reversal potentials. Heterogeneities makes the bump <italic>drift</italic> away from its initial position in a directed manner. For example, the bump position in the randomly connected (<italic>p</italic> &#x003D; 0.5) network of <xref ref-type="fig" rid="fig1">Fig. 1B1</xref> shows a clear upwards drift towards center positions around 0. Repeated simulations of the same attractor network with bumps initialized at different positions provide a more detailed picture of the combined drift and diffusion dynamics: bump center trajectories systematically are biased towards a few stable fixed points (<xref ref-type="fig" rid="fig1">Fig. 1B2</xref>) around which they are distributed for longer simulation times (histogram in <xref ref-type="fig" rid="fig1">Fig. 1B2</xref>, <italic>t</italic> &#x003D; 13.5<italic>s</italic>). The theory developed in this paper aims at analyzing the above phenomena of drift and diffusion of the bump center.</p>
<sec id="s2a">
<title>Theory of diffusion and drift with short-term plasticity</title>
<p>To untangle the observed interplay between diffusion and drift and investigate the effects of short-term plasticity, we derived a theory that reduces the microscopic network dynamics to a simple one-dimensional stochastic differential equation for the bump state. The theory yields analytical expressions for diffusion coefficients and drift fields, that depend on short-term plasticity parameters, the shape of the firing rate profile of the bump, as well as the neuron model chosen to implement the attractor.</p>
<p>First, we <italic>assume</italic> that the system of <xref ref-type="disp-formula" rid="eqn3">Eqs. (3)</xref> together with the network <xref ref-type="disp-formula" rid="eqn1">Eqs. (1)</xref>-<xref ref-type="disp-formula" rid="eqn2">(2)</xref> has a 1-dimensional manifold of meta-stable states, i.e. the network is a ring-attractor network as described in the introduction. This entails, that the network dynamics permit the existence of a family of solutions that can be described as a self-sustained and symmetric bump of firing rates <italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub>(<italic>&#x03C6;</italic>) &#x003D; <italic>F</italic>(<italic>J</italic><sub>0,<italic>i</italic></sub>(<italic>&#x03C6;</italic>)) with corresponding inputs <italic>J</italic><sub>0,<italic>i</italic></sub>(<italic>&#x03C6;</italic>) (for 0 &#x2264; <italic>i</italic> &#x003C; <italic>N</italic>). Importantly, the center <italic>&#x03C6;</italic> of the bump can be located at any arbitrary position <inline-formula><alternatives><inline-graphic xlink:href="424515_inline5.gif"/></alternatives></inline-formula>. For example, if <italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub>(0) is a solution with input <italic>J</italic><sub>0,<italic>i</italic></sub>(0), then <inline-formula><alternatives><inline-graphic xlink:href="424515_inline6.gif"/></alternatives></inline-formula> is also a solution with input <inline-formula><alternatives><inline-graphic xlink:href="424515_inline7.gif"/></alternatives></inline-formula>. This solution is illustrated in <xref ref-type="fig" rid="fig1">Fig. 1C</xref> for a bump centered at <italic>&#x03C6;</italic> &#x003D; 0. Second, we assume that the number <italic>N</italic> of excitatory neurons is large (<italic>N</italic> &#x2192; &#x221E;), such that we can think of the possible positions <italic>&#x03C6;</italic> as a continuum. Our final assumption is that neuronal firing is noisy, with spike counts distributed as Poisson processes, and that we are able to replace the shot-noise of Poisson spiking by white Gaussian noise with the same mean and autocorrelation (see <italic><xref ref-type="sec" rid="s4a3">Diffusion</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref>, and <xref ref-type="sec" rid="s3">Discussion</xref>). Under these assumptions, we are able to reduce the network dynamics to a <bold>one-dimensional Langevin equation</bold>, describing the dynamics of the center <italic>&#x03C6;</italic>(<italic>t</italic>) of the firing rate profile (see <italic><xref ref-type="sec" rid="s4a">Analysis of drift &#x0026; diffusion with STP</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref>):
<disp-formula id="eqn4">
<alternatives>
<graphic xlink:href="424515_eqn4.gif"/>
</alternatives>
</disp-formula></p>
<p>Here, <italic>&#x03B7;</italic>(<italic>t</italic>) is white Gaussian noise with zero mean and correlation function &#x2329;<italic>&#x03B7;</italic>(<italic>t</italic>), <italic>&#x03B7;</italic>(<italic>t</italic>&#x2032;)&#x232A; &#x003D; &#x03B4;(<italic>t</italic> &#x2013; <italic>t</italic>&#x2032;).</p>
<p>The first term is diffusion characterized by a <italic><bold>diffusion strength</bold> B</italic><xref ref-type="fn" rid="fn1"><sup>1</sup></xref>, which describes the random displacement of bump center positions due to fluctuations in neuronal firing. For <italic>A</italic>(<italic>&#x03C6;</italic>) &#x003D; 0 this term causes diffusive displacement of the center <italic>&#x03C6;</italic>(<italic>t</italic>) from its initial position <italic>&#x03C6;</italic>(<italic>t</italic><sub>0</sub>), with a mean (over realizations) squared displacement of positions &#x2329;[<italic>&#x03C6;</italic>(<italic>t</italic>) &#x2212; <italic>&#x03C6;</italic>(<italic>t</italic><sub>0</sub>)]<sup>2</sup>&#x232A; &#x003D; <italic>B</italic> &#x00B7; (<italic>t</italic> &#x2013; <italic>t</italic><sub>0</sub>) that, during an initial phase, increases linearly with time [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c52">52</xref>, <xref ref-type="bibr" rid="c53">53</xref>], before saturating due to the circular domain of possible center positions [<xref ref-type="bibr" rid="c39">39</xref>]. Our theory shows (see <italic><xref ref-type="sec" rid="s4a3">Diffusion</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref>) that the coefficient <italic>B</italic> can be calculated as a weighted sum over the neuronal firing rates (<xref ref-type="fig" rid="fig1">Fig. 1D</xref>)
<disp-formula id="eqn5">
<alternatives>
<graphic xlink:href="424515_eqn5.gif"/>
</alternatives>
</disp-formula>
where <inline-formula><alternatives><inline-graphic xlink:href="424515_inline8.gif"/></alternatives></inline-formula> is the change of the input to neuron <italic>i</italic> under shifts of the center position (<xref ref-type="fig" rid="fig1">Fig. 1C</xref>, orange line), and <italic>S</italic> is a normalizing constant.</p>
<p>The analytical factors <italic>C<sub>i</sub></italic> express the spatial dependence of the diffusion coefficient on the short-term plasticity parameters through
<disp-formula id="eqn6">
<alternatives>
<graphic xlink:href="424515_eqn6.gif"/>
</alternatives>
</disp-formula></p>
<p>The dependence of the single summands in <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref> on short-term plasticity parameters is visualized in <xref ref-type="fig" rid="fig1">Fig. 1D</xref>, where we see that: a) due to the squared spatial derivative <inline-formula><alternatives><inline-graphic xlink:href="424515_inline9.gif"/></alternatives></inline-formula> of the bump shape and the squared factors <italic>C<sub>i</sub></italic>/<italic>S</italic>, the important contributions to the sum arise primarily from the flanks of the bump; b) for a fixed bump shape, summands increase with stronger short-term depression (larger <italic>&#x03C4;<sub>x</sub></italic>) and decrease with stronger short-term facilitation (smaller <italic>U</italic>, larger <italic>&#x03C4;<sub>u</sub></italic>). This result extends the approach of [<xref ref-type="bibr" rid="c39">39</xref>] to synaptic dynamics with short-term plasticity: the limiting case of no facilitation and depression (<italic>U</italic> &#x003D; 1, <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 0<italic>ms</italic>) simplifies the normalization factor <italic>S</italic> considerably, leaves <italic>C<sub>i</sub></italic> &#x003D; 1, and recovers the result for diffusion the stated there [<xref ref-type="bibr" rid="c39">39</xref>, Eq. S18].</p>
<p>The second term in <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref> is the <bold><italic>drift field</italic></bold> <italic>A</italic>(<italic>&#x03C6;</italic>), which describes deterministic drifts due to the inclusion of heterogeneities. For heterogeneity caused by variations in neuronal reversal potentials and random network connectivity, we calculate (see <italic><xref ref-type="sec" rid="s4c">Frozen noise</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref>) systematic deviations &#x0394;<italic>&#x03D5;<sub>i</sub></italic>(<italic>&#x03C6;</italic>) of the single neuronal firing rates from the steady-state bump shape that depend on the current position <italic>&#x03C6;</italic> of the bump center. In <italic><xref ref-type="sec" rid="s4a4">Drift</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref>, we show that the drift field is then given by a weighted sum over the firing rate deviations:
<disp-formula id="eqn7">
<alternatives>
<graphic xlink:href="424515_eqn7.gif"/>
</alternatives>
</disp-formula>
with weighing factors depending on the spatial derivative of the bump shape <inline-formula><alternatives><inline-graphic xlink:href="424515_inline10.gif"/></alternatives></inline-formula> and the parameters of the synaptic dynamics through the same factors <italic>C<sub>i</sub></italic>/<italic>S</italic>. This is illustrated in <xref ref-type="fig" rid="fig1">Fig. 1E</xref>: in contrast to <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref> summands are now asymmetric with respect to the bump center, since the spatial derivative is not squared.</p>
</sec>
<sec id="s2b">
<title>Prediction of continuous attractor dynamics with short-term plasticity</title>
<p>To demonstrate the accuracy of our theory, we chose random connectivity as a first source of frozen variability. Random connectivity was realized in simulations by retaining only a random fraction 0 &#x003C; <italic>p</italic> &#x2264; 1 (connection probability) of excitatory-to-excitatory (EE) connections. The uniform connections from and to inhibitory neurons are taken as all-to-all, since the effects of making these random and sparse would have only indirect effects on the dynamics of the bump center positions.</p>
<p>Our theory accurately predicts the drift-fields <italic>A</italic>(<italic>&#x03C6;</italic>) (see <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>) induced by frozen variability in networks with short-term plasticity (<xref ref-type="fig" rid="fig2">Fig. 2</xref>). Briefly, for each neuron 0 &#x2264; <italic>i</italic> &#x003C; <italic>N</italic>, we treat each realization of frozen variability as a perturbation &#x0394;<sub><italic>i</italic></sub> around the perfectly symmetric system and use an expansion to first order of the input-output relation <italic>F</italic> to calculate the resulting changes in firing rates (see <italic><xref ref-type="sec" rid="s4c">Frozen noise</xref></italic> for details):
<disp-formula id="eqn8">
<alternatives>
<graphic xlink:href="424515_eqn8.gif"/>
</alternatives>
</disp-formula></p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig 2.</label>
<caption><title>Drift field predictions for varying short-term facilitation.</title>
<p>All networks have the same instantiation of random connectivity (<italic>p</italic> &#x003D; 0.5), similar to <xref ref-type="fig" rid="fig1">Fig. 1B1</xref>. <bold>A</bold> Centers of excitatory population activity for 50 repetitions of 13.5<italic>s</italic> delay activity, for 20 different positions of initial cues (cue is turned off at <italic>t</italic> &#x003D; 0) colored by position of the cues. Left: no facilitation (<italic>U</italic> &#x003D; 1). Right: with facilitation (<italic>U</italic> &#x003D; 0.1). <bold>B</bold> Drift field as a function of the bump position. The theoretical prediction (blue line, see <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>) of the drift field is compared to velocity estimations along the trajectories shown in A, colored by the line they were estimated from. The thick black line shows the binned mean of data points in 60 bins. For comparison, the predicted drift field for <italic>U</italic> &#x003D; 0.1 is plotted (thin dashed line). Left: no facilitation (<italic>U</italic> &#x003D; 1), for comparison the theoretical prediction for the case <italic>U</italic> &#x003D; 0. 01 is plotted as a dashed line. Right: with facilitation (<italic>U</italic> &#x003D; 0.01). <bold>C</bold> Trajectories under the same conditions as in A, but obtained by forward-integrating the one-dimensional Langevin equation, <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref>. <bold>D</bold> Normalized histograms of final positions at time <italic>t</italic> &#x003D; 13.5 for data from spiking simulations (gray areas, data from A) and forward solutions of the Langevin equations (blue areas, data from C). Other STP parameters were: <italic>&#x03C4;<sub>u</sub></italic> &#x003D; 650<italic>ms, &#x03C4;<sub>x</sub></italic> &#x003D; 150<italic>ms</italic>.</p></caption>
<graphic xlink:href="424515_fig2.tif"/>
</fig>
<p>The resulting terms are then used in <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref> to predict the magnitude of the drift field <italic>A</italic>(<italic>&#x03C6;</italic>) for any center position <italic>&#x03C6;</italic>, which will, importantly, depend on STP parameters. The same approach can be used to predict drift fields induced by heterogeneous single neuron parameters [<xref ref-type="bibr" rid="c36">36</xref>] (see next sections) and additive noise on the E-E connection weights [<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c38">38</xref>].</p>
<p>We first simulated spiking networks with only short-term depression and without facilitation (<xref ref-type="fig" rid="fig2">Fig. 2A</xref>, left, same network as in <xref ref-type="fig" rid="fig1">Fig. 1B1</xref>), for one instantiation of random random (<italic>p</italic> &#x003D; 0.5) connectivity. Numerical estimates of the drift in spiking simulations (by measuring the displacement of bumps over time as a function of their position, see <italic><xref ref-type="sec" rid="s4d1">Spiking simulations</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details) yielded drift-fields in good agreement with the theoretical prediction (<xref ref-type="fig" rid="fig2">Fig. 2B</xref>, left). At points where the drift field prediction crosses from positive to negative values (e.g. <xref ref-type="fig" rid="fig2">Fig. 2B</xref>, left, <inline-formula><alternatives><inline-graphic xlink:href="424515_inline11.gif"/></alternatives></inline-formula>), we expect stable fixed points of the center position dynamics in agreement with simulation results, which show trajectories converging to these points. Similarly, unstable fixed points (negative-to-positive crossings) can be seen to lead to a separation of trajectories (e.g. <xref ref-type="fig" rid="fig2">Fig. 2A</xref>, left, <inline-formula><alternatives><inline-graphic xlink:href="424515_inline12.gif"/></alternatives></inline-formula>). In regions where the positional drifts are predicted to lie close to zero (e.g. <xref ref-type="fig" rid="fig2">Fig. 2A</xref>, left <italic>&#x03C6;</italic> &#x003D; 0) the effects of diffusive dynamics are more pronounced. Finally, numerical integration of the full 1-dimensional Langevin equation <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref> with coefficients predicted by <xref ref-type="disp-formula" rid="eqn5">Eqs. (5)</xref>-<xref ref-type="disp-formula" rid="eqn7">(7)</xref>, produces trajectories with dynamics very similar to the full spiking network (<xref ref-type="fig" rid="fig2">Fig. 2C</xref>, left). When comparing the center positions after 13.5<italic>s</italic> of delay activity between the full spiking simulation and the simple 1-dimensional Langevin system, we found very similar distributions of final positions (<xref ref-type="fig" rid="fig2">Fig. 2D</xref>, left, compare to <xref ref-type="fig" rid="fig1">Fig. 1B1</xref>, histogram). Our theory thus produces an accurate approximation of the dynamics of center positions in networks of spiking neurons with STP, thereby reducing the complex dynamics of the whole network to a simple equation. It should be noted that, in regions with strong drift or steep negative-to-positive crossings, the numerically estimated drift-fields deviate from the theory due to under-sampling of these regions as trajectories move quickly through them, yielding fewer data points. In <italic><xref ref-type="sec" rid="s2d">Short-term plasticity controls drift</xref></italic> we show that for stronger heterogeneities the theory tends to generally over-predict drift-fields.</p>
<p>Introducing strong short-term facilitation (<italic>U</italic> &#x003D; 0.1) reduces the predicted drift fields (<xref ref-type="fig" rid="fig2">Fig. 2B</xref>, left, dashed line), which resemble a scaled-down version of the drift-field for the unfacilitated case. We confirmed this theoretical prediction by simulations including facilitation (<xref ref-type="fig" rid="fig2">Fig. 2A</xref>, right): the resulting drift fields show significant reduction of speeds (<xref ref-type="fig" rid="fig2">Fig. 2B</xref>, right) while zero crossings remained similar to the unfacilitated network, similar to the results in [<xref ref-type="bibr" rid="c38">38</xref>]. Theoretical predictions of the drift fields with bump shapes extracted from these simulations again show an accurate prediction of the dynamics (<xref ref-type="fig" rid="fig2">Fig. 2B</xref>, right). Thus, as before, forward integrating the simple 1-dimensional Langevin-dynamics yields trajectories (<xref ref-type="fig" rid="fig2">Fig. 2C</xref>, right) highly similar to those of the full spiking network, with closely matching distributions of final positions (<xref ref-type="fig" rid="fig2">Fig. 2D</xref>, right), indicative of a matching strength of diffusion. In summary, our theory predicts the effects of STP on the joint dynamics of diffusion and drift due to network heterogeneities, which we will show in detail in the next sections.</p>
</sec>
<sec id="s2c">
<title>Short-term plasticity controls diffusion</title>
<p>To isolate the effects of STP on diffusion, we simulated networks <italic>without frozen noise</italic> for various STP parameters. For each combination of parameters, we simulated 1000 repetitions of 13.5s delay activity (after cue offset) distributed across 20 uniformly spaced initial cue positions (see <xref ref-type="fig" rid="fig3">Fig. 3A</xref> for an example). From these simulations, the strength of diffusion was estimated by measuring the growth of variance (over repetitions) of the distance of the center position from its initial position as a function of time (see <italic><xref ref-type="sec" rid="s4d1">Spiking simulations</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details). For all parameters considered, this growth was well fit by a linear function (e.g. <xref ref-type="fig" rid="fig3">Fig. 3A</xref>, inset), the slope of which we compared to the theoretical prediction obtained from the diffusion strength <italic>B</italic> (<xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig 3.</label>
<caption><title>Diffusion on continuous attractors is controlled by short-term plasticity.</title>
<p><bold>A</bold> Center positions of 20 repeated simulations of the reference network (<italic>U</italic> &#x003D; 1, <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 150ms) for 10 different initial cue positions each. Inset: Estimated variance of deviations of center positions <italic>&#x03C6;</italic>(<italic>t</italic>) from their positions <italic>&#x03C6;</italic>(0.5) at <italic>t</italic> &#x003D; 0.5<italic>s</italic> (purple) as a function of time (&#x2329;[<italic>&#x03C6;</italic>(<italic>t</italic>) &#x2013; <italic>&#x03C6;</italic>(0.5)]<sup>2</sup>&#x232A;), together with linear fit (dashed line). The slope of the dashed line yields an estimate of <italic>B</italic> (<xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>). <bold>B,C</bold> Diffusion strengths estimated from simulations (dots, error bars show 95&#x0025; confidence interval, estimated by bootstrapping) compared to theory. Dashed lines show theoretical prediction using firing rates measured from the reference network (<italic>U</italic> &#x003D; 1, <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 150<italic>ms</italic>), while crosses are theoretical estimates using firing rates measured for each set of STP parameters separately (crosses). <bold>B</bold> Diffusion strength as a function of facilitation parameter <italic>U</italic>. Inset shows zoom of region indicated in the dashed area in the lower left. Increasing the facilitation time constant <italic>&#x03C4;<sub>u</sub></italic> &#x003D; 650<italic>ms</italic> (blue) to <italic>&#x03C4;<sub>u</sub></italic> &#x003D; 1<italic>s</italic> (orange) affects diffusion only slightly. In panels A and B, the depression time constant is <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 150<italic>ms</italic>. <bold>C</bold> Diffusion strength as a function of depression time constant <italic>&#x03C4;<sub>x</sub></italic>. Results for three different values of <italic>U</italic> are shown (note the change in scale). Colors indicate the two different values for the facilitation time constant also used in panel B.</p></caption>
<graphic xlink:href="424515_fig3.tif"/>
</fig>
<p>We find that facilitation and depression control the amount of diffusion along the attractor manifold in an antagonistic fashion (<xref ref-type="fig" rid="fig3">Fig. 3B,C</xref>). First, increasing facilitation by lowering the facilitation parameter <italic>U</italic> from its baseline <italic>U</italic> &#x003D; 1 (no facilitation) towards <italic>U</italic> &#x003D; 0, while keeping the depression time constant <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 150<italic>ms</italic> fixed, decreases the measured diffusion strength over an order of magnitude (<xref ref-type="fig" rid="fig3">Fig. 3B</xref>, dots). On the other hand, increasing the facilitation time constant <italic>&#x03C4;<sub>u</sub></italic> from <italic>&#x03C4;<sub>u</sub></italic> &#x003D; 650<italic>ms</italic> to <italic>&#x03C4;<sub>u</sub></italic> &#x003D; 1000<italic>ms</italic> (<xref ref-type="fig" rid="fig3">Fig. 3B</xref>, orange and blue dots, respectively) only slightly reduces diffusion. Our theory further predicts that increasing the facilitation time constants above <italic>&#x03C4;<sub>u</sub></italic> &#x003D; 1<italic>s</italic> will not lead to large reductions in the magnitude of diffusion (see S2 Fig). Second, we find that increasing the depression time constant <italic>&#x03C4;<sub>x</sub></italic> for fixed <italic>U</italic>, thereby slowing down recovery from depression, leads to an increase of the measured diffusion (<xref ref-type="fig" rid="fig3">Fig. 3C</xref>). More precisely, increasing the depression time constant from <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 120<italic>ms</italic> to <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 200<italic>ms</italic> leads only to slight increases in diffusion for strong facilitation (<italic>U</italic> &#x003D; 0.1), but to a much larger increase for weak facilitation (<italic>U</italic> &#x003D; 0.8).</p>
<p>For a comparison of these simulations with our theory, we used two different approaches. First, we estimated the diffusion strength by using the precise shape of the stable firing rate profile extracted separately for each network with different sets of parameters. This first comparison with simulations confirms that the theory closely describes the dependence of diffusion on short-term plasticity for each parameter set (<xref ref-type="fig" rid="fig3">Fig. 3B</xref>, crosses). The observed effects could arise directly from changes in STP parameters for a fixed bump shape, or indirectly since STP parameters also influence the shape of the bump. To separate such direct and indirect effects, we used for a second comparison a theory with fixed bump shape, i.e. the bump shape measured in a &#x201C;reference network&#x201D; (<italic>U</italic> &#x003D; 1, <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 150<italic>ms</italic>) and extrapolated curves by changing only STP parameters in <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>. This leads to very similar predictions (<xref ref-type="fig" rid="fig3">Fig. 3B</xref>, dashed lines) and supports the following conclusions: a) the diffusion to be expected in attractor networks with similar observable quantities (mainly, the bump shape) depends only on the short-term plasticity parameters; b) the bump shapes in the family of networks we have investigated are sufficiently similar to be approximated by measurement in a single reference network. It should be noted that the theory tends to slightly over-estimate the amount of diffusion, especially for small facilitation <italic>U</italic> (see <xref ref-type="fig" rid="fig3">Fig. 3B, C</xref> left). This may be because slower bump movement decreases the firing irregularity of flank neurons, which deviates from the Poisson firing assumption of our theory (see also <xref ref-type="sec" rid="s3">Discussion</xref>). However, given the simplifying assumptions needed to derive the theory, the match to the spiking network is surprisingly accurate.</p>
</sec>
<sec id="s2d">
<title>Short-term plasticity controls drift</title>
<p>Having established that our theory is able to predict the effect of STP on diffusion, as well as drift for a single instantiation of random connectivity, we wondered how different sources of heterogeneity (frozen noise) would influence the drift of the bump. We considered two sources of heterogeneity: First, random connectivity as introduced above, and second, heterogeneity of the leak reversal potential parameters of excitatory neurons: leak reversal potentials of excitatory neurons are given by <italic>V<sub>L</sub></italic> &#x002B; &#x0394;<italic><sub>L</sub></italic>, where &#x0394;<italic><sub>L</sub></italic> is normally distributed with zero mean and standard deviation <italic>&#x03C3;<sub>L</sub></italic> [<xref ref-type="bibr" rid="c36">36</xref>]. The resulting fields can be calculated by calculating the resulting perturbations to the firing rates of neurons by <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref> (see <italic><xref ref-type="sec" rid="s4c">Frozen noise</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details).</p>
<p>The theory developed so far allowed us to predict drift-fields for a given realization of frozen noise, controlled by the noise parameters <italic>p</italic> (for random connectivity) and <italic>&#x03C3;<sub>L</sub></italic> (for heterogeneous leak reversal-potentials) (see S3 Fig for a comparison of predicted drift fields to those measured in simulations for varying STP parameters and varying strengths of frozen noises). We wondered, whether we could take the level of abstraction of our theory one step further, by predicting the magnitude of drift fields from the frozen noise parameters only, independently of a specific realization. First, the expectation of drift fields under the distributions of the frozen noises vanishes for any given position: &#x2329;<italic>A</italic> (<italic>&#x03C6;</italic>)&#x232A;<sub>frozen</sub> &#x003D; 0, where the expectation &#x2329;&#x00B7;&#x232A;<sub>frozen</sub> is taken over both noise parameters. We thus turned to the expected squared magnitude of drift fields under the distributions of these parameters (see <italic><xref ref-type="sec" rid="s4c3">Squared field magnitude</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref> for the derivation):
<disp-formula id="eqn9">
<alternatives>
<graphic xlink:href="424515_eqn9.gif"/>
</alternatives>
</disp-formula>
where <italic>s</italic><sub>0,<italic>j</italic></sub> is the steady-state synaptic activation. Here, we introduced the derivatives of the input-output relation with respect to the noise sources that appear in <xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>: <inline-formula><alternatives><inline-graphic xlink:href="424515_inline13.gif"/></alternatives></inline-formula> is the derivative with respect to the steady state synaptic input, and <inline-formula><alternatives><inline-graphic xlink:href="424515_inline14.gif"/></alternatives></inline-formula> is the derivative with respect to the perturbation in the leak potential. In <italic><xref ref-type="sec" rid="s4c3">Squared field magnitude</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref>, we show that <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref> is independent of the center position <italic>&#x03C6;</italic>, and can be estimated from simulations as the variance of the drift field across positions, averaged over an ensemble of network instantiations.</p>
<p>We defined the root of the expected squared magnitude of <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref> as the <italic>expected field magnitude</italic>:
<disp-formula id="eqn10">
<alternatives>
<graphic xlink:href="424515_eqn10.gif"/>
</alternatives>
</disp-formula></p>
<p>This quantity predicts the magnitude of the deviations of drift-fields from zero that are expected from the parameters that control the frozen noise &#x2013; in analogy to the standard deviation for random variables, it predicts the standard deviation of the fields. To compare this quantity to simulations, we varied both heterogeneity parameters. First, the connectivity parameter <italic>p</italic> was varied between 0.25 and 1. Second, for heterogeneities in leak reversal-potentials, we chose values for the standard deviation <italic>&#x03C3;<sub>L</sub></italic> of leak-reversal potentials between 0<italic>mV</italic> and 1.5<italic>mV</italic>, which lead to a similar range of drift magnitudes as those of randomly connected networks. For each combination of heterogeneities and STP parameters (networks had either random connections or heterogeneous leaks) we then realized 18 &#x2013; 20 networks, for which we simulated 400 repetitions of 6.5s of delay activity each (20 uniformly spaced positions of the initial cue). We then estimated the drift-field numerically by recording displacements of bump centers along their trajectories (as in <xref ref-type="fig" rid="fig2">Fig. 2A, B</xref>) and measured the standard deviation of the resulting fields across all positions.</p>
<p>Similar to the analysis of diffusion above, we find that facilitation and depression elicit antagonistic control over the magnitude of drift fields. In both simulations and theory, we find (<xref ref-type="fig" rid="fig4">Fig. 4A,B</xref>) that the expected field magnitude <italic>decreases</italic> as the effect of facilitation is <italic>increased</italic> from unfacilitated networks (<italic>U</italic> &#x003D; 1) through intermediate levels of facilitation (<italic>U</italic> &#x003D; 0.4) to strongly facilitating networks (<italic>U</italic> &#x003D; 0.1). Our theory predicts this effect surprisingly well, which we validated twofold (as for the diffusion magnitude). First, we used <xref ref-type="disp-formula" rid="eqn10">Eq. (10)</xref> with all parameters and coefficients estimated from each spiking simulation separately (<xref ref-type="fig" rid="fig4">Fig. 4A,B</xref>, plus-signs and crosses). Second, we extrapolated the theoretical prediction by using coefficients in <xref ref-type="disp-formula" rid="eqn9">Eq. (9)</xref> measured from the unfacilitated reference network only (<italic>U</italic> &#x003D; 1, <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 150<italic>ms</italic>) but changed the facilitation and heterogeneity parameters (<xref ref-type="fig" rid="fig4">Fig. 4A,B</xref>, dashed lines). The largest differences between the extrapolated and full theory are seen for <italic>U</italic> &#x003C; 1 and randomly connected networks (<italic>p</italic> &#x003C; 1), which we found to result from the fact that bump shapes for these networks tended to be slightly reduced under random and sparse connectivity (e.g. the top firing rate is reduced to &#x007E;35<italic>Hz</italic> for <italic>U</italic> &#x003D; 0.1, <italic>p</italic> &#x003D; 0.25). Generally, as noise levels increase, our theory tends to over-estimate the squared magnitude of fields, since we rely on a linear expansion of perturbations to the firing rates to calculate fields (<xref ref-type="disp-formula" rid="eqn8">Eq. (8)</xref>). Such deviations are expected as the magnitude of firing rate perturbations increases, and could be counter-acted by including higher-order terms. Since in the theory facilitation (and depression) only scales the firing rate perturbations (<xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>), these deviations can also be observed across facilitation parameters. Finally, we performed a similar analysis to investigate the effect of short-term depression on drift fields. Here, we varied the depression time constant <italic>&#x03C4;<sub>x</sub></italic> for randomly connected networks with <italic>p</italic> &#x003D; 0.6, by simulating networks with combinations of short-term plasticity parameters from <italic>U</italic> &#x2208; &#x007B;0.1, 0.4, 0.8&#x007D; and <italic>&#x03C4;<sub>x</sub></italic> &#x2208; &#x007B;120<italic>ms</italic>, 160<italic>ms</italic>, 200<italic>ms</italic>&#x007D; (<xref ref-type="fig" rid="fig4">Fig. 4C</xref>). We find that an increase of the depression time constant leads to increased magnitude of drift fields, which again is well predicted by our theory.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Fig 4.</label>
<caption><title>Drift field magnitude is controlled by short-term plasticity.</title>
<p><bold>A</bold> Expected magnitude of drift fields as a function of the sparsity parameter <italic>p</italic> of recurrent excitatory-to-excitatory connections. Dots are the standard deviation of fields estimated from 400 trajectories (see main text) of each network, averaged over 18 &#x2013; 20 realizations for each noise parameter and facilitation setting (error bars show 95&#x0025; confidence of the mean). Theoretical predictions (dashed lines) are given by <xref ref-type="disp-formula" rid="eqn10">Eq. (10)</xref> extrapolated from the reference network (<italic>U</italic> &#x003D; 1, <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 150). For validation, we also estimated <xref ref-type="disp-formula" rid="eqn10">Eq. (10)</xref> with coefficients measured from each simulated network separately (plus signs). The depression time constant was <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 150ms. <bold>B</bold> Same as in panel A, with heterogeneous leak-reversal potentials as the source of frozen noise. Validation predictions are plotted as crosses. <bold>C</bold> Same as in panels A,B but varying the depression time constant <italic>&#x03C4;<sub>x</sub></italic> for a fixed level of frozen noise (random connectivity, <italic>p</italic> &#x003D; 0.6). In all panels, the facilitation time constant was <italic>&#x03C4;<sub>u</sub></italic> &#x003D; 650ms.</p></caption>
<graphic xlink:href="424515_fig4.tif"/>
</fig>
</sec>
<sec id="s2e">
<title>Short-term plasticity controls memory retention</title>
<p>The theory developed in previous sections shows that diffusion and drift of the bump center <italic>&#x03C6;</italic> are controlled antagonistically by short-term depression and facilitation. In a working memory setup, we can view the attractor dynamics as a noisy communication channel [<xref ref-type="bibr" rid="c54">54</xref>] that maps a set of initial positions <italic>&#x03C6;</italic>(<italic>t</italic> &#x003D; 0<italic>s</italic>) (time of the cue offset in the attractor network) to associated final positions <italic>&#x03C6;</italic>(<italic>t</italic> &#x003D; 6.5<italic>s</italic>), after a memory retention delay of 6.5<italic>s</italic>. We used the distributions of initial and (associated) final positions to investigate the combined impact of diffusion and drift on the retention of memories (<xref ref-type="fig" rid="fig5">Fig. 5A</xref>). Because of diffusion, distributions of positions will widen over time, which degrades the ability to distinguish different initial positions of the bump center (<xref ref-type="fig" rid="fig5">Fig. 5A</xref>, top). Additionally, directed drift of the dynamics will contract distributions of different initial positions around the same fixed points, making them essentially indistinguishable when read out (<xref ref-type="fig" rid="fig5">Fig. 5A</xref>, bottom).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Fig 5.</label>
<caption><title>Short-term facilitation increases memory retention.</title>
<p><bold>A</bold> Illustration of the effects of diffusion (top) and additional drift (bottom) on the temporal evolution of distributions of initial positions <italic>p</italic>(start) towards distributions of final positions <italic>p</italic>(end) over 6.5s of delay activity. The bump is always represented by its center position <italic>&#x03C6;</italic>. Two peaks in the distribution of initial positions <italic>&#x03C6;</italic>(0) and their corresponding final positions <italic>&#x03C6;</italic>(6.5) are highlighted by colors (purple, red), together with example trajectories of the center positions. Top: Diffusion symmetrically widens the initial distribution. Bottom: Strong drift towards one single fixed point of bump centers (<italic>&#x03C6;</italic> &#x003D; 0) makes the origin of trajectories indistinguishable. <bold>B</bold> Normalized mutual information (MI, see text for details) of distributions of initial and final bump center positions in working memory networks for different STP parameters and heterogeneity parameters(blue: strong facilitation, see legend in panel D). Dots and triangles are average MI (18 &#x2013; 20 realizations, error bars show 95&#x0025; CI) obtained from spiking network simulations. Lines show average MI calculated from Langevin dynamics for the same networks, repetitions and realizations (see text, shaded area shows 95&#x0025; CI). Heterogeneity parameters are <italic>&#x03C3;<sub>L</sub></italic> (triangles, in units of <italic>mV</italic>) and 1 &#x2212; <italic>p</italic> (circles), where <italic>p</italic> is the connection probability. <bold>C</bold> Expected displacement &#x007C;&#x0394;<italic>&#x03C6;</italic>&#x007C; (1<italic>s</italic>) for the same networks as in panel B. Dashed lines indicate displacement induced by diffusion only <inline-formula><alternatives><inline-graphic xlink:href="424515_inline15.gif"/></alternatives></inline-formula>, solid lines show the total displacement (including displacement due to drift, calculated as the expected field magnitude <inline-formula><alternatives><inline-graphic xlink:href="424515_inline16.gif"/></alternatives></inline-formula>). <bold>D</bold> Same as panel B, with x-axis showing the expected field magnitude. <bold>E</bold> Same as panel B, with x-axis showing the expected displacement. In panels B-D, all facilitation parameters except <italic>U</italic> were kept constant at <italic>&#x03C4;<sub>u</sub></italic> &#x003D; 650ms, <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 150ms</p></caption>
<graphic xlink:href="424515_fig5.tif"/>
</fig>
<p>As a numerical measure of this ability of such systems to retain memories over the delay period, we turned to mutual information (MI), which provides a measure of the amount of information contained in the readout position about the initially encoded position [<xref ref-type="bibr" rid="c55">55</xref>, <xref ref-type="bibr" rid="c56">56</xref>]. To measure MI from simulations (see <italic><xref ref-type="sec" rid="s4d2">Mutual information measure</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref>), we analyzed network simulations for varying short-term facilitation parameters (<italic>U</italic>) and magnitudes of frozen noises (<italic>p</italic> and <italic>&#x03C3;<sub>L</sub></italic>) (same data set as <xref ref-type="fig" rid="fig4">Fig. 4A,B</xref>). We recorded the center positions encoded in the network at the time of cue-offset (<italic>t</italic> &#x003D; 0) and after 6.5s of delay activity, and used binned histograms (100 bins) to calculate discrete probability distributions of initial (<italic>t</italic> &#x003D; 0) and final positions (<italic>t</italic> &#x003D; 6.5). For each trajectory simulated in spiking networks, we then generated a trajectory starting at the same initial position by using the Langevin equation <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref> that describes the drift and diffusion dynamics of center positions. The MI calculated from the resulting distributions of final positions (again at <italic>t</italic> &#x003D; 6.5) for each network serve as the theoretical prediction for each network. As a reference, we used the spiking network without facilitation (<italic>U</italic> &#x003D; 1, <italic>&#x03C4;</italic><sub>u</sub> &#x003D; 650<italic>ms, &#x03C4;<sub>x</sub></italic> &#x003D; 150<italic>ms</italic>) and no frozen noises (<italic>p</italic>&#x003D;1, <italic>&#x03C3;<sub>L</sub></italic> &#x003D; 0<italic>mV</italic>) and normalized the MI of all other networks (both for spiking simulations and theoretical predictions) with respect to the reference, yielding the measure of <italic>relative MI</italic> presented in <xref ref-type="fig" rid="fig5">Fig. 5B-E</xref>.</p>
<p>We found that the relative MI decreased compared to the reference network as network heterogeneities were introduced (<xref ref-type="fig" rid="fig5">Fig. 5B</xref>, green). This was expected, since directed drift caused by heterogeneities leads to a loss of information about initial positions. There were two effects of increased short-term facilitation (by decreasing the parameter <italic>U</italic>). First, diffusion was reduced, which was visible in a vertical shift of the relative MI for facilitated networks (<xref ref-type="fig" rid="fig5">Fig. 5A</xref>, orange and blue, at 0 heterogeneity).</p>
<p>Second, the effects of frozen noise decreased with increasing facilitation, which was visible in the slopes of the MI decrease (see also S4 Fig). The MI obtained by integration of the Langevin equations (see above) matched those of the simulations well (<xref ref-type="fig" rid="fig5">Fig. 5A</xref>, lines). From earlier results, we expected the drift-fields to be slightly over-estimated by the theory as the heterogeneity parameters increase (<xref ref-type="fig" rid="fig4">Fig. 4</xref>), which would lead to an under-estimation of MI. We did observe this here, although for <italic>U</italic> &#x003D; 1 the effect was slightly counter-balanced by the under-estimated level of diffusion (cf. <xref ref-type="fig" rid="fig3">Fig. 3A</xref>, right), which we expected to increase the MI. For networks with stronger facilitation (<italic>U</italic> &#x003D; 0.1), we systematically over-estimated diffusion (cf. <xref ref-type="fig" rid="fig3">Fig. 3</xref>, left), and therefore under-estimated MI.</p>
<p>Using our theory, we were able to simplify the functional dependence between MI, short-term plasticity, and frozen noise. Combining the effects of both diffusion and drift into a single quantity for each network, we replaced the field <italic>A</italic>(<italic>&#x03C6;</italic>) by our theoretical prediction <inline-formula><alternatives><inline-graphic xlink:href="424515_inline17.gif"/></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref> and forward integrated the differential equation for a time interval &#x0394;<italic>t</italic> &#x003D; 1<italic>s</italic>, to arrive at the <italic>expected displacement</italic> in 1<italic>s</italic>:
<disp-formula id="eqn11">
<alternatives>
<graphic xlink:href="424515_eqn11.gif"/>
</alternatives>
</disp-formula></p>
<p>This quantity describes the expected absolute value of displacement of center positions during 1<italic>s</italic>: it increases as a function of the frozen noise distribution parameters (<xref ref-type="fig" rid="fig5">Fig. 5C</xref>), but even in the absence of frozen noise it is nonzero due to diffusion. Plotting the MI data in dependence of the first term only <inline-formula><alternatives><inline-graphic xlink:href="424515_inline18.gif"/></alternatives></inline-formula>, shows that the MI curves collapse onto a single curve for each facilitation parameter (<xref ref-type="fig" rid="fig5">Fig. 5D</xref>). Finally, plotting the MI data against &#x007C;&#x0394;<italic>&#x03C6;</italic>&#x007C; (1<italic>s</italic>) we find that all data collapse on to nearly a single curve (<xref ref-type="fig" rid="fig5">Fig. 5E</xref>). Thus, the effects of the two sources of frozen noise (corresponding to &#x2329;<italic>A</italic><sup>2</sup>&#x232A;<sub>frozen</sub>) and diffusion (corresponding to <italic>B</italic>) are unified into a single quantity &#x007C;&#x0394;<italic>&#x03C6;</italic>&#x007C; (1<italic>s</italic>).</p>
<p>We performed the same analyses on a large set of network simulations with fixed random connectivity (<italic>p</italic> &#x003D; 0.6) and varying STP parameters for both depression (<italic>&#x03C4;<sub>x</sub></italic>) and facilitation (<italic>U</italic>) (same data set as in <xref ref-type="fig" rid="fig4">Fig. 4C</xref>). Increasing the short-term depression time constant <italic>&#x03C4;<sub>x</sub></italic> leads to decreased relative MI with a positive offset induced through stronger facilitation (<xref ref-type="fig" rid="fig6">Fig. 6A</xref>, blue line). Calculating the expected displacement for these network configurations collapsed the data points mostly onto the same curve as earlier (<xref ref-type="fig" rid="fig6">Fig. 6B</xref>). For strong depression combined with weak facilitation (<italic>&#x03C4;<sub>x</sub></italic> &#x003D; 200<italic>ms, U</italic> &#x003D; 0.8), the drop-off of the relative MI saturates earlier, indicating that for these strongly diffusive networks the effect on MI may not be sufficiently captured by its relationship to &#x007C;&#x0394;<italic>&#x03C6;</italic>&#x007C; (1<italic>s</italic>).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Fig 6.</label>
<caption><title>Short-term depression decreases memory retention.</title>
<p><bold>A</bold> Same as <xref ref-type="fig" rid="fig5">Fig. 5B</xref>, for network simulations with varying <italic>&#x03C4;<sub>x</sub></italic> and <italic>U</italic> (see legend in panel B). MI is normalized to the same value as there. <bold>B</bold> Same as panel A, with x-axis showing the expected displacement. Light gray data points and lines are the data plotted in <xref ref-type="fig" rid="fig5">Fig. 5E</xref>. The facilitation time constant was kept constant at <italic>&#x03C4;<sub>u</sub></italic> &#x003D; 650ms.</p></caption>
<graphic xlink:href="424515_fig6.tif"/>
</fig>
</sec>
<sec id="s2f">
<title>Linking theory to experiments: distractors &#x0026; network size</title>
<p>The large level of abstraction of our theory condenses the complex dynamics of spiking bump attractor networks into a high-level description of a few macroscopic features, which in turn allows matching the theory to behavioral experiments. Here, we demonstrate how such quantitative links could be established using two different features: 1) the sensitivity of the working memory circuit to distractors, and 2) the stability of working memory expressed by the expected displacement. We stress that our model is a simplified description of biological circuits (see <xref ref-type="sec" rid="s3">Discussion</xref>), and thus, at their current level of abstraction, should be seen as a proof of principle rather than a means of deriving accurate quantitative predictions.</p>
<sec id="s2f1">
<title>Predicting the sensitivity to distractor inputs</title>
<p>In a biological setting, drifts introduced by network heterogeneities (frozen noise) could be significantly reduced by (long-term) plasticity [<xref ref-type="bibr" rid="c36">36</xref>]. To measure the intrinsic stability of continuous attractor models, earlier studies [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c46">46</xref>, <xref ref-type="bibr" rid="c57">57</xref>] have alternatively proposed to use <italic>distractor inputs</italic> (<xref ref-type="fig" rid="fig7">Fig. 7A</xref>): providing a short external input centered around a position <italic>&#x03C6;<sub>D</sub></italic> to the network, the center position of an existing bump state will be biased towards the distracting input, with stronger biases appearing for closer distractors. Distractors forcibly expose the working memory system&#x2019;s intrinsic time scale by creating externally induced drifts, thereby testing its sensitivity (in terms of distractor distance), and they are equally implementable in behavioral experiments [<xref ref-type="bibr" rid="c57">57</xref>].</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Fig 7.</label>
<caption><title>Effect of short-term plasticity on distractor inputs.</title>
<p><bold>A</bold> While a bump (&#x201C;Bump&#x201D;) is centered at an initial angle <italic>&#x03C6;</italic><sub>0</sub> (chosen to be 0), additional external input causes neurons centered around the position <italic>&#x03C6;</italic><sub><italic>D</italic></sub> to fire at elevated rates (&#x201C;Distractor&#x201D;). The theory predicts the shape and magnitude of the induced drift field (&#x201C;Field&#x201D;) and the mean bump center <italic>&#x03C6;</italic><sub>1</sub> after 250ms of distractor input. Gray trajectories are example simulations of bump centers of the corresponding Langevin equation <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref>. <bold>B</bold> Mean final positions <italic>&#x03C6;</italic><sub>1</sub> of bump centers (1000 repetitions, shaded areas show 1 standard deviation) as a function of the distractor input location <italic>&#x03C6;</italic><sub><italic>D</italic></sub>. Increased short-term facilitation (blue: strong facilitation, <italic>U</italic> &#x003D; 0.1; orange: intermediate facilitation, <italic>U</italic> &#x003D; 0.4; green: no facilitation <italic>U</italic> &#x003D; 1) leads to less displacement due to the distractor input. Other STP parameters were kept constant at <italic>&#x03C4;<sub>u</sub></italic> &#x003D; 650ms, <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 150ms. <bold>C</bold> Same as panel B, for three different depression time constants <italic>&#x03C4;<sub>x</sub></italic>, while keeping <italic>U</italic> &#x003D; 0.8, <italic>&#x03C4;<sub>u</sub></italic> &#x003D; 650ms fixed. <bold>D</bold> Same as panel B, with a broader bump half-width (<italic>&#x03C3;<sub>g</sub></italic> &#x003D; 0.8rad &#x2248; 45.8deg). All other panels use the same bump half-width as in the rest of the study (<italic>&#x03C3;<sub>g</sub></italic> &#x003D; 0.5rad &#x2248; 28.7deg) (see S1 Fig).</p></caption>
<graphic xlink:href="424515_fig7.tif"/>
</fig>
<p>Our theory can readily yield quantitative predictions for the distractor paradigm. To accommodate distractor inputs in the theory, we assume that they cause some units <italic>i</italic> to fire at elevated rates <italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub> &#x002B; &#x0394;<italic>&#x03D5;</italic><sub><italic>i</italic></sub>, which will introduce a drift field according to <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref> (<xref ref-type="fig" rid="fig7">Fig. 7A</xref>, purple dashed line). The resulting dynamics (<xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref>) of diffusion and drift during the presentation of the distractor input then allow us to calculate the expected shift of center positions as a function of all network parameters, including those of short-term plasticity. Repeating this paradigm for varying positions of the distractor inputs (see <italic><xref ref-type="sec" rid="s4d4">Distractor analysis</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details), our theory predicts that strong facilitation will strongly decrease both the effect and radial reach of distractor inputs (<xref ref-type="fig" rid="fig7">Fig. 7B</xref>, blue), when compared to the unfacilitated system (<xref ref-type="fig" rid="fig7">Fig. 7B</xref>, green) &#x2013; in qualitative agreement with simulation results involving a related (cell-intrinsic) stabilization mechanism [<xref ref-type="bibr" rid="c46">46</xref>]. Conversely, we predict that longer recovery from short-term depression tends to increase the sensitivity to distractors (<xref ref-type="fig" rid="fig7">Fig. 7C</xref>). The displacement caused by each distractor input is given integrating the resulting dynamics of <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref> over the stimulus duration. As such, the magnitude of the displacement will increase both with the amplitude and the duration of the distractor input. Finally, our theory demonstrates that the bump shape, importantly bumps with a broader width, can also significantly increase the effect and radial reach of distractor inputs (<xref ref-type="fig" rid="fig7">Fig. 7D</xref>).</p>
</sec>
<sec id="s2f2">
<title>Constraining the size of working memory networks</title>
<p>The simple theoretical measure of expected displacement &#x007C;&#x0394;<italic>&#x03C6;</italic>&#x007C; (1<italic>s</italic>) introduced in the last section can be related to behavioral experiments: a value of &#x007C;&#x0394;<italic>&#x03C6;</italic>&#x007C; (1<italic>s</italic>) &#x003D; 1.0 deg lies in the upper range of experimentally reported deviations due to diffusive and systematic errors in behavioral studies [<xref ref-type="bibr" rid="c58">58</xref>, <xref ref-type="bibr" rid="c59">59</xref>]. What are the microscopic circuit compositions that can attain such a (high) level of working memory stability? In particular, since an increase in network size can reduce diffusion [<xref ref-type="bibr" rid="c11">11</xref>] and the effects of random heterogeneities [<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c45">45</xref>], we turned to the question: <italic>which networks sizes would be needed to yield this level of stability in a one-dimensional continuous memory system</italic>?</p>
<p>To address the question of network size, we extended our theory to include the size <italic>N</italic> of the excitatory population as an explicit parameter (see <italic><xref ref-type="sec" rid="s4c4">System size scaling</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details). We find (see <xref ref-type="disp-formula" rid="eqn59">Eq. (59)</xref>) that the expected field magnitude <inline-formula><alternatives><inline-graphic xlink:href="424515_inline19.gif"/></alternatives></inline-formula> (cf. <xref ref-type="disp-formula" rid="eqn10">Eq. (10)</xref>) scales with the connectivity parameter <italic>p</italic> and the system size <italic>N</italic> to leading order as <inline-formula><alternatives><inline-graphic xlink:href="424515_inline20.gif"/></alternatives></inline-formula>, whereas the perturbations due to heterogeneity of leak potentials scale as <inline-formula><alternatives><inline-graphic xlink:href="424515_inline21.gif"/></alternatives></inline-formula>, both in accordance with earlier results [<xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c45">45</xref>]. For the diffusion scale <inline-formula><alternatives><inline-graphic xlink:href="424515_inline22.gif"/></alternatives></inline-formula> (cf. <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>) we find a scaling as as <inline-formula><alternatives><inline-graphic xlink:href="424515_inline23.gif"/></alternatives></inline-formula>, also in agreement with earlier work [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c45">45</xref>]. Using numerical coefficients in <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref> extracted from the spiking simulation of a reference network (<italic>U</italic> &#x003D; 1, <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 150 and <italic>N<sub><sc>e</sc></sub></italic> &#x003D; 800), we extrapolated the theory by changing the system size <italic>N</italic> and short-term plasticity parameters. We then constrained parameters of our theory by published data (<xref ref-type="table" rid="tbl1">Table 1</xref>). Short-term plasticity parameters were based on two groups of strongly facilitating synapses found in a study of mammalian (ferret) prefrontal cortex [<xref ref-type="bibr" rid="c60">60</xref>]. The same study reported a general probability <italic>p</italic> &#x003D; 0.12 of pyramidal cells to be connected. However, for pairs of pyramidal cells that were connected by facilitating synapses, the study found a high probability of reciprocal connections (<italic>p<sub>rec</sub></italic> &#x003D; 0.44): thus if neuron A was connected to neuron B (with probability <italic>p</italic>), neuron B was connected to neuron A with high probability (<italic>p<sub>rec</sub></italic>), resulting in a non-random connectivity. To approximate this in the random connectivities supported by our theory, we evaluated a second, slightly elevated, level of random connectivity, that has the same mean connection probability as the non-random connectivity with these additional reciprocal connections: <italic>p</italic> &#x002B; <italic>p</italic> &#x00B7; <italic>p<sub>rec</sub></italic> &#x003D; 0.1728. For the standard deviation of leak reversal-potentials <italic>&#x03C3;<sub>L</sub></italic>, we used values measured in two studies [<xref ref-type="bibr" rid="c61">61</xref>, <xref ref-type="bibr" rid="c62">62</xref>].</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Upper bounds on system-sizes for stable continuous attractor memory in prefrontal cortex.</title></caption>
<graphic xlink:href="424515_tbl1.tif"/>
</table-wrap>
<p>The resulting theory makes quantitative predictions for combinations of network size <italic>N</italic> and all other parameters that yield the desired levels of working memory stability (<xref ref-type="table" rid="tbl1">Table 1</xref>, see also S5 Fig). Network sizes were all smaller than 10<sup>6</sup> neurons, with values depending most strongly on the value of the facilitation parameter <italic>U</italic> and the magnitude of the leak reversal-potential heterogeneities <italic>&#x03C3;<sub>L</sub></italic>. Since the expected field magnitude scales weakly <inline-formula><alternatives><inline-graphic xlink:href="424515_inline24.gif"/></alternatives></inline-formula> with the recurrent connectivity <italic>p</italic>, increasing <italic>p</italic> lead only to comparatively small decreases in the predicted network sizes. Finally, we see that the increasing the reliability of networks comes at a high cost: decreasing the expected displacement to &#x007C;&#x0394;<italic>&#x03C6;</italic>&#x007C; (1<italic>s</italic>) &#x003D; 0.5 deg [<xref ref-type="bibr" rid="c58">58</xref>] increases the required number of neurons by nearly a number of 4 for both facilitation settings we investigated. Nevertheless, these network sizes still lie within anatomically reasonable ranges [<xref ref-type="bibr" rid="c63">63</xref>]. In summary, our theory predicts that, given the simplifying assumptions of our models, continuous attractor networks with realistic values for the strength of facilitation and depression of recurrent connections could achieve sufficient stability, even in the presence of a realistic degree biological variability.</p>
</sec>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We presented a theory of drift and diffusion in continuous working memory models, exemplified on a one-dimensional ring attractor model. Our framework extends earlier approaches calculating the effects of noise by projection onto the attractor manifold [<xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c40">40</xref>] by including the effects of short-term plasticity. Our approach further generalizes earlier work on drift in continuous attractors with short-term plasticity [<xref ref-type="bibr" rid="c38">38</xref>] to include diffusion and the dynamics of short-term depression. The theory predicts that facilitation and depression play opposite roles in making continuous attractors robust against the influences of both dynamic noise (introduced by spiking variability) and frozen noise (introduced by biological variability). We have confirmed the quantitative predictions of our theory in simulations of a ring-attractor implemented in a spiking network model with synaptic facilitation and depression, and found theory and simulation to be quantitatively in good agreement.</p>
<p>In <italic><xref ref-type="sec" rid="s2e">Short-term plasticity controls memory retention</xref></italic>, we demonstrated the effects that STP has through drift and diffusion on the information retained in continuous working memory. Using our theoretical predictions of drift and diffusion we were able to derive the expected displacement &#x007C;&#x0394;<italic>&#x03C6;</italic>&#x007C; as a function of STP parameters and the frozen noise parameters, which provides a simple link between the resulting Langevin dynamics of bump centers and mutual information (MI) as a measure of working memory retention. Our results can be generalized in several directions. First, the choice of 1s of forward integrated time for &#x007C;&#x0394;<italic>&#x03C6;</italic>&#x007C; (<xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref>) was arbitrary. While a choice of &#x007E; 2<italic>s</italic> lets the curves in <xref ref-type="fig" rid="fig5">Fig. 5E</xref> collapse slightly better, we chose 1s to avoid further heuristics. Second, we expect values of MI to decrease as the length of the delay period is increased. Our choice of 6.5<italic>s</italic> is comparable to delay periods often considered in behavioral experiments (usually 3-6s) [<xref ref-type="bibr" rid="c59">59</xref>, <xref ref-type="bibr" rid="c64">64</xref>, <xref ref-type="bibr" rid="c65">65</xref>]. However, a more rigorous link between the MI measure and the underlying attractor dynamics would be desirable. Indeed, for noisy channels governed by Fokker-Planck equations, this might be feasible [<xref ref-type="bibr" rid="c66">66</xref>], but goes beyond the scope of this work.</p>
<p>In <italic><xref ref-type="sec" rid="s2f">Linking theory to experiments: distractors &#x0026; network size</xref></italic>, we demonstrated that the high-level description of the microscopic dynamics obtained by our theory allows its parameters to be constrained by experiments. Considering that our model is a simplified description of its biological counterparts (see next paragraph), these demonstrations are to be seen as a proof of principle, which will not be able to yield biologically accurate predictions in its current level of abstraction. However, since distractor inputs can be implemented in silico as well as in behavioral experiments (see e.g. [<xref ref-type="bibr" rid="c57">57</xref>]), they could eventually provide a quantitative link between the continuous attractor models and working memory systems, by matching the resulting distraction curves. Given the simplifying assumptions, our theory provides a direct map between underlying network parameters and their combined effects on distraction sensitivity, facilitating previous approaches in which these distraction curves had to be extracted through repeated microscopic simulations for single parameter settings [<xref ref-type="bibr" rid="c46">46</xref>]. We further used our theory to derive bounds on network parameters, in particular the size of networks, that lead to &#x201C;tolerable&#x201D; levels of drift and diffusion in the simplified model. For large magnitudes of frozen noise our theory tends to over-estimate the expected magnitude of drift-fields slightly (cf. <xref ref-type="fig" rid="fig4">Fig. 4</xref>). Thus, we expect the predictions made here to be upper bounds on network parameters needed to achieve a certain expected displacement. Finally, while the predictions of our theory might deviate from biological networks, they could be applied to accurately characterize the stability of, and the effects of inputs to, bump attractor networks implemented in neuromorphic hardware for robotics applications [<xref ref-type="bibr" rid="c67">67</xref>].</p>
<p>Our results show, to our knowledge for the first time, that strong facilitation (small values of <italic>U</italic>) does not only slow down directed drift [<xref ref-type="bibr" rid="c38">38</xref>], but also efficiently suppresses diffusion in continuous attractor models. However, in delayed response tasks involving saccades, that presumably involve continuous attractors in the prefrontal cortex [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c22">22</xref>], one does observe an increase of variability in time [<xref ref-type="bibr" rid="c64">64</xref>]: both quickly accumulating systematic errors (alike drift) [<xref ref-type="bibr" rid="c59">59</xref>] and more slowly increasing variable errors (with variability growing linear in time, alike diffusion) [<xref ref-type="bibr" rid="c58">58</xref>] appear. Indeed, there are several other possible sources of variability in cortical working memory circuits, which we did not consider here. For example, noisy synaptic transmission and STP [<xref ref-type="bibr" rid="c68">68</xref>] or heterogeneous STP parameters as found in [<xref ref-type="bibr" rid="c60">60</xref>]. These will probably induce further drift and diffusion and the effectiveness of facilitation as a stabilizing mechanism in coping with them remains to be investigated. For simplicity, we also excluded AMPA currents from the recurrent excitatory interactions, which are found in biological neuronal networks [<xref ref-type="bibr" rid="c11">11</xref>]. However, since STP acts by presynaptic scaling of neurotransmitter release, it will act symmetrically on both AMPA and NMDA receptors, which we thus expect to lead to similar dynamical effects of STP as those reported here and which might be amenable to a similar analytical approach. Additionally, variable errors might be introduced elsewhere in the pathway between visual input and motor output (but see [<xref ref-type="bibr" rid="c69">69</xref>]) or by input from of other noisy local circuits during the delay period [<xref ref-type="bibr" rid="c70">70</xref>].</p>
<p>Several additional dynamical mechanisms might also influence the stability of continuous attractor working memory circuits. For example, intrinsic neuronal currents that modulate the neuronal excitability [<xref ref-type="bibr" rid="c46">46</xref>] or firing-rate adaptation [<xref ref-type="bibr" rid="c71">71</xref>] have been shown to affect bump stability. It should be noted that many such dynamical processes could be accommodated in our theoretical approach, by including their linearized dynamics in the calculation of the projection vector (cf. <italic><xref ref-type="sec" rid="s4a1">Projection of dynamics onto the attractor manifold</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref>). Fast corrective inhibitory feedback has also been shown to stabilize spatial working memory systems (in balanced networks) [<xref ref-type="bibr" rid="c72">72</xref>]. It has been shown that, on the timescale of hours to days, homeostatic processes can be used to counteract the drift introduced by frozen noise [<xref ref-type="bibr" rid="c36">36</xref>]. Finally, inhibitory connections that are distance-dependent [<xref ref-type="bibr" rid="c11">11</xref>] and show short-term plasticity [<xref ref-type="bibr" rid="c73">73</xref>] could also influence bump dynamics.</p>
<p>We have focused here on ring-attractor models that obtain their stable firing-rate profile due to perfectly symmetric connectivity. Our approach can also be employed to analyze ring-attractor networks with short-term plasticity, in which weights show (deterministic or stochastic) deviations from symmetry (see <italic><xref ref-type="sec" rid="s4c">Frozen noise</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref> for stochastic deviations). Although not investigated here, continuous line-attractors arising through a different weight-symmetry should be amenable to similar analyses [<xref ref-type="bibr" rid="c39">39</xref>]. Finally, it should be noted that adequate structuring of the recurrent connectivity can also positively affect the stability of continuous attractors [<xref ref-type="bibr" rid="c14">14</xref>]. For example, translational asymmetries included in the structured heterogeneity can break the continuous attractor into several isolated fixed points, which can lead to decreased diffusion along the attractor [<xref ref-type="bibr" rid="c56">56</xref>].</p>
<p>We provided evidence that short-term synaptic plasticity controls the sensitivity of attractor networks to both fast diffusive and frozen noise. Control of short-term plasticity via neuromodulation [<xref ref-type="bibr" rid="c74">74</xref>] would thus represent an efficient &#x201C;crank&#x201D; for adapting the timescale of computations of such networks. By changing the properties of presynaptic calcium entry [<xref ref-type="bibr" rid="c75">75</xref>], inhibitory modulation mediated via GABA<sub>B</sub> and adenosine A<sub>1</sub> receptors can lead to increased facilitatory components in rodent cerebellar [<xref ref-type="bibr" rid="c76">76</xref>] and avian auditory synapses [<xref ref-type="bibr" rid="c77">77</xref>]. Dopamine, serotonin and noradrenaline have all been shown to differentially modulate short-term depression (and facilitation when blocking GABA receptors) at sensorimotor synapses [<xref ref-type="bibr" rid="c78">78</xref>]. Interestingly, next to short-term facilitation on the timescale of seconds, other dynamic processes up-regulate recurrent excitatory synaptic connections in prefrontal cortex [<xref ref-type="bibr" rid="c60">60</xref>]: synaptic augmentation and post-tetanic potentiation operate on longer time scales (up to tens of seconds), and might be able to support working memory function [<xref ref-type="bibr" rid="c79">79</xref>]. While the long time scales of these processes might again render putative short-term memory networks inflexible, there is evidence that they might also be under tight neuromodulatory control [<xref ref-type="bibr" rid="c80">80</xref>]. Neuromodulation could also generally provide higher flexibility (see above) to a facilitation-stabilized working memory system, by allowing for more dynamical control of the rigidity of memory representations [<xref ref-type="bibr" rid="c46">46</xref>].</p>
<sec id="s3a">
<title>Comparison to earlier work</title>
<p>Similar to an earlier theoretical approach using a simplified rate model [<xref ref-type="bibr" rid="c38">38</xref>], we find that the slowing of drift by facilitation depends mainly on the facilitation parameter <italic>U</italic>, while the time constant <italic>&#x03C4;<sub>u</sub></italic> has a less pronounced effect. While the approach of [<xref ref-type="bibr" rid="c38">38</xref>] relied on the projection of frozen noise onto the derivative of the first spatial Fourier mode of the bump shape along the ring, here we reproduce and extend this result (1) for arbitrary neuronal input-output relations and (2) a more detailed spatial projection that involves the synaptic dynamics and the bump shape. While, our theory can also accommodate noisy recurrent connection weights as frozen noise, as used in in [<xref ref-type="bibr" rid="c38">38</xref>] (see <italic><xref ref-type="sec" rid="s4c">Frozen noise</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref> for derivations), the drifts generated by these heterogeneities were generally small compared to diffusion and the other sources of heterogeneity.</p>
<p>A second study investigated short-term facilitation and showed that it reduces drift and diffusion in a spiking network, for a fixed setting of <italic>U</italic> (although the model of short-term facilitation differs slightly from the one employed here) [<xref ref-type="bibr" rid="c46">46</xref>]. Contrary to what we find here, these authors find that an increase in <italic>&#x03C4;<sub>u</sub></italic> leads to increased diffusion, while we find that an increase over the range they investigated (&#x007E; 0.5<italic>s</italic> &#x2013; 4<italic>s</italic>) would decrease the diffusion by a factor of nearly two. More precisely, for our shape of the bump state (which we keep fixed) we predict a reduction from &#x007E; 26 to &#x007E; 16 <italic>deg</italic><sup>2</sup>/<italic>s</italic> for a similar setting of facilitation <italic>U</italic>. These differences might arise from an increasing width of the bump attractor profile for growing facilitation time constants in [<xref ref-type="bibr" rid="c46">46</xref>], which would then lead to increased diffusion in our model. Whether this effect persists under the two-equation model of saturating NMDA synapses used there remains to be investigated. Finally, increasing the time constant of recurrent NMDA conductances has been shown to also reduce diffusion [<xref ref-type="bibr" rid="c46">46</xref>], in agreement with our theory, according to which the normalization constant <italic>S</italic> increases with <italic>&#x03C4;<sub>s</sub></italic> [<xref ref-type="bibr" rid="c39">39</xref>].</p>
<p>In a study that investigated only a single parameter value for depression (<italic>&#x03C4;<sub>x</sub></italic> &#x003D; 160<italic>ms</italic>, no facilitation) in a spiking network similar to the one investigated here [<xref ref-type="bibr" rid="c44">44</xref>], the authors observed no apparent effect of short-term depression on the stability of the bump. In contrast, we find that stronger short-term depression will indeed increase both diffusion and directed drift along the attractor. Our result agrees qualitatively with earlier studies in rate models, which showed that synaptic depression, similar to neuronal adaptation [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c81">81</xref>], can induce movement of bump attractors [<xref ref-type="bibr" rid="c42">42</xref>, <xref ref-type="bibr" rid="c43">43</xref>, <xref ref-type="bibr" rid="c82">82</xref>, <xref ref-type="bibr" rid="c83">83</xref>]. In particular, the study of [<xref ref-type="bibr" rid="c42">42</xref>] showed that for simpler rate models a regime exists where the bump state moves with constant speed along the attractor manifold. We did not find any such directed movement in our networks, which could be due to fast spiking noise stabilizing the directed movement of bumps observed in noise-free rate models, as shown in [<xref ref-type="bibr" rid="c81">81</xref>]: there, additive fast noise was able to cancel directed bump movement caused by single neuron adaptation.</p>
</sec>
<sec id="s3b">
<title>Extensions &#x0026; Shortcomings</title>
<p>The coefficients of <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref> give clear predictions as to how drift and diffusion will depend on the shape of the bump state and the neural transfer function <italic>F</italic>. The relation is not trivial, since the pre-factors <italic>C<sub>i</sub></italic> and the normalization constant <italic>S</italic> also depend on the bump shape. For the diffusion strength <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>, we explored this relation numerically, by artificially varying the shape of the firing rate profile (while extrapolating other quantities). Although a more thorough analysis remains to be performed, a preliminary analysis shows (see S6 Fig) that diffusion increases both with bump width and top firing rate, consistent with earlier findings [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c32">32</xref>].</p>
<p>In <italic><xref ref-type="sec" rid="s2f">Linking theory to experiments: distractors &#x0026; network size</xref></italic>, we have demonstrated that our theory can be used to predict the shape and effect of drift fields that are generated by localized external inputs due to distractor inputs. More generally, any localized external input (excitatory or inhibitory) will cause a deviation &#x0394;<italic>&#x03D5;</italic><sub>i</sub> from the steady-state firing rates, which, in turn, generates a drift field by <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>. This could predict the strength and location of external inputs that are needed to induce continuous shifts of the bump center at given speeds, for example when these attractor networks are designed to track external inputs (see e.g. [<xref ref-type="bibr" rid="c10">10</xref>, <xref ref-type="bibr" rid="c84">84</xref>]). It should be noted that in our simple approximation of this distractor scheme, we assume the system to remain at approximately steady-state, i.e. that the bump shape is unaffected by the additional external input, except for a shift of the center position. For example, we expect additional feedback inhibition (through the increased firing of excitatory neurons caused by the distractor input) to decrease bump firing rates. A more in depth study and comparison to simulations will be left for further work.</p>
<p>The spiking networks we analyzed here are tuned to display balanced inhibition and excitation in the inhibition dominated uniform state [<xref ref-type="bibr" rid="c85">85</xref>, <xref ref-type="bibr" rid="c86">86</xref>], while the bump state relies on positive currents, mediated through strong recurrent excitatory connections (cf. [<xref ref-type="bibr" rid="c44">44</xref>] for an analysis). Similar to other spiking network models of this class, this mean-driven bump state shows relatively low variability of neuronal inter-spike-intervals, because near the bump center the mean input is close to (or even above) the firing threshold [<xref ref-type="bibr" rid="c87">87</xref>, <xref ref-type="bibr" rid="c88">88</xref>] (see also next paragraph). While the decreased variability appears for neurons in the center of the firing rate profile, we found that neurons at its flanks still display variable firing, with statistics close to that expected of spike trains with Poisson statistics (see S7 Fig), which may be because the flank&#x2019;s position slightly jitters. Since the non-zero contributions to the diffusion strength are constrained to these flanks (cf. <xref ref-type="fig" rid="fig1">Fig. 1D</xref>), the simple theoretical assumption of Poisson statistics of neuronal firing still matches the spiking network quite well. As discussed in <italic><xref ref-type="sec" rid="s2c">Short-term plasticity controls diffusion</xref></italic>, we find that our theory over-estimates the diffusion as bump movement slows down for small values of <italic>U</italic> &#x2013; this may be due to a decrease in firing irregularity in stable bumps, at which the Poisson assumption becomes inaccurate.</p>
<p>More recent bump attractor approaches allow networks to perform working memory function with a high firing variability also during the delay period [<xref ref-type="bibr" rid="c3">3</xref>], in better agreement with experimental evidence [<xref ref-type="bibr" rid="c89">89</xref>]. These networks show bi-stability, where both stable states show balanced excitation and inhibition [<xref ref-type="bibr" rid="c87">87</xref>] and the higher self-sustained activity in the delay activity is evoked by an increase in fluctuations of the input currents (noise-driven) rather than an increase in the mean input [<xref ref-type="bibr" rid="c90">90</xref>]. This was also reported for a ring-attractor network (with distance-dependent connections between all populations), where facilitation and depression are crucial for irregularity of neuronal activity in the self-sustained state [<xref ref-type="bibr" rid="c45">45</xref>]. Application of our approach to these setups is left for future work.</p>
</sec>
</sec>
<sec id="s4">
<title>Materials and methods</title>
<sec id="s4a">
<title>Analysis of drift &#x0026; diffusion with STP</title>
<p>For the following, we define a concatenated 3 &#x00B7; <italic>N</italic> dimensional column vector of state variables <bold>y</bold> &#x003D; (<bold>s</bold><sup><italic>T</italic></sup>, <bold>u</bold><sup><italic>T</italic></sup>, <bold>x</bold><sup><italic>T</italic></sup>)<sup><italic>T</italic></sup> of the system <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>. Given a (numerical) solution of the stable firing rate profile <inline-formula><alternatives><inline-graphic xlink:href="424515_inline25.gif"/></alternatives></inline-formula> we can calculate the stable fixed point of this system by setting the l.h.s. of <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref> to zero. This yields steady-state solutions for the synaptic activations, facilitation and depression variables <bold>y</bold><sub>0</sub> &#x003D; (<bold>s</bold><sub>0</sub>, <bold>u</bold><sub>0</sub>, <bold>x</bold><sub>0</sub>):
<disp-formula id="eqn12">
<alternatives>
<graphic xlink:href="424515_eqn12.gif"/>
</alternatives>
</disp-formula></p>
<p>We then linearize the system <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref> at the fixed point <bold>y</bold><sub>0</sub>, introducing a change of variables consisting of perturbations around the fixed point: <bold>y</bold> &#x003D; <bold>y</bold><sub>0</sub> &#x002B; <italic>&#x03B4;</italic><bold>y</bold> &#x003D; <bold>y</bold><sub>0</sub> &#x002B; (<italic>&#x03B4;</italic><bold>s</bold><sup><italic>T</italic></sup>, <italic>&#x03B4;</italic><bold>u</bold><sup><italic>T</italic></sup>, <italic>&#x03B4;</italic><bold>x</bold><sup><italic>T</italic></sup>) and <italic>&#x03D5;</italic><sub><italic>i</italic></sub> &#x003D; <italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub> &#x002B; <italic>&#x03B4;&#x03D5;<sub>i</sub></italic>. To reach a self-consistent linear system, we further assume a separation of time scales between the neuronal dynamics and the synaptic variables, in that the neuronal firing rate changes as an immediate function of the (slow) input. This allows replacing <inline-formula><alternatives><inline-graphic xlink:href="424515_inline26.gif"/></alternatives></inline-formula>, where we introduce the shorthand <inline-formula><alternatives><inline-graphic xlink:href="424515_inline27.gif"/></alternatives></inline-formula>. Finally, keeping only linear orders in all perturbations, we arrive at the linearized system equivalent of <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>:
<disp-formula id="eqn13">
<alternatives>
<graphic xlink:href="424515_eqn13.gif"/>
</alternatives>
</disp-formula></p>
<p>Here, dots between vectors indicate element-wise multiplication, the operator <italic>D</italic>: &#x211D;<sup><italic>n</italic></sup> &#x2192; &#x211D;<sup><italic>n</italic>&#x00D7;<italic>n</italic></sup> creates diagonal matrices from vectors, and <italic>W</italic> &#x003D; (<italic>w<sub>ij</sub></italic>) is the synaptic weight matrix of the network.</p>
<sec id="s4a1">
<title>Projection of dynamics onto the attractor manifold</title>
<p>To project the dynamical system <xref ref-type="disp-formula" rid="eqn13">Eq. (13)</xref> onto movement of the center position <italic>&#x03C6;</italic> of the firing rate profile, we assume that <italic>N</italic> is large enough to treat the center position <italic>&#x03C6;</italic> as a continuous variable. We also assume that the network implements a ring-attractor: the system dynamics are such that the firing rate profile <inline-formula><alternatives><inline-graphic xlink:href="424515_inline28.gif"/></alternatives></inline-formula> can be freely shifted to different positions along the ring, changing the center position <italic>&#x03C6;</italic>, while retaining the same shape. All other possible directions of change in this system are assumed to be constrained by the system dynamics. In the system at hand, this implies that the matrix <italic>K</italic> of <xref ref-type="disp-formula" rid="eqn13">Eq. (13)</xref>, which captures the linearized dynamics around any of these fixed points, will have a <italic>zero eigenvalue</italic> corresponding to the eigenvector of a change of the dynamical variables under a change of position <italic>&#x03C6;</italic>, while all other eigenvalues are negative [<xref ref-type="bibr" rid="c39">39</xref>].</p>
<p>Formally, the column eigenvector to the eigenvalue 0 is given by changes in the state variables as the bump center position <italic>&#x03C6;</italic> is translated along the manifold:
<disp-formula id="eqn14">
<alternatives>
<graphic xlink:href="424515_eqn14.gif"/>
</alternatives>
</disp-formula></p>
<p>Let <italic>e<sub>l</sub></italic> be the associated row left-eigenvector (also to eigenvalue 0) of <italic>K</italic>, normalized such that:
<disp-formula id="eqn15">
<alternatives>
<graphic xlink:href="424515_eqn15.gif"/>
</alternatives>
</disp-formula></p>
<p>In Section 1 of <xref ref-type="sec" rid="s5a">S1 Supporting Information</xref>, we show that the eigenvector <italic>e<sub>l</sub></italic> projects the system <xref ref-type="disp-formula" rid="eqn13">Eq. (13)</xref> onto dynamics of of the center position:
<disp-formula id="eqn16">
<alternatives>
<graphic xlink:href="424515_eqn16.gif"/>
</alternatives>
</disp-formula></p>
<p>Under the linearized ring-attractor dynamics <italic>K</italic>, the center position is thus not subject to any dynamics, making it susceptible to any displacements by noise.</p>
</sec>
<sec id="s4a2">
<title>Calculation of the left eigenvector <italic>e<sub>l</sub></italic></title>
<p>If the matrix <italic>K</italic> is symmetric, the left and right eigenvectors <italic>e<sub>l</sub></italic> and <italic>e<sub>r</sub></italic> for the same eigenvalue 0 are the transpose of each other. Unfortunately, here this is not the case (see <xref ref-type="disp-formula" rid="eqn13">Eq. (13)</xref>), and we need to compute the unknown vector <italic>e<sub>l</sub></italic>, which will depend on the coefficients of the known vector <italic>e<sub>r</sub></italic>. In particular, we look for a parametrized vector <bold>y</bold>&#x2032;(<bold>y</bold>) &#x003D; (<bold>t</bold><sup><italic>T</italic></sup>(<bold>y</bold>), <bold>v</bold><sup><italic>T</italic></sup>(<bold>y</bold>), <bold>z</bold><sup><italic>T</italic></sup>(<bold>y</bold>))<sup><italic>T</italic></sup> that for <bold>y</bold> &#x003D; <italic>e<sub>r</sub></italic> fulfills the transposed eigenvalue equation of the left eigenvector:
<disp-formula id="eqn17">
<alternatives>
<graphic xlink:href="424515_eqn17.gif"/>
</alternatives>
</disp-formula></p>
<p>In Section 2 of <xref ref-type="sec" rid="s5a">S1 Supporting Information</xref>, we derive variables <bold>y</bold>&#x2032; that fulfill the transposed dynamics <bold>&#x1E8F;</bold>&#x2032; &#x003D; <italic>K<sup><italic>T</italic></sup></italic><bold>y</bold>&#x2032; and for which it holds that <bold>&#x1E8F;</bold>&#x2032;(<italic>e<sub>r</sub></italic>) &#x003D; 0, thus fulfilling the condition <xref ref-type="disp-formula" rid="eqn17">Eq. (17)</xref>. In this case we know that (due to uniqueness of the 1-dimensional eigenspace associated to the 0 eigenvalue) the vector <bold>y</bold>&#x2032;<sup><italic>T</italic></sup> is proportional to <italic>e<sub>l</sub></italic>:
<disp-formula id="eqn18">
<alternatives>
<graphic xlink:href="424515_eqn18.gif"/>
</alternatives>
</disp-formula>
where <italic>S</italic> is a proportionality constant and <inline-formula><alternatives><inline-graphic xlink:href="424515_inline29.gif"/></alternatives></inline-formula> is the change of the steady-state input arriving at neuron <italic>i</italic> under shifts of the center position <italic>&#x03C6;</italic>.</p>
<p>Finally, the proportionality constant <italic>S</italic> can be calculated by using <xref ref-type="disp-formula" rid="eqn18">Eq. (18)</xref> in <xref ref-type="disp-formula" rid="eqn15">Eq. (15)</xref> (see Section 3 of <xref ref-type="sec" rid="s5a">S1 Supporting Information</xref> for details):
<disp-formula id="eqn19">
<alternatives>
<graphic xlink:href="424515_eqn19.gif"/>
</alternatives>
</disp-formula>
where <inline-formula><alternatives><inline-graphic xlink:href="424515_inline30.gif"/></alternatives></inline-formula> is the linear change of the firing rate of neuron <italic>i</italic> at its steady-state input <italic>J</italic><sub>0,<italic>i</italic></sub>.</p>
</sec>
<sec id="s4a3">
<title>Diffusion</title>
<p>To be able to describe diffusion on the continuous attractor, we need to extend the model by a treatment of the noise induced into the system through the variable process of neuronal spike emission. Starting from <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>, we assume that neurons <italic>i</italic> fire according to independent Poisson processes <inline-formula><alternatives><inline-graphic xlink:href="424515_inline31.gif"/></alternatives></inline-formula>, where <italic>t<sub>i,k</sub></italic> is a Poisson point process with time-dependent rate <italic>&#x03D5;<sub>i</sub></italic>, The variability of the point process <italic>&#x03BE;<sub>i</sub></italic>(<italic>t</italic>) introduces noise in the synaptic variables. By neglecting the shot-noise (jump-like) nature of this process, we can capture the neurally induced variability simply as white noise with variance proportional to the incoming firing rates [<xref ref-type="bibr" rid="c47">47</xref>], <inline-formula><alternatives><inline-graphic xlink:href="424515_inline32.gif"/></alternatives></inline-formula>, where <italic>&#x03B7;<sub>i</sub></italic> are white Gaussian noise processes with mean &#x2329;<italic>&#x03B7;<sub>i</sub></italic>&#x232A; &#x003D; 0, and correlation function &#x2329;<italic>&#x03B7;<sub>i</sub></italic>(<italic>t</italic>)<italic>&#x03B7;<sub>j</sub></italic>(<italic>t</italic>&#x2032;)&#x232A; &#x003D; <italic>&#x03B4;</italic>(<italic>t</italic> &#x2212; <italic>t</italic>&#x2032;)<italic>&#x03B4;<sub>ij</sub></italic>. This model of <italic>&#x03BE;<sub>i</sub></italic>(<italic>t</italic>) preserves the mean and the auto-correlation function of the original Poisson processes. Here, we introduce diffusive noise for each synaptic variable separately, but later average their <italic>linear</italic> contributions over the large population, when projecting onto movement along the continuous manifold (see below, and also [<xref ref-type="bibr" rid="c39">39</xref>, Supplementary Material] for a discussion).</p>
<p>Substituting the noisy processes <italic>&#x03BE;<sub>i</sub></italic>(<italic>t</italic>) for <italic>&#x03D5;<sub>i</sub></italic>(<italic>t</italic>) in <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref> results in the following system of 3 &#x00B7; <italic>N</italic> coupled Ito-SDEs:
<disp-formula id="eqn20">
<alternatives>
<graphic xlink:href="424515_eqn20.gif"/>
</alternatives>
</disp-formula></p>
<p>Note that the noise inputs <italic>&#x03B7;<sub>i</sub></italic> to the synaptic variables for neuron <italic>i</italic> are all identical, since they result from the same presynaptic spike train.</p>
<p>Linearizing this system around the noise-free steady-state <xref ref-type="disp-formula" rid="eqn12">Eq. (12)</xref> and considering only the unperturbed noise (we neglect multiplicative noise terms by replacing the terms <inline-formula><alternatives><inline-graphic xlink:href="424515_inline33.gif"/></alternatives></inline-formula>), we arrive at the linearized system equivalent of <xref ref-type="disp-formula" rid="eqn20">Eq. (20)</xref>:
<disp-formula id="eqn21">
<alternatives>
<graphic xlink:href="424515_eqn21.gif"/>
</alternatives>
</disp-formula></p>
<p>Note that the same vector of white noises <inline-formula><alternatives><inline-graphic xlink:href="424515_inline34.gif"/></alternatives></inline-formula> appears three times.</p>
<p>Left-multiplying this system with the eigenvector e<sub>l</sub> yields a stochastic differential equation for the center position (cf. <xref ref-type="disp-formula" rid="eqn16">Eq. (16)</xref>):
<disp-formula id="eqn22">
<alternatives>
<graphic xlink:href="424515_eqn22.gif"/>
</alternatives>
</disp-formula></p>
<p>Through the normalization by <italic>S</italic> (<xref ref-type="disp-formula" rid="eqn18">Eq. (18)</xref>), which sums over all neurons, the individual contributions <italic>e<sub>l,k</sub></italic> become small as the number of neurons <italic>N</italic> increases (this scaling is made explicit in <italic><xref ref-type="sec" rid="s4c4">System size scaling</xref></italic>). Thus, for large networks we average the small contributions of many single noise sources, which validates the diffusion approximation above.</p>
<p>In Section 4 of <xref ref-type="sec" rid="s5a">S1 Supporting Information</xref>, we show that we can rewrite <xref ref-type="disp-formula" rid="eqn22">Eq. (22)</xref> by introducing a single Gaussian white noise process with intensity <italic>B</italic> (<xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref> of the main text), that matches the correlation function of the summed noises:
<disp-formula id="eqn23">
<alternatives>
<graphic xlink:href="424515_eqn23.gif"/>
</alternatives>
</disp-formula>
where <italic>&#x03B7;</italic> is a white Gaussian noise process with &#x2329;<italic>&#x03B7;</italic>&#x232A; &#x003D; 0 and &#x2329;<italic>&#x03B7;</italic>(<italic>t</italic>)<italic>&#x03B7;</italic>(<italic>t</italic>&#x2032;)&#x232A; &#x003D; <italic>&#x03B4;</italic>(<italic>t</italic> &#x2212; <italic>t</italic>&#x2032;). Note, that the value of <italic>B</italic> is the same under changes of the center position <italic>&#x03C6;</italic>: these correspond to index-shifting (mod <italic>N</italic>) all vectors in <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>, which leaves the sum invariant.</p>
</sec>
<sec id="s4a4">
<title>Drift</title>
<p>While the linearization and diffusion coefficient calculated above are invariant with respect to shifts of the bump center, the directed drift introduced by frozen variability will depend on the bump center position <italic>&#x03C6;</italic>. Let the center position of the bump be <inline-formula><alternatives><inline-graphic xlink:href="424515_inline35.gif"/></alternatives></inline-formula> (for 0 &#x2264; <italic>k</italic> &#x003C; <italic>N</italic>) with the corresponding firing rate profile <inline-formula><alternatives><inline-graphic xlink:href="424515_inline36.gif"/></alternatives></inline-formula>, which is an index-shifted version of the <italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub>(<italic>&#x03C6;<sub>k</sub></italic>) &#x003D; <italic>&#x03D5;</italic><sub>0,<italic>i</italic>&#x2212;<italic>k</italic></sub>(<italic>&#x03C6;</italic><sub>0</sub>).</p>
<p>To calculate the directed drift for a given position <italic>&#x03C6;<sub>k</sub></italic>, we first rotate the system such that the steady-state firing rate profile is again centered at <italic>&#x03C6;</italic><sub>0</sub> &#x003D; &#x2212;<italic>&#x03C0;</italic>. Then, we assume that the effect of all sources of frozen variability can be expressed by a vector of small firing rate perturbations <inline-formula><alternatives><inline-graphic xlink:href="424515_inline37.gif"/></alternatives></inline-formula> to the steady-state firing rate profile:
<disp-formula id="eqn24">
<alternatives>
<graphic xlink:href="424515_eqn24.gif"/>
</alternatives>
</disp-formula></p>
<p>These firing rate perturbations stem from any deviation of the neural system from the &#x201C;baseline&#x201D; case and change with each center position <italic>&#x03C6;<sub>k</sub></italic> of the bump. The resulting drift field will thus depend on the center position around which we linearize the synaptic dynamics in the following. In <italic><xref ref-type="sec" rid="s4c">Frozen noise</xref></italic> we will calculate the perturbations induced by random network connectivity, as well as heterogeneous leak reversal-potentials in excitatory neurons of the spiking network.</p>
<p>The firing rate perturbations <xref ref-type="disp-formula" rid="eqn24">Eq. (24)</xref> add an additional term in the linearized equations <xref ref-type="disp-formula" rid="eqn21">Eq. (21)</xref>:
<disp-formula id="eqn25">
<alternatives>
<graphic xlink:href="424515_eqn25.gif"/>
</alternatives>
</disp-formula></p>
<p>As before, we left-multiply by the left eigenvector <italic>e<sub>l</sub></italic>, thereby projecting the dynamics onto changes of the center position. This eliminates the linear response kernel <italic>K</italic> and yields a drift-term in the formerly purely diffusive SDE <xref ref-type="disp-formula" rid="eqn23">Eq. (23)</xref> (see Section 5 of <xref ref-type="sec" rid="s5a">S1 Supporting Information</xref> for details):
<disp-formula id="eqn26">
<alternatives>
<graphic xlink:href="424515_eqn26.gif"/>
</alternatives>
</disp-formula></p>
<p>We again assume that the number of neurons <italic>N</italic> is large enough to treat the center position as a continuous variable <italic>&#x03C6;</italic> &#x2208; [&#x2212;<italic>&#x03C0;,&#x03C0;</italic>), allowing us to treat the set of point-wise linearizations of the dynamics at <italic>&#x03C6;<sub>k</sub></italic> as a drift-field <italic>A</italic>(<italic>&#x03C6;</italic>) in <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref> of the main text.</p>
<p>For this, it is important to note that, even for random sources of variability, the first term in the point-wise linearization <xref ref-type="disp-formula" rid="eqn26">Eq. (26)</xref> at each <italic>&#x03C6;<sub>k</sub></italic> will vary nearly continuously with changes in the center position. Intuitively, the sum weighs the vector <inline-formula><alternatives><inline-graphic xlink:href="424515_inline38.gif"/></alternatives></inline-formula> of random numbers with a smooth function of the smoothly varying firing-rate profile <inline-formula><alternatives><inline-graphic xlink:href="424515_inline39.gif"/></alternatives></inline-formula>. Shifts in the center position <italic>&#x03C6;<sub>k</sub></italic> yield (to first order) index-shifts in the vector of random numbers (see <italic><xref ref-type="sec" rid="s4c">Frozen noise</xref></italic>), equivalent to index-shifts of the firing-rate profile. Thus, small changes in center positions will lead to small changes in the summands of <xref ref-type="disp-formula" rid="eqn26">Eq. (26)</xref>. While our results validate the approach, a more rigorous proof of these arguments will be left for future work.</p>
</sec>
</sec>
<sec id="s4b">
<title>Spiking network model</title>
<p>Spiking simulations are based on a variation of a popular ring-attractor model of visuospatial working memory of [<xref ref-type="bibr" rid="c11">11</xref>] (and used with variations in [<xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c32">32</xref>, <xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c46">46</xref>]). The recurrent excitatory connections of the original network model have been simplified, to allow for faster simulation as well as analytical derivations of the recurrent synaptic activation. The implementation details are given below, however the major changes are: 1) all recurrent excitatory conductances are voltage independent; 2) a model of synaptic short-term plasticity via facilitation and depression [<xref ref-type="bibr" rid="c48">48</xref>, <xref ref-type="bibr" rid="c91">91</xref>, <xref ref-type="bibr" rid="c92">92</xref>] is used to dynamically regulate the weights of the incoming spike-trains 3) recurrent excitatory conductances are computed as linear filters of the weighted incoming spike trains instead of the second-order kinetics for NMDA saturation used in [<xref ref-type="bibr" rid="c11">11</xref>].</p>
<sec id="s4b1">
<title>Neuron model</title>
<p>Neurons are modeled by leaky integrate-and-fire dynamics with conductance based synaptic transmission [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c49">49</xref>]. The network consists of recurrently connected populations of <italic>N<sub>E</sub></italic> excitatory and <italic>N<sub>I</sub></italic> inhibitory neurons, both additionally receiving external spiking input with spike times generated by <italic>N</italic><sub>ext</sub> independent, homogeneous Poisson processes, with rates <italic>v<sub>ext</sub></italic>. We assume that external excitatory inputs are mediated by fast AMPA receptors, while, for simplicity, recurrent excitatory currents are mediated only by slower NMDA channels.</p>
<p>The dynamics of neurons in both excitatory and inhibitory populations are governed by the following system of differential equations indexed by <italic>i</italic> &#x2208; &#x007B;0, &#x2026;, <italic>N<sub>E/I</sub></italic> &#x2212; 1&#x007D;:
<disp-formula id="eqn27">
<alternatives>
<graphic xlink:href="424515_eqn27.gif"/>
</alternatives>
</disp-formula>
where <italic>P</italic> &#x2208; &#x007B;L,Ext,I,E&#x007D;, <italic>V</italic> denotes voltages (membrane potential) and <italic>I</italic> denotes currents. Here, <italic>C</italic><sub>m</sub> is the membrane capacitance and <italic>V</italic><sub>L</sub>, <italic>V</italic><sub>E</sub>, <italic>V</italic><sub>I</sub> are the reversal potentials for leak, excitatory currents, and inhibitory currents, respectively. The parameters <italic>g<sub>P</sub></italic> for <italic>P</italic> &#x2208; &#x007B;L,Ext,I,E&#x007D; are fixed scales for leak (L), external input (Ext) and recurrent excitatory (E) and inhibitory (I) synaptic conductances, which are dynamically gated by the unit-less gating variables <inline-formula><alternatives><inline-graphic xlink:href="424515_inline40.gif"/></alternatives></inline-formula>. These gating variables are described in detail below, however we set the leak conductance gating variable to <inline-formula><alternatives><inline-graphic xlink:href="424515_inline41.gif"/></alternatives></inline-formula>. For excitatory neurons, we refer to the excitatory and inhibitory conductance scales by <italic>g</italic><sub>EE</sub> &#x2261; <italic>g</italic><sub>E</sub> and <italic>g</italic><sub>EI</sub> &#x2261; <italic>g</italic><sub>I</sub>, respectively. Similarly, for inhibitory neurons, we refer to the excitatory and inhibitory conductance scales by <italic>g</italic><sub>IE</sub> &#x2261; <italic>g</italic><sub>E</sub> and <italic>g</italic><sub>II</sub> &#x2261; <italic>g</italic><sub>I</sub>, respectively.</p>
<p>The model neuron dynamics (<xref ref-type="disp-formula" rid="eqn27">Eq. 27</xref>) are integrated until their voltage reaches a threshold <italic>V</italic><sub>thr</sub>. At any such time, the respective neuron emits a spike and its membrane potential is reset to the value <italic>V</italic><sub>res</sub>. After each spike, voltages are clamped to <italic>V</italic><sub>res</sub> for a refractory period of <italic>&#x03C4;</italic><sub>ref</sub>. See the Tables in S1 Table and S2 Table for parameter values used in simulations.</p>
</sec>
<sec id="s4b2">
<title>Synaptic gating variables and short-term plasticity</title>
<p>The unit-less synaptic gating variables <inline-formula><alternatives><inline-graphic xlink:href="424515_inline42.gif"/></alternatives></inline-formula> for <italic>P</italic> &#x2208; &#x007B;Ext,I&#x007D; (external and inhibitory currents) are exponential traces of the spike trains of all presynaptic neurons <italic>j</italic> with firing times <italic>t<sub>j</sub></italic>:
<disp-formula id="eqn28">
<alternatives>
<graphic xlink:href="424515_eqn28.gif"/>
</alternatives>
</disp-formula>
where pre(P) indicates all neurons presynaptic to the neuron <italic>i</italic> for the the connection type <italic>P</italic>. The factors <inline-formula><alternatives><inline-graphic xlink:href="424515_inline43.gif"/></alternatives></inline-formula> are unit-less synaptic efficacies for the connection from neuron <italic>j</italic> to neuron <italic>i</italic>. For the excitatory gating variables of inhibitory neurons <inline-formula><alternatives><inline-graphic xlink:href="424515_inline44.gif"/></alternatives></inline-formula> (IE denotes connections from E to <italic>I</italic> neurons) we also use the linear model of <xref ref-type="disp-formula" rid="eqn28">Eq. (28)</xref> with time constant <italic>&#x03C4;<sub>IE</sub></italic> &#x003D; <italic>&#x03C4;<sub>E</sub></italic>.</p>
<p>For excitatory to excitatory conductances, we use a well established model of synaptic short-term plasticity (STP) [<xref ref-type="bibr" rid="c48">48</xref>, <xref ref-type="bibr" rid="c91">91</xref>, <xref ref-type="bibr" rid="c92">92</xref>] which provides dynamic scaling of synaptic efficacies depending on presynaptic firing. This yields two additional dynamical variables, the facilitating synaptic efficacy <italic>u<sub>j</sub></italic>(<italic>t</italic>), as well as the fraction of available synaptic resources <italic>x<sub>j</sub></italic>(<italic>t</italic>) of the outgoing connections of a presynaptic neuron <italic>j</italic>, which are implemented according to the following differential equation:
<disp-formula id="eqn29">
<alternatives>
<graphic xlink:href="424515_eqn29.gif"/>
</alternatives>
</disp-formula></p>
<p>Here, the indices <inline-formula><alternatives><inline-graphic xlink:href="424515_inline45.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="424515_inline46.gif"/></alternatives></inline-formula> indicate that for the incremental update of the variables upon spike arrival, the values of the respective variables immediately before the spike arrival are used [<xref ref-type="bibr" rid="c92">92</xref>]. The variable <italic>U</italic> appears in the equation for <italic>u</italic>(<italic>t</italic>) both as the steady-state value in the absence of spikes and as a scale for the per-spike &#x2013; intuitively, <italic>U</italic> is thus the first value of <italic>u</italic>(<italic>t</italic>) transmitted on a spike after inactivity of the connection [<xref ref-type="bibr" rid="c48">48</xref>].</p>
<p>The dynamics of recurrent excitatory-to-excitatory transmission with STP are then given by gating variables that linearly filter the incoming spikes, which are scaled by facilitation and depression:
<disp-formula id="eqn30">
<alternatives>
<graphic xlink:href="424515_eqn30.gif"/>
</alternatives>
</disp-formula>
<disp-formula id="eqn31">
<alternatives>
<graphic xlink:href="424515_eqn31.gif"/>
</alternatives>
</disp-formula></p>
<p>Here, pre(EE) indicates all excitatory neurons that make synaptic connections to the neuron <italic>i</italic>. See the Table in S2 Table for synaptic parameters used in simulations.</p>
<p>This makes the system <xref ref-type="disp-formula" rid="eqn29">Eqs. (29)</xref> and <xref ref-type="disp-formula" rid="eqn31">(31)</xref> a spiking variant of the rate-based dynamics of <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref>, and <inline-formula><alternatives><inline-graphic xlink:href="424515_inline47.gif"/></alternatives></inline-formula> a variable related to the input <italic>J<sub>i</sub></italic> (cf. <xref ref-type="disp-formula" rid="eqn2">Eq. (2)</xref>). In <italic><xref ref-type="sec" rid="s4b4">Firing rate approximation</xref></italic> we will make this link explicit.</p>
</sec>
<sec id="s4b3">
<title>Network connectivity</title>
<p>All connections except for the recurrent excitatory connections are all-to-all and uniform, with unit-less connection strengths set to <inline-formula><alternatives><inline-graphic xlink:href="424515_inline48.gif"/></alternatives></inline-formula> and for inhibitory neurons additionally <inline-formula><alternatives><inline-graphic xlink:href="424515_inline49.gif"/></alternatives></inline-formula>. The recurrent excitatory connections are distance-dependent and symmetric. Each neuron of the excitatory population with index <italic>i</italic> &#x2208; &#x007B;0, &#x2026;, <italic>N<sub>E</sub></italic> &#x2212; 1&#x007D; is assigned an angular position <inline-formula><alternatives><inline-graphic xlink:href="424515_inline50.gif"/></alternatives></inline-formula>. Recurrent excitatory connection weights <inline-formula><alternatives><inline-graphic xlink:href="424515_inline51.gif"/></alternatives></inline-formula> from neuron <italic>j</italic> to neuron <italic>i</italic> are then given by the Gaussian function <italic>w</italic><sup>EE</sup>(<italic>&#x03B8;</italic>) as (see the Table in S2 Table for parameters used in simulations):
<disp-formula id="eqn32">
<alternatives>
<graphic xlink:href="424515_eqn32.gif"/>
</alternatives>
</disp-formula></p>
<p>Additionally, for each neuron we keep the integral over all recurrent connection weights normalized, resulting in the normalization condition <inline-formula><alternatives><inline-graphic xlink:href="424515_inline52.gif"/></alternatives></inline-formula>. This normalization ensures that varying the maximum weight <italic>w</italic><sub>&#x002B;</sub> will not change the total recurrent excitatory input if all excitatory neurons fire at the same rate. Here, we choose <italic>w</italic><sub>&#x002B;</sub> as a free parameter constraining the baseline connection weight to:
<disp-formula id="ueqn1">
<alternatives>
<graphic xlink:href="424515_ueqn1.gif"/>
</alternatives>
</disp-formula></p>
</sec>
<sec id="s4b4">
<title>Firing rate approximation</title>
<p>We first replace the synaptic activation variables <italic>s<sup>P</sup></italic>(<italic>V,t</italic>) for <italic>P</italic> &#x2208; &#x007B;I, ext&#x007D; by their expectation values under input with Poisson statistics. We assume that the inhibitory population fires at rates <italic>v<sub>I</sub></italic>. For the linear synapses this yields
<disp-formula id="eqn33">
<alternatives>
<graphic xlink:href="424515_eqn33.gif"/>
</alternatives>
</disp-formula>
<disp-formula id="eqn34">
<alternatives>
<graphic xlink:href="424515_eqn34.gif"/>
</alternatives>
</disp-formula></p>
<p>For the recurrent excitatory-to-excitatory synapses with short-term plasticity, we set the differential <xref ref-type="disp-formula" rid="eqn29">equations (29)</xref> to zero, and also average them over the Poisson statistics. Akin to the &#x201C;mean-field&#x201D; model of [<xref ref-type="bibr" rid="c48">48</xref>], we average the steady-state values of facilitation and depression separately over the Poisson statistics. This implicitly assumes that facilitation and depression are statistically independent, with respect to the distributions of spike times &#x2013; while this is not strictly true, the approximations work well, as has been previously reported [<xref ref-type="bibr" rid="c48">48</xref>]. This allows a fairly straightforward evaluation of the mean steady-state value of the combined facilitation and depression variables &#x2329;<italic>u<sub>j</sub>x<sub>j</sub></italic>&#x232A;, under the assumption that the neuron <italic>j</italic> fires at a mean rate <italic>v<sub>j</sub></italic> with Poisson statistics, and yields rate approximations of the steady-state values similar to <xref ref-type="disp-formula" rid="eqn12">Eq. (12)</xref>:
<disp-formula id="eqn35">
<alternatives>
<graphic xlink:href="424515_eqn35.gif"/>
</alternatives>
</disp-formula></p>
<p>We now assume that the excitatory population of <italic>N<sub>E</sub></italic> neurons fires at the steady-state rates <italic>&#x03D5;<sub>j</sub></italic> (0 &#x2264; <italic>j</italic> &#x003C; <italic>N</italic>). To calculate the synaptic activation of excitatory-to-excitatory connections <inline-formula><alternatives><inline-graphic xlink:href="424515_inline53.gif"/></alternatives></inline-formula>, we set <xref ref-type="disp-formula" rid="eqn30">Eq. (30)</xref> to zero, and average over Poisson statistics (again neglecting correlations), which yields &#x2329;<italic>s<sub>j</sub></italic>&#x232A; &#x003D; <italic>&#x03C4;<sub>E</sub></italic>&#x2329;<italic>u<sub>j</sub>x<sub>j</sub></italic>&#x232A;<italic>&#x03D5;<sub>j</sub></italic> and <inline-formula><alternatives><inline-graphic xlink:href="424515_inline54.gif"/></alternatives></inline-formula>. Let the the normalized steady-state input <italic>J<sub>i</sub></italic> be:
<disp-formula id="eqn36">
<alternatives>
<graphic xlink:href="424515_eqn36.gif"/>
</alternatives>
</disp-formula></p>
<p>The steady-state input <xref ref-type="disp-formula" rid="eqn36">Eq. (36)</xref> links the general framework of <xref ref-type="disp-formula" rid="eqn2">Eq. (2)</xref> to the spiking network. The additional factor 1/<italic>N</italic><sub>E</sub> is introduced to make the scaling of the excitatory-to-excitatory conductance with the size of the excitatory population <italic>N</italic><sub>E</sub> explicit, which will be used in <italic><xref ref-type="sec" rid="s4c4">System size scaling</xref></italic>. To see this, we assume that the excitatory conductance scale of excitatory neurons gEE is scaled such that the total conductance is invariant under changes of <italic>N<sub>E</sub></italic> [<xref ref-type="bibr" rid="c93">93</xref>]: <inline-formula><alternatives><inline-graphic xlink:href="424515_inline55.gif"/></alternatives></inline-formula>, for some fixed value <inline-formula><alternatives><inline-graphic xlink:href="424515_inline56.gif"/></alternatives></inline-formula>. This yields the total excitatory-to-excitatory conductance <inline-formula><alternatives><inline-graphic xlink:href="424515_inline57.gif"/></alternatives></inline-formula> with <italic>J<sub>i</sub></italic> as introduced above, where the scaling with <italic>N<sub>E</sub></italic> is now shifted to the input variable <italic>J<sub>i</sub></italic>.</p>
<p>For the synaptic activation of excitatory to inhibitory connections, we get the mean activations:
<disp-formula id="eqn37">
<alternatives>
<graphic xlink:href="424515_eqn37.gif"/>
</alternatives>
</disp-formula></p>
<p>We then follow [<xref ref-type="bibr" rid="c49">49</xref>] to reduce the differential equations of <xref ref-type="disp-formula" rid="eqn27">Eq. (27)</xref> to a dimensionless form. The main difference consists in the absence of the voltage dependent NMDA conductance, which is achieved by setting the two associated parameters <italic>&#x03B2;</italic> &#x2192; 0, <italic>&#x03B3;</italic> &#x2192; 0 in [<xref ref-type="bibr" rid="c49">49</xref>], to arrive at:
<disp-formula id="eqn38">
<alternatives>
<graphic xlink:href="424515_eqn38.gif"/>
</alternatives>
</disp-formula>
<disp-formula id="eqn39">
<alternatives>
<graphic xlink:href="424515_eqn39.gif"/>
</alternatives>
</disp-formula>
<disp-formula id="eqn40">
<alternatives>
<graphic xlink:href="424515_eqn40.gif"/>
</alternatives>
</disp-formula>
<disp-formula id="eqn41">
<alternatives>
<graphic xlink:href="424515_eqn41.gif"/>
</alternatives>
</disp-formula>
where <inline-formula><alternatives><inline-graphic xlink:href="424515_inline58.gif"/></alternatives></inline-formula> are effective timescales of external and inhibitory inputs, and <inline-formula><alternatives><inline-graphic xlink:href="424515_inline59.gif"/></alternatives></inline-formula> is a dimensionless scale for the excitatory conductance. Here, <italic>&#x03BC;<sub>i</sub></italic> is the bias of the membrane potential due to synaptic inputs, and <italic>&#x03C3;<sub>i</sub></italic> measures the scale of fluctuations in the membrane potential due to random spike arrival approximated by the Gaussian process <italic>&#x03B7;<sub>i</sub></italic>.</p>
<p>The mean firing rates <italic>F</italic> and mean voltages &#x2329;<italic>V<sub>i</sub></italic>&#x232A; of populations of neurons governed by this type of differential equation can then be approximated by:
<disp-formula id="eqn42">
<alternatives>
<graphic xlink:href="424515_eqn42.gif"/>
</alternatives>
</disp-formula>
<disp-formula id="eqn43">
<alternatives>
<graphic xlink:href="424515_eqn43.gif"/>
</alternatives>
</disp-formula>
<disp-formula id="eqn44">
<alternatives>
<graphic xlink:href="424515_eqn44.gif"/>
</alternatives>
</disp-formula>
<disp-formula id="eqn45">
<alternatives>
<graphic xlink:href="424515_eqn45.gif"/>
</alternatives>
</disp-formula></p>
</sec>
<sec id="s4b5">
<title>Derivatives of the rate prediction</title>
<p>Here we calculate derivatives of the input-output relation (<xref ref-type="disp-formula" rid="eqn42">Eq. (42)</xref>) that will be used below in <italic><xref ref-type="sec" rid="s4c">Frozen noise</xref></italic>.</p>
<p>The expressions for drift and diffusion (see <italic><xref ref-type="sec" rid="s4a">Analysis of drift &#x0026; diffusion with STP</xref></italic>) contain the derivative <inline-formula><alternatives><inline-graphic xlink:href="424515_inline60.gif"/></alternatives></inline-formula> of the input-output relation <italic>F</italic> (<xref ref-type="disp-formula" rid="eqn42">Eq. (42)</xref>) with respect to the recurrent excitatory input; <italic>J<sub>i</sub></italic>. Note, that <italic>F</italic> depends on <italic>J<sub>i</sub></italic> through all three arguments <italic>&#x03BC;<sub>i</sub>,&#x03C3;<sub>i</sub></italic> and <italic>&#x03C4;<sub>i</sub></italic>. First, we define <italic>X</italic>(<italic>u</italic>) &#x2261; exp(<italic>u</italic><sup>2</sup>) [1 &#x002B; erf(<italic>u</italic>)], and the shorthand <italic>F<sub>i</sub></italic> &#x003D; <italic>F</italic> [<italic>&#x03BC;<sub>i</sub>,&#x03C3;<sub>i</sub>,&#x03C4;<sub>i</sub></italic>]. The derivative can then be readily evaluated as (to shorten the notation in the following, we skip noting the evaluation points for derivatives in the following):
<disp-formula id="eqn46">
<alternatives>
<graphic xlink:href="424515_eqn46.gif"/>
</alternatives>
</disp-formula>
where <italic>&#x03B1;</italic>/<italic>&#x03B2;</italic> stands as a placeholder for either function, and the expressions for <italic>&#x03B1;</italic> and <italic>&#x03B2;</italic> are given in <xref ref-type="disp-formula" rid="eqn43">Eqs. (43)</xref>-<xref ref-type="disp-formula" rid="eqn44">(44)</xref>.</p>
<p>A second expression involving the derivative of <xref ref-type="disp-formula" rid="eqn42">Eq. (42)</xref> is <inline-formula><alternatives><inline-graphic xlink:href="424515_inline61.gif"/></alternatives></inline-formula> which appears in the theory when estimating firing rate perturbations caused by frozen heterogeneities in the leak potentials of excitatory neurons (see <xref ref-type="disp-formula" rid="eqn54">Eq. (54)</xref>). The resulting derivatives are almost similar, which can be seen by the fact that replacing <inline-formula><alternatives><inline-graphic xlink:href="424515_inline62.gif"/></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="eqn27">Eq. (27)</xref> only leads to an additional term <inline-formula><alternatives><inline-graphic xlink:href="424515_inline63.gif"/></alternatives></inline-formula> in <xref ref-type="disp-formula" rid="eqn39">Eq. (39)</xref>. Thus, for neuron <italic>i</italic> the derivative can be evaluated to
<disp-formula id="eqn47">
<alternatives>
<graphic xlink:href="424515_eqn47.gif"/>
</alternatives>
</disp-formula></p>
<p>In practice, given a vector <italic>&#x03D5;</italic><sub><italic>i</italic>,0</sub> of firing rates in the attractor state, as well as the mean firing rate of inhibitory neurons <italic>v<sub>I</sub></italic>, we evaluate the right hand side of <xref ref-type="disp-formula" rid="eqn46">Eq. (46)</xref> and <xref ref-type="disp-formula" rid="eqn47">Eq. (47)</xref> by replacing <italic>F<sub>i</sub></italic> &#x2192; <italic>&#x03D5;</italic><sub><italic>i</italic>,0</sub>. This allows efficiently calculating the derivatives without having to perform any numerical integration. The two terms will be exactly equal if <italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub> is a self-consistent solution of <xref ref-type="disp-formula" rid="eqn42">Eq. (42)</xref> for firing rates of the excitatory neurons across the network. We used numerical estimates of <italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub> and <italic>v<sub>I</sub></italic> that were measured from simulations and were very close to firing-rate predictions for all networks we investigated.</p>
</sec>
<sec id="s4b6">
<title>Optimization of network parameters</title>
<p>We used an optimization procedure [<xref ref-type="bibr" rid="c94">94</xref>] to retune network parameters to produce approximately similar bump shapes as the parameters of short-term plasticity are varied. Briefly, we replace the network activity <italic>&#x03D5;<sub>j</sub></italic> in the total input <italic>J<sub>i</sub></italic> of <xref ref-type="disp-formula" rid="eqn36">Eq. (36)</xref> by a parametrization
<disp-formula id="eqn48">
<alternatives>
<graphic xlink:href="424515_eqn48.gif"/>
</alternatives>
</disp-formula></p>
<p>Approximating sums <inline-formula><alternatives><inline-graphic xlink:href="424515_inline64.gif"/></alternatives></inline-formula> with integrals <inline-formula><alternatives><inline-graphic xlink:href="424515_inline65.gif"/></alternatives></inline-formula> we arrive at
<disp-formula id="ueqn2">
<alternatives>
<graphic xlink:href="424515_ueqn2.gif"/>
</alternatives>
</disp-formula>
where <italic>J<sub>i</sub></italic>(<italic>g</italic>) indicates that the total input depends on the parameters <italic>g</italic><sub>0</sub>, <italic>g</italic><sub>1</sub>, <italic>g<sub>&#x03C3;</sub>, g<sub>r</sub></italic> of the parametrization <italic>g</italic>.</p>
<p>We then substitute this relation in <xref ref-type="disp-formula" rid="eqn42">Eq. (42)</xref> to arrive at a self-consistency relation between the parametrized network activity <italic>g</italic>(<italic>&#x03B8;<sub>i</sub></italic>) at the position of neuron <italic>i</italic> and the firing-rate <italic>F</italic> predicted by the theory:
<disp-formula id="eqn49">
<alternatives>
<graphic xlink:href="424515_eqn49.gif"/>
</alternatives>
</disp-formula></p>
<p>As for the input <italic>J<sub>i</sub></italic>(<italic>g</italic>), we indicate the dependence of quantities on the parameters of the parametrization <italic>g</italic> by appending (<italic>g</italic>). The explicit dependence of the voltage &#x2329;<italic>V<sub>i</sub></italic>&#x232A;(<italic>g</italic>) on <italic>g</italic> is obtained by additionally substituting <italic>&#x03D5;<sub>i</sub></italic> &#x2192; <italic>g</italic>(<italic>&#x03B8;<sub>i</sub></italic>) in <xref ref-type="disp-formula" rid="eqn45">Eq. (45)</xref>.</p>
<p>We then optimized networks to fulfill <xref ref-type="disp-formula" rid="eqn49">Eq. (49)</xref>. First, we imposed the following targets for the parameters of <italic>g</italic>: <italic>g</italic><sub>0</sub> &#x003D; 0.1Hz, <italic>g</italic><sub>1</sub> &#x003D; 40Hz, <italic>v</italic><sub><italic>E</italic>,basal</sub> &#x003D; 0.5Hz, <italic>v</italic><sub><italic>I</italic>,basal</sub> &#x003D; 3Hz. For all networks we chose <italic>w</italic><sub>&#x002B;</sub> &#x003D; 4.0, <italic>g<sub>r</sub></italic> &#x003D; 2.5. The following parameters were then optimized: <italic>v<sub>I</sub>, g<sub>&#x03C3;</sub>, g</italic><sub>EE</sub> (excitatory conductance <italic>g<sub>E</sub></italic> on excitatory neurons); <italic>g</italic><sub>IE</sub> (excitatory conductance <italic>g</italic><sub>E</sub> on inhibitory neurons); <italic>g</italic><sub>EI</sub> (inhibitory conductance <italic>g<sub>I</sub></italic> on excitatory neurons); <italic>g</italic><sub>II</sub> (inhibitory conductance <italic>g</italic><sub>II</sub> on inhibitory neurons). The basal firing rates (firing rates in the uniform state of the network, prior to being cued) yielded two equations from <xref ref-type="disp-formula" rid="eqn49">Eq. (49)</xref> by setting <italic>w</italic><sub>&#x002B;</sub> &#x003D; 1. This left 4 free parameters, which were constrained by evaluating <xref ref-type="disp-formula" rid="eqn49">Eq. (49)</xref> at 4 points as described in [<xref ref-type="bibr" rid="c94">94</xref>]. The basal firing rates were chosen to be fairly low to make the uniform state more stable (as in [<xref ref-type="bibr" rid="c44">44</xref>]). This procedure does not yield a fixed value for <italic>g<sub>&#x03C3;</sub></italic>, since <italic>g<sub>&#x03C3;</sub></italic> is optimized for and is not set as a target value. We thus iterated the following until a solution was found with <italic>g<sub>&#x03C3;</sub></italic> &#x2248; 0.5: a) change the width of the recurrent weights <italic>w<sub>&#x03C3;</sub></italic>; b) optimize network parameters as described here; c) optimize the expected bump shape for the new network parameters to predict <italic>g<sub>&#x03C3;</sub></italic>. The resulting parameter values are given in Table in S2 Table.</p>
</sec>
</sec>
<sec id="s4c">
<title>Frozen noise</title>
<sec id="s4c1">
<title>Random and heterogeneous connectivity</title>
<p>Introducing random connectivity, we replace the recurrent weights in <xref ref-type="disp-formula" rid="eqn36">Eq. (36)</xref> by:
<disp-formula id="eqn50">
<alternatives>
<graphic xlink:href="424515_eqn50.gif"/>
</alternatives>
</disp-formula></p>
<p>Here, <italic>p<sub>ij</sub></italic> &#x2208; &#x007B;0,1&#x007D; are Bernoulli variables, with <italic>P</italic>(<italic>p<sub>ij</sub></italic> &#x003D; 1) &#x003D; <italic>p</italic>, where the connectivity parameter <italic>p</italic> &#x2208; (0,1] controls the overall sparsity of recurrent excitatory connections. For <italic>p</italic> &#x003D; 1 the entire network is all-to-all connected. Additionally, we provide derivations for additive synaptic heterogeneities <inline-formula><alternatives><inline-graphic xlink:href="424515_inline66.gif"/></alternatives></inline-formula> (as in [<xref ref-type="bibr" rid="c38">38</xref>]), where &#x007B;<italic>&#x03B7;<sub>ij</sub></italic>&#x007C;1 &#x2264; <italic>i, j</italic> &#x2264; <italic>N<sub>E</sub></italic>&#x007D; are independent, normally distributed random variables with zero mean and unit variance. We did not investigate this type of heterogeneity in the main text, since increasing lead <italic>&#x03C3;<sub>w</sub></italic> to a loss of the attractor state before creating large enough directed drifts to be comparable to the other sources of frozen noise considered here &#x2013; most of the small effects were &#x201C;hidden&#x201D; behind diffusive displacement [<xref ref-type="bibr" rid="c81">81</xref>]. Nevertheless, we included this case in the analysis here for completeness.</p>
<p>Let the center position of the bump be <inline-formula><alternatives><inline-graphic xlink:href="424515_inline67.gif"/></alternatives></inline-formula>. Subject to the perturbed weights, the recurrent steady-state excitatory input <italic>J<sub>i</sub></italic>(<italic>&#x03C6;<sub>k</sub></italic>) <xref ref-type="disp-formula" rid="eqn36">Eq. (36)</xref> to any excitatory neuron can be written as the unperturbed input <italic>J</italic><sub>0,<italic>i</italic></sub>(<italic>&#x03C6;<sub>k</sub></italic>) plus an additional input <inline-formula><alternatives><inline-graphic xlink:href="424515_inline68.gif"/></alternatives></inline-formula> arising from the perturbed connectivity. Note that the synaptic steady-state activations <italic>s</italic><sub>0,<italic>j</italic></sub>(<italic>&#x03C6;<sub>k</sub></italic>) change with varying bump centers &#x2013; in the following, we denote <inline-formula><alternatives><inline-graphic xlink:href="424515_inline69.gif"/></alternatives></inline-formula>:
<disp-formula id="ueqn3">
<alternatives>
<graphic xlink:href="424515_ueqn3.gif"/>
</alternatives>
</disp-formula></p>
<p>Note that <italic>J</italic><sub>0,<italic>i</italic></sub>(<italic>&#x03C6;<sub>k</sub></italic>) is an index-shifted version of the steady-state input: <italic>J</italic><sub>0,<italic>i</italic></sub>(<italic>&#x03C6;<sub>k</sub></italic>) &#x003D; <italic>J</italic><sub>0,<italic>i</italic>&#x2212;<italic>k</italic></sub>. However, such a relation does not hold for <inline-formula><alternatives><inline-graphic xlink:href="424515_inline70.gif"/></alternatives></inline-formula>, since the random numbers <italic>p<sub>ij</sub></italic> will change the resulting value for varying center positions.</p>
<p>We calculate the firing rate perturbations <italic>&#x03B4;&#x03D5;<sub>i</sub></italic>(<italic>&#x03C6;<sub>k</sub></italic>) resulting from the additional input by a linear expansion around the steady-state firing rates <italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub>(<italic>&#x03C6;<sub>k</sub></italic>) &#x2192; <italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub>(<italic>&#x03C6;<sub>k</sub></italic>) &#x002B; <italic>&#x03B4;&#x03D5;<sub>i</sub></italic>(<italic>&#x03C6;<sub>k</sub></italic>). These evaluate to:
<disp-formula id="eqn51">
<alternatives>
<graphic xlink:href="424515_eqn51.gif"/>
</alternatives>
</disp-formula></p>
<p>See <italic><xref ref-type="sec" rid="s4b5">Derivatives of the rate prediction</xref></italic> for the derivation of the function <inline-formula><alternatives><inline-graphic xlink:href="424515_inline71.gif"/></alternatives></inline-formula> for the spiking network used in the main text.</p>
<p>In the sum of <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>, we keep the firing rate profile <inline-formula><alternatives><inline-graphic xlink:href="424515_inline72.gif"/></alternatives></inline-formula> centered at <italic>&#x03C6;</italic><sub>0</sub> while calculating the drift for varying center positions. To accommodate the shifted indices resulting from moving center positions, we re-index the summands to yields the perturbations <italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub> &#x2192; <italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub> &#x002B; &#x0394;<italic>&#x03D5;<sub>i</sub></italic>(<italic>&#x03C6;<sub>k</sub></italic>) used there:
<disp-formula id="eqn52">
<alternatives>
<graphic xlink:href="424515_eqn52.gif"/>
</alternatives>
</disp-formula></p>
</sec>
<sec id="s4c2">
<title>Heterogeneous leak reversal potentials</title>
<p>We further investigated random distributions of the leak reversal potential <italic>V</italic><sub>L</sub>. These are implemented by the substitution
<disp-formula id="eqn53">
<alternatives>
<graphic xlink:href="424515_eqn53.gif"/>
</alternatives>
</disp-formula>
where the <inline-formula><alternatives><inline-graphic xlink:href="424515_inline73.gif"/></alternatives></inline-formula> are independent normally distributed variables with zero mean, i.e. <inline-formula><alternatives><inline-graphic xlink:href="424515_inline74.gif"/></alternatives></inline-formula>. The parameter <italic>&#x03C3;</italic><sub>L</sub> controls the standard deviation of these random variables, and thus the noise level of the leak heterogeneities.</p>
<p>Let <inline-formula><alternatives><inline-graphic xlink:href="424515_inline75.gif"/></alternatives></inline-formula> for 0 &#x2264; <italic>k</italic> &#x003C; <italic>N</italic> be the center position of the bump. First, note that the heterogeneities <inline-formula><alternatives><inline-graphic xlink:href="424515_inline76.gif"/></alternatives></inline-formula> do not depend on the center position <italic>&#x03C6;<sub>k</sub></italic>, since they are single neuron properties. As in the last section, we calculate the firing rate perturbations <italic>&#x03B4;&#x03D5;<sub>i</sub></italic>(<italic>&#x03C6;<sub>k</sub></italic>) resulting from the additional input by a linear expansion around the steady-state firing rates <italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub>(<italic>&#x03C6;<sub>k</sub></italic>) &#x2192; <italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub>(<italic>&#x03C6;<sub>k</sub></italic>) &#x002B; <italic>&#x03B4;&#x03D5;<sub>i</sub></italic>(<italic>&#x03C6;<sub>k</sub></italic>):
<disp-formula id="eqn54">
<alternatives>
<graphic xlink:href="424515_eqn54.gif"/>
</alternatives>
</disp-formula></p>
<p>Here, <inline-formula><alternatives><inline-graphic xlink:href="424515_inline77.gif"/></alternatives></inline-formula> is the derivative of the input-output relation of neuron <italic>i</italic> in a bump centered at <italic>&#x03C6;<sub>k</sub></italic>, with respect to the leak perturbation. We introduced <inline-formula><alternatives><inline-graphic xlink:href="424515_inline78.gif"/></alternatives></inline-formula> as a shorthand notation for this derivative, since it is evaluated at the steady-state input <italic>J</italic><sub><italic>i</italic>,0</sub>(<italic>&#x03C6;<sub>k</sub></italic>). For the spiking network of the main text, this is derived in <italic><xref ref-type="sec" rid="s4b5">Derivatives of the rate prediction</xref></italic>.</p>
<p>In the sum of <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>, we keep the firing rate profile <inline-formula><alternatives><inline-graphic xlink:href="424515_inline79.gif"/></alternatives></inline-formula> centered at <italic>&#x03C6;</italic><sub>0</sub> while calculating the drift for varying center positions. As in the last section, we re-index the sum to yield the perturbations <italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub> &#x2192; <italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub> &#x002B; &#x0394;<italic>&#x03D5;<sub>i</sub></italic>(<italic>&#x03C6;<sub>k</sub></italic>) used there:
<disp-formula id="eqn55">
<alternatives>
<graphic xlink:href="424515_eqn55.gif"/>
</alternatives>
</disp-formula></p>
</sec>
<sec id="s4c3">
<title>Squared field magnitude</title>
<p>Using the equation of the drift field in <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>, and the firing rate perturbations <xref ref-type="disp-formula" rid="eqn51">Eqs. (51)</xref>-<xref ref-type="disp-formula" rid="eqn54">(54)</xref>, it is straight forward to see that for any center position <italic>&#x03C6;</italic> the expected drift field averaged over the noise parameters is 0, since all single firing rate perturbations vanish in expectation. In the following we calculate the variance of the drift field averaged over noise realizations, which turns out to be additive with respect to the two noise sources.</p>
<p>We begin by calculating the correlations between frozen noises caused by random connectivity and leak heterogeneities. For the Bernoulli distributed variables <italic>p<sub>ij</sub></italic> it holds that &#x2329;<italic>p<sub>ij</sub></italic>&#x232A; &#x003D; <italic>p</italic>, &#x2329;<italic>p<sub>ij</sub>p<sub>lk</sub></italic>&#x232A; &#x003D; <italic>&#x03B4;<sub>il</sub>&#x03B4;<sub>jk</sub>p</italic> &#x002B; (1 &#x2212; <italic>&#x03B4;<sub>il</sub>&#x03B4;<sub>jk</sub></italic>)<italic>p</italic><sup>2</sup>. For the other independent random variables it holds that <inline-formula><alternatives><inline-graphic xlink:href="424515_inline80.gif"/></alternatives></inline-formula>. Again, the weight heterogeneities <inline-formula><alternatives><inline-graphic xlink:href="424515_inline81.gif"/></alternatives></inline-formula>, are only included for completeness &#x2013; all analyses of the main text assume that <italic>&#x03C3;<sub>w</sub></italic>&#x003D;0.</p>
<p>For the correlations between the perturbations we then know that (for brevity, we omit the dependence on the center position <italic>&#x03C6;</italic>):
<disp-formula id="ueqn4">
<alternatives>
<graphic xlink:href="424515_ueqn4.gif"/>
</alternatives>
</disp-formula></p>
<p>Starting from <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>, we use as a firing rate perturbation the sum of firing rate perturbations from both <xref ref-type="disp-formula" rid="eqn51">Eq. (51)</xref> and <xref ref-type="disp-formula" rid="eqn54">Eq. (54)</xref>. With the pre-factor <inline-formula><alternatives><inline-graphic xlink:href="424515_inline82.gif"/></alternatives></inline-formula>, the expected squared field averaged over ensemble of frozen noises is then:
<disp-formula id="eqn56">
<alternatives>
<graphic xlink:href="424515_eqn56.gif"/>
</alternatives>
</disp-formula></p>
<p>One can see directly that the two last terms are invariant under shifts of the bump center <italic>&#x03C6;</italic>, since these introduce symmetric shifts of the indexes <italic>i</italic>. Similarly, it is easy to see that the first term is also invariant. Let <italic>&#x03C6;</italic>&#x2032; be shifted to the right by one index from <italic>&#x03C6;</italic>. It then holds that:
<disp-formula id="ueqn5">
<alternatives>
<graphic xlink:href="424515_ueqn5.gif"/>
</alternatives>
</disp-formula></p>
<p>The final equation holds since, in ring-attractor networks, <inline-formula><alternatives><inline-graphic xlink:href="424515_inline83.gif"/></alternatives></inline-formula> consists of index-shifted rows of the same vector (see e.g. <italic><xref ref-type="sec" rid="s4b3">Network connectivity</xref></italic> for the spiking network weights).</p>
<p>In summary, &#x2329;<italic>A</italic>(<italic>&#x03C6;</italic>)<sup>2</sup>&#x232A;<sub>frozen</sub>, will evaluate to the same quantity &#x2329;<italic>A</italic><sup>2</sup>&#x232A;<sub>frozen</sub>. for all center positions <italic>&#x03C6;</italic>. In the main text, we use this fact to estimate &#x2329;<italic>A</italic><sup>2</sup>&#x232A;<sub>frozen</sub> from simulations, by additionally averaging over the all center positions and interchanging the ensemble and positional averages:
<disp-formula id="ueqn6">
<alternatives>
<graphic xlink:href="424515_ueqn6.gif"/>
</alternatives>
</disp-formula></p>
<p>Thus, we can compare the value of &#x2329;<italic>A</italic><sup>2</sup>&#x232A;<sub>frozen</sub> to the mean squared drift field over all center positions, averaged over instantiations of noises.</p>
</sec>
<sec id="s4c4">
<title>System size scaling</title>
<p>Generally, sums over the discretized intervals [&#x2212;<italic>&#x03C0;, &#x03C0;</italic>) as they appear in <xref ref-type="disp-formula" rid="eqn5">Eqs. (5)</xref> and <xref ref-type="disp-formula" rid="eqn7">(7)</xref> will scale with the number <italic>N</italic> chosen for the discretization of the positions on the continuous ring <inline-formula><alternatives><inline-graphic xlink:href="424515_inline84.gif"/></alternatives></inline-formula>. Consider two discretizations of the ring, partitioned into <italic>N</italic><sub>1</sub> and <italic>N</italic><sub>2</sub> uniformly spaced bins of width <inline-formula><alternatives><inline-graphic xlink:href="424515_inline85.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="424515_inline86.gif"/></alternatives></inline-formula>. We can then approximate integrals over any continuous (Riemann integrable) function <italic>f</italic> on the ring by the two Riemann sums:
<disp-formula id="eqn57">
<alternatives>
<graphic xlink:href="424515_eqn57.gif"/>
</alternatives>
</disp-formula>
where, <inline-formula><alternatives><inline-graphic xlink:href="424515_inline87.gif"/></alternatives></inline-formula> (for <italic>N</italic><sub>2</sub> and <italic>&#x03C6;</italic><sub>2,<italic>i</italic></sub> analogously) are points in the bins [<xref ref-type="bibr" rid="c95">95</xref>].</p>
<p>Numerical quantities for the results of the main text have been calculated for <italic>N<sub>E</sub></italic> &#x003D; 800. In the following we denote all of these quantities with an asterisk (&#x002A;). To generalize these results to arbitrary system size <italic>N</italic>, we replace sums over <italic>N</italic> bins bye scaled sums over <italic>N<sub>E</sub></italic> bins using the relation <xref ref-type="disp-formula" rid="eqn57">Eq. (57)</xref>:
<disp-formula id="ueqn7">
<alternatives>
<graphic xlink:href="424515_ueqn7.gif"/>
</alternatives>
</disp-formula></p>
<p>First, we find that the normalization constant scales as <inline-formula><alternatives><inline-graphic xlink:href="424515_inline88.gif"/></alternatives></inline-formula>, and thus (dots indicate the summands, which are omitted for clarity) for the diffusion strength B (cf. <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref>):
<disp-formula id="eqn58">
<alternatives>
<graphic xlink:href="424515_eqn58.gif"/>
</alternatives>
</disp-formula></p>
<p>For the drift magnitude we turn to the expected squared drift magnitude calculated earlier (cf. <xref ref-type="disp-formula" rid="eqn56">Eq. (56)</xref>), for which we find that (setting <italic>&#x03C3;<sub>w</sub></italic> &#x2192; 0 for simplicity, as throughout the main text):
<disp-formula id="eqn59">
<alternatives>
<graphic xlink:href="424515_eqn59.gif"/>
</alternatives>
</disp-formula></p>
<p>Note, that we could not resolve this scaling in dependence of <inline-formula><alternatives><inline-graphic xlink:href="424515_inline89.gif"/></alternatives></inline-formula>, since the two sources of frozen noise (connectivity and leak heterogeneity) show different scaling with <italic>N</italic>.</p>
</sec>
</sec>
<sec id="s4d">
<title>Numerical methods</title>
<sec id="s4d1">
<title>Spiking simulations</title>
<p>All network simulations and models were implemented in the NEST simulator [<xref ref-type="bibr" rid="c96">96</xref>]. Neuronal dynamics are integrated by the Runge-Kutta-Fehlberg method as implemented in the GSL library [<xref ref-type="bibr" rid="c97">97</xref>] (gsl_odeiv_step_rkf45) &#x2013; this forward integration scheme is used in the NEST simulator for all conductance-based models (at the time of writing). The short-term plasticity model is integrated exactly, based on inter-spike intervals. Code for network simulations will be made available upon publication at <ext-link ext-link-type="uri" xlink:href="https://github.com/EPFL-LCN/pub-seeholzer2018">https://github.com/EPFL-LCN/pub-seeholzer2018</ext-link>.</p>
<sec id="s4d1a">
<title>Simulation protocol</title>
<p>In all experiments (except those involving bi-stability, see below) spiking networks were simulated for a transient initial period of <italic>t</italic><sub>initial</sub> &#x003D; 500<italic>ms</italic>. To center the network in an attractor state at a given angle &#x2212;<italic>&#x03C0;</italic> &#x2264; <italic>&#x03C6;</italic> &#x003C; <italic>&#x03C0;</italic>, we gave an initial cue signal by stimulating 0.1 &#x00B7; <italic>N<sub>E</sub></italic> neurons centered at <italic>&#x03C6;</italic> by strong excitatory input mediated by additional Poisson firing onto AMPA receptors (1<italic>s</italic>, 2kHz) with connections scaled down by a factor of <italic>g</italic><sub>signal</sub> &#x003D; 0.5. The external input ceased at <italic>t</italic> &#x003D; <italic>t</italic><sub>off</sub> &#x003D; 1.5<italic>s</italic>. For simulations to estimate the diffusion we simulated until <italic>t</italic><sub>max</sub> &#x003D; 15<italic>s</italic>, yielding 13.5<italic>s</italic> of delay activity after the cue offset. For simulations to estimate drift we set <italic>t</italic><sub>max</sub> &#x003D; 8<italic>s</italic>, yielding 6.5<italic>s</italic> of delay activity after the cue offset.</p>
<p>For simulations exploring the bi-stability between the uniform state and a bump state (<xref ref-type="fig" rid="fig1">Fig. 1B1</xref>), we added an additional input prior to the spontaneous state. We stimulated simultaneously 20 excitatory neurons around 4 equally spaced cue points each (80 neurons in total, 500<italic>ms</italic>, 1.5kHz, AMPA connections scaled by a factor <italic>g</italic><sub>signal</sub> &#x003D; 2). This was applied to settle networks into the uniform state more stably &#x2013; without this perturbation, networks sometimes approached the bump state after being uniformly initialized. In both figures, we show population activity only after this initial stimulus was applied.</p>
</sec>
<sec id="s4d1b">
<title>Estimation of centers and mean bump shapes</title>
<p>To estimate centers of bump states, simulations were run until <italic>t</italic> &#x003D; <italic>t</italic><sub>max</sub> and spikes were recorded from the excitatory population and converted to firing rates by convolving them with an exponential kernel (<italic>&#x03C4;</italic> &#x003D; 100<italic>ms</italic>) [<xref ref-type="bibr" rid="c98">98</xref>] and then sampled at resolution 1ms. This results in vectors of firing rates <italic>v<sub>j</sub></italic>(<italic>t</italic>), 0 &#x2264; <italic>j</italic> &#x2264; <italic>N<sub>E</sub></italic> &#x2212; 1 for every time <italic>t</italic>. We calculated the population center <italic>&#x03C6;</italic>(<italic>t</italic>) for time <italic>t</italic> by measuring the phase of the first spatial Fourier coefficient of the firing rates. This is given by <inline-formula><alternatives><inline-graphic xlink:href="424515_inline90.gif"/></alternatives></inline-formula>. For all analyses below, we identify <italic>t</italic> &#x003D; 0 to be the time <italic>t</italic> &#x003D; <italic>t</italic><sub>off</sub> of the initial cue.</p>
<p>To measure the mean bump shapes, we first rectified the vectors <italic>v<sub>j</sub></italic>(<italic>t</italic>) for every <italic>t</italic> by rotating the vector until <italic>&#x03C6;</italic>(<italic>t</italic>) &#x003D; 0. We then sampled the rectified firing rates starting from 1<italic>s</italic> after cue offset at intervals of 20ms, which were used to calculate the mean firing rates. S1 Fig shows mean rates for each simulation averaged over the &#x007E; 1000 repetitions performed in the diffusion estimation (below).</p>
</sec>
<sec id="s4d1c">
<title>Exclusion of bump trajectories</title>
<p>Sometimes bump trajectories would leave the attractor state and return to the uniform state. We identified these trajectories in all experiments by identifying maximal firing rates across the population that dropped below 10Hz during the delay period. The such identified repetitions were excluded from the analyses, which occurred mostly in networks with no facilitation for <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 150<italic>ms, &#x03C4;<sub>u</sub></italic> &#x003D; 650<italic>ms</italic>: at <italic>U</italic> &#x003D; 1, we excluded 222/1000 repetitions from the diffusion estimation, while for all other <italic>U</italic> &#x2264; 0.8 at most 15/1000 were excluded. Increasing the depression time constant also lead to less stable attractor states: for <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 200<italic>ms, &#x03C4;<sub>u</sub></italic> &#x003D; 650<italic>ms</italic> and <italic>U</italic> &#x003D; 0.8, we had to exclude 250/1000 repetitions. During the simulations for drift estimation, we observed that frozen noise also leads to less stable bumps under weak facilitation for random and sparse connectivity (<italic>p</italic> &#x226A; 1) and high leak variability (<italic>&#x03C3;<sub>L</sub></italic> &#x226B; 0).</p>
</sec>
<sec id="s4d1d">
<title>Diffusion estimation</title>
<p>Diffusion was estimated for each combination of network parameters by simulating 1000 repetitions (10 initial cue positions, 100 repetitions each) of 13.5<italic>s</italic> of delay activity. Center positions <italic>&#x03C6;<sub>k</sub></italic>(<italic>t</italic>) were estimated for each repetition <italic>k</italic> as described above. We then calculated for each repetition the offset relative to the position at 500<italic>ms</italic> by &#x0394;<italic>&#x03C6;<sub>k</sub></italic>(<italic>t</italic>) &#x003D; <italic>&#x03C6;<sub>k</sub></italic>(<italic>t</italic> &#x2212; 500<italic>ms</italic>) &#x2212; <italic>&#x03C6;<sub>k</sub></italic>(500<italic>ms</italic>), effectively discarding the first 500<italic>ms</italic> after cue-offset. The time-dependent variance of <italic>K</italic> repetitions (excluding those repetitions in which the bump state was lost, see above) was then calculated as <inline-formula><alternatives><inline-graphic xlink:href="424515_inline91.gif"/></alternatives></inline-formula>. The diffusion strength can then be estimated from the slope of a linear least-squares regression (using the Scipy method <italic>scipy.stats.linregress</italic> [<xref ref-type="bibr" rid="c99">99</xref>]) to the variance as a function of time: <italic>V</italic>(<italic>t</italic>) &#x2248; <italic>D</italic><sub>0</sub> &#x002B; <italic>D</italic> &#x00B7; <italic>t</italic>, where the intercept <italic>D</italic><sub>0</sub> is included to account for initial transients. We estimated confidence intervals by bootstrapping [<xref ref-type="bibr" rid="c100">100</xref>]: sampling K elements out of the K repetitions with replacement (5000 samples) and estimating the confidence level of 0.95 by the bias corrected and accelerated bootstrap implemented in <italic>scikits-bootstrap</italic> [<xref ref-type="bibr" rid="c101">101</xref>]. As a control, we calculated confidence intervals for <italic>D</italic> additionally by Jackknifing: after building a distribution of estimates of D on <italic>K</italic> one-left-out samples of all repetitions, the standard error of the mean can be calculated and is multiplied by 1.96 to obtain the 95&#x0025; confidence interval [<xref ref-type="bibr" rid="c102">102</xref>] &#x2013; confidence intervals obtained by this method were almost indistinguishable from confidence intervals obtained by bootstrapping.</p>
</sec>
<sec id="s4d1e">
<title>Drift estimation</title>
<p>Drift was estimated numerically for each combination of network and frozen noise parameters by simulating 400 repetitions (20 initial cue positions, 20 repetitions each) of 6.5<italic>s</italic> of delay activity. Centers positions <italic>&#x03C6;<sub>k</sub></italic>(<italic>t</italic>) were estimated for all <italic>K</italic> repetitions (excluding those repetitions in which the bump state was lost, see above) as explained above. We then computed displacements in time by computing a set of discrete differences
<disp-formula id="ueqn8">
<alternatives>
<graphic xlink:href="424515_ueqn8.gif"/>
</alternatives>
</disp-formula>
where we chose <italic>dt</italic> &#x003D; 1.5<italic>s</italic> and <italic>t</italic><sub>0</sub> &#x2208; &#x007B;500<italic>ms</italic>, 700<italic>ms</italic>, 900<italic>ms</italic>, &#x2026;, 1900<italic>ms</italic>&#x007D;. All differences are calculated with periodic boundary conditions on the circle [&#x2212;<italic>&#x03C0;, &#x03C0;</italic>), i.e. the maximal difference was <italic>&#x03C0;</italic>/<italic>dt</italic>. We then calculated a binned mean (100 bins on the ring, unless mentioned otherwise) of differences calculated for all <italic>K</italic> trajectories, to approximate the drift-fields as a function of positions on the ring.</p>
</sec>
</sec>
<sec id="s4d2">
<title>Mutual information measure</title>
<p>We are estimating the mutual information between a set of initial positions <italic>x</italic> &#x2208; [0, 2<italic>&#x03C0;</italic>) and associated final positions <italic>y</italic>(<italic>x</italic>) &#x2208; [0, 2<italic>&#x03C0;</italic>) of the trajectories of a continuous attractor network over a fixed delay period of <italic>T</italic>. For our results, we take <italic>T</italic> &#x003D; 6.5<italic>s</italic>. We constructed binned and normalized histograms (with bin size <italic>n</italic> &#x003D; 100, but see below) as approximate probability distributions of initial positions <inline-formula><alternatives><inline-graphic xlink:href="424515_inline92.gif"/></alternatives></inline-formula> and all final positions <inline-formula><alternatives><inline-graphic xlink:href="424515_inline93.gif"/></alternatives></inline-formula> (with bins indexed by 1 &#x2264; <italic>i</italic> &#x2264; <italic>n</italic>), as well as the bivariate probability distribution <inline-formula><alternatives><inline-graphic xlink:href="424515_inline94.gif"/></alternatives></inline-formula>.</p>
<p>Using these, we can calculate the mutual information as [<xref ref-type="bibr" rid="c54">54</xref>, <xref ref-type="bibr" rid="c55">55</xref>] <inline-formula><alternatives><inline-graphic xlink:href="424515_inline95.gif"/></alternatives></inline-formula> Note, that the sum effectively counts only nonzero entries of <italic>r<sub>ij</sub></italic> (trajectories that started in bin <italic>i</italic> and ended in bin <italic>j</italic>): these imply that <italic>p<sub>i</sub></italic> &#x2260; 0 (a trajectory started in bin <italic>i</italic>) and <italic>q<sub>j</sub></italic> &#x2260; 0 (a trajectory ended in bin <italic>j</italic>), which makes the sum well defined. Although the value of MI depends on the number of bins <italic>n</italic>, in <xref ref-type="fig" rid="fig5">Fig. 5</xref> and <xref ref-type="fig" rid="fig6">Fig. 6</xref> we normalize MI to that of the reference network (<italic>U</italic> &#x003D; 1, no frozen noise, see <italic><xref ref-type="sec" rid="s2e">Short-term plasticity controls memory retention</xref></italic>), which leaves the resulting plot nearly invariant under a change of bin numbers.</p>
</sec>
<sec id="s4d3">
<title>Numerical integration of Langevin equations</title>
<p>Numerically integration of the homogeneous Langevin equations (<xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref>) describing drift and diffusion of bump positions <italic>&#x03C6;</italic> &#x2208; [&#x2212;<italic>&#x03C0;, &#x03C0;</italic>) (with circular boundary conditions) has been implemented as a <italic>C</italic> extension in Cython [<xref ref-type="bibr" rid="c103">103</xref>] to the Python language [<xref ref-type="bibr" rid="c104">104</xref>]. Since the drift fields <italic>A</italic>(<italic>&#x03C6;</italic>) are estimated on a discretization of the interval [&#x2212;<italic>&#x03C0;, &#x03C0;</italic>) into <italic>N</italic> bins, we first interpolate drift fields <italic>A</italic> given as <italic>N</italic> discretized values to obtain continuous fields &#x2013; interpolations are obtained using cubic splines on periodic boundary conditions using the class <italic>gsl_interp-cspline_periodic</italic> of the Gnu Scientific Library [<xref ref-type="bibr" rid="c97">97</xref>].</p>
<p>For forward integration of the Langevin equation <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref> from time <italic>t</italic> &#x003D; 0, we start from an initial position <italic>&#x03C6;</italic><sub>0</sub> &#x003D; <italic>&#x03C6;</italic>(<italic>t</italic> &#x003D; 0). Given a time resolution <italic>dt</italic> (unless otherwise stated we use <italic>dt</italic> &#x003D; 0.1<italic>s</italic>) and a maximal time <italic>t</italic><sub>max</sub> we repeat the following operations until we reach <italic>t</italic> &#x003D; <italic>t</italic><sub>max</sub>:
<disp-formula id="ueqn9">
<alternatives>
<graphic xlink:href="424515_ueqn9.gif"/>
</alternatives>
</disp-formula></p>
<p>Here, for each iteration <italic>r</italic> is a random number drawn from a normal distribution with zero mean and unit variance (&#x2329;<italic>r</italic>&#x232A;&#x003D;0 and &#x2329;<italic>r</italic><sup>2</sup>&#x232A; &#x003D; 1). The last step is performed to implement the circular boundary conditions on [&#x2212;<italic>&#x03C0;, &#x03C0;</italic>).</p>
<p>Code implementing this numerical integration scheme will be made available upon publication at <ext-link ext-link-type="uri" xlink:href="https://github.com/EPFL-LCN/pub-seeholzer2018-langevin">https://github.com/EPFL-LCN/pub-seeholzer2018-langevin</ext-link>.</p>
</sec>
<sec id="s4d4">
<title>Distractor analysis</title>
<p>For the distractor analysis in <xref ref-type="fig" rid="fig7">Fig. 7</xref>, we let 40 neurons centered at the distractor position <inline-formula><alternatives><inline-graphic xlink:href="424515_inline96.gif"/></alternatives></inline-formula> fire at rates increased by 20Hz, yielding a vector of firing rate perturbations &#x0394;<italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub> &#x003D; 20Hz if &#x007C;<italic>i</italic> &#x2212; <italic>j</italic>&#x007C; &#x2264; 20 and &#x0394;<italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub> &#x003D; 0Hz otherwise. The vectors &#x0394;<italic>&#x03D5;</italic><sub>0,<italic>i</italic></sub> foreach distractor position <italic>&#x03C6;<sub>D</sub></italic> are then used in <xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref> to calculate the corresponding drift fields. To calculate the final position <italic>&#x03C6;</italic><sub>1</sub> after 250ms of presenting the distractor, we generate 1000 trajectories starting from <italic>&#x03C6;</italic><sub>0</sub> &#x003D; 0 by integrating the Langevin equation <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref> for 250ms (<italic>dt</italic> &#x003D; 0.01), the final positions of which are used to measure mean and standard deviation of <italic>&#x03C6;</italic><sub>1</sub>. For the broader bump in <xref ref-type="fig" rid="fig7">Fig. 7D</xref>, we stretched (and interpolated) the firing rates <italic>&#x03D5;</italic><sub>0</sub> as well as the associated vectors <italic>J</italic><sub>0</sub> and <inline-formula><alternatives><inline-graphic xlink:href="424515_inline97.gif"/></alternatives></inline-formula> along the x-axis to obtain vectors for bumps of the desired width, and then re-calculated the values of <inline-formula><alternatives><inline-graphic xlink:href="424515_inline98.gif"/></alternatives></inline-formula>.</p>
</sec>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>The authors thank Tilo Schwalger and Johanni Brea for helpful discussions and feedback. Research was supported by the European Union Seventh Framework Program (FP7) under grant agreement no. 604102 (Human Brain Project, M.D.) and by the Swiss National Science Foundation (200020_147200, A.S.).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Goldman-Rakic</surname> <given-names>PS</given-names></string-name>. <article-title>Cellular Basis of Working Memory</article-title>. <source>Neuron</source>. <year>1995</year>;<volume>14</volume>(<issue>3</issue>):<fpage>477</fpage>&#x2013;<lpage>485</lpage>. doi:<pub-id pub-id-type="doi">10.1016/0896-6273(95)90304-6</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Constantinidis</surname> <given-names>C</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>XJ</given-names></string-name>. <article-title>A Neural Circuit Basis for Spatial Working Memory</article-title>. <source>The Neuroscientist</source>. <year>2004</year>;<volume>10</volume>(<issue>6</issue>):<fpage>553</fpage>&#x2013;<lpage>65</lpage>. doi:<pub-id pub-id-type="doi">10.1177/1073858404268742</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Chaudhuri</surname> <given-names>R</given-names></string-name>, <string-name><surname>Fiete</surname> <given-names>I</given-names></string-name>. <article-title>Computational Principles of Memory</article-title>. <source>Nature Neuroscience</source>. <year>2016</year>;<volume>19</volume>(<issue>3</issue>):<fpage>394</fpage>&#x2013;<lpage>403</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.4237</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Curtis</surname> <given-names>CE</given-names></string-name>, <string-name><surname>D&#x2019;Esposito</surname> <given-names>M</given-names></string-name>. <article-title>Persistent Activity in the Prefrontal Cortex during Working Memory</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2003</year>;<volume>7</volume>(<issue>9</issue>):<fpage>415</fpage>&#x2013;<lpage>423</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S1364-6613(03)00197-9</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Durstewitz</surname> <given-names>D</given-names></string-name>, <string-name><surname>Seamans</surname> <given-names>JK</given-names></string-name>, <string-name><surname>Sejnowski</surname> <given-names>TJ</given-names></string-name>. <article-title>Neurocomputational Models of Working Memory</article-title>. <source>Nature Neuroscience</source>. <year>2000</year>;<volume>3</volume>:<fpage>1184</fpage>&#x2013;<lpage>91</lpage>. doi:<pub-id pub-id-type="doi">10.1038/81460</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Barak</surname> <given-names>O</given-names></string-name>, <string-name><surname>Tsodyks</surname> <given-names>M</given-names></string-name>. <article-title>Working Models of Working Memory</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2014</year>;<volume>25</volume>:<fpage>20</fpage>&#x2013;<lpage>24</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.conb.2013.10.008</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Duarte</surname> <given-names>R</given-names></string-name>, <string-name><surname>Seeholzer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Zilles</surname> <given-names>K</given-names></string-name>, <string-name><surname>Morrison</surname> <given-names>A</given-names></string-name>. <article-title>Synaptic Patterning and the Timescales of Cortical Dynamics</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2017</year>;<volume>43</volume>:<fpage>156</fpage>&#x2013;<lpage>165</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.conb.2017.02.007</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Amari</surname> <given-names>Si</given-names></string-name>. <article-title>Dynamics of Pattern Formation in Lateral-Inhibition Type Neural Fields</article-title>. <source>Biological cybernetics</source>. <year>1977</year>;<volume>87</volume>:<fpage>77</fpage>&#x2013;<lpage>87</lpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Camperi</surname> <given-names>M</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>XJ</given-names></string-name>. <article-title>A Model of Visuospatial Working Memory in Prefrontal Cortex: Recurrent Network and Cellular Bistability</article-title>. <source>Journal of Computational Neuroscience</source>. <year>1998</year>;<volume>5</volume>(<issue>4</issue>):<fpage>383</fpage>&#x2013;<lpage>405</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="book"><string-name><surname>Hansel</surname> <given-names>D</given-names></string-name>, <string-name><surname>Sompolinsky</surname> <given-names>H</given-names></string-name>. <chapter-title>Modeling Feature Selectivity in Local Cortical Circuits</chapter-title>. In: <person-group person-group-type="editor"><string-name><surname>Koch</surname> <given-names>C</given-names></string-name>, <string-name><surname>Segev</surname> <given-names>I</given-names></string-name></person-group>, editors. <source>Methods in Neural Modeling. from Synapses to Networks</source>. <publisher-name>MIT Press</publisher-name>; <year>1998</year>. p. <fpage>499</fpage>&#x2013;<lpage>567</lpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Compte</surname> <given-names>A</given-names></string-name>, <string-name><surname>Brunel</surname> <given-names>N</given-names></string-name>, <string-name><surname>Goldman-Rakic</surname> <given-names>PS</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>XJ</given-names></string-name>. <article-title>Synaptic Mechanisms and Network Dynamics Underlying Spatial Working Memory in a Cortical Network Model</article-title>. <source>Cerebral Cortex</source>. <year>2000</year>;<volume>10</volume>:<fpage>910</fpage>&#x2013;<lpage>923</lpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Samsonovich</surname> <given-names>A</given-names></string-name>, <string-name><surname>McNaughton</surname> <given-names>BL</given-names></string-name>. <article-title>Path Integration and Cognitive Mapping in a Continuous Attractor Neural Network Model</article-title>. <source>Journal of Neuroscience</source>. <year>1997</year>;<volume>17</volume>(<issue>15</issue>):<fpage>5900</fpage>&#x2013;<lpage>20</lpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Stringer</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Trappenberg</surname> <given-names>TP</given-names></string-name>, <string-name><surname>Rolls</surname> <given-names>ET</given-names></string-name>, <string-name><surname>de Araujo</surname> <given-names>IET</given-names></string-name>. <article-title>Self-Organizing Continuous Attractor Networks and Path Integration: One-Dimensional Models of Head Direction Cells</article-title>. <source>Network</source>. <year>2002</year>;<volume>13</volume>(<issue>2</issue>):<fpage>217</fpage>&#x2013;<lpage>42</lpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Burak</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Fiete</surname> <given-names>IR</given-names></string-name>. <article-title>Accurate Path Integration in Continuous Attractor Network Models of Grid Cells</article-title>. <source>PLOS Computational Biology</source>. <year>2009</year>;<volume>5</volume>(<issue>2</issue>):<fpage>e1000291</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1000291</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Ben-Yishai</surname> <given-names>R</given-names></string-name>, <string-name><surname>Bar-Or</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Sompolinsky</surname> <given-names>H</given-names></string-name>. <article-title>Theory of Orientation Tuning in Visual Cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1995</year>;<volume>92</volume>(<issue>9</issue>):<fpage>3844</fpage>&#x2013;<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname> <given-names>K</given-names></string-name>. <article-title>Representation of Spatial Orientation by the Intrinsic Dynamics of the Head-Direction Cell Ensemble: A Theory</article-title>. <source>Journal of Neuroscience</source>. <year>1996</year>;<volume>16</volume>(<issue>6</issue>):<fpage>2112</fpage>&#x2013;<lpage>2126</lpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Seung</surname> <given-names>HS</given-names></string-name>. <article-title>Continuous Attractors and Oculomotor Control</article-title>. <source>Neural Networks</source>. <year>1998</year>;<volume>11</volume>(<issue>7</issue>):<fpage>1253</fpage>&#x2013;<lpage>1258</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0893-6080(98)00064-1</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Knierim</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>K</given-names></string-name>. <article-title>Attractor Dynamics of Spatially Correlated Neural Activity in the Limbic System</article-title>. <source>Annual Review of Neuroscience</source>. <year>2012</year>;<volume>35</volume>:<fpage>267</fpage>&#x2013;<lpage>85</lpage>. doi:<pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150351</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Moser</surname> <given-names>EI</given-names></string-name>, <string-name><surname>Roudi</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Witter</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Kentros</surname> <given-names>C</given-names></string-name>, <string-name><surname>Bonhoeffer</surname> <given-names>T</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>MB</given-names></string-name>. <article-title>Grid Cells and Cortical Representation</article-title>. <source>Nature Reviews Neuroscience</source>. <year>2014</year>;<volume>15</volume>(<issue>7</issue>):<fpage>466</fpage>&#x2013;<lpage>481</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nrn3766</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Burak</surname> <given-names>Y</given-names></string-name>. <article-title>Spatial Coding and Attractor Dynamics of Grid Cells in the Entorhinal Cortex</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2014</year>;<volume>25</volume>:<fpage>169</fpage>&#x2013;<lpage>175</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.conb.2014.01.013</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Wu</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wong</surname> <given-names>KYM</given-names></string-name>, <string-name><surname>Fung</surname> <given-names>CCA</given-names></string-name>, <string-name><surname>Mi</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>W</given-names></string-name>. <article-title>Continuous Attractor Neural Networks: Candidate of a Canonical Model for Neural Information Representation</article-title>. <source>F1000Research</source>. <year>2016</year>;<volume>5</volume>. doi:<pub-id pub-id-type="doi">10.12688/f1000research.7387.1</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Wimmer</surname> <given-names>K</given-names></string-name>, <string-name><surname>Nykamp</surname> <given-names>DQ</given-names></string-name>, <string-name><surname>Constantinidis</surname> <given-names>C</given-names></string-name>, <string-name><surname>Compte</surname> <given-names>A</given-names></string-name>. <article-title>Bump Attractor Dynamics in Prefrontal Cortex Explains Behavioral Precision in Spatial Working Memory</article-title>. <source>Nature Neuroscience</source>. <year>2014</year>;<volume>17</volume>(<issue>3</issue>):<fpage>431</fpage>&#x2013;<lpage>439</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.3645</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Yoon</surname> <given-names>K</given-names></string-name>, <string-name><surname>Buice</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Barry</surname> <given-names>C</given-names></string-name>, <string-name><surname>Hayman</surname> <given-names>R</given-names></string-name>, <string-name><surname>Burgess</surname> <given-names>N</given-names></string-name>, <string-name><surname>Fiete</surname> <given-names>IR</given-names></string-name>. <article-title>Specific Evidence of Low-Dimensional Continuous Attractor Dynamics in Grid Cells</article-title>. <source>Nature Neuroscience</source>. <year>2013</year>;<volume>16</volume>(<issue>8</issue>):<fpage>1077</fpage>&#x2013;<lpage>1084</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.3450</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Seelig</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Jayaraman</surname> <given-names>V</given-names></string-name>. <article-title>Neural Dynamics for Landmark Orientation and Angular Path Integration</article-title>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>(<issue>7551</issue>):<fpage>186</fpage>&#x2013;<lpage>191</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nature14446</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Kim</surname> <given-names>SS</given-names></string-name>, <string-name><surname>Rouault</surname> <given-names>H</given-names></string-name>, <string-name><surname>Druckmann</surname> <given-names>S</given-names></string-name>, <string-name><surname>Jayaraman</surname> <given-names>V</given-names></string-name>. <article-title>Ring Attractor Dynamics in the Drosophila Central Brain</article-title>. <source>Science</source>. <year>2017</year>; p. <fpage>eaal4835</fpage>. doi:<pub-id pub-id-type="doi">10.1126/science.aal4835</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Macoveanu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Klingberg</surname> <given-names>T</given-names></string-name>, <string-name><surname>Tegn&#x00E9;r</surname> <given-names>J</given-names></string-name>. <article-title>A Biophysical Model of Multiple-Item Working Memory: A Computational and Neuroimaging Study</article-title>. <source>Neuroscience</source>. <year>2006</year>;<volume>141</volume>(<issue>3</issue>):<fpage>1611</fpage>&#x2013;<lpage>8</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroscience.2006.04.080</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Wei</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>XJ</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>DH</given-names></string-name>. <article-title>From Distributed Resources to Limited Slots in Multiple-Item Working Memory: A Spiking Network Model with Normalization</article-title>. <source>Journal of Neuroscience</source>. <year>2012</year>;<volume>32</volume>(<issue>33</issue>):<fpage>11228</fpage>&#x2013;<lpage>40</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.0735-12.2012</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Roggeman</surname> <given-names>C</given-names></string-name>, <string-name><surname>Klingberg</surname> <given-names>T</given-names></string-name>, <string-name><surname>Feenstra</surname> <given-names>HEM</given-names></string-name>, <string-name><surname>Compte</surname> <given-names>A</given-names></string-name>, <string-name><surname>Almeida</surname> <given-names>R</given-names></string-name>. <article-title>Trade-off between Capacity and Precision in Visuospatial Working Memory</article-title>. <source>Journal of Cognitive Neuroscience</source>. <year>2014</year>;<volume>26</volume>(<issue>2</issue>):<fpage>211</fpage>&#x2013;<lpage>222</lpage>. doi:<pub-id pub-id-type="doi">10.1162/jocna00485</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Almeida</surname> <given-names>R</given-names></string-name>, <string-name><surname>Barbosa</surname> <given-names>Ja</given-names></string-name>, <string-name><surname>Compte</surname> <given-names>A</given-names></string-name>. <article-title>Neural Circuit Basis of Visuo-Spatial Working Memory Precision: A Computational and Behavioral Study</article-title>. <source>Journal of Neurophysiology</source>. <year>2015</year>;<volume>114</volume>(<issue>3</issue>):<fpage>1806</fpage>&#x2013;<lpage>1818</lpage>. doi:<pub-id pub-id-type="doi">10.1152/jn.00362.2015</pub-id>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Cano-Colino</surname> <given-names>M</given-names></string-name>, <string-name><surname>Almeida</surname> <given-names>R</given-names></string-name>, <string-name><surname>Compte</surname> <given-names>A</given-names></string-name>. <article-title>Serotonergic Modulation of Spatial Working Memory: Predictions from a Computational Network Model</article-title>. <source>Frontiers in Integrative Neuroscience</source>. <year>2013</year>;<volume>7</volume>. doi:<pub-id pub-id-type="doi">10.3389/fnint.2013.00071</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Cano-Colino</surname> <given-names>M</given-names></string-name>, <string-name><surname>Almeida</surname> <given-names>R</given-names></string-name>, <string-name><surname>Gomez-Cabrero</surname> <given-names>D</given-names></string-name>, <string-name><surname>Artigas</surname> <given-names>F</given-names></string-name>, <string-name><surname>Compte</surname> <given-names>A</given-names></string-name>. <article-title>Serotonin Regulates Performance Nonmonotonically in a Spatial Working Memory Network</article-title>. <source>Cerebral Cortex</source>. <year>2014</year>;<volume>24</volume>(<issue>9</issue>):<fpage>2449</fpage>&#x2013;<lpage>2463</lpage>. doi:<pub-id pub-id-type="doi">10.1093/cercor/bht096</pub-id>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Murray</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Anticevic</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gancsos</surname> <given-names>M</given-names></string-name>, <string-name><surname>Ichinose</surname> <given-names>M</given-names></string-name>, <string-name><surname>Corlett</surname> <given-names>PR</given-names></string-name>, <string-name><surname>Krystal</surname> <given-names>JH</given-names></string-name>, <etal>et al.</etal> <article-title>Linking Microcircuit Dysfunction to Cognitive Impairment: Effects of Disinhibition Associated with Schizophrenia in a Cortical Working Memory Model</article-title>. <source>Cerebral Cortex</source>. <year>2012</year>; doi:<pub-id pub-id-type="doi">10.1093/cercor/bhs370</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Cano-Colino</surname> <given-names>M</given-names></string-name>, <string-name><surname>Compte</surname> <given-names>A</given-names></string-name>. <article-title>A Computational Model for Spatial Working Memory Deficits in Schizophrenia</article-title>. <source>Pharmacopsychiatry</source>. <year>2012</year>;<volume>45</volume>(<issue>S 01</issue>):<fpage>S49</fpage>&#x2013;<lpage>S56</lpage>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Tsodyks</surname> <given-names>M</given-names></string-name>, <string-name><surname>Sejnowski</surname> <given-names>T</given-names></string-name>. <article-title>Associative Memory and Hippocampal Place Cells</article-title>. <source>Neural Systems</source>. <year>1995</year>;<volume>6</volume>:<fpage>81</fpage>&#x2013;<lpage>86</lpage>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Spiridon</surname> <given-names>M</given-names></string-name>, <string-name><surname>Gerstner</surname> <given-names>W</given-names></string-name>. <article-title>Effect of Lateral Connections on the Accuracy of the Population Code for a Network of Spiking Neurons</article-title>. <source>Network</source>. <year>2001</year>;<volume>12</volume>(<issue>4</issue>):<fpage>409</fpage>&#x2013;<lpage>21</lpage>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Renart</surname> <given-names>A</given-names></string-name>, <string-name><surname>Song</surname> <given-names>P</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>XJ</given-names></string-name>. <article-title>Robust Spatial Working Memory through Homeostatic Synaptic Scaling in Heterogeneous Cortical Networks</article-title>. <source>Neuron</source>. <year>2003</year>;<volume>38</volume>(<issue>3</issue>):<fpage>473</fpage>&#x2013;<lpage>85</lpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Wu</surname> <given-names>S</given-names></string-name>, <string-name><surname>Hamaguchi</surname> <given-names>K</given-names></string-name>, <string-name><surname>Amari</surname> <given-names>Si</given-names></string-name>. <article-title>Dynamics and Computation of Continuous Attractors</article-title>. <source>Neural Computation</source>. <year>2008</year>;<volume>20</volume>(<issue>4</issue>):<fpage>994</fpage>&#x2013;<lpage>1025</lpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Itskov</surname> <given-names>V</given-names></string-name>, <string-name><surname>Hansel</surname> <given-names>D</given-names></string-name>, <string-name><surname>Tsodyks</surname> <given-names>M</given-names></string-name>. <article-title>Short-Term Facilitation May Stabilize Parametric Working Memory Trace</article-title>. <source>Frontiers in Computational Neuroscience</source>. <year>2011</year>;<volume>5</volume>(<month>October</month>):<fpage>40</fpage>&#x2013;<lpage>40</lpage>. doi:<pub-id pub-id-type="doi">10.3389/fncom.2011.00040</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Burak</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Fiete</surname> <given-names>IR</given-names></string-name>. <article-title>Fundamental Limits on Persistent Activity in Networks of Noisy Neurons</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2012</year>;<volume>109</volume>(<issue>43</issue>):<fpage>17645</fpage>&#x2013;<lpage>17650</lpage>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="other"><string-name><surname>Kilpatrick</surname> <given-names>ZP</given-names></string-name>, <string-name><surname>Ermentrout</surname> <given-names>B</given-names></string-name>. <source>Wandering Bumps in Stochastic Neural Fields</source>. arXiv:<pub-id pub-id-type="arxiv">12053072</pub-id> [math, nlin, q-bio]. <year>2012</year>;.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Brody</surname> <given-names>C</given-names></string-name>, <string-name><surname>Romo</surname> <given-names>R</given-names></string-name>, <string-name><surname>Kepecs</surname> <given-names>A</given-names></string-name>. <article-title>Basic Mechanisms for Graded Persistent Activity: Discrete Attractors, Continuous Attractors, and Dynamic Representations</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2003</year>; p. <fpage>204</fpage>&#x2013;<lpage>211</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0959-4388(03)00050-3</pub-id>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>York</surname> <given-names>LC</given-names></string-name>, <string-name><surname>van Rossum</surname> <given-names>MCW</given-names></string-name>. <article-title>Recurrent Networks with Short Term Synaptic Depression</article-title>. <source>Journal of Computational Neuroscience</source>. <year>2009</year>;<volume>27</volume>(<issue>3</issue>):<fpage>607</fpage>&#x2013;<lpage>620</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s10827-009-0172-4</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><surname>Romani</surname> <given-names>S</given-names></string-name>, <string-name><surname>Tsodyks</surname> <given-names>M</given-names></string-name>. <article-title>Short-Term Plasticity Based Network Model of Place Cells Dynamics</article-title>. <source>Hippocampus</source>. <year>2015</year>;<volume>25</volume>(<issue>1</issue>):<fpage>94</fpage>&#x2013;<lpage>105</lpage>. doi:<pub-id pub-id-type="doi">10.1002/hipo.22355</pub-id>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>Barbieri</surname> <given-names>F</given-names></string-name>, <string-name><surname>Brunel</surname> <given-names>N</given-names></string-name>. <article-title>Irregular Persistent Activity Induced by Synaptic Excitatory Feedback</article-title>. <source>Frontiers in Computational Neuroscience</source>. <year>2007</year>;<volume>1</volume>. doi:<pub-id pub-id-type="doi">10.3389/neuro.10.005.2007</pub-id>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>Hansel</surname> <given-names>D</given-names></string-name>, <string-name><surname>Mato</surname> <given-names>G</given-names></string-name>. <article-title>Short-Term Plasticity Explains Irregular Persistent Activity in Working Memory Tasks</article-title>. <source>The Journal of Neuroscience</source>. <year>2013</year>;<volume>33</volume>(<issue>1</issue>):<fpage>133</fpage>&#x2013;<lpage>49</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.3455-12.2013</pub-id>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><surname>Pereira</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>XJ</given-names></string-name>. <article-title>A Tradeoff Between Accuracy and Flexibility in a Working Memory Circuit Endowed with Slow Feedback Mechanisms</article-title>. <source>Cerebral Cortex</source>. <year>2015</year>;<volume>25</volume>(<issue>10</issue>):<fpage>3586</fpage>. doi:<pub-id pub-id-type="doi">10.1093/cercor/bhu202</pub-id>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="book"><string-name><surname>Gerstner</surname> <given-names>W</given-names></string-name>, <string-name><surname>Kistler</surname> <given-names>WM</given-names></string-name>. <source>Spiking Neuron Models: Single Neurons, Populations, Plasticity</source>. <publisher-name>Cambridge University Press</publisher-name>; <year>2002</year>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><surname>Tsodyks</surname> <given-names>M</given-names></string-name>, <string-name><surname>Pawelzik</surname> <given-names>K</given-names></string-name>, <string-name><surname>Markram</surname> <given-names>H</given-names></string-name>. <article-title>Neural Networks with Dynamic Synapses</article-title>. <source>Neural Computation</source>. <year>1998</year>;<volume>10</volume>(<issue>4</issue>):<fpage>821</fpage>&#x2013;<lpage>835</lpage>. doi:<pub-id pub-id-type="doi">10.1162/089976698300017502</pub-id>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><surname>Brunel</surname> <given-names>N</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>X</given-names></string-name>. <article-title>Effects of Neuromodulation in a Cortical Network Model of 0bject Working Memory Dominated by Recurrent Inhibition</article-title>. <source>Journal of Computational Neuroscience</source>. <year>2001</year>;<volume>11</volume>:<fpage>63</fpage>&#x2013;<lpage>85</lpage>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Richardson</surname> <given-names>M</given-names></string-name>. <article-title>Firing-Rate Response of Linear and Nonlinear Integrate-and-Fire Neurons to Modulated Current-Based and Conductance-Based Synaptic Drive</article-title>. <source>Physical Review E</source>. <year>2007</year>;<volume>76</volume>(<issue>2</issue>):<fpage>1</fpage>&#x2013;<lpage>15</lpage>. doi:<pub-id pub-id-type="doi">10.1103/PhysRevE.76.021919</pub-id>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname> <given-names>XJ</given-names></string-name>. <article-title>Synaptic Basis of Cortical Persistent Activity: The Importance of NMDA Receptors to Working Memory</article-title>. <source>The Journal of Neuroscience</source>. <year>1999</year>;<volume>19</volume>(<issue>21</issue>):<fpage>9587</fpage>&#x2013;<lpage>603</lpage>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="book"><string-name><surname>van Kampen</surname> <given-names>NG</given-names></string-name>. <source>Stochastic Processes in Physics and Chemistry</source>. <edition>2nd ed.</edition> <publisher-name>North Holland</publisher-name>; <year>1992</year>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="book"><string-name><surname>Gardiner</surname> <given-names>C</given-names></string-name>. <source>Stochastic Methods: A Handbook for the Natural and Social Sciences</source>. <edition>4th ed.</edition> <publisher-name>Springer</publisher-name>; <year>2009</year>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><string-name><surname>Latham</surname> <given-names>PE</given-names></string-name>, <string-name><surname>Roudi</surname> <given-names>Y</given-names></string-name>. <article-title>Mutual Information</article-title>. <source>Scholarpedia</source>. <year>2009</year>;<volume>4</volume>(<issue>1</issue>):<fpage>1658</fpage>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="book"><string-name><surname>Cover</surname> <given-names>TM</given-names></string-name>, <string-name><surname>Thomas</surname> <given-names>JA</given-names></string-name>. <source>Elements of Information Theory</source>. <publisher-name>John Wiley &#x0026; Sons</publisher-name>; <year>2012</year>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><string-name><surname>Kilpatrick</surname> <given-names>ZP</given-names></string-name>, <string-name><surname>Ermentrout</surname> <given-names>B</given-names></string-name>, <string-name><surname>Doiron</surname> <given-names>B</given-names></string-name>. <article-title>Optimizing Working Memory with Heterogeneity of Recurrent Cortical Excitation</article-title>. <source>Journal of Neuroscience</source>. <year>2013</year>;<volume>33</volume>(<issue>48</issue>):<fpage>18999</fpage>&#x2013;<lpage>19011</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.1641-13.2013</pub-id>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><string-name><surname>Macoveanu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Klingberg</surname> <given-names>T</given-names></string-name>, <string-name><surname>Tegn&#x00E9;r</surname> <given-names>J</given-names></string-name>. <article-title>Neuronal Firing Rates Account for Distractor Effects on Mnemonic Accuracy in a Visuo-Spatial Working Memory Task</article-title>. <source>Biological Cybernetics</source>. <year>2007</year>;<volume>96</volume>(<issue>4</issue>):<fpage>407</fpage>&#x2013;<lpage>419</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s00422-006-0139-8</pub-id>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><string-name><surname>Ploner</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Gaymard</surname> <given-names>B</given-names></string-name>, <string-name><surname>Rivaud</surname> <given-names>S</given-names></string-name>, <string-name><surname>Agid</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Pierrot-Deseilligny</surname> <given-names>C</given-names></string-name>. <article-title>Temporal Limits of Spatial Working Memory in Humans</article-title>. <source>European Journal of Neuroscience</source>. <year>1998</year>;<volume>10</volume>(<issue>2</issue>):<fpage>794</fpage>&#x2013;<lpage>797</lpage>. doi:<pub-id pub-id-type="doi">10.1046/j.1460-9568.1998.00101.x</pub-id>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><string-name><surname>White</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Sparks</surname> <given-names>DL</given-names></string-name>, <string-name><surname>Stanford</surname> <given-names>TR</given-names></string-name>. <article-title>Saccades to Remembered Target Locations: An Analysis of Systematic and Variable Errors</article-title>. <source>Vision Research</source>. <year>1994</year>;<volume>34</volume>(<issue>1</issue>):<fpage>79</fpage>&#x2013;<lpage>92</lpage>. doi:<pub-id pub-id-type="doi">10.1016/0042-6989(94)90259-3</pub-id>.</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Markram</surname> <given-names>H</given-names></string-name>, <string-name><surname>Goodman</surname> <given-names>PH</given-names></string-name>, <string-name><surname>Berger</surname> <given-names>TK</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>J</given-names></string-name>, <string-name><surname>Goldman-Rakic</surname> <given-names>PS</given-names></string-name>. <article-title>Heterogeneity in the Pyramidal Network of the Medial Prefrontal Cortex</article-title>. <source>Nature Neuroscience</source>. <year>2006</year>;<volume>9</volume>(<issue>4</issue>):<fpage>534</fpage>&#x2013;<lpage>42</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn1670</pub-id>.</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><string-name><surname>Yang</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Seamans</surname> <given-names>JK</given-names></string-name>, <string-name><surname>Gorelova</surname> <given-names>N</given-names></string-name>. <article-title>Electrophysiological and Morphological Properties of Layers V-VI Principal Pyramidal Cells in Rat Prefrontal Cortex in Vitro</article-title>. <source>Journal of Neuroscience</source>. <year>1996</year>;<volume>16</volume>(<issue>5</issue>):<fpage>1904</fpage>&#x2013;<lpage>1921</lpage>.</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><string-name><surname>D&#x00E9;gen&#x00E8;tais</surname> <given-names>E</given-names></string-name>, <string-name><surname>Thierry</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Glowinski</surname> <given-names>J</given-names></string-name>, <string-name><surname>Gioanni</surname> <given-names>Y</given-names></string-name>. <article-title>Electrophysiological Properties of Pyramidal Neurons in the Rat Prefrontal Cortex: An In Vivo Intracellular Recording Study</article-title>. <source>Cerebral Cortex</source>. <year>2002</year>;<volume>12</volume>(<issue>1</issue>):<fpage>1</fpage>&#x2013;<lpage>16</lpage>. doi:<pub-id pub-id-type="doi">10.1093/cercor/12.1.1</pub-id>.</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><string-name><surname>Collins</surname> <given-names>CE</given-names></string-name>, <string-name><surname>Turner</surname> <given-names>EC</given-names></string-name>, <string-name><surname>Sawyer</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Reed</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Young</surname> <given-names>NA</given-names></string-name>, <string-name><surname>Flaherty</surname> <given-names>DK</given-names></string-name>, <etal>et al.</etal> <article-title>Cortical Cell and Neuron Density Estimates in One Chimpanzee Hemisphere</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2016</year>;<volume>113</volume>(<issue>3</issue>):<fpage>740</fpage>&#x2013;<lpage>745</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1524208113</pub-id>.</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><string-name><surname>Funahashi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Bruce</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Goldman-Rakic</surname> <given-names>PS</given-names></string-name>. <article-title>Mnemonic Coding of Visual Space in the Monkey&#x2019;s Dorsolateral Prefrontal Cortex</article-title>. <source>Journal of Neurophysiology</source>. <year>1989</year>; p. <fpage>331</fpage>&#x2013;<lpage>349</lpage>.</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><string-name><surname>Chafee</surname> <given-names>MV</given-names></string-name>, <string-name><surname>Goldman-Rakic</surname> <given-names>PS</given-names></string-name>. <article-title>Matching Patterns of Activity in Primate Prefrontal Area 8a and Parietal Area 7ip Neurons During a Spatial Working MemoryTask</article-title>. <source>Journal of Neurophysiology</source>. <year>1998</year>;<volume>79</volume>(<issue>6</issue>):<fpage>2919</fpage>&#x2013;<lpage>2940</lpage>.</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="other"><string-name><surname>Wibisono</surname> <given-names>A</given-names></string-name>, <string-name><surname>Jog</surname> <given-names>V</given-names></string-name>, <string-name><surname>Loh</surname> <given-names>PL</given-names></string-name>. <source>Information and Estimation in Fokker-Planck Channels</source>. arXiv preprint arXiv:<pub-id pub-id-type="arxiv">170203656</pub-id>. <year>2017</year>;.</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><string-name><surname>Sandamirskaya</surname> <given-names>Y</given-names></string-name>. <article-title>Dynamic Neural Fields as a Step toward Cognitive Neuromorphic Architectures</article-title>. <source>Frontiers in Neuroscience</source>. <year>2014</year>;<volume>7</volume>. doi:<pub-id pub-id-type="doi">10.3389/fnins.2013.00276</pub-id>.</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><string-name><surname>Pan</surname> <given-names>B</given-names></string-name>, <string-name><surname>Zucker</surname> <given-names>RS</given-names></string-name>. <article-title>A General Model of Synaptic Transmission and Short-Term Plasticity</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>62</volume>(<issue>4</issue>):<fpage>539</fpage>&#x2013;<lpage>554</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2009.03.025</pub-id>.</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><string-name><surname>Brunton</surname> <given-names>BW</given-names></string-name>, <string-name><surname>Botvinick</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Brody</surname> <given-names>CD</given-names></string-name>. <article-title>Rats and Humans Can Optimally Accumulate Evidence for Decision-Making</article-title>. <source>Science</source>. <year>2013</year>;<volume>340</volume>(<issue>6128</issue>):<fpage>95</fpage>&#x2013;<lpage>98</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1233912</pub-id>.</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><string-name><surname>Selemon</surname> <given-names>LD</given-names></string-name>, <string-name><surname>Goldman-Rakic</surname> <given-names>PS</given-names></string-name>. <article-title>Common Cortical and Subcortical Targets of the Dorsolateral Prefrontal and Posterior Parietal Cortices in the Rhesus Monkey: Evidence for a Distributed Neural Network Subserving Spatially Guided Behavior</article-title>. <source>Journal of Neuroscience</source>. <year>1988</year>;<volume>8</volume>(<issue>11</issue>):<fpage>4049</fpage>&#x2013;<lpage>4068</lpage>.</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><string-name><surname>Roach</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Ben-Jacob</surname> <given-names>E</given-names></string-name>, <string-name><surname>Sander</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Zochowski</surname> <given-names>MR</given-names></string-name>. <article-title>Formation and Dynamics of Waves in a Cortical Model of Cholinergic Modulation</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year>;<volume>11</volume>(<issue>8</issue>):<fpage>e1004449</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1004449</pub-id>.</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="journal"><string-name><surname>Lim</surname> <given-names>S</given-names></string-name>, <string-name><surname>Goldman</surname> <given-names>MS</given-names></string-name>. <article-title>Balanced Cortical Microcircuitry for Spatial Working Memory Based on Corrective Feedback Control</article-title>. <source>Journal of Neuroscience</source>. <year>2014</year>;<volume>34</volume>(<issue>20</issue>):<fpage>6790</fpage>&#x2013;<lpage>6806</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.4602-13.2014</pub-id>.</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><string-name><surname>Anwar</surname> <given-names>H</given-names></string-name>, <string-name><surname>Li</surname> <given-names>X</given-names></string-name>, <string-name><surname>Bucher</surname> <given-names>D</given-names></string-name>, <string-name><surname>Nadim</surname> <given-names>F</given-names></string-name>. <article-title>Functional Roles of Short-Term Synaptic Plasticity with an Emphasis on Inhibition</article-title>. <source>Current 0pinion in Neurobiology</source>. <year>2017</year>;<volume>43</volume>:<fpage>71</fpage>&#x2013;<lpage>78</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.conb.2017.01.002</pub-id>.</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="journal"><string-name><surname>Nadim</surname> <given-names>F</given-names></string-name>, <string-name><surname>Bucher</surname> <given-names>D</given-names></string-name>. <article-title>Neuromodulation of Neurons and Synapses</article-title>. <source>Current Opinion in Neurobiology</source>. <year>2014</year>;<volume>29</volume>:<fpage>48</fpage>&#x2013;<lpage>56</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.conb.2014.05.003</pub-id>.</mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><string-name><surname>Oh</surname> <given-names>M</given-names></string-name>, <string-name><surname>Zhao</surname> <given-names>S</given-names></string-name>, <string-name><surname>Matveev</surname> <given-names>V</given-names></string-name>, <string-name><surname>Nadim</surname> <given-names>F</given-names></string-name>. <article-title>Neuromodulatory Changes in Short-Term Synaptic Dynamics May Be Mediated by Two Distinct Mechanisms of Presynaptic Calcium Entry</article-title>. <source>Journal of Computational Neuroscience</source>. <year>2012</year>;<volume>33</volume>(<issue>3</issue>). doi:<pub-id pub-id-type="doi">10.1007/s10827-012-0402-z</pub-id>.</mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><string-name><surname>Kreitzer</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Regehr</surname> <given-names>WG</given-names></string-name>. <article-title>Modulation of Transmission during Trains at a Cerebellar Synapse</article-title>. <source>Journal of Neuroscience</source>. <year>2000</year>;<volume>20</volume>(<issue>4</issue>):<fpage>1348</fpage>&#x2013;<lpage>1357</lpage>.</mixed-citation></ref>
<ref id="c77"><label>77.</label><mixed-citation publication-type="journal"><string-name><surname>Brenowitz</surname> <given-names>S</given-names></string-name>, <string-name><surname>David</surname> <given-names>J</given-names></string-name>, <string-name><surname>Trussell</surname> <given-names>L</given-names></string-name>. <article-title>Enhancement of Synaptic Efficacy by Presynaptic GABAB Receptors</article-title>. <source>Neuron</source>. <year>1998</year>;<volume>20</volume>(<issue>1</issue>):<fpage>135</fpage>&#x2013;<lpage>141</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0896-6273(00)80441-9</pub-id>.</mixed-citation></ref>
<ref id="c78"><label>78.</label><mixed-citation publication-type="journal"><string-name><surname>Barriere</surname> <given-names>G</given-names></string-name>, <string-name><surname>Tartas</surname> <given-names>M</given-names></string-name>, <string-name><surname>Cazalets</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Bertrand</surname> <given-names>SS</given-names></string-name>. <article-title>Interplay between Neuromodulator-Induced Switching of Short-Term Plasticity at Sensorimotor Synapses in the Neonatal Rat Spinal Cord</article-title>. <source>The Journal of Physiology</source>. <year>2008</year>;<volume>586</volume>(Pt <issue>7</issue>):<fpage>1903</fpage>&#x2013;<lpage>1920</lpage>. doi:<pub-id pub-id-type="doi">10.1113/jphysiol.2008.150706</pub-id>.</mixed-citation></ref>
<ref id="c79"><label>79.</label><mixed-citation publication-type="journal"><string-name><surname>Hempel</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Hartman</surname> <given-names>KH</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>XJ</given-names></string-name>, <string-name><surname>Turrigiano</surname> <given-names>GG</given-names></string-name>, <string-name><surname>Nelson</surname> <given-names>SB</given-names></string-name>. <article-title>Multiple Forms of Short-Term Plasticity at Excitatory Synapses in Rat Medial Prefrontal Cortex</article-title>. <source>Journal of Neurophysiology</source>. <year>2000</year>;<volume>83</volume>(<issue>5</issue>):<fpage>3031</fpage>&#x2013;<lpage>3041</lpage>.</mixed-citation></ref>
<ref id="c80"><label>80.</label><mixed-citation publication-type="journal"><string-name><surname>Sakurai</surname> <given-names>A</given-names></string-name>, <string-name><surname>Katz</surname> <given-names>PS</given-names></string-name>. <article-title>State-, Timing-, and Pattern-Dependent Neuromodulation of Synaptic Strength by a Serotonergic Interneuron</article-title>. <source>Journal of Neuroscience</source>. <year>2009</year>;<volume>29</volume>(<issue>1</issue>):<fpage>268</fpage>&#x2013;<lpage>279</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.4456-08.2009</pub-id>.</mixed-citation></ref>
<ref id="c81"><label>81.</label><mixed-citation publication-type="journal"><string-name><surname>Laing</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Longtin</surname> <given-names>A</given-names></string-name>. <article-title>Noise-Induced Stabilization of Bumps in Systems with Long-Range Spatial Coupling</article-title>. <source>Physica D: Nonlinear Phenomena</source>. <year>2001</year>;<volume>160</volume>(<issue>3</issue>):<fpage>149</fpage>&#x2013;<lpage>172</lpage>.</mixed-citation></ref>
<ref id="c82"><label>82.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname> <given-names>H</given-names></string-name>, <string-name><surname>Lam</surname> <given-names>K</given-names></string-name>, <string-name><surname>Fung</surname> <given-names>CCA</given-names></string-name>, <string-name><surname>Wong</surname> <given-names>KYM</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>S</given-names></string-name>. <article-title>Rich Spectrum of Neural Field Dynamics in the Presence of Short-Term Synaptic Depression</article-title>. <source>Physical Review E</source>. <year>2015</year>;<volume>92</volume>(<issue>3</issue>):<fpage>032908</fpage>. doi:<pub-id pub-id-type="doi">10.1103/PhysRevE.92.032908</pub-id>.</mixed-citation></ref>
<ref id="c83"><label>83.</label><mixed-citation publication-type="journal"><string-name><surname>Mi</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Lin</surname> <given-names>X</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>S</given-names></string-name>. <article-title>Neural Computations in a Dynamical System with Multiple Time Scales</article-title>. <source>Frontiers in Computational Neuroscience</source>. <year>2016</year>;<volume>10</volume>. doi:<pub-id pub-id-type="doi">10.3389/fncom.2016.00096</pub-id>.</mixed-citation></ref>
<ref id="c84"><label>84.</label><mixed-citation publication-type="journal"><string-name><surname>Fung</surname> <given-names>C</given-names></string-name>, <string-name><surname>Wong</surname> <given-names>K</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>S</given-names></string-name>. <article-title>A Moving Bump in a Continuous Manifold: A Comprehensive Study of the Tracking Dynamics of Continuous Attractor Neural Networks</article-title>. <source>Neural Computation</source>. <year>2010</year>;<volume>22</volume>(<issue>3</issue>):<fpage>752</fpage>&#x2013;<lpage>792</lpage>.</mixed-citation></ref>
<ref id="c85"><label>85.</label><mixed-citation publication-type="journal"><string-name><surname>Brunel</surname> <given-names>N</given-names></string-name>. <article-title>Dynamics of Sparsely Connected Networks of Excitatory and Inhibitory Spiking Neurons</article-title>. <source>Journal of Computational Neuroscience</source>. <year>2000</year>;<volume>8</volume>(<issue>3</issue>):<fpage>183</fpage>&#x2013;<lpage>208</lpage>.</mixed-citation></ref>
<ref id="c86"><label>86.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname> <given-names>XJ</given-names></string-name>. <article-title>Synaptic Reverberation Underlying Mnemonic Persistent Activity</article-title>. <source>Trends in Neurosciences</source>. <year>2001</year>;<volume>24</volume>(<issue>8</issue>):<fpage>455</fpage>&#x2013;<lpage>63</lpage>.</mixed-citation></ref>
<ref id="c87"><label>87.</label><mixed-citation publication-type="journal"><string-name><surname>Renart</surname> <given-names>A</given-names></string-name>, <string-name><surname>Moreno-Bote</surname> <given-names>R</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>XJ</given-names></string-name>, <string-name><surname>Parga</surname> <given-names>N</given-names></string-name>. <article-title>Mean-Driven and Fluctuation-Driven Persistent Activity in Recurrent Networks</article-title>. <source>Neural Computation</source>. <year>2007</year>;<volume>19</volume>(<issue>1</issue>):<fpage>1</fpage>&#x2013;<lpage>46</lpage>. doi:<pub-id pub-id-type="doi">10.1162/neco.2007.19.1.1</pub-id>.</mixed-citation></ref>
<ref id="c88"><label>88.</label><mixed-citation publication-type="journal"><string-name><surname>Barbieri</surname> <given-names>F</given-names></string-name>, <string-name><surname>Brunel</surname> <given-names>N</given-names></string-name>. <article-title>Can Attractor Network Models Account for the Statistics of Firing during Persistent Activity in Prefrontal Cortex?</article-title> <source>Frontiers in neuroscience</source>. <year>2008</year>;<volume>2</volume>(<issue>1</issue>):<fpage>114</fpage>&#x2013;<lpage>122</lpage>.</mixed-citation></ref>
<ref id="c89"><label>89.</label><mixed-citation publication-type="journal"><string-name><surname>Compte</surname> <given-names>A</given-names></string-name>, <string-name><surname>Constantinidis</surname> <given-names>C</given-names></string-name>, <string-name><surname>Tegn&#x00E9;r</surname> <given-names>J</given-names></string-name>, <string-name><surname>Raghavachari</surname> <given-names>S</given-names></string-name>, <string-name><surname>Chafee</surname> <given-names>MV</given-names></string-name>, <string-name><surname>Goldman-Rakic</surname> <given-names>PS</given-names></string-name>, <etal>et al.</etal> <article-title>Temporally Irregular Mnemonic Persistent Activity in Prefrontal Neurons of Monkeys During a Delayed Response Task</article-title>. <source>Journal of Neurophysiology</source>. <year>2003</year>;<volume>90</volume>(<issue>5</issue>):<fpage>3441</fpage>&#x2013;<lpage>3454</lpage>. doi:<pub-id pub-id-type="doi">10.1152/jn.00949.2002</pub-id>.</mixed-citation></ref>
<ref id="c90"><label>90.</label><mixed-citation publication-type="journal"><string-name><surname>Mongillo</surname> <given-names>G</given-names></string-name>, <string-name><surname>Hansel</surname> <given-names>D</given-names></string-name>, <string-name><surname>van Vreeswijk</surname> <given-names>C</given-names></string-name>. <article-title>Bistability and Spatiotemporal Irregularity in Neuronal Networks with Nonlinear Synaptic Transmission</article-title>. <source>Physical Review Letters</source>. <year>2012</year>;<volume>108</volume>(<issue>15</issue>):<fpage>158101</fpage>. doi:<pub-id pub-id-type="doi">10.1103/PhysRevLett.108.158101</pub-id>.</mixed-citation></ref>
<ref id="c91"><label>91.</label><mixed-citation publication-type="journal"><string-name><surname>Markram</surname> <given-names>H</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Tsodyks</surname> <given-names>M</given-names></string-name>. <article-title>Differential Signaling via the Same Axon of Neocortical Pyramidal Neurons</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1998</year>;<volume>95</volume>(<issue>9</issue>):<fpage>5323</fpage>&#x2013;<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c92"><label>92.</label><mixed-citation publication-type="journal"><string-name><surname>Mongillo</surname> <given-names>G</given-names></string-name>, <string-name><surname>Barak</surname> <given-names>O</given-names></string-name>, <string-name><surname>Tsodyks</surname> <given-names>M</given-names></string-name>. <article-title>Synaptic Theory of Working Memory</article-title>. <source>Science</source>. <year>2008</year>;<volume>319</volume>(<issue>5869</issue>):<fpage>1543</fpage>&#x2013;<lpage>1546</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1150769</pub-id>.</mixed-citation></ref>
<ref id="c93"><label>93.</label><mixed-citation publication-type="book"><string-name><surname>Gerstner</surname> <given-names>W</given-names></string-name>, <string-name><surname>Kistler</surname> <given-names>WM</given-names></string-name>, <string-name><surname>Naud</surname> <given-names>R</given-names></string-name>, <string-name><surname>Paninski</surname> <given-names>L</given-names></string-name>. <source>Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition</source>. <publisher-name>Cambridge University Press</publisher-name>; <year>2014</year>.</mixed-citation></ref>
<ref id="c94"><label>94.</label><mixed-citation publication-type="other"><string-name><surname>Seeholzer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Deger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Gerstner</surname> <given-names>W</given-names></string-name>. <source>Efficient Low-Dimensional Approximation of Continuous Attractor Networks</source>. arXiv:<pub-id pub-id-type="arxiv">171108032</pub-id> [q-bio]. <year>2017</year>;.</mixed-citation></ref>
<ref id="c95"><label>95.</label><mixed-citation publication-type="book"><string-name><surname>Forster</surname> <given-names>O</given-names></string-name>. <source>Analysis 1: Differential- und Integralrechnung einer Ver&#x00E4;nderlichen</source>. <publisher-name>Springer-Verlag</publisher-name>; <year>2016</year>.</mixed-citation></ref>
<ref id="c96"><label>96.</label><mixed-citation publication-type="website"><string-name><surname>Bos</surname> <given-names>H</given-names></string-name>, <string-name><surname>Morrison</surname> <given-names>A</given-names></string-name>, <string-name><surname>Peyser</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hahne</surname> <given-names>J</given-names></string-name>, <string-name><surname>Helias</surname> <given-names>M</given-names></string-name>, <string-name><surname>Kunkel</surname> <given-names>S</given-names></string-name>, <etal>et al.</etal> <source>NEST 2.10.0</source>; <year>2015</year>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.44222">https://doi.org/10.5281/zenodo.44222</ext-link>.</mixed-citation></ref>
<ref id="c97"><label>97.</label><mixed-citation publication-type="other"><string-name><surname>Galassi</surname> <given-names>M</given-names></string-name>, <string-name><surname>Davies</surname> <given-names>J</given-names></string-name>, <string-name><surname>Theiler</surname> <given-names>J</given-names></string-name>, <string-name><surname>Gough</surname> <given-names>B</given-names></string-name>, <string-name><surname>Jungman</surname> <given-names>G</given-names></string-name>, <string-name><surname>Booth</surname> <given-names>M</given-names></string-name>, <etal>et al.</etal> <source>GNU Scientific Library Reference Manual</source>. <edition>3rd ed.</edition>; <year>2009</year>.</mixed-citation></ref>
<ref id="c98"><label>98.</label><mixed-citation publication-type="journal"><string-name><surname>Nawrot</surname> <given-names>M</given-names></string-name>, <string-name><surname>Aertsen</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rotter</surname> <given-names>S</given-names></string-name>. <article-title>Single-Trial Estimation of Neuronal Firing Rates: From Single-Neuron Spike Trains to Population Activity</article-title>. <source>Journal of Neuroscience Methods</source>. <year>1999</year>;<volume>94</volume>:<fpage>81</fpage>&#x2013;<lpage>92</lpage>.</mixed-citation></ref>
<ref id="c99"><label>99.</label><mixed-citation publication-type="website"><string-name><surname>Jones</surname> <given-names>E</given-names></string-name>, <string-name><surname>Oliphant</surname> <given-names>T</given-names></string-name>, <string-name><surname>Peterson</surname></string-name>. <source>SciPy.Org &#x2014; SciPy.Org</source>; <year>2017</year>. <ext-link ext-link-type="uri" xlink:href="http://scipy.org/">http://scipy.org/</ext-link>.</mixed-citation></ref>
<ref id="c100"><label>100.</label><mixed-citation publication-type="book"><string-name><surname>Efron</surname> <given-names>B</given-names></string-name>, <string-name><surname>Tibshirani</surname> <given-names>RJ</given-names></string-name>. <source>An Introduction to the Bootstrap</source>. <publisher-name>CRC Press</publisher-name>; <year>1994</year>.</mixed-citation></ref>
<ref id="c101"><label>101.</label><mixed-citation publication-type="website"><string-name><surname>Evans</surname> <given-names>C</given-names></string-name>. <source>Scikits-Bootstrap</source>; <year>2017</year>. <ext-link ext-link-type="uri" xlink:href="https://github.com/cgevans/scikits-bootstrap">https://github.com/cgevans/scikits-bootstrap</ext-link>.</mixed-citation></ref>
<ref id="c102"><label>102.</label><mixed-citation publication-type="other"><string-name><surname>Abdi</surname> <given-names>H</given-names></string-name>, <string-name><given-names>Williams L.</given-names> <surname>Jackknife</surname></string-name>. <source>Encyclopedia of research design</source>. <year>2010</year>; p. <fpage>1</fpage>&#x2013;<lpage>10</lpage>.</mixed-citation></ref>
<ref id="c103"><label>103.</label><mixed-citation publication-type="journal"><string-name><surname>Behnel</surname> <given-names>S</given-names></string-name>, <string-name><surname>Bradshaw</surname> <given-names>R</given-names></string-name>, <string-name><surname>Citro</surname> <given-names>C</given-names></string-name>, <string-name><surname>Dalcin</surname> <given-names>L</given-names></string-name>, <string-name><surname>Seljebotn</surname> <given-names>DS</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>K</given-names></string-name>. <article-title>Cython: The Best of Both Worlds</article-title>. <source>Computing in Science Engineering</source>. <year>2011</year>;<volume>13</volume>(<issue>2</issue>):<fpage>31</fpage>&#x2013;<lpage>39</lpage>. doi:<pub-id pub-id-type="doi">10.1109/MCSE.2010.118</pub-id>.</mixed-citation></ref>
<ref id="c104"><label>104.</label><mixed-citation publication-type="journal"><string-name><surname>Oliphant</surname> <given-names>TE</given-names></string-name>. <article-title>Python for Scientific Computing</article-title>. <source>Computing in Science &#x0026; Engineering</source>. <year>2007</year>;<volume>9</volume>(<issue>3</issue>):<fpage>10</fpage>&#x2013;<lpage>20</lpage>. doi:<pub-id pub-id-type="doi">10.1109/MCSE.2007.58</pub-id>.</mixed-citation></ref>
</ref-list>
<sec id="s5" sec-type="supplementary-material">
<title>Supporting information</title>
<p><bold>S1 Fig. Spiking networks produce similar stable firing rate profiles across parameters.</bold> For each choice of short-term plasticity parameters <italic>U, &#x03C4;<sub>u</sub></italic>, and <italic>&#x03C4;<sub>x</sub></italic>, we tuned the recurrent conductances (<italic>g</italic><sub>EE</sub>, <italic>g</italic><sub>EI</sub>, <italic>g</italic><sub>IE</sub>, <italic>g</italic><sub>II</sub>) and the width <italic>&#x03C3;<sub>w</sub></italic> of the distance-dependent weights (cf. <xref ref-type="disp-formula" rid="eqn32">Eq. (32)</xref>) such that the &#x201C;bump&#x201D; shape of the stable firing rate profile is close to a generalized Gaussian <inline-formula><alternatives><inline-graphic xlink:href="424515_inline99.gif"/></alternatives></inline-formula> with parameters <italic>g</italic><sub>0</sub> &#x003D; 0.1Hz, <italic>g</italic><sub>1</sub> &#x003D; 40.0Hz, <italic>g<sub>&#x03C3;</sub></italic> &#x003D; 0.5, <italic>g<sub>r</sub></italic> &#x003D; 2.5. See <italic><xref ref-type="sec" rid="s4b6">Optimization of network parameters</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref> for details, S2 Table for parameter values after tuning, and S1 Table for parameters that stay constant. <bold>A</bold> After tuning, the resulting firing rate profiles for different parameter values of <italic>U</italic> and <italic>&#x03C4;<sub>u</sub></italic> are very similar. Averaged mean firing rates in bump state, measured from &#x007E; 1000 spiking simulations. <bold>A1-A3</bold> Remaining slight parameter-dependent changes of bump shapes, measured by fitting the generalized Gaussian <italic>v</italic>(<italic>&#x03B8;</italic>) to the measured firing rate profiles displayed in A. <bold>A1</bold> Top firing rate <italic>g</italic><sub>1</sub>. <bold>A2</bold> Half-width parameter <italic>g<sub>&#x03C3;</sub></italic>. <bold>A3</bold> Sharpness parameter <italic>g<sub>r</sub></italic>. <bold>B</bold> and <bold>B1-B3</bold> Same as in A and A1-A3, for additional variation of the depression time scale <italic>&#x03C4;<sub>x</sub></italic>.</p>
<p><bold>S2 Fig. Theoretical prediction of diffusion strength as a function of STP parameters.</bold> All color values display diffusion magnitude estimated from <italic>B</italic> in <xref ref-type="disp-formula" rid="eqn4">Eq. (4)</xref> with bump shape estimated from the reference network (<italic>U</italic> &#x003D; 1, <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 150ms, compare 3B,C, dashed lines). Units of color values are <inline-formula><alternatives><inline-graphic xlink:href="424515_inline100.gif"/></alternatives></inline-formula> with values of level lines as indicated. <bold>A</bold> Diffusion as function of facilitation <italic>U</italic> and depression time constant <italic>&#x03C4;<sub>x</sub></italic>. Facilitation time constant was <italic>&#x03C4;<sub>u</sub></italic> &#x003D; 650<italic>ms</italic>. <bold>B</bold> Diffusion as function of facilitation <italic>U</italic> and facilitation time constant <italic>&#x03C4;<sub>u</sub></italic>. Depression time constant was <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 150<italic>ms</italic>. <bold>C</bold> Diffusion as function of depression time constant <italic>&#x03C4;<sub>x</sub></italic> and facilitation time constant <italic>&#x03C4;<sub>u</sub></italic>. Facilitation <italic>U</italic> was <italic>U</italic> &#x003D; 0.5.</p>
<p><bold>S3 Fig. Comparison of theoretically predicted fields to simulations. A</bold> Averaged root mean square error (RMSE) between predicted fields (<xref ref-type="disp-formula" rid="eqn7">Eq. (7)</xref>) and fields extracted from simulations (mean over 18-20 networks, error bars show 95&#x0025; confidence of the mean). Both frozen noise parameters (<italic>&#x03C3;</italic><sub>L</sub> and 1 &#x2212; <italic>p</italic>) are plotted on the same x-axis. <bold>B</bold> Normalized RMSE: each RMSE is normalized by the range (max &#x2212; min) of the joint data of simulated and predicted fields it is calculated on. Colors as in A. <bold>C</bold> Average RMSE (same data as in A) plotted as a function of the mean expected field magnitude (estimated separately for each network, then averaged). Colors as in A. <bold>D</bold> Worst (top) and best (bottom) match between predicted field (blue line) and field extracted from simulations (black line) of the group with the largest mean RMSE in panels A, C (<italic>U</italic> &#x003D; 1, 1 &#x2212; <italic>p</italic> &#x003D; 0.75). Shaded areas show 1 standard deviation of points included in the binned mean estimate (100 bins) of the extracted field.</p>
<p><bold>S4 Fig. Mutual information normalized to compare slopes.</bold> Same data as in <xref ref-type="fig" rid="fig5">Fig. 5B</xref>, but MI is normalized to the average MI of each spiking network without heterogeneities (leftmost dot for each green, orange, and blue group of curves/dots), making explicitly visible the change in slope of the drop-off as heterogeneity parameters are increased. Dashed lines connect the means, for visual guidance.</p>
<p><bold>S5 Fig. Theoretical predictions of working memory stability.</bold> All panels show theoretically predicted expected displacement over 1 second (<xref ref-type="disp-formula" rid="eqn11">Eq. (11)</xref>) for networks with random and sparse connections (<italic>p</italic> &#x003D; 0.12) and leak reversal potential heterogeneity (<italic>&#x03C3;<sub>L</sub></italic> &#x003D; 1.7<italic>mV</italic>). White lines show displacement contour lines for 1, 2 and 5 deg. <bold>A</bold> Displacement as a function of the facilitation time constant <italic>&#x03C4;<sub>u</sub></italic> and facilitation <italic>U</italic> for <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 150<italic>ms</italic> and <italic>N</italic> &#x003D; 5000.<bold>B</bold> Displacement as a function of system size and facilitation <italic>U</italic> for <italic>&#x03C4;<sub>x</sub></italic> &#x003D; 150<italic>ms</italic> and <italic>&#x03C4;<sub>u</sub></italic> &#x003D; 650<italic>ms</italic>. <bold>C-D</bold> Displacement as a function of depression time constant <italic>&#x03C4;<sub>x</sub></italic> and facilitation <italic>U</italic> for <italic>N</italic> &#x003D; 5000 (C) and <italic>N</italic> &#x003D; 20000 (D). In both panels <italic>&#x03C4;<sub>u</sub></italic> &#x003D; 650<italic>ms</italic>.</p>
<p><bold>S6 Fig. Dependence of diffusion strength B on shape parameters.</bold> Diffusion was calculated from <xref ref-type="disp-formula" rid="eqn5">Eq. (5)</xref> with bump solutions <inline-formula><alternatives><inline-graphic xlink:href="424515_inline101.gif"/></alternatives></inline-formula>. The values of <inline-formula><alternatives><inline-graphic xlink:href="424515_inline102.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="424515_inline103.gif"/></alternatives></inline-formula> were calculated by fitting and extrapolating (linearly, for <italic>&#x03D5;</italic><sub>0</sub> &#x003C; 40.31Hz) curves <inline-formula><alternatives><inline-graphic xlink:href="424515_inline104.gif"/></alternatives></inline-formula> and <italic>&#x03D5;</italic><sub>0</sub> &#x2192; <italic>J</italic><sub>0</sub> that were obtained from the numerical values extracted for <italic>g</italic><sub>1</sub> &#x003D; 40.31Hz, <italic>g<sub>&#x03C3;</sub></italic> &#x003D; 0.51 by theory (see <italic><xref ref-type="sec" rid="s4b4">Firing rate approximation</xref></italic> in <xref ref-type="sec" rid="s4">Materials and Methods</xref>). Thus, any nonlinearity or saturation of the inputs and input-output relation for <italic>&#x03D5;</italic><sub>0</sub> &#x003C; 40.31Hz was not included. This approximate analysis shows that the major dependence of the diffusion expected in the system is on the bump width <italic>g<sub>&#x03C3;</sub></italic>, although a minor dependence on <italic>g</italic><sub>1</sub> is seen.</p>
<p><bold>S7 Fig. Short-term plasticity does not affect spiking statistics.</bold> Mean firing rate, coefficient of variation of the inter-spike interval distribution (CV), and local CV (<italic>CV</italic><sub>2</sub> [<xref ref-type="bibr" rid="c89">89</xref>]) for two attractor networks with different STP parameters. All measures were computed on spike-trains measured over a period of 4<italic>s</italic>, recorded 500<italic>ms</italic> after offset of the external input which was centered at angle 0. Across STP parameters, networks display similarly reduced CVs for increased mean firing rates, leading to large CVs for neurons located in the flanks of the firing rate profile and low CVs for neurons located near the center. <bold>A</bold> Networks with large diffusion coefficient (<italic>U</italic> &#x003D; 0.8, <italic>&#x03C4;<sub>u</sub></italic> &#x003D; 650<italic>ms, &#x03C4;<sub>x</sub></italic> &#x003D; 200<italic>ms</italic>) that underwent non-stationary diffusion during the recording of spikes: the measured mean firing rates (gray line) differ visibly from the firing rates estimated after centering the firing rate distribution at each point in time. Due to this non-stationarity, CVs at intermediate firing rates appear elevated, while the local CV (<italic>CV</italic><sub>2</sub>) shows values close to stationary networks (see B). <bold>B</bold> The same network as in A, with strong facilitation (<italic>U</italic> &#x003D; 0.1). Reduced diffusion leads to a nearly stationary firing rate profile, and coincident CV and <italic>CV</italic><sub>2</sub> measures.</p>
<sec id="s5a">
<label>S1 Supporting Information.</label>
<title>Detailed mathematical derivations</title>
<p><bold>S1 Table. Parameters for spiking simulations.</bold> Parameter values are modified from [<xref ref-type="bibr" rid="c11">11</xref>] and [<xref ref-type="bibr" rid="c49">49</xref>]. For recurrent conductances see the table in S2 Table.</p>
<p><bold>S2 Table. Conductance and connectivity parameters for spiking simulations.</bold> For all networks we set <italic>w</italic><sub>&#x002B;</sub> &#x003D; 4.0. Recurrent conductance parameters are given for combinations of short-term plasticity parameters according to the following notation. <italic>g</italic><sub>EE</sub>: excitatory conductance <italic>g</italic><sub>E</sub> on excitatory neurons; <italic>g</italic><sub>IE</sub>: excitatory conductance <italic>g</italic><sub>E</sub> on inhibitory neurons; <italic>g</italic><sub>EI</sub>: inhibitory conductance <italic>g</italic><sub>I</sub> on excitatory neurons; <italic>g</italic><sub>II</sub>: inhibitory conductance <italic>g</italic><sub>I</sub> on inhibitory neurons.</p>
</sec>
</sec>
<fn-group>
<fn id="fn1"><label>1</label><p>In Brownian motion, the <italic>diffusion constant</italic> is usually defined as <italic>D</italic> &#x003D; <italic>B</italic>/2.</p></fn>
</fn-group>
</back>
</article>
