<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/466920</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Prefrontal cortex creates novel navigation sequences from hippocampal place-cell replay with spatial reward propagation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Cazin</surname>
<given-names>Nicolas</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Alonso</surname>
<given-names>Martin Llofriu</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chiodi</surname>
<given-names>Pablo Scleidorovich</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Pelc</surname>
<given-names>Tatiana</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1199-136X</contrib-id>
<name>
<surname>Harland</surname>
<given-names>Bruce</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Weitzenfeld</surname>
<given-names>Alfredo</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Fellous</surname>
<given-names>Jean-Marc</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9318-179X</contrib-id>
<name>
<surname>Dominey</surname>
<given-names>Peter Ford</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Human and Robot Cognitive Systems Group</institution>, INSERM, <country>France</country></aff>
<aff id="a2"><label>2</label><institution>College of Engineering, University of South Florida</institution>, <country>USA</country></aff>
<aff id="a3"><label>3</label><institution>Department of Psychology, University of Arizona</institution>, <country>USA</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><bold>Corresponding Author</bold>: Peter Ford Dominey</corresp>
<fn fn-type="conflict"><p><bold>Conflict of Interest</bold>: No conflict of interest</p></fn>
</author-notes>
<pub-date pub-type="epub"><year>2018</year></pub-date>
<elocation-id>466920</elocation-id>
<history>
<date date-type="received">
<day>09</day>
<month>11</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>09</day>
<month>11</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>09</day>
<month>11</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="466920.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>As rats learn to search for multiple sources of food or water in a complex environment, they generate increasingly efficient trajectories between reward sites, across multiple trials. This optimization capacity has been characterized in the Traveling Salesrat Problem (TSP) (<underline><xref ref-type="bibr" rid="c6">de Jong et al (2011)</xref></underline>. Such spatial navigation capacity involves the replay of hippocampal place-cells during awake states, generating small sequences of spatially related place-cell activity that we call &#x201C;snippets&#x201D;. These snippets occur primarily during sharp-wave-ripple (SWR) events. Here we focus on the role of replay during the awake state, as the animal is learning across multiple trials. We hypothesize that snippet replay generates synthetic data that can substantially expand and restructure the experience available to make PFC learning more optimal. We developed a model of snippet generation that is modulated by reward, propagated in the forward and reverse directions. This implements a form of spatial credit assignment for reinforcement learning. We use a biologically motivated computational framework known as &#x2018;reservoir computing&#x2019; to model PFC in sequence learning, in which large pools of prewired neural elements process information dynamically through reverberations. This PFC model is ideal to consolidate snippets into larger spatial sequences that may be later recalled by subsets of the original sequences. Our simulation experiments provide neurophysiological explanations for two pertinent observations related to navigation. Reward modulation allows the system to reject non-optimal segments of experienced trajectories, and reverse replay allows the system to &#x201C;learn&#x201D; trajectories that is has not physically experienced, both of which significantly contribute to the TSP behavior.</p>
<sec>
<title>Author Summary</title>
<p>As rats search for multiple sources of food in a complex environment, they generate increasingly efficient trajectories between reward sites, across multiple trials, characterized in the Traveling Salesrat Problem (TSP). This likely involves the coordinated replay of place-cell &#x201C;snippets&#x201D; between successive trials. We hypothesize that &#x201C;snippets&#x201D; can be used by the prefrontal cortex (PFC) to implement a form of reward-modulated reinforcement learning. Our simulation experiments provide neurophysiological explanations for two pertinent observations related to navigation. Reward modulation allows the system to reject non-optimal segments of experienced trajectories, and reverse replay allows the system to &#x201C;learn&#x201D; trajectories that it has not physically experienced, both of which significantly contribute to the TSP behavior.</p>
</sec>
</abstract>
<counts>
<page-count count="40"/>
</counts>
</article-meta>
<ack>
<title>Acknowledgements</title>
<p>Work supported by NSF-ANR CRCNS (#1429929, Spaquence)</p>
</ack>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Spatial navigation in the rat involves the replay of place-cell subsequences (snippets) during awake and sleep states in the hippocampus during sharp-wave-ripple (SWR) events (<underline><xref ref-type="bibr" rid="c3">Carr et al 2011</xref></underline>, <underline><xref ref-type="bibr" rid="c5">Davidson et al 2009</xref></underline>, <underline><xref ref-type="bibr" rid="c12">Euston et al 2007</xref></underline>, <underline><xref ref-type="bibr" rid="c21">Kudrimoti et al 1999</xref></underline>). We focus on the role of replay during the awake state (<xref ref-type="bibr" rid="c20">Karlsson &#x0026; Frank 2009</xref>), as the animal generates increasingly efficient trajectories between reward sites, across multiple trials. This trend toward near-optimal solutions is reminiscent of the classic Traveling Salesperson Problem (TSP) (<underline><xref ref-type="bibr" rid="c6">de Jong et al 2011</xref></underline>). While it appears likely that replay contributes to this learning behavior, the underlying neurophysiological mechanisms remain to be understood.</p>
<p>One obvious advantage of replay would be to provide extra training examples to otherwise slow reinforcement learning systems. This approach has been previously exploited with good results (<underline><xref ref-type="bibr" rid="c19">Johnson &#x0026; Redish 2005</xref></underline>). We will go beyond this by prioritizing replay based on a spatial gradient of reward proximity that is built up during replay. We hypothesize (a) that snippet replay allows recurrent dynamics in prefrontal cortex (PFC) to consolidate snippet representations into novel efficient sequences, by rejecting other sequences that are less robustly coded in the input, and (b) that a form of reward-modulated replay in hippocampus implements a simple and efficient form of reinforcement learning to achieve this (<xref ref-type="bibr" rid="c26">Singer &#x0026; Frank 2009</xref>).</p>
<p>An example of the behavior in question is illustrated in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Panel A illustrates the optimal path linking the 5 feeders (ABCDE) in red. Panels B-D illustrate navigation trajectories that contain subsequences of the optimal path (in red), as well as non-optimal subsequences (in blue). In the framework of reward modulated replay, snippets from the efficient subsequences in panels B-D will be replayed more frequently, and will lead the system to autonomously generate the optimal sequence as illustrated in panel A. We thus require a sequence learning system that can re-assemble the target sequences from these replayed snippets. For this we choose a biologically inspired recurrent network model of prefrontal cortex (<underline><xref ref-type="bibr" rid="c9">Dominey 1995</xref></underline>, <underline><xref ref-type="bibr" rid="c11">Enel et al 2016</xref></underline>) that we believe will be able to integrate snippets from examples of non-optimal trajectories and to synthesize an optimal path.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><p>An optimal trajectory between feeders <underline>ABCDE</underline> is represented in panel A. Panel B, C and D display non optimal trajectories that contain a sub trajectory of the <underline>ABCDE</underline> trajectory. The sub trajectory shared with the <underline>ABCDE</underline> trajectory is displayed in red and the non-optimal parts in blue. Panel B contains the <underline>ABC</underline>ED, panel C the <underline>EBCDA</underline> trajectory and panel D the BA<underline>CDE</underline> trajectory.</p></caption>
<graphic xlink:href="466920_fig1.tif"/>
</fig>
<p>For sequence learning, recurrent networks provide inherent sensitivity to serial and temporal structure. Modification of recurrent connections requires different methods of unwinding the recurrent connections in time, which limits the full dynamics of the recurrent system over extended time. To avoid this temporal cut-off and the space and time complexity required in the calculation of credit assignment to recurrent connections we used the framework of reservoir computing in which input and recurrent connections are fixed, and learning-related plasticity occur outside the reservoir network (<xref ref-type="bibr" rid="c9">Dominey 1995</xref>). The readout connections from the recurrent network learn the statistical structure of the data that the system is trained on, which places requirements on the mechanism that trains the model. We test the hypothesis that the structure of snippet replay from the hippocampus will provide the PFC with constraints that can be integrated in order to contribute to solving the TSP problem.</p>
<p>Two principal physical and neurophysiological properties of navigation and replay are exploited by the model and contribute to the system&#x2019;s ability to converge onto an acceptable solution to the TSP. First, during navigation between baited food wells in the TSP task, non-optimal trajectories by definition cover more distance between rewards than near-optimal ones. Second, during the replay of recently activated places cells, the trajectories are encoded in forward and reverse directions (<underline><xref ref-type="bibr" rid="c8">Diba &#x0026; Buzsaki 2007</xref></underline>, <underline><xref ref-type="bibr" rid="c13">Foster &#x0026; Wilson 2006</xref></underline>). Exploiting these observations, we test the hypotheses that:
<list list-type="order">
<list-item><p>With replay biased by distance to reward, non-optimal trajectories will be less represented in replay, allowing the PFC to eliminate non-optimal subsequences in constructing the final efficient trajectory.</p></list-item>
<list-item><p>Reverse replay will allow the model to exploit the information provided by a given sequence in forward and backward directions, whereas the actual trajectory run by the rat has one direction only.</p></list-item>
</list>
</p>
<p>In testing these hypotheses, we will illustrate how the system can meet the following challenges:
<list list-type="order">
<list-item><p>Learn a global place-cell activation sequence from an unordered set of snippets</p></list-item>
<list-item><p>Consolidate multiple non-optimal sequences into a trajectory that efficiently links rewarded locations, thus converging to a good solution to the TSP problem.</p></list-item>
<list-item><p>Experience a trajectory in the forward direction and the learn to generate it in forward and backward direction, including concatenating parts of both forward and reverse replayed snippets in order to generate novel trajectories as demonstrated in <underline><xref ref-type="bibr" rid="c14">Gupta et al (2010)</xref></underline>.</p></list-item>
</list>
</p>
<p>The objective is to provide a coherent explanation of how critical aspects of replay &#x2013; notably its modulation by reward, and the forward and reverse aspects, can be exploited by a cortical sequence learning system in order to display novel and efficient navigation trajectory generation.</p>
<p>The model developed in this research provides a possible explanation of mechanisms that allow PFC and hippocampus to interact to perform path optimization. This implies functional connectivity between these two structures. In a recent review of hippocampal&#x2013;prefrontal interactions in memory-guided behavior <underline><xref ref-type="bibr" rid="c25">Shin and Jadhav (2016)</xref></underline> outlined a diverse set of direct and indirect connections that allow bi-directional interaction between these structures. Principal direct connections to PFC originate in the ventral and intermediate CA1 regions of the hippocampus (<underline><xref ref-type="bibr" rid="c4">Cenquizca &#x0026; Swanson 2007</xref></underline>, <underline><xref ref-type="bibr" rid="c15">Harland et al 2018</xref></underline>). direct connections between hippocampus and PFC pass via the medial temporal lobe (subiculus, entorhinal cortex, peri- and post-rhinal cortex) (<underline><xref ref-type="bibr" rid="c7">Delatour &#x0026; Witter 2002</xref></underline>), and the nucleus reuniens (<underline><xref ref-type="bibr" rid="c28">Vertes et al 2007</xref></underline>). Indeed memory replay is observed to be coordinated across hippocampus and multiple cortical areas including V1 (<underline><xref ref-type="bibr" rid="c18">Ji &#x0026; Wilson 2007</xref></underline>). These studies allow us to consider that there are anatomical pathways that justify the modeling of bi-directional interaction between PFC and hippocampus (<underline><xref ref-type="bibr" rid="c24">McClelland et al 1995</xref></underline>).</p>
</sec>
<sec id="s2">
<title>Material &#x0026; methods</title>
<p>Experiments are performed on navigation trajectories (observed from rat behavior, or generated automatically) that represent the recent experience from the simulated rat. Snippets are extracted from this experience, and used to train the output connections of the PFC reservoir. This requires the specification of a model of place-cell activation in order to generate snippets. Based on this training, the sequence generation performance is evaluated to test the hypotheses specified. The evaluation requires a method for comparing sequences generated with expected sequences that is based on the Fr&#x00E9;chet distance.</p>
<sec id="s2a">
<title>Navigation Behavior and Trajectories</title>
<p>A trajectory is a sequence of N contiguous two-dimensional coordinates sampled from time <italic>t</italic><sub>1</sub> to time <italic>t<sub>N</sub></italic> noted <italic>L</italic>(<italic>t</italic><sub>1</sub> &#x2192; <italic>t<sub>N</sub></italic>) that corresponds to the rat&#x2019;s traversal of the baited feeders. The spatial resolution of trajectories depicted in is 20 <italic>points/m</italic> along the trajectory. Experiments were performed using navigation trajectories, including those displayed in <xref ref-type="fig" rid="fig1">Figure 1</xref>, based on data recorded from rats as they ran the TSP task (<underline><xref ref-type="bibr" rid="c6">de Jong et al 2011</xref></underline>) in a circular arena having a radius of 151cm. Twenty-one fixed feeders are distributed according to a spiral shape. In a typical configuration, 5 feeders are baited with a food pellet. For a given configuration, the rat runs several trials which are initially random and inefficient, and become increasingly efficient over successive trials, characterizing the TSP behavior (<underline><xref ref-type="bibr" rid="c6">de Jong et al 2011</xref></underline>). Rat data that characterizes the TSP behavior is detailed in S1, section Rat navigation data. The principle concept is that TSP behavior can be characterized as illustrated in <xref ref-type="fig" rid="fig1">Figure 1</xref>, where a system that is exposed to trajectories that contain elements of the efficient path can extract and concatenate these subsequences in order to generate the efficient trajectory.</p>
</sec>
<sec id="s2b">
<title>Place-cells</title>
<p>The modeled rat navigates in a closed space of 2&#x00D7;2 meters where it can move freely in all direction within a limited range (&#x00B1; 110&#x00B0; left and right of straight ahead), and encodes locations using hippocampus place-cell activity. A given location <italic>s</italic> &#x003D; (<italic>x</italic>, <italic>y</italic>) is associated with a place-cell activation pattern by a set of 2D Gaussian place-fields:
<disp-formula id="eqn1">
<alternatives>
<graphic xlink:href="466920_eqn1.gif"/>
</alternatives>
</disp-formula>
</p>
<p>Where:
<list list-type="bullet">
<list-item><p><italic>k</italic> is the index of the place-cell</p></list-item>
<list-item><p><italic>f<sub>k</sub></italic>(<italic>s</italic>) is the mean firing rate of the <italic>k<sup>th</sup></italic> place-cell</p></list-item>
<list-item><p><italic>c<sub>k</sub></italic> is the (<italic>x</italic>, <italic>y</italic>) coordinate of the <italic>k<sup>th</sup></italic>place-cell</p></list-item>
<list-item><p><inline-formula><alternatives><inline-graphic xlink:href="466920_inline1.gif"/></alternatives></inline-formula> is a constant that will constrain the highest activations of the place-cell to be mostly contained in a circle of radius <italic>r<sub>k</sub></italic>, centered in <italic>c<sub>k</sub></italic></p></list-item>
<list-item><p><italic>r<sub>k</sub></italic> is the radius of the <italic>k<sup>th</sup></italic> place-field</p></list-item>
<list-item><p><italic>&#x0398;</italic> is the radius threshold which controls the spatial selectivity of the place-cell</p></list-item>
</list>
</p>
<p>Parameter <italic>w<sub>k</sub></italic> is a manner of defining the variance of the 2D Gaussian surface with a distance to center related parameter <italic>r<sub>k</sub></italic>. We model a uniform grid of 16&#x00D7;16 Gaussian place-fields of equal size (mimicking dorsal hippocampus). In <xref ref-type="fig" rid="fig2">Figure 2</xref> the spatial position and extent of the place fields of several place-cells is represented in panel A by red circles. The degree of red transparency represents the mean firing rate.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><p>Place-cell and Snippet coding. Panel A represents the place-cell activations that correspond to a single point. Place-cell centers are represented by red points and the mean firing rate of each place-cell by a red circle with a fixed radius, centered on the place-cell center. The transparency level of the circle represents the magnitude of the mean firing rate. Panel B depicts the ABCED trajectory, the two randomly selected reward sites <italic>R<sub>1</sub></italic> and <italic>R<sub>2</sub></italic> and a snippet randomly drawn between <italic>R<sub>1</sub></italic> and <italic>R<sub>2</sub></italic>. The snippet length is <italic>s &#x003D; 5</italic>. Panel C represents the raster of the place-cell activation along the ABCED trajectory. The time index where feeders A,B (<italic>R<sub>1</sub></italic>), C,D and E(<italic>R<sub>2</sub></italic>) are encountered during the ABCED trajectory are tagged above the raster and represented by a thin white vertical line. The snippet represented in panel B is emphasized by a blue rectangle in panel C. Panel D represents the snippet replay likelihood as learnt by the Hippocampal replay model and panel E represents the spatial extent of the snippet replay likelihood</p></caption>
<graphic xlink:href="466920_fig2.tif"/>
</fig>
<p>A mean firing rate close to one will result in an bright circle if the location <italic>s</italic> is close to the place-field center <italic>c<sub>k</sub></italic> of the place-cell <italic>k</italic>. For a more distant place-field center <italic>c<sub>l</sub></italic> of place-cell <italic>l</italic>, the mean firing rate will be less important and the red circle representing this mean firing rate will be dimmer.</p>
<p>Thus at each time step, the place-cell coding that corresponds to a particular point in a trajectory is defined as the projection of this <italic>L</italic>(<italic>t<sub>n</sub></italic>) point through <italic>K</italic> radial basis functions (i.e. Gaussian place-fields spatial response)
<disp-formula id="eqn2">
<alternatives>
<graphic xlink:href="466920_eqn2.gif"/>
</alternatives>
</disp-formula>
</p>
<p>Each coordinate of the input vector <italic>X<sub>in</sub></italic>(<italic>t<sub>n</sub></italic>) represents the mean firing rate of hippocampus place-cells and its value lies between 0 and 1. <xref ref-type="fig" rid="fig2">Figure 2</xref> represents in panel B the ABCED trajectory <italic>L</italic>(<italic>t</italic><sub>1</sub> &#x2192; <italic>t<sub>N</sub></italic>) and the corresponding place-cell mean firing rate raster <italic>X<sub>in</sub></italic>(<italic>t</italic><sub>1</sub> &#x2192; <italic>t<sub>N</sub></italic>) is depicted in panel C</p>
</sec>
<sec id="s2c">
<title>Hippocampus replay</title>
<p>The hippocampus replay observed during SWR complexes in the active rest phase (between two trials in a given configuration of baited food wells) is modeled by generating condensed (time compressed) subsequences of place-cell activation patterns (snippets) that are then replayed at random so as to constitute a training set. The sampling distribution for drawing a random place-cell activation pattern might be uniform or modulated by new or rewarding experience as described in (<underline><xref ref-type="bibr" rid="c3">Carr et al 2011</xref></underline>). <underline><xref ref-type="bibr" rid="c1">Ambrose et al (2016)</xref></underline> show that during SWR sequences place-cell activation occur in reverse order at the end of a run. In particular, we model a random replay that is biased by reward. We will demonstrate an innovative method for spatial propagation of reward during replay that yields a computationally simple form of reinforcement learning.</p>
<p>We define a snippet as the concatenation of a pattern of successive place-cell activation:
<disp-formula id="eqn3">
<alternatives>
<graphic xlink:href="466920_eqn3.gif"/>
</alternatives>
</disp-formula>
</p>
<p>Where:
<list list-type="bullet">
<list-item><p><italic>s</italic> is the number of place-cell activations.</p></list-item>
</list>
</p>
<p>We define a time budget noted <italic>T</italic> that corresponds to the duration of a replay episode (experimentally, typically 70-100 ms). A replay episode <italic>E</italic> is a set of snippets of length <italic>s</italic>:
<disp-formula id="eqn4">
<alternatives>
<graphic xlink:href="466920_eqn4.gif"/>
</alternatives>
</disp-formula>
</p>
<p>The sum of the durations of snippets replayed in <italic>E</italic> is constrained by <italic>T</italic>. If the time budget is exceeded one snippet is truncated in order to fit the time budget.</p>
<p>In <xref ref-type="fig" rid="fig2">Figure 2</xref>, Panel B represents a particular trajectory through feeders A, B, C, E and D. The depicted snippet is a subsequence of 5 contiguous locations belonging to the ABCED sequence. The B and E feeders are baited and marked as rewarding (R<sub>1</sub> and R<sub>2</sub>). Panel B shows the spatial extent of a given snippet chosen in sequence ABCED and panel C shows the place-cell activation pattern of the ABCED trajectory and the corresponding snippet location in the raster.</p>
<sec id="s2c1">
<title>Reward Propagation</title>
<p>The snippet replay model favors snippets that are on efficient paths linking rewarded sites (e.g. paths linking feeders A, B and C in panel B <xref ref-type="fig" rid="fig2">Figure 2</xref>), and not those that are on inefficient paths (as in paths linking C, D and E in the same panel). This is achieved by propagating reward value backwards from rewarded locations, and calculating the probability of replay as a function of proximity to a reward. Panel D of <xref ref-type="fig" rid="fig3">Figure 3</xref> illustrates the resulting probability distributions for snippet selection along the complete path. Panel E represents the spatial extent of snippet replay likelihood. Note that the paths linking A, B and C have the highest probabilities for snippet replay.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><p>Reservoir Computing Model. The Temporal Recurrent Network (TRN) is a model of the prefrontal cortex (PFC) that take into account cortico-cortical loops by defining a fixed recurrent adjacency matrix for the leaky integrator neurons that model PFC neurons. Inputs of the TRN are modelled hippocampus (HIPP) place-cells. During the training phase, place-cells activations are provided by the algorithmic model of SWR replay (red pathway), and the striatum model learns to predict the next place-cell activation from the PFC model states by modifying the synaptic weights that project the PFC model into the striatum model according the delta learning rule. During the generation phase, the model is no longer learning and the place-cell activation patterns result from the new position of the agent, reconstructed with a Bayesian algorithm from the next place-cell activation prediction of the modeled striatum (blue pathway)</p></caption>
<graphic xlink:href="466920_fig3.tif"/>
</fig>
<p>Hippocampus place-cell replay can occur in forward or backward direction as suggested in (<underline><xref ref-type="bibr" rid="c13">Foster &#x0026; Wilson 2006</xref></underline>). We model the reverse replay as follows: For a given trajectory k of <italic>N<sub>k</sub></italic> samples, there are <italic>N<sub>k</sub></italic> &#x2212; <italic>s</italic> possible snippets that can be replayed but only a limited number of snippets will be selected to fit the time budget <italic>T</italic>. A snippet <italic>S</italic>(<italic>n</italic>) has a likelihood of being replayed if it is related to a reward prediction. A generative model of snippet replay likelihood is first learnt by propagating a time delayed reward information according to the replay direction and the snippet duration. The timespan of a snippet acts as a propagation vector during the estimation phase of the snippet replay likelihood.</p>
<p>The reward prediction <italic>v</italic>(<italic>t</italic><sub>1</sub>&#x2212;&#x003E; <inline-formula><alternatives><inline-graphic xlink:href="466920_inline4.gif"/></alternatives></inline-formula>) is learnt by initializing it to small positive random values and then iteratively refined by applying the procedure (8) K times:
<list list-type="order">
<list-item><p>Draw a random contiguous time index subset <italic>&#x03C4;</italic> &#x2261; <italic>T</italic>(<italic>n</italic>, <italic>s</italic>, <italic>r</italic>; <italic>&#x03B2;<sub>learn</sub></italic>) according to the reverse rate <italic>&#x03B2;<sub>learn</sub></italic>:
<list list-type="alpha-lower">
<list-item><p>Select a time-step <italic>t<sub>n</sub></italic> such that <italic>n</italic> &#x2208; {1 &#x2026; <italic>N<sub>k</sub></italic>}</p></list-item>
<list-item><p>according to the replay likelihood defined by:
<disp-formula id="eqn5">
<alternatives>
<graphic xlink:href="466920_eqn5.gif"/>
</alternatives>
</disp-formula>
</p></list-item>
<list-item><p>Select a random number <italic>r</italic> &#x2208; [0, 1] and a contiguous and monotonous time index sequence <italic>&#x03C4;</italic> such that:
<disp-formula id="eqn6">
<alternatives>
<graphic xlink:href="466920_eqn6.gif"/>
</alternatives>
</disp-formula>
</p></list-item>
</list>
</p></list-item>
<list-item><p>Update the reward estimate V over increasing indices of <italic>&#x03C4;</italic> by computing the update equation:
<disp-formula id="eqn7">
<alternatives>
<graphic xlink:href="466920_eqn7.gif"/>
</alternatives>
</disp-formula>
</p>
<p>Where:
<list list-type="bullet">
<list-item><p><italic>&#x03B1;</italic> &#x2208; [0, 1] is the learning rate constant</p></list-item>
<list-item><p><italic>&#x03B3;</italic> &#x2208; [0, 1] is the discount constant</p></list-item>
<list-item><p><italic>R</italic>(<italic>t</italic>) is the observed instantaneous reward information</p></list-item>
</list>
</p></list-item>
</list>
</p>
<p>This is a convex combination of the current estimate of the reward information <italic>v</italic>(<italic>&#x03C4;<sub>k</sub></italic>) at the next time step and the instantaneous reward information <italic>R</italic>(<italic>&#x03C4;<sub>k&#x2212;1</sub></italic>) &#x002B; <italic>&#x03B3;V</italic>(<italic>&#x03C4;<sub>k&#x2212;1</sub></italic>) based on the previously observed reward signal <italic>R</italic>(<italic>&#x03C4;<sub>k&#x2212;1</sub></italic>) and delayed previous reward estimate <italic>&#x03B3;V</italic>(<italic>&#x03C4;<sub>k&#x2212;1</sub></italic>)). <xref ref-type="disp-formula" rid="eqn7">Equation 7</xref> implements a form of temporal difference learning. It is sufficient to define a coarse reward signal as:
<disp-formula id="eqn8">
<alternatives>
<graphic xlink:href="466920_eqn8.gif"/>
</alternatives>
</disp-formula>
</p>
<p>The snippet generation procedure is simply the repetition of the steps a and c of procedure (8) with <italic>&#x03B2;<sub>generate</sub></italic> used instead of <italic>&#x03B2;<sub>learn</sub></italic> until the sum of time subsequences durations overflows the fixed budget duration. These snippets will serve as inputs to the reservoir model of PFC described next. As illustrated in <xref ref-type="fig" rid="fig2">Figure 2 D and E</xref>, the replay is biased by proximity to reward, which has spatially propagated.</p>
</sec>
</sec>
<sec id="s2d">
<title>Reservoir model of PFC for snippet consolidation</title>
<p>We model the prefrontal cortex as a recurrent reservoir network. Reservoir computing refers to a class of recurrent network models with fixed recurrent connections. The reservoir units are driven by external inputs and the network dynamics provides a high dimensional representation of the inputs from which the desired outputs can then be read out by a trained linear combination of the reservoir unit activities. The principle has been co-developed in distinct contexts as the temporal recurrent network (<underline><xref ref-type="bibr" rid="c9">Dominey 1995</xref></underline>), the liquid state machine (<underline><xref ref-type="bibr" rid="c23">Maass et al 2002</xref></underline>), and the echo state network (<underline><xref ref-type="bibr" rid="c16">Jaeger 2001</xref></underline>). The version that we use to model the frontal cortex employs leaky integrator neurons in the recurrent network. This model of PFC is particularly appropriate because the recurrent network generates dynamic state trajectory that will allow overlapping snippets to have overlapping state trajectories. This property will favor consolidation of a whole sequence from its snippet parts. At each time-step, the network is updated according to the following schema:</p>
<p>The hippocampus place-cells project into the reservoir through feed-forward synaptic connections noted <italic>W<sub>ffwd</sub></italic>. The projection operation is a simple matrix-vector product. Hence, the input projection through feed-forward synaptic connections is defined by:
<disp-formula id="eqn9">
<alternatives>
<graphic xlink:href="466920_eqn9.gif"/>
</alternatives>
</disp-formula>
</p>
<p>Where:
<list list-type="bullet">
<list-item><p><italic>W<sub>ffwd</sub></italic> is a fixed connectivity matrix whose values do not depend on time.</p></list-item>
</list>
</p>
<p>Synaptic weights are randomly selected at the beginning of the simulation. Practically speaking (<underline><xref ref-type="bibr" rid="c22">Lukosevicius 2012</xref></underline>), sampling U[&#x2212;1, 1] a uniform distribution is sufficient. A positive synaptic weight in a connectivity matrix models an excitatory connection and a negative weight models an inhibitory connection between two neurons (that could be implemented via an intervening inhibitory interneuron). Let N be the number of neurons in the Reservoir. Reservoir&#x2019;s neurons are driven by both sensory position inputs <italic>X<sub>in</sub></italic>(<italic>t<sub>n</sub></italic>) and, importantly by the recurrent connections that project an image of the previous reservoir state back into the reservoir. The recurrent projection is defined as:
<disp-formula id="eqn10">
<alternatives>
<graphic xlink:href="466920_eqn10.gif"/>
</alternatives>
</disp-formula>
</p>
<p>Where:
<list list-type="bullet">
<list-item><p><italic>W<sub>rec</sub></italic> is a N by N square connectivity matrix.</p></list-item>
</list>
</p>
<p>Synaptic weights are drawn from a U[&#x2212;1, 1] uniform distribution, scaled by a <italic>S</italic>(<italic>N</italic>; <italic>K</italic>) &#x003D; <inline-formula><alternatives><inline-graphic xlink:href="466920_inline2.gif"/></alternatives></inline-formula> factor.</p>
<p>The same sign convention as in <xref ref-type="disp-formula" rid="eqn9">equation (9)</xref> applies for the recurrent connectivity matrix.</p>
<p>Self-connections (i.e. <inline-formula><alternatives><inline-graphic xlink:href="466920_inline3.gif"/></alternatives></inline-formula> with <italic>i</italic> &#x2208; 1 &#x2026; <italic>N</italic>) are forced to zero. <italic>W<sub>rec</sub></italic> is also fixed and its values do not depend on time. The contributions of afferent neurons to the reservoir&#x2019;s neurons is summarized by
<disp-formula id="eqn11">
<alternatives>
<graphic xlink:href="466920_eqn11.gif"/>
</alternatives>
</disp-formula>
</p>
<p>The membrane potential of the reservoir&#x2019;s neurons <italic>P<sub>res</sub></italic> then is computed by solving the following ordinary derivative equation (ODE):
<disp-formula id="eqn12">
<alternatives>
<graphic xlink:href="466920_eqn12.gif"/>
</alternatives>
</disp-formula>
</p>
<p>Where:
<list list-type="bullet">
<list-item><p><italic>&#x03C4;</italic> is the neuron&#x2019;s time constant. It models the resistive and capacitive properties of the neuron&#x2019;s membrane.</p></list-item>
</list>
</p>
<p>In this article, we will consider a contiguous assembly of neurons that share the same time constant. The inverse of the time constant is called the leak rate and is noted <italic>h</italic>. By choosing the Euler&#x2019;s forward method for solving equation(12), the membrane potential is computed recursively by the equation:
<disp-formula id="eqn13">
<alternatives>
<graphic xlink:href="466920_eqn13.gif"/>
</alternatives>
</disp-formula>
</p>
<p>This is a convex combination between instantaneous contributions of afferents neurons <italic>U<sub>res</sub></italic>(<italic>t<sub>n</sub></italic>) and the previous value <italic>P<sub>res</sub></italic>(<italic>t<sub>n&#x2212;1</sub></italic>) of the membrane potential. The current membrane potential state carries information about the previous activation values of the reservoir, provided by the recurrent weights. The influence of the history is partially controlled by the leak rate. A high leak rate will result in a responsive reservoir with a very limited temporal memory. A low leak rate will result in a slowly varying network whose activation values depend more on the global temporal structure of the input sequence.</p>
<p>Finally, the mean firing rate of a reservoir&#x2019;s neuron is given by:
<disp-formula id="eqn14">
<alternatives>
<graphic xlink:href="466920_eqn14.gif"/>
</alternatives>
</disp-formula>
</p>
<p>Where:
<list list-type="bullet">
<list-item><p>&#x03C3;<italic><sub>res</sub></italic> is the non-linear activation function of the reservoir neurons</p></list-item>
<list-item><p><italic>&#x0398;<sub>res</sub></italic> is a bias that will act as a threshold for the neuron&#x2019;s activation function.</p></list-item>
</list>
</p>
<p>We choose a &#x03C3;<italic><sub>res</sub></italic> &#x2261; <italic>tanh</italic> hyperbolic tangent activation function with a zero bias. Negative firing rate values represent the inhibitory/excitatory connection type in conjunction with the sign of the synaptic weight. Only the product of the mean firing rate of the afferent neuron by its associated synaptic weight is viewed by the leaky integrator neuron. See S1 High dimensional processing in the reservoir for more details on interpreting activity in the reservoir.</p>
</sec>
<sec id="s2e">
<title>Learning in Modifiable PFC Connections to Readout</title>
<p>Based on the rich activity patterns in the reservoir, it is possible to decode the reservoir&#x2019;s state in a supervised manner in order to produce the desired output as a function of the input sequence. This decoding is provided by the readout layer and the matrix of modifiable synaptic weights linking the reservoir to the readout layer, noted <italic>W<sub>ro</sub></italic> and represented by dash lines in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p>
<p>The readout activation pattern <italic>X<sub>ro</sub></italic>(<italic>t<sub>n</sub></italic>) is given by the equation:
<disp-formula id="eqn15">
<alternatives>
<graphic xlink:href="466920_eqn15.gif"/>
</alternatives>
</disp-formula>
</p>
<p>Where:
<list list-type="bullet">
<list-item><p>&#x03C3;<italic><sub>ro</sub></italic> is the non-linear activation function of the readout neurons</p></list-item>
<list-item><p><italic>&#x0398;<sub>ro</sub></italic> is a bias that will act as a threshold for the neuron&#x2019;s activation function</p></list-item>
</list>
</p>
<p>We choose a &#x03C3;<italic><sub>ro</sub></italic> &#x2261; <italic>tanh</italic> hyperbolic tangent activation function with a zero bias.</p>
<p>Notice that the update algorithm described above is a very particular procedure inherited from feedforward neural networks. We chose to use it because it is computationally efficient and deterministic.</p>
<p>Once the neural network states are updated, the readout synaptic weights are updated by using a stochastic gradient descent algorithm. By deriving the Widrow-Hoff Delta rule (<underline><xref ref-type="bibr" rid="c29">Widrow &#x0026; Hoff 1960</xref></underline>) for hyperbolic tangent readout neurons, we have the following update equation:
<disp-formula id="eqn16">
<alternatives>
<graphic xlink:href="466920_eqn16.gif"/>
</alternatives>
</disp-formula>
</p>
<p>Where:
<list list-type="bullet">
<list-item><p><italic>&#x03B1;</italic> is a small positive constant called the learning rate</p></list-item>
<list-item><p><italic>t<sub>n</sub></italic><sub>&#x2212;<italic>b</italic>&#x002B;1</sub> &#x2192; <italic>t<sub>n</sub></italic> is the concatenation of b time steps from <italic>t<sub>n</sub></italic><sub>&#x2212;<italic>b</italic>&#x002B;1</sub>to <italic>t<sub>n</sub></italic></p></list-item>
</list>
</p>
<p>When <italic>b</italic> &#x003D; 1, <xref ref-type="disp-formula" rid="eqn16">equation (16)</xref> computes a stochastic gradient descent. The case when <italic>b</italic> &#x003E; 1 is called a mini-batch gradient descent and allows one to estimate the synaptic weight gradient base on b successive observations of predicted and desired activation values. A mini batch gradient allows one to compute efficiently and robustly the synaptic weight gradient. Empirically, <italic>b</italic> &#x003D; 32 gives satisfying results.</p>
<p>In this study, we will focus on the prediction of the next place-cell activation pattern:
<disp-formula id="eqn17">
<alternatives>
<graphic xlink:href="466920_eqn17.gif"/>
</alternatives>
</disp-formula>
</p>
<p>This readout is considered to take place in the striatum, as part of a cortico-striatal learning system. This is consistent with data indicating that while hippocampus codes future paths, the striatum codes actual location (<underline><xref ref-type="bibr" rid="c27">van der Meer et al 2010</xref></underline>).</p>
</sec>
<sec id="s2f">
<title>Training</title>
<p>After each trial, the model is trained using a dataset that is generated online by the snippet replay mechanism described above in the paragraph on Hippocampus replay. The readout synaptic weights are also learned online by using the learning rule described in the Learning in Modifiable PFC Connections to Readout section. The model does not receive any form of feedback from the environment and it learns place-cell activation sequences based only on random replay of snippets.</p>
<p>Between each sequence of the training set (snippets in our case), the states of the reservoir and readout are set to a small random uniform value centered on zero. This models a time between the replays of two snippets that is sufficiently long for inducing states in the neural network that are not correlated with the previous stimulus. This is required for having the same effect as simulating a longer time after each snippet but without having to pay the computational cost associated to this extra simulation time.</p>
</sec>
<sec id="s2g">
<title>Embodied Simulation of Sensory-Motor loop via the spatial filter</title>
<p>Once the model is trained, we need to evaluate its performance and the trajectories it can generate. The model is primed with the first <italic>p</italic> steps of the place-cell activation sequence the model is supposed to produce. This sequence is called the target sequence. Then the model&#x2019;s ability to generate a place-cell activation sequence is evaluated by injecting the output prediction of the next place-cell activation pattern as the input at the next step. In this iterative procedure, the system should autonomously reproduce the trained sequence pattern of place-cell activations.</p>
<p>Predicted place-cell activation values might be noisy, and the reinjection of even small amounts of noise in this autonomous generation procedure can lead to divergence. We thus employ a procedure that determines the location coded by the place-cell activation vector output, and reconstructs a proper place-cell activation vector coding this location. We call this denoising procedure the spatial filter as referred to in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p>
<p>We model the rat action as &#x2018;reaching the most probable nearby location&#x2019;. Since only the prediction of the next place-cell activation pattern <italic>&#x03B7;</italic> is available, we need to estimate the most probable point <italic>s<sup>&#x2217;</sup></italic>(<italic>t<sub>n</sub></italic><sub>&#x002B;1</sub>) &#x003D; (<italic>x<sup>&#x2217;</sup></italic>(<italic>t<sub>n</sub></italic><sub>&#x002B;1</sub>), <italic>y<sup>&#x2217;</sup></italic>(<italic>t<sub>n</sub></italic><sub>&#x002B;1</sub>)). From a Bayesian point of view, we need to determine the most probable next location <italic>s</italic>(<italic>t<sub>n</sub></italic><sub>&#x002B;1</sub>), given the current location <italic>s</italic>(<italic>t<sub>n</sub></italic>) and the predicted place-cell activation pattern <italic>&#x03B7;</italic>(<italic>t<sub>n</sub></italic>). We can state our problem as:
<disp-formula id="eqn18">
<alternatives>
<graphic xlink:href="466920_eqn18.gif"/>
</alternatives>
</disp-formula>
</p>
<p>Where:
<list list-type="bullet">
<list-item><p><italic>u</italic> is a noise function sampling a uniform distribution <italic>&#x1D4B0;</italic>(0, <italic>m</italic>)</p></list-item>
</list>
</p>
<p><italic>u</italic> is useful at least in degenerate cases when a zero place-cell activation prediction generates an invalid location coding. It is also used for biasing the generation procedure and to explore other branches of the possible trajectories the model can generate as described in section Evaluating Behavior with Random walk.</p>
<p>The system is then moved to this new location <italic>s<sup>&#x2217;</sup></italic> and a new noise/interference free place-cell activation pattern is generated by the place-field model. We refer to this place-cell prediction/de-noising method as the spatial filter, which emulates a sensory-motor loop for the navigating rat in this study. <xref ref-type="fig" rid="fig3">Figure 3</xref> depicts this sensory motor loop.</p>
</sec>
<sec id="s2h">
<title>Evaluating Behavior with Random walk</title>
<p>Once the model has been trained, it is then primed with place-cell activation inputs corresponding to the first few steps of the trajectory to be generated. The readout from the PFC reservoir generates the next place-cell activation pattern in the trajectory, which is then reinjected into the reservoir via the spatial filter, in a closed loop process. This loop evaluation procedure is called <italic>autonomous</italic> generation. In order to evaluate the model in a particular experimental condition, several instances of the same model are evaluated multiple times in a random walk procedure. The batch of generated trajectories (typically 1000) are accumulated in a stencil buffer which acts as a two dimensional histogram showing the most frequently generated trajectories. The arena is drawn with its feeders and a vector field is computed from trajectories in order to show the main direction of these trajectories. Trajectories are superimposed and summed, resulting in a two-dimensional histogram representing the space occupied by trajectories. <xref ref-type="fig" rid="fig4">Figure 4</xref> shows an example of random walk trajectories, illustrating the model&#x2019;s ability to autonomously generate a long and complex sequence when learning without snippets.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><p>Sequence learning. Panel A illustrates a long convoluted trajectory taken by a rat in configuration 38. Panel B illustrates the probability maps of trajectories generated by the trained model in autonomous sequence generation mode. Note that there are two locations where the trajectory crosses itself, which introduces ambiguity that the model is able to disambiguate. This illustrates that the model is well able to learn such sequences.</p></caption>
<graphic xlink:href="466920_fig4.tif"/>
</fig>
<p>In cases where small errors in the readout are reinjected as input, they can be amplified, causing the trajectory to diverge. It is possible to overcome this difficulty by providing as input the expected position at each time step instead of the predicted position. The error/distance measurement can still be made, and will quantify the diverging prediction, while allowing the trajectory generation to continue. This method is called <italic>non-autonomous</italic> generation and it evaluates only the ability of a model to predict the next place-cell activation pattern, given an input sequence of place-cell activations.</p>
</sec>
<sec id="s2i">
<title>Comparing produced and ideal sequences using Discrete Fr&#x00E9;chet distance</title>
<p>The joint PFC-HIPP model can be evaluated by comparing an expected place-cell firing pattern with its prediction by the readout layer. At each time step, an error metric is computed and then averaged over the duration of the expected neurons firing rate sequence. The simplest measure is the mean square error. This is the error that the learning rule described in <xref ref-type="disp-formula" rid="eqn16">equation (16)</xref> minimizes.</p>
<p>Although the model output is place-cell coding, what is of interest is the corresponding spatial trajectory. A useful measurement in the context of comparing spatial trajectories is the discrete Fr&#x00E9;chet distance. It is a measure of similarity between two curves that takes into account the location and ordering of the points along the curve. We use the discrete Fr&#x00E9;chet distance applied to polygonal curves as initially described in <underline><xref ref-type="bibr" rid="c10">Eiter and Mannila (1994)</xref></underline>. In (<underline><xref ref-type="bibr" rid="c30">Wylie 2013</xref></underline>) the Discrete Fr&#x00E9;chet distance <italic>F</italic> between two curves <italic>A</italic> and <italic>B</italic> is defined by:
<disp-formula id="eqn19">
<alternatives>
<graphic xlink:href="466920_eqn19.gif"/>
</alternatives>
</disp-formula>
</p>
<p>Where <italic>d</italic>(.,.) is the Euclidean distance, <italic>m</italic> is the number of steps of the curve A, <italic>n</italic> is the number of steps of the curve B, and <italic>&#x03B1;</italic>, <italic>&#x03B2;</italic> are reparametrizations of the curves A and B. Parameterization of this measure is described in more detail in S1 Frechet distance parameters.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<p>For robustness purposes, results are based on a population of neural networks rather than a single instance. The population size is usually 1000 for evaluating a condition and the metrics described above are aggregated by computing their mean <italic>&#x03BC;</italic>(.) and standard-deviation <italic>&#x03C3;</italic>(.). For convenience, we define a custom score function associated to a batch of coherent measurements as:
<disp-formula id="eqn20">
<alternatives>
<graphic xlink:href="466920_eqn20.gif"/>
</alternatives>
</disp-formula>
</p>
<p>Results having a low mean and standard deviation will be reported as low score whilst other possible configurations will result in a higher score. We choose this method rather than Z-score, which penalizes low standard deviations. We first established that the model displays standard sequence learning capabilities (e.g. illustrated in <xref ref-type="fig" rid="fig4">Figure 4</xref>) and studied parameter sensitivity (see S1 Basic Sequence learning and parameter search), and then addressed consolidation from replay.</p>
<sec id="s3a">
<title>Consolidation from snippet replay</title>
<p>The model is able to learn and generate navigation sequences from place-cell activation patterns. The important questions is whether a sequence can be learned by the same model when it is trained on randomly presented snippets, instead of the continuous sequence.</p>
<p>In this experiment, no reward is used, and thus each snippet has equal chance of being replayed. The only free parameter is the snippet size. In order to analyze the reservoir response, we collect the state-trajectories of reservoir neurons when exposed to snippets. Recall that the internal state of the reservoir is driven by the external inputs, and by recurrent internal dynamics, thus the reservoir adopts a dynamical state-trajectory when presented with an input sequence. Such a trajectory is visualized in <xref ref-type="fig" rid="fig5">Figure 5D</xref>. This is a 2D (low dimensional) visualization, via PCA, of the high dimensional state transitions realized by the 1000 neurons reservoir as the input sequence corresponding to ABCDE is presented. Panels A-C illustrate the trajectories that the reservoir state traverses as it is exposed to an increasing number of randomly selected snippets generated for the same ABCDE sequence. We observe that as snippets are presented, the corresponding reservoir state-trajectories start roughly from the same point because of the random initial state of the reservoir before each snippet is replayed. Then the trajectories evolve and partially overlap with the state-trajectory produced by the complete sequence. In other words, snippets quickly drive the reservoir state from an initial random activation (corresponding to the grey area at the center of each panel) onto their corresponding locations in the reservoir activation state-trajectory of the complete sequence. Replaying snippets at random thus has no negative impact because the reservoir states overlap when snippet trajectories overlap.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><p>Illustration of snippet integration in reservoir state space. Here we visualize the high dimensional reservoir space in a low (2D) PCA space, in order to see how pieces (snippets) of the overall sequence are consolidated. In this experiment, the sequence ABCDE is broken into snippets, which are then used to train the model. The challenge is that only local structure is presented to the model, which must consolidate the global structure. Panels A-C represent the state trajectory of reservoir activation after 100, 1000 and 1000 snippets. While each snippet represents part of the actual trajectory, each is taken out of its overall spatial context in the sequence. Panel D represents the trajectory of reservoir state during the complete presentation of the intact sequence. Panel C reproduces this trajectory, but in addition we see &#x201C;ghost&#x201D; trajectories leading to the ABCDE trajectory. These ghost elements represent the reservoir state transitions from an initial random state as the first few elements of each snippet take the reservoir from the initial undefined state onto the component of the ABCDE trajectory coded by that snippet.</p></caption>
<graphic xlink:href="466920_fig5.tif"/>
</fig>
<p>Thus, we see that the state trajectories traversed by driving the reservoir with snippets overlaps that from the original intact sequence. See further details of sequence learning by snippet replay in S1 Sequence complexity effects on consolidation.</p>
</sec>
<sec id="s3b">
<title>Longer paths are rejected</title>
<p>Here we examine how using reward proximity to modulate snippet replay probability distributions (as described in the hippocampal replay description) allows the rejection of longer, inefficient paths between rewarded targets. In this experiment, 1000 copies of the model are run 10 times. Each is exposed to the reward modulated replay of two sequences ABC and ABD having a common prefix AB as illustrated in <xref ref-type="fig" rid="fig6">Figure 6</xref>. The model is exposed to a random replay of sequences ABC and ABE. The random replay is not uniform and takes into account the reward associated with a baited feeder when food was consumed. Snippets close to a reward have more chance to be replayed and thus to be consolidated into a trajectory.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><p>Longer paths are rejected. Panels A and B illustrate snippet counts for T maze trajectories pictured in panels C and D. In Panel C, sequences begin at location A, and rewards are given at locations C and D. Based on the reward proximity and propagation, there is a higher probability of snippets being selected along path AC than path AD. This is revealed in panel A, a histogram of snippets for the sequences ABC (in Blue) and ABD (in Orange). Panels B and D illustrate how distance and reward intensity interact. By increasing the strength of the reward, a longer trajectory can be rendered virtually shorter and more favored, by increasing the probability that snippets will be selected from this trajectory, as revealed in Panel B. Panel E and F confirm a neat tendency to generate autonomously sequences significantly similar to the ABC and ABD sequence respectively (p-value &#x003D; 0).</p></caption>
<graphic xlink:href="466920_fig6.tif"/>
</fig>
<p>Panel A in <xref ref-type="fig" rid="fig6">Figure 6</xref> illustrates the distribution of snippets selected from the two sequences, ABC in pink and ABD in blue. At the crucial point of choice at location B, the distribution of snippets for sequence ABC largely outnumbers those for sequence ABD. This is due to the propagation of rewards respectively from points C and D. Per design, rewards propagated from a more proximal location will have a greater influence on snippet generation. Panel C shows the 2D histogram of autonomously generated sequences when the model is primed with the initial sequence prefix starting at point A. We observe a complete preference for the shorter sequence ABC illustrated in panel E.</p>
<p>The snippet generation model described above takes into account the location of rewards, and the magnitude of rewards. Panel B illustrates the distribution of snippets allocated to paths ABC and ABD when a 10x stronger reward is presented at location D. This strong reward dominates the snippet generation and produces a distribution that strongly favors the trajectory towards location D, despite its farther distance. Panel F illustrates the error mesures for model reconstruction of the two sequences and confirms this observation. This suggests an interesting interaction between distance and reward magnitude. For both conditions, distances to the expected sequence have been measured for every trajectory generated (10 000 for ABC and 10 000 for ABD). Then a Kruskall Wallis test confirms (p-value &#x007E;&#x003D; 0) for both cases that trajectories generated autonomously are significantly more accurate for the expected trajectories (i.e. ABC when rewards are equal and ABD when reward at D is x10).</p>
</sec>
<sec id="s3c">
<title>Novel efficient sequence creation</title>
<p>Based on the previously demonstrated dynamic properties, we determined that when rewards of equal magnitudes are used, the model would favor shorter trajectories between rewards. We will now test the model&#x2019;s ability to exploit this capability, in order to generate a novel and efficient trajectory from trajectories that contain sub-paths of the efficient trajectory. That is, we determine whether the model can assemble the efficient subsequences together, and reject the longer inefficient subsequences in order to generate a globally efficient trajectory. <xref ref-type="fig" rid="fig1">Figure 1</xref> (Panel A) illustrates the desired trajectory that should be created without direct experience, after experience with the three trajectories in panels B-D that each contain part of the optimal trajectory (red), which will be used to train the model.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7:</label>
<caption><p>Efficient Sequence Synthesis. A. Distribution of snippets drawn from the sequences illustrated in <xref ref-type="fig" rid="fig1">Figure 1 B, C and D</xref>. Globally we observe snippet selection favors snippets from the beginning of sequence ABCED (blue), the middle of EBCDA (yellow), and the end of sequence BACDE (pink), which corresponds exactly to the efficient subsequences (ABC, BCD, and CDE) of these three sequences. This distribution of snippets is used to train the model. The results of the training are illustrated in panel B. Here we see a 2D histogram of sequences generated by the model in the ABCDE recombination experiment. 10 batches of 100 reservoirs each were trained and each model instance was evaluated 10 times with noise. Panel C confirms that the trajectories generated autonomously are significantly more similar to the target sequence ABCDE (p-value &#x003D; 5.9605e-08) These results are very robust and satisfying, demonstrating that our hypothesis for efficient sequence discovery based on reward-modulated replay is validated.</p></caption>
<graphic xlink:href="466920_fig7.tif"/>
</fig>
<p>The reward-biased replay is based on the following trajectories: (1) <underline>ABC</underline>ED that contains the ABC part of the ABCDE target sequence, (2) E<underline>BCD</underline>A that contains the BCD part of the ABCDE target sequence, and (3) BA<underline>CDE</underline> that contains the CDE part of the ABCDE target sequence. <xref ref-type="fig" rid="fig7">Figure 7</xref> A illustrates how the hippocampal replay model generates distributions of snippets that significantly favor the representation of the efficient subsequences of each of the three training sequences. This is revealed as the three successive peaks of snippet distributions on the time histogram for the blue (<underline>ABC</underline>ED) sequence, favoring its initial part ABC, the yellow (E<underline>BCD</underline>A) sequence, favoring its middle part BCD, and the pink (BA<underline>CDE</underline>) sequence, favoring its final part CDE. When observing each of the three color-coded snippet distributions corresponding to each of the three sequences we see that each sequence is favored (with high replay density) precisely where it is most efficient. Thus, based on this distribution of snippets that is biased towards the efficient subsequences, the reservoir should be able to extract the efficient sequence.</p>
<p>This is shown in panel B, which illustrates the autonomously generated sequences for 1000 instances of the model executed 10 times each. The spatial histogram reveals that the model is able to extract and concatenate the efficient subsequences to create the optimal path, though it was never seen in its entirely in the input. Panel C illustrate the significant differences in performance between the favored efficient sequence vs. the three that contain non-efficient subsequences. A Kruskal-Wallis test confirms these significant differences reconstruction error for the efficient vs non-efficient sequences (maximum p &#x003D; 5.9605e-08).</p>
</sec>
<sec id="s3d">
<title>Reverse replay</title>
<p>In (<underline><xref ref-type="bibr" rid="c3">Carr et al 2011</xref></underline>), hippocampus replay during SWR is characterized by the activation order of the place-cells which occurs in backward and forward direction. We hypothesize that reverse replay allows the rat to explore a trajectory in one direction but consolidate it in both directions. This means that an actual trajectory, and its unexplored reverse version, can equally contribute to new behavior. Thus fewer actual trajectories are required for gathering information for solving the TSP problem. A systematic treatment of this effect on learning can be seen in S1 Analysis of different degrees of reverse replay.</p>
<p>We now investigate how reverse replay can be exploited in a recombination task where some sequences are experienced in the forward direction, and others in the reverse direction, with respect to the order of the sequence to be generated. We use the same setup as described above for novel sequence generation, but we invert the direction of sequence EBCDA in the training set. Without EBCDA, the model is not exposed to sub trajectories linking feeders B to C and C to D and the recombination cannot occur. We then introduce a partial reverse replay, which allows snippets to be played in forward and reverse order. This allows the reservoir to access segments BC and CD (even though they are not present in the forward version of the experienced trajectory.</p>
<p><xref ref-type="fig" rid="fig8">Figure 8</xref> illustrates the histogram of sequence performance for 10000 runs of the model (1000 models run 10 times each) on this novel sequence generation task with and without 50&#x0025; reverse replay. We observe a significant shift towards reduced errors (i.e. towards the left) in the presence of reverse replay.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8:</label>
<caption><p>Reverse replay facilitates efficient sequence discovery. Using the same sequences illustrated in <xref ref-type="fig" rid="fig1">Figure 1</xref>, we reversed the direction of sequence EBCDA, and then tested the model&#x2019;s ability to synthesize the ABCDE sequence from <underline>ABC</underline>ED A<underline>CDB</underline>E and BA<underline>CDE</underline></p></caption>
<graphic xlink:href="466920_fig8.tif"/>
</fig>
<p>We then examine a more realistic situation based on the observation of spontaneous creation of &#x201C;shortcuts&#x201D; described in (<underline><xref ref-type="bibr" rid="c14">Gupta et al 2010</xref></underline>). The model is exposed to a random replay of snippets extracted from two trajectories having different direction (clockwise CW and counter clockwise CCW). The system thus experiences different parts of the maze in different directions. We examine whether the use of reverse replay can allow the system to generate novel shortcuts.</p>
<p>The left and right trajectories used for training are illustrated in <xref ref-type="fig" rid="fig9">Figure 9A</xref> and B. In A, the system starts at MS, head up and to the left at T2 (counter clockwise) and terminates back at MS. In B, up and to the right (clockwise) again terminating at MS. Possible shortcuts can take place at the end of a trajectory at MS as the system continues on to complete the whole outer circuit rather than stopping at MS. We can also test for shortcuts that traverse the top part of the maze by starting at MS and heading left or right and following the outer circuit in the CW or CCW direction, thus yielding 4 possible shortcuts. The model is trained with snippets from the sequences in A and B using different random replay rates, and evaluated in non-autonomous mode with sequences representing the 4 possible types of shortcut. Figure 16 C shows with no reverse replay, when attempting the CCW path, there is low error until the system enters the zone that has only been learned in the CW direction. In panel D, with 50&#x0025; reverse replay, this error is reduced and the system can perform the shortcut without having experienced the right hand part in the correct direction. Thus, in the right hand part of the maze, it is as if the system had experienced this already in the CCW direction, though in reality this has never occurred, but is simulated by the reverse replay. This illustrates the utility of mixed forward and reverse replay. Panel E illustrates the difficulty when 100&#x0025; reverse replay is used. Figure 16F illustrates the reconstruction errors for a shortcut path as a function of degree of reverse The trajectory is evaluated in non-autonomous mode and the position of the agent necessarily follows the target trajectory. In this case, the expected trajectory describes a CCW path. Results are not significantly different with a replay rate 25&#x0025; and 75&#x0025; (p &#x003D; 0.02), where the best performance is observed, and all the other conditions are significantly different (p &#x2264; 1.1921e-07). This phenomenon was obtained for the 4 possible shortcuts.</p>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure 9:</label>
<caption><p>Reverse replay allows novel shortcut path generation. Panels A and B illustrate the trajectories for left and right trajectories, based on Gupta et al. After training on these two trajectories, we test the ability to generate a shortcut that makes the complete outer loop in one direction. Panel C &#x2013; without reverse replay, significant spatial errors are revealed when the system attempts to complete the counter-clockwise loop on the right side of the maze Panel D illustrates the beneficial effects of reverse replay during trajectory learning. Panel E illustrates the effect of a model training with 100&#x0025; reverse replay. It is similar to using a 0&#x0025; reverse replay but the effect is observed on the left lap trajectory part. Panel F &#x2013; when reverse replay is introduced, this error is attenuated.</p></caption>
<graphic xlink:href="466920_fig9.tif"/>
</fig>
</sec>
<sec id="s3e">
<title>Effects of Consolidation and Reverse Replay</title>
<p>The model demonstrates the ability to accumulate and consolidate paths over multiple trials, and to exploit reverse replay. Here we examine these effects on the more extensive and variable dataset extracted from rat behavior (<underline><xref ref-type="bibr" rid="c6">de Jong et al 2011</xref></underline>). We show the positive effects of replay on trajectories from rats trying to optimize spatial navigation in the TSP task. In the prototypical TSP behavior, in a given configuration of baited wells, on successive trials the rat traverses different efficient subsequences of the overall efficient sequence, and then finally puts it all together and generates the efficient sequence. This suggests that as partial data about the efficient sequence are successively accumulated, the system performance will successively improve. To explore this, the model is trained on navigation trajectories that were generated by rats in the TSP task. We selected data from configurations where the rats found the optimal path after first traversing subsequences of that path in previous trials. Interestingly, these data contain examples where the previous informative trails include traversal of part of the optimal sequence in either the forward or reverse directions, and sometimes both (see S Rat navigation data). We trained training the model with random replay of combinations of informative trials where informative trials are successively added, in order to evaluate the ability of the model to successively accumulate information. For each combination of informative trials, the random replay is evaluated with 0&#x0025;, 25&#x0025;, 50&#x0025;, 75&#x0025; and 100&#x0025; of reverse replay rate in order to assess the joint effect of random replay and combination of informative trials. The model is then evaluated in non-autonomous mode with the target sequences that consist in a set of trajectories linking the baited feeders in the correct order. An idealized sequence is added to the target sequence set because trajectories generated by the rat might contain edges that do not relate the shortest distance between two vertices. Agent&#x2019;s moves are restricted to a circle having a 10 cm radius.</p>
<p><xref ref-type="fig" rid="fig10">Figure 10</xref> illustrates the combined effects of successive integration of experience and its contribution to reducing error, and of the presence of different mixtures of forward and reverse replay. The ANOVA revealed that there is a significant effect for combination (F(2, 585) &#x003D; 32.84, p &#x003C; 0.01), as performance increases with exposure to more previous experience (Panel A). There is also a significant effect for reverse replay rate (F(4, 585) &#x003D; 3.71, p &#x2264; 0.01), illustrated in Panel B. There was no significant interaction between consolidation and replay direction (F(8, 585) &#x003D; 0.03, p &#x003D; 1). This indicates that when trained on trajectories produced by behaving rats, the model displays the expected behavior of improving with more experience, and of benefitting from a mixture of forward and reverse replay.</p>
<fig id="fig10" position="float" fig-type="figure">
<label>Figure 10:</label>
<caption><p>Consolidation and reverse replay applied to behavioral data. Measured variable is Frechet distance between generated and desired sequence. Data from the rat TSP configurations are used for training and testing the model. A. Effects of consolidation: as successive trials are added to the replay repertoire; the trajectory reconstruction error is significantly reduced. B. Effects of reverse replay: as reverse replay is introduced in snippet formation for training the PFC model, reconstruction error is significantly reduced.</p></caption>
<graphic xlink:href="466920_fig10.tif"/>
</fig>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>We tested the hypothesis that hippocampus replay observed during sharp wave ripple events in the awake animal can play a role in memory consolidation by exposing the prefrontal cortex between successive trials to short subsequences of place-cell activation patterns. This replay can potentially play a crucial role in learning, essentially by generating synthetic data (based on experience) for training the system. The behavior of interest is a form of spatial navigation trajectory optimization in a task, mimicking the well-known traveling salesperson problem. It is a NP-Hard problem and finding an exact solution would require significant time and computing resources. Nevertheless, it has been observed that a rat was able to quickly solve simplified versions of this problem (<underline><xref ref-type="bibr" rid="c2">Bure&#x0161; et al 1992</xref></underline>, <underline><xref ref-type="bibr" rid="c6">de Jong et al 2011</xref></underline>). The idea of exploiting replay in navigation sequence learning has been demonstrated to have a positive influence on learning (<underline><xref ref-type="bibr" rid="c19">Johnson &#x0026; Redish 2005</xref></underline>), and here we go beyond this by further exploiting reward structure in the replay.</p>
<p>In the behavior of interest, rats are observed to converge quickly to a near-optimal path linking 5 baited food wells in a 151cm radius open arena. During their successive approximation to the optimal path, the rats often traversed segments of the optimal trajectory, as well as non-optimal segments. Observing this behavior, we conjectured the existence of neural mechanisms that would allow the optimal segments to be reinforced and the non-optical segments to be rejected, thus leading to the production of the overall near-optimal trajectory. The overall mechanism we propose can be decomposed into two distinct neural systems. The first is a replay mechanism that favors the representation of snippets that occurred on these optimal segments, and that in contrast will give reduced representation to snippets that correspond to non-optimal trajectory segments. Here we demonstrate a simple but powerful method based on spatial reward propagation that implements this mechanism. Interestingly, this characterization of replay is broadly consistent with the effects of reward on replay observed in behaving animals (<underline><xref ref-type="bibr" rid="c1">Ambrose et al 2016</xref></underline>).</p>
<p>The second neural system required to achieve this integrative performance is a sequence learning system that can integrate multiple subsequences (i.e. snippets) into a consolidated representation, taking into consideration the probability distributions of replay so as to favor more frequently replayed snippets. Here we considered a well-characterized model of sequence learning based on recurrent connections in prefrontal cortex that is perfectly suited to meet the sequence learning requirements.</p>
<sec id="s4a">
<sec id="s4a1">
<title>Replay mechanism</title>
<p>Replay is modeled using a procedure that randomly selects a subset of place-cells coding part of a sequence, and outputs this snippet while taking into account the proximity of this snippet to a future reward. Each time a reward is encountered, it is taken into consideration in generating the snippet, and reward value is propagated backwards along the sequence, thus implementing a form of spatio-temporal credit assignment. This can be viewed in the <xref ref-type="fig" rid="fig2">figures 2</xref>, <xref ref-type="fig" rid="fig6">6</xref> and <xref ref-type="fig" rid="fig7">7</xref> illustrating the snippet probability densities. The replay mechanism also implements a second feature observed in animal data, which is a tendency to replay snippets in reverse order. These two features of the replay model correspond to what is observed in the rat neurophysiology, and they also make fundamental contributions to the model&#x2019;s ability to converge on an efficient navigation path. This extends previous demonstrations of the value of replay to include reward-modulated optimization (<underline><xref ref-type="bibr" rid="c19">Johnson &#x0026; Redish 2005</xref></underline>).</p>
</sec>
<sec id="s4a2">
<title>Reservoir network</title>
<p>Reservoir computing exploits the spatio-temporal dynamics of recurrently connected neurons that are sensitive to the spaiotemporal structure of input sequences (<underline><xref ref-type="bibr" rid="c9">Dominey 1995</xref></underline>, <underline><xref ref-type="bibr" rid="c17">Jaeger &#x0026; Haas 2004</xref></underline>, <underline><xref ref-type="bibr" rid="c23">Maass et al 2002</xref>)</underline>. The frontal cortex has been demonstrated to operate on these reservoir properties (<underline><xref ref-type="bibr" rid="c11">Enel et al 2016</xref></underline>). Here we demonstrated how a reservoir model of PFC meets two requirements for sequence learning: First, it can concatenate randomly replayed subsequences (snippets) in order to generate the compete original sequence. Second, it is sensitive to the statistics of replay, and thus can learn to ignore rare snippets (which correspond to snippets on inefficient subsequences, far from rewards) thus learning to optimize.</p>
</sec>
<sec id="s4a3">
<title>Effects of reward</title>
<p>The instantaneous reward information acquired during a past experience is used for recursively updating the snippet replay likelihood in the hippocampus model. The resulting time distribution of snippets features multiple modes defined around the moments that rewards are obtained. This creates a reward gradient and allows sensory-motor associations to be learned by the prefrontal cortex and striatum model. This is a novel form of reinforcement learning, and the main effect of reward combined with hippocampus replay is to reinforce existing efficient paths between rewards. A secondary effect of rewards could be observed when rewards are sufficiently close for allowing a mutual contribution to the snippet replay likelihood surrounding the moments associated with reward delivery. Thus, we predict that a cluster of reward sites will have the effect of propagating the reward information farther than a single reward.</p>
</sec>
<sec id="s4a4">
<title>Effects of reverse replay</title>
<p>The reverse replay mechanism has a dual effect. First, a snippet replayed in reverse order will play the role of a short-term support for the reward propagation in reverse direction in the snippet replay likelihood online learning. Place-cell activation sequences leading to a nearby reward are represented more frequently and earlier than other paths, and allow less efficient sub-paths to be rejected. It results in a rewarding path selection mechanism by implementing a form of saliency based on reward information. This is a form of spatio-temporal credit assignment that allows to take advantage of the reservoir network ability to combine multiple snippets into a whole sequence. We showed that it is possible to consolidate multiple sequences featuring parts of the same underlying optimal sequence into one efficient sequence and to generate it autonomously. Second, when the snippet replay likelihood is learned, a non-zero reverse replay rate allows the prefrontal cortex to be exposed to sequences of place-cell activations in both forward and reverse direction. This results in sequence learning in both directions while having experienced a place-cell activation sequence in one direction only. These results can be tested experimentally by recording place cells activities in SWR during the task.</p>
</sec>
<sec id="s4a5">
<title>Conclusions and limitations</title>
<p>The model we studied here is able to mimic the rat&#x2019;s ability to find good approximations to the traveling salesperson problem by taking advantage of recent rewarding experiences for updating a trajectory generative model using hippocampus awake replay. We showed that reverse replay allows the agent to reduce the TSP task complexity by considering an undirected graph where feeders are vertices and trajectories are the edges instead of a directed graph. In this case, autonomous sequence generation is no longer possible but the information available in each prediction of the prefrontal cortex contains the expected locations. This allows the building of a navigation policy taking into account the salient actions suggested by the prefrontal cortex predictions, which are learned from hippocampus replay.</p>
</sec>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Ambrose</surname> <given-names>RE</given-names></string-name>, <string-name><surname>Pfeiffer</surname> <given-names>BE</given-names></string-name>, <string-name><surname>Foster</surname> <given-names>DJ.</given-names></string-name> <year>2016</year>. <article-title>Reverse replay of hippocampal place cells is uniquely modulated by changing reward</article-title>. <source>Neuron</source> <volume>91</volume>: <fpage>1124</fpage>-<lpage>36</lpage></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Bure&#x0161;</surname> <given-names>J</given-names></string-name>, <string-name><surname>Bure&#x0161;ov&#x00E1;</surname> <given-names>O</given-names></string-name>, <string-name><surname>Nerad</surname> <given-names>L.</given-names></string-name> <year>1992</year>. <article-title>Can rats solve a simple version of the traveling salesman problem?</article-title> <source>Behavioural brain research</source> <volume>52</volume>: <fpage>133</fpage>-<lpage>42</lpage></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Carr</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Jadhav</surname> <given-names>SP</given-names></string-name>, <string-name><surname>Frank</surname> <given-names>LM.</given-names></string-name> <year>2011</year>. <article-title>Hippocampal replay in the awake state: a potential substrate for memory consolidation and retrieval</article-title>. <source>Nature neuroscience</source> <volume>14</volume>: <fpage>147</fpage>-<lpage>53</lpage></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Cenquizca</surname> <given-names>LA</given-names></string-name>, <string-name><surname>Swanson</surname> <given-names>LW.</given-names></string-name> <year>2007</year>. <article-title>Spatial organization of direct hippocampal field CA1 axonal projections to the rest of the cerebral cortex</article-title>. <source>Brain research reviews</source> <volume>56</volume>: <fpage>1</fpage>-<lpage>26</lpage></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Davidson</surname> <given-names>TJ</given-names></string-name>, <string-name><surname>Kloosterman</surname> <given-names>F</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>MA.</given-names></string-name> <year>2009</year>. <article-title>Hippocampal replay of extended experience</article-title>. <source>Neuron</source> <volume>63</volume>: <fpage>497</fpage>-<lpage>507</lpage></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>de Jong</surname> <given-names>LW</given-names></string-name>, <string-name><surname>Gereke</surname> <given-names>B</given-names></string-name>, <string-name><surname>Martin</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Fellous</surname> <given-names>J-M.</given-names></string-name> <year>2011</year>. <article-title>The traveling salesrat: insights into the dynamics of efficient spatial navigation in the rodent</article-title>. <source>Journal of Neural Engineering</source> <volume>8</volume>: <fpage>065010</fpage></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Delatour</surname> <given-names>B</given-names></string-name>, <string-name><surname>Witter</surname> <given-names>M.</given-names></string-name> <year>2002</year>. <article-title>Projections from the parahippocampal region to the prefrontal cortex in the rat: evidence of multiple pathways</article-title>. <source>European Journal of Neuroscience</source> <volume>15</volume>: <fpage>1400</fpage>-<lpage>07</lpage></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Diba</surname> <given-names>K</given-names></string-name>, <string-name><surname>Buzsaki</surname> <given-names>G.</given-names></string-name> <year>2007</year>. <article-title>Forward and reverse hippocampal place-cell sequences during ripples</article-title>. <source>Nature neuroscience</source> <volume>10</volume>: <fpage>1241</fpage></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Dominey</surname> <given-names>PF.</given-names></string-name> <year>1995</year>. <article-title>Complex sensory-motor sequence learning based on recurrent state representation and reinforcement learning</article-title>. <source>Biol Cybern</source> <volume>73</volume>: <fpage>265</fpage>-<lpage>74</lpage></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="book"><string-name><surname>Eiter</surname> <given-names>T</given-names></string-name>, <string-name><surname>Mannila</surname> <given-names>H.</given-names></string-name> <year>1994</year>. <chapter-title>Computing discrete Frechet distance</chapter-title>. <source>Tech. Report CD-TR 94/64, Information Systems Department</source>, <publisher-name>Technical University of Vienna</publisher-name>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Enel</surname> <given-names>P</given-names></string-name>, <string-name><surname>Procyk</surname> <given-names>E</given-names></string-name>, <string-name><surname>Quilodran</surname> <given-names>R</given-names></string-name>, <string-name><surname>Dominey</surname> <given-names>PF.</given-names></string-name> <year>2016</year>. <article-title>Reservoir Computing Properties of Neural Dynamics in Prefrontal Cortex</article-title>. <source>PLoS computational biology</source> <volume>12</volume>: <fpage>e1004967</fpage></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Euston</surname> <given-names>DR</given-names></string-name>, <string-name><surname>Tatsuno</surname> <given-names>M</given-names></string-name>, <string-name><surname>McNaughton</surname> <given-names>BL.</given-names></string-name> <year>2007</year>. <article-title>Fast-forward playback of recent memory sequences in prefrontal cortex during sleep</article-title>. <source>Science</source> <volume>318</volume>: <fpage>1147</fpage>-<lpage>50</lpage></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Foster</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>MA.</given-names></string-name> <year>2006</year>. <article-title>Reverse replay of behavioural sequences in hippocampal place cells during the awake state</article-title>. <source>Nature</source> <volume>440</volume>: <fpage>680</fpage>-<lpage>83</lpage></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Gupta</surname> <given-names>AS</given-names></string-name>, <string-name><surname>van der Meer</surname> <given-names>MAA</given-names></string-name>, <string-name><surname>Touretzky</surname> <given-names>DS</given-names></string-name>, <string-name><surname>Redish</surname> <given-names>AD.</given-names></string-name> <year>2010</year>. <article-title>Hippocampal replay is not a simple function of experience</article-title>. <source>Neuron</source> <volume>65</volume>: <fpage>695</fpage>-<lpage>705</lpage></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="other"><string-name><surname>Harland</surname> <given-names>B</given-names></string-name>, <string-name><surname>Contreras</surname> <given-names>M</given-names></string-name>, <string-name><surname>Fellous</surname> <given-names>J-M.</given-names></string-name> <year>2018</year>. <article-title>A Role for the Longitudinal Axis of the Hippocampus in Multiscale Representations of Large and Complex Spatial Environments and Mnemonic Hierarchies</article-title> In <source>The Hippocampus-Plasticity and Functions</source>: IntechOpen</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="book"><string-name><surname>Jaeger</surname> <given-names>H.</given-names></string-name> <year>2001</year>. <source>The" echo state" approach to analysing and training recurrent neural networks-with an erratum note&#x2019;</source>. <publisher-loc>Bonn, Germany</publisher-loc>: <publisher-name>German National Research Center for Information Technology GMD Technical Report</publisher-name> <fpage>148</fpage></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Jaeger</surname> <given-names>H</given-names></string-name>, <string-name><surname>Haas</surname> <given-names>H.</given-names></string-name> <year>2004</year>. <article-title>Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless communication</article-title>. <source>Science</source> <volume>304</volume>: <fpage>78</fpage>-<lpage>80</lpage></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Ji</surname> <given-names>D</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>MA.</given-names></string-name> <year>2007</year>. <article-title>Coordinated memory replay in the visual cortex and hippocampus during sleep</article-title>. <source>Nat Neurosci</source> <volume>10</volume>: <fpage>100</fpage>-<lpage>7</lpage></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Johnson</surname> <given-names>A</given-names></string-name>, <string-name><surname>Redish</surname> <given-names>AD.</given-names></string-name> <year>2005</year>. <article-title>Hippocampal replay contributes to within session learning in a temporal difference reinforcement learning model</article-title>. <source>Neural Netw</source> <volume>18</volume>: <fpage>1163</fpage>-<lpage>71</lpage></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Karlsson</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Frank</surname> <given-names>LM.</given-names></string-name> <year>2009</year>. <article-title>Awake replay of remote experiences in the hippocampus</article-title>. <source>Nat Neurosci</source> <volume>12</volume>: <fpage>913</fpage>-<lpage>8</lpage></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Kudrimoti</surname> <given-names>HS</given-names></string-name>, <string-name><surname>Barnes</surname> <given-names>CA</given-names></string-name>, <string-name><surname>McNaughton</surname> <given-names>BL.</given-names></string-name> <year>1999</year>. <article-title>Reactivation of hippocampal cell assemblies: effects of behavioral state, experience, and EEG dynamics</article-title>. <source>J Neurosci</source> <volume>19</volume>: <fpage>4090</fpage>-<lpage>101</lpage></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="book"><string-name><surname>Lukosevicius</surname> <given-names>M.</given-names></string-name> <year>2012</year>. <chapter-title>A practical guide to applying echo state networks</chapter-title> In <source>Neural networks: tricks of the trade</source>, pp. <fpage>659</fpage>-<lpage>86</lpage>: <publisher-name>Springer</publisher-name></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Maass</surname> <given-names>W</given-names></string-name>, <string-name><surname>Natschlager</surname> <given-names>T</given-names></string-name>, <string-name><surname>Markram</surname> <given-names>H.</given-names></string-name> <year>2002</year>. <article-title>Real-time computing without stable states: a new framework for neural computation based on perturbations</article-title>. <source>Neural Comput</source> <volume>14</volume>: <fpage>2531</fpage>-<lpage>60</lpage></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>McClelland</surname> <given-names>JL</given-names></string-name>, <string-name><surname>McNaughton</surname> <given-names>BL</given-names></string-name>, O&#x2019;<string-name><surname>Reilly</surname> <given-names>RC.</given-names></string-name> <year>1995</year>. <article-title>Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory</article-title>. <source>Psychol Rev</source> <volume>102</volume>: <fpage>419</fpage>-<lpage>57</lpage></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Shin</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Jadhav</surname> <given-names>SP.</given-names></string-name> <year>2016</year>. <article-title>Multiple modes of hippocampal&#x2013;prefrontal interactions in memory-guided behavior</article-title>. <source>Current opinion in neurobiology</source> <volume>40</volume>: <fpage>161</fpage>-<lpage>69</lpage></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Singer</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Frank</surname> <given-names>LM.</given-names></string-name> <year>2009</year>. <article-title>Rewarded Outcomes Enhance Reactivation of Experience in the Hippocampus</article-title>. <source>Neuron</source> <volume>64</volume>: <fpage>910</fpage>-<lpage>21</lpage></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>van der Meer</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Johnson</surname> <given-names>A</given-names></string-name>, <string-name><surname>Schmitzer-Torbert</surname> <given-names>NC</given-names></string-name>, <string-name><surname>Redish</surname> <given-names>AD.</given-names></string-name> <year>2010</year>. <article-title>Triple dissociation of information processing in dorsal striatum, ventral striatum, and hippocampus on a learned spatial decision task</article-title>. <source>Neuron</source> <volume>67</volume>: <fpage>25</fpage>-<lpage>32</lpage></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Vertes</surname> <given-names>RP</given-names></string-name>, <string-name><surname>Hoover</surname> <given-names>WB</given-names></string-name>, <string-name><surname>Szigeti-Buck</surname> <given-names>K</given-names></string-name>, <string-name><surname>Leranth</surname> <given-names>C.</given-names></string-name> <year>2007</year>. <article-title>Nucleus reuniens of the midline thalamus: link between the medial prefrontal cortex and the hippocampus</article-title>. <source>Brain research bulletin</source> <volume>71</volume>: <fpage>601</fpage>-<lpage>09</lpage></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="book"><string-name><surname>Widrow</surname> <given-names>B</given-names></string-name>, <string-name><surname>Hoff</surname> <given-names>ME.</given-names></string-name> <year>1960</year>. <source>Adaptive switching circuits</source>, <publisher-name>STANFORD UNIV CA STANFORD ELECTRONICS LABS</publisher-name></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="book"><string-name><surname>Wylie</surname> <given-names>TR.</given-names></string-name> <year>2013</year>. <source>The discrete Fr&#x00E9;chet distance with applications</source>. <publisher-name>Montana State University-Bozeman, College of Engineering</publisher-name></mixed-citation></ref>
</ref-list>
</back>
</article>
