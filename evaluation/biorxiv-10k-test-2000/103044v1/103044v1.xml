<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/103044</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Decoding the categorization of visual motion with magnetoencephalography</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Bekhti</surname> <given-names>Yousra</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">&#x002B;</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Gramfort</surname> <given-names>Alexandre</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">&#x002B;</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Zilber</surname> <given-names>Nicolas</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Wassenhove</surname> <given-names>Virginie van</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<aff id="a1">
<label>1</label><institution>LTCI, T&#x00E9;l&#x00E9;com ParisTech, Universit&#x00E9; Paris-Saclay</institution>, 75013, Paris, <country>France</country></aff>
<aff id="a2">
<label>2</label><institution>CEA, DRF/I2BM, NeuroSpin Center, Universit&#x00E9; Paris-Saclay</institution>, F-91191 Gif-sur-Yvette, <country>France</country></aff>
<aff id="a3">
<label>3</label><institution>Cognitive Neuroimaging Unit, INSERM, Universit&#x00E9; Paris-Sud, Universit&#x00E9; Paris-Saclay</institution>, 91191 Gif/Yvette, <country>France</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label>Corresponding author: <email>yousra.bekhti@gmail.com</email></corresp>
<fn id="n1" fn-type="equal"><label>&#x002B;</label><p>These authors contributed equally to this work</p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2017</year>
</pub-date>
<elocation-id>103044</elocation-id>
<history>
<date date-type="received">
<day>25</day>
<month>1</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>25</day>
<month>1</month>
<year>2017</year>
</date>
</history><permissions><copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="103044.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>ABSTRACT</title>
<p>Brain decoding techniques are particularly efficient at deciphering weak and distributed neural patterns. Brain decoding has primarily been used in cognitive neurosciences to predict differences between pairs of stimuli (e.g. faces <italic>vs.</italic> houses), but how distinct brain/perceptual states can be decoded following the presentation of continuous sensory stimuli is unclear. Here, we developed a novel approach to decode brain activity recorded with magnetoencephalography while participants discriminated the coherence of two intermingled clouds of dots. Seven levels of visual motion coherence were tested and participants reported the colour of the most coherent cloud. The decoding approach was formulated as a ranked-classification problem, in which the model was evaluated by its capacity to predict the order of a pair of trials, each tested with two distinct visual motion coherence levels. Two brain states were decoded as a function of the degree of visual motion coherence. Importantly, perceptual motion coherence thresholds were found to match the decoder boundaries in a fully data-driven way. The algorithm revealed the earliest categorization in hMT&#x002B;, followed by V1/V2, IPS, and vlPFC.</p>
</abstract>
<counts>
<page-count count="11"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>In natural environments, coherent motion is a vital sensory cue that helps the brain individuate objects in the world. Seminal neurophysiological work has described neurons in the middle temporal (MT) lobe of monkeys that were selective to the coherence<sup><xref rid="c1" ref-type="bibr">1</xref></sup> and the direction<sup><xref rid="c2" ref-type="bibr">2</xref></sup> of visual motion. During a perceptual classification task, direction-selectivity can be decoded from the activity of neural populations in MT<sup><xref rid="c3" ref-type="bibr">3</xref>, <xref rid="c4" ref-type="bibr">4</xref></sup>. As visual motion processing relies on neural population codes, it is amenable to non-invasive functional human brain imaging such as fMRI or magnetoencephalography (MEG). Supervised learning techniques such as Multivariate Pattern Analysis (MVPA) are increasingly successful at characterizing where and when the neural analysis of stimuli such as visual orientation, motion direction or object classification is being realized<sup><xref rid="c5" ref-type="bibr">5</xref>&#x2013;<xref rid="c13" ref-type="bibr">13</xref></sup>.</p>
<p>In one of the earliest fMRI studies using MVPA, the direction of motion was successfully decoded from hMT&#x002B; (human analog of MT) activity<sup><xref rid="c14" ref-type="bibr">14</xref></sup>) but also, and surprisingly, from visual cortices V1, V2, V3 and V4<sup><xref rid="c15" ref-type="bibr">15</xref></sup>. The successful decoding of visual motion in V1, V3 and hMT&#x002B; has since been reported several times<sup><xref rid="c5" ref-type="bibr">5</xref>, <xref rid="c15" ref-type="bibr">15</xref>&#x2013;<xref rid="c18" ref-type="bibr">18</xref></sup>. Visual motion decoding in lower visual areas has been functionally interpreted as an indication of feature-based attention when required by the task<sup><xref rid="c15" ref-type="bibr">15</xref></sup> and as an effect of top-down modulation of early visual areas for conscious perception<sup><xref rid="c16" ref-type="bibr">16</xref></sup>. However, whether brain decoding using MVPA captures the selectivity of neural populations has been a subject of debate on the interpretational weigh given to decoding<sup><xref rid="c11" ref-type="bibr">11</xref>, <xref rid="c19" ref-type="bibr">19</xref>&#x2013;<xref rid="c21" ref-type="bibr">21</xref></sup>. Relevant to the current study, recent fMRI work has suggested that the sources of decoding in early visual areas may reflect the perceptual priors and biases of motion direction computation<sup><xref rid="c22" ref-type="bibr">22</xref></sup>.</p>
<p>To disambiguate the functional role of different brain regions in motion selectivity, characterizing the temporal unfolding of pattern classification within and across visual regions could be helpful. Here, we thus exploited the temporal sensitivity of MEG to find the latency at which sufficient information had been integrated to reach a stable classification boundary<sup><xref rid="c23" ref-type="bibr">23</xref>&#x2013;<xref rid="c25" ref-type="bibr">25</xref></sup>. 36 participants were recorded with MEG while performing a visual motion coherence discrimination task in which two intermingled populations of visual dots (red and green random-dot-kinematograms) moved randomly on the screen until one of them moved more coherently than the other<sup><xref rid="c26" ref-type="bibr">26</xref></sup> (<xref rid="fig1" ref-type="fig">Fig. 1-A</xref>). Participants were asked to report which of the two populations became most coherent over time. Seven motion coherence levels were tested and a novel multivariate decoding approach combining ridge regression and a ranking metric was developed. Contrary to classical decoding approaches based on binary classifiers such as support vector machines (SVM), a single decoder was estimated for all coherence levels, allowing robust parameter estimation despite high dimensional data. The ranking metric allowed taking into account the fact that visual motion coherence was an ordered variable<sup><xref rid="c27" ref-type="bibr">27</xref>, <xref rid="c28" ref-type="bibr">28</xref></sup>. This novel decoder was applied to brain activity recorded at the sensor level and to cortically-constrained source estimates. Using this decoding technique, we report the categorization of two separate brain states as a function of the degree of visual motion coherence. The categorization boundary matched participants&#x2019; behavioral outcomes. Our results suggest that incorporating such decoding methods may be suitable to address questions relevant to predictive coding and perceptual decision-making.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Categorization Decoding. a) One experimental trial in which participants discriminated which of the red or green cloud of moving dots was most coherent. b) Left: simulated data (gray) were best modeled by ordinal (red) than by a linear (red) fit. Right: similarity matrix providing a score of the decoding performance for each pairwise comparisons. c) Significant time-resolved decoding of visual motion coherence levels were found 100 to 600 ms (green) post-stimulus onset. d) Grand-average (n=36) similarity matrices in sensors (top), hMT&#x002B; (middle) and frontal-pole (bottom) for the selected time window. Distribution of behavioral perceptual thresholds (gray histogram) and the mean (dashed line). e) Correlation scores between each template and similarity matrix (black histograms) and likeliest boundary decoded from MEG data (dashed red line).</p></caption>
<graphic xlink:href="103044_fig1.tif"/>
</fig>
</sec>
<sec id="s2">
<title>Methods</title>
<sec id="s2a">
<title>Participants</title>
<p>Thirty-six participants took part in the study (16 females, mean 22.1 &#x002B;/- 2.2 y.o.). All were right-handed, had normal hearing and normal or corrected-to-normal vision. Prior to the experiment, all participants gave a written informed consent. All methods were carried out in accordance with relevant guidelines and regulations and by a named NeuroSpin (Gif-sur-Yvette, France). The study was conducted in agreement with the Declaration of Helsinki (2008) and was approved by the Ethics Committee on Human Research at Neurospin (Gif-sur-Yvette, France).</p>
</sec>
<sec id="s2b">
<title>Experimental design</title>
<p>The MEG session consisted of twelve experimental blocks alternating between rest and task <sup><xref rid="c26" ref-type="bibr">26</xref></sup>. Here, we solely focused on the main experimental task blocks in which participants&#x2019; performance on a visual motion coherence task was being assessed. During the task, one trial started with the presentation of a fixation cross followed by two intermixed clouds of dots or Random Dot Kinematograms (RDKs) (red and green) whose motion was fully incoherent. After a variable interval of 0.3 to 0.6 s, one of the two RDKs became more coherent than the other (<xref rid="fig1" ref-type="fig">Fig. 1-A</xref>). Participant had to determine by button press which of the red or green RDKs became more coherent. Seven possible levels of visual motion coherence were tested (15&#x0025;, 25&#x0025;, 35&#x0025;, 45&#x0025;, 55&#x0025;, 75&#x0025;, or 95&#x0025;), randomly assigned to a colour and to a direction. Each participant was tested with 28 trials per visual coherence level.</p>
</sec>
<sec id="s2c">
<title>Visual stimuli</title>
<p>The red and green RDKs were individually calibrated to isoluminance. To prevent local tracking of dots, a white fixation cross was located at the center of a 4&#x00B0; gray disk mask. RDKs were presented within an annulus of 4&#x00B0;-15&#x00B0; of visual angle. Dots had a radius of 0.2&#x00B0;. The flow of RDKs was 16.7 dots per deg<sup>2</sup> &#x00D7; sec with a speed of 10&#x00B0;/s. During the first 0.3 to 0.6 s of a given trial, both RDKs were incoherent (0&#x0025; of coherent motion). The duration of the incoherent phase was pseudo-randomized across each trial in order to increase the difficulty of the task by preventing participants&#x2019; expectation of the temporal onset coherent motion. After the incoherent phase, one RDK became more coherent than the other for one second. The direction of coherent dots was comprised within an angle of 45&#x00B0;-90&#x00B0; around the azimuth. 50&#x0025; of the trials were upward coherent motion and the remaining 50&#x0025; of the trials were downward coherent motion. At each frame, 5&#x0025; of all dots were randomly reassigned to new positions and incoherent dots to a new direction of motion. Dots going into collision in the next frame were also reassigned a new direction of motion.</p>
</sec>
<sec id="s2d">
<title>Psychophysical analysis</title>
<p>The performance of each individual was averaged as a function of the seven degree of visual motion coherence of the stimuli, irrespective of colour or direction of motion. The coherence discrimination threshold was set to 75&#x0025; of correctness for each individual&#x2019;s data, as typically used in a two-alternative forced choice (2-AFC) paradigm, forcing participants to adopt the same decision criterion for all stimuli<sup><xref rid="c29" ref-type="bibr">29</xref></sup>. Here, the 75&#x0025; detection threshold corresponds to chance level. They were then separately fitted to psychometric functions with the maximum-likelihood methodology (Psignifit<sup><xref rid="c30" ref-type="bibr">30</xref></sup>) which provided valid estimates of perceptual thresholds on a per individual basis (more details in <sup><xref rid="c26" ref-type="bibr">26</xref></sup>).</p>
</sec>
<sec id="s2e">
<title>MEG pre-processing and source reconstruction</title>
<p>All data pre-processing and source-imaging were done according to well accepted MEG guidelines<sup><xref rid="c31" ref-type="bibr">31</xref></sup>. Signal-Space-Separation (BS) was performed on raw data using Maxfilter (Elekta-Neuromag<sup><xref rid="c32" ref-type="bibr">32</xref></sup>) to compensate for external magnetic interferences. MEG data were band-pass filtered (2 to 45 Hz), down-sampled to 250 Hz and epoched from -100 ms to 1000 ms relative to the onset of RDK coherence. Trials that were contaminated by artifacts were rejected (e.g. peak-to-peak amplitude difference above 150 microvolts in EOG data) leaving 89&#x0025; of trials considered to have an appropriate signal-to-noise ratio. The cortically constrained source reconstruction was done using the dSPM method following the guidelines of the MNE software<sup><xref rid="c33" ref-type="bibr">33</xref></sup>. The entire pre-processing was done using MNE<sup><xref rid="c34" ref-type="bibr">34</xref></sup>.</p>
</sec>
<sec id="s2f">
<title>MEG decoding</title>
<p>Decoding generally consists in predicting a target variable <italic>y</italic> from one pattern of brain activity <inline-formula><alternatives><inline-graphic xlink:href="103044_inline1.gif"/></alternatives>
</inline-formula> among all possible patterns or brain states. When the target can take a finite number <italic>K</italic> of possible values, like a multi-class classification problem, one has that <inline-formula><alternatives><inline-graphic xlink:href="103044_inline2.gif"/></alternatives></inline-formula>. Here, when <italic>x</italic> were MEG signals, <italic>p</italic> was the number of channels and time points used for the prediction. When <italic>x</italic> was the amplitude of cortical sources, <italic>p</italic> corresponded to the number of source locations. The first goal of this study was to estimate how well each pair of visual motion coherence level could be discriminated against each other. Considering that multi-class classification approaches do not take into account the ordinal nature of the target to predict, indeed predicting 1 instead of 7 is as bad as predicting 1 instead of 2 although the mistake is obviously smaller in the second case, we instead built a decoder which could yield high pattern classification accuracy for distinguishable coherence levels, and low pattern classification accuracy for nearby levels of visual motion coherence which were perceptually hard to differentiate (cf. next two sections on method).</p>
<p>The second goal of the study was to find whether separate categorical brain states (two or more) emerged following the presentation of the stimuli as a function of the seven levels of visual motion coherence. Specifically, the task of participants consisted in deciding whether the red or the green cloud of dots was most coherent as a function of coherence level. One working hypothesis was thus that at least one boundary delimiting a possible threshold between the neural activations induced by low vs. high coherent motion would be found during decoding.</p>
<p>To address this question, we opted out of a regression model estimated jointly for all levels of coherence, and combined it with a ranking metric adapted to discrete and ordered targets. Although an alternative approach could have consisted in testing the incoherent portion of the stimuli against each level of visual coherence, this would have lead to a strongly imbalanced training dataset (i.e. 196 incoherence trials for 28 trials per level of coherence) which is heavily problematic for MVPA classification approaches<sup><xref rid="c35" ref-type="bibr">35</xref></sup>. Specifically, with this formulation of the decoding, an inaccurate model which always predicts incoherence instead of coherence would have 85&#x0025; of accuracy due to the imbalanced dataset. The ranking technique proposed here does not suffer from such class imbalance considering that a single regression model was learnt for all coherence levels, and the ranking metric employed yielded 50&#x0025; accuracy levels in spite of the low number of trials.</p>
<p>We now describe in detail the regression model employed.</p>
</sec>
<sec id="s2g">
<title>Regression model</title>
<p>Due to the limited number of data points available for learning, and the high dimensional nature of the neuroimaging data, we used a linear model following the standard approach in MVPA studies<sup><xref rid="c5" ref-type="bibr">5</xref>, <xref rid="c11" ref-type="bibr">11</xref>, <xref rid="c23" ref-type="bibr">23</xref></sup>. The target values <inline-formula><alternatives><inline-graphic xlink:href="103044_inline3.gif"/></alternatives></inline-formula> here provided for the <italic>n</italic> data points available for statistical inference, were derived from a linear combination of data <inline-formula><alternatives><inline-graphic xlink:href="103044_inline4.gif"/></alternatives></inline-formula> where <inline-formula><alternatives><inline-graphic xlink:href="103044_inline5.gif"/></alternatives></inline-formula> was a weight vector and <italic>X</italic> was a <italic>n</italic>-by-<italic>p</italic> data matrix. The value <italic>n</italic> here corresponded to the number of stimuli presentations, a.k.a. single trials or epochs. For each i<italic><sup>th</sup></italic> observation, the target <inline-formula><alternatives><inline-graphic xlink:href="103044_inline6.gif"/></alternatives></inline-formula> could take <italic>K</italic> different values: in this study, <italic>K</italic> = 7 corresponded to the seven levels of visual motion coherence defining the number of classes. Again, a multi-class classification approach could have been adopted, yet this strategy would have ignored that target values were ordered. For instance, decoding the 5th instead of the 2nd level of motion coherence is worse than predicting the 3rd level of motion coherence instead of the 2nd one. This is an information that a multi-class linear SVM model could not exploit. An SVM would also estimate <italic>p &#x00D7; K</italic> parameters instead of <italic>p</italic> which would naturally increase the risk of overfitting and reduced the interpretability of the results. Instead, we chose a ridge regression method, and evaluated the predictive performance with a metric tailored for ordinal problems. The ridge regression model was defined as the solution to the convex optimization problem:</p>
<disp-formula id="eqn1"><alternatives><graphic xlink:href="103044_eqn1.gif"/></alternatives></disp-formula>
<p>The ridge regression model is a popular approach whose practical success is due to fast estimation, robustness to noise and limited sensitivity to rough tuning of the parameter <italic>&#x03BB;</italic>. Indeed results obtained by ridge regression are known to be far less sensitive to the choice of <italic>&#x03BB;</italic> parameter compared to sparse estimators such as Lasso. In our experiments, <italic>&#x03BB;</italic> was the same for all subjects<sup><xref rid="c36" ref-type="bibr">36</xref></sup>.</p>
<p>Decoding was performed on a per individual basis using all epochs. The 204 gradiometers and different time windows were tested: for example, for the time window ranging from 100 to 600 ms, the dimensions of the data were the number of samples <italic>n</italic> = 196 (at most, 28 trials &#x00D7; 7 coherence levels) depending on the number of dropped epochs times the number of features <italic>p</italic> = 204 &#x00D7; 126 &#x007E; 2.5 &#x00D7; 10<sup>4</sup> where the temporal window ranging from 100 ms to 600 ms contained up to 126 samples. The performance of the method was evaluated with a 10-fold stratified cross-validation which preserved the percentage of samples for each class or motion coherence level in each fold.</p>
<p>Decoding was also performed on source-reconstructed data in bilateral regions of interest (ROI) previously reported as being implicated in the task<sup><xref rid="c26" ref-type="bibr">26</xref></sup>. In source-space, the dimensions of the data were <italic>n</italic> = 196 at most and, for instance, <italic>p</italic> = 126x117&#x007E;10<sup>6</sup> depending on the size of the ROI (here, 117 dipoles in the ROI).</p>
<p>Following estimation of the ridge regression model, a ranking metric was then employed to quantify the model performance while taking into account that the targets have a natural order.</p>
</sec>
<sec id="s2h">
<title>Assessing decoding performance with pairwise ranking metric</title>
<p>Although ridge regression preserves the order of the target variables, it does not provide a relevant metric for the evaluation of the success rate of the decoder with an ordered set of categories. When using a linear regression model, the mean square error (MSE) is the natural performance metric. Yet, in high dimensional settings with a limited number of samples (<italic>n</italic> &#x226A; <italic>p</italic>) as we are dealing with here, MSE is a poor metric. In order to reduce the variance of the estimated coefficients, high values of <italic>&#x03BB;</italic> were used causing a strong amplitude bias on the coefficients and a poor performance when measured using MSE. Performance evaluated with MSE was also affected in the presence of a bimodal state as illustrated in <xref rid="fig1" ref-type="fig">Fig. 1</xref>-B. Note that this strong bias problem is what motivates certain authors to use a Pearson correlation as measure of performance rather than the MSE, although MSE is natural when using ridge regression<sup><xref rid="c37" ref-type="bibr">37</xref></sup>.</p>
<p>To leverage the ordinal nature of the target values <italic>y</italic>, we quantified the performance in terms of ranking, where we tested the ability of the decoder to properly order pairs of samples, trials, based on the target to predict<sup><xref rid="c27" ref-type="bibr">27</xref>, <xref rid="c28" ref-type="bibr">28</xref></sup>. The ranking metric consisted in comparing the real values of <italic>y</italic> and the predicted ones. Let us consider two trials from the validation dataset with <inline-formula><alternatives><inline-graphic xlink:href="103044_inline7.gif"/></alternatives></inline-formula> and where (<italic>y<sub>i</sub>; y <sub>j</sub></italic>) denote their associated labels.</p>
<p>Let <inline-formula><alternatives><inline-graphic xlink:href="103044_inline8.gif"/></alternatives></inline-formula> be the set of pairs with different labels. One quantifies the prediction accuracy <italic>Acc</italic> with the percentage of correct orderings for pairs of trials:
<disp-formula id="eqn2">
<alternatives>
<graphic xlink:href="103044_eqn2.gif"/>
</alternatives>
</disp-formula></p>
<p>For each pair of trials, there were two possible options and the chance level was therefore 50&#x0025;. This quantity is related to Kendall&#x2019;s rank correlation metric<sup><xref rid="c38" ref-type="bibr">38</xref></sup> which can be seen as a non-parametric correlation measure. To go beyond average accuracy, a key insight of this work was to inspect for which pair of trials the decoder made a mistake. For this, we thus defined a 7-by-7 similarity matrix <italic>M</italic>:
<disp-formula id="eqn3">
<alternatives>
<graphic xlink:href="103044_eqn3.gif"/>
</alternatives>
</disp-formula></p>
<p>Each <italic>M<sub>i;</sub><sub>j</sub></italic> was a value between 0 and 1 that told us how well we could distinguish the level <italic>i</italic> from the level <italic>j</italic>, 1 being the best; inversely, if the level <italic>i</italic> was similar or close to the level <italic>j</italic>, this decoding value would be close to chance level 0.5. The matrix was symmetric since comparing the levels <italic>i</italic> and <italic>j</italic> or <italic>j</italic> and <italic>i</italic> provides the same score. Such matrices, that can be seen as confusion matrices adapted for our pairwise ranking metric, are presented in <xref rid="fig1" ref-type="fig">Fig. 1</xref>-D.</p>
</sec>
<sec id="s2i">
<title>Criteria for decoding categorization</title>
<p>Template matrices were defined for the discrete values of theoretically possible categorization into two brain states driven by the motion coherence levels, namely: 15&#x0025;, 25&#x0025;, 35&#x0025;, 45&#x0025;, 55&#x0025;, 75&#x0025;, or 95&#x0025;. Each matrix had an on/off pattern at a given threshold (e.g. 55&#x0025;) with values of 0.5 (off) or 0.65 (on). An example is provided in the black matrices of <xref rid="fig1" ref-type="fig">Fig. 1</xref>-E. The correlation between the empirical matrices (fully based on MEG data) and all the possible template matrices as defined above, thus provided the selection criterion to decode a categorization pattern at a specific threshold. Specifically, for each empirical similarity matrix, the template matrix yielding the highest correlation score was considered a good predictor of the participants&#x2019; motion coherence thresholds eliciting the choice boundary from MEG data indicated as a dashed vertical line in <xref rid="fig1" ref-type="fig">Fig. 1</xref>-D.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3j">
<title>Modeling of simulated data as proof of concept</title>
<p>First, we modeled typical behavioral profiles observed during a perceptual discrimination task by using simulated data (e.g. ranging from 7&#x0025; to 92&#x0025; of coherence). The modeling allowed validating the use of an ordinal model which fitted better the data than a linear model (<xref rid="fig1" ref-type="fig">Fig. 1</xref>-B, left panel). As detailed above, the simulated trials were decoded using cross-validation by fitting a ridge regression to the training data and evaluating the performance of the model on all possible pairwise combination of test trials. The similarity matrix (<xref rid="fig1" ref-type="fig">Fig. 1</xref>-B, right panel), which represents the predictive power in distinguishing two coherence levels, was evaluated with a 10-fold stratified cross-validation method. Each entry in the similarity matrix shows how similar each coherence level is to another one; alternatively, each entry can also be interpreted as how well one coherence level can be distinguished from another using a linear multivariate statistical model. All pairwise comparisons given in the similarity matrix built an anti-diagonal pattern: the lighter blocks in the similarity matrix were coherence levels for which no differences in brain responses could be captured yielding a decoding score at chance level; conversely, the darker blocks (red) captured high decoding accuracy scores for which brain responses highly differed between two coherent motion <italic>e.g.</italic>, brain responses to 7&#x0025; coherent motion were highly distinguishable from those obtained during the presentation of 92&#x0025; coherent motion. When comparing the neighboring levels 64&#x0025; and 78&#x0025; in (<xref rid="fig1" ref-type="fig">Fig. 1</xref>-B, right panel), the high accuracy of decoding demonstrated a difference in brain activity patterns, reflecting a discontinuity in the activation profiles despite a progressive change in the visual motion coherence levels. The observed discontinuity or edge located between 50&#x0025; and 64&#x0025; of visual motion coherence revealed the presence of a categorical boundary.</p>
</sec>
<sec id="s3k">
<title>Spatial selectivity of decoding categorization</title>
<p>The appropriate time window for best decoding performance was established using time-resolved cross-validation techniques<sup><xref rid="c24" ref-type="bibr">24</xref></sup>. The overall best decoding performance was obtained for latencies ranging from 100 ms to 600 ms post-motion coherence onset as illustrated in (<xref rid="fig1" ref-type="fig">Fig. 1</xref>-C). The decoder was applied to MEG data in this time window on a per individual basis. Similarity matrices scored how well pairs of visual motion coherence could be distinguished, and then ordered, on the basis of brain activity. <xref rid="fig1" ref-type="fig">Fig. 1</xref>-D reports the similarity matrices computed on grand-average MEG data (n = 36 participants). Similarity matrices obtained for the MEG sensors (gradiometers) are reported in the top panel. Similarity matrices obtained for source-reconstructed estimates in the ROI hMT&#x002B; and in a control region &#x201C;frontal pole&#x201D; are provided in the middle and bottom panels, respectively.</p>
<p>The similarity matrices obtained in sensor and hMT&#x002B; data showed two distinct categories as an anti-block-diagonal patterns: two light blocks of decoding score at chance level (&#x007E;50&#x0025;) for close coherence levels (low levels: 15&#x0025;-45&#x0025; against themselves, high levels: 55&#x0025;-95&#x0025; against themselves), and two dark blocks of decoding score nearing &#x007E;65&#x0025; for coherence levels that were apart, namely 15&#x0025;-45&#x0025; against 55&#x0025;-95&#x0025;. These results conform with the notion of perceptual categories, namely: visual motion coherence levels 45&#x0025; and 55&#x0025; were close from the point of view of the coherence level in visual stimulation, but distant in perceptual space with the former most likely classified incoherent and the latter as coherent. The two brain states thus defined by the similarity matrix are compatible with categorical classification of the stimuli in this task. Specifically, visual motion coherence stimuli could either elicit a pattern consistent with not detecting the coherent signal in the display and not discriminating within the ensemble of stimuli whose coherence could not be detected (below the boundary) and detecting the coherent signal in the display but not discriminating within the ensemble of stimuli whose coherence could be detected (above the boundary).</p>
<p>To further investigate the link between brain activity at the single trial level and behavioral outcomes, we systematically compared the boundary delimited by the decoding approach with the perceptual threshold obtained from psychometric fits. The mean perceptual threshold was obtained in the task from the previous study<sup><xref rid="c26" ref-type="bibr">26</xref></sup> and shown here in the histogram over the 36 subjects (<xref rid="fig1" ref-type="fig">Fig. 1</xref>-D, bottom panel). The emerging categorical boundary at 45-55&#x0025; of visual motion coherence in both sensors and hMT&#x002B; (but not frontal pole) matched well the mean perceptual threshold observed behaviorally (black dotted line; <xref rid="fig1" ref-type="fig">Fig. 1</xref>-D).</p>
<p>To establish a quantitative criterion for this observation, template matrices were constructed to model each theoretically possible perceptual threshold. Each template matrix was then correlated with each of the decoding similarity matrices obtained from empirical measurements (<xref rid="fig1" ref-type="fig">Fig. 1</xref>-E). The aim was to find the peak of the correlation between the template threshold and the emerging boundary. This procedure, which is similar in spirit to the Representational Similarity Analysis (RSA) approach<sup><xref rid="c9" ref-type="bibr">9</xref>,<xref rid="c39" ref-type="bibr">39</xref></sup>, insured that the decoding similarity matrix was not forced to look like any specific template matrix. The quantitative metric confirmed our qualitative assessment (<xref rid="fig1" ref-type="fig">Fig. 1</xref>-D). Specifically, the peaks of the correlations were found for template matrices corresponding to a mean perceptual threshold of 55&#x0025; in both MEG sensors and in source-reconstructed hMT&#x002B;; the control ROI showed no selectivity.</p>
</sec>
<sec id="s3l">
<title>Temporal accumulation selectivity of categorization decoding</title>
<p>The spatiotemporal sensitivity of source-reconstructed MEG data was exploited to test at which latency sufficient information had been integrated to reach a reliable and stable classification pattern. To explicit the choice of the cumulative time window range, <xref rid="fig2" ref-type="fig">Fig. 2</xref>-A shows the grand average time course in response to the seven motion coherence levels over the 36 subjects in hMT&#x002B;. As can be seen (<xref rid="fig2" ref-type="fig">Fig. 2</xref>-A) and as previously reported<sup><xref rid="c26" ref-type="bibr">26</xref></sup>, main differences were located at these latencies although no clear categorization were visible in the time response. For this, scoring was established in a temporally cumulative manner from 100 ms post-motion coherence onset on by adding the consecutive 50 ms time window to each previous one (<xref rid="fig2" ref-type="fig">Fig. 2</xref>-B) until 450 ms. The decoder was applied to sensors and to source estimates in the regions of interest as well as additional cortical sources known to be involved in the task<sup><xref rid="c26" ref-type="bibr">26</xref></sup>, namely: hMT&#x002B; and the control region frontal pole but also the medial primary and secondary visual cortices (V1/V2), the intraparietal sulcus (IPS) and ventrolateral prefrontal region (VLPFC) (<xref rid="fig2" ref-type="fig">Fig. 2</xref>, bottom left). In <xref rid="fig2" ref-type="fig">Fig. 2</xref>-B, in which all similarity matrices are reported, two brain categories of coherence levels seemed to emerge. As one of the focuses was to link the decoding to the behavioral data, the black dotted lines illustrated the known average perceptual threshold to find how well it fitted with the boundary found in the similarity matrices.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2</label>
<caption><p>Temporal-accumulation decoding. A) Grand average hMT&#x002B; time courses in response to the seven motion coherencelevels. B) Grand average similarity matrices (n = 36) in sensors, MT, V1/V2, IPS, VLPFC and frontal pole (top to bottom rows, respectively). Incremental decoding of the similarity matrices within the selected time window could be seen. Colored frames indicate the earliest decoding pattern capturing the perceptual thresholds (dashed lines) e.g. 250 ms for MT. C) Each similarity matrix was correlated with the template matrix optimally capturing perceptual thresholds. Correlations were cumulatively performed over the full time course of brain responses</p></caption>
<graphic xlink:href="103044_fig2.tif"/>
</fig>
<p>Using reverse-inference, we selected the template matrix which corresponded to the known mean perceptual thresholds of the 36 participants. We then computed the correlations in specific cortical regions to capture an anatomic and temporal discrimination. The correlation scores between the perceptual templates and the similarity matrices in the different cortical regions are provided in <xref rid="fig2" ref-type="fig">Fig. 2</xref>-C. The stability of the similarity matrices (<xref rid="fig2" ref-type="fig">Fig. 2</xref>-B) and the plateau of correlations between the template and the similarity matrices (<xref rid="fig2" ref-type="fig">Fig. 2</xref>-C) were first reached in hMT&#x002B; followed by V1/V2 in occipital regions, IPS and VLPFC. The latency of optimal decoding was consistent with seminal neurophysiology work suggesting functional selectivity of motion computation in hMT&#x002B; which may also be indicative of behavioral choice boundary<sup><xref rid="c3" ref-type="bibr">3</xref>, <xref rid="c4" ref-type="bibr">4</xref>, <xref rid="c16" ref-type="bibr">16</xref>, <xref rid="c40" ref-type="bibr">40</xref></sup>. Perceptual boundaries for motion coherence discrimination could also be decoded later on in regions implicated in the task (V1/V2, IPS and much later in VlPFC) but not in the control region. These observations suggest that the decoder was anatomically and temporally selective. Specifically, the sequence of decoding latencies suggests that the outcome of categorization computed in hMT&#x002B; may be forwarded downstream to V1/V2 &#x2013; as a possible general mechanism contributing to plasticity - as well as VLPFC, as a likely consequence of perceptual decisions required by the task. Decision-related aspect was likely not encoded in low-level sensory areas, however the categorization pattern was still visible in hMT&#x002B; when appearing in VLPFC due to accumulation of evidence over the whole time range.</p>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>In this study, we showed that brain decoding could classify brain states as a function of visual motion coherence during a discrimination task. The categorical boundary partitioning two brain states was consistent with participants&#x2019; discrimination performance as indexed by their perceptual thresholds. Specifically, while the decoder was at chance level in discriminating between two motion coherence levels within the same perceptual category (within perceived or within non-perceived levels of visual motion coherence), the decoder performed well in discriminating brain activity in response to motion coherence levels across different categories (across perceived and non-perceived levels of visual motion coherence). We discuss below the implications and limitations of our findings.</p>
<p>In the visual motion coherence discrimination task used here, the intermixed clouds of dots (or RDKs) were identifiable by two distinct parameters: their color (red or green) and the increased degree of motion coherence in one cloud as comparedto the other. The task required participants to identify the colour of the most coherent cloud of dots. Although the employed stimuli were quite typical for visual motion tasks, a couple requirements set this task apart. First, the selective feature in the display was the coherence of motion irrespective of the direction of motion. This differed from feature-based attention tasks in which the relevant feature is the direction of coherent motion<sup><xref rid="c41" ref-type="bibr">41</xref>, <xref rid="c42" ref-type="bibr">42</xref></sup>. Second, the task required the discrimination of two clouds of dots simultaneously presented and spatially intermingled; this was distinct from a previous decoding study in which the two populations were spatially segregated<sup><xref rid="c16" ref-type="bibr">16</xref></sup>. Nevertheless, and consistent with prior decoding work on visual motion processing<sup><xref rid="c5" ref-type="bibr">5</xref>, <xref rid="c15" ref-type="bibr">15</xref>&#x2013;<xref rid="c18" ref-type="bibr">18</xref></sup> the earliest robust decoding of motion coherence was found in hMT&#x002B;, as well as V1/V2. Third, the color of the most coherent cloud of dots was randomized on every trial; as such, the color feature was orthogonal to the task requirement although participants&#x2019; effectively classified their responses as&#x201D;red&#x201D; or&#x201D;green&#x201D;. Accordingly, the successful decoding for any given pair of RDK coherence levels reported here (cf. cells in the decoding matrix being &#x003E; 50&#x0025;) captured information about motion coherence <italic>per se</italic>, not its color nor its direction.</p>
<p>The behavioral discrimination of continuous sensory information, such as coherent motion, requires the setting up of an internal criterion classifying sensory information into two or more categories<sup><xref rid="c3" ref-type="bibr">3</xref>, <xref rid="c40" ref-type="bibr">40</xref></sup>. Seminal work has shown that visual motion coherence at which neural activity reaches 50&#x0025; of its maximum value can be estimated by means of a neurometric threshold<sup><xref rid="c2" ref-type="bibr">2</xref></sup>. A similar approach has been used on MEG source estimates in this task, revealing the extent to which the neurometric thresholds computed in the local brain area hMT&#x002B; could effectively reflect participants&#x2019; discrimination of visual motion coherence<sup><xref rid="c26" ref-type="bibr">26</xref></sup>. While perceptual thresholds can be derived using several analytical steps and fitting procedures, we have shown that a multivariate decoder can directly capture the partitioning of brain activity as a function of participants&#x2019; performance by using a dedicated ranking metric associated with a template matrix correlated with the errors of the decoder when evaluated on left out test data. Our approach also showed that the partitioning of brain states fitting participants&#x2019; perceptual thresholds could be found at different timings and at different cortical locations.</p>
<p>Additionally, we found that the more sensory evidence accumulated over time, the more stable and robust the similarity matrices became both at the scalp level and in brain regions. The first stable decoding pattern emerged in hMT&#x002B; (&#x007E;250 ms), consistent with the known likelihood estimations and evidence accumulation of visual motion in this region and at this latency<sup><xref rid="c2" ref-type="bibr">2</xref>, <xref rid="c3" ref-type="bibr">3</xref></sup>. By 300 ms, a comparable decoding pattern was found in V1/V2, followed by IPS and by 450 ms by VLPFC. The early decoding latencies found in posterior regions and the later latencies found in frontal regions were overall consistent with decoding accuracies reported in perceptual decoding studies. Visual awareness can typically be decoded early in occipital regions and late in frontal areas<sup><xref rid="c20" ref-type="bibr">20</xref>, <xref rid="c43" ref-type="bibr">43</xref>&#x2013;<xref rid="c46" ref-type="bibr">46</xref></sup> and while the late decoding component is related to perceptual awareness, it can alsor reflect expectation, task requirements, and attentional selection<sup><xref rid="c20" ref-type="bibr">20</xref>, <xref rid="c43" ref-type="bibr">43</xref></sup>.</p>
<p>The observed spatiotemporal sequencing and stabilization of peak decoding in regions implicated in the task (but not others, i.e. control area frontal pole) suggest that motion selectivity and choice probability computed in hMT&#x002B; could be passed on downstream to early visual cortices as well as to decision-related areas (IPS). Recent models of visual motion processing<sup><xref rid="c4" ref-type="bibr">4</xref></sup> and recent fMRI data<sup><xref rid="c22" ref-type="bibr">22</xref></sup> have suggested that perceptual priors in early visual cortices may be shaped on the basis of higher-levels computations. Both seminal and recent findings have suggested that attention and feature-selectivity may be crucial in the modulation of early sensory cortices<sup><xref rid="c5" ref-type="bibr">5</xref>, <xref rid="c16" ref-type="bibr">16</xref>, 42</sup>. Our MEG decoding results add to this literature by suggesting that selectivity to higher-order features computed in hMT&#x002B; such as motion coherence irrespective of direction or color may feedback to early visual cortices. These and other<sup><xref rid="c17" ref-type="bibr">17</xref>, <xref rid="c18" ref-type="bibr">18</xref></sup> results also suggest that the classification boundaries computed in hMT&#x002B; may have lasting effects for the analysis of visual motion. In particular, and consistent with previous literature<sup><xref rid="c5" ref-type="bibr">5</xref>, <xref rid="c15" ref-type="bibr">15</xref>&#x2013;<xref rid="c18" ref-type="bibr">18</xref></sup>, the latency of the categorization pattern across brain regions suggest the possibility that information relevant to perceptual boundaries from hMT&#x002B; feedbacks to V1/V2 consistent with predictive coding models of visual processing<sup><xref rid="c4" ref-type="bibr">4</xref>, <xref rid="c47" ref-type="bibr">47</xref></sup> and learning theories<sup><xref rid="c48" ref-type="bibr">48</xref>&#x2013;<xref rid="c51" ref-type="bibr">51</xref></sup>.</p>
<p>Nevertheless, it is noteworthy that in the context of perceptual categorization tasks such as the one employed here, the dissociation between the perceptual and the decisional components are difficult to disentangle<sup><xref rid="c20" ref-type="bibr">20</xref>, <xref rid="c52" ref-type="bibr">52</xref>, <xref rid="c53" ref-type="bibr">53</xref></sup>. Several studies have discussed the dissociation between perceptual processing and decision-making<sup><xref rid="c20" ref-type="bibr">20</xref>, <xref rid="c21" ref-type="bibr">21</xref>, <xref rid="c52" ref-type="bibr">52</xref>, <xref rid="c54" ref-type="bibr">54</xref>&#x2013;<xref rid="c59" ref-type="bibr">59</xref></sup>. For instance, a temporal dissociation between early sensory processing in occipital areas and decision-related processing in parieto-frontal regions have been shown to be increasingly pronounced over time<sup><xref rid="c20" ref-type="bibr">20</xref></sup>. The perceptual thresholds used here to model the best fitting category do not readily dissociate between these two possibilities. Although the present study suggests that multivariate decoding can successfully retrieve perceptual thresholds, it is important to remain skeptical about the link between the information allowing decoding neural activity and its relationship to the computations effectively used to perform the task. For instance, brain activity categorized early on in hMT&#x002B; may contain top-down information feedback from decisional brain regions that may have helped the decoded categorization boundaries. However, three main aspects suggest that the decisional component may not be implicated here: first, the decision was made on the orthogonal feature color which was not used in the classifier as reported above. Second, the decoding in parietal cortices occurred much later than the stabilization observed in hMT&#x002B;. Although response-locked analyses<sup>52</sup> could be used to disentangle the perceptual and decisional component, one limitation of the current decoder is that it is sensitive to any statistical differences in amplitude or in latency. Hence, analyzing the same time window sorted on the basis of the stimulus onset or of the response would not allow to draw stronger conclusions regarding the (perceptual or decisional) nature of the cortical representations enabling the categorization of brain states. Third, recent evidence suggests that the inactivation of parietal regions are not decisive for motion categorization in monkeys<sup><xref rid="c60" ref-type="bibr">60</xref></sup>.</p>
<p>In sum, we presented a new MEG decoding technique that can capture the perceived categorization of continuous sensory information. Our results showed a sustainable pattern over time that correlated with participants&#x2019; perceptional threshold and which successively implicated hMT&#x002B;, V1/V2, IPS and VLPFC, consistent with general models of decision-making in motion categorization tasks<sup><xref rid="c61" ref-type="bibr">61</xref></sup>. Future work should aim at disentangling the perceptual analysis and the decisional components of perceptual decision-making tasks.</p>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Stoner</surname>, <given-names>G. R.</given-names></string-name> &#x0026; <string-name><surname>Albright</surname>, <given-names>T. D.</given-names></string-name><article-title>Neural correlates of perceptual motion coherence</article-title>. <source>Nature</source><volume>358</volume>, <fpage>412</fpage>&#x2013;<lpage>414</lpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Britten</surname>, <given-names>K. H.</given-names></string-name>, <string-name><surname>Shadlen</surname>, <given-names>M. N.</given-names></string-name>, <string-name><surname>Newsome</surname>, <given-names>W. T.</given-names></string-name> &#x0026; <string-name><surname>Movshon</surname>, <given-names>J. A.</given-names></string-name><article-title>The analysis of visual motion: a comparison of neuronal and psychophysical performance</article-title>. <source>The Journal of Neuroscience</source><volume>12</volume>, <fpage>4745</fpage>&#x2013;<lpage>4765</lpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Jazayeri</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>Movshon</surname>, <given-names>J. A.</given-names></string-name><article-title>Optimal representation of sensory information by neural populations</article-title>. <source>Nature neuroscience</source><volume>9</volume>, <fpage>690</fpage>&#x2013;<lpage>696</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Rust</surname>, <given-names>N. C.</given-names></string-name>, <string-name><surname>Mante</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name> &#x0026; <string-name><surname>Movshon</surname>, <given-names>J. A.</given-names></string-name><article-title>How MT cells analyze the motion of visual patterns</article-title>. <source>Nature neuroscience</source><volume>9</volume>, <fpage>1421</fpage>&#x2013;<lpage>1431</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Kamitani</surname>, <given-names>Y.</given-names></string-name> &#x0026; <string-name><surname>Tong</surname>, <given-names>F.</given-names></string-name><article-title>Decoding the visual and subjective contents of the human brain</article-title>. <source>Nature neuroscience</source><volume>8</volume>, <fpage>679</fpage>&#x2013;<lpage>685</lpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Wessberg</surname>, <given-names>J</given-names></string-name>. <etal>et al.</etal><article-title>Real-time prediction of hand trajectory by ensembles of cortical neurons in primates</article-title>. <source>Nature</source><volume>408</volume>, <fpage>361</fpage>&#x2013;<lpage>365</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Haynes</surname>, <given-names>J.-D.</given-names></string-name> &#x0026; <string-name><surname>Rees</surname>, <given-names>G.</given-names></string-name><article-title>Decoding mental states from brain activity in humans</article-title>. <source>Nature Reviews Neuroscience</source><volume>7</volume>, <fpage>523</fpage>&#x2013;<lpage>534</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Poldrack</surname>, <given-names>R. A.</given-names></string-name><article-title>Inferring mental states from neuroimaging data: from reverse inference to large-scale decoding</article-title>. <source>Neuron</source><volume>72</volume>, <fpage>692</fpage>&#x2013;<lpage>697</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name>, <string-name><surname>Pantazis</surname>, <given-names>D.</given-names></string-name> &#x0026; <string-name><surname>Oliva</surname>, <given-names>A.</given-names></string-name><article-title>Resolving human object recognition in space and time</article-title>. <source>Nature neuroscience</source><volume>17</volume>, <fpage>455</fpage>&#x2013;<lpage>462</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Horikawa</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Tamaki</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Miyawaki</surname>, <given-names>Y.</given-names></string-name> &#x0026; <string-name><surname>Kamitani</surname>, <given-names>Y.</given-names></string-name><article-title>Neural decoding of visual imagery during sleep</article-title>. <source>Science</source><volume>340</volume>, <fpage>639</fpage>&#x2013;<lpage>642</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Haynes</surname>, <given-names>J.-D.</given-names></string-name><article-title>A primer on pattern-based approaches to fMRI: principles, pitfalls, and perspectives</article-title>. <source>Neuron</source><volume>87</volume>, <fpage>257</fpage>&#x2013;<lpage>270</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Wardle</surname>, <given-names>S. G.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Grootswagers</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Khaligh-Razavi</surname>, <given-names>S.-M.</given-names></string-name> &#x0026; <string-name><surname>Carlson</surname>, <given-names>T. A.</given-names></string-name><article-title>Perceptual similarity of visual patterns predicts dynamic neural activation patterns measured with meg</article-title>. <source>NeuroImage</source><volume>132</volume>, <fpage>59</fpage>&#x2013;<lpage>70</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Ritchie</surname>, <given-names>J. B.</given-names></string-name> &#x0026; <string-name><surname>Carlson</surname>, <given-names>T. A.</given-names></string-name><article-title>Neural decoding and &#x201C;inner&#x201D; psychophysics: A distance-to-bound approach for linking mind, brain, and behavior</article-title>. <source>Frontiers in neuroscience</source><volume>10</volume> (<issue>2016</issue>).</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Beauchamp</surname>, <given-names>M. S.</given-names></string-name>, <string-name><surname>Cox</surname>, <given-names>R. W.</given-names></string-name> &#x0026; <string-name><surname>Deyoe</surname>, <given-names>E. A.</given-names></string-name><article-title>Graded effects of spatial and featural attention on human area MT and associated motion processing areas</article-title>. <source>Journal of neurophysiology</source><volume>78</volume>, <fpage>516</fpage>&#x2013;<lpage>520</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Kamitani</surname>, <given-names>Y.</given-names></string-name> &#x0026; <string-name><surname>Tong</surname>, <given-names>F.</given-names></string-name><article-title>Decoding seen and attended motion directions from activity in the human visual cortex</article-title>. <source>Current biology</source><volume>16</volume>, <fpage>1096</fpage>&#x2013;<lpage>1102</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Serences</surname>, <given-names>J. T.</given-names></string-name> &#x0026; <string-name><surname>Boynton</surname>, <given-names>G. M.</given-names></string-name><article-title>The representation of behavioral choice for motion in human visual cortex</article-title>. <source>The Journal of Neuroscience</source><volume>27</volume>, <fpage>12893</fpage>&#x2013;<lpage>12899</lpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Hogendoorn</surname>, <given-names>H.</given-names></string-name> &#x0026; <string-name><surname>Verstraten</surname>, <given-names>F. A.</given-names></string-name><article-title>Decoding the motion aftereffect in human visual cortex</article-title>. <source>NeuroImage</source><volume>82</volume>, <fpage>426</fpage>&#x2013;<lpage>432</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c18"><mixed-citation><string-name><surname>Van Kemenade</surname>, <given-names>B. M.</given-names></string-name>, <string-name><surname>Seymour</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Christophel</surname>, <given-names>T. B.</given-names></string-name>, <string-name><surname>Rothkirch</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>Sterzer</surname>, <given-names>P.</given-names></string-name> Decoding pattern motion information in V1. cortex <volume>57</volume>, <fpage>177</fpage>&#x2013;<lpage>187</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Carlson</surname>, <given-names>T. A.</given-names></string-name> &#x0026; <string-name><surname>Wardle</surname>, <given-names>S. G.</given-names></string-name><article-title>Sensible decoding</article-title>. <source>NeuroImage</source><volume>110</volume>, <fpage>217</fpage>&#x2013;<lpage>218</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Mostert</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name> &#x0026; <string-name><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name><article-title>Dissociating sensory from decision processes in human perceptual decision making</article-title>. <source>Scientific reports</source><volume>5</volume> (<issue>2015</issue>).</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Pitts</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Mart&#x00B4;&#x0131;nez</surname>, <given-names>A.</given-names></string-name> &#x0026; <string-name><surname>Hillyard</surname>, <given-names>S. A.</given-names></string-name><article-title>Visual processing of contour patterns under conditions of inattentional blindness</article-title>. <source>Journal of cognitive neuroscience</source><volume>24</volume>, <fpage>287</fpage>&#x2013;<lpage>303</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Vintch</surname>, <given-names>B.</given-names></string-name> &#x0026; <string-name><surname>Gardner</surname>, <given-names>J. L.</given-names></string-name><article-title>Cortical correlates of human motion perception biases</article-title>. <source>The Journal of Neuroscience</source><volume>34</volume>, <fpage>2592</fpage>&#x2013;<lpage>2604</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Mitchell</surname>, <given-names>T. M</given-names></string-name>. <etal>et al.</etal><article-title>Predicting human brain activity associated with the meanings of nouns</article-title>. <source>Science</source><volume>320</volume>, <fpage>1191</fpage>&#x2013;<lpage>1195</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Ramkumar</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Jas</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pannasch</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hari</surname>, <given-names>R.</given-names></string-name> &#x0026; <string-name><surname>Parkkonen</surname>, <given-names>L.</given-names></string-name><article-title>Feature-specific information processing precedes concerted activation in human visual cortex</article-title>. <source>The Journal of neuroscience</source><volume>33</volume>, <fpage>7691</fpage>&#x2013;<lpage>7699</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>King</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name><article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title>. <source>Trends in cognitive sciences</source><volume>18</volume>, <fpage>203</fpage>&#x2013;<lpage>210</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Zilber</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Ciuciu</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Gramfort</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Azizi</surname>, <given-names>L.</given-names></string-name> &#x0026; <string-name><surname>Van Wassenhove</surname>, <given-names>V.</given-names></string-name><article-title>Supramodal processing optimizes visual perceptual learning and plasticity</article-title>. <source>Neuroimage</source><volume>93</volume>, <fpage>32</fpage>&#x2013;<lpage>46</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="book"><string-name><surname>Herbrich</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Graepel</surname>, <given-names>T.</given-names></string-name> &#x0026; <string-name><surname>Obermayer</surname>, <given-names>K.</given-names></string-name><chapter-title>Large margin rank boundaries for ordinal regression</chapter-title>. <source>Advances in Large Margin Classifiers, 115&#x2013;132</source> (<publisher-name>MIT Press</publisher-name>, <publisher-loc>Cambridge, MA</publisher-loc>, <year>2000</year>).</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="book"><string-name><surname>Joachims</surname>, <given-names>T.</given-names></string-name><source>Optimizing search engines using clickthrough data. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &#x2019;02, 133&#x2013;142</source> (<publisher-name>ACM</publisher-name>, <publisher-loc>New York, NY, USA</publisher-loc>, <year>2002</year>).</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="book"><string-name><surname>Green</surname>, <given-names>D. M.</given-names></string-name> &#x0026; <string-name><surname>Swets</surname>, <given-names>J. A.</given-names></string-name> <source>Signal detection theory and psychophysics</source> (<publisher-name>Huntington, N.Y.: R. E. Krieger Pub. Co</publisher-name>, <year>1966</year>).</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Wichmann</surname>, <given-names>F. A.</given-names></string-name> &#x0026; <string-name><surname>Hill</surname>, <given-names>N. J.</given-names></string-name> <article-title>The psychometric function: I. fitting, sampling, and goodness of fit</article-title>. <source>Perception &#x0026; psychophysics</source> <volume>63</volume>, <fpage>1293</fpage>&#x2013;<lpage>1313</lpage> (<year>2001</year>).</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Gross</surname>, <given-names>J</given-names></string-name>. <etal>et al.</etal><article-title>Good practice for conducting and reporting MEG research</article-title>. <source>Neuroimage</source><volume>65</volume>, <fpage>349</fpage>&#x2013;<lpage>363</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Taulu</surname>, <given-names>S.</given-names></string-name> &#x0026; <string-name><surname>Simola</surname>, <given-names>J.</given-names></string-name><article-title>Spatiotemporal signal space separation method for rejecting nearby interference in MEG measurements</article-title>. <source>Physics in medicine and biology</source><volume>51</volume>, <fpage>1759</fpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Gramfort</surname>, <given-names>A</given-names></string-name>. <etal>et al.</etal><article-title>MNE software for processing MEG and EEG data</article-title>. <source>Neuroimage</source><volume>86</volume>, <fpage>446</fpage>&#x2013;<lpage>460</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Gramfort</surname>, <given-names>A</given-names></string-name>. <etal>et al.</etal><article-title>MEG and EEG data analysis with MNE-Python</article-title>. <source>Frontiers in neuroscience</source><volume>7</volume>, <fpage>267</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>He</surname>, <given-names>H.</given-names></string-name> &#x0026; <string-name><surname>Garcia</surname>, <given-names>E. A.</given-names></string-name><article-title>Learning from imbalanced data</article-title>. <source>IEEE Trans. on Knowl. and Data Eng</source>. <volume>21</volume>, <fpage>1263</fpage>&#x2013;<lpage>1284</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Varoquaux</surname>, <given-names>G</given-names></string-name>. <etal>et al.</etal><article-title>Assessing and tuning brain decoders: Cross-validation, caveats, and guidelines. NeuroImage 145</article-title>, <source>Part B</source>, <volume>166&#x2013;179</volume> (<issue>2017</issue>).</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Kay</surname>, <given-names>K. N.</given-names></string-name>, <string-name><surname>Naselaris</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Prenger</surname>, <given-names>R. J.</given-names></string-name> &#x0026; <string-name><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name><article-title>Identifying natural images from human brain activity</article-title>. <source>Nature</source><volume>452</volume>, <fpage>352</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Kruskal</surname>, <given-names>W. H.</given-names></string-name><article-title>Ordinal measures of association</article-title>. <source>Journal of the American Statistical Association</source><volume>53</volume>, <fpage>814</fpage>&#x2013;<lpage>861</lpage> (<year>1958</year>).</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Mur</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>Bandettini</surname>, <given-names>P.</given-names></string-name><article-title>Representational similarity analysis - connecting the branches of systems neuroscience. Frontiers in Systems</article-title><source>Neuroscience</source><volume>2</volume>, <fpage>4</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Britten</surname>, <given-names>K. H</given-names></string-name>. <etal>et al.</etal><article-title>A relationship between behavioral choice and the visual responses of neurons in macaque mt</article-title>. <source>Visual neuroscience</source><volume>13</volume>, <fpage>87</fpage>&#x2013;<lpage>100</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Treue</surname>, <given-names>S.</given-names></string-name> &#x0026; <string-name><surname>Trujillo</surname>, <given-names>J. C. M.</given-names></string-name><article-title>Feature-based attention influences motion processing gain in macaque visual cortex</article-title>. <source>Nature</source><volume>399</volume>, <fpage>575</fpage>&#x2013;<lpage>579</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Saproo</surname>, <given-names>S.</given-names></string-name> &#x0026; <string-name><surname>Serences</surname>, <given-names>J. T.</given-names></string-name><article-title>Attention improves transfer of motion information between V1 and MT</article-title>. <source>The Journal of Neuroscience</source><volume>34</volume>, <fpage>3586</fpage>&#x2013;<lpage>3596</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Andersen</surname>, <given-names>L. M.</given-names></string-name>, <string-name><surname>Pedersen</surname>, <given-names>M. N.</given-names></string-name>, <string-name><surname>Sandberg</surname>, <given-names>K.</given-names></string-name> &#x0026; <string-name><surname>Overgaard</surname>, <given-names>M.</given-names></string-name><article-title>Occipital MEG activity in the early time range (&#x003C; 300 ms) predicts graded changes in perceptual consciousness</article-title>. <source>Cerebral Cortex</source><volume>26</volume>, <fpage>2677</fpage>&#x2013;<lpage>2688</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>De Gardelle</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Charles</surname>, <given-names>L.</given-names></string-name> &#x0026; <string-name><surname>Kouider</surname>, <given-names>S.</given-names></string-name><article-title>Perceptual awareness and categorical representation of faces: Evidence from masked priming</article-title>. <source>Consciousness and cognition</source><volume>20</volume>, <fpage>1272</fpage>&#x2013;<lpage>1281</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Salti</surname>, <given-names>M</given-names></string-name>. <etal>et al.</etal><article-title>Distinct cortical codes and temporal dynamics for conscious and unconscious percepts</article-title>. <source>Elife</source><volume>4</volume>, <fpage>e05652</fpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>King</surname>, <given-names>J.-R.</given-names></string-name>, <string-name><surname>Pescetelli</surname>, <given-names>N.</given-names></string-name> &#x0026; <string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name><article-title>Brain mechanisms underlying the brief maintenance of seen and unseen sensory information</article-title>. <source>Neuron</source><volume>92</volume>, <fpage>1122</fpage>&#x2013;<lpage>1134</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Rao</surname>, <given-names>R. P.</given-names></string-name> &#x0026; <string-name><surname>Ballard</surname>, <given-names>D. H.</given-names></string-name><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title>. <source>Nature neuroscience</source><volume>2</volume>, <fpage>79</fpage>&#x2013;<lpage>87</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Ahissar</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>Hochstein</surname>, <given-names>S.</given-names></string-name><article-title>The reverse hierarchy theory of visual perceptual learning</article-title>. <source>Trends in cognitive sciences</source><volume>8</volume>, <fpage>457</fpage>&#x2013;<lpage>464</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Gilbert</surname>, <given-names>C. D.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>W.</given-names></string-name> &#x0026; <string-name><surname>Piech</surname>, <given-names>V.</given-names></string-name><article-title>Perceptual learning and adult cortical plasticity</article-title>. <source>The Journal of Physiology</source><volume>587</volume>, <fpage>2743</fpage>&#x2013;<lpage>2751</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Sasaki</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Nanez</surname>, <given-names>J. E.</given-names></string-name> &#x0026; <string-name><surname>Watanabe</surname>, <given-names>T.</given-names></string-name><article-title>Advances in visual perceptual learning and plasticity</article-title>. <source>Nature Reviews Neuroscience</source><volume>11</volume>, <fpage>53</fpage>&#x2013;<lpage>60</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Roelfsema</surname>, <given-names>P. R.</given-names></string-name>, <string-name><surname>Van Ooyen</surname>, <given-names>A.</given-names></string-name> &#x0026; <string-name><surname>Watanabe</surname>, <given-names>T.</given-names></string-name><article-title>Perceptual learning rules based on reinforcers and attention</article-title>. <source>Trends in cognitive sciences</source><volume>14</volume>, <fpage>64</fpage>&#x2013;<lpage>71</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Kelly</surname>, <given-names>S. P.</given-names></string-name> &#x0026; <string-name><surname>O&#x2019;Connell</surname>, <given-names>R. G.</given-names></string-name><article-title>Internal and external influences on the rate of sensory evidence accumulation in the human brain</article-title>. <source>The Journal of Neuroscience</source><volume>33</volume>, <fpage>19434</fpage>&#x2013;<lpage>19441</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Heekeren</surname>, <given-names>H. R.</given-names></string-name>, <string-name><surname>Marrett</surname>, <given-names>S.</given-names></string-name> &#x0026; <string-name><surname>Ungerleider</surname>, <given-names>L. G.</given-names></string-name><article-title>The neural systems that mediate human perceptual decision making</article-title>. <source>Nature reviews neuroscience</source><volume>9</volume>, <fpage>467</fpage>&#x2013;<lpage>479</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Philiastides</surname>, <given-names>M. G.</given-names></string-name> &#x0026; <string-name><surname>Sajda</surname>, <given-names>P.</given-names></string-name><article-title>Temporal characterization of the neural correlates of perceptual decision making in the human brain</article-title>. <source>Cerebral cortex</source><volume>16</volume>, <fpage>509</fpage>&#x2013;<lpage>518</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Philiastides</surname>, <given-names>M. G.</given-names></string-name>, <string-name><surname>Ratcliff</surname>, <given-names>R.</given-names></string-name> &#x0026; <string-name><surname>Sajda</surname>, <given-names>P.</given-names></string-name><article-title>Neural representation of task difficulty and decision making during perceptual categorization: a timing diagram</article-title>. <source>The Journal of Neuroscience</source><volume>26</volume>, <fpage>8965</fpage>&#x2013;<lpage>8975</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Ratcliff</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Philiastides</surname>, <given-names>M. G.</given-names></string-name> &#x0026; <string-name><surname>Sajda</surname>, <given-names>P.</given-names></string-name><article-title>Quality of evidence for perceptual decision making is indexed by trial-to-trial variability of the EEG</article-title>. <source>Proceedings of the National Academy of Sciences</source><volume>106</volume>, <fpage>6539</fpage>&#x2013;<lpage>6544</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>O&#x2019;Connell</surname>, <given-names>R. G.</given-names></string-name>, <string-name><surname>Dockree</surname>, <given-names>P. M.</given-names></string-name> &#x0026; <string-name><surname>Kelly</surname>, <given-names>S. P.</given-names></string-name><article-title>A supramodal accumulation-to-bound signal that determines perceptual decisions in humans</article-title>. <source>Nature neuroscience</source><volume>15</volume>, <fpage>1729</fpage>&#x2013;<lpage>1735</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><surname>Wyart</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>De Gardelle</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Scholl</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Summerfield</surname>, <given-names>C.</given-names></string-name><article-title>Rhythmic fluctuations in evidence accumulation during decision making in the human brain</article-title>. <source>Neuron</source><volume>76</volume>, <fpage>847</fpage>&#x2013;<lpage>858</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><string-name><surname>De Lange</surname>, <given-names>F. P.</given-names></string-name>, <string-name><surname>Rahnev</surname>, <given-names>D. A.</given-names></string-name>, <string-name><surname>Donner</surname>, <given-names>T. H.</given-names></string-name> &#x0026; <string-name><surname>Lau</surname>, <given-names>H.</given-names></string-name><article-title>Prestimulus oscillatory activity over motor cortex reflects perceptual expectations</article-title>. <source>The Journal of Neuroscience</source><volume>33</volume>, <fpage>1400</fpage>&#x2013;<lpage>1410</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><string-name><surname>Katz</surname>, <given-names>L. N.</given-names></string-name>, <string-name><surname>Yates</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Pillow</surname>, <given-names>J. W.</given-names></string-name> &#x0026; <string-name><surname>Huk</surname>, <given-names>A. C.</given-names></string-name><article-title>Dissociated functional significance of decision-related activity in the primate dorsal stream</article-title>. <source>Nature</source><volume>535</volume>, <fpage>285</fpage>&#x2013;<lpage>288</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><string-name><surname>Mazurek</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Roitman</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Ditterich</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Shadlen</surname>, <given-names>M. N.</given-names></string-name><article-title>A role for neural integrators in perceptual decision making</article-title>. <source>Cerebral cortex</source><volume>13</volume>, <fpage>1257</fpage>&#x2013;<lpage>1269</lpage> (<year>2003</year>).</mixed-citation></ref>
</ref-list>
<ack>
<title>Acknowledgements</title>
<p>This work was supported by an ERC-YStG-263584 and an ANR10JCJC-1904 to V.vW. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Preliminary results of this work was presented by V.vW. at the Synaesthesia in Perspective workshop (Hamburg, Germany, 2014), by Y. B. at Pattern Recognition in NeuroImaging (Tubingen,&#x00A8; Germany, 2014) and by A. G. at Biomag conference (Halifax, Canada, 2014).</p>
</ack>
<sec id="s5">
<title>Author contributions statement</title>
<p>N. Z., and V.vW. designed research; Y. B, A. G, N. Z., and V.vW. performed research; Y. B., A. G., and V.vW. contributed unpublished reagents/analytic tools; Y. B., A. G., and V.vW. analyzed data; Y. B., A. G., and V.vW. wrote the paper.</p>
</sec>
<sec sec-type="COI-statement">
<title>Additional information</title>
<p>No competing financial interests.</p>
</sec>
</back></article>