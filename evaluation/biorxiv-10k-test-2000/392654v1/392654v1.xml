<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/392654</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Real-time experimental control using network-based parallel processing</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7159-5134</contrib-id>
<name><surname>Kim</surname><given-names>Byounghoon</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Kenchappa</surname><given-names>Shobha</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Sunkara</surname><given-names>Adhira</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Chang</surname><given-names>Ting-Yu</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Thompson</surname><given-names>Lowell</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Doudlah</surname><given-names>Raymond</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8606-2987</contrib-id>
<name><surname>Rosenberg</surname><given-names>Ari</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Neuroscience, School of Medicine and Public Health, University of Wisconsin</institution> &#x2013; Madison, Madison, WI, 53705, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution>Department of Surgery, Stanford University School of Medicine</institution>, Stanford, CA. 94305, <country>USA</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1">Correspondence: Ari Rosenberg Department of Neuroscience School of Medicine and Public Health University of Wisconsin &#x2013; Madison 1111 Highland Ave. WIMR-II, Office 5505 Madison, WI. 53705 Email: <email>ari.rosenberg@wisc.edu</email></corresp>
<fn id="fn1" fn-type="conflict"><p><bold>Competing Interests:</bold> The authors have no competing financial or non-financial interests.</p></fn>
</author-notes>
<pub-date pub-type="epub"><year>2018</year></pub-date>
<elocation-id>392654</elocation-id>
<history>
<date date-type="received">
<day>15</day>
<month>8</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>15</day>
<month>8</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>16</day>
<month>8</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="392654.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Modern neuroscience research often requires the coordination of multiple processes such as stimulus generation, real-time experimental control, as well as behavioral and neural measurements. The technical demands required to simultaneously manage these processes with high temporal fidelity limits the number of labs capable of performing such work. Here we present an open-source network-based parallel processing framework that eliminates these barriers. The Real-Time Experimental Control with Graphical User Interface (REC-GUI) framework offers multiple advantages: (<italic>i</italic>) a modular design agnostic to coding language(s) and operating system(s) that maximizes experimental flexibility and minimizes researcher effort, (<italic>ii</italic>) simple interfacing to connect measurement and recording devices, (<italic>iii</italic>) high temporal fidelity by dividing task demands across CPUs, and (<italic>iv</italic>) real-time control using a fully customizable and intuitive GUI. Testing results demonstrate that the REC-GUI framework facilitates technically demanding, behavior-contingent neuroscience research. Sample code and hardware configurations are downloadable, and future developments will be regularly released.</p>
</abstract>
<counts>
<page-count count="21"/>
</counts>
</article-meta>
<ack>
<title>Acknowledgments</title>
<p>This work was supported by the Alfred P. Sloan Foundation, Whitehall Foundation Research Grant 2016-08-18, and National Institutes of Health Grants DC014305 and EY029438. Further support was provided by National Institutes of Health Grant P51OD011106 to the Wisconsin National Primate Research Center, University of Wisconsin &#x2013; Madison.</p>
</ack>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Many areas of cutting-edge neuroscience research require real-time experimental control contingent on behavioral and neuronal events, rendering and presentation of complex stimuli, and high-density measurements of neuronal activity. These processes must operate in parallel, and with high temporal resolution. The number of labs that can perform such research is limited by the high technical demands required to set up and maintain an appropriate experimental control system. In particular, a control system must balance the need to precisely coordinate different processes and the flexibility to implement new experimental designs with minimal effort. Systems favoring system precision over usability can hinder productivity because there is a large overhead to learning esoteric or low-level coding languages, and extensive coding demands slow the development of new paradigms. In contrast, systems favoring usability over precision can limit the complexity of supportable paradigms and the ability to perform experiments with high real-time computational demands. Here we present the Real-Time Experimental Control with Graphical User Interface (REC-GUI) framework, which overcomes technical challenges limiting previous solutions by using network-based parallel processing to provide both system precision and usability.</p>
<p>The REC-GUI framework segregates tasks into major groups such as experimental control and monitoring, and stimulus rendering and presentation. Each major group is executed on a different CPU, with communications between CPUs achieved using internet protocols. An additional CPU supports data acquisition and precise temporal alignment of multiple experimental processes. The REC-GUI framework aims to overcome technical challenges that hinder productivity and consume lab resources by providing a solution that works out of the box and reduces the time and effort required to implement experimental paradigms. We have therefore developed a version of the REC-GUI framework in which all processes are implemented with high-level programming environments: experimental control uses a GUI coded in Python, and stimulus rendering and presentation is performed with Psychtoolbox 3 in MATLAB (<xref ref-type="bibr" rid="c3">Brainard, 1997</xref>; <xref ref-type="bibr" rid="c13">Pelli, 1997</xref>; <xref ref-type="bibr" rid="c10">Kleiner et al., 2007</xref>). Psychtoolbox is widely used for its stimulus generation functions that eliminate the need for extensive knowledge of low-level coding languages. The framework is also inherently modular, so system components (e.g., MATLAB for stimulus presentation) can be easily modified or substituted to meet changing research needs. Because the REC-GUI framework achieves precise experimental control with high-level programming environments, it has a low barrier to conducting cutting-edge neuroscience experiments compared to other systems and does not require professional programmers or low-level coding languages. Our testing results confirm that the REC-GUI framework facilitates technically demanding experiments aimed at relating neuronal activity to perception and behavior.</p>
<p>Sample code and hardware configurations providing templates for adaptation and customization are available for download here: <ext-link ext-link-type="uri" xlink:href="https://rosenberg.neuro.wisc.edu/">https://rosenberg.neuro.wisc.edu/</ext-link>. Developments will be posted with notifications sent through a mailing list. The REC-GUI framework will free time to focus on experimental questions and design, help improve scientific reproducibility by increasing research transparency, and enable scientific advances by reducing technical barriers associated with complex neuroscience studies.</p>
</sec>
<sec id="s2">
<title>Materials and methods</title>
<sec id="s2a">
<title>Serial processing, multithreading, and network-based parallel processing alternatives for experimental control</title>
<p>Several approaches can be used to implement experimental control. Here we benchmark different alternatives and identify an option that can satisfy the joint needs of high temporal precision, experimental flexibility, and minimizing coding demands. To illustrate differences between approaches, consider the task of mapping the visual receptive field of a neuron in an awake behaving animal. To perform this task, the animal must hold its gaze on a fixation target presented on a screen. While the animal maintains fixation, the experimenter must control the movement of a visual stimulus such as a bar on the screen. The success of the mapping depends upon the animal maintaining accurate and precise gaze on the target. As such, if the animal breaks fixation, the visual stimulus must disappear until the animal reacquires fixation. The code required to implement this simple task includes three major processes that interface multiple hardware and software components: (<italic>i</italic>) eye position tracker with monitoring routines, (<italic>ii</italic>) visual display for stimulus presentation with functions to account for fixation status, and (iii) interactive experimental control for moving the stimulus, changing stimulus parameters, etc. in real time.</p>
<p>A <italic>serial processing framework</italic> executes these processes serially within a while-loop (<italic><bold><xref rid="fig1" ref-type="fig">Figure 1A</xref></bold></italic>). As a consequence of serial processing, every iteration of the loop has a pause in eye position monitoring while the stimulus-related and user-related processes finish. This delay can be problematic, especially if the stimulus rendering demands are high. For example, if rendering and presenting the stimulus takes longer than one cycle of the eye position monitoring process, cycles of eye position data will be lost, and the accuracy of the gaze-contingent control compromised. Using this problem as an illustrative example of the challenges that arise for an experimental control system, we next consider how the problem might be solved using a multithreading framework.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Alternative experimental control frameworks. (A) Serial processing: All processes are executed serially in a while-loop. (B) Multithreading: A single CPU executes multiple processes in parallel on different threads. (C) Network-based parallel processing: Multiple processes are executed in parallel on different CPUs coordinated over a network. Individual processes shown in CPU #2 can be implemented serially or using multithreading. Arrows specify the direction of information flow.</p></caption>
<graphic xlink:href="392654_fig1.tif"/>
</fig>
<p>A multithreading framework provides a solution to this problem by allowing the CPU to execute multiple processes concurrently (<italic><bold><xref rid="fig1" ref-type="fig">Figure 1B</xref></bold></italic>). Specifically, by separating the eye position monitoring, stimulus-related, and user-related processes onto independent threads, a CPU can execute the processes in parallel such that eye position monitoring can proceed without having to wait for the stimulus- and user-related processes to finish. However, some major coding environments, such as MATLAB, do not currently support multithreading for customized routines. This limitation in multithreading together with the high system demands of such environments can limit real-time experimental control capabilities. Furthermore, since all tasks are implemented on a single CPU, unresolvable system conflicts may arise if different hardware components are only compatible with certain operating systems.</p>
<p>Network-based parallel processing provides a versatile solution to this problem by dividing experimental tasks across multiple CPUs (<italic><bold><xref rid="fig1" ref-type="fig">Figure 1C</xref></bold></italic>). In particular, this allows tasks to be executed as parallel processes even if multithreading is not supported. Thus, one major benefit of the REC-GUI framework is that challenges arising from the lack of multithreading support in some high-level programming environments such as MATLAB can be resolved without sacrificing the development benefits of widely used software packages such as Psychtoolbox. This is particularly valuable if computationally demanding real-time stimulus rendering is required. A further benefit not possible with multithreading on a single CPU is that different task components can be implemented using different coding languages and on different operating systems. This feature is especially beneficial since cutting-edge research often requires multiple distinct system components. In the implementation of the REC-GUI framework described here, we highlight this versatility by using MATLAB to render and present stimuli with Psychtoolbox 3 on one CPU, and Python to run a GUI that implements experimental control and behavioral monitoring on a second CPU. With this setup, information about changes in fixation status and user-provided inputs to the GUI are relayed via a network packet to MATLAB which updates the stimulus accordingly. Importantly, this ensures that effectively no cycles of eye position data are lost since sending a network packet takes microseconds. More broadly, network-based parallel processing allows the REC-GUI framework to support a broad range of experimental preparations, as long as the system components support network interfacing.</p>
</sec>
<sec id="s2b">
<title>Overview of the REC-GUI framework</title>
<p>Experimental control is implemented in the REC-GUI framework using network-based parallel processing. In the implementation described here, experimental tasks are divided into two major groups: (<italic>i</italic>) experimental control and monitoring, and (<italic>ii</italic>) stimulus rendering and presentation (<italic><bold><xref rid="fig2" ref-type="fig">Figure 2</xref></bold></italic>). However, the number of components and how they are divided can be flexibly determined based on experimental needs. Different groups are executed on separate CPUs that communicate through internet protocols: user datagram protocol (UDP) and transmission control protocol (TCP). The choice of where to use UDP or TCP depends on the task demands. UDP is fast because it does not perform error-checking (processing continues without waiting for a return signal confirming if a data packet was received). TCP is slower than UDP, but extremely reliable for communication because it uses error-checking and temporally ordered data transmission.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Schematic of the Real-Time Experimental Control with Graphical User Interface (REC-GUI) framework. Experimental control and monitoring is achieved using a GUI that coordinates stimulus rendering/presentation, external devices for measuring behavior and delivering rewards, and data acquisition. Dashed lines show communication pathways for
using non-VPixx displays. Arrows specify the direction of information flow.</p></caption>
<graphic xlink:href="392654_fig2.tif"/>
</fig>
<p>For testing, we implemented a system to investigate the neural basis of three-dimensional (3D) visual perception in non-human primates (<xref ref-type="bibr" rid="c16">Rosenberg et al., 2013</xref>; <xref ref-type="bibr" rid="c14">Rosenberg and Angelaki, 2014a</xref>, b). The experiments require the stereoscopic display of stimuli rendered using multi-view geometry (<xref ref-type="bibr" rid="c5">Hartley and Zisserman, 2003</xref>), eye tracking for gaze-contingent stimulus presentation and detecting perceptual reports, reward delivery, and neuronal recordings. Experimental control and real-time monitoring are performed with a GUI coded in Python 2.7 and run on Linux (Ubuntu 14.04, Intel i3 Processor, 8 GB RAM, Intel HD500). Stimulus rendering is performed using Psychtoolbox 3 on MATLAB 2017a, and run on Linux (Ubuntu 16.04, Intel Xeon Processor, 24 GB RAM, NVIDIA GeForce GTX 970). Data acquisition is performed using a Scout Processor (Ripple, Inc.) run on Windows 10 (Intel Xeon Processor, 8 GB RAM, NVIDIA GeForce GT 720).</p>
<p>A detailed schematic of communication between the experimental control and stimulus CPUs during the behavioral task (described below) used for testing is shown in <italic><bold><xref rid="figS1" ref-type="fig">Figure 2#x2013;figure supplement 1</xref></bold></italic>. The experimental control CPU uses multithreading to run two major threads for: (<italic>i</italic>) monitoring and evaluating eye position, and (<italic>ii</italic>) updating experimental parameters in real-time based on inputs from the GUI. The stimulus CPU uses a MATLAB while-loop to render stimuli with parameters provided by the GUI, and to achieve gaze-contingent stimulus presentation by repeatedly querying the GUI through a UDP connection established in asynchronous mode so that the while-loop (execution flow) does not pause while waiting for data packets.</p>
</sec>
<sec id="s2c">
<title>Hardware components</title>
<p>Visual stimuli were rear-projected onto a screen at 240 Hz (120 Hz per eye) using a PROPixx projector (VPixx Technologies, Inc.) and a circular polarizer for stereoscopic presentation. This setup was used for all reported testing because it attains current state-of-the-art limits in 3D display capabilities. A major challenge that arises here is the real-time rendering and high frame rate presentation of 3D stimuli with large depth variations and occlusion without dropped frames or time lags, while enforcing gaze contingencies. Two other setups were used to confirm that the framework is robust to system changes. The second replaced the PROPixx with a VIEWPixx/3D display (VPixx Technologies, Inc.) operating at 120 Hz with active shutter glasses for stereoscopic presentation. The third used a 3D monitor (LG Electronics Inc.) and NVIDIA-2 3D Vision Kit operating at 120 Hz with active shutter glasses (run on Windows 10, Intel Xeon processor, 8 GB RAM, NVIDIA Quadro K4000 graphics card). Results from the two latter setups are not presented here because they confirm the more stringent testing results from the first setup.</p>
<p>The REC-GUI framework supports eye tracking using video or scleral search coil (<xref ref-type="bibr" rid="c7">Judge et al., 1980</xref>) methods (<italic><bold><xref rid="fig2" ref-type="fig">Figure 2</xref></bold></italic>). For the current study, we used video tracking with an EyeLink 1000 plus (SR-Research, Inc.). Binocular eye positions were sampled and digitized by EyeLink, and the measurements sent to the GUI through a TCP connection for real-time analysis. The same measurements were converted into an analog signal and transferred to the Scout Processor on its analog input channels for offline analysis. For the GUI to perform real-time analysis of eye movements measured with search coils, the analog outputs of the coil system would be sampled and digitized using an analog to digital converter (USB-1608G, Measurement Computing, Inc.). The same outputs would be transferred to the Scout Processor for offline analysis.</p>
<p>A Scout Processor (Ripple, Inc.) was used for the acquisition of electrophysiological data and storage of analog and digital signals from external devices. Other acquisition systems that support network interfacing can be substituted with minimal effort. Because timestamps are generated in the Scout Processor, no additional synchronization is required to temporally align input signals. As such, the Scout Processor serves as the main data storage unit, generating and storing a data file with synchronized behavioral and neural signals. The GUI also saves a file containing all data that it transmits and receives along with event codes signaling the occurrence of specific experimental events (e.g., fixation point on, stimulus on) and behavioral events (e.g., fixation acquired, choice made) on the experimental control CPU. Thus, the experimental control CPU provides a backup copy of certain data, and can serve as the main data server for studies that do not have large data demands requiring a standalone acquisition machine.</p>
</sec>
<sec id="s2d">
<title>System communications</title>
<p>System components communicate through four types of connections: UDP, TCP, analog signals, and transistor-to-transistor logic (TTL) using a digital input/output (DIO). UDP and TCP connections are achieved with network switches (<italic><bold><xref rid="fig2" ref-type="fig">Figure 2</xref></bold></italic>,<bold><xref rid="figS2" ref-type="fig">Figure 2&#x2013;figure supplement 2</xref></bold>), and used for communications between the experimental control CPU, stimulus CPU, EyeLink, and Scout Processor. In this implementation of the REC-GUI framework, two groups of digital event codes are carried over UDP and TCP connections: (<italic>i</italic>) stimulus-related (e.g., fixation target on/off, stimulus on/off, choice target on/off), and (<italic>ii</italic>) behavior-related (e.g., fixation acquired, fixation broken, saccade-to-choice target made). Stimulus-related event codes are triggered in MATLAB and sent to the GUI using UDP and the Scout Processor using TCP (<italic><bold><xref rid="fig2" ref-type="fig">Figure 2</xref></bold></italic>, <italic><bold><xref rid="fig2" ref-type="fig">Figure 2</xref>&#x2015;figure supplement 1</bold></italic>). Currently, there is no Python API for real-time communication between the GUI and Scout Processor. As such, all behavior-related events triggered by the experimental control CPU are sent to MATLAB, which relays event codes to the Scout Processor.</p>
<p>Digital communications are used for a control signal to deliver liquid rewards by sending a TTL pulse to a digital relay output board that opens a solenoid valve. The TTL pulse is either controlled by MATLAB and generated by a DataPixx (VPixx Technologies, Inc.) or controlled by the GUI and generated by a USB DIO interface (USB-1608G, Measurement Computing Inc.), depending on the setup (<italic><bold><xref rid="fig2" ref-type="fig">Figure 2</xref></bold></italic>).</p>
<p>Experimental systems often require specialized hardware purchased from multiple companies. Network-based communications with that hardware must often occur over non-configurable, predefined subgroups of IP addresses. A simple way to set up communication with such hardware is using multiple parallel networks such that each hardware piece has a single dedicated network switch. In this setup, both the EyeLink and Scout Processor have non-configurable, predefined subgroups of IP addresses that cannot be routed over the same network interface card (NIC). Consequently, two network switches and two NICs are required for both the stimulus and experimental control CPUs, with each NIC assigned to a different subgroup of IP addresses (<italic><bold><xref rid="fig2" ref-type="fig">Figure 2</xref>&#x2015;figure supplement 2</bold></italic>). With this configuration, both the stimulus and experimental control CPUs can directly communicate with the EyeLink and Scout Processor.</p>
</sec>
<sec id="s2e">
<title>Configurable experimental control GUI</title>
<p>Information about the ongoing status of an experiment and graphical feedback of critical data is often required to monitor and adjust parameters in real-time using a GUI. To accommodate different experimental paradigms, it is critical that a GUI provides a general method for easily adding/removing parameters from the control set and for manipulating parameters in real-time. To achieve these goals, the REC-GUI framework provides a highly flexible and intuitive user interface for real-time experimental control (<italic><bold><xref rid="fig3" ref-type="fig">Figure 3</xref></bold></italic>). The GUI displays continuously sampled measurements (e.g., eye position, animal location in an arena, arm position, membrane potential, firing rate, etc.) in the monitoring window. To visually evaluate contingencies, the monitoring window can also display boundary conditions which are used in determining if the measurements fall within a certain criterion range (e.g., if an animal&#x2019;s gaze is within a certain distance of a fixation target, or if an animal has entered a specific area of an arena). This functionality can also be used to trigger event signals (e.g., TCP or UDP packet, or TTL pulse) to control external devices such as a solenoid, pellet dropper, or neural stimulator. An experimenter can turn boundaries on/off, or change their size, number, locations, etc. in real-time through inputs in the GUI.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Graphical user interface provided with REC-GUI. The GUI has multiple control panels that are fully customizable. The upper left corner is a monitoring window, here showing a scaled depiction of the visual display. Fixation windows and eye position markers are seen at the center of the monitoring window. Eight choice windows for the 3D orientation discrimination task are also shown (correct choice in red). Below the monitoring window are eye configuration and receptive field mapping tools, which can be substituted for other experiment-specific tools. Task control for starting, pausing, or stopping a protocol is at the center top, along with subject-specific and system-specific configuration parameters. The sending panel in the upper right allows the experimenter to modify task parameters in real time. The receiving panel below that is used to display information about the current stimulus and experiment progress. The lower right panel shows the data log.</p></caption>
<graphic xlink:href="392654_fig3.tif"/>
</fig>
<p>Such changes are implemented in the REC-GUI framework using UDP communication to send/receive strings (<italic><bold><xref rid="fig2" ref-type="fig">Figure 2</xref></bold></italic>, <italic><bold><xref rid="fig2" ref-type="fig">Figure 2</xref>&#x2015;figure supplement 1</bold></italic>). Each string in the UDP packet consists an identifier (e.g., &#x2212;106; a number which the MATLAB code uniquely associates with a specific variable such as &#x2018;stimulus duration&#x2019;) and a value (e.g., 1 to specify a 1 s duration), followed by a terminator (/qqqq&#x2026;.q padded to 1,024 characters). In this example, the string would be &#x2212;106 1 /qqqq&#x2026;.q (see User Manual for details). The GUI contains separate panels for sending and receiving UDP packets between the stimulus and experimental control CPUs (<italic><bold><xref rid="fig3" ref-type="fig">Figure 3</xref></bold></italic>). From the sending panel, an experimenter enters values (e.g., 1) for predefined variables (e.g., stimulus duration) with associated identifiers (e.g., &#x2018;-106&#x2019;) and clicks &#x2018;Submit&#x2019; to send UDP packets to the stimulus CPU. In the current configuration, the sending panel sends control information to the stimulus CPU, but it can also send information to any other machine capable of receiving UDP packets, as required for an experiment. The receiving panel allows the experimenter to predefine identifiers/variables for receiving and displaying information from the stimulus or other CPU (<italic><bold><xref rid="fig3" ref-type="fig">Figure 3</xref></bold></italic>). In this way, ongoing experimental information can be monitored in real-time. The GUI also contains placeholders in the lower left corner for specialized tools. The default GUI contains interfaces to control eye calibration and receptive field mapping, but these can be easily substituted with other tools. This text-based approach provides a simple way to reconfigure the GUI to meet the demands of different experimental paradigms.</p>
</sec>
<sec id="s2f">
<title>Animal preparation, behavioral task, and neural recording</title>
<p>The functionality and performance of the REC-GUI framework was tested using a 3D visual orientation discrimination task performed by a rhesus monkey (<italic>Macaca mulatta</italic>). All surgeries and procedures were approved by the Institutional Animal Care and Use Committee at the University of Wisconsin&#x2013;Madison, and in accordance with NIH guidelines. A male rhesus monkey was surgically implanted with a lightweight Delrin ring for head restraint. At the time of the procedure, the animal was approximately 4.25 years of age and 7.2 kg in weight. After recovery, the animal was trained to fixate a visual target within 2&#x00B0; version and 1&#x00B0; vergence windows using standard operant conditioning techniques.</p>
<p>The animal was trained to perform an eight-alternative 3D orientation discrimination task (<italic><bold><xref rid="fig4" ref-type="fig">Figure 4</xref></bold></italic>). In the task, the animal viewed 3D oriented planar surfaces, and reported the direction of planar tilt with a saccadic eye movement to an appropriate choice target. Planar surfaces were presented at tilts ranging from 0&#x00B0; to 315&#x00B0; in 45&#x00B0; steps, and slants ranging from 15&#x00B0; to 60&#x00B0; in 15&#x00B0; steps. A frontoparallel plane (tilt undefined, slant &#x003D; 0&#x00B0;) was also presented. The surfaces were defined as random dot stereograms with perspective and stereoscopic cues (N &#x003D; 250 dots, each dot subtending &#x007E;0.3&#x00B0; of visual angle at the screen distance of 57 cm). Stimuli were centered on the fixation target. At the start of each trial, the animal fixated a target on an otherwise blank screen for 300 ms. The stimulus was then presented for 1,000 ms while fixation was maintained. The stimulus and fixation target then disappeared, and eight choice targets appeared at a radial distance of 11.5&#x00B0; with angular locations of 0&#x00B0; to 315&#x00B0; in 45&#x00B0; increments (corresponding to the possible planar tilts). The animal was rewarded with a drop of water or juice for choosing the target in the direction that the plane was closest to the animal. If fixation was broken before the appearance of the choice targets, the trial was aborted.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Behavioral task. The animal fixated a target (red) at the screen center for 300 ms. A planar surface was then presented for 1,000 ms (one eye&#x2019;s view is shown) while fixation was maintained. The plane and fixation target then disappeared, and eight choice targets corresponding to the possible planar tilts appeared. A liquid reward was provided for a saccade (yellow arrow) to the target in the direction that the plane was closest to the animal. Dot sizes are exaggerated and number reduced for clarity.</p></caption>
<graphic xlink:href="392654_fig4.tif"/>
</fig>
<p>We measured the 3D orientation tuning of neurons in the caudal intraparietal (CIP) area (<xref ref-type="bibr" rid="c16">Rosenberg et al., 2013</xref>; <xref ref-type="bibr" rid="c14">Rosenberg and Angelaki, 2014a</xref>, b) while the animal performed this behavioral task. This data was used to confirm the temporal alignment of stimulus presentation, behavioral performance, and neural data in the REC-GUI framework. A tungsten microelectrode (&#x007E;1M&#x2126;; FHC, Inc.) was targeted to CIP using magnetic resonance imaging scans. The CARET software was used to segment visual areas, and CIP was identified as the lateral occipitoparietal zone (<xref ref-type="bibr" rid="c17">Van Essen et al., 2001</xref>; <xref ref-type="bibr" rid="c16">Rosenberg et al., 2013</xref>). A recording grid for guiding electrode penetrations was aligned with the brain scan in stereotaxic coordinates using ear bar and grid markers (<xref ref-type="bibr" rid="c11">Laurens et al., 2016</xref>). Neuronal responses were sampled and digitized at 30 Khz using the Scout Processor. Single-neuron action potentials were identified by waveform (voltage-time profile) using Offline Sorter (Plexon, Inc.). All subsequent analyses were performed in MATLAB.</p>
</sec>
</sec>
<sec id="s3">
<title>RESULTS</title>
<sec id="s3a">
<title>Network and system performance tests</title>
<p>The REC-GUI framework uses network-based parallel processing to implement experimental control and synchronize multiple experimental devices. In this section, we test the latency of communication between the stimulus and experimental control CPUs resulting from hardware and software processing, as well as the performance of the main loop responsible for stimulus rendering and presentation. Except where otherwise noted, testing was performed with the 3D orientation discrimination task described in the Materials and methods.</p>
<p>Since different experimental processes are implemented on independent CPUs that communicate over a network, it is possible that limitations in network capacity can introduce delays that adversely affect system performance. To test this possibility, we measured the delay resulting from the network hardware. Network latency was measured using a simple &#x2018;ping&#x2019; command that is used to test reachability, signal fidelity, and latency in network communication between two hosts (<xref ref-type="bibr" rid="c1">Abdou et al., 2017</xref>). This ping sends an internet control message protocol requesting an echo reply from the target host. The latency of the ping is the round-trip duration of the packets between the two hosts. Network latency between the experimental control and stimulus CPUs was found to be negligibly small (average latency &#x003D; 24 &#x00B5;s; standard deviation: &#x03C3; &#x003D; 5.1 &#x00B5;s; N &#x003D; 1,000 pings), indicating that the network hardware did not introduce substantial delays that could adversely affect performance (<italic><bold><xref rid="fig5" ref-type="fig">Figure 5A</xref></bold></italic>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><p>System performance. (A) Ping latency of the network hardware (N &#x003D;1,000 pings). (B) Overall system performance measured using the round-trip latency of UDP packets between the experimental control (GUI) and stimulus (MATLAB) CPUs during the 3D orientation discrimination task (N &#x003D;500 round-trip packet pairs). (C) Duration of the main while-loop in the stimulus (MATLAB) CPU for rendering/presenting stimuli (N &#x003D;3,000 iterations). Vertical gray dotted lines mark mean durations.</p></caption>
<graphic xlink:href="392654_fig5.tif"/>
</fig>
<p>Next we measured the overall system performance which includes hardware delays tested above as well as software delays from the main loops (i.e., including all processes executed) on both the CPUs. In this test, the GUI sends a UDP packet to the stimulus CPU and after that packet is detected, MATLAB returns another UDP packet. We measured the latency of multiple round-trip sent/received packets (GUI &#x2192; MATLAB &#x2192; GUI), and the distribution of measured latencies is shown in <italic><bold><xref rid="fig5" ref-type="fig">Figure 5B</xref></bold></italic>. On average, the total duration of the round-trip packets during the 3D orientation discrimination task was 6.79 ms (&#x03C3; &#x003D; 2.9 ms; N &#x003D; 500). This latency determines the time interval required to synchronize the two CPUs during this computationally intensive task. For many experiments, system performance would likely exceed this level since stimulus preparation in this task was computationally demanding. To determine an upper-bound of system performance with the described REC-GUI implementation, we removed all stimulus-related processes from the MATLAB main while-loop and measured the round-trip latency of the UDP packets. In this case, the round-trip latency dropped to an average of 4.3 ms (&#x03C3; &#x003D; 2.3 ms; N &#x003D; 500). Note that this latency only limits the real-time control of stimulus parameters, and does not limit the precision of the temporal alignment that can be achieved offline, as demonstrated in the neural recording tests described below. Moreover, even with the longer latencies that occurred when rendering complex 3D stimuli, the achieved millisecond-level of control synchronization is sufficient for most experiments.</p>
<p>Lastly, we measured the duration of the main-loop in the stimulus CPU, excluding the UDP processing time for communication with the experimental control CPU. Thus, this test includes all serially executed processes associated with stimulus rendering and presentation, as well as conditional statements for controlling experiment flow. During the 3D orientation discrimination task, the duration of the main-loop was on average 3.39 ms (&#x03C3; &#x003D; 1.3 ms; N &#x003D; 3,000 iterations; <italic><bold><xref rid="fig5" ref-type="fig">Figure 5C</xref></bold></italic>), thus accounting for approximately half of the 6.79 ms required to synchronize the two CPUs. To determine a lower-bound duration of the MATLAB main-loop with the same setup, we removed the stimulus-related routines. After removing this code, the latency of the main-loop dropped to an average of 1.7 ms (&#x03C3; &#x003D; 0.11 ms; N &#x003D; 3,000 iterations). Together, these tests show that the performance of the REC-GUI framework can facilitate complex experimental tasks in real-time, with very low latencies using simple, high-level programming environments.</p>
</sec>
<sec id="s3b">
<title>Stimulus presentation tests</title>
<p>A critical component of experimental studies is the ability to accurately and precisely present stimuli. To evaluate the ability of the REC-GUI framework to present demanding visual stimuli, our tests used large, computationally intensive stereoscopic stimuli presented at 240 Hz (see Materials and methods). The stimuli were rendered as separate right and left eye &#x2018;half-images&#x2019; in MATLAB with Psychtoolbox 3, and the &#x2018;flip&#x2019; command (<xref ref-type="bibr" rid="c10">Kleiner et al., 2007</xref>) was used to alternately present the appropriate image to the right or left eye (120 Hz per eye) for 1 second. To account for the temporal difference between the projector&#x2019;s refresh rate and the execution rate of the MATLAB script, the flip command was set to wait until the next available cycle of the projector refresh. This setting minimizes the variability in the delay between the flip command and the appearance of the stimulus. To assess the fidelity of the presentation, phototransistor circuits (diagram in the User Manual) were used to track the appearance of stimuli on the screen by detecting a small bright patch in the lower right/left corner of the corresponding (right/left) eye half-images.</p>
<p>The Scout Processor saves the voltage traces generated by the phototransistor circuits to provide a precise signal for aligning events to the stimulus as well as to confirm the fidelity of the stereoscopic presentation on a trial-by-trial basis (e.g., if dropped frames occur on a certain trial, this can be detected and the trial discarded). Example traces showing the presentation of right and left eye images are shown in <italic><bold><xref rid="fig6" ref-type="fig">Figure 6A</xref></bold></italic> (blue and orange traces, respectively). The latency between the initial flip command and the appearance of the stimulus was approximately one cycle of the video refresh (average delay &#x003D; 4.71 ms, &#x03C3; &#x003D; 0.16 ms, N &#x003D; 500 trials; <italic><bold><xref rid="fig6" ref-type="fig">Figure 6B</xref></bold></italic>). The fidelity of alternating right and left eye frames was confirmed by assessing the time lag between the two voltage traces. Since the stereoscopic images were presented at 240 Hz (120 Hz per eye), the right and left eye frame signals should be temporally shifted by &#x007E;4.17 ms. We measured the timing difference between each alternation of the right and left eye frames over the 1 s stimulus duration for 500 trials. The histogram of timing differences shows a strong peak at 4.16 ms (minimum &#x003D; 3.9 ms; maximum &#x003D; 4.4 ms; &#x03C3; &#x003D; 0.066 ms), indicating that the right and left eye frames were well synchronized at the intended 240 Hz stimulus presentation rate (<italic><bold><xref rid="fig6" ref-type="fig">Figure 6C</xref></bold></italic>). This histogram also confirms that no frames were dropped over the cumulative 500 s of stimulus presentation (dropped frames would appear as timing differences &#x2265; 12.5 ms).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><p>Quantification of the fidelity of stimulus presentation. (A) Right and left eye frame signals measured on the screen. The anti-phase rise and fall of the two signals indicates that the right and left eye signals are temporally synchronized. (B) Latency between the initial flip command in MATLAB and the appearance of the stimulus (N &#x003D; 500 trials). Approximately one frame occurs between the flip command and stimulus appearance. (C) Histogram of timing differences between the two eyes&#x2019; frames peaks at 4.16 ms indicating that 240 Hz (120 Hz per eye) stimulus presentation is reliably achieved (data from 500 trials). Voltage traces were sampled at 30 kHz. Mean times are indicated by vertical gray dotted lines.</p></caption>
<graphic xlink:href="392654_fig6.tif"/>
</fig>
</sec>
<sec id="s3c">
<title>Real-time experimental control</title>
<p>Real-time monitoring to guide the control of ongoing processes is critical for many experimental studies. We evaluated this capability of the REC-GUI framework using gaze-contingent stimulus presentation. Right and left eye positions were sampled at 1kHz using an EyeLink 1000 plus (SR-Research Inc.). The eye position data were fed to the GUI which implements routines for evaluating if the animal is holding fixation on the target, if fixation is broken, or if a particular choice is made in the 3D orientation discrimination task. Depending on the results of these routines, the experimental control CPU then directs the stimulus CPU to enter particular experimental stages (e.g., fixation only, stimulus presentation, choice targets, etc.).</p>
<p>We first confirmed that the GUI successfully enforced version and vergence eye position during a fixation task. Fixation targets were presented at three distances relative to the viewing screen, and the task required that fixation be maintained for 1 s. The top row of <italic><bold><xref rid="fig7" ref-type="fig">Figure 7A</xref></bold></italic> shows right and left eye version enforcement windows along with eye traces for 4 representative successfully completed trials at each distance. The bottom row of <italic><bold><xref rid="fig7" ref-type="fig">Figure 7A</xref></bold></italic> shows the vergence errors for the same trials. Second, we confirmed the successful enforcement of gaze relative to the fixation target during the 3D orientation discrimination task. <italic><bold><xref rid="fig7" ref-type="fig">Figure 7B</xref></bold></italic> shows horizontal and vertical eye displacements for each eye for 34 representative successfully completed trials. Lastly, <italic><bold><xref rid="fig7" ref-type="fig">Figure 7C</xref></bold></italic> shows the full eye traces for the same 34 trials, showing that saccadic eye movements to each of the eight choice targets were accurately detected.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><p>Verifying behavior-contingent stimulus presentation. (A) Enforcement of version and vergence during fixation at different distances. Top row shows left and right eye traces while fixation was held at three different distances using stereoscopically rendered targets (N &#x003D;4 trials/distance). Circular windows show 2&#x00B0; version enforcement windows. Bottom row shows simultaneously measured vergence error traces. Gray dotted lines show a 1&#x00B0; vergence enforcement window. (B) Right and left eye traces during the 3D orientation discrimination task, aligned to the stimulus onset (N &#x003D;34 trials). The horizontal (vertical) component of the eye movements are shown in purple (red). (C) Eye movement traces showing the choice target selection (same data as in B).</p></caption>
<graphic xlink:href="392654_fig7.tif"/>
</fig>
</sec>
<sec id="s3d">
<title>Temporal alignment of neural data to stimulus-related and behavioral events</title>
<p>To confirm the ability to precisely align events in time, we measured the 3D surface orientation tuning of a CIP neuron while the animal performed the 3D orientation discrimination task. First, we confirmed the ability to precisely align stimulus-driven neuronal responses to the stimulus onset detected by the phototransistor. A raster plot showing the timing of individual action potentials for 545 trials aligned to the stimulus onset (each row shows a different trial) along with the spike density function (red curve; convolution with a Gaussian function of &#x03C3; &#x003D; 10 ms) (<xref ref-type="bibr" rid="c12">MacPherson and Aldridge, 1979</xref>) is shown in <italic><bold><xref rid="fig8" ref-type="fig">Figure 8A</xref></bold></italic>. The precise alignment of spike times relative to the stimulus onset reveals a sharp visual transient in the spike density function. Second,</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8.</label>
<caption><p>Temporal alignment of neuronal responses to stimulus-related and behavioral events. (A) Raster plot showing the timing of action potentials aligned to the stimulus onset (N &#x003D; 545 trials). Shaded region marks the stimulus duration. (B) Raster plot showing the timing of action potentials aligned to the saccade onset. Each row is a different trial, and each dot marks a single action potential. Red curves are spike density functions. (C) 3D orientation tuning. Each curve shows tilt tuning at a fixed slant.</p></caption>
<graphic xlink:href="392654_fig8.tif"/>
</fig>
<p>We confirmed the ability to precisely align neuronal responses to the measured choice saccades. Saccade onsets were detected offline as the first time point at which the eye movement was faster than 150&#x00B0;/s (<xref ref-type="bibr" rid="c8">Kim and Basso, 2008</xref>, <xref ref-type="bibr" rid="c9">2010</xref>). Spike times were aligned to the saccade onset and the spike density function calculated (<italic><bold><xref rid="fig8" ref-type="fig">Figure 8B</xref></bold></italic>). Note the build-up of neuronal activity preceding the saccade that was not evident when the responses were aligned to the stimulus onset. As expected in CIP (<xref ref-type="bibr" rid="c16">Rosenberg et al., 2013</xref>), the neuron was jointly tuned for slant and tilt (<italic><bold><xref rid="fig8" ref-type="fig">Figure 8C</xref></bold></italic>).</p>
</sec>
</sec>
<sec id="s4">
<title>DISCUSSION</title>
<p>Test results confirm that the REC-GUI framework provides an accurate and precise solution for implementing demanding neuroscience studies with millisecond-level control. By achieving robust experimental control with high-level programming environments, technical challenges that hinder labs from conducting complex, behaviorally relevant research can be overcome without the need for low-level programing languages or professional programmers. Beyond more traditional experiments with a fixed trial structure, the REC-GUI framework capabilities can support research involving the use of naturalistic, complex stimuli that are dynamically updated based on real-time behavioral or neuronal measurements. For example, the system can support closed-loop experiments in which multisensory visual&#x2013;vestibular stimuli are updated based on active steering behaviors rather than passive, predefined motion profiles. Alternatively, stimuli can be updated or neural activity perturbed through electrical or magnetic stimulation with very short latencies triggered by real-time behavioral or neuronal measurements. Such experiments will be critical to understanding the relationship between the dynamic activity of neural populations, perception, and action during natural behaviors. The REC-GUI framework can facilitate such research by reducing the overhead associated with parlaying the necessary technology into a cohesive experimental system. By reducing technical hurdles and providing a flexible experimental framework, complex experimental protocols will be accessible to a greater number of labs, and the opportunity for scientific discovery increased.</p>
<p>Towards this goal, the REC-GUI framework offers several advantages compared to other control systems. First, network-based parallel processing makes the framework inherently modular and highly flexible. Since the only constraint on incorporating specialized software or hardware is that network interfacing is supported, a broad range of devices for stimulus presentation (visual displays, speakers, pellet droppers, etc.), behavioral measurement (button presses, eye movements, biometrics, location, etc.), and other experimental needs (stimulator, osmotic pump, etc.) can be used out of the box. Such flexibility allows the framework to be adapted to a broad range of experimental preparations (<italic>in vitro</italic>, anesthetized, or awake-behaving) and neuroscience research domains (sensory, cognitive, motor, etc.), and makes it agnostic to the type of neural data recorded (electrophysiological, optical, magnetic resonance imaging, etc.). For instance, we also use the REC-GUI framework to conduct perceptual learning studies with adolescents with autism. Second, dividing computing demands across CPUs improves system performance and enables researchers to implement system components using different coding languages and operating systems. This reduces compatibility issues and increases the efficiency with which experimental setups are configured. Likewise, while the performance of the REC-GUI implementation presented here is sufficient for most experimental needs, if greater precision is required, this can be achieved using a lower-level programming language such as C or C&#x002B;&#x002B; for stimulus rendering and presentation. Third, diverse data types can be collected in parallel at different temporal resolutions, and precisely aligned, allowing for multi-faceted research. Lastly, a user-friendly, customizable GUI allows for intuitive and flexible experimental design changes.</p>
<p>A number of other control systems exist, so we briefly review some of them and benchmark the REC-GUI framework with them to help facilitate system selection. The Laboratory of Sensorimotor Research (LSR) real-time software suite from the National Eye Institute provides a full package including a user interface, visual stimulus rendering, data acquisition, and offline data analysis tools (<xref ref-type="bibr" rid="c6">Hays et al., 1982</xref>). Similar to REC-GUI, the LSR suite divides experimental control, stimulus processing, and data acquisition across CPUs. The LSR suite is highly powerful, and can meet the demands of many complex behavioral paradigms. However, the LSR scripting language is relatively complex, making the overhead of learning the system and developing new experiments quite high. By using high-level programming environments, the REC-GUI framework aims to reduce this overhead, particularly as it relates to modifying or adding new experimental tasks. Additionally, the LSR suite is specialized for visuomotor studies, whereas the REC-GUI framework is agnostic to the subdomain of neuroscience research.</p>
<p>A freely available system that is fully implemented in MATLAB is MonkeyLogic (<xref ref-type="bibr" rid="c2">Asaad et al., 2013</xref>). The system achieves millisecond-level temporal resolution, and provides a user interface with real-time behavioral monitoring. In addition, it is convenient to implement control flows for new behavioral paradigms. MonkeyLogic is designed for a single CPU, so experimental control is performed serially due to MATLAB&#x2019;s multithreading limitations. This can be problematic for real-time control when the stimulus rendering/presentation demands are high. For example, since stimuli are transferred to the video buffer during the inter-trial interval without compression, it may not be suitable for presenting long-duration stimuli at high frame rates. For instance, using MonkeyLogic for the experiment implemented here would result in long inter-trial intervals to transfer the stimuli to the video card, dropped frames, and presentation lags. Along this line, the MonkeyLogic forum indicates that it cannot support 240 Hz visual stimulus presentation (<ext-link ext-link-type="uri" xlink:href="http://forums.monkeylogic.org/post/high-refresh-rates-vpixx-8408242">http://forums.monkeylogic.org/post/high-refresh-rates-vpixx-8408242</ext-link>). Similar limitations will exist for other single CPU, MATLAB-based control systems (e.g., PLDAPS), though some limitations may be at least partially remediated if the real-time monitoring and control features provided by a GUI are eliminated (<xref ref-type="bibr" rid="c4">Eastman and Huk, 2012</xref>). Such systems may be ideal for tasks that do not have high real-time behavioral contingency and stimulus rendering/presentation demands, since network communications make the REC-GUI framework slightly more complex. Additionally, there is some added cost to setting up the REC-GUI framework compared to single CPU systems since it requires multiple CPUs, but that cost difference is relatively small. In exchange, the REC-GUI framework allows high-level programming environments to be used to robustly control computationally demanding and behaviorally complex neuroscience experiments.</p>
<p>To facilitate customization and future developments, we provide sample MATLAB scripts and Python GUI code (<ext-link ext-link-type="uri" xlink:href="https://rosenberg.neuro.wisc.edu/">https://rosenberg.neuro.wisc.edu/</ext-link>). We will maintain the REC-GUI framework as an open-source project, and welcome development contributions from others. Our hope is that this will help researchers perform multi-faceted research combining physiology and behavior by reducing time spent solving technical problems, and increasing time focused on experimental questions and design. The REC-GUI framework will also help promote research transparency, standardize data acquisition, and improve reproducibility by facilitating cheap and easy replication of experimental paradigms.</p>
</sec>
</body>
<back>

<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="other"><string-name><surname>Abdou</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Matrawy</surname> <given-names>A</given-names></string-name>, <string-name><surname>van Oorschot</surname> <given-names>PC</given-names></string-name> (<year>2017</year>) <article-title>Accurate manipulation of delay-based internet geolocation. In: Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security</article-title>, pp <fpage>887</fpage>&#x2013;<lpage>898</lpage>. <source>Abu Dhabi, United Arab Emirates</source>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Asaad</surname> <given-names>WF</given-names></string-name>, <string-name><surname>Santhanam</surname> <given-names>N</given-names></string-name>, <string-name><surname>McClellan</surname> <given-names>S</given-names></string-name>, <string-name><surname>Freedman</surname> <given-names>DJ</given-names></string-name> (<year>2013</year>) <article-title>High-performance execution of psychophysical tasks with complex visual stimuli in MATLAB</article-title>. <source>J Neurophysiol</source> <volume>109</volume>:<fpage>249</fpage>&#x2013;<lpage>260</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Brainard</surname> <given-names>DH</given-names></string-name> (<year>1997</year>) <article-title>The Psychophysics Toolbox</article-title>. <source>Spat Vis</source> <volume>10</volume>:<fpage>433</fpage>&#x2013;<lpage>436</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Eastman</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Huk</surname> <given-names>AC</given-names></string-name> (<year>2012</year>) <article-title>PLDAPS: A Hardware Architecture and Software Toolbox for Neurophysiology Requiring Complex Visual Stimuli and Online Behavioral Control</article-title>. <source>Front Neuroinform</source> <volume>6</volume>:<fpage>1</fpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="book"><string-name><surname>Hartley</surname> <given-names>R</given-names></string-name>, <string-name><surname>Zisserman</surname> <given-names>A</given-names></string-name> (<year>2003</year>) <source>Multiple view geometry in computer vision</source>: <publisher-name>Cambridge university press</publisher-name>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="other"><string-name><surname>Hays</surname> <given-names>AV</given-names></string-name>, <string-name><surname>Richmond</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Optican</surname> <given-names>LM</given-names></string-name> (<year>1982</year>) <article-title>A UNIX-based multiple process system for real-time data acquisition and control</article-title>. In: <source>WESCON</source> pp <fpage>1</fpage>&#x2013;<lpage>10</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Judge</surname> <given-names>SJ</given-names></string-name>, <string-name><surname>Richmond</surname> <given-names>BJ</given-names></string-name> <string-name><surname>Chu</surname> <given-names>FC</given-names></string-name> (<year>1980</year>) <article-title>Implantation of magnetic search coils for measurement of eye position: an improved method</article-title>. <source>Vision Res</source> <volume>20</volume>:<fpage>535</fpage>&#x2013;<lpage>538</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Kim</surname> <given-names>B</given-names></string-name>, <string-name><surname>Basso</surname> <given-names>MA</given-names></string-name> (<year>2008</year>) <article-title>Saccade target selection in the superior colliculus: a signal detection theory approach</article-title>. <source>J Neurosci</source> <volume>28</volume>:<fpage>2991</fpage>&#x2013;<lpage>3007</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Kim</surname> <given-names>B</given-names></string-name>, <string-name><surname>Basso</surname> <given-names>MA</given-names></string-name> (<year>2010</year>) <article-title>A probabilistic strategy for understanding action selection</article-title>. <source>J Neurosci</source> <volume>30</volume>:<fpage>2340</fpage>&#x2013;<lpage>2355</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Kleiner</surname> <given-names>M</given-names></string-name>, <string-name><surname>Brainard</surname> <given-names>D</given-names></string-name>, <string-name><surname>Pelli</surname> <given-names>D</given-names></string-name>, <string-name><surname>Ingling</surname> <given-names>A</given-names></string-name>, <string-name><surname>Murray</surname> <given-names>R</given-names></string-name>, <string-name><surname>Broussard</surname> <given-names>C</given-names></string-name> (<year>2007</year>) <article-title>What&#x2019;s new in Psychtoolbox-3</article-title>. <source>Perception</source> <volume>36</volume>:<fpage>1</fpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Laurens</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>B</given-names></string-name>, <string-name><surname>Dickman</surname> <given-names>JD</given-names></string-name> <string-name><surname>Angelaki</surname> <given-names>DE</given-names></string-name> (<year>2016</year>) <article-title>Gravity orientation tuning in macaque anterior thalamus</article-title>. <source>Nat Neurosci</source> <volume>19</volume>:<fpage>1566</fpage>&#x2013;<lpage>1568</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>MacPherson</surname> <given-names>JM</given-names></string-name> <string-name><surname>Aldridge</surname> <given-names>JW</given-names></string-name> (<year>1979</year>) <article-title>A quantitative method of computer analysis of spike train data collected from behaving animals</article-title>. <source>Brain Res</source> <volume>175</volume>:<fpage>183</fpage>&#x2013;<lpage>187</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Pelli</surname> <given-names>DG</given-names></string-name> (<year>1997</year>) <article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title>. <source>Spat Vis</source> <volume>10</volume>:<fpage>437</fpage>&#x2013;<lpage>442</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Rosenberg</surname> <given-names>A</given-names></string-name>, <string-name><surname>Angelaki</surname> <given-names>DE</given-names></string-name> (<year>2014</year>a) <article-title>Gravity influences the visual representation of object tilt in parietal cortex</article-title>. <source>J Neurosci</source> <volume>34</volume>:<fpage>14170</fpage>&#x2013;<lpage>14180</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Rosenberg</surname> <given-names>A</given-names></string-name>, <string-name><surname>Angelaki</surname> <given-names>DE</given-names></string-name> (<year>2014</year>b) <article-title>Reliability-dependent contributions of visual orientation cues in parietal cortex</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>111</volume>:<fpage>18043</fpage>&#x2013;<lpage>18048</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Rosenberg</surname> <given-names>A</given-names></string-name>, <string-name><surname>Cowan</surname> <given-names>NJ</given-names></string-name> <string-name><surname>Angelaki</surname> <given-names>DE</given-names></string-name> (<year>2013</year>) <article-title>The visual representation of 3D object orientation in parietal cortex</article-title>. <source>J Neurosci</source> <volume>33</volume>:<fpage>19352</fpage>&#x2013;<lpage>19361</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Van Essen</surname> <given-names>DC</given-names></string-name>, <string-name><surname>Lewis</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Drury</surname> <given-names>HA</given-names></string-name>, <string-name><surname>Hadjikhani</surname> <given-names>N</given-names></string-name>, <string-name><surname>Tootell</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Bakircioglu</surname> <given-names>M</given-names></string-name> <string-name><surname>Miller</surname> <given-names>MI</given-names></string-name> (<year>2001</year>) <article-title>Mapping visual cortex in monkeys and humans using surface-based atlases</article-title>. <source>Vision Res</source> <volume>41</volume>:<fpage>1359</fpage>&#x2013;<lpage>1378</lpage>.</mixed-citation></ref>
</ref-list>
<sec>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure 2 &#x2013; figure supplement 1.</label>
<caption><p>Communication flowchart between the stimulus CPU (MATLAB) and experimental control CPU (Python) showing the exchange of experimental parameters for stimulus rendering and behavioral control. Arrows specify the direction of information flow.</p></caption>
<graphic xlink:href="392654_figS1.tif"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure 2 &#x2013; figure supplement 2.</label>
<caption><p>Network configuration. The Scout Processor and EyeLink have non-configurable IP addresses, so two network switches are required to route two different subgroups of IP addresses to the stimulus and experimental control CPUs. Arrows specify the direction of information flow.</p></caption>
<graphic xlink:href="392654_figS2.tif"/>
</fig>
</sec>
</back>
</article>