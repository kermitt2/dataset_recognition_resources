<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/231381</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Feature-specific awake reactivation in human V1 after visual training</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9739-3474</contrib-id>
<name><surname>Bang</surname><given-names>Ji Won</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Sasaki</surname><given-names>Yuka</given-names></name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4562-5376</contrib-id>
<name><surname>Watanabe</surname><given-names>Takeo</given-names></name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5265-2559</contrib-id>
<name><surname>Rahnev</surname><given-names>Dobromir</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Psychology, Georgia Institute of Technology</institution>, Atlanta, GA, 30318, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution>Department of Ophthalmology, School of Medicine, New York University</institution>, New York, NY, 10016, <country>USA</country></aff>
<aff id="a3"><label>3</label><institution>Department of Cognitive, Linguistic, and Psychological Sciences, Brown University</institution>, Providence, RI, 02912, <country>USA</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label><bold>Correspondence:</bold> Ji Won Bang, New York University, 455 First Avenue New York, NY 10016, <email>JiWon.Bang@nyumc.org</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2017</year>
</pub-date><elocation-id>231381</elocation-id>
<history>
<date date-type="received">
<day>08</day>
<month>12</month>
<year>2017</year>
</date>
<date date-type="rev-recd">
<day>08</day>
<month>12</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>08</day>
<month>12</month>
<year>2017</year>
</date>
</history><permissions>
<copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="231381.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract><title>Abstract</title>
<p>Converging human studies have demonstrated that brain activity patterns observed during task performance reemerge in the following restful awake state. Such &#x201C;awake reactivation&#x201D; has been demonstrated across higher-order cortex for complex images or associations. However, it remains unclear what specific training components are reactivated in these studies. Here we sought to provide evidence for the reactivation of a particular visual feature &#x2013; Gabor orientation. Following extensive training on a visual task, we found robust reactivation in human V1 that lasted at least eight minutes. This effect was not present in higher retinotopic areas such as V2, V3, V3A, or V4v, demonstrating that the effects in V1 are not due to top-down processes such as conscious rehearsal. Furthermore, the amount of awake reactivation predicted the amount of performance improvement on the visual task. These results demonstrate that functionally-relevant awake reactivation of specific visual features occurs in early sensory cortex.</p>
</abstract>
<kwd-group kwd-group-type="author"><title>Keywords</title>
<kwd>Reactivation</kwd>
<kwd>awake reactivation</kwd>
<kwd>learning</kwd>
<kwd>V1</kwd>
<kwd>fMRI</kwd>
</kwd-group>
<counts>
<page-count count="38"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1"><title>Introduction</title>
<p>Understanding how the human brain learns is one of the central goals in neuroscience. A growing body of literature demonstrates that new learning becomes consolidated during offline states (<xref ref-type="bibr" rid="c9">Diekelmann &#x0026; Born, 2010</xref>; <xref ref-type="bibr" rid="c19">McGaugh, 2000</xref>; <xref ref-type="bibr" rid="c22">Sasaki, Nanez, &#x0026; Watanabe, 2010</xref>). More specifically, recent functional magnetic resonance imaging (fMRI) studies in humans point to memory reactivation &#x2013; the reemergence of the brain activity patterns observed during task performance &#x2013; as a potential contributor to offline memory consolidation (<xref ref-type="bibr" rid="c7">Deuker et al., 2013</xref>; <xref ref-type="bibr" rid="c23">Schlichting &#x0026; Preston, 2014</xref>; <xref ref-type="bibr" rid="c26">Staresina, Alink, Kriegeskorte, &#x0026; Henson, 2013</xref>; <xref ref-type="bibr" rid="c27">Tambini &#x0026; Davachi, 2013</xref>). This reactivation occurs during sleep (<xref ref-type="bibr" rid="c7">Deuker et al., 2013</xref>) but also during restful wakefulness (<xref ref-type="bibr" rid="c4">Chelaru et al., 2016</xref>; <xref ref-type="bibr" rid="c6">de Voogd, Fernandez, &#x0026; Hermans, 2016</xref>; <xref ref-type="bibr" rid="c7">Deuker et al., 2013</xref>; <xref ref-type="bibr" rid="c14">Guidotti, Del Gratta, Baldassarre, Romani, &#x0026; Corbetta, 2015</xref>; <xref ref-type="bibr" rid="c23">Schlichting &#x0026; Preston, 2014</xref>; <xref ref-type="bibr" rid="c26">Staresina et al., 2013</xref>; <xref ref-type="bibr" rid="c27">Tambini &#x0026; Davachi, 2013</xref>) in which case it is referred to as <italic>awake reactivation.</italic></p>
<p>Pioneering human studies have demonstrated the existence of awake reactivation for episodic memories in the medial temporal lobe including in the hippocampus (<xref ref-type="bibr" rid="c27">Tambini &#x0026; Davachi, 2013</xref>), the entorhinal and the retrosplenial cortex (<xref ref-type="bibr" rid="c26">Staresina et al., 2013</xref>). Specifically, brain patterns during encoding of associations between different categories persisted into the following rest period (<xref ref-type="bibr" rid="c26">Staresina et al., 2013</xref>; <xref ref-type="bibr" rid="c27">Tambini &#x0026; Davachi, 2013</xref>). The amount of reactivation in the hippocampus also predicted later behavioral performance (<xref ref-type="bibr" rid="c27">Tambini &#x0026; Davachi, 2013</xref>).</p>
<p>Building on these early findings, several lines of human studies found that awake reactivation exists beyond the medial temporal lobe (<xref ref-type="bibr" rid="c4">Chelaru et al., 2016</xref>; <xref ref-type="bibr" rid="c6">de Voogd et al., 2016</xref>; <xref ref-type="bibr" rid="c7">Deuker et al., 2013</xref>; <xref ref-type="bibr" rid="c14">Guidotti et al., 2015</xref>; <xref ref-type="bibr" rid="c23">Schlichting &#x0026; Preston, 2014</xref>). The brain patterns during encoding of associations between objects and spatial locations were shown to reemerge in both inferior temporal cortex and occipital cortex (<xref ref-type="bibr" rid="c7">Deuker et al., 2013</xref>). In addition, category-specific awake reactivation was demonstrated in the inferior temporal cortex (<xref ref-type="bibr" rid="c6">de Voogd et al., 2016</xref>) and the fusiform face area (<xref ref-type="bibr" rid="c23">Schlichting &#x0026; Preston, 2014</xref>). More recent studies used diverse paradigms beyond hippocampus-dependent episodic memory. Repeated exposure to movies led to reactivation in a large cortical network, consisting mostly of the frontal and temporal lobes (<xref ref-type="bibr" rid="c4">Chelaru et al., 2016</xref>). Similarly, training on a letter discrimination task induced reactivation across medial and lateral parietal regions and higher-order visual cortex (<xref ref-type="bibr" rid="c14">Guidotti et al., 2015</xref>). This converging evidence suggests that awake reactivation occurs across higher-order areas beyond the medial temporal lobe.</p>
<p>However, previous research could not determine what specific training components were reactivated. Indeed, some previous studies used associative learning between different categories (<xref ref-type="bibr" rid="c7">Deuker et al., 2013</xref>; <xref ref-type="bibr" rid="c23">Schlichting &#x0026; Preston, 2014</xref>; <xref ref-type="bibr" rid="c26">Staresina et al., 2013</xref>; <xref ref-type="bibr" rid="c27">Tambini &#x0026; Davachi, 2013</xref>). The use of associative learning makes it difficult to disentangle brain processes related to the coding of each stimulus from the brain processes related to the binding of different stimuli (<xref ref-type="bibr" rid="c7">Deuker et al., 2013</xref>). The studies that did not use associative learning employed relatively complex visual stimuli such as animals, fruits, vegetables, letters and cartoon movies (<xref ref-type="bibr" rid="c4">Chelaru et al., 2016</xref>; <xref ref-type="bibr" rid="c6">de Voogd et al., 2016</xref>; <xref ref-type="bibr" rid="c7">Deuker et al., 2013</xref>; <xref ref-type="bibr" rid="c14">Guidotti et al., 2015</xref>; <xref ref-type="bibr" rid="c23">Schlichting &#x0026; Preston, 2014</xref>; <xref ref-type="bibr" rid="c26">Staresina et al., 2013</xref>; <xref ref-type="bibr" rid="c27">Tambini &#x0026; Davachi, 2013</xref>). These complex visual stimuli consist of a large number of individual features that could potentially be reactivated. Further, these studies observed awake reactivation in higher-order brain regions that have mixed selectivity to features (<xref ref-type="bibr" rid="c17">Huth, de Heer, Griffiths, Theunissen, &#x0026; Gallant, 2016</xref>), making it again difficult to infer what specific features are reactivated.</p>
<p>Isolating a specific feature that is reactivated is critical for at least two reasons. First, it will demonstrate the limits of the phenomenon. Does reactivation in the human brain occur only at the level of complete objects? Is it a phenomenon that can be observed only in higher-level cortex? Or is awake reactivation a phenomenon that can be observed across the whole brain and at different levels? Second, providing evidence for awake reactivation of specific features will link the studies in humans even more tightly to the phenomenon of neuronal replay observed in animals where replay is typically demonstrated for a specific feature (e.g., location in space (<xref ref-type="bibr" rid="c3">Carr, Jadhav, &#x0026; Frank, 2011</xref>; <xref ref-type="bibr" rid="c5">Davidson, Kloosterman, &#x0026; Wilson, 2009</xref>; <xref ref-type="bibr" rid="c8">Diba &#x0026; Buzsaki, 2007</xref>; <xref ref-type="bibr" rid="c13">Foster &#x0026; Wilson, 2006</xref>) or placement on the screen (<xref ref-type="bibr" rid="c15">Han, Caporale, &#x0026; Dan, 2008</xref>)).</p>
<p>Further, previous research often did not prevent subjects from consciously rehearsing the trained stimuli (<xref ref-type="bibr" rid="c6">de Voogd et al., 2016</xref>; <xref ref-type="bibr" rid="c7">Deuker et al., 2013</xref>; <xref ref-type="bibr" rid="c14">Guidotti et al., 2015</xref>; <xref ref-type="bibr" rid="c23">Schlichting &#x0026; Preston, 2014</xref>; <xref ref-type="bibr" rid="c27">Tambini &#x0026; Davachi, 2013</xref>). In fact, some of these studies may have even encouraged such conscious rehearsal. Therefore, it is unclear whether the observed awake reactivation in prior studies reflects spontaneous brain activity or purposeful, top-down effects. Since awake reactivation is usually conceptualized as an automatic process outside of conscious awareness, it is important that the phenomenon is demonstrated even when conscious rehearsal is explicitly prevented.</p>
<p>In the current study, we set to address above-mentioned limitations and provide evidence for reactivation of a specific visual feature that is not driven by conscious rehearsal. To do so, we focused on the primary visual cortex (V1), which is known to code stimulus orientation (<xref ref-type="bibr" rid="c31">Yacoub, Harel, &#x0026; Ugurbil, 2008</xref>) and show changes after visual training (<xref ref-type="bibr" rid="c21">Rosenthal, Andrews, Antoniades, Kennard, &#x0026; Soto, 2016</xref>; <xref ref-type="bibr" rid="c22">Sasaki et al., 2010</xref>). We used oriented Gabor patches rather than complex stimuli for training and examined whether the trained orientation is reactivated in V1 after extensive visual training. To prevent any conscious rehearsal, we required participants to perform a challenging fixation task while their pre- and post-training spontaneous brain activity was measured. Our results show that the activity patterns of V1 are more likely to be classified as the trained, compared to an untrained orientation shortly after visual training. However, higher retinotopic areas such as V2, V3, V3A, or ventral V4 (V4v), did not show this effect, thus providing direct evidence that the effects in V1 were not driven by top-down processes such as conscious rehearsal. Furthermore, greater amount of awake reactivation in V1 was found to be associated with greater learning amount on the trained stimulus. These findings demonstrate that feature-specific, spontaneous awake reactivation takes place after visual training in V1 and that this reactivation is functionally important for visual learning.</p>
</sec>
<sec id="s2"><title>Results</title>
<p>We examined whether a process of feature-specific, spontaneous awake reactivation occurs after visual training in human V1. We trained human subjects (n &#x003D; 12) to detect a specific Gabor orientation (45&#x00B0; or 135&#x00B0;, counter-balanced between subjects; <bold><xref ref-type="fig" rid="fig1">Figure 1A</xref></bold>). The training induced significantly better performance on the post- compared to the pre-training test (F(1,11)&#x003D;34.351, P&#x003C;0.001; <bold><xref ref-type="fig" rid="figS1">Figure S1</xref></bold>), thus demonstrating that significant behavioral learning took place.</p>
<fig id="fig1" position="float"><label>Figure 1.</label>
<caption><title>Task and experimental procedure.</title>
<p><italic>(A) Subjects performed a 2-interval-forced-choice (2IFC) orientation detection task, in which they indicated whether a Gabor patch appeared in the first or second interval. (B) The experiment consisted of three days. Visual training (&#x223C;40 minutes on average) was conducted on Day 2. We collected two spontaneous measurement scans (5 minutes/scan) before (combined into a single pre baseline) and after training (post1 and post2). During these scans subjects detected a color change in a small fixation dot (see Methods). Decoder construction (&#x223C;50 minutes) and retinotopy (&#x223C;20 minutes) scans were collected on Day 1. Pre- and post-training behavioral tests were conducted on Days 1 and 3 to confirm that training improved task performance.</italic></p></caption>
<graphic xlink:href="231381_fig1.tif"/></fig>
<p>Before the training, we constructed a decoder that could distinguish between the multivoxel pattern of blood-oxygen-level dependent (BOLD) activity elicited by each Gabor orientation (<bold><xref ref-type="fig" rid="fig1">Figure 1B</xref></bold>). We then applied this decoder on the spontaneous activity scans conducted before (pre, two 5-min scans that we combined for analysis) and after (post1 &#x0026; post2: consecutive two 5-min scans analyzed separately) the training.</p>
<p>If feature-specific awake reactivation indeed occurs in V1, it would manifest itself as post-training, but not pre-training, spontaneous activity appearing more similar to the trained, compared to the untrained, orientation. To test for such feature-specific awake reactivation in V1, we analyzed the decoder&#x2019;s probability of classifying spontaneous activity in V1 as the trained orientation. Note that in this analysis, spontaneous activity was always classified as either the trained or untrained Gabor orientation at each data point (see Methods), and therefore we only report the probability of &#x201C;trained orientation&#x201D; classifications. In order to examine whether the decoder&#x2019;s classification for the trained orientation significantly increased after the training, we applied a one-way repeated measures ANOVA to the decoder&#x2019;s probability. The result showed a significant main effect of time (pre vs. post1 vs. post2; F(2,22)&#x003D;6.321, P&#x003D;0.007). Consistent with the existence of feature-specific awake reactivation, the activation pattern in V1 was more likely to be classified as the trained orientation immediately after (post1) compared to before (pre) the visual training (t(11)&#x003D;3.607, P&#x003D;0.004, uncorrected paired sample t-test; <bold><xref ref-type="fig" rid="fig2">Figure 2A,B</xref></bold>). A significant quadratic trend was also observed in the time-course of the decoder&#x2019;s probability (F(1,11)&#x003D;12.041, P&#x003D;0.005), indicating a peak right after training. Furthermore, the probability of classification as the trained orientation was significantly greater than chance in both post-training periods (post1: t(11)&#x003D;4.467, P&#x003D;0.001; post2: t(11)&#x003D;2.929, P&#x003D;0.014, one-sample t-tests) but not before training (pre, t(11)&#x003D;0.243, P&#x003D;0.812, one-sample t-test).</p>
<fig id="fig2" position="float"><label>Figure 2.</label>
<caption><title>Probability of classifying spontaneous brain activity as trained orientation in V1.</title>
<p><italic>Consistent with the existence of awake reactivation, spontaneous activity in V1 immediately after training was more likely to be classified as the trained orientation. The effect is visible both in the group (A) and individual data (B). Error bars represent s.e.m. &#x002A; P&#x003C;0.05, &#x002A;&#x002A; P&#x003C;0.01, N.S. not significant.</italic></p></caption>
<graphic xlink:href="231381_fig2.tif"/></fig>
<p>The above analysis focused on the decoder&#x2019;s probability of classifying activity as the trained orientation based on the decoder&#x2019;s binary classification (either trained or untrained orientation) at each data point. However, for each data point, the decoder could also produce a continuous measure: the likelihood that the data point came from the trained or untrained orientation. Analyzing these continuous likelihood values (rather than the binary classification performance), we still found the same pattern of results (main effect of time in one-way repeated measures ANOVA, F(2,22)&#x003D;3.780, P&#x003D;0.039; <bold><xref ref-type="fig" rid="figS2">Figure S2</xref></bold>; see Methods). The likelihood of classification as the trained orientation was also significantly greater than chance in both post-training periods (post1: t(11)&#x003D;3.584, P&#x003D;0.004; post2: t(11)&#x003D;2.400, P&#x003D;0.035, one-sample t-tests) but not before training (pre, t(11)&#x003D;0.427, P&#x003D;0.678, one-sample t-test). Thus, both methods of analysis show that posttraining spontaneous activity in V1 appears more similar to the trained orientation.</p>
<p>One potential concern is that these results could be due to subjects consciously imagining the trained stimulus during the spontaneous brain measurement (post1 and post2). We made such behavior unlikely by requiring subjects to perform a challenging task in which they monitored for a slight color change at fixation during the spontaneous brain measurement (<bold><xref ref-type="fig" rid="fig1">Figure 1B</xref></bold>; see Methods). We further delayed any post-training test until the next day, thus making it unnecessary for subjects to mentally practice the trained task immediately after the training was over.</p>
<p>Despite these precautions, it is still possible that some amount of conscious rehearsal of the trained stimulus took place. We reasoned that if any conscious rehearsal occurred, reactivation-like activity should be observed in higher visual areas since top-down processes such as visual imagery (<xref ref-type="bibr" rid="c1">Albers, Kok, Toni, Dijkerman, &#x0026; de Lange, 2013</xref>), attention (<xref ref-type="bibr" rid="c28">R. B. Tootell et al., 1998</xref>), and working memory (<xref ref-type="bibr" rid="c16">Harrison &#x0026; Tong, 2009</xref>) have at least as strong (and often much stronger) influence on these higher visual areas as on V1. We therefore analyzed the decoder&#x2019;s probability of classifying spontaneous activity as the trained orientation based on activity patterns in V2, V3, V3A, and V4v. In order to test whether the decoder&#x2019;s performance in higher visual areas changed after the training, we conducted a two-way repeated measures ANOVA with factors time (pre vs. post1 vs. post2) and region (V2, V3, V3A, and V4v). The ANOVA revealed no significant effects of time (F(2,22)&#x003D;0.240, P&#x003D;0.789), region (F(3,33)&#x003D;2.113, P&#x003D;0.117), or interaction between the two (F(6,66)&#x003D;2.022, P&#x003D;0.075). Despite the lack of significant overall effects (<bold><xref ref-type="fig" rid="fig3">Figure 3</xref></bold>; individual results are shown in <bold><xref ref-type="fig" rid="figS3">Figure S3</xref></bold>), we wanted to ensure that none of these higher retinotopic regions showed the pattern of results in V1 where decodability of the trained stimulus increased immediately after training (post1) compared to before training (pre). In fact, we found no such effects in V2, V3, V3A, or V4v (all P values &#x003E; 0.6 for pre vs. post1 in each area before correction: V2, t(11)&#x003D;0.333, P&#x003D;0.746; V3, t(11)&#x003D;-0.492, P&#x003D;0.632; V3A, t(11)&#x003D;-0.380, P&#x003D;0.711; V4v, t(11)&#x003D;-0.514, P&#x003D;0.618, paired sample t-tests). Therefore, the results in V1 are unlikely to be due to subjects consciously rehearsing the trained stimulus.</p>
<fig id="fig3" position="float"><label>Figure 3.</label>
<caption><title>Probability of classifying spontaneous brain activity as trained orientation in V2, V3, V3A, and V4v.</title>
<p><italic>Consistent with a lack of awake reactivation in higher retinotopic areas, spontaneous activity in V2, V3, V3A, and V4v was classified as the trained orientation at chance level (0.5) immediately after training. Error bars represent s.e.m. N.S. not significant.</italic></p></caption>
<graphic xlink:href="231381_fig3.tif"/></fig>
<p>We analyzed the data from V1 separately from the other four retinotopic areas based on our a priori hypothesis. However, if all five retinotopic regions are entered into a twoway repeated measures ANOVA, then consistent with the existence of awake reactivation exclusively in V1, a significant interaction between region and time emerges (F(8,88)&#x003D;2.303, P&#x003D;0.027). In addition, we further examined whether the decoder&#x2019;s probability of classifying spontaneous activity as the trained orientation in V1 is significantly greater than that in V2, V3, V3A, and V4v immediately after training (post1). We found a greater decodability of the trained stimulus in V1 compared to V2, V3A, and V4v immediately after training (post1) (V1 vs. V2, t(11)&#x003D;4.196, P&#x003D;0.001; V1 vs. V3, t(11)&#x003D;1.478, P&#x003D;0.168; V1 vs. V3A, t(11)&#x003D;2.655, P&#x003D;0.022; V1 vs. V4v, t(11)&#x003D;2.708, P&#x003D;0.020, paired sample t-tests, before correction). Again, these results support the notion that the pattern of increased decodability of the trained stimulus immediately after training is found exclusively in V1.</p>
<p>Finally, we examined whether the observed awake reactivation in V1 had functional consequences for learning. To address this question, we examined whether subjects who showed greater reactivation exhibited stronger behavioral improvement. We constructed subject-specific learning index that measures the amount of learning that was specific to the trained orientation. Similarly, we constructed a subject-specific reactivation index that measures the amount of reactivation by comparing the probability of decoding the trained stimulus in the pre and post-training spontaneous measurement scans (see Methods for details about both indices). We then compared the learning index across subjects with a high vs. low amount of reactivation (using a median split). We found that the learning index for those who showed higher awake reactivation was significantly greater than that for those who showed lower awake reactivation (t(10)&#x003D;2.616, P&#x003D;0.026, independent samples t-test; <bold><xref ref-type="fig" rid="fig4">Figure 4</xref></bold>). This result indicates that greater amount of awake reactivation is associated with greater visual learning on the trained stimulus and suggests that awake reactivation has a functional role in the learning.</p>
<fig id="fig4" position="float"><label>Figure 4.</label>
<caption><title>Strength of awake reactivation predicts performance improvement.</title>
<p><italic>The amount of behavioral improvement specific to the trained orientation was significantly greater for subjects who showed higher levels of awake reactivation. Dots represent individual data. Error bars represent s.e.m. &#x002A; P&#x003C;0.05.</italic></p></caption>
<graphic xlink:href="231381_fig4.tif"/></fig>
</sec>
<sec id="s3"><title>Discussion</title>
<p>We investigated whether a process of spontaneous, feature-specific awake reactivation takes place in human V1 following training with a novel visual stimulus. We found that shortly after training, the spontaneous brain activity in V1 was more likely to be classified as the trained orientation. This awake reactivation was specific to V1; higher retinotopic areas failed to show similar effects. Furthermore, the strength of awake reactivation predicted behavioral performance improvement on the trained stimulus. These results suggest that spontaneous, feature-specific awake reactivation occurs in V1 after the visual training and that this awake reactivation may serve to bolster the subsequent visual learning after the initial training is completed.</p>
<p>Our findings go beyond the previous literature on awake reactivation in three important ways. First, we demonstrate feature-specific reactivation. Indeed, by comparing Gabor stimuli that only vary in their orientation, we revealed that a basic visual feature such as orientation is specifically reactivated in the brain. Conversely, previous human studies could not determine what specific aspect of the trained stimulus was reactivated. Some of the prior studies used associative learning between two different categories (<xref ref-type="bibr" rid="c7">Deuker et al., 2013</xref>; <xref ref-type="bibr" rid="c23">Schlichting &#x0026; Preston, 2014</xref>; <xref ref-type="bibr" rid="c26">Staresina et al., 2013</xref>; <xref ref-type="bibr" rid="c27">Tambini &#x0026; Davachi, 2013</xref>). However, the reemergence of the brain processes during encoding of two different categories is insufficient to determine whether the reactivated information relates to the coding of each stimulus or the binding process (<xref ref-type="bibr" rid="c7">Deuker et al., 2013</xref>). Other studies did not use associative learning but employed complex stimuli such as animals, fruits, vegetables, letters and cartoon movies (<xref ref-type="bibr" rid="c4">Chelaru et al., 2016</xref>; <xref ref-type="bibr" rid="c6">de Voogd et al., 2016</xref>; <xref ref-type="bibr" rid="c7">Deuker et al., 2013</xref>; <xref ref-type="bibr" rid="c14">Guidotti et al., 2015</xref>; <xref ref-type="bibr" rid="c23">Schlichting &#x0026; Preston, 2014</xref>; <xref ref-type="bibr" rid="c26">Staresina et al., 2013</xref>; <xref ref-type="bibr" rid="c27">Tambini &#x0026; Davachi, 2013</xref>) that contain a large number of individual features that could potentially be reactivated. By demonstrating feature-specific reactivation, our study shows that awake reactivation in the human brain is not limited to the level of whole objects but extends down to the most basic visual features.</p>
<p>Second, we show that reactivation in humans is not exclusive to higher-order brain areas but exists even in the primary visual cortex V1. Prior studies demonstrated the existence of reactivation mostly in the association cortex but not in the primary sensory areas (<xref ref-type="bibr" rid="c4">Chelaru et al., 2016</xref>; <xref ref-type="bibr" rid="c6">de Voogd et al., 2016</xref>; <xref ref-type="bibr" rid="c7">Deuker et al., 2013</xref>; <xref ref-type="bibr" rid="c14">Guidotti et al., 2015</xref>; <xref ref-type="bibr" rid="c23">Schlichting &#x0026; Preston, 2014</xref>; <xref ref-type="bibr" rid="c26">Staresina et al., 2013</xref>; <xref ref-type="bibr" rid="c27">Tambini &#x0026; Davachi, 2013</xref>). Therefore, our study suggests that human reactivation is likely to occur across most (if not all) brain regions.</p>
<p>Third, our study demonstrates that awake reactivation can occur without conscious rehearsal. Most of the prior human studies on awake reactivation did not have a robust way of preventing subjects from engaging in conscious rehearsal (<xref ref-type="bibr" rid="c6">de Voogd et al., 2016</xref>; <xref ref-type="bibr" rid="c7">Deuker et al., 2013</xref>; <xref ref-type="bibr" rid="c14">Guidotti et al., 2015</xref>; <xref ref-type="bibr" rid="c23">Schlichting &#x0026; Preston, 2014</xref>; <xref ref-type="bibr" rid="c27">Tambini &#x0026; Davachi, 2013</xref>) and in some cases, may have actually encouraged such rehearsal. This issue is critical since awake reactivation is usually conceptualized as an automatic process outside of conscious awareness but evidence for this view has been lacking.</p>
<p>Several pieces of evidence suggest that awake reactivation in our study was not due to conscious rehearsal of the trained stimulus. First, we required subjects to perform a challenging task at fixation (see Methods) during the pre- and post-training periods. This fixation task introduced attentional demands that were not present in the no-task resting-state scans used in the previous studies. The presence of a demanding task made it unlikely that subjects would engage in a purposeful recall of the trained stimulus. At the same time, the fixation task was designed so that the parts of the visual cortex corresponding to the trained stimulus were not prevented from engaging in reactivation processes. This was achieved by making the target in the fixation task small and spatially non-overlapping with the trained stimulus. Second, we delayed any further tests on the trained task until the next day. This manipulation removed any immediate motivation to purposefully rehearse the trained stimulus during the fixation task. Third, we checked whether reactivation-like activity could be detected in higher retinotopic areas such as V2, V3, V3A, and V4v. It is known that top-down processes such as visual imagery (<xref ref-type="bibr" rid="c1">Albers et al., 2013</xref>), attention (<xref ref-type="bibr" rid="c28">R. B. Tootell et al., 1998</xref>), and working memory (<xref ref-type="bibr" rid="c16">Harrison &#x0026; Tong, 2009</xref>) have a strong influence on these higher visual areas. The lack of reactivation in those higher regions thus shows that the results in V1 are indeed not due to conscious top-down rehearsal of the trained stimulus. Finally, after completing the experiment, we asked subjects if they mentally imagined the trained stimulus while performing the fixation task and they all denied doing so. Note, however, that a verbal statement alone should not be considered as sufficient evidence for a lack of conscious rehearsal.</p>
<p>Despite the fact that fMRI has limited spatio-temporal resolution compared to neuronal recordings, our results, together with prior human studies on awake reactivation, appear to reveal processes akin to neuronal replay in animals (<xref ref-type="bibr" rid="c3">Carr et al., 2011</xref>). Neuronal replay, that is the reemergence of neuronal patterns that represent previous learning, has been studied extensively in hippocampal cells (<xref ref-type="bibr" rid="c3">Carr et al., 2011</xref>; <xref ref-type="bibr" rid="c5">Davidson et al., 2009</xref>; <xref ref-type="bibr" rid="c8">Diba &#x0026; Buzsaki, 2007</xref>; <xref ref-type="bibr" rid="c13">Foster &#x0026; Wilson, 2006</xref>; <xref ref-type="bibr" rid="c33">Yao, Shi, Han, Gao, &#x0026; Dan, 2007</xref>) but also in areas of the neocortex, particularly the visual (<xref ref-type="bibr" rid="c15">Han et al., 2008</xref>; <xref ref-type="bibr" rid="c18">Ji &#x0026; Wilson, 2007</xref>; <xref ref-type="bibr" rid="c33">Yao et al., 2007</xref>) and the frontal cortex (<xref ref-type="bibr" rid="c12">Euston, Tatsuno, &#x0026; McNaughton, 2007</xref>) in animals. More recent studies have shown that neuronal patterns representing previous learning can be preplayed by a cue even before a task begins (<xref ref-type="bibr" rid="c10">Eagleman &#x0026; Dragoi, 2012</xref>; <xref ref-type="bibr" rid="c11">Ekman, Kok, &#x0026; de Lange, 2017</xref>; <xref ref-type="bibr" rid="c30">Xu, Jiang, Poo, &#x0026; Dan, 2012</xref>). These studies demonstrated the existence of cue-triggered preplay during wakefulness in both rodent V1 (<xref ref-type="bibr" rid="c30">Xu et al., 2012</xref>), monkey V4 (<xref ref-type="bibr" rid="c10">Eagleman &#x0026; Dragoi, 2012</xref>) and human V1 (<xref ref-type="bibr" rid="c11">Ekman et al., 2017</xref>). Based on this converging evidence, we suspect that our results reflect spontaneous firing of the same pools of V1 neurons that code for the trained stimulus after training is completed. Future studies using finer spatio-temporal resolution are needed to directly examine this possibility.</p>
<p>In our study, the decodability of the trained orientation in V1 was highest in the first posttraining period (post1) but remained significant in the second post-training period (post2). Therefore, awake reactivation in our study continued at least until the onset of the second post-training period (post2), which occurred on average 7.75 minutes after the offset of visual training. In other words, reactivation persisted for at least &#x007E;8 minutes after subjects last experienced the trained stimulus. This time-course of awake reactivation in V1 agrees well with neuronal results in anesthetized rodent V1 (<xref ref-type="bibr" rid="c15">Han et al., 2008</xref>). In that study, 50 (125) stimulus repetitions led to &#x007E;3 (&#x007E;14) minutes of replay activity. Compared to this study, our visual training was significantly longer but reactivation processes lasted for a shorter time. The shortened time length of reactivation is likely due to the fact that, unlike in anesthesia, the awake brain needs to remain responsive to its environment, which likely diminishes its capacity for reactivation. Future studies should include various training periods in order to delineate the time-course of the reactivation processes in humans.</p>
<p>In summary, we found evidence for spontaneous, feature-specific awake reactivation in area V1 after training on a visual task in humans. Critically, these effects were not observed in higher retinotopic areas. Further, the amount of awake reactivation in V1 predicted the behavioral learning amount on the trained stimulus. These results suggest that feature-specific awake reactivation might be a critical mechanism that supports offline memory consolidation.</p>
</sec>
<sec id="s4"><title>Materials and Methods</title>
<sec id="s4a"><title>Participants</title>
<p>Twelve healthy subjects (19 to 25 years old, 7 females) with normal or corrected-to-normal vision participated in this study. Exclusion criteria comprised a history of neurological, psychiatric disorders and contraindications to MRI. All subjects provided their demographic information and written informed consent, which was approved by the Institutional Review Board of Georgia Institute of Technology. All experiments were performed during daytime. All twelve subjects&#x2019; data were included in the analysis. Each experiment was performed once for all subjects.</p>
</sec>
<sec id="s4b"><title>Procedures</title>
<p>The study consisted of three days (<bold><xref ref-type="fig" rid="fig1">Figure 1B</xref></bold>): decoder construction, retinotopy, and a pre-training test on Day 1, spontaneous brain activity measurements and training on Day 2, and a post-training test on Day 3. Days 1 and 2 were separated by multiple days, while Day 3 always immediately followed Day 2.</p>
<sec id="s4b1"><title>Orientation detection task</title>
<p>Subjects completed a 2-interval-forced-choice (2IFC) orientation detection task during training, as well as during the pre- and post-training tests. Subjects indicated which of two intervals contained a Gabor patch. The Gabor patch (contrast &#x003D; 100&#x0025;, spatial frequency &#x003D; 1 cycle/degree, Gaussian filter sigma &#x003D; 2.5 degree, random spatial phase) was presented within an annulus subtending 0.75 to 5 degrees and was masked by noise generated from a sinusoidal luminance distribution at a given signal-to-noise (S/N) ratio.</p>
<p>For instance, 10&#x0025; S/N ratio meant that noise replaced 90&#x0025; of the pixels in the Gabor patch. The interval without the Gabor patch consisted of pure noise (0&#x0025; S/N ratio). The interval in which the Gabor patch was presented was determined randomly. During the entire orientation detection task, subjects were asked to fix their eyes on a white bull&#x2019;s-eye on a gray disc (0.75&#x00B0; radius) at the center of the screen.</p>
<p>Each trial began with a 500-ms fixation period. After the fixation period, two stimuli were presented for 50 ms each, separated by a 300-ms blank period. Subjects indicated the interval in which the Gabor patch appeared by pressing a button on a keypad. No feedback was provided.</p>
<p>The training, as well as the pre- and post-training tests consisted of individual blocks. Within each block, we controlled the difficulty of the task by adjusting the S/N ratio using a 2-down 1-up staircase method. Each block started with stimuli at 25&#x0025; S/N ratio and terminated after 10 reversals. Each block lasted approximately 1-2 minutes, contained 30-40 trials, and presented stimuli with just one orientation (45&#x00B0; or 135&#x00B0;).</p>
</sec>
<sec id="s4b2"><title>Training</title>
<p>Training was conducted on Day 2 of our experiment. During the training, the subjects performed the 2IFC orientation detection task on one orientation only, randomly selected for each subject (to be either 45&#x00B0; or 135&#x00B0;). Subjects completed 16 blocks in total lasting approximately 40 minutes. The training was performed in the MR scanner.</p>
</sec>
<sec id="s4b3"><title>Pre- and post-training tests</title>
<p>The purpose of the pre- and post-training tests was to obtain each subject&#x2019;s threshold S/N ratio for each Gabor orientation. During these tests, subjects performed the same 2IFC orientation detection task. Each test consisted of two blocks, one for each Gabor orientation (45&#x00B0; or 135&#x00B0;). The order of presentation of the two orientations was randomized for each test and each subject. Subjects performed the pre- and post-training tests in a mock scanner located immediately adjacent to the scanner room.</p>
</sec>
<sec id="s4b4"><title>Threshold S/N ratio</title>
<p>The threshold S/N ratio was determined separately for each block of testing. We computed the threshold S/N ratio as the geometric mean of the S/N ratio during the last 6 reversals in a block.</p>
</sec>
<sec id="s4b5"><title>Relationship between awake reactivation and behavioral improvement</title>
<p>In order to check whether the amount of awake reactivation predicted the behavioral improvement, we first constructed an index of subjects&#x2019; learning. The learning index measures the relative learning amount for trained orientation compared to untrained orientation. The learning amount was defined as the change in the thresholds for pre- and post-training tests divided by the threshold at pre-training test:
<disp-formula id="ueqn1"><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="231381_ueqn1.gif"/></alternatives></disp-formula>
where <italic>T<sub>pre</sub></italic> and <italic>T<sub>post</sub></italic> refer to the threshold S/N ratios before and after training. We computed the learning index by subtracting learning amount for the untrained orientation from learning amount for the trained orientation.</p>
<p>To relate the learning index to the amount of awake reactivation, we constructed an equivalent index of subject-specific awake reactivation. The reactivation index measures the increase in decodability of the trained stimulus in V1 from the pre- to the post-training spontaneous brain measurement:
<disp-formula id="ueqn2"><alternatives><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="231381_ueqn2.gif"/></alternatives></disp-formula>
where <italic>P<sub>pre</sub></italic> and <italic>P<sub>post</sub></italic> refer to the probability of classifying brain activity as the trained orientation in the pre- and post-training spontaneous brain measurements, respectively.</p>
</sec>
<sec id="s4b6"><title>Decoder construction</title>
<p>The purpose of the decoder construction stage was to obtain each subject&#x2019;s blood-oxygen-level dependent (BOLD) signal patterns corresponding to each of the two Gabor orientations (45&#x00B0; and 135&#x00B0;). The data obtained from the decoder construction stage was later used to compute the parameters for the decoder that distinguishes between the two Gabor orientations.</p>
<p>During the decoder construction scan, the subjects performed 10 runs of frequency detection task (see below for details of the task) inside the MR scanner. Each run consisted of 18 trials, plus 6-sec fixation period in the beginning and the end of the run (1 run &#x003D; 300 sec). Each trial had two parts, 12-sec stimulus presentation period and the following 4-sec response period (1 trial &#x003D; 16 sec).</p>
<p>During 12-sec stimulus presentation period, 12 Gabor patches were presented at a rate of 1 Hz. Each Gabor patch was flashed for 500 ms allowing for 500 ms blank period between consecutive Gabor stimuli. All 12 Gabor patches had one specific orientation (45&#x00B0; or 135&#x00B0;) chosen randomly on each trial. In half of the 18 trials, one of the 12 Gabor patches had slightly higher spatial frequency compared to the other patches. For the other half of 18 trials, all 12 Gabor patches had the same spatial frequency. After the end of the 12-sec stimulus presentation, subjects indicated whether a Gabor patch with a different spatial frequency was presented. All Gabor patches were displayed with 50&#x0025; S/N.</p>
<p>During each run, a white bull&#x2019;s-eye on a gray disc (0.75&#x00B0; radius) was presented as a fixation point at the center of the screen. In the beginning of the 12-sec stimulus presentation period, the fixation point at the center of the screen changed its color from white to green to indicate that the stimulus presentation period had started. The color of the fixation point remained green throughout the 12-sec stimulus presentation period and changed to white again when the response period started.</p>
<p>The subjects were instructed to press a button on a key pad during 4-sec response period if they detected any change in the spatial frequency during preceding stimulus presentation period. If they did not detect any frequency change, they were instructed not to press a button. We controlled the difficulty of the task using an adaptive staircase method. The initial degree of the spatial frequency change in the first run was set to 0.24 Hz and decreased by 0.02 in the case of a hit and increased by 0.02 in the case of a false alarm or miss. In the case of a correct rejection, the degree of the frequency did not change. The following run always started from the last degree of the frequency in the previous run.</p>
</sec>
<sec id="s4b7"><title>Retinotopy</title>
<p>We defined the retinotopically-organized areas V1, V2, V3, V3A, and ventral V4 (V4v) using standard retinotopic methods (<xref ref-type="bibr" rid="c24">Sereno et al., 1995</xref>; <xref ref-type="bibr" rid="c29">R. B. H. Tootell et al., 1997</xref>). Briefly, we presented a flickering checkerboard pattern at vertical/horizontal meridians and in the upper/lower visual fields. Based on the contrast map of BOLD signals, we delineated visual cortical areas of V1, V2, V3, V3A, and V4v. Furthermore, we presented an annulus stimulus in order to delineate the retinotopic regions in each visual area corresponding to the visual fields stimulated by the Gabor patch. The stimulus was a flickering checkerboard pattern within an annulus subtending 0.75 to 5 degree from the center of the screen, which is identical with the size of the Gabor patch. Only the voxels activated by the flickering pattern within an annulus stimulus were included in the main analyses.</p>
</sec>
<sec id="s4b8"><title>Spontaneous brain activity measurement</title>
<p>We recorded each subject&#x2019;s &#x201C;spontaneous&#x201D; BOLD activity before (pre, two 5-min scans that we combined for analysis) and after (post1 &#x0026; post2: consecutive two 5-min scans analyzed separately) the visual training. In order to ensure that the subjects do not consciously imagine the trained orientation, we required them to perform a fixation task while they were scanned. During the fixation task, the subjects fixated their eyes on a white bull&#x2019;s-eye on a gray disc (0.75&#x00B0; radius) at the center of the screen. They were instructed to press a button on a keypad as soon as they detected a color change of the center dot. A fixation dot changed its color from white ([R, G, B] &#x003D; [255, 255, 255]) to faint pink ([R, G, B] &#x003D; [255, 255 &#x2013; x, 255 &#x2013; x]) in an unpredictable way and returned to white 1.5 sec later. For the button press to be regarded as a hit, the subjects had to press a button within 1.5 sec (i.e., during the period when the color had changed). Initially, the color change x was set to 40 and then controlled by 2-down 1-up staircase rule.</p>
<p>To confirm that the subjects&#x2019; performance during the fixation task remained at a similar level over time (pre, post1, post2), we conducted a one-way repeated measures ANOVA on the accuracy rate with a factor of time (pre vs. post1 vs. post2). The result showed no significant main effect of time (F(2,22)&#x003D;2.045, Huynh-Feldt correction, epsilon&#x003D;0.724, P&#x003D;0.169), suggesting that the subjects&#x2019; performance was constant throughout time.</p>
</sec></sec>
<sec id="s4c"><title>MRI Data Acquisition</title>
<p>Subjects were scanned inside a 3T MR scanner (Siemens 3T Trio) with a 12-channel head coil at Georgia Institute of Technology MRI Research Facility. For the anatomical reconstruction, high-resolution T1-weighted MR images were acquired using a multiecho magnetization-prepared rapid gradient echo (MPRAGE; 256 slices, voxel size &#x003D; 1 &#x00D7; 1 &#x00D7; 1 mm<sup>3</sup>, TR &#x003D; 2530 ms, FoV &#x003D; 256 mm). Functional MR images were acquired using gradient echo EPI sequences (voxel size &#x003D; 3 &#x00D7; 3 &#x00D7; 3.5 mm, TR &#x003D; 2000 ms, TE &#x003D; 30 ms, flip angle &#x003D; 79&#x00B0;). Thirty-three contiguous slices were positioned parallel to the AC-PC plane to cover the whole brain.</p>
</sec>
<sec id="s4d"><title>fMRI Data Analysis</title>
<p>We analyzed the brain data using Freesurfer software (<ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu/">http://surfer.nmr.mgh.harvard.edu/</ext-link>). Since we obtained each subject&#x2019;s brain data on separate days (days 1 and 2), we processed the structural images from two different days with the longitudinal stream (<xref ref-type="bibr" rid="c20">Reuter, Schmansky, Rosas, &#x0026; Fischl, 2012</xref>), that is known to create an unbiased within-subject structural template using robust, inverse consistent registration. All functional data were motion-corrected. No spatial or temporal smoothing was applied to the functional data. We registered the functional data from two different days&#x2019; scans to the individual structural template that was created via the longitudinal stream using rigid-body transformations. A gray matter mask was used for extracting BOLD signals from voxels located within the gray matter.</p>
<p>To localize the retinotopic areas V1, V2, V3, V3A, and V4v, as well as their sub-regions corresponding to the visual fields stimulated by the Gabor patch during the 2IFC orientation detection task, we performed a conventional amplitude analysis (see Retinotopy in Methods for more details about stimuli). Briefly, using the contrast map between vertical/horizontal meridians and upper/lower visual fields, we delineated visual cortical areas of V1, V2, V3, V3A, and V4v. Then using a separate scan during the Retinotopy session, we localized the sub-regions in each visual area corresponding to the part of the visual field occupied by the Gabor patch during the 2IFC orientation detection task. To do so, we contrasted two conditions: an annulus ON condition, in which a flickering checkerboard pattern filled an annulus subtending 0.75 to 5 degrees (same size as in the 2IFC task) and an annulus OFF condition in which a flickering checkerboard pattern filled the whole screen except an annulus subtending 0.75 to 5 degrees.</p>
<p>After we identified the sub-regions of V1, V2, V3, V3A, and V4v, we extracted the time-courses of BOLD signal intensities from each voxel within the sub-regions using MATLAB (MathWorks, Natick, MA). We shifted the BOLD signals by 6 sec to account for the hemodynamic delay and removed a linear trend. Then, within each run, we normalized (&#x201C;z-scored&#x201D;) each voxel&#x2019;s BOLD time-courses to minimize the baseline changes across runs. Specifically, to create the data sample for decoding, we averaged the BOLD signals across 6 volumes (12 sec) that correspond to the duration of the stimulus presentation period in the decoder construction stage.</p>
<p>We used sparse logistic regression (SLR) (<xref ref-type="bibr" rid="c32">Yamashita, Sato, Yoshioka, Tong, &#x0026;Kamitani, 2008</xref>) and the SLR toolbox (<ext-link ext-link-type="uri" xlink:href="http://www.cns.atr.jp/&#x007E;oyamashi/SLRWEB">http://www.cns.atr.jp/&#x007E;oyamashi/SLRWEB</ext-link>). SLR selects relevant voxels in the ROIs automatically while estimating their weight parameters for classification. We selected the voxels within the sub-regions of V1, V2, V3, V3A, and V4v as the input voxels. Then we trained the decoder to classify the BOLD patterns as either the Gabor stimulus with 45&#x00B0; or 135&#x00B0; orientation using 180 data samples from 10 runs of decoder construction stage (90 samples for each orientation). We tested the accuracy of the decoder using a leave-one-run-out cross-validation procedure for each of sub-regions of V1, V2, V3, V3A, and V4v. <bold><xref ref-type="fig" rid="figS4">Figure S4</xref></bold> shows the decoder accuracy for each ROI. The accuracy of the decoder was significantly above the baseline for all ROIs (all P values &#x003C; 0.005).</p>
<p>Finally, we applied the same decoder to the spontaneous BOLD signals obtained before and after the visual training. The time-courses of the spontaneous BOLD signals were again averaged across 6 volumes. Then the decoder calculated how likely each spontaneous BOLD activity pattern averaged across 6 volumes is to the pattern elicited by the real Gabor orientation during decoder construction stage. Based on this likelihood result, the decoder classified each pattern averaged across 6 volumes to either trained or untrained orientation. <bold><xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig3">3</xref></bold> and <bold><xref ref-type="fig" rid="figS3">Figure S3</xref></bold> show the probability of the brain activity being classified as the trained orientation. <bold><xref ref-type="fig" rid="figS2">Figure S2</xref></bold> shows the likelihood of the brain activity being classified as the trained orientation.</p>
</sec>
<sec id="s4e"><title>Statistics</title>
<p>All statistical tests were two-tailed and the alpha level was set to 0.05. We used parametric statistical tests such as t-tests and ANOVA. For all repeated measures ANOVAs, we used Mauchly&#x2019;s test of Sphericity to test the assumption of sphericity. We used Huynh-Feldt correction with the estimated epsilon when the sphericity assumption was violated. All such violations are reported when they occurred. All effects reported as significant remained such after application of Bonferroni correction. The sample size was determined based on similar fMRI experiments on visual learning (<xref ref-type="bibr" rid="c14">Guidotti et al., 2015</xref>; <xref ref-type="bibr" rid="c25">Shibata, Sasaki, Kawato, &#x0026; Watanabe, 2016</xref>). The data collection and analysis were not conducted blindly by the experimenters.</p>
</sec>
<sec id="s4f"><title>Apparatus</title>
<p>All visual stimuli were created via MATLAB and Psychtoolbox 3 on Mac OS X (<xref ref-type="bibr" rid="c2">Brainard, 1997</xref>). We presented visual stimuli on LCD display (1024 &#x00D7; 768 resolution, 60 Hz refresh rate) inside a mock scanner and via MRI-compatible LCD projector (1024 &#x00D7; 768 resolution, 60 Hz refresh rate) inside a 3T MR scanner.</p>
</sec>
<sec id="s4g"><title>Data and Software Availability</title>
<p>The data and the computer codes are freely available online at: <ext-link ext-link-type="uri" xlink:href="http://osf.io/9du8v/">https://osf.io/9du8v/</ext-link></p>
</sec></sec>
</body>
<back>
<ack><title>Acknowledgements</title>
<p>This work was funded by a startup grant to D.R. from the Georgia Institute of Technology.</p>
</ack>
<sec id="s5"><title>Author Contributions</title>
<p>J.W.B., Y.S., T.W. and D.R. conceived the study; J.W.B. and D.R. collected and analyzed data; J.W.B., Y.S., T.W. and D.R. wrote the manuscript.</p>
</sec>
<sec id="s6"><title>Competing Financial Interests</title>
<p>The authors declare no competing financial interests.</p>
</sec>
<sec id="s7">
<fig id="figS1" position="float"><label>Figure S1.</label>
<caption><title>Threshold S/N (mean &#x00B1; s.e.m.) during pre- and post-training stages for trained (white) and untrained (gray) orientations.</title>
<p>To confirm that the subjects showed learning between pre- and post-training test stages, we performed a two-way repeated measures ANOVA with factors of orientation (trained vs. untrained orientation) and time (pre- vs. post-training) on the threshold S/N ratio. The results showed a significant main effect of time, indicating that the learning occurred for both orientations (time effect: F(1,11)&#x003D;34.351, P&#x003C;0.001). The dots represent individual data.</p></caption>
<graphic xlink:href="231381_figS1.tif"/></fig>
<fig id="figS2" position="float"><label>Figure S2.</label>
<caption><title>Likelihood of classification as trained orientation in V1.</title>
<p>Individual data is plotted on the group data (mean &#x00B1; s.e.m.). The likelihood of classification as trained orientation refers to a continuous measure (rather than a binary decision) of likelihood that a particular brain signal corresponds to the trained or untrained orientation. A oneway repeated measures ANOVA to the decoder&#x2019;s likelihood of classification as the trained orientation showed a significant main effect of time (pre vs. post1 vs. post2; F(2,22)&#x003D;3.780, P&#x003D;0.039). The likelihood of classification as the trained orientation increased immediately after (post1) compared to before (pre) the visual training (t(11)&#x003D;- 2.750, P&#x003D;0.019, uncorrected paired sample t-test). In addition, the likelihood of classification as the trained orientation was significantly higher than chance immediately after training (post1, t(11)&#x003D;3.584, P&#x003D;0.004, one-sample t-test) and 5 minutes after training (post2, t(11)&#x003D;2.400, P&#x003D;0.035, one-sample t-test) but not before training (pre, t(11)&#x003D;0.427, P&#x003D;0.678, one-sample t-test). Consistent with the result from the probability of classification, this result indicates that post-training spontaneous activity in V1 appears more similar to the trained orientation.</p></caption>
<graphic xlink:href="231381_figS2.tif"/></fig>
<fig id="figS3" position="float"><label>Figure S3.</label>
<caption><title>Probability of classifying spontaneous brain activity as trained orientation in V2, V3, V3A, and V4v.</title>
<p>Individual data is plotted on the group data (mean &#x00B1; s.e.m.) for each region of interest. The probability for the trained orientation did not increase immediately after training in V2, V3, V3A, and V4v.</p></caption>
<graphic xlink:href="231381_figS3.tif"/></fig>
<fig id="figS4" position="float"><label>Figure S4.</label>
<caption><title>Accuracy of decoder during leave-one-run-out cross-validation in V1, V2, V3, V3A and V4v.</title>
<p>The decoder was tested through &#x2018;leave-one-run-out&#x2019; cross-validation using brain data obtained from the decoder construction scan. The accuracy of the decoder (mean &#x00B1; s.e.m.) was above chance level (0.5) for all regions of interest (all P values &#x003C; 0.005 before correction, one-sample t-tests; V1, t(11)&#x003D;10.856, P&#x003D;0.000; V2, t(11)&#x003D;11.746, P&#x003D;0.000; V3, t(11)&#x003D;8.345, P&#x003D;0.000; V3A, t(11)&#x003D;3.851, P&#x003D;0.003; V4v, t(11)&#x003D;4.241, P&#x003D;0.001).</p></caption>
<graphic xlink:href="231381_figS4.tif"/></fig>
</sec>
<ref-list><title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Albers</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Toni</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Dijkerman</surname>, <given-names>H. C.</given-names></string-name>, &#x0026; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name> (<year>2013</year>). <article-title>Shared representations for working memory and mental imagery in early visual cortex</article-title>. <source>Current Biology</source>, <volume>23</volume>(<issue>15</issue>), <fpage>1427</fpage>-<lpage>1431</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2013.05.065</pub-id></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Brainard</surname>, <given-names>D. H.</given-names></string-name> (<year>1997</year>). <article-title>The Psychophysics Toolbox</article-title>. <source>Spatial Vision</source>, <volume>10</volume>(<issue>4</issue>), <fpage>433</fpage>-<lpage>436</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Carr</surname>, <given-names>M. F.</given-names></string-name>, <string-name><surname>Jadhav</surname>, <given-names>S. P.</given-names></string-name>, &#x0026; <string-name><surname>Frank</surname>, <given-names>L. M.</given-names></string-name> (<year>2011</year>). <article-title>Hippocampal replay in the awake state: a potential substrate for memory consolidation and retrieval</article-title>. <source>Nature Neuroscience</source>, <volume>14</volume>(<issue>2</issue>), <fpage>147</fpage>-<lpage>153</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.2732</pub-id></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Chelaru</surname>, <given-names>M. I.</given-names></string-name>, <string-name><surname>Hansen</surname>, <given-names>B. J.</given-names></string-name>, <string-name><surname>Tandon</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Conner</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Szukalski</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Slater</surname>, <given-names>J. D.</given-names></string-name>,&#x2026;<string-name><surname>Dragoi</surname>, <given-names>V.</given-names></string-name> (<year>2016</year>). <article-title>Reactivation of visual-evoked activity in human cortical networks</article-title>. <source>J Neurophysiol</source>, <volume>115</volume>(<issue>6</issue>), <fpage>3090</fpage>-<lpage>3100</lpage>. doi:<pub-id pub-id-type="doi">10.1152/jn.00724.2015</pub-id></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Davidson</surname>, <given-names>T. J.</given-names></string-name>, <string-name><surname>Kloosterman</surname>, <given-names>F.</given-names></string-name>, &#x0026; <string-name><surname>Wilson</surname>, <given-names>M. A.</given-names></string-name> (<year>2009</year>). <article-title>Hippocampal replay of extended experience</article-title>. <source>Neuron</source>, <volume>63</volume>(<issue>4</issue>), <fpage>497</fpage>-<lpage>507</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.027</pub-id></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>de Voogd</surname>, <given-names>L. D.</given-names></string-name>, <string-name><surname>Fernandez</surname>, <given-names>G.</given-names></string-name>, &#x0026; <string-name><surname>Hermans</surname>, <given-names>E. J.</given-names></string-name> (<year>2016</year>). <article-title>Awake reactivation of emotional memory traces through hippocampal-neocortical interactions</article-title>. <source>Neuroimage</source>, <volume>134</volume>, <fpage>563</fpage>-<lpage>572</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.04.026</pub-id></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Deuker</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Olligs</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Fell</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kranz</surname>, <given-names>T. A.</given-names></string-name>, <string-name><surname>Mormann</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Montag</surname>, <given-names>C.</given-names></string-name>,&#x2026;<string-name><surname>Axmacher</surname>, <given-names>N.</given-names></string-name> (<year>2013</year>). <article-title>Memory Consolidation by Replay of Stimulus-Specific Neural Activity</article-title>. <source>Journal of Neuroscience</source>, <volume>33</volume>(<issue>49</issue>), <fpage>19373</fpage>-<lpage>19383</lpage>. doi:<pub-id pub-id-type="doi">10.1523/Jneurosci.0414-13.2013</pub-id></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Diba</surname>, <given-names>K.</given-names></string-name>, &#x0026; <string-name><surname>Buzsaki</surname>, <given-names>G.</given-names></string-name> (<year>2007</year>). <article-title>Forward and reverse hippocampal place-cell sequences during ripples</article-title>. <source>Nature Neuroscience</source>, <volume>10</volume>(<issue>10</issue>), <fpage>1241</fpage>-<lpage>1242</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn1961</pub-id></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Diekelmann</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Born</surname>, <given-names>J.</given-names></string-name> (<year>2010</year>). <article-title>The memory function of sleep</article-title>. <source>Nat Rev Neurosci</source>, <volume>11</volume> (<issue>2</issue>), <fpage>114</fpage>-<lpage>126</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nrn2762</pub-id></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Eagleman</surname>, <given-names>S. L.</given-names></string-name>, &#x0026; <string-name><surname>Dragoi</surname>, <given-names>V.</given-names></string-name> (<year>2012</year>). <article-title>Image sequence reactivation in awake V4 networks</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>109</volume>(<issue>47</issue>), <fpage>19450</fpage>-<lpage>19455</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1212059109</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Ekman</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kok</surname>, <given-names>P.</given-names></string-name>, &#x0026; <string-name><surname>de Lange</surname>, <given-names>F. P.</given-names></string-name> (<year>2017</year>). <article-title>Time-compressed preplay of anticipated events in human primary visual cortex</article-title>. <source>Nat Commun</source>, <volume>8</volume>, <fpage>15276</fpage>. doi:<pub-id pub-id-type="doi">10.1038/ncomms15276</pub-id></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Euston</surname>, <given-names>D. R.</given-names></string-name>, <string-name><surname>Tatsuno</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>McNaughton</surname>, <given-names>B. L.</given-names></string-name> (<year>2007</year>). <article-title>Fast-forward playback of recent memory sequences in prefrontal cortex during sleep</article-title>. <source>Science</source>, <volume>318</volume>(<issue>5853</issue>), <fpage>1147</fpage>-<lpage>1150</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1148979</pub-id></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Foster</surname>, <given-names>D. J.</given-names></string-name>, &#x0026; <string-name><surname>Wilson</surname>, <given-names>M. A.</given-names></string-name> (<year>2006</year>). <article-title>Reverse replay of behavioural sequences in hippocampal place cells during the awake state</article-title>. <source>Nature</source>, <volume>440</volume>(<issue>7084</issue>), <fpage>680</fpage>-<lpage>683</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nature04587</pub-id></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Guidotti</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Del Gratta</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Baldassarre</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Romani</surname>, <given-names>G. L.</given-names></string-name>, &#x0026; <string-name><surname>Corbetta</surname>, <given-names>M.</given-names></string-name> (<year>2015</year>). <article-title>Visual Learning Induces Changes in Resting-State fMRI Multivariate Pattern of Information</article-title>. <source>Journal of Neuroscience</source>, <volume>35</volume>(<issue>27</issue>), <fpage>9786</fpage>-<lpage>9798</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.3920-14.2015</pub-id></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Han</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Caporale</surname>, <given-names>N.</given-names></string-name>, &#x0026; <string-name><surname>Dan</surname>, <given-names>Y.</given-names></string-name> (<year>2008</year>). <article-title>Reverberation of recent visual experience in spontaneous cortical waves</article-title>. <source>Neuron</source>, <volume>60</volume>(<issue>2</issue>), <fpage>321</fpage>-<lpage>327</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2008.08.026</pub-id></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Harrison</surname>, <given-names>S. A.</given-names></string-name>, &#x0026; <string-name><surname>Tong</surname>, <given-names>F.</given-names></string-name> (<year>2009</year>). <article-title>Decoding reveals the contents of visual working memory in early visual areas</article-title>. <source>Nature</source>, <volume>458</volume>(<issue>7238</issue>), <fpage>632</fpage>-<lpage>635</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nature07832</pub-id></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Huth</surname>, <given-names>A. G.</given-names></string-name>, <string-name><surname>de Heer</surname>, <given-names>W. A.</given-names></string-name>, <string-name><surname>Griffiths</surname>, <given-names>T. L.</given-names></string-name>, <string-name><surname>Theunissen</surname>, <given-names>F. E.</given-names></string-name>, &#x0026; <string-name><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name> (<year>2016</year>). <article-title>Natural speech reveals the semantic maps that tile human cerebral cortex</article-title>. <source>Nature</source>, <volume>532</volume>(<issue>7600</issue>), <fpage>453</fpage>-<lpage>&#x002B;</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nature17637</pub-id></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Ji</surname>, <given-names>D.</given-names></string-name>, &#x0026; <string-name><surname>Wilson</surname>, <given-names>M. A.</given-names></string-name> (<year>2007</year>). <article-title>Coordinated memory replay in the visual cortex and hippocampus during sleep</article-title>. <source>Nature Neuroscience</source>, <volume>10</volume>(<issue>1</issue>), <fpage>100</fpage>-<lpage>107</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn1825</pub-id></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>McGaugh</surname>, <given-names>J. L.</given-names></string-name> (<year>2000</year>). <article-title>Memory--a century of consolidation</article-title>. <source>Science</source>, <volume>287</volume>(<issue>5451</issue>), <fpage>248</fpage>-<lpage>251</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Reuter</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Schmansky</surname>, <given-names>N. J.</given-names></string-name>, <string-name><surname>Rosas</surname>, <given-names>H. D.</given-names></string-name>, &#x0026; <string-name><surname>Fischl</surname>, <given-names>B.</given-names></string-name> (<year>2012</year>). <article-title>Within-subject template estimation for unbiased longitudinal image analysis</article-title>. <source>Neuroimage</source>, <volume>61</volume> (<issue>4</issue>), <fpage>1402</fpage>-<lpage>1418</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.084</pub-id></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Rosenthal</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Andrews</surname>, <given-names>S. K.</given-names></string-name>, <string-name><surname>Antoniades</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Kennard</surname>, <given-names>C.</given-names></string-name>, &#x0026; <string-name><surname>Soto</surname>, <given-names>D.</given-names></string-name> (<year>2016</year>). <article-title>Learning and Recognition of a Non-conscious Sequence of Events in Human Primary Visual Cortex</article-title>. <source>Current Biology</source>, <volume>26</volume>(<issue>6</issue>), <fpage>834</fpage>-<lpage>841</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2016.01.040</pub-id></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Sasaki</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Nanez</surname>, <given-names>J. E.</given-names></string-name>, &#x0026; <string-name><surname>Watanabe</surname>, <given-names>T.</given-names></string-name> (<year>2010</year>). <article-title>Advances in visual perceptual learning and plasticity</article-title>. <source>Nat Rev Neurosci</source>, <volume>11</volume> (<issue>1</issue>), <fpage>53</fpage>-<lpage>60</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nrn2737</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Schlichting</surname>, <given-names>M. L.</given-names></string-name>, &#x0026; <string-name><surname>Preston</surname>, <given-names>A. R.</given-names></string-name> (<year>2014</year>). <article-title>Memory reactivation during rest supports upcoming learning of related content</article-title>. <source>Proc Natl Acad Sci U S A</source>, <volume>111</volume> (<issue>44</issue>), <fpage>15845</fpage>-<lpage>15850</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1404396111</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Sereno</surname>, <given-names>M. I.</given-names></string-name>, <string-name><surname>Dale</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Reppas</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Kwong</surname>, <given-names>K. K.</given-names></string-name>, <string-name><surname>Belliveau</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Brady</surname>, <given-names>T. J.</given-names></string-name>,&#x2026;<string-name><surname>Tootell</surname>, <given-names>R. B.</given-names></string-name> (<year>1995</year>). <article-title>Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging</article-title>. <source>Science</source>, <volume>268</volume>(<issue>5212</issue>), <fpage>889</fpage>-<lpage>893</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Shibata</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Sasaki</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Kawato</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Watanabe</surname>, <given-names>T.</given-names></string-name> (<year>2016</year>). <article-title>Neuroimaging Evidence for 2 Types of Plasticity in Association with Visual Perceptual Learning</article-title>. <source>Cereb Cortex</source>, <volume>26</volume>(<issue>9</issue>), <fpage>3681</fpage>-<lpage>3689</lpage>. doi:<pub-id pub-id-type="doi">10.1093/cercor/bhw176</pub-id></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Staresina</surname>, <given-names>B. P.</given-names></string-name>, <string-name><surname>Alink</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name>, &#x0026; <string-name><surname>Henson</surname>, <given-names>R. N.</given-names></string-name> (<year>2013</year>). <article-title>Awake reactivation predicts memory in humans</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>110</volume>(<issue>52</issue>), <fpage>21159</fpage>-<lpage>21164</lpage>. doi:<pub-id pub-id-type="doi">Doi 10.1073/Pnas.1311989110</pub-id></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Tambini</surname>, <given-names>A.</given-names></string-name>, &#x0026; <string-name><surname>Davachi</surname>, <given-names>L.</given-names></string-name> (<year>2013</year>). <article-title>Persistence of hippocampal multivoxel patterns into postencoding rest is related to memory</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>110</volume>(<issue>48</issue>), <fpage>19591</fpage>-<lpage>19596</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1308499110</pub-id></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Tootell</surname>, <given-names>R. B.</given-names></string-name>, <string-name><surname>Hadjikhani</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Hall</surname>, <given-names>E. K.</given-names></string-name>, <string-name><surname>Marrett</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Vanduffel</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Vaughan</surname>, <given-names>J. T.</given-names></string-name>, &#x0026; <string-name><surname>Dale</surname>, <given-names>A. M.</given-names></string-name> (<year>1998</year>). <article-title>The retinotopy of visual spatial attention</article-title>. <source>Neuron</source>, <volume>21</volume> (<issue>6</issue>), <fpage>1409</fpage>-<lpage>1422</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Tootell</surname>, <given-names>R. B. H.</given-names></string-name>, <string-name><surname>Mendola</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Hadjikhani</surname>, <given-names>N. K.</given-names></string-name>, <string-name><surname>Ledden</surname>, <given-names>P. J.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>A. K.</given-names></string-name>, <string-name><surname>Reppas</surname>, <given-names>J. B.</given-names></string-name>,&#x2026;<string-name><surname>Dale</surname>, <given-names>A. M.</given-names></string-name> (<year>1997</year>). <article-title>Functional analysis of V3A and related areas in human visual cortex</article-title>. <source>Journal of Neuroscience</source>, <volume>17</volume>(<issue>18</issue>), <fpage>7060</fpage>-<lpage>7078</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Xu</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Jiang</surname>, <given-names>W. C.</given-names></string-name>, <string-name><surname>Poo</surname>, <given-names>M. M.</given-names></string-name>, &#x0026; <string-name><surname>Dan</surname>, <given-names>Y.</given-names></string-name> (<year>2012</year>). <article-title>Activity recall in a visual cortical ensemble</article-title>. <source>Nature Neuroscience</source>, <volume>15</volume>(<issue>3</issue>), <fpage>449</fpage>-<lpage>U141</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.3036</pub-id></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Yacoub</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Harel</surname>, <given-names>N.</given-names></string-name>, &#x0026; <string-name><surname>Ugurbil</surname>, <given-names>K.</given-names></string-name> (<year>2008</year>). <article-title>High-field fMRI unveils orientation columns in humans</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>105</volume>(<issue>30</issue>), <fpage>10607</fpage>-<lpage>10612</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.0804110105</pub-id></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Yamashita</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Sato</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Yoshioka</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Tong</surname>, <given-names>F.</given-names></string-name>, &#x0026; <string-name><surname>Kamitani</surname>, <given-names>Y.</given-names></string-name> (<year>2008</year>). <article-title>Sparse estimation automatically selects voxels relevant for the decoding of fMRI activity patterns</article-title>. <source>Neuroimage</source>, <volume>42</volume>(<issue>4</issue>), <fpage>1414</fpage>-<lpage>1429</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.05.050</pub-id></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Yao</surname>, <given-names>H. S.</given-names></string-name>, <string-name><surname>Shi</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Han</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Gao</surname>, <given-names>H. F.</given-names></string-name>, &#x0026; <string-name><surname>Dan</surname>, <given-names>Y.</given-names></string-name> (<year>2007</year>). <article-title>Rapid learning in cortical coding of visual scenes</article-title>. <source>Nature Neuroscience</source>, <volume>10</volume>(<issue>6</issue>), <fpage>772</fpage>-<lpage>778</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn1895</pub-id></mixed-citation></ref>
</ref-list>
</back>
</article>