<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/335216</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Confirmatory Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Bioinformatics</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Evaluation of Deep Learning Strategies for Nucleus Segmentation in Fluorescence Images</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Caicedo</surname>
<given-names>Juan C.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Roth</surname>
<given-names>Jonathan</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Goodman</surname>
<given-names>Allen</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9615-0799</contrib-id>
<name>
<surname>Becker</surname>
<given-names>Tim</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5126-5805</contrib-id>
<name>
<surname>Karhohs</surname>
<given-names>Kyle W.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>McQuin</surname>
<given-names>Claire</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Singh</surname>
<given-names>Shantanu</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1555-8261</contrib-id>
<name>
<surname>Carpenter</surname>
<given-names>Anne E.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Broad Institute of MIT and Harvard</institution></aff>
<aff id="a2"><label>2</label><institution>Technical University of Munich</institution></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label>corresponding author: <email>anne@broadinstitute.org</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2018</year>
</pub-date>
<elocation-id>335216</elocation-id>
<history>
<date date-type="received">
<day>30</day>
<month>5</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>30</day>
<month>5</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>31</day>
<month>5</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="335216.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Identifying nuclei is often a critical first step in analyzing microscopy images of cells, and classical image processing algorithms are still commonly used for this task. Recent studies indicate that deep learning may yield superior accuracy, but its performance has not been evaluated for high-throughput nucleus segmentation in large collections of images. We compare two deep learning strategies for identifying nuclei in fluorescence microscopy images (U-Net and DeepCell) alongside a classical approach that does not use machine learning. We measure accuracy, types of errors, and computational complexity to benchmark these approaches on a large data set. We publicly release the set of 23,165 manually annotated nuclei and source code to reproduce the results. Our evaluation shows that U-Net outperforms both pixel-wise classification networks and classical algorithms. Although deep learning requires more computation and annotation time than classical algorithms, it improves accuracy and halves the number of errors.</p>
</abstract>
<counts>
<page-count count="21"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Main</title>
<p>Image analysis is a powerful tool in cell biology to collect quantitative measurements in time and space with precision, speed, and sensitivity. From image-based assays to high-content screening <sup><xref rid="c1" ref-type="bibr">1</xref>,<xref rid="c2" ref-type="bibr">2</xref></sup>, microscopy images have led to understanding genetic perturbations, pursuing drug discovery, and phenotyping cells in biomedical applications and cell biology research <sup><xref rid="c3" ref-type="bibr">3</xref>,<xref rid="c4" ref-type="bibr">4</xref></sup>. The most widely used quantitative imaging technique in biological laboratories is fluorescence imaging; with automation it can easily produce terabytes of primary research data <sup><xref rid="c5" ref-type="bibr">5</xref></sup>. Accurate and automated analysis methods are key to successfully quantify relevant biology in such large image collections.</p>
<p>One critical step in quantifying fluorescent images is often the identification of the nucleus of each cell with a DNA stain, and there is a long history of research efforts to designing and improving nuclear and cellular segmentation <sup><xref rid="c6" ref-type="bibr">6</xref></sup>. One of the most commonly used strategies for nucleus segmentation is Otsu&#x2019;s thresholding method <sup><xref rid="c7" ref-type="bibr">7</xref></sup> followed by seeded watershed <sup><xref rid="c8" ref-type="bibr">8</xref>,<xref rid="c9" ref-type="bibr">9</xref></sup>, because of its effectiveness, simplicity of use and computational efficiency. Machine learning-based segmentation methods have also been introduced for segmenting cells <sup><xref rid="c10" ref-type="bibr">10</xref></sup>, which typically require annotated examples in the form of segmentation masks or interactive scribbles. Many of these strategies are readily available in various bioimage software packages <sup><xref rid="c11" ref-type="bibr">11</xref></sup>, including open source options such as CellProfiler <sup><xref rid="c12" ref-type="bibr">12</xref></sup>, Ilastik <sup><xref rid="c10" ref-type="bibr">10</xref></sup>, and ImageJ/Fiji <sup><xref rid="c13" ref-type="bibr">13</xref></sup>, facilitating their adoption in routine biological research.</p>
<p>Despite widespread adoption, segmentation tools in biology generally do yield non-trivial amounts of segmentation error. These may silently propagate to downstream analyses, yielding unreliable measures or systemic noise that is difficult to quantify and factor out. There are several causes for segmentation errors. First, existing algorithms have limitations due to the assumptions made in the computational design that do not always hold, such as thresholding methods that assume bimodal intensity distributions, or region growing that expects clearly separable boundaries. Second, the most popular solutions for nucleus segmentation were originally formulated and adopted several decades ago when the biological systems and phenotypes of interest were often simpler; however, as biology pushes the limits of high-throughput cellular and tissue models, natural and subtle variations of biologically meaningful phenotypes are more challenging to segment. Finally, algorithms are usually configured using a few &#x2013;hopefully representative&#x2013; images from the experiment, but variations in signal quality and the presence of noise pose challenges to the robustness and reliability of the solution at large scale.</p>
<p>The ideal approach to nucleus segmentation would be a generic, robust and fully automated solution that is as reliable as modern face detection technologies deployed in mobile applications and social networks. The current state of the art in face detection and many other computer vision tasks is based on deep learning <sup><xref rid="c14" ref-type="bibr">14</xref></sup>, which has demonstrated high accuracy, even surpassing human-level performance in certain tasks <sup><xref rid="c15" ref-type="bibr">15</xref></sup>. Several models based on deep learning have already been proposed for cell segmentation in biological applications, most notably U-Net <sup><xref rid="c16" ref-type="bibr">16</xref></sup> and DeepCell <sup><xref rid="c17" ref-type="bibr">17</xref></sup>, which are based on convolutional neural networks.</p>
<p>In this paper, we evaluate the performance of these two deep learning-based strategies, and investigate their potential to improve the accuracy of nucleus segmentation in fluorescence images. Expert biologists on our team hand-annotated more than 20,000 nuclei in an image collection of 200 images of the DNA channel from a large image-based chemical screen, sampled from a diverse set of treatments <sup><xref rid="c18" ref-type="bibr">18</xref></sup>. Our evaluation methodology measures the success of identifying objects, whereas previous studies focused on pixel or boundary accuracy measures <sup><xref rid="c17" ref-type="bibr">17</xref>,<xref rid="c19" ref-type="bibr">19</xref></sup> that may ignore certain object-level errors. We also analyze different types of segmentation errors in each method, computational efficiency, and the impact of quantity and quality of training data for creating deep learning models.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>Identifying nuclei in an image is best framed as an &#x201C;instance segmentation&#x201D; problem <sup><xref rid="c20" ref-type="bibr">20</xref></sup>, where the challenge is to find distinct regions corresponding to a single class of objects: the nucleus. Semantic segmentation <sup><xref rid="c21" ref-type="bibr">21</xref></sup>, which splits an image to regions of various classes without requiring objects to be separated, is not helpful for nucleus segmentation because there is only one class, and touching nuclei would not be distinguished from each other. Both of the deep learning strategies evaluated in this paper are cases of instance segmentation that formulate nucleus segmentation as a boundary detection problem.</p>
<p>The boundary detection problem consists of identifying three different types of pixels in an image of nuclei: 1) background, 2) interior of nuclei, and 3) boundaries of nuclei. This formulation simplifies the problem of instance segmentation into a three-class, pixel-wise classification problem (<xref rid="fig1" ref-type="fig">Figure 1</xref>). If the boundary mask is correctly predicted, individual instances of nuclei can be recovered from the interior mask using a connected component labeling algorithm <sup><xref rid="c22" ref-type="bibr">22</xref></sup>, thus successfully distinguishing two or more touching nuclei. Note that while we pose this as a pixel-wise classification problem of boundaries, we evaluate the performance on the success of identifying entire objects.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Strategy of the evaluated deep learning approaches.</title>
<p>Our main goal is to follow the popular strategy of segmenting each nucleus and micronucleus as a distinct entity, regardless of whether it shares the same cell body with another nucleus. It is generally possible to group nuclei within a single cell using other channels of information in a post-processing step if the assay requires it. a) Images of the DNA channel are manually annotated, labeling each nucleus as a separate object. Then, labeled instances are transformed to masks for background, nucleus interior and boundaries. A convolutional neural network (CNN) is trained using the images and their corresponding masks. b) The trained CNN generates predictions for the three class classification problem. Each pixel belongs to only one of the three categories. In post-processing, the predicted boundary mask is used to identify each individual instance of a nucleus.</p></caption>
<graphic xlink:href="335216_fig1.tif"/>
</fig>
<p>A diverse set of convolutional neural network (CNN) architectures can address pixel-wise classification; here we evaluate two, representing two families of prominent models: DeepCell <sup><xref rid="c17" ref-type="bibr">17</xref></sup> and U-Net <sup><xref rid="c16" ref-type="bibr">16</xref></sup> (Online Methods). We use the same preprocessing and postprocessing pipeline when evaluating both CNN models, so differences in performance are explained by architectural choices only.</p>
<sec id="s2a">
<title>Deep learning improves nucleus segmentation accuracy</title>
<p>Overall, we find that deep learning models exhibit higher accuracy than classical segmentation algorithms, both in terms of the number of correctly identified objects, as well as the localization of boundaries of each nucleus (<xref rid="fig2" ref-type="fig">Figure 2</xref>). We evaluate these properties using the F1 score (the harmonic average of precision and recall) averaged over increasingly stringent thresholds of overlap between the ground truth and prediction. U-Net and DeepCell obtained higher average F1 scores, yielding 0.85 and 0.78 respectively, versus 0.74 and 0.72 for the advanced and basic CellProfiler pipelines selected as a baseline (see Online Methods). This 20&#x0025; improvement is a significant margin when experiments are run at large scale with thousands of images. U-Net yields a higher average F1 score across larger thresholds (<xref rid="fig2" ref-type="fig">Figure 2a</xref>), indicating that the boundaries of objects are more precisely mapped to the correct contours compared to the other methods.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Segmentation performance of four strategies compared against ground-truth expert segmentations.</title>
<p>a) Average F1 score vs. nucleus coverage for U-Net (green), DeepCell (yellow), CellProfiler advanced (red), and CellProfiler basic (blue). The y axis is average F1 score (higher is better), which measures the proportion of correctly segmented objects. The x axis represents intersection-over-union (IoU) thresholds as a measurement of how well aligned the ground truth and estimated segmentations must be to count a correctly detected nucleus. Higher thresholds indicate stricter boundary matching. Notice that average F1 scores remain nearly constant up to IoU=0.80; at higher thresholds, performance decreases sharply, which indicates that the proportion of correctly segmented objects decreases when stricter boundaries are required to count a positive detection. b) Example segmentations obtained with each of the four evaluated methods sampled to illustrate performance differences. Segmentation boundaries are in red, and errors are indicated with yellow arrows.</p></caption>
<graphic xlink:href="335216_fig2.tif"/>
</fig>
<p>The most common errors for all methods are merged objects, which occur when the segmentation fails to separate two or more touching nuclei (yellow arrows in <xref rid="fig2" ref-type="fig">Figure 2b</xref>). Deep learning strategies tend to reduce this type of error (more in <xref rid="fig3" ref-type="fig">Figure 3c</xref>) and provide tighter and smoother segmentation boundaries than those estimated by global Otsu thresholding and declumping, which is at the core of the baseline CellProfiler pipelines for nucleus segmentation.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Analysis of segmentation errors (missed, splits, merged objects).</title>
<p>The 5,720 nuclei in the test set were used in this analysis. a) Counts of missed nuclei by object size (see table). Missed objects in this analysis were counted using an IoU threshold of 0.7, which offers a good balance between strict nucleus coverage and robustness to noise in ground truth annotations. b) Example image illustrating sizes of nuclei. c) Counts of merged and split nuclei. These errors are identified by masks that cover multiple objects with at least 0.1 IoU. d) Example merges and splits.</p></caption>
<graphic xlink:href="335216_fig3.tif"/>
</fig>
<p>Qualitatively, nucleus boundaries predicted by the U-Net appear to define objects better than those produced by human annotators using an assistive annotation tool, which can introduce boundary artifacts (Online Methods). Neural nets can learn to provide edges closer to the nuclei with fewer gaps and better-delineated shapes, despite being trained with examples that have such boundary artifacts, showing ability to generalize beyond noise. Overcoming the limitation of assisted annotations is a major strength of this approach because fixing boundary artifacts by hand in the training data is very time consuming. We suspect that the accuracy drop observed in the segmentation performance plot at IoU=0.85 (<xref rid="fig2" ref-type="fig">Figure 2a</xref>) may be partly explained by inaccurate boundaries in ground truth annotations, i.e. improved segmentations may be unfairly scored at high thresholds.</p>
</sec>
<sec id="s2b">
<title>Deep learning excels at correct splitting of adjacent nuclei</title>
<p>Deep learning methods make fewer segmentation mistakes compared to classical pipelines, effectively correcting most of their typical errors (<xref rid="fig3" ref-type="fig">Figure 3a</xref>). Here, an error is defined as when a nucleus in the ground truth is missed in an estimated segmentation mask after applying a minimum IoU threshold of 0.7. By this metric, U-Net achieves an error rate of 8.1&#x0025;, DeepCell 14.0&#x0025;, advanced CellProfiler 15.5&#x0025; and basic CellProfiler 20.1&#x0025;. These results are consistent with the evaluation of accuracy performed at multiple IoU thresholds, indicating that U-Net obtains significantly better performance.</p>
<p>To understand the performance differences among the evaluated methods, we categorized missed objects by size (<xref rid="fig3" ref-type="fig">Figure 3a, b</xref>) and segmentation errors by type (merges vs. splits) (<xref rid="fig3" ref-type="fig">Figure 3b, c</xref>). An object is missed when the segmentation does not meet the minimum IoU threshold criterion. A merge is counted when one estimated mask is found covering more than one ground truth mask. Similarly, a split is counted when a ground truth mask is being covered by more than one estimated mask. Note that splits and merges are a subset of the total number of errors, and partially overlap with the number of missed objects. That is, some splits and all merges result in one or more missing objects, but not all missing objects are a result of a split or merge.</p>
<p>Deep learning corrects almost all of the errors made by classical pipelines for larger nuclei. We also note that all methods usually fail to capture tiny nuclei correctly (generally, micronuclei, which are readily confounded with debris or artifacts, and represent about 15&#x0025; of all objects in the test set) (<xref rid="fig3" ref-type="fig">Figure 3a</xref>). Interestingly, although they make almost the same number of mistakes (14.0&#x0025; vs 15.5&#x0025;), DeepCell tends to accumulate errors for tiny nuclei only while the advanced CellProfiler pipeline tends to make errors across all sizes (<xref rid="fig3" ref-type="fig">Figure 3b</xref>). Tiny nuclei may be particularly hard to segment using the boundary detection approach because most of the pixels are encoded in the boundary, and very few pixels are left for the interior which is used to produce segmentation masks. This might be addressed in part by increasing image resolution, either optically or computationally (Discussion).</p>
<p>Both deep learning approaches are effective at recognizing boundaries to separate touching nuclei and correct typical error modes of classical algorithms: merges and splits (<xref rid="fig3" ref-type="fig">Figure 3c</xref> and <xref rid="fig3" ref-type="fig">3d</xref>). Split errors produced by the advanced CellProfiler pipeline reveal a trade-off when configuring the parameters of classical algorithms: in order to fix merges we have to accept some more splits. A similar situation happens with U-Net: it has learned to separate clumped nuclei very effectively because the boundary class has 10 times more weight in the loss function (Online Methods), which at the same time forces the network to make some splits to avoid the cost of missing real boundaries.</p>
</sec>
<sec id="s2c">
<title>More training data improves accuracy and reduces errors</title>
<p>Using U-Net models only &#x2013;given their better performance and faster running times&#x2013; we found that training with just two images performs more accurately than an advanced CellProfiler pipeline (<xref rid="fig4" ref-type="fig">Figure 4a</xref>). This is consistent with previous reports on DeepCell <sup><xref rid="c17" ref-type="bibr">17</xref></sup> and U-Net <sup><xref rid="c16" ref-type="bibr">16</xref></sup>, which were designed to learn from few images by processing patches and using data augmentation. Since training a convolutional neural network requires the additional effort of manually annotating example images for learning, limiting the investment of time from expert biologists is valuable.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Impact of the number of annotated images used for training a U-Net model.</title>
<p>Basic augmentations include flips, 90 degree rotations and random crops. Extra augmentations include the basic plus elastic deformations. Accuracy improves as a function of the number of training images, up to a plateau around 20 images (representing roughly 2,000 nuclei). b) Segmentation errors are reduced overall as the number of training images increases, but the impact differs for merges vs. splits. The advanced CellProfiler pipeline is shown as dotted lines throughout. Results are reported using the validation set to prevent over-optimizing models in the test set (holdout). For all experiments, we randomly sampled (with replacement) subsets (n = 2, 4, 6, 8, 10, 20, 40, 60, 80, 100) of images from the training set (n=100) and repeated 10 times to evaluate performance. Data points in plots are the mean of repetitions. Although the percent overlap between the random samples increases with increasing sample size, and is 100&#x0025; for n=100, we nonetheless kept the number of repeats fixed (=10) for consistency.</p></caption>
<graphic xlink:href="335216_fig4.tif"/>
</fig>
<p>Providing more annotated examples improved segmentation accuracy and reduced the number of errors significantly (<xref rid="fig4" ref-type="fig">Figure 4</xref>). Accuracy improves with more data, gaining a few points of performance as more annotated images are used, up to the full 100 images in the training set (<xref rid="fig4" ref-type="fig">Figure 4a</xref>). We found little difference in this trend whether using basic data augmentation vs. using extra augmentations based on elastic deformations (Online Methods).</p>
<p>Segmentation errors are reduced significantly with more annotated examples, by roughly half (<xref rid="fig4" ref-type="fig">Figure 4b</xref>), but as above, even training with two images produces results better than the advanced CellProfiler baseline. Touching nuclei particularly benefit from more training data, which helps to reduce the number of merge errors. The trend for split errors is to increase with more data as an effect of learning to recognize difficult boundaries; however, this represents a very small fraction of the total number of errors that are still fewer than the number of splits made by the advanced CellProfiler pipeline.</p>
</sec>
<sec id="s2d">
<title>Providing a variety of training images improves generalization</title>
<p>We found that training with images that exhibit different types of noise produces models that transfer better to other sets (<xref rid="fig5" ref-type="fig">Figure 5a</xref>). In contrast, training on an image set that has homogeneous acquisition conditions does not transfer as well to other experiments (<xref rid="fig5" ref-type="fig">Figure 5b</xref>). We tested U-Net models trained on one image set and evaluated their performance when transferring to another set. In one case we took images of nuclei from a prior study (&#x201C;Van Valen&#x2019;s Set&#x201D;)<sup><xref rid="c17" ref-type="bibr">17</xref></sup>, representing different experiments and including representative examples of diverse signal qualities, cell lines, and acquisition conditions (<xref rid="fig5" ref-type="fig">Figure 5d</xref>). In the other case we used the BBBC022 image collection which exhibits homogeneous signal quality and was acquired under similar technical conditions (<xref rid="fig5" ref-type="fig">Figure 5c</xref>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Signal quality is the main challenge when transferring models across experiments</title>
<p>Performance differences when models are trained and evaluated in different experiments. a) Models evaluated on the BBBC022 test set, including a U-Net trained on the same set, another U-Net trained on Van Valen&#x2019;s set, and a CellProfiler pipeline. The results indicate that transferring the model from one screen to another can bring improved performanceModels evaluated in Van Valen&#x2019;s test set, including CellProfiler baselines adapted to this set (Online Methods), a U-Net trained on the same set, and another U-Net trained on BBBC022. The results illustrate the challenges of dealing with large signal variation. c) Example images from BBBC022 showing homogeneous signal with uniform background. d) Example images from Van Valen&#x2019;s set illustrating various types of realistic artifacts, such as background noise and high signal variance. Number of training images: 100 in BBBC022 and 9 in Van Valen. Number of test images: 50 in BBBC022 and 3 in Van Valen.</p></caption>
<graphic xlink:href="335216_fig5.tif"/>
</fig>
<p>A model trained only on the 9 diverse images of Van Valen&#x2019;s set generalizes well to test images in BBBC022, improving performance over the baseline (<xref rid="fig5" ref-type="fig">Figure 5a</xref>) and reaching comparable performance to the model trained on BBBC022 training images. Note that training a network on images of BBBC022 improves performance with respect to the CellProfiler baseline. The transferred model does not fix all the errors, likely because the number of training examples is limited. Nevertheless, the transferred performance indicates that it is possible to reuse models across experiments to improve segmentation accuracy.</p>
<p>A transfer from the more homogenous BBBC022 set to the more diverse Van Valen set is less successful: a model trained with 100 examples from the BBBC022 set fails to improve on the test set of Van Valen&#x2019;s images despite the availability of more data (<xref rid="fig5" ref-type="fig">Figure 5b</xref>). This demonstrates the challenges of dealing with varying signal quality, which is a frequent concern in high-throughput and high-content screens. The large gap in performance is explained by varying signal conditions (<xref rid="fig5" ref-type="fig">Figure 5c, d</xref>): because the U-Net did not observe these variations during training, it fails to correctly segment test images.</p>
<p>The CellProfiler pipelines also confirm the difficulty of handling noisy images. A single pipeline cannot deal with all variations in Van Valen&#x2019;s test set, requiring the adjustment of advanced settings and the splitting of cases into two different pipelines (Online Methods). In BBBC022, a single pipeline works well due in part to the homogeneity of signal in this collection; the errors are due to challenging phenotypic variations, such as tiny nuclei or clumped objects.</p>
</sec>
<sec id="s2e">
<title>Deep learning needs more computing and annotation time than classical methods</title>
<p>Although we found the performance of deep learning to be favorable in terms of improving segmentation accuracy, we also found that this comes at higher computational cost and annotation time. First, deep learning requires significantly more time to prepare training data with manual annotations (<xref rid="fig6" ref-type="fig">Figure 6a</xref>). Second, deep learning needs the researchers to train a model and tune its parameters (<xref rid="fig6" ref-type="fig">Figure 6b</xref>), usually with special hardware. Third, when a model has been trained, it is slower to run on new images than classical algorithms (<xref rid="fig6" ref-type="fig">Figure 6c</xref>). However, running times can be accelerated using graphic cards, which makes the technique usable in practice (<xref rid="fig6" ref-type="fig">Figure 6d</xref>).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Evaluation of the time needed to create annotations, train, and run segmentation models.</title>
<p>a) Preparation time measures hands on, expert time annotating images or creating CellProfiler pipelines. Manually annotating 100 training images with about 11,500 nuclei requires significantly longer times. b) Neural networks need to be trained while CellProfiler pipelines do not need additional processing. Training algorithms were run on a single NVIDIA Titan X GPU. DeepCell trains an ensemble of 5 models, which was used in all other evaluations. c) CellProfiler pipelines and trained neural networks are run on new images using only CPU cores to measure the computational cost of segmenting a single image. Deep learning needs significantly more resources to accomplish the task. d) Deep learning models can be accelerated using GPUs, which have thousands of computing cores that allow algorithms to run operations in parallel. This reduces significantly the elapsed time, making it practical and even faster than classical solutions. The GPU used in this evaluation is a single NVIDIA Titan X GPU.</p></caption>
<graphic xlink:href="335216_fig6.tif"/>
</fig>
<p>We observed that the time invested by experts for annotating images is significantly longer than configuring CellProfiler segmentation pipelines (<xref rid="fig6" ref-type="fig">Figure 6a</xref>). We estimate that manually annotating 100 images for training (&#x02DC;11,500 objects) takes 50 hours of work using an assisted-segmentation tool (Online Methods). In contrast, a basic CellProfiler pipeline can be calibrated in 15 to 30 minutes of interaction with the tool, setting up a configuration that even users without extensive experience nor computational expertise could complete. CellProfiler is very flexible and allows users to add more modules in order to correct certain errors and factor out artifacts, creating an advanced pipeline that can take from 1 to 3 hours.</p>
<p>Training deep learning models takes substantial computing time on GPUs, while CellProfiler pipelines do not need any additional training or post-processing (<xref rid="fig6" ref-type="fig">Figure 6b</xref>). In our study, the deep learning models under evaluation (Online Methods) are big enough to need a GPU for training, but light enough to be trained in a few hours. In particular, a U-Net can be trained in a single Nvidia Titan X GPU in just one hour, while DeepCell takes 25 hours (an ensemble of 5 models as suggested in the original work <sup><xref rid="c17" ref-type="bibr">17</xref></sup>, which required 5 hours each in our experiments). Also, training models may need preliminary experiments to calibrate hyperparameters of the neural network (e.g. learning rate, batch size, epochs), which adds more hands-on time.</p>
<p>When segmenting new images using CPU cores, deep learning models are slower than CellProfiler pipelines (<xref rid="fig6" ref-type="fig">Figure 6c</xref>). The computational complexity in terms of space (memory) and time (operations) of a convolutional neural network is proportional to the number of layers, the number of filters, and the size of images. As these architectures get deeper and more complex, they involve more operations to produce the final result, and thus require more computing power. This is in contrast to classical segmentation algorithms whose thresholding and filtering operations have relatively limited computing requirements that scale with the size of images. Even with 8 times more cores, a U-Net takes 10.1 seconds to segment a single image, which results in about 20 times more computing power requirements than the advanced CellProfiler pipeline. CellProfiler pipelines are run in a single CPU core and take 2.2 and 4.3 seconds for the basic and advanced pipelines respectively.</p>
<p>Using GPU acceleration can significantly speed up the computations of deep learning models, making them very usable and efficient in practice (<xref rid="fig6" ref-type="fig">Figure 6d</xref>). Segmenting a single image with a U-Net model takes only 0.6 seconds on a Nvidia Titan X GPU, improving computation times by a factor of 16X. Note that no batching was used for prediction, which can accelerate computation of groups of images even further. This result shows that a deep learning model is faster than classical algorithms when using appropriate hardware. The DeepCell model takes significantly more computing time even with GPU acceleration, which is explained by the patch-based design of the model not fully exploiting parallel hardware. However, DeepCell is more memory efficient and could process larger images without tiling.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Our results show that deep learning strategies improve segmentation accuracy and reduce the number of errors significantly as compared to baselines based on classical image processing algorithms. In our benchmark, U-Net provided the best performance in all the tests that measured accuracy and error rates. Despite requiring significant annotation effort and computational cost, deep learning methods can have a positive impact on the quality and reliability of the measurements extracted from fluorescence images.</p>
<sec id="s3a">
<title>Improved accuracy</title>
<p>The results show that U-Net outperforms DeepCell in all our benchmarks. This can be explained by several architectural differences between the two models: U-Net makes special use of skip connections, which are known to improve training stability, speed, and accuracy of convolutional neural networks <sup><xref rid="c23" ref-type="bibr">23</xref></sup>. In contrast, DeepCell follows a simpler design with a stack of convolutional layers. Also, the U-Net architecture has a total of 7.7 million parameters versus 2.5 million for DeepCell, which translates in more learning capacity in the U-Net. More parameters may make the model prone to overfitting; however, we did not observe evidence of this being a problem in our experiments with a single dataset, i.e., test performance did not drop significantly with respect to training and validation when running evaluations in the BBBC022 set.</p>
<p>The analysis of errors indicates that deep learning can fix most of the segmentation errors observed in classical algorithms, especially merges. One special type of error that represents a challenge for both deep learning models is the segmentation of tiny nuclei. If needed for a given application, this could be solved by increasing the resolution of images, either during acquisition <sup><xref rid="c24" ref-type="bibr">24</xref></sup>/sup> or with computational methods such as resizing images to make objects look bigger. Alternatively, different loss functions adapted to this problem might be designed.</p>
</sec>
<sec id="s3b">
<title>Training data</title>
<p>In our evaluation, the amount of training data was shown to be an important factor to reduce the number of errors. Our results confirm that training a neural network with only a few images is enough to get improved performance relative to non-deep learning baselines. However, in order to improve accuracy and leverage the learning capacity of deep learning models, more data is required. Importantly, a neural network can also be reused across experiments, as long as the training data incorporates variations in morphological phenotypes as well as variations in signal quality and acquisition conditions. We argue that a single deep learning model might be constructed to address all the challenges of nucleus segmentation in fluorescence images if a diverse database of annotated examples were to be collected to incorporate these two critical axes of variation. We advocate for collecting that data collaboratively from different research labs, so everyone will benefit from a shared resource that can be used for training robust neural networks. We have begun such an effort via the 2018 Data Science Bowl <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/c/data-science-bowl-2018/">www.kaggle.com/c/data-science-bowl-2018/</ext-link>.</p>
</sec>
<sec id="s3c">
<title>Computational cost</title>
<p>We observed that U-Net is faster than DeepCell when using GPU acceleration despite U-Net&#x2019;s largernumber of parameters. By design, DeepCell processes images using a patch-based approach for single pixel classification, which involves redundant computations and does not take full advantage of GPU parallelism. CellProfiler pipelines do not require acceleration, but are limited by the accuracy of the underlying algorithms.</p>
<p>Deep learning models generally run a higher computational cost. GPUs can be useful in microscopy laboratories for accelerating accurate neural network models; if acquisition or maintenance is prohibitive, cloud computing allows laboratories to run deep learning models using remote computing resources on demand. Adopting these solutions will equip biologists with essential tools for many other image analysis tasks based on artificial intelligence in the future.</p>
</sec>
</sec>
<sec id="s4">
<title>Online Methods</title>
<sec id="s4a">
<title>Experimental Design</title>
<p>The main dataset in this paper, BBBC022, was split in three subsets, using 50&#x0025; of the images for training, 25&#x0025; for validation, and 25&#x0025; for testing (holdout set). All optimization steps of the deep learning strategies were tuned using the training and validation sets, and the reported results are obtained with final evaluations using the holdout test set (unless otherwise indicated).</p>
<p>We evaluate four segmentation strategies, two based on deep learning, and two baseline pipelines based on the CellProfiler software. The deep learning strategies are U-Net <sup><xref rid="c16" ref-type="bibr">16</xref></sup>, representing a family of neural networks originally designed for segmentation; and DeepCell <sup><xref rid="c17" ref-type="bibr">17</xref></sup>, representing another family of neural networks for patch-based, pixel-wise classification. The baseline segmentation pipelines include an advanced settings mode and a basic settings mode.</p>
<p>With trained deep learning models and calibrated CellProfiler pipelines, we proceeded to segment all images in the test set. To evaluate and compare the estimated masks we use the average F1 score and modes of error.</p>
</sec>
<sec id="s4b">
<title>Image Collection</title>
<p>The image set is a high-throughput experiment of chemical perturbations on U2OS cells, comprising 1,600 bioactive compounds <sup><xref rid="c18" ref-type="bibr">18</xref></sup>. The effect of treatments was imaged using the Cell Painting assay <sup><xref rid="c25" ref-type="bibr">25</xref></sup> which labels cell structures using six stains, including Hoechst for nuclei. From this image collection, we randomly sampled 200 fields of view of the DNA channel, each selected from a different compound. By doing so, phenotypes induced by 200 distinct chemical perturbations were sampled.</p>
<p>The original image collection is part of the Broad Bioimage Benchmark Collection, with accession number BBBC022, and publicly available at <ext-link ext-link-type="uri" xlink:href="https://data.broadinstitute.org/bbbc/BBBC022/">https://data.broadinstitute.org/bbbc/BBBC022/</ext-link>. We will contribute the set of manually annotated nuclei to this collection.</p>
</sec>
<sec id="s4c">
<title>Expert Annotations</title>
<p>Each image in the sampled subset was reviewed and manually annotated by PhD-level expert biologists. Annotations were made to label each single nucleus as a distinguishable object, even if nuclei happen to be clumped together or appear to be touching each other. Nuclei of all sizes and shapes were included as our goal was to densely annotate every single nucleus that can be recognized in the sampled images, regardless of its phenotype. In this way, a wide variety of phenotypes was covered, including micronuclei, toroid nuclei, fragmented nuclei, round nuclei, and elongated nuclei, among others <sup><xref rid="c18" ref-type="bibr">18</xref></sup>.</p>
<p>We created a web-based annotation tool to complete single-object masks for all images. Our user interface allows the expert to zoom in and out to double check details, and also presents the annotation mask overlaid on top of the original image using a configurable transparency layer. Importantly, our annotation tool is based on assisted segmentation based on superpixels, which are computed based on intensity features to facilitate user interactions. The source code for the annotation tools is made publicly available.</p>
</sec>
<sec id="s4d">
<title>DeepCell</title>
<p>DeepCell <sup><xref rid="c17" ref-type="bibr">17</xref></sup> is a CNN-based strategy to segment images of cells, using a patch-based, single pixel classification objective. The network architecture has seven convolutional layers, each equipped with a ReLu nonlinearity <sup><xref rid="c26" ref-type="bibr">26</xref></sup> and batch normalization <sup><xref rid="c27" ref-type="bibr">27</xref></sup>; three max-pooling layers to progressively reduce the spatial support of feature maps; and two fully connected layers; totalling about 2.5 million parameters for training. This architecture has a receptive field of 61&#x00D7;61 pixels, which is the approximate area needed to cover a single cell, and produces as output a three-class probability distribution for the pixel centered in the patch.</p>
<p>In our evaluation, we use the recommended configuration reported by Van Valen et al. <sup><xref rid="c17" ref-type="bibr">17</xref></sup>, which was demonstrated to be accurate on a variety of cell segmentation tasks, including mammalian cell segmentation and nuclei. Their configuration include training an ensemble of five replicate networks to make predictions in images. The final segmentation mask is the average of the outputs produced by each individual network. The ensemble increases processing time, but can also improve segmentation accuracy. The settings of the DeepCell system that we used in our experiments can be reproduced using the following Docker container: <ext-link ext-link-type="uri" xlink:href="https://hub.docker.com/r/jccaicedo/deepcell/">https://hub.docker.com/r/jccaicedo/deepcell/</ext-link>.</p>
</sec>
<sec id="s4e">
<title>U-Net</title>
<p>The U-Net architecture <sup><xref rid="c16" ref-type="bibr">16</xref></sup> resembles an autoencoder <sup><xref rid="c28" ref-type="bibr">28</xref></sup> with two main sub-structures: 1) an encoder, which takes an input image and reduces its spatial resolution through multiple convolutional layers to create a representation encoding. 2) A decoder, which takes the representation encoding and increases spatial resolution back to produce a reconstructed image as output. The U-Net introduces two innovations to this architecture: First, the objective function is set to reconstruct a segmentation mask using a classification loss; and second, the convolutional layers of the encoder are connected to the corresponding layers of the same resolution in the decoder using skip connections.</p>
<p>The U-Net evaluated in our work consists of eight convolutional layers and three max pooling layers in the encoder branch, and eight equivalent convolutional layers with upscaling layers in the decoder branch. All convolutional layers are followed by a batch normalization layer, and the skip connections copy the feature maps from the encoder to the decoder. The receptive field of the U-Net is set to 256 &#x00D7;; 256 pixels during training, which can cover a large group of cells at the same time. This architecture has a total of 7.7 million trainable parameters.</p>
<p>We adapted the objective function as a weighted classification loss, giving 10 times more importance to the boundary class. We apply basic data augmentation during training, including random cropping, flips, 90 degree rotations, and illumination variations. Also, we apply additional data augmentation using elastic deformations, as discussed by the authors <sup><xref rid="c16" ref-type="bibr">16</xref></sup>. The training parameters for this network were tuned using the training and validation sets, and the final model is applied to the test set to report performance. The source code of our U-Net implementation can be found in <ext-link ext-link-type="uri" xlink:href="https://github.com/carpenterlab/unet4nuclei">https://github.com/carpenterlab/unet4nuclei</ext-link>.</p>
</sec>
<sec id="s4f">
<title>Evaluation metrics</title>
<p>Measuring the performance of cell segmentation has been generally approached as measuring the difference between two segmentation masks: a reference mask with ground truth objects representing the true segmentation, vs. the predicted/estimated segmentation mask. These metrics include root-mean-square deviation <sup><xref rid="c29" ref-type="bibr">29</xref></sup>, Jaccard index <sup><xref rid="c17" ref-type="bibr">17</xref></sup>, and bivariate similarity index <sup><xref rid="c19" ref-type="bibr">19</xref></sup>, among others. However, these metrics focus on evaluating pixel-wise segmentation accuracy only, and fail to quantify object-level errors explicitly (such as missed or merged objects).</p>
<p>In our evaluation, we adopt an object-based accuracy metric that uses a measure of area coverage to identify correctly segmented nuclei. Intuitively, the metric counts the number of single objects that have been correctly separated from the rest using a minimum area coverage threshold. The metric relies on the computation of intersection-over-union between ground truth objects T and estimated objects E:
<disp-formula id="ueqn1">
<alternatives><graphic xlink:href="335216_ueqn1.gif"/></alternatives>
</disp-formula>
</p>
<p>Consider <italic>n</italic> true objects and <italic>m</italic> estimated objects in an image. A matrix <italic>C</italic><sub><italic>n&#x00D7;;m</italic></sub> is computed with all IoU scores between true objects and estimated objects to identify the best pairing. This is a very sparse matrix because only a few pairs share enough common area to score a non-zero IoU value. To complete the assignment, a threshold greater than 0.5 IoU is applied to the matrix to identify segmentation matches. In our evaluation, we do not accept overlapping objects, i.e., one pixel belongs only to a single nucleus. Thus, a threshold greater than 0.5 ensures that for each nucleus in the ground truth there is no more than one match in the predictions, and vice versa. At a given threshold, the object-based segmentation F1 score is then computed as:
<disp-formula id="ueqn2">
<alternatives><graphic xlink:href="335216_ueqn2.gif"/></alternatives>
</disp-formula>
where <italic>t</italic> is the evaluated IoU threshold. We compute the average F1 score across multiple thresholds, starting at <italic>t</italic> = 0.05 up to <italic>t</italic> = 0.95 with increments. &#x0394;<italic>t</italic> = 0.05 This score summarizes the quality of segmentations by simultaneously looking at the proportion of correctly identified objects as well as the pixel-wise accuracy of their estimated masks. Our evaluation metric is similar in spirit to other evaluation metrics used in computer vision problems, such as object detection in the PASCAL challenge <sup><xref rid="c30" ref-type="bibr">30</xref></sup> and instance segmentation in the COCO challenge <sup><xref rid="c20" ref-type="bibr">20</xref>,<xref rid="c31" ref-type="bibr">31</xref></sup>. One important difference of these metrics and ours is that our problem considers a single object category (the nucleus), and therefore, it is more convenient to adopt the F1 score instead of average precision.</p>
<p>In our evaluation, we also measure other quality metrics, including the number and type of errors to facilitate performance analysis <sup><xref rid="c32" ref-type="bibr">32</xref></sup>. The following are different types of errors that a segmentation algorithm can make: false negatives (missed objects); merges (under-segmentations), which are identified by several true objects being covered by a single estimated mask; and splits (over-segmentations), which are identified by a single true object being covered by multiple estimated masks. We identify these errors in the matrix <italic>C</italic> of IoU scores using a fixed threshold for evaluation, and keep track of them to understand the difficulties of an algorithm to successfully segment an image.</p>
</sec>
<sec id="s4g">
<title>Baseline Segmentation</title>
<p>We use CellProfiler 3.0 <sup><xref rid="c33" ref-type="bibr">33</xref></sup> pipelines to create baseline segmentations. CellProfiler was used as a baseline over other tools because it offers great flexibility to configure multi-step image processing pipelines that connect different algorithms for image analysis, and it is widely used in biology labs and high-throughput microscopy facilities. The pipelines are configured and tested by an expert image analyst using images from the training set, and then run in the validation and test set for evaluation. We refer to two CellProfiler pipelines for obtaining baseline segmentations: basic and advanced.</p>
<p>The basic pipeline relies only on the configuration of the module IdentifyPrimaryObjects, which is frequently used to identify nuclei. The module combines thresholding techniques with area and shape rules to separate and filter objects of interest. This is the simplest way of segmenting nuclei images when the user does not have extensive experience with image analysis operations, yet it is complete enough to allow them to configure various critical parameters.</p>
<p>The advanced pipeline incorporates other modules for preprocessing the inputs and postprocessing the outputs of the IdentifyPrimaryObjects module. In our advanced configuration we included illumination correction, median filters and opening operations, to enhance and suppress features in the input images before applying thresholding. These operations are useful to remove noise and prepare images to the same standard for segmentation using the same configuration. The postprocessing steps include measuring objects to apply additional filters and generate the output masks.</p>
<p>A single pipeline was used for segmenting images in the BBBC022 dataset, while Van Valen&#x2019;s set required to split the workflow in two different pipelines. We observed large signal variation in Van Valen&#x2019;s set given that these images come from different experiments and reflect realistic acquisition modes. Two settings were needed for thresholding, the first for normal single mode pixel intensity distributions and another one for bimodal distributions. The latter is applied to cases where subpopulations of nuclei are significantly brighter than the rest, requiring two thresholds. We used a clustering approach to automatically decide which images needed which pipeline. The pipelines used in our experiments are released together with the data and code.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank Beth Cimini and Minh Doan for their efforts and guidance when annotating the image set used for this research. We also thank Mohammad Robahn and Beth Cimini for fruitful discussions and key insights to design experiments and write the manuscript. Funding was provided by the National Institute of General Medical Sciences of the National Institutes of Health under MIRA award number R35 GM122547 (to AEC).</p>
</ack>
<sec id="s5">
<title>Author Contributions</title>
<p>JCC contributed experiments, data analysis, software development, and manuscript writing. JR contributed experiments, data preparation, data analysis and software development. AG contributed experimental design, data preparation and software development. TB contributed experiments, software development and manuscript writing. KWK contributed experiments, data preparation and data analysis. CM contributed data preparation and software development. SS contributed experimental design and manuscript writing. AEC contributed experimental design and manuscript writing.</p>
</sec>
<ref-list>
<title>Reference</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Boutros</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Heigwer</surname>, <given-names>F.</given-names></string-name> &#x0026; <string-name><surname>Laufer</surname>, <given-names>C.</given-names></string-name> <article-title>Microscopy-Based High-Content Screening</article-title>. <source>Cell</source> <volume>163</volume>, <fpage>1314</fpage>&#x2013;<lpage>1325</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><given-names>Mattiazzi</given-names> <surname>Usaj, M.</surname></string-name> <etal>et al.</etal> <article-title>High-Content Screening for Quantitative Cell Biology</article-title>. <source>Trends Cell Biol</source>. <volume>26</volume>, <fpage>598</fpage>&#x2013;<lpage>611</lpage> (<year>2016</year>/8).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Caicedo</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Singh</surname>, <given-names>S.</given-names></string-name> &#x0026; <string-name><surname>Carpenter</surname>, <given-names>A. E.</given-names></string-name> <article-title>Applications in image-based profiling of perturbations</article-title>. <source>Curr. Opin. Biotechnol</source>. <volume>39</volume>, <fpage>134</fpage>&#x2013;<lpage>142</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Bougen-Zhukov</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Loh</surname>, <given-names>S. Y.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>H. K.</given-names></string-name> &#x0026; <string-name><surname>Loo</surname>, <given-names>L.-H.</given-names></string-name> <article-title>Large-scale image-based screening and profiling of cellular phenotypes</article-title>. <source>Cytometry A</source> (<year>2016</year>). doi:<pub-id pub-id-type="doi">10.1002/cyto.a.22909</pub-id></mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Williams</surname>, <given-names>E.</given-names></string-name> <etal>et al.</etal> <article-title>The Image Data Resource: A Bioimage Data Integration and Publication Platform</article-title>. <source>Nat. Methods</source> <volume>14</volume>, <fpage>775</fpage>&#x2013;<lpage>781</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Meijering</surname>, <given-names>E.</given-names></string-name> <article-title>Cell Segmentation: 50 Years Down the Road [Life Sciences]</article-title>. <source>IEEE Signal Process. Mag</source>. <volume>29</volume>, <fpage>140</fpage>&#x2013;<lpage>145</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Otsu</surname>, <given-names>N.</given-names></string-name> <article-title>A Threshold Selection Method from Gray-Level Histograms</article-title>. <source>IEEE Trans. Syst. Man Cybern</source>. <volume>9</volume>, <fpage>62</fpage>&#x2013;<lpage>66</lpage> (<year>1979</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Beucher</surname>, <given-names>S.</given-names></string-name> &#x0026; <string-name><surname>Lantuejoul</surname>, <given-names>C.</given-names></string-name> <article-title>Use of watersheds in contour detection</article-title>. in <source>Proceedings of the International Workshop on Image Processing</source> (<year>1979</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>W&#x00E4;hlby</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>SinW&#x00E4;torn</surname>, <given-names>I.-M.</given-names></string-name>, <string-name><surname>Erlandsson</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Borgefors</surname>, <given-names>G.</given-names></string-name> &#x0026; <string-name><surname>Bengtsson</surname>, <given-names>E.</given-names></string-name> <article-title>Combining intensity, edge and shape information for 2D and 3D segmentation of cell nuclei in tissue sections</article-title>. <source>J. Microsc</source>. <volume>215</volume>, <fpage>67</fpage>&#x2013;<lpage>76</lpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Sommer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Straehle</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>K&#x00F6;the</surname>, <given-names>U.</given-names></string-name> &#x0026; <string-name><surname>Hamprecht, F.</surname> <given-names>A.</given-names></string-name> <article-title>Ilastik: Interactive learning and segmentation toolkit. in 2011 IEEE International Symposium on Biomedical Imaging</article-title>: <source>From Nano to Macro</source> <fpage>230</fpage>&#x2013;<lpage>233</lpage> (ieeexplore.ieee.org, <year>2011</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Eliceiri</surname>, <given-names>K. W.</given-names></string-name> <etal>et al.</etal> <article-title>Biological imaging software tools</article-title>. <source>Nat. Methods</source> <volume>9</volume>, <fpage>697</fpage>&#x2013;<lpage>710</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Carpenter</surname>, <given-names>A. E.</given-names></string-name> <etal>et al.</etal> <article-title>CellProfiler: image analysis software for identifying and quantifying cell phenotypes</article-title>. <source>Genome Biol</source>. <volume>7</volume>, <fpage>R100</fpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Schindelin</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title>Fiji: an open-source platform for biological-image analysis</article-title>. <source>Nat. Methods</source> <volume>9</volume>, <fpage>676</fpage>&#x2013;<lpage>682</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>LeCun</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name> &#x0026; <string-name><surname>Hinton</surname>, <given-names>G.</given-names></string-name> <article-title>Deep learning</article-title>. <source>Nature</source> <volume>521</volume>, <fpage>436</fpage>&#x2013;<lpage>444</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Mnih</surname>, <given-names>V.</given-names></string-name> <etal>et al.</etal> <article-title>Human-level control through deep reinforcement learning</article-title>. <source>Nature</source> <volume>518</volume>, <fpage>529</fpage>&#x2013;<lpage>533</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Ronneberger</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Fischer</surname>, <given-names>P.</given-names></string-name> &#x0026; <string-name><surname>Brox</surname>, <given-names>T.</given-names></string-name> <article-title>U-net: Convolutional networks for biomedical image segmentation</article-title>. <source>Med. Image Comput. Comput. Assist. Interv</source>. (<year>2015</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Van Valen</surname>, <given-names>D. A.</given-names></string-name> <etal>et al.</etal> <article-title>Deep Learning Automates the Quantitative Analysis of Individual Cells in Live-Cell Imaging Experiments</article-title>. <source>PLoS Comput. Biol</source>. <volume>12</volume>, <fpage>e1005177</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Gustafsdottir</surname>, <given-names>S. M.</given-names></string-name> <etal>et al.</etal> <article-title>Multiplex cytological profiling assay to measure diverse cellular states</article-title>. <source>PLoS One</source> <volume>8</volume>, <fpage>e80999</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Dima</surname>, <given-names>A. A.</given-names></string-name> <etal>et al.</etal> <article-title>Comparison of segmentation algorithms for fluorescence microscopy images of cells</article-title>. <source>Cytometry A</source> <volume>79</volume>, <fpage>545</fpage>&#x2013;<lpage>559</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Hariharan</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Arbel&#x00E1;ez</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Girshick</surname>, <given-names>R.</given-names></string-name> &#x0026; <string-name><surname>Malik</surname>, <given-names>J.</given-names></string-name> <article-title>Simultaneous Detection and Segmentation</article-title>. <source>arXiv [cs.CV]</source> (<year>2014</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Shelhamer</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Long</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Darrell</surname>, <given-names>T.</given-names></string-name> <article-title>Fully Convolutional Networks for Semantic Segmentation</article-title>. <source>arXiv [cs.CV]</source> (<year>2016</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Fiorio</surname>, <given-names>C.</given-names></string-name> &#x0026; <string-name><surname>Gustedt</surname>, <given-names>J.</given-names></string-name> <article-title>Two linear time Union-Find strategies for image processing</article-title>. <source>Theor. Comput. Sci</source>. <volume>154</volume>, <fpage>165</fpage>&#x2013;<lpage>181</lpage> (<year>1996</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>S.</given-names></string-name> &#x0026; <string-name><surname>Sun</surname>, <given-names>J.</given-names></string-name> <article-title>Deep Residual Learning for Image Recognition</article-title>. <source>arXiv [cs.CV]</source> (<year>2015</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Chen</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Tillberg</surname>, <given-names>P. W.</given-names></string-name> &#x0026; <string-name><surname>Boyden</surname>, <given-names>E. S.</given-names></string-name> <article-title>Optical imaging. Expansion microscopy</article-title>. <source>Science</source> <volume>347</volume>, <fpage>543</fpage>&#x2013;<lpage>548</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Bray</surname>, <given-names>M.-A.</given-names></string-name> <etal>et al.</etal> <article-title>Cell Painting, a high-content image-based assay for morphological profiling using multiplexed fluorescent dyes</article-title>. <source>bioRxiv 049817</source> (<year>2016</year>). doi:<pub-id pub-id-type="doi">10.1101/049817</pub-id></mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Nair</surname>, <given-names>V.</given-names></string-name> &#x0026; <string-name><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name> <article-title>Rectified linear units improve restricted boltzmann machines</article-title>. in <source>Proceedings of the 27th international conference on machine learning (ICML-10)</source> <fpage>807</fpage>&#x2013;<lpage>814</lpage> (cs.toronto.edu, <year>2010</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Ioffe</surname>, <given-names>S.</given-names></string-name> &#x0026; <string-name><surname>Szegedy</surname>, <given-names>C.</given-names></string-name> <article-title>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</article-title>. in <source>International Conference on Machine Learning</source> <fpage>448</fpage>&#x2013;<lpage>456</lpage> (jmlr.org, <year>2015</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name> &#x0026; <string-name><surname>Salakhutdinov</surname>, <given-names>R. R.</given-names></string-name> <article-title>Reducing the dimensionality of data with neural networks</article-title>. <source>Science</source> <volume>313</volume>, <fpage>504</fpage>&#x2013;<lpage>507</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Gudla</surname>, <given-names>P. R.</given-names></string-name> <etal>et al.</etal> <article-title>A high-throughput system for segmenting nuclei using multiscale techniques</article-title>. <source>Cytometry A</source> <volume>73</volume>, <fpage>451</fpage>&#x2013;<lpage>466</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Everingham</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Van Gool</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>C. K. I.</given-names></string-name>, <string-name><surname>Winn</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Zisserman</surname>, <given-names>A.</given-names></string-name> <article-title>The Pascal Visual Object Classes (VOC) Challenge</article-title>. <source>Int. J. Comput. Vis</source>. <volume>88</volume>, <fpage>303</fpage>&#x2013;<lpage>338</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="book"><string-name><surname>Lin</surname>, <given-names>T.-Y.</given-names></string-name> <etal>et al.</etal> <chapter-title>Microsoft COCO: Common Objects in Context</chapter-title>. in <source>Computer Vision &#x2013; ECCV 2014</source> <fpage>740</fpage>&#x2013;<lpage>755</lpage> (<publisher-name>Springer International Publishing</publisher-name>, <year>2014</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="book"><string-name><surname>Hoiem</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Chodpathumwan</surname>, <given-names>Y.</given-names></string-name> &#x0026; <string-name><surname>Dai</surname>, <given-names>Q.</given-names></string-name> <chapter-title>Diagnosing Error in Object Detectors</chapter-title>. in <source>Computer Vision &#x2013; ECCV 2012</source> <fpage>340</fpage>&#x2013;<lpage>353</lpage> (<publisher-name>Springer</publisher-name> <publisher-loc>Berlin Heidelberg</publisher-loc>, <year>2012</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>McQuin</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal> <article-title>CellProfiler 3.0: next generation image processing for biology</article-title>. <source>PLoS Comput. Biol</source>. (<year>2018</year>).</mixed-citation></ref>
</ref-list>
</back>
</article>
