<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/402735</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Task-specific vision models explain task-specific areas of visual cortex</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6442-7140</contrib-id>
<name>
<surname>Dwivedi</surname>
<given-names>Kshitij</given-names>
</name>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6439-8076</contrib-id>
<name>
<surname>Roig</surname>
<given-names>Gemma</given-names>
</name>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
</contrib>
<aff id="a1"><institution>Singapore University of Technology and Design</institution>, <country>Singapore</country></aff>
</contrib-group>
<author-notes>
<fn id="n1"><label>&#x002A;</label><p><email>gemma_roig@sutd.edu.sg</email></p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2018</year>
</pub-date>
<elocation-id>402735</elocation-id>
<history>
<date date-type="received">
<day>28</day>
<month>8</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>28</day>
<month>8</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>28</day>
<month>8</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="402735.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Computational models such as deep neural networks (DNN) trained for classification are often used to explain responses of the visual cortex. However, not all the areas of the visual cortex are involved in object/scene classification. For instance, scene selective occipital place area (OPA) plays a role in mapping navigational affordances. Therefore, for explaining responses of such task-specific brain area, we investigate if a model that performs a related task can serve as a better computational model than a model that performs an unrelated task. We found that DNN trained on a task (scene-parsing) related to the function (navigational affordances) of a brain region (OPA) explains its responses better than a DNN trained on a task (scene-classification) which is not explicitly related. In a subsequent analysis, we found that the DNNs that showed high correlation with a particular brain region were trained on a task that was consistent with functions of that brain region reported in previous neuroimaging studies. Our results demonstrate that the task is paramount for selecting a computational model of a brain area. Further, explaining the responses of a brain area by a diverse set of tasks has the potential to shed some light on its functions.</p>
<sec>
<title>Author summary</title>
<p>Areas in the human visual cortex are specialized for specific behaviors either due to supervision and interaction with the world or due to evolution. A standard way to gain insight into the function of these brain region is to design experiments related to a particular behavior, and localize the regions showing significant relative activity corresponding to that behavior. In this work, we investigate if we can figure out the function of a brain area in visual cortex using computational vision models. From our results, we find that explaining responses of a brain region using DNNs trained on a diverse set of possible vision tasks can help us gain insights into its function. The consistency of our results using DNNs with the previous neuroimaging studies suggest that the brain region may be specialized for behavior similar to the tasks for which DNNs showed a high correlation with its responses.</p>
</sec>
</abstract>
<counts>
<page-count count="26"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Deep neural networks (DNN) are currently the state of the art models for explaining cortical responses in the visual cortex [<xref rid="c1" ref-type="bibr">1</xref>&#x2013;<xref rid="c11" ref-type="bibr">11</xref>]. DNNs trained on a large dataset of images for the object classification task have been shown to explain the human and monkey cortical responses in the inferior temporal cortex (IT) area known for playing a role in object recognition. Further, in Razavi and Kriegsorte [<xref rid="c4" ref-type="bibr">4</xref>], it has been revealed that unsupervised models are unable to explain the IT responses as well as the models supervised for the classification task, thus emphasizing that supervision for classification task is crucial for explaining IT responses.</p>
<p>In some recent works [<xref rid="c12" ref-type="bibr">12</xref>, <xref rid="c13" ref-type="bibr">13</xref>], they show that the fMRI responses in the scene-selective brain regions like OPA and parahippocampal place area (PPA) are correlated with a DNN trained for the classification task. However, in earlier work from Bonner and Epstein [<xref rid="c14" ref-type="bibr">14</xref>], they showed that OPA responses are related to navigational affordances in the scenes. These results raise the question that why a DNN trained for a generic classification task can explain responses for a spatial property like navigational affordances. One possible argument is that the representations of the intermediate layers of a DNN perform generic visual processing. Therefore, these representations are not task-specific and are highly transferrable. An alternative way to explain the above result is that intermediate layers learn representations of the spatial structure of the scene to classify the scene correctly. However, it has not yet been studied in the case of OPA, if a DNN trained for a task related to the function of the brain region in study explains its responses better than a DNN trained on a less related task. We attempt to bridge this gap by taking tasks into account for explaining the brain responses.</p>
<p>In this work, we hypothesize that a DNN trained on a task related to the function of brain region will explain its responses better than a DNN trained on a task which is not explicitly related. Here, we consider that two tasks are different if they generate a different output structure or the predictions are from different domains, for instance, object domain or scene domain. We validate this hypothesis through two different analyses. In the first analysis, we consider a particular case of OPA and explain its responses by a DNN trained on a task (scene-parsing [<xref rid="c15" ref-type="bibr">15</xref>]), which we argue is related to navigational affordances. We then compare the results with a DNN trained on generic scene-classification task. In the second analysis, we select DNNs trained on a diverse set of computer vision tasks selected from Taskonomy [<xref rid="c16" ref-type="bibr">16</xref>] dataset. We then investigate if the tasks for which DNNs show a high correlation are consistent with functions of scene-selective areas OPA, PPA and Early Visual Cortex (EVC) reported in the previous works [14, 17&#x2013;23].</p>
<p>The navigational affordances (<xref rid="fig1" ref-type="fig">Fig 1A</xref>) as described in Bonner and Epstein [<xref rid="c12" ref-type="bibr">12</xref>] is computed by localizing the free space available for navigation in the scene. Thus, a DNN trained on a computer vision task to localize the free space for navigation can serve as a computational model for explaining the navigational affordance related responses in the visual cortex. The scene-parsing (<xref rid="fig1" ref-type="fig">Fig 1B</xref> center) task, where the aim is to predict the label of each pixel in the image is, therefore, suitable for our purpose.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig 1.</label>
<caption><title>Navigational affordances and scene-parsing task.</title>
<p>(A) An example stimulus image (left) presented to the subject for behavior and fMRI experiments in [<xref rid="c12" ref-type="bibr">12</xref>]. A path indicated by a rater as instructed in [<xref rid="c12" ref-type="bibr">12</xref>] to walk through the scene starting from the bottom center of the image (center). Heat map of possible navigational trajectories produced by combining the data across different raters (right). (reproduced with the permission from Bonner and Epstein [<xref rid="c12" ref-type="bibr">12</xref>]). B: Output generated by the scene parsing model (center). Activation of floor unit of the scene-parsing model (right)</p></caption>
<graphic xlink:href="402735_fig1.tif"/>
</fig>
<p>The output from the scene-parsing task can label the free space (<xref rid="fig1" ref-type="fig">Fig 1B</xref> right) available for navigation and also the obstacles present in the scene. In the scene classification task, the aim is to predict the category of the scene, and the output is a vector containing probabilities of each scene category. Since there is no straight-forward way to localize free space from scene classification output, it is not explicitly related to the navigational affordances. Therefore, we argue that a scene-parsing DNN model will explain the spatial scene property like navigational affordances, and hence, the OPA responses better than a scene classification DNN model.</p>
<p>In the second scenario, we selected DNNs trained on a diverse set of computer-vision tasks from the Taskonomy [<xref rid="c16" ref-type="bibr">16</xref>] dataset. The Taskonomy dataset consists of a large set of images with annotations and pretrained DNN models for a diverse set of tasks (<xref rid="fig2" ref-type="fig">Fig 2</xref>). We considered all the task DNNs and compared their correlation with OPA, PPA, and EVC. The results of this analysis were consistent with the function of the brain region in consideration.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig 2.</label>
<caption><title>Computer vision tasks from Taskonomy [<xref rid="c16" ref-type="bibr">16</xref>] dataset.</title>
<p>An example stimulus image (top-left) presented to the subject for behavior and fMRI experiments in [<xref rid="c12" ref-type="bibr">12</xref>]. Rest of the images are the output generated by pretrained DNNs optimized for the corresponding tasks selected from the Taskonomy dataset given the stimulus image as the input</p></caption>
<graphic xlink:href="402735_fig2.tif"/>
</fig>
<p>Here, we list the key findings of the above analysis:
<list list-type="order">
<list-item><p>The brain responses of a particular region show high correlation with the DNNs trained on a task related to its function than with the DNNs trained for the classification task.</p></list-item>
<list-item><p>The tasks on which the pretrained DNN activations show high correlation with a particular brain region&#x2019;s responses are consistent with the functions of the brain region reported in the previous studies.</p></list-item>
<list-item><p>The correlation comparison of a diverse set of task DNN activations with a brain area&#x2019;s responses provides insight into its previously known/unknown functions.</p></list-item>
</list></p>
</sec>
<sec id="s2">
<title>Results</title>
<p>In this work, we use Representation similarity analysis (RSA) [<xref rid="c24" ref-type="bibr">24</xref>] to compare the correlation of computational and behavioral models with human brain responses. We present the results through two sets of analysis. In the first set, we select a task (scene-parsing) which we argue is similar to the navigational affordances and, hence, the related responses in the OPA. We compute the correlation of brain and behavior Representation Dissimilarity matrices (RDMs) with scene-parsing DNN (VGG<sub>scene-parse</sub>) and compare the results with a scene-classification DNN (VGG<sub>scene-class</sub>). The scene classification task is not as relevant to mapping navigational affordances as the scene parsing task. Therefore, a comparison between these two can provide insights into whether training the DNN in a task related to the function of the brain region in the study is required to explain its responses. In the second set, we select a diverse set of computer vision tasks from the Taskonomy dataset and use DNNs trained for these individual tasks to explain the cortical responses of scene-selective brain regions and early visual cortex. We then compare the correlation with the brain RDMs with the DNNs trained on the above tasks to gain insights into the functions of the brain areas.</p>
<sec id="s2a">
<title>Scene-parsing DNN explains OPA responses related to navigational affordances better than scene-classification DNN</title>
<p>The DNNs are widely used as a potential candidate for computational modeling of areas in visual cortex. Here, we consider two DNNs, one which is optimized on a task (scene-parsing) related to mapping navigational affordances and other on a task (scene classification) not explicitly related to mapping navigational affordances. The DNN models we consider here are VGG<sub>scene-parse</sub> (<xref rid="fig3" ref-type="fig">Fig 3A</xref>) and VGG<sub>scene-class</sub>(<xref rid="fig3" ref-type="fig">Fig 3B</xref>) which are similar in architecture except for the last three layers. In VGG<sub>scene-parse</sub>, the last three layers are convolutional to generate spatial mask corresponding to each category, while in VGG<sub>scene-class</sub>, the last three layers are fully connected (FC) to predict the probabilities of possible scene categories. First 13 layers of both the models consist of blocks of convolutional layers with 5 pooling layers in between the blocks.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig 3.</label>
<caption><title>Scene parsing and Scene classification DNNs.</title>
<p>(A) DNN model architecture trained on scene-parsing task (VGG<sub>scene-parse</sub>) (B) DNN model architecture trained on scene-classification task VGG<sub>scene-class</sub>. The kxk denotes the kernel dimensions for the convolution and deconvolution layers while the value after the layer type denotes the channel dimensions. Blue layers in the DNNs were selected for comparison</p></caption>
<graphic xlink:href="402735_fig3.tif"/>
</fig>
<p>For computing RDMs, we select the DNN activations after the pooling layers and the last three layers of both the DNNs. <xref rid="fig4" ref-type="fig">Fig 4A</xref> shows the RDMs of the behavioral model for navigational affordance map (NAM), OPA, and the prefinal layer RDMs of both the DNNs. We argue that deeper layers are more task-related rather than early layers of the DNN. Therefore, we show the RDMs of prefinal layer activations in <xref rid="fig4" ref-type="fig">Fig 4A</xref>.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig 4.</label>
<caption><title>Scene-parsing vs. Scene-classification to explain navigational affordance related responses in the brain.</title>
<p>(A) RDMs of navigational affordance, OPA, VGG<sub>scene-class</sub>, and VGG<sub>scene-parse</sub>. (B) (left) Correlation of the OPA responses with VGG<sub>scene-parse</sub>, and VGG<sub>scene-class</sub>, (right) Correlation of the navigational affordance with VGG<sub>scene-parse</sub>, and VGG<sub>scene-class</sub>. (C) Variance partitioning analysis showing the unique and shared variances of behavior and computational models. The asterisk at the top indicates the significance of difference (&#x002A;p &#x003C;0.05, &#x002A;&#x002A;p &#x003C;0.01, &#x002A;&#x002A;&#x002A;p&#x003C;0.001)</p></caption>
<graphic xlink:href="402735_fig4.tif"/>
</fig>
<p>We focus on the correlation with the RDMs of the behavioral model for navigational affordances and OPA responses that are related to navigational affordances. From the results in <xref rid="fig4" ref-type="fig">Fig 4B</xref>, we note three key findings:
<list list-type="order">
<list-item><p>All the layers show a significant correlation (p&#x003C;0.001) with the brain RDM while only the deeper layers show significant correlation with the behavior RDM</p></list-item>
<list-item><p>Deeper layers for both the DNNs show a higher correlation with brain and behavior RDMs as compared to earlier layers</p></list-item>
<list-item><p>The difference between the correlation values of the deeper layers in both the DNNs with the brain and behavior RDMs is higher and significant (p&#x003C;0.05 for layer15, layer 16 with OPA, and layer 15 with NAM) in some cases.</p></list-item>
</list></p>
<p>The correlation values for all the comparisons are higher and significant for the VGG<sub>scene-parse</sub> in the deeper layers validating our hypothesis that task-relevant DNNs explain the task-specific regions of the brain better than a generic classification DNN.</p>
</sec>
<sec id="s2b">
<title>Scene-parsing DNN explains a major portion of the shared variance of the behavior and scene-classification DNN</title>
<p>We combined the RSA with variance partitioning [<xref rid="c25" ref-type="bibr">25</xref>] analysis to investigate how uniquely does each model (behavior, VGG<sub>scene-parse</sub>, and VGG<sub>scene-class</sub>) explain the responses of OPA. In variance partitioning approach, using a multiple regression model, we can divide the unique and shared variance contributed by all of its predictors. In this case, OPA RDM was the predictand, and the DNN models and behavior were the predictors. For the DNN models, we selected the RDMs of the layer showing the highest correlation (layer 15 for VGG<sub>scene-parse</sub>, and layer 13 for VGG<sub>scene-class</sub>) with the OPA RDM.</p>
<p>From the results of this analysis (<xref rid="fig4" ref-type="fig">Fig 4C</xref>), we note the following points:
<list list-type="order">
<list-item><p>VGG<sub>scene-class</sub> shares a major portion (96.62&#x0025;) of its variance with VGG<sub>scene-parse</sub>.</p></list-item>
<list-item><p>behavior shares more than half of its variance with VGG<sub>scene-parse</sub> (57.42&#x0025;) and VGGscene-class (52.35&#x0025;)</p></list-item>
<list-item><p>VGG<sub>scene-parse</sub>&#x2019;s unique variance is more than one-forth of the total variance (25.40&#x0025; of the total variance) explained by the three models</p></list-item>
</list></p>
<p>The above results suggest that VGG<sub>scene-parse</sub> can equally or better account for the navigational affordance related responses in the OPA than VGG<sub>scene-class</sub>, while at the same time uniquely explaining the OPA responses which are neither related to navigational affordances nor scene classification.</p>
</sec>
<sec id="s2c">
<title>Floor and free space activations explain the behavior but not the brain responses better than the scene-parsing output</title>
<p>The navigational affordance, visually, is related to the free space available for navigation. Therefore, in this analysis, we investigate the case if units corresponding to the free space show a higher correlation with the behavior and brain RDMs than the readout layer of the VGG<sub>scene-parse</sub>. The readout layer of the VGG<sub>scene-parse</sub> consists of 151 channels with 150 channel each containing an output corresponding to a particular class in the ADE20k [<xref rid="c26" ref-type="bibr">26</xref>] dataset and 1 channel corresponding to the background. Therefore, it is straightforward to separate specific category activation from the readout layer. We consider 15 such labels (floor, road, earth, rug, grass, sidewalk, field, sand, stairs, runway, stairway, dirt, land, stage, and step) that represent free space and consider a particular case of floor label as in the dataset by Bonner and Epstein all the stimuli images were from indoor scenes.</p>
<p>We observe from <xref rid="fig5" ref-type="fig">Fig 5A</xref> that while floor RDM (<italic>&#x03C1;</italic> = 0.2120, p &#x003C;0.001) showed a higher correlation with the behavior RDM than output RDM (<italic>&#x03C1;</italic> = 0.1610, p &#x003C;0.001), the output RDM (<italic>&#x03C1;</italic> = 0.3950, p &#x003C;0.001) showed a higher correlation with OPA RDM than floor RDM (<italic>&#x03C1;</italic> = 0.3054, p &#x003C;0.001). We perform a similar analysis with PSP<sub>scene-parse</sub> [<xref rid="c27" ref-type="bibr">27</xref>], which is a scene-parsing model that has been shown to achieve higher prediction accuracy than VGG<sub>scene-parse</sub>, to ensure that the results are consistent. The results from <xref rid="fig5" ref-type="fig">Fig 5B</xref> show that the trend is consistent among the different models and clears the ambiguity due to the poor performance of the VGG<sub>scene-parse</sub> model.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Fig 5.</label>
<caption><title>RSA of Scene parsing category-specific activations with OPA and navigational affordances.</title>
<p>(A) RSA of floor, freespace and output activations from VGG<sub>scene-parse</sub> with OPA (left), and behavior (right). (B) RSA of floor, freespace and output activations from PSP<sub>scene-parse</sub> with OPA (left), and behavior (right). Error bars represent bootstrap <italic>&#x00B1;</italic>1 s.e.m. (&#x002A;p &#x003C;0.05, &#x002A;&#x002A;p &#x003C;0.01, &#x002A;&#x002A;&#x002A;p &#x003C;0.001)</p></caption>
<graphic xlink:href="402735_fig5.tif"/>
</fig>
<p>Further, it is interesting to note that due to more accurate prediction from PSP<sub>scene-parse</sub> model, the correlation of navigational affordance model with the floor increased to <italic>&#x03C1;</italic> = 0.3893 as compared to floor activation from VGG<sub>scene-parse</sub> (<italic>&#x03C1;</italic> =0.2120). The results from this analysis suggest that while just the floor activations can explain navigational affordances, the OPA representation consist of more information about the scene than just navigational affordances.</p>
</sec>
<sec id="s2d">
<title>Highly correlated categorical DNN units provide insights into the functionality of brain regions</title>
<p>In this analysis, we probe further by computing the correlation of each DNN category unit&#x2019;s activation with the brain and behavior RDMs. We investigate the top-10 highly correlated categories with the brain and behavior RDMs and observe whether this analysis support the previous works which investigated the functions of OPA and PPA. For this purpose, we use the PSP<sub>scene-parse</sub> as the predictions generated by PSP<sub>scene-parse</sub> are more accurate than the VGG<sub>scene-parse</sub> model.</p>
<p>From <xref rid="fig6" ref-type="fig">Fig 6</xref> (left), we observe that 8 (rug, sidewalk, runway, etc.) out of 10 highly correlated categories with the behavior RDM are indicative of free space with floor RDM showing the highest correlation. Surprisingly, the objects such as vase and clock also showed high correlation with the behavior RDM. A possible explanation for this may be that the vase and clock are generally placed on floor and wall, respectively.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Fig 6.</label>
<caption><title>RSA of top-10 correlated categories.</title>
<p>RSA of top-10 highly correlated category activations from PSP<sub>scene-parse</sub> with behavior (left), OPA (center), and PPA (right). Error bars represent bootstrap <italic>&#x00B1;</italic>1 s.e.m. (&#x002A;p &#x003C;0.05, &#x002A;&#x002A;p &#x003C;0.01, &#x002A;&#x002A;&#x002A;p &#x003C;0.001)</p></caption>
<graphic xlink:href="402735_fig6.tif"/>
</fig>
<p>The OPA RDM (<xref rid="fig6" ref-type="fig">Fig 6</xref> center) showed high correlation with only 5 (runway, floor, sidewalk, path, dirt) out of 10 highly correlated categories corresponding to free space. Rest of the labels include object categories: plate, vase, sink, kitchen, and barrel. One possible explanation for these categories being highly correlated is the experimental design [<xref rid="c14" ref-type="bibr">14</xref>] in which the OPA responses were recorded. The subjects were asked to classify whether the room displayed is a bathroom or not. The objects such as sink, plate, and vase are highly indicative of the room type, and therefore, OPA responses may be related to the scene classification task. Hence, the high correlation of OPA with these objects is explained by assuming that OPA is involved in the scene classification task. Further, knowing the scene category is also crucial for planning navigation. A related possible explanation is that the objects also suggest the spatial layout of the scene by indicating the presence of obstacles and therefore can be relevant for navigational affordances.</p>
<p>PPA, on the other hand, is hypothesized to represent the spatial layout of the scenes and is insensitive to the navigational affordance as shown in [<xref rid="c14" ref-type="bibr">14</xref>]. The results from this analysis (<xref rid="fig6" ref-type="fig">Fig 6</xref> right) are consistent with [<xref rid="c14" ref-type="bibr">14</xref>], as the majority of the categories with high correlation are objects that are indicative of scene layout and category and only 2 of the highly correlated categories correspond to free space.</p>
<p>The above analysis demonstrated that categorical units from the scene parsing output are consistent with the functions of the OPA and PPA reported in the previous studies. This result suggests that a categorical analysis has the potential to be used in investigating the functions of brain regions.</p>
</sec>
<sec id="s2e">
<title>Different task DNNs show different correlation with brain responses</title>
<p>In the above comparison analysis, there were several variables which could play an important role in explaining the brain data. Such variables are the dataset used for learning the DNN models, the DNN architecture, and the tasks for which the DNN was optimized. To disentangle the aforementioned variables, in this analysis, we use the pretrained DNN models which have the same architecture (except the readout layer), and that are optimized on the same set of images from the Taskonomy dataset [<xref rid="c16" ref-type="bibr">16</xref>], to perform different tasks.</p>
<p>The provided Taskonomy DNNs architectures consist of an encoder which is same for all the tasks, and a decoder which can vary depending on the task. The encoder architecture for all the tasks is a fully convolutional ResNet-50 [<xref rid="c28" ref-type="bibr">28</xref>] with 4 residual blocks and without any pooling layers. The decoder architecture, however, is task-dependent, for example, the decoder of the classification tasks consists of fully-connected layers while the decoder of the tasks in which the output is spatial consist of all convolutional layers. In this analysis, we consider the tasks in which the output is spatial and therefore the decoder architecture is same across all the tasks. In this way, the DNN architecture is the same across all the selected tasks, and only the task is the variable.</p>
<p>We argue that deeper layers of the DNNs decoder may be more task-specific than early layers. To support our argument, we report the mean and variance correlation between the RDMs of several layers from the DNNs and brain RDM in <xref rid="fig7" ref-type="fig">Fig 7</xref>. We observe that in earlier layers of the encoder and decoder, the correlation values do not vary significantly across the tasks while the variance increases as we go deeper (<xref rid="fig7" ref-type="fig">Fig 7A</xref> left). The mean correlation remains almost constant and decreases on going deeper (<xref rid="fig7" ref-type="fig">Fig 7A</xref> center). Yet, the maximum correlation with the brain RDMs consistently increases as we go deeper (<xref rid="fig7" ref-type="fig">Fig 7A</xref> right) in the DNN architecture. These results when considered together suggest that for deeper DNN layers, DNNs trained on related tasks show higher correlation while the DNNs trained on unrelated tasks start showing lower correlation values with the brain RDMs. We also observed that the order of tasks correlation values is more consistent in the deeper layers. The above analysis provides evidence that for comparison we should consider the deeper layers of the DNN decoder for all tasks. Thus, in the following experiments, we use the prefinal layer of the decoder for all the tasks to perform the correlation analysis between each of the DNNs RDMs and the brain RDM.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Fig 7.</label>
<caption><title>RSA of DNNs (with same architecture) trained on a subset of tasknomy tasks with navigational affordances, OPA, and PPA.</title>
<p>(A) Correlation variance (left), mean (center), and maximum (right) of different task DNNs across different layers. The correlation between a task DNN and brain responses was computed for multiple layers located at different depths in the DNN. For the encoder, we selected the output after each residual block and for the decoder we selected the output after each layer. (B) RSA of DNNs (with same architecture) trained on a subset of Taskonomy tasks with behavior (left), OPA (center), and PPA (right). Error bars represent bootstrap <italic>&#x00B1;</italic>1 s.e.m. (&#x002A;p &#x003C;0.05, &#x002A;&#x002A;p &#x003C;0.01, &#x002A;&#x002A;&#x002A;p &#x003C;0.001)</p></caption>
<graphic xlink:href="402735_fig7.tif"/>
</fig>
<p>In this analysis, we focus on the correlation with the RDMs of the behavioral model for navigational affordances, OPA responses that are related to navigational affordances, and PPA responses that are related to spatial layout and scene classification. From <xref rid="fig7" ref-type="fig">Fig 7B</xref>, we observe that tasks that are related to 3-dimensional layout of the scene such as 3-d keypoints (<italic>&#x03C1;</italic> = 0.4299 for OPA, and <italic>&#x03C1;</italic> = 0.4253 for PPA), 2.5d segmentation (<italic>&#x03C1;</italic> = 0.3702 for OPA, and <italic>&#x03C1;</italic> = 0.3609 for PPA) and curvature (<italic>&#x03C1;</italic> = 0.3543 for OPA, and <italic>&#x03C1;</italic>= 0.2816 for PPA) show the highest correlation with both the OPA and PPA RDMs. Further, the semantic segmentation task (<italic>&#x03C1;</italic> = 0.3341 for OPA, and <italic>&#x03C1;</italic> = 0.2652 for PPA) that requires categorical and spatial layout information also shows a high correlation with both these brain regions. On the other hand, tasks such as autoencoding (<italic>&#x03C1;</italic> = 0.0804 for OPA, and <italic>&#x03C1;</italic> = 0.0715 for PPA), and inpainting (<italic>&#x03C1;</italic> = 0.0630 for OPA, and <italic>&#x03C1;</italic> = 0.0527 for PPA) that are neither related to 3-dimensional representation or scene-semantics show low and insignificant correlations with the brain areas. However, on comparing with behavior RDM, we observed that 2.5-D segmentation (<italic>&#x03C1;</italic> = 0.2122) although showed the highest correlation but semantic segmentation task showed low correlation (<italic>&#x03C1;</italic> = 0.0352). We argue that the low correlation of semantic segmentation task DNN with behavior is because the categories in this task do not contain floor or free space as compared to scene-parsing task.</p>
<p>The results from the above analysis suggest that for explaining the brain responses of task-specific brain regions using the DNNs, the DNN should be optimized on a task related to the function of that brain region. The above analysis also reveals that DNNs of same architecture that are trained with the same dataset show a difference in correlation due to the task.</p>
</sec>
<sec id="s2f">
<title>Task optimized DNNs provide insights into the functionality of brain regions</title>
<p>Above, we demonstrated through a series of analysis that a DNN optimized to perform a task related to a particular brain region function, better explains the responses of such brain region. In this analysis, we take a different route and ask the question if we can gain an insight about the functions of the brain region by comparing the correlation of different task DNNs with the brain RDMs.</p>
<p>In the current analysis, we focus on the correlation with the RDMs of OPA, PPA, and EVC. We consider all the single image tasks from the Taskonomy dataset and use the prefinal layer RDM as the representative DNN RDM for that particular task. Then, we perform the comparison of correlation between the brain RDMs and the prefinal layer RDMs of the DNNs trained on selected tasks.</p>
<p>The results (<xref rid="fig8" ref-type="fig">Fig 8A</xref>) of OPA and PPA are similar to the previous analysis. Now, we also observe a high correlation for both areas and the scene and object classification tasks, which were not reported in the previous analysis. These results suggest that while PPA and OPA representations are spatial to perform tasks requiring spatial layout information, the representations in these areas are also semantic to perform classification tasks. We observe that EVC is highly correlated to edge2d (<italic>&#x03C1;</italic> = 0.6972, p = 0.0002), object classification (<italic>&#x03C1;</italic> = 0.6289, p = 0.0002) while insignificantly related to tasks such as vanishing point (<italic>&#x03C1;</italic> = 0.0516, p = 0.0546) and rgb2mist (<italic>&#x03C1;</italic> = 0.0567, p = 0.0414).</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Fig 8.</label>
<caption><title>RSA of DNNs trained on tasknomy dataset with EVC, OPA, and PPA.</title>
<p>(A) RSA of DNNs trained on Taskonomy tasks with EVC (left), OPA (center), and PPA (right). (B) RSA of three layers of DNNs at early stage (left), middle stage (center), and final stage (right) trained on a subset of Taskonomy tasks with EVC. Error bars represent bootstrap <italic>&#x00B1;</italic>1 s.e.m. (&#x002A;p &#x003C;0.05, &#x002A;&#x002A;p &#x003C;0.01, &#x002A;&#x002A;&#x002A;p &#x003C;0.001)</p></caption>
<graphic xlink:href="402735_fig8.tif"/>
</fig>
<p>The EVC and the initial layers of the DNN are known to have a general visual representation that is not task-specific while the deeper layers and regions of higher visual cortex are known to have more task-specific representations. To investigate this, we probe further into EVC by observing the correlation of EVC with different task DNNs (<xref rid="fig8" ref-type="fig">Fig 8B</xref>) at block 1, block 4 layers of the encoder and prefinal layer of the decoder. We observe that in block 1 all the tasks have very high and significant correlation (<italic>&#x03C1;></italic>0.5, p&#x003C;0.001) with the EVC RDM. On the other hand, the correlation for some tasks starts dropping as we move from block 1 to block 4 and prefinal layer. The results support previous work [<xref rid="c7" ref-type="bibr">7</xref>] showing that EVC representation is more similar to early layers of DNN. Further, the tasks which show very high correlation with the EVC in deeper layers are mostly related to low-level visual cues (edge2d, keypoint 3d, segment2d, etc.) or the classification (object and scene classification). The high correlation with the classification DNNs may be due to the emergence of object detectors in the early visual cortex similar to as shown to emerge in DNNs [<xref rid="c29" ref-type="bibr">29</xref>].</p>
<p>Thus, the above analysis shows that performing RSA of a brain region with a diverse set of tasks has the potential to shed some light on the functionality of that particular brain region in the visual cortex.</p>
</sec>
<sec id="s2g">
<title>Similar task DNNs share more variance than dissimilar task DNNs</title>
<p>We probe further whether the tasks that are similar according to the Taskonomy transfer matrix share more variance than the tasks that are less similar. This analysis is performed to investigate whether two dissimilar tasks can be used to uniquely explain the responses of brain areas corresponding to different behaviors.</p>
<p>We use the variance partitioning [<xref rid="c25" ref-type="bibr">25</xref>] approach to calculate the unique and shared variance of different models. The brain RDMs (OPA, PPA, and EVC) are the predictand, and three task DNNs (pairs of similar and dissimilar tasks) are the predictors. We used the RDMs of the prefinal layer for all the DNNs tested.</p>
<p>Following <xref rid="fig8" ref-type="fig">Fig 8A</xref>, the tasks selected in the similar pair for OPA were Keypoint3d and curvature, for PPA were Keypoint3d and curvature, and for EVC were edge2d and segment2d. The tasks selected in the dissimilar pair for OPA were Keypoint3d and scene-classification, for PPA were Keypoint3d and scene-classification, and for EVC were edge2d and object classification.</p>
<p>The results from partitioning analysis (<xref rid="fig9" ref-type="fig">Fig 9A</xref>) of all three brain RDMs show that unique variance of dissimilar task (19.93&#x0025; vs. 4.13&#x0025; for OPA, 18.73&#x0025; vs. 1.35&#x0025; for PPA, 6.53&#x0025; vs. 2.10&#x0025; for EVC) is higher than the similar task. This analysis suggests that two DNNs optimized for dissimilar tasks may be used to explain the brain responses uniquely related to each task.</p>
<fig id="fig9" position="float" fig-type="figure">
<label>Fig 9.</label>
<caption><title>Task similarity using variance partitioning analysis and RSA.</title>
<p>(A) Variance partitioning analysis of EVC (left), OPA (center), and PPA (right) with three task DNNs. Tasks a and b are similar while tasks a and c are dissimilar for all the three cases. (B) RSA of keypoint3D DNN with other task DNNs at the early stage (block 1, left), center stage (block4, center), and final stage (prefinal layer, right). Error bars represent bootstrap <italic>&#x00B1;</italic>1 s.e.m. (&#x002A;p &#x003C;0.05, &#x002A;&#x002A;p &#x003C;0.01, &#x002A;&#x002A;&#x002A;p &#x003C;0.001)</p></caption>
<graphic xlink:href="402735_fig9.tif"/>
</fig>
</sec>
<sec id="s2h">
<title>Correlation between different task DNNs</title>
<p>In this analysis, we investigate how correlated are the DNN representations across different tasks. For this purpose, we choose a single task (3D keypoints) DNN and compute the correlation with other task DNNs across different layers in the DNN architecture. First observation (<xref rid="fig9" ref-type="fig">Fig 9B</xref> left) to note is that earlier layers of all the tasks are highly correlated with each other. Secondly, in the deeper layers (<xref rid="fig9" ref-type="fig">Fig 9B</xref> center, right) the task similar according to Taskonomy transfer matrix show high correlation whereas the dissimilar tasks show a lower correlation with the task in consideration.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>In this work, we demonstrated the importance of task selection for using pretrained DNNs as a computational model for task-specific regions of visual cortex. We list the key findings from our analyses below.
<list list-type="bullet">
<list-item><p>A DNN trained on a task (scene-parsing) related to the function (navigational affordance) of the brain region (OPA) shows a higher correlation with its responses than a DNN trained on a task (scene-classification) not explicitly related.</p></list-item>
<list-item><p>Category-specific activations generated from the scene-parsing DNN provide insights into functions of the scene-selective cortex.</p></list-item>
<list-item><p>Training DNNs with same architecture on the same dataset but for different tasks results in different correlation with the brain responses.</p></list-item>
<list-item><p>DNNs that show a high correlation with the brain responses are trained on tasks related to the functions of the brain areas reported in the previous studies.</p></list-item>
</list></p>
<p>In the following paragraphs, we discuss the strength and limitations of the key analysis and findings of this work.</p>
<sec id="s3a">
<title>Finding a DNN trained on a task related to the function of the brain region</title>
<p>In our first analysis, we selected a scene-parsing task and showed how it could be related to navigational affordances in the scene. However, it is not always possible to find a computer vision task that could be explicitly related to the function of the brain area. Further, even if we find a related task, there might be a possibility that the annotations for such a task are not readily available. Hence, finding a DNN pretrained on this particular task might not be possible. Likely, due to these reasons most of the previous studies adhered to a DNN pretrained on classification.</p>
<p>With the advance in the computer vision field, new datasets with annotations are continuously made available to the public. The Taskonomy dataset used in this work is a large-scale image dataset with annotations available for most of the commonly studied computer vision task on images. For more complex tasks, such as navigation planning, and visual question answering there are new datasets [<xref rid="c30" ref-type="bibr">30</xref>&#x2013;<xref rid="c34" ref-type="bibr">34</xref>] available with virtual 3D environments where an agent can navigate and interact with the environment. The DNN models trained on these datasets can shed some light on the functions of brain areas related to visual memory, navigation, and interaction with the environment. Thus, as new datasets for a wide variety of tasks, and hence, pretrained DNNs on these datasets become available, the comparison of these DNNs with the brain responses will shed new light into the functions of different brain regions. Through a specific example of OPA, navigational affordances, and scene-parsing task, we believe our work has paved a way towards future studies using task-optimized DNNs as potential computational models for task-specific brain regions.</p>
</sec>
<sec id="s3b">
<title>RSA with categorical units: a potential method to investigate functions of a semantic brain area</title>
<p>We showed that the responses of categorical units which are generated as the output of the scene-parsing task could be used to gain insights into the functions of OPA and PPA. For OPA, while half of the top correlated category units corresponded to free space for navigation, other half corresponded to objects indicative of scene category.</p>
<p>The result was consistent with earlier neuroimaging works investigating the function of OPA [14, 35, 36]. While in Bonner and Epstein [<xref rid="c14" ref-type="bibr">14</xref>], it is shown that OPA is involved in navigational affordance of scenes, in Dilks et al. [<xref rid="c36" ref-type="bibr">36</xref>] it has been shown that OPA might also be playing a role in scene classification. Similarly, for PPA, the top correlated classes mostly contained objects indicative of scene category and layout and only 2 out of the top 10 correlated classes corresponded to free space. These results further provide the evidence that PPA responses are insensitive to navigational affordance which is also consistent with the findings related to PPA in Bonner and Epstein [<xref rid="c14" ref-type="bibr">14</xref>]. Thus, the above results suggest that RSA with categorical activations can be a potential method to investigate functions of a brain area. Further, it is important to note that we were unable to distinguish the functions of OPA and PPA through the analysis involving a diverse set of tasks since both the OPA and PPA RDMs showed high correlation with the same set of tasks. In such cases, where the functional difference is due to semantical categories and not the spatial tasks, the categorical activations can be used to distinguish the functions of these brain regions.</p>
<p>However, there are few potential shortcomings with this approach. First, the number and type of categories are limited by the dataset used for training the DNN. Therefore, in a new set of stimuli which contains categories that were not present in the dataset used for training of DNN, the top correlated classes might not provide any useful insights. Also, it is not always the case that the brain areas are categorical and therefore this approach may not provide any meaningful insight into the functionality of those brain areas.</p>
</sec>
<sec id="s3c">
<title>Difference in correlation: Is it because of task?</title>
<p>In the first analysis, we found that scene-parsing DNN showed a higher correlation with OPA and navigational affordances than the scene-classification DNN. Yet, it is important to note that there were three differences in the DNNs used for comparison. First was the architecture difference, while the last 3 layers of scene-parsing DNN were convolution, the last 3 layers of scene-classification DNN were fully connected. Second, the dataset used for training both the DNNs were different, ADE20k [<xref rid="c15" ref-type="bibr">15</xref>] for scene-parsing DNN and Places-365 [<xref rid="c37" ref-type="bibr">37</xref>] for scene-classification DNN. Thirdly, the task on which the models were trained were different. Therefore, the difference in the correlation of 2 DNNs with OPA and navigational affordances could be attributed to any of these factors. To clear this ambiguity, we selected DNNs with same architecture trained on the same set of images but for different tasks. We then showed that the DNNs trained on tasks related with the function of brain area were highly correlated while the DNNs trained on unrelated tasks showed low or insignificant correlation with the brain area. From this analysis, we found that training on different tasks leads to a difference in correlation of the DNN activations with the responses of the brain regions. The correlation of the DNN with the brain region depends on how similar the task is with the function of the brain region.</p>
</sec>
<sec id="s3d">
<title>DNNs trained on a diverse set of tasks: a potential method to assess unknown functions of a brain area</title>
<p>We compared the correlation of DNNs trained on a diverse set of tasks with different brain areas. The above comparison was performed to investigate whether the highly correlated tasks are related to and are consistent with the previously reported functions of the brain areas. The top-3 task DNNs (3D Keypoints, curvature, and 25d segmentation) showing the highest correlation with the scene-selective visual areas (OPA and PPA) were related to the 3-D structure of the scenes. In an electrophysiological study [<xref rid="c21" ref-type="bibr">21</xref>], they demonstrated the importance of structure defining contours through the electrophysiological investigations of scene-selective visual cortex in the macaque brain. In a related neuroimaging work [<xref rid="c22" ref-type="bibr">22</xref>], they showed that scene category could be decoded from the PPA even if the stimuli images are just the line drawings of the corresponding scene. In Choo and Walther [<xref rid="c23" ref-type="bibr">23</xref>], they show that intact contour junctions are crucial for scene category representation in PPA. Thus, the high correlation of OPA and PPA responses with the DNNs trained to predict 3-D keypoints and curvature demonstrate that our results are consistent with the previous studies investigating the representation of scene-selective visual cortex.</p>
<p>Further, the semantic tasks such as scene/object classification and semantic segmentation also showed high correlation with the scene-selective visual cortex. This is consistent with the results of Bonner and Epstein [<xref rid="c14" ref-type="bibr">14</xref>] and Epstein et al. [<xref rid="c17" ref-type="bibr">17</xref>] where they report that representation of OPA and PPA is also semantic. Thus, the results from the task comparison analysis are consistent with the previous studies of the scene-selective visual cortex and provide the evidence that the representation of scene selective visual cortex is both semantic and visual. Further analysis with the early visual cortex showed that the tasks which require low-level vision cues such as 2d edges, 2d segmentation are highly correlated with the EVC responses. These results taken together suggest a strong potential for using a diverse set of tasks for gaining insights into the function of different brain regions.</p>
<p>One counter-argument to our approach might be that humans are never supervised the same way as these DNNs. The DNNs were supervised using the task-specific annotations. However, no such annotations are available to humans, and they learn to perform these tasks intuitively. However, one should also note that humans learn through interaction with the environment, by moving around, and learning from others. Therefore, these intermediate vision tasks may have been learned through supervision of much complex goal. Learning complex tasks is still a challenging area in the artificial intelligence and a single model is not able to perform all the tasks a human can perform. Therefore, in this work, we focused only on scene-selective regions in visual cortex and tried to explain its responses by DNNs trained on different tasks. Further, in this work, we are only interested in the correlation of the end state of the representations and not how either the DNNs or the human learned these representations.</p>
</sec>
</sec>
<sec id="s4">
<title>Conclusion</title>
<p>In this study, we presented the evidence supporting our hypothesis of using task-specific DNN models to explain responses of task-specific brain regions. We first validated this hypothesis by considering the particular case of OPA which has been reported to be associated with navigational affordances. We showed that a scene-parsing DNN that is related to the navigational affordances shows a higher correlation with OPA responses than a DNN trained on a less related task (scene-classification). We further validated this hypothesis by comparing the correlation of the responses of scene-selective visual areas with a large and diverse set of task DNNs. Although in this work, we only considered scene-selective visual areas we believe that the similar results can be obtained for other higher cognitive brain areas such as hippocampus and prefrontal cortex. One other limitation of this work is that we only considered the tasks that apply to single static images. In future studies, we aim at considering performing a similar analysis with more complex functions and with models trained on complex tasks in virtual 3-D environments. We believe this study has paved a way towards using task-optimized DNNs as potential computational models for task-specific brain regions.</p>
</sec>
<sec id="s5">
<title>Materials and methods</title>
<p>In the first section, we describe Representation similarity analysis (RSA) [<xref rid="c24" ref-type="bibr">24</xref>] which is a standard method to compare the correlation of computational and behavioral models with human brain responses. In the second section, we describe the variance partitioning analysis which was used to find the unique and shared variance of the computational models used to predict brain responses. In the third section, we briefly describe the dataset we used in this work and then in the last section we provide the details of the DNN models used for analysis.</p>
<sec id="s5a">
<title>Representation similarity analysis (RSA)</title>
<p>RSA is used to compare the information encoded in brain responses with a computational or behavioral model by computing the correlation of the corresponding Representation Dissimilarity matrices (RDMs). In the case of comparison with DNNs, we compute the correlation of RDMs of the brain responses with the RDM of layer activations of the DNNs.</p>
<sec id="s5a1">
<title>Representation Dissimilarity Matrix (RDM)</title>
<p>The RDM for a dataset is constructed by computing dissimilarities of all possible pairs of stimulus images. For fMRI data, the RDMs are computed by comparing the fMRI responses while for DNNs the RDMs are computed by comparing the layer activations for each image pair in the dataset. The dissimilarity metric used in this work is 1<italic>-&#x03C1;</italic> where <italic>&#x03C1;</italic> is the Pearson&#x2019;s correlation coefficient. Although in the previous work [<xref rid="c12" ref-type="bibr">12</xref>], where a scene classification DNN was compared with the navigational affordance the dissimilarity metric used was the Euclidean distance, we observed that with 1<italic>-&#x03C1;</italic> as the dissimilarity metric, the correlation was higher. Hence, in this work for all the analysis 1<italic>-&#x03C1;</italic> is used as the dissimilarity metric to compute RDMs of layer activations. We did not perform PCA on layer activations as done in [<xref rid="c12" ref-type="bibr">12</xref>] since the spatial information in the case of convolutional layer outputs is lost. The spatial information is lost because for performing PCA as done in [<xref rid="c12" ref-type="bibr">12</xref>]; first, the convolutional layer output is flattened and then principal components are selected due to which information from some spatial location is never considered for the analysis. For the first set of analysis with scene-parsing and scene-classification DNN, we consider OPA and PPA RDMs for comparison as these areas have been hypothesized to represent scene affordances [<xref rid="c14" ref-type="bibr">14</xref>] and scene layout [<xref rid="c17" ref-type="bibr">17</xref>] respectively. We also compare the DNN RDMs with a behavior Navigational Affordance Map (NAM) [<xref rid="c12" ref-type="bibr">12</xref>] that represents navigational affordances in a scene. For the second set of analysis with Taskonomy DNNs, we consider OPA, PPA, and EVC RDMs to compare with DNN RDMs.</p>
</sec>
<sec id="s5a2">
<title>Statistical analysis</title>
<p>We use RSA toolbox [<xref rid="c38" ref-type="bibr">38</xref>] to compute RDM correlations and corresponding p-values and standard deviation, using bootstrap similar to [<xref rid="c12" ref-type="bibr">12</xref>]. For determining which RDM better explains the behavioral or neural RDMs, we perform a two-sided statistical comparison. The p-values are estimated as the proportion of bootstrap samples further in the tails than 0. The number of bootstrap iterations for all the analysis was set to 5000.</p>
</sec>
</sec>
<sec id="s5b">
<title>Variance partitioning analysis</title>
<p>Variance partitioning method is used to determine the unique and shared contribution of individual models when considered in conjunction with the other models. We describe the analysis by considering the case of OPA predicted by behavior model related to navigational affordance, scene-parsing DNN, and scene-classification DNN. First, the off-diagonal elements of the OPA RDM is assigned as the dependent variable (predictand). Then, the off-diagonal elements of behavior RDM and layer RDMs representing scene-parsing and scene-classification tasks are selected as the independent variable. Then, we perform seven multiple regression analysis: one with all three independent variables as predictors, three with three different possible combinations of two independent variables as predictors, and three with individual independent variables as the predictors. Then, by comparing the explained variance (<italic>r</italic><sup>2</sup>) of a model used alone with the explained variance when it was used with other models, the amount of unique and shared variance between different predictors can be inferred. For the other variance partitioning analysis in this work, the predictors and predictands were modified accordingly, and the steps of analysis were the same. The area proportional venn diagrams for the variance partitioning analysis were generated using EulerAPE software [<xref rid="c39" ref-type="bibr">39</xref>].</p>
</sec>
<sec id="s5c">
<title>Navigational Affordance Dataset and Model</title>
<p>The stimuli images used for analysis consisted of 50 images of indoor environments. The subject&#x2019;s fMRI responses were obtained while they performed a category-recognition task (bathroom or not). In this work, we directly use the precomputed subject averaged RDMs of the navigational affordance map (NAM), PPA and OPA provided by Bonner and Epstein [<xref rid="c12" ref-type="bibr">12</xref>].</p>
<p>To obtain NAM, first, an independent group of subjects was asked to indicate the paths in each image starting from the bottom using a computer mouse. The probabilistic maps of paths for each image were created followed by histogram construction of navigational probability in one-degree angular bins radiating from the bottom center of the image. This histogram represents a probabilistic map of potential navigation routes from the viewer&#x2019;s perspective. For further details of the navigational affordance map or dataset, please refer to [<xref rid="c12" ref-type="bibr">12</xref>, <xref rid="c14" ref-type="bibr">14</xref>].</p>
</sec>
<sec id="s5d">
<title>Deep Neural Network Models to explain brain responses</title>
<p>In this section, we describe the architecture of the DNN models used in the analysis.</p>
<sec id="s5d1">
<title>Scene-classification model</title>
<p>We choose VGG16 [<xref rid="c40" ref-type="bibr">40</xref>] trained on Places [<xref rid="c37" ref-type="bibr">37</xref>] (a scene classification dataset) dataset as the scene classification model (VGG<sub>scene-class</sub>) (pretrained model downloaded from <ext-link ext-link-type="uri" xlink:href="https://github.com/CSAILVision/places365">https://github.com/CSAILVision/places365</ext-link>). The reason behind the different choice of scene classification model from the one used in Bonner and Epstein [<xref rid="c12" ref-type="bibr">12</xref>] was that we were unable to find a pretrained scene-parsing model with similar architecture as Alexnet [<xref rid="c41" ref-type="bibr">41</xref>]. VGG16 model (<xref rid="fig3" ref-type="fig">Fig 3A</xref>) contains 13 convolutional layers with 5 pooling layer after a convolutional block of either 2 or 3 convolutional layers and 3 fully connected (FC) layers after the last pooling layer.</p>
</sec>
<sec id="s5d2">
<title>Scene-parsing models</title>
<p>We use fully convolutional modification of VGG16 [<xref rid="c42" ref-type="bibr">42</xref>] trained on ADE20k [<xref rid="c26" ref-type="bibr">26</xref>], [<xref rid="c15" ref-type="bibr">15</xref>] (a scene-parsing dataset) as the scene-parsing model (VGG<sub>scene-parse</sub>). In VGG<sub>scene-parse</sub> (pretrained model downloaded from <ext-link ext-link-type="uri" xlink:href="https://github.com/hellochick/semantic-segmentation-tensorflow">https://github.com/hellochick/semantic-segmentation-tensorflow</ext-link>) (<xref rid="fig3" ref-type="fig">Fig 3B</xref>), the FC layers are replaced by convolutional layers to predict pixel-wise spatial mask. The model has additional deconvolutional layers to upsample the spatial mask obtained from the intermediate layers. We use pyramid scene parsing network (PSP<sub>scene-parse</sub>) for performing analysis of category specific outputs as PSP<sub>scene-parse</sub> (pretrained model downloaded from <ext-link ext-link-type="uri" xlink:href="https://github.com/hellochick/semantic-segmentation-tensorflow">https://github.com/hellochick/semantic-segmentation-tensorflow</ext-link>) outperforms VGG<sub>scene-parse</sub> on scene-parsing task and hence the categorical outputs are more accurate and suitable for that particular analysis. The PSP<sub>scene-parse</sub> model introduces a pyramid pooling module that fuses features of four different scales to obtain superior performance over VGG<sub>scene-parse</sub>.</p>
</sec>
<sec id="s5d3">
<title>Taskonomy models</title>
<p>Taskonomy dataset is a large-scale image dataset containing 4 million images with annotations and pretrained DNN models available for 26 vision related tasks. The tasks included in this dataset cover most common computer vision tasks related to 2D, 3D, and semantics. The tasks involved range from low-level visual tasks like edge detection to more abstract semantic tasks like scene/object classification. The architecture of DNNs (pretrained models downloaded from <ext-link ext-link-type="uri" xlink:href="https://github.com/StanfordVL/taskonomy/tree/master/taskbank">https://github.com/StanfordVL/taskonomy/tree/master/taskbank</ext-link>) trained on different tasks from Taskonomy dataset share a common encoder architecture. The encoder is a fully convolutional ResNet-50 [<xref rid="c28" ref-type="bibr">28</xref>] without any pooling layers. The encoder architecture consists of 4 residual blocks each containing multiple convolutional layers. The decoder architecture, however, varies according to the output structure of each task. For the tasks where the output is spatial (like edge detection, semantic segmentation), the decoder is a 15-layer fully convolutional network. For the tasks where the output is lower dimensional like scene/object classification, the decoder contains 2-3 FC layers.</p>
</sec>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We thank Michael Bonner for providing the permission to use the images from his paper, and insightful discussions. We thank Astha Gupta and Debidatta Dwibedi for helpful comments on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Yamins</surname> <given-names>DLK</given-names></string-name>, <string-name><surname>Hong</surname> <given-names>H</given-names></string-name>, <string-name><surname>Cadieu</surname> <given-names>CF</given-names></string-name>, <string-name><surname>Solomon</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Seibert</surname> <given-names>D</given-names></string-name>, <string-name><surname>DiCarlo</surname> <given-names>JJ.</given-names></string-name> <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2014</year>;<volume>111</volume>(<issue>23</issue>):<fpage>8619</fpage>&#x2013;<lpage>8624</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Martin Cichy</surname> <given-names>R</given-names></string-name>, <string-name><surname>Khosla</surname> <given-names>A</given-names></string-name>, <string-name><surname>Pantazis</surname> <given-names>D</given-names></string-name>, <string-name><surname>Oliva</surname> <given-names>A.</given-names></string-name> <article-title>Dynamics of scene representations in the human brain revealed by magnetoencephalography and deep neural networks</article-title>. <source>NeuroImage</source>. <year>2017</year>;<volume>153</volume>:<fpage>346</fpage>&#x2013;<lpage>358</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.03.063</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Tacchetti</surname> <given-names>A</given-names></string-name>, <string-name><surname>Isik</surname> <given-names>L</given-names></string-name>, <string-name><surname>Poggio</surname> <given-names>T.</given-names></string-name> <source>Invariant recognition drives neural representations of action sequences</source>. <year>2016</year>; p. <fpage>1</fpage>&#x2013;<lpage>23</lpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1005859</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Khaligh-Razavi</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Kriegeskorte</surname> <given-names>N.</given-names></string-name> <article-title>Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation</article-title>. <source>PLoS Computational Biology</source>. <year>2014</year>;<volume>10</volume>(<issue>11</issue>). doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Cichy</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Khosla</surname> <given-names>A</given-names></string-name>, <string-name><surname>Pantazis</surname> <given-names>D</given-names></string-name>, <string-name><surname>Torralba</surname> <given-names>A</given-names></string-name>, <string-name><surname>Oliva</surname> <given-names>A.</given-names></string-name> <article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title>. <source>Scientific Reports</source>. <year>2016</year>;<volume>6</volume>(June):<fpage>1</fpage>&#x2013;<lpage>13</lpage>. doi:<pub-id pub-id-type="doi">10.1038/srep27755</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Yamins</surname> <given-names>DL</given-names></string-name>, <string-name><surname>DiCarlo</surname> <given-names>JJ.</given-names></string-name> <article-title>Using goal-driven deep learning models to understand sensory cortex</article-title>. <source>Nature neuroscience</source>. <year>2016</year>;<volume>19</volume>(<issue>3</issue>):<fpage>356</fpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Horikawa</surname> <given-names>T</given-names></string-name>, <string-name><surname>Kamitani</surname> <given-names>Y.</given-names></string-name> <article-title>Generic decoding of seen and imagined objects using hierarchical visual features</article-title>. <source>Nature communications</source>. <year>2017</year>;<volume>8</volume>:<fpage>15037</fpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Nayebi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bear</surname> <given-names>D</given-names></string-name>, <string-name><surname>Kubilius</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kar</surname> <given-names>K</given-names></string-name>, <string-name><surname>Ganguli</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sussillo</surname> <given-names>D</given-names></string-name>, <etal>et al.</etal> <article-title>Task-Driven Convolutional Recurrent Models of the Visual System</article-title>. <source>arXiv preprint arXiv</source>:<fpage>180700053</fpage>. <year>2018</year>;.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Seeliger</surname> <given-names>K</given-names></string-name>, <string-name><surname>Fritsche</surname> <given-names>M</given-names></string-name>, <string-name><surname>G&#x00FC;&#x00E7;cl&#x00FC;</surname> <given-names>U</given-names></string-name>, <string-name><surname>Schoenmakers</surname> <given-names>S</given-names></string-name>, <string-name><surname>Schoffelen</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Bosch</surname> <given-names>S</given-names></string-name>, <etal>et al.</etal> <article-title>Convolutional neural network-based encoding and decoding of visual object recognition in space and time</article-title>. <source>NeuroImage</source>. <year>2017</year>;.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>van Gerven</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bohte</surname> <given-names>S.</given-names></string-name> <article-title>Editorial: Artificial Neural Networks as Models of Neural Information Processing</article-title>. <source>Artificial Neural Networks as Models of Neural Information Processing</source>. <year>2018</year>; p. <fpage>5</fpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Gu&#x00FC;c&#x00E7;lu&#x00FC;</surname> <given-names>U</given-names></string-name>, <string-name><surname>van Gerven</surname> <given-names>MAJ.</given-names></string-name> <source>Modeling the dynamics of human brain activity with recurrent neural networks</source>. <year>2016</year>;<volume>11</volume>(February):<fpage>1</fpage>&#x2013;<lpage>14</lpage>. doi:<pub-id pub-id-type="doi">10.3389/fncom.2017.00007</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Bonner</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Epstein</surname> <given-names>RA.</given-names></string-name> <article-title>Computational mechanisms underlying cortical responses to the affordance properties of visual scenes</article-title>. <source>PLOS Computational Biology</source>;doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1006111</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Groen</surname> <given-names>II</given-names></string-name>, <string-name><surname>Greene</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Baldassano</surname> <given-names>C</given-names></string-name>, <string-name><surname>Fei-Fei</surname> <given-names>L</given-names></string-name>, <string-name><surname>Beck</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Baker</surname> <given-names>CI.</given-names></string-name> <article-title>Distinct contributions of functional and deep neural network features to representational similarity of scenes in human brain and behavior</article-title><source>Elife</source>. <year>2018</year>;<volume>7</volume>:<fpage>e32962</fpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Bonner</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Epstein</surname> <given-names>RA.</given-names></string-name> <article-title>Coding of navigational affordances in the human visual system</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2017</year>;<volume>114</volume>(<issue>18</issue>):<fpage>4793</fpage>&#x2013;<lpage>4798</lpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Zhou</surname> <given-names>B</given-names></string-name>, <string-name><surname>Zhao</surname> <given-names>H</given-names></string-name>, <string-name><surname>Puig</surname> <given-names>X</given-names></string-name>, <string-name><surname>Fidler</surname> <given-names>S</given-names></string-name>, <string-name><surname>Barriuso</surname> <given-names>A</given-names></string-name>, <string-name><surname>Torralba</surname> <given-names>A.</given-names></string-name> <article-title>Scene Parsing Through ADE20K Dataset</article-title>. In: <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>; <year>2017</year>. p. <fpage>633</fpage>&#x2013;<lpage>641</lpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Zamir</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Sax</surname> <given-names>A</given-names></string-name>, <string-name><surname>Shen</surname> <given-names>W.</given-names></string-name> <source>Taskonomy: Disentangling Task Transfer Learning</source>;.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Epstein</surname> <given-names>R</given-names></string-name>, <string-name><surname>Harris</surname> <given-names>A</given-names></string-name>, <string-name><surname>Stanley</surname> <given-names>D</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N.</given-names></string-name> <source>The parahippocampal place area: Recognition, navigation, or encoding? Neuron</source>. <year>1999</year>;<volume>23</volume>(<issue>1</issue>):<fpage>115</fpage>&#x2013;<lpage>125</lpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Bradley</surname> <given-names>D.</given-names></string-name> <article-title>Early visual cortex: Smarter than you think</article-title>. <source>Current Biology</source>. <year>2001</year>;<volume>11</volume>(<issue>3</issue>):<fpage>R95</fpage>&#x2013;<lpage>R98</lpage>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Tootell</surname> <given-names>RB</given-names></string-name>, <string-name><surname>Hadjikhani</surname> <given-names>NK</given-names></string-name>, <string-name><surname>Vanduffel</surname> <given-names>W</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Mendola</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Sereno</surname> <given-names>MI</given-names></string-name>, <etal>et al.</etal> <article-title>Functional analysis of primary visual cortex (V1) in humans</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1998</year>;<volume>95</volume>(<issue>3</issue>):<fpage>811</fpage>&#x2013;<lpage>817</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Biederman</surname> <given-names>I.</given-names></string-name> <article-title>Recognition-by-components: a theory of human image understanding</article-title>. <source>Psychological review</source>. <year>1987</year>;<volume>94</volume>(<issue>2</issue>):<fpage>115</fpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Kornblith</surname> <given-names>S</given-names></string-name>, <string-name><surname>Cheng</surname> <given-names>X</given-names></string-name>, <string-name><surname>Ohayon</surname> <given-names>S</given-names></string-name>, <string-name><surname>Tsao</surname> <given-names>DY.</given-names></string-name> <article-title>A network for scene processing in the macaque temporal lobe</article-title>. <source>Neuron</source>. <year>2013</year>;<volume>79</volume>(<issue>4</issue>):<fpage>766</fpage>&#x2013;<lpage>781</lpage>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Walther</surname> <given-names>DB</given-names></string-name>, <string-name><surname>Chai</surname> <given-names>B</given-names></string-name>, <string-name><surname>Caddigan</surname> <given-names>E</given-names></string-name>, <string-name><surname>Beck</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Fei-Fei</surname> <given-names>L.</given-names></string-name> <article-title>Simple line drawings suffice for functional MRI decoding of natural scene categories</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2011</year>;<volume>108</volume>(<issue>23</issue>):<fpage>9661</fpage>&#x2013;<lpage>9666</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Choo</surname> <given-names>H</given-names></string-name>, <string-name><surname>Walther</surname> <given-names>DB.</given-names></string-name> <article-title>Contour junctions underlie neural representations of scene categories in high-level human visual cortex</article-title>. <source>Neuroimage</source>. <year>2016</year>;<volume>135</volume>:<fpage>32</fpage>&#x2013;<lpage>44</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Kriegeskorte</surname> <given-names>N</given-names></string-name>, <string-name><surname>Mur</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bandettini</surname> <given-names>PA.</given-names></string-name> <article-title>Representational similarity analysis-connecting the branches of systems neuroscience</article-title>. <source>Frontiers in systems neuroscience</source>. <year>2008</year>;<volume>2</volume>:<fpage>4</fpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Nimon</surname> <given-names>KF</given-names></string-name>, <string-name><surname>Oswald</surname> <given-names>FL.</given-names></string-name> <article-title>Understanding the results of multiple linear regression: Beyond standardized regression coefficients</article-title>. <source>Organizational Research Methods</source>. <year>2013</year>;<volume>16</volume>(<issue>4</issue>):<fpage>650</fpage>&#x2013;<lpage>674</lpage>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Zhou</surname> <given-names>B</given-names></string-name>, <string-name><surname>Zhao</surname> <given-names>H</given-names></string-name>, <string-name><surname>Puig</surname> <given-names>X</given-names></string-name>, <string-name><surname>Fidler</surname> <given-names>S</given-names></string-name>, <string-name><surname>Barriuso</surname> <given-names>A</given-names></string-name>, <string-name><surname>Torralba</surname> <given-names>A.</given-names></string-name> <article-title>Semantic understanding of scenes through the ade20k dataset</article-title>. <source>arXiv preprint arXiv</source>:<fpage>160805442</fpage>. <year>2016</year>;.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Zhao</surname> <given-names>H</given-names></string-name>, <string-name><surname>Shi</surname> <given-names>J</given-names></string-name>, <string-name><surname>Qi</surname> <given-names>X</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Jia</surname> <given-names>J.</given-names></string-name> <article-title>Pyramid Scene Parsing Network</article-title>. In: <source>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>; <year>2017</year>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>He</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Ren</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sun</surname> <given-names>J.</given-names></string-name> <article-title>Deep residual learning for image recognition</article-title>. In: <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>; <year>2016</year>. p. <fpage>770</fpage>&#x2013;<lpage>778</lpage>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Zhou</surname> <given-names>B</given-names></string-name>, <string-name><surname>Khosla</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lapedriza</surname> <given-names>A</given-names></string-name>, <string-name><surname>Oliva</surname> <given-names>A</given-names></string-name>, <string-name><surname>Torralba</surname> <given-names>A.</given-names></string-name> <article-title>Object detectors emerge in deep scene cnns</article-title>. <source>arXiv preprint arXiv:14126856</source>. <year>2014</year>;.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Kolve</surname> <given-names>E</given-names></string-name>, <string-name><surname>Mottaghi</surname> <given-names>R</given-names></string-name>, <string-name><surname>Gordon</surname> <given-names>D</given-names></string-name>, <string-name><surname>Zhu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Gupta</surname> <given-names>A</given-names></string-name>, <string-name><surname>Farhadi</surname> <given-names>A.</given-names></string-name> <article-title>AI2-THOR: An interactive 3d environment for visual AI</article-title>. <source>arXiv preprint arXiv:171205474</source>. <year>2017</year>;.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Gordon</surname> <given-names>D</given-names></string-name>, <string-name><surname>Kembhavi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rastegari</surname> <given-names>M</given-names></string-name>, <string-name><surname>Redmon</surname> <given-names>J</given-names></string-name>, <string-name><surname>Fox</surname> <given-names>D</given-names></string-name>, <string-name><surname>Farhadi</surname> <given-names>A.</given-names></string-name> <source>IQA: Visual question answering in interactive environments</source>;.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Savva</surname> <given-names>M</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>AX</given-names></string-name>, <string-name><surname>Dosovitskiy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Funkhouser</surname> <given-names>T</given-names></string-name>, <string-name><surname>Koltun</surname> <given-names>V.</given-names></string-name> <article-title>MINOS: Multimodal indoor simulator for navigation in complex environments</article-title>. <source>arXiv preprint arXiv:171203931</source>. <year>2017</year>;.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Das</surname> <given-names>A</given-names></string-name>, <string-name><surname>Datta</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gkioxari</surname> <given-names>G</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>S</given-names></string-name>, <string-name><surname>Parikh</surname> <given-names>D</given-names></string-name>, <string-name><surname>Batra</surname> <given-names>D.</given-names></string-name> <source>Embodied question answering</source>;.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Yan</surname> <given-names>C</given-names></string-name>, <string-name><surname>Misra</surname> <given-names>D</given-names></string-name>, <string-name><surname>Bennnett</surname> <given-names>A</given-names></string-name>, <string-name><surname>Walsman</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bisk</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Artzi</surname> <given-names>Y.</given-names></string-name> <source>CHALET: Cornell house agent learning environment</source>. arXiv preprint arXiv:180107357. <year>2018</year>;.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Ganaden</surname> <given-names>RE</given-names></string-name>, <string-name><surname>Mullin</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Steeves</surname> <given-names>JK.</given-names></string-name> <article-title>Transcranial magnetic stimulation to the transverse occipital sulcus affects scene but not object processing</article-title>. <source>Journal of cognitive neuroscience</source>. <year>2013</year>;<volume>25</volume>(<issue>6</issue>):<fpage>961</fpage>&#x2013;<lpage>968</lpage>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Dilks</surname> <given-names>DD</given-names></string-name>, <string-name><surname>Julian</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Paunov</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N.</given-names></string-name> <article-title>The occipital place area is causally and selectively involved in scene perception</article-title>. <source>Journal of Neuroscience</source>. <year>2013</year>;<volume>33</volume>(<issue>4</issue>):<fpage>1331</fpage>&#x2013;<lpage>1336</lpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Zhou</surname> <given-names>B</given-names></string-name>, <string-name><surname>Lapedriza</surname> <given-names>A</given-names></string-name>, <string-name><surname>Khosla</surname> <given-names>A</given-names></string-name>, <string-name><surname>Oliva</surname> <given-names>A</given-names></string-name>, <string-name><surname>Torralba</surname> <given-names>A.</given-names></string-name> <article-title>Places: A 10 million image database for scene recognition</article-title>. <source>IEEE transactions on pattern analysis and machine intelligence</source>. <year>2017</year>;.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Nili</surname> <given-names>H</given-names></string-name>, <string-name><surname>Wingfield</surname> <given-names>C</given-names></string-name>, <string-name><surname>Walther</surname> <given-names>A</given-names></string-name>, <string-name><surname>Su</surname> <given-names>L</given-names></string-name>, <string-name><surname>Marslen-Wilson</surname> <given-names>W</given-names></string-name>, <string-name><surname>Kriegeskorte</surname> <given-names>N.</given-names></string-name> <article-title>A toolbox for representational similarity analysis</article-title>. <source>PLoS computational biology</source>. <year>2014</year>;<volume>10</volume>(<issue>4</issue>):<fpage>e1003553</fpage>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Micallef</surname> <given-names>L</given-names></string-name>, <string-name><surname>Rodgers</surname> <given-names>P.</given-names></string-name> <article-title>eulerAPE: drawing area-proportional 3-Venn diagrams using ellipses</article-title>. <source>PloS one</source>. <year>2014</year>;<volume>9</volume>(<issue>7</issue>):<fpage>e101717</fpage>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Simonyan</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zisserman</surname> <given-names>A.</given-names></string-name> <article-title>Very deep convolutional networks for large-scale image recognition</article-title>. <source>arXiv preprint arXiv:14091556</source>. <year>2014</year>;.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Krizhevsky</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sutskever</surname> <given-names>I</given-names></string-name>, <string-name><surname>Hinton</surname> <given-names>GE.</given-names></string-name> <article-title>Imagenet classification with deep convolutional neural networks</article-title>. In: <source>Advances in neural information processing systems</source>;<year>2012</year>. p. <fpage>1097</fpage>&#x2013;<lpage>1105</lpage>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Long</surname> <given-names>J</given-names></string-name>, <string-name><surname>Shelhamer</surname> <given-names>E</given-names></string-name>, <string-name><surname>Darrell</surname> <given-names>T.</given-names></string-name> <article-title>Fully convolutional networks for semantic segmentation</article-title>. In: <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>; <year>2015</year>. p. <fpage>3431</fpage>&#x2013;<lpage>3440</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>