<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/424507</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Bioengineering</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Automated Summarisation of SDOCT Volumes using Deep Learning: Transfer Learning vs <italic>de novo</italic> Trained Networks</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6882-2444</contrib-id>
<name>
<surname>Antony</surname>
<given-names>Bhavna J.</given-names>
</name>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Maetschke</surname>
<given-names>Stefan</given-names>
</name>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Garnavi</surname>
<given-names>Rahil</given-names>
</name>
</contrib>
<aff id="a1"><institution>IBM Research Australia</institution>, 22/60 City Road, IBM Center, Southbank, VIC 3006, <country>Australia</country></aff>
</contrib-group>
<author-notes>
<fn id="n1"><label>&#x002A;</label><p><email>bhavna.antony@au1.ibm.com</email></p></fn>
</author-notes>
<pub-date pub-type="epub"><year>2018</year></pub-date>
<elocation-id>424507</elocation-id>
<history>
<date date-type="received">
<day>21</day>
<month>9</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>21</day>
<month>9</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>21</day>
<month>9</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="424507.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Spectral-domain optical coherence tomography (SDOCT) is a non-invasive imaging modality that generates high-resolution volumetric images. This modality finds widespread usage in ophthalmology for the diagnosis and management of various ocular conditions. The volumes generated can contain 200 or more B-scans. Manual inspection of such large quantity of scans is time consuming and error prone in most clinical settings. Here, we present a method for the generation of visual summaries of SDOCT volumes, wherein a small set of B-scans that highlight the most clinically relevant features in a volume are extracted. The method was trained and evaluated on data acquired from age-related macular degeneration patients, and &#x201C;relevance&#x201D; was defined as the presence of visibly discernible structural abnormalities. The summarisation system consists of a detection module, where relevant B-scans are extracted from the volume, and a set of rules that determines which B-scans are included in the visual summary. Two deep learning approaches are presented and compared for the classification of B-scans - transfer learning and <italic>de novo</italic> learning. Both approaches performed comparably with AUCs of 0.97 and 0.96, respectively, obtained on an independent test set. The <italic>de novo</italic> network, however, was 98&#x0025; smaller than the transfer learning approach, and had a run-time that was also significantly shorter.</p>
</abstract>
<counts>
<page-count count="25"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>The detection of key frames is a common approach employed in video analysis, particularly for the summarisation of video sequences. The techniques typically rely on the detection of explicit features of interest such as motion [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>] as well as other features such as edge information [<xref ref-type="bibr" rid="c3">3</xref>] and self-similarity [<xref ref-type="bibr" rid="c4">4</xref>]. Condensing videos via shot boundary detection has also been applied to the summarisation of video sequences [<xref ref-type="bibr" rid="c5">5</xref>].</p>
<p>In medical imaging, the detection of keyframes is more commonly found in the analysis of angiogram video sequences, but is less common in other radiographic modalities. Gibson <italic>et al</italic>. [<xref ref-type="bibr" rid="c6">6</xref>] described an approach for the compression of angiogram videos by detecting diagnostically relevant frames in videos and ensuring they were preserved. Syeda-Mahmood <italic>et al</italic>. [<xref ref-type="bibr" rid="c7">7</xref>] presented an approach for the detection of key frames in angiogram video analysis by detecting the vessels and selecting frames in which their visibility was best. However, both of these approaches relied on the explicit detection of features of interest in the images in order to identify them as &#x201C;key&#x201D;.</p>
<p>In ophthalmology, spectral-domain optical coherence tomography (SDOCT) [<xref ref-type="bibr" rid="c8">8</xref>] has begun to find widespread use for the diagnosis and management of various ocular conditions. This non-invasive imaging modality relies on laser interferometry to generate high-resolution images of the retina, which allows for the visualisation and quantification of structures in 3-D. These volumetric images are comprised of B-scans, which numbers can range from as few as five to two hundred or more. Summarisation of these volumes in current scanning systems is usually limited to a report that indicates the thicknesses of retinal layers. While such a report shows large pathologies such as choroidal neovascularizations (CNV), smaller abnormal indicators such as drusen, epiretinal membranes and microcytic macular edema would not be visible. Thus, visual summaries could complement the existing approach, by highlighting the pathological conditions that are currently not quantified. Previously, Chakravarthy <italic>et. al</italic> [<xref ref-type="bibr" rid="c9">9</xref>] described an approach for the detection of B-scans that show choroidal neovascularization (CNV). The method relies on the detection of the retina, followed by a machine-learning approach for the detection of possible fluid patches in the images. While this approach does in fact extract the specific B-scans, the method is limited to CNVs associated with wet-AMD.</p>
<p>Here, we present a deep learning approach for the automated summarisation of SDOCT volumes. Similar to previous summarisation techniques our proposed system begins with the detection of &#x201C;key&#x201D; B-scans. The system was trained and tested on SDOCT volumes acquired from patients that presented with age-related macular degeneration (AMD), and &#x201C;relevance&#x201D; was defined on the basis of the presence of visibly discernible structural abnormalities. Using a deep learning approach for this task allows it to be posed as a recognition task, and thus, does not require the explicit extraction of features (such as CNV) in order to characterise the B-scan. We employed and compared two deep learning techniques for keyframe extraction, where one is a transfer learning technique based on a pretrained network, while the second is a <italic>de novo</italic> trained custom convolutional neural network (CNN) that is significantly smaller. Transfer learning is a commonly used technique that allows for the repurposing of pretrained networks in applications where data might be scarce (as is commonly the case in medical imaging). Once the relevant B-scans had been identified, a set of rules were applied to generate the visual summary.</p>
<p>The paper is organised as follows: <xref ref-type="sec" rid="s2">Section 2</xref> details the data used in this experiment; Section!3 describes the two deep learning networks as well as the summarisation rules. The evaluation and comparison of the two networks is presented in Section!4, and a final discussion of the results can be found in <xref ref-type="sec" rid="s5">Section 5</xref>.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Data</title>
<p>The data used in the experiments were SDOCT images acquired as part of the AREDS2 Ancilliary Study. As detailed in [<xref ref-type="bibr" rid="c10">10</xref>], the dataset was registered at ClinicalTrials.gov (Identifier: NCT00734487) and approved by the institutional review boards at 4 A2A SDOCT clinics. With adherence to the tenets of the Declaration of Helsinki, informed consent was obtained from all subjects.</p>
<p>The study cohort consisted of 115 healthy individuals and 269 patients with age-related macular degeneration (AMD). The images were acquired on a Bioptigen SDOCT scanner (Leica Microsystems Inc., Illinois) from an approximately 6.7&#x00D7;6.7mm area centred on the fovea. Each volumes consisted of 100 B-scans, each containing 1000 A-scans and 512 pixels per A-scan (see <xref ref-type="fig" rid="fig1">Fig. 1(a)</xref>). Further details of the study are provided in [<xref ref-type="bibr" rid="c10">10</xref>].</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p>(a) The dimensions of an SDOCT volume depicted on the en-face image (left) and the central B-scan of the volume (right). Examples of poor quality scans with (b) poor contrast, (c) large shadows, and (d) incorrect mirror position.</p></caption>
<graphic xlink:href="424507_fig1a.tif"/>
<graphic xlink:href="424507_fig1b.tif"/>
<graphic xlink:href="424507_fig1c.tif"/>
</fig>
<sec id="s2a">
<label>2.1</label>
<title>Data Annotation</title>
<p>The volumes were manually annotated and labeled as being healthy, relevant (containing visibly discernible structural change) or low-quality. For this, each B-scan was visualised and labeled relevant if any visual structural change was observed. Thus, retinal layer thinning (which is difficult to identify visually) was not considered a key feature. However, B-scans with even minor disruptions like small drusen, reticular pseudodrusen and epiretinal membranes were all labeled as relevant &#x201C;key&#x201D; B-scans.</p>
<p>The presence of large shadows, poor contrast and other artefacts (such as vinetting, mirror location errors) were flagged as poor quality B-scans (see <xref ref-type="fig" rid="fig1">Fig. 1(b)-(d)</xref>).</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Methods</title>
<p>Deep learning [<xref ref-type="bibr" rid="c11">11</xref>] has been successfully employed for a number of applications in computer vision such as image recognition [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>] and semantic segmentation [<xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>]. This technique has also found application in medical imaging [<xref ref-type="bibr" rid="c16">16</xref>] for recognition [<xref ref-type="bibr" rid="c17">17</xref>], segmentation [<xref ref-type="bibr" rid="c18">18</xref>&#x2013;<xref ref-type="bibr" rid="c20">20</xref>] as well as image registration [<xref ref-type="bibr" rid="c21">21</xref>&#x2013;<xref ref-type="bibr" rid="c23">23</xref>]. While larger architectures have shown to perform better than shallower networks, their training also requires larger datasets.</p>
<p>Transfer learning is a technique that re-purposes existing, trained models for new tasks by retraining only small parts of the network. As most weights of the network are left unchanged, this reduces the amount of training data required. Transfer learning lends itself to medical imaging quite well, as large datasets are difficult to acquire in the medical domain. Thus, this was the first technique we employed for the detection of relevant B-scans (detailed in <xref ref-type="sec" rid="s3a">Section 3.1</xref>). We utilised the 16-layer VGG network [<xref ref-type="bibr" rid="c13">13</xref>] that was initially trained for the ImageNet Challenge - a classification problem consisting of 1000 classes of natural scene images [<xref ref-type="bibr" rid="c24">24</xref>].</p>
<p>Transfer learning, however, is not without problems. The pre-trained networks were designed for object recognition in natural scene images, and require the input to be a 3-channel RGB image. SDOCT images are however, grayscale. Thus, a B-scan either has to be replicated three times to meet the required input dimensions, or a section of three slices (and only three) has to be used as network input. Designing and training a network <italic>de novo</italic> allows to circumvent this requirement and potentially could also result in a smaller or more accurate network. For comparison we therefore also designed a significantly smaller convolutional neural network (CNN) that was trained <italic>de novo</italic> (detailed in <xref ref-type="sec" rid="s3b">Section 3.2</xref>).</p>
<p>The networks were developed in Keras [<xref ref-type="bibr" rid="c25">25</xref>] with TensorFlow [<xref ref-type="bibr" rid="c26">26</xref>] as the backend and nuts-flow/ml [<xref ref-type="bibr" rid="c27">27</xref>] for the data pre-processing.</p>
<sec id="s3a">
<label>3.1</label>
<title>Transfer Learning Approach</title>
<p>The structure of the 16-layer VGG network [<xref ref-type="bibr" rid="c13">13</xref>] is as shown in <xref ref-type="fig" rid="fig2">Fig. 2(a)</xref>. It consists of 3&#x00D7;3 convolutional filters with a stride of 1; padded to preserve spatial resolution. All layers utilised rectified linear unit [<xref ref-type="bibr" rid="c12">12</xref>] (ReLU) activation. The original network for an input RGB (3-channel) image of 224&#x2212;224 pixels in size contained 138 million parameters. See [<xref ref-type="bibr" rid="c13">13</xref>] regarding training of the original network.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><p>The structure of the (a) original VGG-16 network [<xref ref-type="bibr" rid="c13">13</xref>], and (b) the modified transfer network for keyframe detection. Note that the layers in the first five banks of CNNs have not been changed from the original network, and that only the last three fully-connected layers were re-trained.</p></caption>
<graphic xlink:href="424507_fig2.tif"/>
</fig>
<p>Since the classification task at hand is a 2-class problem, the model was changed to reflect this (see <xref ref-type="fig" rid="fig2">Fig. 2(b)</xref>). Furthermore, the two fully connected layers prior to the final layer were reduced in size from 4096 to 1024 and 512, respectively. Removal of the two original fully connected layers allowed for the input size of the network to be changed to 300&#x00D7;512&#x00D7;3 pixels. For an input image of this size, the total size of the network is 18.6 million parameters. The five blocks of convolutional filters were used as feature extractors and were not re-trained or fine-tuned, resulting in a network with 3.8 million trainable parameters.</p>
<p>Network weights were opimized by Adam [<xref ref-type="bibr" rid="c28">28</xref>], with parameters set to recommended values (learning rate set to 1<sup>&#x2212;6</sup>, <italic>&#x03B2;</italic><sub>1</sub> = 0.9, <italic>&#x03B2;</italic><sub>2</sub> = 0.999, and <italic>&#x2208;</italic> = 1<sup>&#x2212;8</sup>). The loss function was the balanced cross-entropy loss function:
<disp-formula id="eqn1">
<alternatives>
<graphic xlink:href="424507_eqn1.gif"/>
</alternatives>
</disp-formula>
where, <italic>y<sub>i</sub></italic> is the true label and <italic>y&#x0302;<sub>i</sub></italic> is the predicated label of the <italic>i</italic>-th sample, and <italic>C</italic><sub>1</sub> and <italic>C</italic><sub>2</sub> are the number of samples of the first and second class in the batch, respectively. This loss function, being normalised by the number of samples in each class helps with class imbalances. Training was stopped when the validation loss did not decrease by more than 0.1 or after 150 epochs.</p>
<sec id="s3a1">
<title>Data preprocessing</title>
<p>The individual B-scans in each volume are 512&#x00D7;1024 pixels in size. The retina however, does not encompass the entire B-scan, with a large portion of the image showing the vitreous, choroid and scleral tissue. Thus, detecting and extracting the image region that contains the retina reduces the image size. Therefore, the image was filtered with a gaussian derivative filter (first order, <italic>&#x03C3;</italic>=6.0). This generated a high response at the internal limiting membrane (ILM), and the ellipsoid zone of the photoreceptors as shown in <xref ref-type="fig" rid="fig3">Fig. 3(a)-(b)</xref>. This response was then thresholded (using a threshold obtained by Otsu&#x2019;s method [<xref ref-type="bibr" rid="c29">29</xref>]), and the largest connected components were detected. Since the largest two components belong to the retinal surfaces, their locations defined the bounding boxes (at least 300 pixels in height) around the retina, and B-scan were cropped to this size. The cropped B-scan was finally resized to 300&#x00D7;512, and replicated three times to match the requirements of the VGG-16 network, which expects the input to be a 3-channel image. The training data was augmented through random rotations (&#x00B1;5&#x00B0;), translations (&#x00B1;10 pixels), contrast scaling (0.3, 1.7) and flipping along the horizontal axis.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Outline of the data preprocessing steps showing (a) the original slice, (b) the filtered image, (c) connected components of the thresholded image, and (d) the final cropped image indicated by the red box.</p></caption>
<graphic xlink:href="424507_fig3.tif"/>
</fig>
</sec>
</sec>
<sec id="s3b">
<label>3.2</label>
<title>De Novo Network</title>
<p>The <italic>de novo</italic> CNN consisted of 4-layers with 32, 64, 128, and 128 filters (3&#x00D7;3 in size). Skip connections [<xref ref-type="bibr" rid="c30">30</xref>] were introduced between the layers, with the outputs from the previous layers being concatenated prior to pooling. ReLU activation, and pooling (maximum in a 2&#x00D7;2 window) followed each CNN layer (see <xref ref-type="fig" rid="fig4">Fig. 4</xref>). A global average pooling (GAP) layer was added to enable the generation of class activation maps (CAM) [<xref ref-type="bibr" rid="c31">31</xref>]. Finally, a fully connected layer (size = 2) with a softmax output provided the class probability for the input B-scan. The resulting network contained only 391300 trainable parameters, and is 2&#x0025; the size of the transfer learning network. As before, training was performed by Adam with the balanced cross-entropy loss function (see <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Architecture of the <italic>de novo</italic> network for keyframe detection.</p></caption>
<graphic xlink:href="424507_fig4.tif"/>
</fig>
<sec id="s3b1">
<title>Data Preprocessing</title>
<p>The individual B-scans were processed as described for the transfer-learning approach, beginning with the detection of the retina followed by image cropping at the bounding box. The resulting images were down-sampled by a factor of 2 (final size 150&#x00D7;256 pixels) and directly inputted into the network as 1-channel gray-scale images. Training data was augmented as before, employing random rotations (&#x00B1; 5&#x00B0;), translations (&#x00B1; 10 pixels), contrast scaling (0.3, 1.7) and flipping along the horizontal axis.</p>
</sec>
</sec>
<sec id="s3c">
<label>3.3</label>
<title>Experimental Setup</title>
<p>This annotated dataset was then divided into training, validation and testing sets containing 75&#x0025;, 10&#x0025; and 15&#x0025; of the SDOCT volumes, respectively. The allocation of an entire volume to a set ensured that B-scans from a single volume were not distributed across the sets. The final numbers of B-scans - healthy and relevant - in each set are as shown in <xref ref-type="table" rid="tbl1">Table 1</xref>.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Sizes of training, validation and testing sets.</title></caption>
<graphic xlink:href="424507_tbl1.tif"/>
</table-wrap>
<p>The performance of the two networks was evaluated using the area under the curve (AUC). The false positive and false negative rates were also computed and compared for the two networks.</p>
</sec>
<sec id="s3d">
<label>3.4</label>
<title>Summarisation Rules</title>
<p>Once the relevant B-scans have been extracted, a set of rules is imposed to select the key-frames (see <xref ref-type="fig" rid="ufig1">Algorithm 1</xref>). If no relevant B-scans were identified by the deep learning framework, then three slices (two peripheral and one central slice) are returned by the system. Otherwise, the function <italic>RegionDetector()</italic> analyses the set of relevant B-scans <italic>F<sub>i</sub></italic>, <italic>i</italic> = 1, 2, <italic>&#x2026; N</italic>, and groups them into regions. If two of the identified B-scans are only separated by a small distance (preset threshold <italic>T</italic>), they are considered to part of the same region. This not only compensates for mislabeled B-scans (not correctly identified as relevant for the summary), but also aggregates small regions that show disease-induced change. For instance, drusen may be present in a small number of B-scans near the fovea, but the individual slices may be separated by a few slices that show no pathology. In such a situation, it is reasonable to aggregate them into a larger region. A flexible threshold <italic>T</italic>, controls the aggregation during run time.</p>
<p>Next, each region is represented by the first, median and last scan of each region. Thus, if <italic>M</italic> regions are detected in the volume, a total of 3<italic>M</italic> key-frames will be returned by the algorithm. Note, that selecting the median and not the midpoint between the first and last B-scans, ensures that the B-scan included in the summary will be one that was identified by the deep learning classifier as being relevant for the visual summary.</p>
<statement>
<p>
<fig id="ufig1" position="float" fig-type="figure">
<label>Algorithm 1:</label>
<caption><title>Summarisation Rules</title></caption>
<graphic xlink:href="424507_ufig1.tif"/>
</fig>
</p>
</statement>
</sec>
</sec>
<sec id="s4">
<label>4</label>
<title>Results</title>
<p>The training of the transfer learning network required approximately 1000 epochs, while <italic>de novo</italic> training needed nearly 2000 epochs. This is to be expected as the transfer learning network is pre-trained. The initial loss was also substantially larger for the <italic>de novo</italic> network (Note the difference in <italic>y</italic>-axes scales in <xref ref-type="fig" rid="fig5">Fig. 5</xref>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Loss and accuracy (AUC) monitored during (a) transfer learning and (b) <italic>de novo</italic> training.</p></caption>
<graphic xlink:href="424507_fig5.tif"/>
</fig>
<p>The AUC, computed on the test set alone, was 0.97 and 0.96 for the transfer learning and the <italic>de novo</italic> networks, respectively (see <xref ref-type="fig" rid="fig6">Fig. 6(c)</xref>). The sensitivity (upper left quadrant of the confusion matrix) was found to be 0.91 for the transfer learning and 0.90 for the <italic>de novo</italic> network (see <xref ref-type="fig" rid="fig6">Fig. 6(a)-(b)</xref>). Similarly, the specificity was found to be 0.91 and 0.89 for the transfer learning and <italic>de novo</italic> networks, respectively. <xref ref-type="fig" rid="fig7">Fig. 7</xref> shows instances of B-scans with mild ((a)-(c)) and severe ((d)-(f)) AMD-related pathologies that were correctly identified by the networks as being relevant to the visual summary. The CAM visualisation for mild and severe B-scans are shown in <xref ref-type="fig" rid="fig7">Fig. 7</xref>.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><p>Confusion matrices for the (a) transfer learning (TL) and (b) the <italic>de novo</italic> network. (c) The AUC plot for the two networks.</p></caption>
<graphic xlink:href="424507_fig6.tif"/>
</fig>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><p>Examples of AMD B-scans with small/mild (top row) and significant pathologies (third row). CAMs from the <italic>de novo</italic> network for the same images are shown in the second and fourth rows, respectively.</p></caption>
<graphic xlink:href="424507_fig7.tif"/>
</fig>
<p>The percentages of false positives arising from the control datasets was also computed, and found to be 6&#x0025; and 7.5&#x0025; for the transfer learning and <italic>de novo</italic> networks, respectively. Visualisations of these B-scans showed that poor quality scans from the healthy controls were sometimes misclassified. Errors in the retinal localisation as shown in <xref ref-type="fig" rid="fig8">Fig. 8(b)</xref> also led to misclassifications. The false negatives were analysed further in order to investigate why they were misclassified. Visually inspecting the results obtained from the transfer learning framework revealed that the false negatives in 73&#x0025; of the cases contained small pathological conditions such as isolated drusen (see <xref ref-type="fig" rid="fig8">Fig. 8(c)</xref>). Similarly, for the <italic>de novo</italic> network, 91&#x0025; of the false negatives showed &#x201C;mild&#x201D; visually discernible pathologies. However, there were also instances where geographic atrophy was not correctly identified as a pathology (see <xref ref-type="fig" rid="fig8">Fig. 8(d)</xref>). The dataset consists of horizontal as well as vertical scans [<xref ref-type="bibr" rid="c10">10</xref>], where normal B-scans close to the optic nerve head are visually very similar to geographic atrophy. Since the model did not incorporate this additional piece of information (horizontal or vertical scan), it is not surprising to see misclassifications of this particular pathology.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8.</label>
<caption><p>Examples of false positives from control subject scans (top row), and false positives in AMD scans (bottom row).</p></caption>
<graphic xlink:href="424507_fig8.tif"/>
</fig>
<sec id="s4a">
<label>4.1</label>
<title>Summarisation Result</title>
<p>An example of a visual summary generated by the system is displayed in <xref ref-type="fig" rid="fig9">Fig. 9</xref>. This scan showed pigment epithelial detachment at the fovea, and only a single region was identified by the summarisation rules.</p>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure 9.</label>
<caption><p>(a) The en-face projection image of the volume, with (b) all the key B-scans and (c) the final visual summary. The B-scans that correspond to the three locations are shown below, and colour coded to indicate their location in the volume.</p></caption>
<graphic xlink:href="424507_fig9.tif"/>
<graphic xlink:href="424507_fig9a.tif"/>
</fig>
<p>A second example of an SDOCT volume with drusen and geographic atrophy is presented in <xref ref-type="fig" rid="fig10">Fig. 10</xref>. Here three separate regions were detected by the summarisation algorithm as indicated by the blue, red and green regions in <xref ref-type="fig" rid="fig10">Fig. 10(b)</xref>. The final visual summary consisting of the B-scans that represent the three regions depicted in <xref ref-type="fig" rid="fig10">Fig. 10(c)</xref>. The individual B-scans from the three regions are shown in the three rows <xref ref-type="fig" rid="fig10">Fig. 10(d) - (l)</xref>, with the colours of the bounding boxes corresponding to the location indicated in <xref ref-type="fig" rid="fig10">Fig. 10(c)</xref>.</p>
<fig id="fig10" position="float" fig-type="figure">
<label>Figure 10.</label>
<caption><p>(a) The en-face projection image of the volume, with (b) all the key B-scans and (c) the final visual summary. The three B-scans corresponding (d) - (f) to the first region (blues), (g) - (i) the second region (reds), and (j) - (l) to the third region (greens).</p></caption>
<graphic xlink:href="424507_fig10.tif"/>
<graphic xlink:href="424507_fig10a.tif"/>
</fig>
</sec>
</sec>
<sec id="s5">
<label>5</label>
<title>Discussion &#x0026; Conclusions</title>
<p>SDOCT finds extensive use in ophthalmology for the visualisation and quantification of structures in the retina. This high-resolution modality generates vast quantities of data (&#x223C; 50MB per volume), making the visual inspection of these images time-consuming, tiring and therefore error prone. Summarisation of the SDOCT volumes has been limited to the extraction of structural measurements, such as retinal layer thicknesses or optic nerve head parameters such as cup-to-disc ratio. However, other conditions such as epiretinal membranes or intra-layer cysts (that do not affect retinal layer thickness) require the manual inspection of the B-scans in the OCT volume. Visual summaries that retrieve key B-scans and identify relevant regions of the scan can be a valuable addition to the existing diagnostic framework.</p>
<p>Previously proposed summarisation methods for videos or volumetric medical images rely on the detection of relevant features, similar to our approach. However, our method does not explicitly segment AMD-related pathologies (see [<xref ref-type="bibr" rid="c9">9</xref>]), but uses a deep learning network for the detection of B-scans that show structural abnormalities. Our method can be extended to any structural abnormality or even use a different definition of &#x201C;relevance&#x201D;. For instance, a similar system could be designed to extract B-scans where maximal temporal change is identified. This would allow to monitor a variety of conditions such as AMD (dry or wet) or even glaucoma, where changes at the optic cup are recorded over time. A system that analyses temporal SDOCT volumes, however, could not utilise transfer learning and would require a <italic>de novo</italic> network, designed and trained for the specific application. In our experiments, we found that the <italic>de novo</italic> training and network design can offer advantages over transfer learning, the most important being the relaxation of input restriction, e.g. three-channel input image of specific size vs a grayscale image of selectable size.</p>
<p>A separate experiment was conducted with the transfer learning network, where the input consisted of three adjacent B-scans instead of a replication of a single B-scan. Intuitively, one would expect that the use of adjacent B-scans would bolster the network&#x2019;s ability to detect the key B-scans, but this network performed worse. The conclusion to be drawn is not that the adjacent B-scans have no additional useful information, but that the transfer learning network is ill-equipped to leverage this. The VGG-16 network was originally designed for three-channel colour images where each channel presented different colour characteristics of the same image. Here, the adjacent B-scans might show differing structures (healthy B-scans adjacent to one with small drusen), and the network was not able to efficiently leverage this additional information. A <italic>de novo</italic> network, designed and trained for this, might do better, but we did not pursue this.</p>
<p>The <italic>de novo</italic> network, being custom designed, also allowed for the incorporation of a the global average pooling layer (to generate the CAMs), as well as skip-connections (known to assist in training). The inclusion of the CAM brings a degree of &#x201C;explainability&#x201D; to the system, where this visualisation indicates the source of the final class label. This output could also be used as an input to the rules that generate the visual summaries, where the size of would CAMs impact the inclusion in the visual summary. The <italic>de novo</italic> network was 98&#x0025; smaller than the VGG-16 network, but was found to be just as accurate and robust. This small size allowed for rapid training and required only two days (on a K80 NVIDIA GPU) instead of the six needed by the transfer learning network. Run-time is also affected by network size and the classification of an entire SDOCT volume only took four seconds for the <italic>de novo</italic> network, while the larger transfer learning network took 190 seconds (computed on a 2.5GHz Intel Core i7, 16GB RAM system).</p>
<p>In conclusion, the presented <italic>de novo</italic> network allows for the rapid and reliable detection of key B-scans in SDOCT volumes, which are used to generate visual summaries of volumes. In the future, we intend to extend the summarisation techniques to other disease models, as well as explore the use of small networks, designed specifically for the task at hand.</p>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>H. J.</given-names> <surname>Zhang</surname></string-name>, and <string-name><given-names>F.</given-names> <surname>Qi</surname></string-name>, &#x201C;<article-title>A novel video key-frame-extraction algorithm based on perceived motion energy model</article-title>,&#x201D; <source>IEEE Transactions on Circuits and Systems for Video Technology</source>, vol. <volume>13</volume>, no. <issue>10</issue>, pp. <fpage>1006</fpage>&#x2013;<lpage>1013</lpage>, <year>2003</year>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="confproc"><string-name><given-names>B.</given-names> <surname>Fauvet</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Bouthemy</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Gros</surname></string-name>, and <string-name><given-names>F.</given-names> <surname>Spindler</surname></string-name>, &#x201C;<article-title>A geometrical key-frame selection method exploiting dominant motion estimation in video</article-title>,&#x201D; <conf-name>International Conference on Image and Video Retrieval</conf-name>, pp. <fpage>419</fpage>&#x2013;<lpage>427</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Nam</surname></string-name> and <string-name><given-names>A. H.</given-names> <surname>Tewfik</surname></string-name>, &#x201C;<article-title>Detection of gradual transitions in video sequences using B-spline interpolation</article-title>,&#x201D; <source>IEEE Transactions on Multimedia</source>, vol. <volume>7</volume>, no. <issue>4</issue>, pp. <fpage>667</fpage>&#x2013;<lpage>679</lpage>, <year>2005</year>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="other"><string-name><given-names>P.</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Hilton</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Starck</surname></string-name>, &#x201C;<article-title>Automatic 3D video summarization: key frame extraction from self-similarity</article-title>,&#x201D; <source>The Fourth International Symposium on 3D Data Processing, Visualization and Transmission (3DPVT&#x2019;08)</source>, pp. <fpage>71</fpage>&#x2013;<lpage>78</lpage>, <year>2008</year>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Cotsaces</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Pitas</surname></string-name>, and <string-name><given-names>N.</given-names> <surname>Nikolaidis</surname></string-name>, &#x201C;<article-title>Video shot detection and condensed representation: a review</article-title>,&#x201D; <source>IEEE Signal Processing Magazine</source>, vol. <volume>23</volume>, no. <issue>2</issue>, pp. <fpage>28</fpage>&#x2013;<lpage>37</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><given-names>D.</given-names> <surname>Gibson</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Spann</surname></string-name>, and <string-name><given-names>S. I.</given-names> <surname>Woolley</surname></string-name>, &#x201C;<article-title>A wavelet-based region of interest encoder for the compression of angiogram video sequences</article-title>,&#x201D; <source>IEEE Transactions on Information Technology in Biomedicine</source>, vol. <volume>8</volume>, no. <issue>2</issue>, pp. <fpage>103</fpage>&#x2013;<lpage>113</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="confproc"><string-name><given-names>T.</given-names> <surname>Syeda-Mahmood</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Beymer</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Mahmood</surname></string-name>, <string-name><given-names>R. J.</given-names> <surname>Lundstrom</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Shafee</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Holve</surname></string-name>, &#x201C;<article-title>Automatic selection of keyframes from angiogram videos</article-title>,&#x201D; <conf-name>Proceedings - International Conference on Pattern Recognition</conf-name>, pp. <fpage>4008</fpage>&#x2013;<lpage>4011</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><given-names>D.</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>E. A.</given-names> <surname>Swanson</surname></string-name>, <string-name><given-names>C. P.</given-names> <surname>Lin</surname></string-name>, <string-name><given-names>J. S.</given-names> <surname>Schuman</surname></string-name>, <string-name><given-names>W. G.</given-names> <surname>Stinson</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Chang</surname></string-name>, <string-name><given-names>M. R.</given-names> <surname>Hee</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Flotte</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Gregory</surname></string-name>, and <string-name><given-names>C. A.</given-names> <surname>Puliafito</surname></string-name>, &#x201C;<article-title>Optical coherence tomography</article-title>,&#x201D; <source>Science</source>, vol. <volume>254</volume>, no. <issue>5035</issue>, pp. <fpage>1178</fpage>&#x2013;<lpage>1181</lpage>, <year>1991</year>, 1706.00490.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><given-names>U.</given-names> <surname>Chakravarthy</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Goldenberg</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Young</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Havilio</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Rafaeli</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Benyamini</surname></string-name>, and <string-name><given-names>A.</given-names> <surname>Loewenstein</surname></string-name>, &#x201C;<article-title>Automated identification of lesion activity in neovascular age-related macular degeneration</article-title>,&#x201D; <source>Ophthalmology</source>, vol. <volume>123</volume>, no. <issue>8</issue>, pp. <fpage>1731</fpage>&#x2013;<lpage>1736</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="other"><string-name><given-names>S.</given-names> <surname>Farsiu</surname></string-name>, <string-name><given-names>S. J.</given-names> <surname>Chiu</surname></string-name>, <string-name><given-names>R. V. O.</given-names> <surname>Connell</surname></string-name>, <string-name><given-names>F. A.</given-names> <surname>Folgar</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Yuan</surname></string-name>, <string-name><given-names>J. A.</given-names> <surname>Izatt</surname></string-name>, and <string-name><given-names>C. A.</given-names> <surname>Toth</surname></string-name>, &#x201C;<article-title>Quantitative classification of eyes with and without Intermediate age-related macular degeneration using optical coherence tomography</article-title>,&#x201D; <source>Ophthalmology</source>, pp. <fpage>1</fpage>&#x2013;<lpage>11</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Lecun</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name>, and <string-name><given-names>G.</given-names> <surname>Hinton</surname></string-name>, &#x201C;<article-title>Deep learning</article-title>,&#x201D; <source>Nature</source>, vol. <volume>521</volume>, no. <issue>7553</issue>, pp. <fpage>436</fpage>&#x2013;<lpage>444</lpage>, <year>2015</year>, arXiv:<pub-id pub-id-type="arxiv">1312.6184v5</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="other"><string-name><given-names>A.</given-names> <surname>Krizhevsky</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Sutskever</surname></string-name>, and <string-name><given-names>G. E.</given-names> <surname>Hinton</surname></string-name>, &#x201C;<article-title>ImageNet classification with deep convolutional neural networks</article-title>,&#x201D; in <source>Advances In Neural Information Processing Systems</source>, pp. <fpage>1097</fpage>&#x2013;<lpage>1105</lpage>, <year>2012</year>, 1102.0183.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="confproc"><string-name><given-names>K.</given-names> <surname>Simonyan</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Zisserman</surname></string-name>, &#x201C;<article-title>Very deep convolutional networks for large-scale image recognition</article-title>,&#x201D; in <conf-name>International Conference on Learning Representations (ICRL)</conf-name>, pp. <fpage>1</fpage>&#x2013;<lpage>14</lpage>, <year>2015</year>, 1409.1556.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="confproc"><string-name><given-names>S.</given-names> <surname>J&#x00E9;gou</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Drozdzal</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Vazquez</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Romero</surname></string-name>, and <string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name>, &#x201C;<article-title>The one hundred layers tiramisu: fully convolutional DenseNets for semantic segmentation</article-title>,&#x201D; in <conf-name>IEEE Conference on Computer Vision and Pattern Recognition Workshops</conf-name>, pp. <fpage>1175</fpage>&#x2013;<lpage>1183</lpage>, <year>2016</year>, 1611.09326.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="other"><string-name><given-names>L.-C.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Papandreou</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Kokkinos</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Murphy</surname></string-name>, and <string-name><given-names>A. L.</given-names> <surname>Yuille</surname></string-name>, &#x201C;<article-title>Deeplab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</article-title>,&#x201D; pp. <fpage>1</fpage>&#x2013;<lpage>14</lpage>, <year>2016</year>, 1606.00915.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><given-names>G.</given-names> <surname>Litjens</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Kooi</surname></string-name>, <string-name><given-names>B. E.</given-names> <surname>Bejnordi</surname></string-name>, <string-name><given-names>A. A. A.</given-names> <surname>Setio</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Ciompi</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Ghafoorian</surname></string-name>, <string-name><given-names>J. A. W. M.</given-names> <surname>van der Laak</surname></string-name>, <string-name><given-names>B.</given-names> <surname>van Ginneken</surname></string-name>, and <string-name><given-names>C. I.</given-names> <surname>S&#x00E1;nchez</surname></string-name>, &#x201C;<article-title>A survey on deep learning in medical image analysis</article-title>,&#x201D; <source>Medical Image Analysis</source>, vol. <volume>42</volume>, pp. <fpage>60</fpage>&#x2013;<lpage>88</lpage>, <year>2017</year>, 1702.05747.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><given-names>D. S.</given-names> <surname>Kermany</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Goldbaum</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Cai</surname></string-name>, <string-name><given-names>C. C.</given-names> <surname>Valentim</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Liang</surname></string-name>, <string-name><given-names>S. L.</given-names> <surname>Baxter</surname></string-name>, <string-name><given-names>A.</given-names> <surname>McKeown</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Yan</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Dong</surname></string-name>, <string-name><given-names>M. K.</given-names> <surname>Prasadha</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Pei</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Ting</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Zhu</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Hewett</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Dong</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Ziyar</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Shi</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Zheng</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Hou</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Shi</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Fu</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Duan</surname></string-name>, <string-name><given-names>V. A.</given-names> <surname>Huu</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Wen</surname></string-name>, <string-name><given-names>E. D.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>C. L.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Li</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>M. A.</given-names> <surname>Singer</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Sun</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Xu</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Tafreshi</surname></string-name>, <string-name><given-names>M. A.</given-names> <surname>Lewis</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Xia</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Zhang</surname></string-name>, &#x201C;<article-title>Identifying medical diagnoses and treatable diseases by image-based deep learning</article-title>,&#x201D; <source>Cell</source>, vol. <volume>172</volume>, no. <issue>5</issue>, pp. <fpage>1122</fpage>&#x2013;<lpage>1131.e9</lpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><given-names>S. K.</given-names> <surname>Devalla</surname></string-name>, <string-name><given-names>K. S.</given-names> <surname>Chin</surname></string-name>, <string-name><given-names>J.-M.</given-names> <surname>Mari</surname></string-name>, <string-name><given-names>T. A.</given-names> <surname>Tun</surname></string-name>, <string-name><given-names>N. G.</given-names> <surname>Strouthidis</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Aung</surname></string-name>, <string-name><given-names>A. H.</given-names> <surname>Thiery</surname></string-name>, and M. <string-name><given-names>J. A.</given-names> <surname>Girard</surname></string-name>, &#x201C;<article-title>A deep learning approach to digitally stain optical coherence tomography images of the optic nerve head</article-title>,&#x201D; <source>Investigative Opthalmology &#x0026; Visual Science</source>, vol. <volume>59</volume>, no. <issue>1</issue>, pp. <fpage>63</fpage>&#x2013;<lpage>74</lpage>, <year>2018</year>, arXiv:<pub-id pub-id-type="arxiv">1803.00232v</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="other"><string-name><given-names>A.</given-names> <surname>Shah</surname></string-name>, <string-name><given-names>M. D.</given-names> <surname>Abramoff</surname></string-name>, and <string-name><given-names>X.</given-names> <surname>Wu</surname></string-name>, &#x201C;<article-title>Simultaneous multiple surface segmentation using deep learning</article-title>,&#x201D; <year>2017</year>, arXiv:<pub-id pub-id-type="arxiv">1705.07142v1</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="other"><string-name><given-names>Y.</given-names> <surname>He</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Carass</surname></string-name>, <string-name><given-names>B. M.</given-names> <surname>Jedynak</surname></string-name>, <string-name><given-names>S. D.</given-names> <surname>Solomon</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Saidha</surname></string-name>, <string-name><given-names>P. A.</given-names> <surname>Calabresi</surname></string-name>, and <string-name><given-names>J. L.</given-names> <surname>Prince</surname></string-name>, &#x201C;<article-title>Topology guaranteed segmentation of the human retina from OCT using convolutional neural networks</article-title>,&#x201D; <year>2018</year>, arXiv:<pub-id pub-id-type="arxiv">1803.05120v1</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="other"><string-name><given-names>B. D.</given-names> <surname>de Vos</surname></string-name>, <string-name><given-names>F. F.</given-names> <surname>Berendsen</surname></string-name>, <string-name><given-names>M. A.</given-names> <surname>Viergever</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Staring</surname></string-name>, and <string-name><given-names>I.</given-names> <surname>Isgum</surname></string-name>, &#x201C;<article-title>End-to-end unsupervised deformable image network</article-title>,&#x201D; <source>Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</source>, pp. <fpage>204</fpage>&#x2013;<lpage>212</lpage>, <year>2017</year>, arXiv:<pub-id pub-id-type="arxiv">1704.06065v1</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><given-names>X.</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Kwitt</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Styner</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Niethammer</surname></string-name>, &#x201C;<article-title>Quicksilver: Fast predictive image registration &#x2013; a deep learning approach</article-title>,&#x201D; <source>NeuroImage</source>, vol. <volume>158</volume>, pp. <fpage>378</fpage>&#x2013;<lpage>396</lpage>, <year>2017</year>, 1703.10908.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="other"><string-name><given-names>M.</given-names> <surname>Dwarikanath</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Antony</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Sedai</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Garnavi</surname></string-name>, &#x201C;<article-title>Deformable medical image registration using generative adversarial networks</article-title>,&#x201D; <source>IEEE International Symposium on Biomedical Imaging</source>, pp. <fpage>1449</fpage>&#x2013;<lpage>1453</lpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><given-names>O.</given-names> <surname>Russakovsky</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Deng</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Su</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Krause</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Satheesh</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Ma</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Karpathy</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Khosla</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Bernstein</surname></string-name>, <string-name><given-names>A. C.</given-names> <surname>Berg</surname></string-name>, and <string-name><given-names>L.</given-names> <surname>Fei-Fei</surname></string-name>, &#x201C;<article-title>ImageNet Large Scale Visual Recognition Challenge</article-title>,&#x201D; <source>International Journal of Computer Vision</source>, vol. <volume>115</volume>, no. <issue>3</issue>, pp. <fpage>211</fpage>&#x2013;<lpage>252</lpage>, <year>2015</year>, 1409.0575.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="other"><string-name><given-names>F.</given-names> <surname>Chollet</surname></string-name>, &#x201C;<article-title>Keras</article-title>,&#x201D; <year>2015</year>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="other"><string-name><given-names>M.</given-names> <surname>Abadi</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Agarwal</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Barham</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Brevdo</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Citro</surname></string-name>, <string-name><given-names>G. S.</given-names> <surname>Corrado</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Davis</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Dean</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Devin</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Ghemawat</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Goodfellow</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Harp</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Irving</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Isard</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Jozefowicz</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Jia</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Kaiser</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Kudlur</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Levenberg</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Man&#x00E9;</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Schuster</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Monga</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Moore</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Murray</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Olah</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Shlens</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Steiner</surname></string-name>, <string-name><given-names>K. T.</given-names> <surname>Ilya Sutskever</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Tucker</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Vanhoucke</surname></string-name>, <string-name><given-names>F. V.</given-names> <surname>Vijay Vasudevan</surname></string-name>, <string-name><given-names>M. W.</given-names> <surname>Oriol Vinyals</surname></string-name>, <string-name><given-names>Pete</given-names> <surname>Warden</surname></string-name>, <string-name><given-names>Martin</given-names> <surname>Wattenberg</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Yu</surname></string-name>, and <string-name><given-names>X.</given-names> <surname>Zheng</surname></string-name>., &#x201C;<article-title>TensorFlow: Large-scale machine learning on heterogeneous systems</article-title>,&#x201D; <year>2015</year>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="other"><string-name><given-names>S.</given-names> <surname>Maetschke</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Tennakoon</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Vecchiola</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Garnavi</surname></string-name>, &#x201C;<article-title>nuts-flow/ml : data pre-processing for deep learning</article-title>,&#x201D; <year>2018</year>, arXiv:<pub-id pub-id-type="arxiv">1708.06046v2</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="confproc"><string-name><given-names>D. P.</given-names> <surname>Kingma</surname></string-name> and <string-name><given-names>J. L.</given-names> <surname>Ba</surname></string-name>, &#x201C;<article-title>Adam: a method for stochastic optimization</article-title>,&#x201D; <conf-name>International Conference on Learning Representations 2015</conf-name>, pp. <fpage>1</fpage>&#x2013;<lpage>15</lpage>, <year>2015</year>, 1412.6980.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><given-names>N.</given-names> <surname>Otsu</surname></string-name>, &#x201C;<article-title>A threshold selection method from gray-level histograms</article-title>,&#x201D; <source>IEEE Transactions on Systems, Man and Cybernetics</source>, vol. <volume>9</volume>, pp. <fpage>62</fpage>&#x2013;<lpage>66</lpage>, <year>1979</year>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="confproc"><string-name><given-names>K.</given-names> <surname>He</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Ren</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Sun</surname></string-name>, &#x201C;<article-title>Deep residual learning for image recognition</article-title>,&#x201D; in <conf-name>IEEE Conference on Computer Vision and Pattern Recognition</conf-name>, pp. <fpage>770</fpage>&#x2013;<lpage>778</lpage>, <year>2016</year>, 1512.03385.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="other"><string-name><given-names>B.</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Khosla</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Lapedriza</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Oliva</surname></string-name>, and <string-name><given-names>A.</given-names> <surname>Torralba</surname></string-name>, &#x201C;<article-title>Learning deep features for discriminative localization</article-title>,&#x201D; pp. <fpage>2921</fpage>&#x2013;<lpage>2929</lpage>, <year>2015</year>, 1512.04150.</mixed-citation></ref>
</ref-list>
</back>
</article>