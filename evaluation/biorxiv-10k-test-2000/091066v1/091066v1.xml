<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/091066</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A Bayesian Heteroscedastic GLM with Application to fMRI Data with Motion Spikes</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Eklund</surname><given-names>Anders</given-names></name>
<xref ref-type="aff" rid="a1">a</xref>
<xref ref-type="aff" rid="a2">b</xref>
<xref ref-type="aff" rid="a3">c</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Lindquist</surname><given-names>Martin A.</given-names></name>
<xref ref-type="aff" rid="a4">d</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Villani</surname><given-names>Mattias</given-names></name>
<xref ref-type="aff" rid="a1">a</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<aff id="a1"><label>a</label><institution>Division of Statistics &#x0026; Machine Learning, Department of Computer and Information Science, Link&#x00F6;ping University</institution>, Link&#x00F6;ping, <country>Sweden</country></aff>
<aff id="a2"><label>b</label><institution>Division of Medical Informatics, Department of Biomedical Engineering, Link&#x00F6;ping University</institution>, Link&#x00F6;ping, <country>Sweden</country></aff>
<aff id="a3"><label>c</label><institution>Center for Medical Image Science and Visualization (CMIV), Link&#x00F6;ping University</institution>, Link&#x00F6;ping, <country>Sweden</country></aff>
<aff id="a4"><label>d</label><institution>Department of Biostatistics, Johns Hopkins University</institution>, Baltimore, <country>USA</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label>Corresponding author Email address: <email>mattias.villani@liu.se</email> (Mattias Villani&#x002A;)</corresp>
</author-notes>
<pub-date pub-type="epub"><year>2016</year></pub-date>
<elocation-id>091066</elocation-id>
<history>
<date date-type="received">
<day>02</day>
<month>12</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>02</day>
<month>12</month>
<year>2016</year>
</date>
</history>
<permissions><copyright-statement>&#x00A9; 2016, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2016</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="091066.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract><title>Abstract</title>
<p>We propose a voxel-wise general linear model with autoregressive noise and heteroscedastic noise innovations (GLMH) for analyzing functional magnetic resonance imaging (fMRI) data. The model is analyzed from a Bayesian perspective and has the benefit of automatically down-weighting time points close to motion spikes in a data-driven manner. We develop a highly efficient Markov Chain Monte Carlo (MCMC) algorithm that allows for Bayesian variable selection among the regressors to model both the mean (i.e., the design matrix) and variance. This makes it possible to include a broad range of explanatory variables in both the mean and variance (e.g., time trends, activation stimuli, head motion parameters and their temporal derivatives), and to compute the posterior probability of inclusion from the MCMC output. Variable selection is also applied to the lags in the autoregressive noise process, making it possible to infer the lag order from the data simultaneously with all other model parameters. We use both simulated data and real fMRI data from OpenfMRI to illustrate the importance of proper modeling of heteroscedasticity in fMRI data analysis. Our results show that the GLMH tends to detect more brain activity, compared to its homoscedastic counterpart, by allowing the variance to change over time depending on the degree of head motion.</p>
</abstract>
<kwd-group kwd-group-type="author"><title>Keywords</title>
<kwd>Bayesian</kwd>
<kwd>fMRI</kwd>
<kwd>Heteroscedastic</kwd>
<kwd>MCMC</kwd>
<kwd>Head motion</kwd>
<kwd>Motion spikes</kwd>
</kwd-group>
<counts>
<page-count count="17"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1"><label>1.</label><title>Introduction</title>
<p>Functional magnetic resonance imaging (fMRI) is a non-invasive technique that has become the de facto standard for imaging human brain function in both healthy and diseased populations. The standard approach for analyzing fMRI data is to use the general linear model (GLM), proposed by Friston et al. [<xref ref-type="bibr" rid="c12">12</xref>]. The standard GLM has been extremely successful in a large number of empirical studies, but relies on a number of assumptions, including linearity, independency, Gaussianity and homoscedasticity (constant variance). Much work has been done to relax the assumption of independent errors, and several alternative noise models have been proposed [<xref ref-type="bibr" rid="c11">11</xref>, <xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c19">19</xref>, <xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c7">7</xref>]. In addition, it has also been investigated whether results are improved by using a Rician noise model [<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c21">21</xref>, <xref ref-type="bibr" rid="c32">32</xref>], instead of a Gaussian. While heteroscedastic models exist for group analyses [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c5">5</xref>], the homoscedasticity assumption for single subject analysis has received little attention. Luo and Nichols [<xref ref-type="bibr" rid="c20">20</xref>] used the Cook-Weisberg test for homoscedasticity to detect problematic voxels, but did not propose a heteroscedastic model to handle these. Diedrichsen and Shadmehr [<xref ref-type="bibr" rid="c6">6</xref>] claim that the homoscedasticity assumption is often violated in practice due to head motion, and propose an algorithm that estimates the noise variance separately at each time point. The estimated variances are then used to perform weighted least squares regression. The aim of this study is to further explore the appropriateness of the homoscedasticity assumption for single subject fMRI analysis, and evalute the effects of deviations from it.</p>
<sec id="s1a"><label>1.1.</label><title>Is fMRI noise heteroscedastic?</title>
<p>Consider a simple simulation where actual head motion is repeatedly applied to a single volume from an fMRI dataset, to generate a new 4D fMRI dataset where all the signal variation comes from simulated motion. Even if motion correction is applied to the dataset, the dataset will still contain motion related signal variation [<xref ref-type="bibr" rid="c14">14</xref>], due to the fact that the interpolation mixes voxels with low and high signal intensity (especially at the edge of the brain, and at the border between different tissue types). It is therefore common to include the estimated head motion parameters in the design matrix, to regress out any motion related variance that remains after the motion correction, and to also account for spin-history artifacts. It is also common to include the temporal derivative of the head motion parameters, to better model motion spikes. <xref ref-type="fig" rid="fig1">Figure 1</xref> shows a single time series from an fMRI dataset with simulated motion, before and after motion correction, and one of the head motion parameters. The selected voxel is at the border between white and gray matter. <xref ref-type="fig" rid="fig2">Figure 2</xref> shows three residual time series calculated using three different design matrices (and ordinary least squares regression), the first containing only an intercept and time trends, the second also containing motion covariates, and the third also containing the temporal derivative of the head motion. It is clear that using the estimated head motion as additional covariates removes most of the motion related variance, but not all of it. The residual time series still contain effects of a motion spike, which makes the noise heteroscedastic. It should also be stressed that real fMRI data are far more complicated, for example due to the fact that each fMRI volume is sampled one slice at a time, thus potentially giving rise to so-called &#x2018;spin-history&#x2019; artifacts which can further embed motion-related noise into the fMRI data.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure"><label>Figure 1:</label>
<caption><p>A single time series from a simulated fMRI dataset, before and after motion correction. Note that the motion corrected data still has a high signal variance, which is correlated with the head motion.</p></caption>
<graphic xlink:href="091066_fig1.tif"/>
</fig>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure"><label>Figure 2:</label>
<caption><p>Residual time series obtained after fitting models with three different design matrices. The first design matrix only contains covariates for the intercept and time trends (4 covariates in total). The second design matrix also contains head motion covariates (10 covariates in total), and the third design matrix also contains the temporal derivative of the head motion (16 covariates in total). For visualization purposes, a mean of 200 was added to the green residual, and a mean of 400 was added to the red residual. Note that a motion spike is still present in the green and the blue residual, making the noise heteroscedastic.</p></caption>
<graphic xlink:href="091066_fig2.tif"/>
</fig>
</sec>
<sec id="s1b"><label>1.2.</label><title>Modeling the heteroscedasticity</title>
<p>We propose a Bayesian heteroscedastic extension of the GLM, which uses covariates for both the mean and variance, and also incorporates an autoregressive noise model. We develop highly efficient Markov Chain Monte Carlo (MCMC) algorithms for simulating from the joint posterior distribution of all model parameters. Allowing for heteroscedasticity, where the noise variance is allowed to change over time, has the effect of automatically discounting scans with large uncertainty when inferring brain activity or connectivity. One way of thinking of this effect is in terms of weighted least squares estimation, where the optimal weights are learned from the data.</p>
</sec>
<sec id="s1c"><label>1.3.</label><title>Is fMRI noise heteroscedastic in all voxels?</title>
<p><xref ref-type="fig" rid="fig3">Figure 3</xref> shows three residual time series for a voxel in gray matter (close to the voxel shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>). Clearly, this voxel has a very low correlation with the simulated motion, and the residuals are not heteroscedastic. It is therefore not optimal to use the same weights in all voxels. Compared to the work by Diedrichsen and Shadmehr [<xref ref-type="bibr" rid="c6">6</xref>], our Bayesian approach independently estimates a heteroscedastic model for each voxel, instead of using variance scaling parameters that are the same for all voxels. Furthermore, Diedrichsen and Shadmehr [<xref ref-type="bibr" rid="c6">6</xref>] used a fix autoregressive (AR) model for the noise (AR(1) &#x002B; white noise with the AR parameter fixed to 0.2, as in the SPM software package), while we estimate an AR(<italic>k</italic>) model in each voxel. The fixed AR(1) model used by SPM has been shown to perform poorly [<xref ref-type="bibr" rid="c7">7</xref>], especially for short repetition times made possible with recently developed MR scanner sequences.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure"><label>Figure 3:</label>
<caption><p>Residual time series obtained after fitting models with three different design matrices. The first design matrix only contains covariates for intercept and time trends (4 covariates in total). The second design matrix also contains head motion covariates (10 covariates in total), and the third design matrix also contains the temporal derivative of the head motion (16 covariates in total). For visualization purposes, a mean of 200 was added to the green residual, and a mean of 400 was added to the red residual. Note that the time series in this voxel has a very low correlation with the simulated motion, and the residuals are therefore homoscedastic.</p></caption>
<graphic xlink:href="091066_fig3.tif"/>
</fig>
<p>Our Bayesian approach also differs from recently developed methods used in the field, where scrubbing or censoring is used to remove volumes with excessive head motion [<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c30">30</xref>]. Such approaches are ad hoc in the sense that an arbitrary motion threshold first needs to be applied, to determine which volumes to remove or censor. Another problem with these approaches is that they can significantly alter the temporal structure of the data.</p>
</sec>
<sec id="s1d"><label>1.4.</label><title>Variable selection</title>
<p>It can be difficult to determine which variables to include in the design matrix (i.e., the mean function) of the GLM, including those that capture scanner drift, or residual head movement effects after motion correction. It can be even more difficult to choose the appropriate explanatory variables to use in the variance function. For this reason we introduce variable selection priors in both the mean and variance function, which has the effect of automatically determining the set of explanatory variables; more precisely, we obtain the posterior inclusion probability for each of the candidate variables and the posterior distribution of their effect sizes from a single MCMC run. In addition, we have a third variable selection prior acting on the lags of the AR noise process which allows us to estimate the model order of the AR process directly from the data. This aspect is particularly important for high (sub-second) temporal resolution data. Our analysis here is massively univariate without modeling spatial dependencies, however we discuss possible extensions to spatial models in the Discussion.</p>
</sec>
</sec>
<sec id="s2"><label>2.</label><title>GLM with heteroscedastic autoregressive noise</title>
<p>We propose the following voxel-wise GLM with heteroscedastic noise innovations (GLMH) for blood oxygenation level dependent (BOLD) time series:
<disp-formula id="eqn1"><alternatives><graphic xlink:href="091066_eqn1.gif"/></alternatives></disp-formula>
where <italic>y</italic><sub><italic>t</italic></sub> is the observed fMRI signal at time <italic>t</italic>, <bold>x</bold><sub><italic>t</italic></sub> is a vector with <italic>p</italic> covariates for modeling the mean, <italic>&#x03B5;</italic><sub><italic>t</italic></sub> is zero mean Gaussian white noise with unit variance, and <bold>z</bold><sub><italic>t</italic></sub> is a vector of <italic>q</italic> covariates for modeling the variance of the heteroscedastic noise innovations as ln <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline1.gif"/></alternatives></inline-formula>. Note that we are here using the logarithmic link function for the variance, but our methodology is applicable to any invertible and twice-differentiable link function. The GLMH model introduces heteroscedasticity through noise innovations with the effect that a large variance at time <italic>t</italic> is likely to generate a large innovation in the <italic>u</italic><sub><italic>t</italic></sub> equation, which is propagated through the autoregressive structure. The effect is that the noise remains large in subsequent scans, which is desireable as it has been shown that motion related signal changes can persist more than 10 seconds after motion ceases [<xref ref-type="bibr" rid="c26">26</xref>].</p>
<p>Let <bold>y</bold> = (<italic>y</italic><sub>1</sub>,&#x2026;,<italic>y</italic><sub><italic>T</italic></sub>)<sup><italic>T</italic></sup> be a <italic>T</italic>-dimensional vector consisting of observed fMRI signals at a specific voxel and define <italic>u</italic> and <italic>&#x03B5;</italic> analogously. Also, define <bold>X</bold> = (<bold>x</bold><sub>1</sub>,&#x2026;, <bold>x</bold><sub><bold><italic>T</italic></bold></sub>)<sup><italic>T</italic></sup> and <bold>Z</bold> = (<bold>z</bold><sub>1</sub>,&#x2026;, <bold>z</bold><sub><italic>T</italic></sub>)<sup><italic>T</italic></sup> to be <italic>T</italic> &#x00D7; <italic>p</italic> and <italic>T</italic> &#x00D7; <italic>q</italic> matrices consisting of covariates. Further, let <italic>&#x03C1;</italic> = (<italic>&#x03C1;</italic><sub>1</sub>,&#x2026;, <italic>&#x03C1;</italic><sub><italic>k</italic></sub>)<sup><italic>T</italic></sup>. The GLMH model can then be written as follows:
<disp-formula id="eqn2"><alternatives><graphic xlink:href="091066_eqn2.gif"/></alternatives></disp-formula>
where <bold>U</bold> is a <italic>T</italic> &#x00D7; <italic>k</italic> matrix consisting of lagged values of <bold>u</bold>, assuming that <italic>k</italic> pre-sample observations are available.</p>
</sec>
<sec id="s3"><label>3.</label><title>Bayesian inference</title>
<p>We begin by defining the binary indicators <italic>&#x1D4D8;</italic><sub><italic>&#x03B2;</italic></sub>, &#x1D4D8;<sub><italic>&#x03B3;</italic></sub>, and <italic>&#x1D4D8;</italic><sub><italic>&#x03C1;</italic></sub>, which are used for variable selection purposes. Here <italic>&#x1D4D8;</italic><sub><italic>&#x00DF;</italic></sub> is a <italic>p</italic> &#x00D7; 1 vector whose jth element takes the value 1 if <italic>&#x03B2;</italic><sub><italic>j</italic></sub> is non-zero and 0 otherwise. The indicators <italic>&#x1D4D8;</italic><sub><italic>&#x03B3;</italic></sub> and <italic>&#x1D4D8;</italic><sub><italic>&#x03C1;</italic></sub> are defined analogously. We take a Bayesian approach with the aim of computing the joint posterior distribution <italic>p</italic>(<italic>&#x03B2;, &#x03B3;, &#x03C1;, &#x1D4D8;</italic><sub><italic>&#x03B2;</italic></sub>, <italic>&#x1D4D8;</italic><sub><italic>&#x03B3;</italic></sub>, <italic>&#x1D4D8;</italic><sub><italic>&#x03C1;</italic></sub>|<bold>y, X, Z</bold>). This distribution is intractable and we use Metropolis-within-Gibbs sampling [<xref ref-type="bibr" rid="c4">4</xref>] to generate draws from the joint posterior. The algorithm iterates between the following three full conditional posteriors:
<list list-type="order">
<list-item><p>(<italic>&#x03B2;, &#x1D4D8;</italic><sub><italic>&#x03B2;</italic></sub>)|<bold>y, X, Z, &#x00B7;</bold></p></list-item>
<list-item><p>(<italic>&#x03C1;, &#x1D4D8;</italic><sub><italic>&#x03C1;</italic></sub>)|<bold>y, X, Z, &#x00B7;</bold></p></list-item>
<list-item><p>(<italic>&#x03B3;, &#x1D4D8;</italic><sub><italic>&#x03B3;</italic></sub>)|<bold>y, X, Z, &#x00B7;</bold></p></list-item>
</list>
where &#x00B7; denotes all other model parameters.</p>
<sec id="s3a"><label>3.1.</label><title>Prior distribution</title>
<p>We assume prior independence between <italic>&#x03B2;, &#x03B3;</italic> and <italic>&#x03C1;</italic>, and let
<disp-formula id="eqn3"><alternatives><graphic xlink:href="091066_eqn3.gif"/></alternatives></disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline2.gif"/></alternatives></inline-formula> and <italic>&#x00B5;</italic><sub><italic>&#x03C1;</italic></sub> = (<italic>r</italic>, 0,&#x2026;, 0)<sup><italic>T</italic></sup>. The prior mean <italic>&#x00B5;</italic><sub><italic>&#x03B2;</italic></sub> is set to 0 for all parameters, except for the term corresponding to the intercept which is set to 800. The prior mean <italic>&#x00B5;</italic><sub><italic>&#x03B3;</italic></sub> is set equal to 0 for all parameters. Note that the <italic>N</italic>(<italic>&#x00B5;</italic><sub><italic>&#x03C1;</italic></sub>, &#x03A9;<sub><italic>&#x03C1;</italic></sub>) prior centers the prior on the <italic>AR</italic>(1) process <italic>u</italic><sub><italic>t</italic></sub> = <italic>r</italic> &#x00B7; <italic>u</italic><sub><italic>t</italic>&#x2212;1</sub> &#x002B; <italic>&#x03B5;</italic><sub><italic>t</italic></sub>, with coefficients corresponding to longer lags more tightly shrunk toward zero. We also restrict the prior on <italic>&#x03C1;</italic> to the stationarity region. The user is required to specify the prior hyperparameters <italic>&#x03C4;</italic><sub><italic>&#x00DF;</italic></sub>, <italic>&#x03C4;</italic><sub><italic>&#x03B3;</italic></sub>, <italic>&#x03C4;</italic><sub><italic>&#x03C1;</italic></sub>, <italic>r</italic> and <italic>&#x03B6;</italic>. As default values we use <italic>&#x03C4;</italic><sub><italic>&#x00DF;</italic></sub> = <italic>&#x03C4;</italic><sub><italic>&#x03B3;</italic></sub> = 10, <italic>&#x03C4;</italic><sub><italic>&#x03C1;</italic></sub> = 1, <italic>r</italic> = 0.5 and <italic>&#x03B6;</italic> = 1, providing a rather uninformative prior. A more complex prior, which for example allows for prior dependence between <italic>&#x00DF;</italic> and <italic>&#x03B3;</italic>, can easily be incorporated into our framework.</p>
</sec>
<sec id="s3b"><label>3.2.</label><title>Variable selection</title>
<p>Our MCMC algorithm presented in <xref ref-type="sec" rid="s3d">Section 3.4</xref> performs Bayesian variable selection among both sets of covariates, <bold>x</bold><sub><italic>t</italic></sub> (mean) and <bold>z</bold><sub><italic>t</italic></sub> (variance). We also use Bayesian variable selection in the AR noise process, thereby automatically learning about the order <italic>k</italic> of the AR process. The first element of <italic>&#x00DF;</italic> and <italic>&#x03B3;</italic> (i.e., the intercepts in the mean and log variance, respectively) are not subject to variable selection. To describe the variable selection prior, let us focus on <italic>&#x00DF;</italic>. Let <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline3.gif"/></alternatives></inline-formula> denote the subset of regression coefficients selected by &#x1D4D8;<sub><italic>&#x00DF;</italic></sub>. To allow for variable selection we take the prior for the unrestricted <italic>&#x00DF;</italic> &#x007E; <italic>N</italic>(<italic>&#x00B5;</italic><sub><italic>&#x00DF;</italic></sub>, &#x03A9;<sub><italic>&#x00DF;</italic></sub>) and condition on the zeros dictated by <italic>&#x1D4D8;</italic><sub><italic>&#x00DF;</italic></sub>. Since all our prior covariance matrices are diagonal, the conditional distributions are simply the marginal distributions, e.g. <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline4.gif"/></alternatives></inline-formula>, where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline5.gif"/></alternatives></inline-formula> is the subset of elements of <italic>&#x00B5;</italic> corresponding to <italic>&#x1D4D8;</italic><sub><italic>&#x00DF;</italic></sub>, and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline6.gif"/></alternatives></inline-formula> is the number of elements in <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline7.gif"/></alternatives></inline-formula>. To complete the variable selection prior we let the elements of <italic>&#x1D4D8;</italic><sub><italic>&#x00DF;</italic></sub> be apriori independent and Bernoulli distributed with Pr (<italic>&#x1D4D8;</italic><sub><italic>&#x00DF;,j</italic></sub> = 1) = <italic>&#x03C0;&#x00DF;</italic>. It is straightforward to extend the prior by letting <italic>&#x03C0;</italic><sub><italic>&#x00DF;</italic></sub> follow a Beta(<italic>a, b</italic>) prior and then adding an MCMC updating step for <italic>&#x03C0;</italic><sub><italic>&#x00DF;</italic></sub> [<xref ref-type="bibr" rid="c17">17</xref>]. Other priors on <italic>&#x1D4D8;</italic><sub><italic>&#x00DF;</italic></sub> are handled similarly.</p>
</sec>
<sec id="s3c"><label>3.3.</label><title>Variable selection in linear regression using MCMC</title>
<p>This section describes how to simulate from the joint posterior of the regression coefficients, and their variable selection indicators in the Gaussian linear regression model with unit noise variance
<disp-formula><alternatives><graphic xlink:href="091066_uneqn1.gif"/></alternatives></disp-formula>
where <italic>&#x03B5;</italic> = (<italic>&#x03B5;</italic><sub>1</sub>,&#x2026;, <italic>&#x03B5;</italic><sub><italic>T</italic></sub>)<sup><italic>T</italic></sup> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline8.gif"/></alternatives></inline-formula>. This will be an important building block in our Metropolis-within-Gibbs algorithms described in <xref ref-type="sec" rid="s2">Sections 2</xref> and <xref ref-type="sec" rid="s3d">3.4</xref>. Similar to Smith and Kohn [<xref ref-type="bibr" rid="c31">31</xref>], we sample <italic>&#x00DF;</italic> jointly with its variable selection indicators <italic>&#x1D4D8;</italic> (we drop the subscript <italic>&#x00DF;</italic> here) by first generating from the marginal posterior <italic>p</italic>(<italic>&#x1D4D8;</italic>|<bold>y, X</bold>) followed by a draw from <italic>p</italic>(<italic>&#x00DF;</italic>|<italic>&#x1D4D8;</italic>, <bold>y, X</bold>). A draw from <italic>p</italic>(<italic>&#x00DF;</italic>|<italic>&#x1D4D8;</italic>, <bold>y, X</bold>) is easily obtained by sampling the non-zero elements of <italic>&#x00DF;</italic> as
<disp-formula id="eqn4"><alternatives><graphic xlink:href="091066_eqn4.gif"/></alternatives></disp-formula>
where <italic>X</italic><sub><italic>&#x1D4D8;</italic></sub> is the <italic>T</italic> &#x00D7; <italic>p</italic><sub><italic>&#x1D4D8;</italic></sub> matrix with covariates from the subset &#x1D4D8; and <italic>&#x00DF;</italic><sub><italic>&#x1D4D8;</italic></sub> is given in <xref ref-type="app" rid="app2">Appendix B</xref>. A closed form expression for <italic>p</italic>(<italic>&#x1D4D8;</italic>|<bold>y, X</bold>), the marginal posterior of <italic>&#x1D4D8;</italic>, is given in <xref ref-type="app" rid="app2">Appendix B</xref>, from which we can obtain <italic>p</italic>(<italic>&#x1D4D8;</italic><sub><italic>j</italic></sub>|<bold>y, X</bold>, &#x1D4D8;<sub>&#x2212;<italic>j</italic></sub>) &#x221D; <italic>p</italic>(<italic>&#x1D4D8;</italic>|<bold>y, X</bold>), where <italic>&#x1D4D8;</italic><sub>&#x2212;<italic>j</italic></sub> denotes <italic>&#x1D4D8;</italic> with the <italic>j</italic>th element excluded. Simulating from the joint posterior of <italic>&#x00DF;</italic> and <italic>&#x1D4D8;</italic> is therefore acheived by simulating from each <italic>p</italic>(<italic>&#x1D4D8;</italic><sub><italic>j</italic></sub>|<bold>y, X</bold>, &#x1D4D8;<sub>&#x2212;<italic>j</italic></sub>) in turn, followed by sampling of <italic>&#x00DF;</italic><sub><italic>&#x1D4D8;</italic></sub> from (<xref ref-type="disp-formula" rid="eqn4">4</xref>).</p>
</sec>
<sec id="s3d"><label>3.4.</label><title>MCMC for the GLMH model</title>
<sec id="s3d1"><title>Updating (<italic>&#x00DF;, &#x1D4D8;</italic><sub><italic>&#x00DF;</italic></sub>)</title>
<p>To sample from the full conditional posterior of (<italic>&#x00DF;, &#x1D4D8;</italic><sub><italic>&#x00DF;</italic></sub>) conditional on <italic>p</italic> and <italic>&#x03B3;</italic>, let us re-formulate the model as
<disp-formula id="eqn5"><alternatives><graphic xlink:href="091066_eqn5.gif"/></alternatives></disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline9.gif"/></alternatives></inline-formula>, and <italic>&#x03C1;</italic>(<italic>L</italic>) = 1 &#x2212; <italic>&#x03C1;</italic><sub>1</sub> <italic>L</italic> &#x2212; &#x2026; &#x2212; <italic>&#x03C1;</italic><sub><italic>k</italic></sub><italic>L</italic><sup><italic>k</italic></sup> is the usual lag polynomial in the lag operator <italic>L</italic><sup><italic>k</italic></sup><italic>y</italic><sub><italic>t</italic></sub> = <italic>y</italic><sub><italic>t&#x2212;k</italic></sub> from time series analysis. The Jacobian of the transformation <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline10.gif"/></alternatives></inline-formula> is <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline11.gif"/></alternatives></inline-formula>, which can be seen as follows. The inverse transformation is <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline12.gif"/></alternatives></inline-formula>, where <italic>&#x03C1;</italic><sup>&#x2212;1</sup> (<italic>L</italic>) = 1 &#x002B; <italic>&#x03C8;</italic><sub>1</sub><italic>L</italic> &#x002B; <italic>&#x03C8;</italic><sub>2</sub><italic>L</italic><sup>2</sup> &#x002B; &#x2026; is the inverse lag polynomial for some coeffcients <italic>&#x03A8;</italic><sub>1</sub>, <italic>&#x03A8;</italic><sub>2</sub>, &#x2026; This system of equations is recursive so the Jacobian is <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline13.gif"/></alternatives></inline-formula> which proves the result. Note that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline14.gif"/></alternatives></inline-formula> does not depend on <italic>&#x00DF;</italic> and can therefore be ignored when deriving the full conditional posterior of <italic>&#x00DF;</italic>. Now, <italic>&#x00DF;</italic> in (<xref ref-type="disp-formula" rid="eqn5">5</xref>) are the coefficients in a linear regression with unit noise variance and we can draw from the full conditional <italic>p</italic>(<italic>&#x00DF;, &#x1D4D8;</italic><sub><italic>&#x00DF;</italic></sub> |<bold>y, X, Z, &#x00B7;</bold>) as described in <xref ref-type="sec" rid="s3c">Section 3.3</xref> with <bold>y</bold> and <bold>X</bold> replaced by <bold>&#x1EF9;</bold> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline15.gif"/></alternatives></inline-formula>, respectively.</p>
</sec>
<sec id="s3d2"><title>Updating (<italic>&#x03C1;, &#x1D4D8;</italic><sub><italic>&#x03C1;</italic></sub>)</title>
<p>The AR process can be rewritten as
<disp-formula><alternatives><graphic xlink:href="091066_uneqn2.gif"/></alternatives></disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline16.gif"/></alternatives></inline-formula>. The Jacobian of this transformation is <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline17.gif"/></alternatives></inline-formula> which does not depend on <italic>&#x03C1;</italic> and can therefore be ignored when updating <italic>&#x03C1;</italic>. Now, <italic>&#x03C1;</italic> are the coefficients in a linear regression with unit noise variance and we can draw from the full conditional <italic>p</italic>(<italic>&#x03C1;, &#x1D4D8;</italic><sub><italic>&#x03C1;</italic></sub>|<bold>y, X, Z, &#x00B7;</bold>) as described in <xref ref-type="sec" rid="s3c">Section (3.3)</xref>.</p>
</sec>
<sec id="s3d3"><title>Updating (<italic>&#x03B3;, &#x1D4D8;</italic><sub><italic>&#x03C1;</italic></sub>)</title>
<p>The full conditional posterior of (<italic>&#x03B3;, &#x1D4D8;</italic><sub><italic>&#x03B3;</italic></sub>) is a complicated distribution which we can not easily sample from. However, it is clear from the model
<disp-formula><alternatives><graphic xlink:href="091066_uneqn3.gif"/></alternatives></disp-formula>
that the conditional likelihood of <italic>&#x03B3;</italic> is of the form described in Villani et al. [<xref ref-type="bibr" rid="c35">35</xref>] where the observations (the <italic>u</italic><sub><italic>t</italic></sub> in this case) are conditionally independent and <italic>&#x03B3;</italic> enters each factor in the likelihood linearly <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline18.gif"/></alternatives></inline-formula> through a scalar valued quantity <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline19.gif"/></alternatives></inline-formula>. The MCMC update with a finite step Newton proposal with variable selection described in [<xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref>] can therefore be used. In fact, Villani et al. [<xref ref-type="bibr" rid="c34">34</xref>] contains the details for the Gaussian heteroscedastic regression, which is exactly the model when we condition on <italic>&#x03B2;</italic> (since <bold>u</bold> is then known). The algorithm in [<xref ref-type="bibr" rid="c35">35</xref>] proposes <italic>&#x03B3;</italic> and <italic>&#x1D4D8;</italic><sub><italic>&#x03B3;</italic></sub> jointly by randomly changing a subset of the indicators in <italic>&#x1D4D8;</italic><sub><italic>&#x03B3;</italic></sub> followed by a proposal from <italic>&#x03B3;</italic> |<italic>&#x1D4D8;</italic><sub><italic>&#x03B3;</italic></sub> using a multivariate-<italic>t</italic> distribution tailored to the full conditional posterior. The tailoring is acheived by taking a small number of Newton steps toward the posterior mode, and using the negative inverse Hessian at the terminal point as the covariance matrix in the multivariate<italic>-t</italic> proposal distribution. The update is fast, since the Jacobian and Hessian can be computed in closed form using the chain rule and compact matrix algebra. It is also possible to compute the expected Hessian (Fisher information) in closed form. The expected Hessian tends to be more stable numerically with only marginally worse tailoring to the posterior. Note also that the Newton iterations always start from the current value of <italic>&#x03B3;</italic>, which is typically not far from the mode, so even one or two Newton steps are usually sufficient. We refer to Villani et al. [<xref ref-type="bibr" rid="c35">35</xref>] for details of the general algorithm, and to Villani et al. [<xref ref-type="bibr" rid="c34">34</xref>] for expressions of the Jacobian, Hessian and expected Hessian for <italic>&#x03B3;</italic>.</p>
</sec>
</sec>
</sec>
<sec id="s4"><label>4.</label><title>Implementation</title>
<p>A drawback of using MCMC is that processing of a single fMRI dataset can take several hours [<xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c29">29</xref>]. Our implementation of the heteroscedastic GLM is therefore written in C&#x002B;&#x002B;, using the Eigen library [<xref ref-type="bibr" rid="c16">16</xref>] for all matrix operations. The random number generators available in the C&#x002B;&#x002B; standard library (available from C&#x002B;&#x002B; 2011) were used, together with the Eigen library, to make random draws from multivariate distributions. The OpenMP (Open Multi Processing) library was used to take advantage of all the CPU cores, by analyzing several voxels in parallel. For all analyses the number of Newton steps is set to 2. To lower the processing time, the variable selection indicators for the variance covariates are only updated in 60&#x0025; of the draws.</p>
</sec>
<sec id="s5"><label>5.</label><title>Results</title>
<sec id="s5a"><label>5.1.</label><title>Simulated data</title>
<sec id="s5a1"><label>5.1.1.</label><title>GLMH vs Bayesian GLM with homoscedastic noise</title>
<p>To verify that the heteroscedastic model works as expected, and to compare it to a homoscedastic model for data with a known activity pattern, the algorithms were applied to simulated data with homoscedastic and heteroscedastic noise. The simulated data were created using (posterior mean) beta estimates from real fMRI data (with several motion spikes), together with the applied design matrix, to create a timeseries in each voxel. The design matrix consisted of an intercept, time trends for modeling drift (linear, quadratic and cubic), activity covariates, estimated head motion parameters and their temporal derivative (in total 16 covariates in addition to the activity covariates). Beta values for active voxels were generated from a <italic>abs</italic>(<italic>N</italic>(0, 9)) &#x002B; 3 distribution, and for non-active voxels from a <italic>N</italic>(0,0.06) distribution. The simulated activity is thereby very easy to detect, and the difficult part is to model the heteroscedastic noise.</p>
<p>For approximately half of the active voxels, heteroscedastic noise was added according to <xref ref-type="disp-formula" rid="eqn1">Equation 1</xref>. For one covariate at a time (either an activation or head motion covariate), the corresponding <italic>&#x03B3;</italic> parameter was set to 1, 2, or 3. For one covariate representing the (absolute value of the) temporal derivative of the head motion, the <italic>&#x03B3;</italic> parameter was instead set to 1, 1.25 or 1.5 (as motion spikes can be rather large, and thereby make the simulation unrealistic). To simulate simultaneous heteroscedasticity from several covariates, the <italic>&#x03B3;</italic> parameters for the activity and the head motion covariates were simultaneously set to 1, 2, or 3, while the <italic>&#x03B3;</italic> parameter for the derivated head motion covariate was set to 1.25 for all cases. The <italic>&#x03B3;</italic> parameter for the intercept covariate was always set to 1, and all other <italic>&#x03B3;</italic> parameters were set to 0. For all other voxels, homoscedastic noise was added (<italic>&#x03B3;</italic> = 1 for the intercept only). The four autocorrelation parameters were set to 0.4, 0.2, 0.1 and 0.05, respectively. The simulated data thereby consists of four regions; active voxels with homoscedastic or heteroscedastic noise, and non-active voxels with homoscedastic or heteroscedastic noise. To lower the processing time, only a single slice of data was simulated. See <xref ref-type="fig" rid="fig4">Figure 4</xref> for the gray matter mask, the mask for active voxels and the mask for voxels with heteroscedastic noise.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure"><label>Figure 4:</label>
<caption><p>Left: A mask for gray matter voxels. Middle: Voxels with simulated activity. Right: Voxels with heteroscedastic noise. The simulated data consists of four regions; active voxels with homoscedastic or heteroscedastic noise, and non-active voxels with homoscedastic or heteroscedastic noise.</p></caption>
<graphic xlink:href="091066_fig4.tif"/>
</fig>
<p>For each simulated dataset, the analysis was performed (i) including only an intercept for the variance (i.e., a homoscedastic model) and (ii) including all covariates for the variance (i.e., a heteroscedastic model). In both cases all covariates (except the intercept) were standardized, to have zero mean and variance 1. For the mean covariates, the original temporal derivative of the head motion parameters was used. For the variance covariates, the absolute value was used instead, as the variance should always increase at a motion spike regardless of the direction (positive or negative). For both models, a fourth order AR model was used in each voxel. Variable selection was performed on all covariates (mean and variance), except for the intercept, as well as for the four AR parameters. Stationarity was enforced for the AR parameters, by discarding draws where the absolute value of any eigenvalue of the companion matrix is larger than or equal to 1. For each voxel, a total of 1,000 draws were used for MCMC burn-in and another 1,000 draws were saved for inference.</p>
<p><xref ref-type="fig" rid="fig5">Figures 5</xref> &#x2013; <xref ref-type="fig" rid="fig8">8</xref> show receiver operating characteristic (ROC) curves for the two models, for different types (activity, motion, motion derivative, all) and levels (<italic>&#x03B3;</italic> = 1, 2 or 3) of heteroscedasticity. The ROC curves were generated by varying the threshold for the posterior probability maps (PPMs) from 0.01 to 1.00. It is clear that both models detect virtually all the active voxels for low levels of heteroscedasticity, while the homoscedastic model fails to detect a large portion of the active voxels with heteroscedastic noise for higher levels of heteroscedasticity. The posterior inclusion probabilities for the variance parameters (<italic>&#x03B3;</italic>) indicate that the heteroscedastic model in virtually all voxels only includes the covariates that were used to generate the heteroscedastic noise (not shown).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure"><label>Figure 5:</label>
<caption><p>ROC curves for simulated data with heteroscedastic noise from one motion covariate, and different levels of heteroscedasticity. Both models perform well for low levels of heteroscedasticity, but the homoscedastic model performs worse for high levels of heteroscedasticity.</p></caption>
<graphic xlink:href="091066_fig5.tif"/>
</fig>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure"><label>Figure 6:</label>
<caption><p>ROC curves for simulated data with heteroscedastic noise from the temporal derivative of one motion covariate, and different levels of heteroscedasticity. The homoscedastic model has a lower performance, and fails to detect a large portion of the active voxels.</p></caption>
<graphic xlink:href="091066_fig6.tif"/>
</fig>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure"><label>Figure 7:</label>
<caption><p>ROC curves for simulated data with heteroscedastic noise from one activity covariate, and different levels of heteroscedasticity. The homoscedastic model has a lower performance, and fails to detect a large portion of the active voxels.</p></caption>
<graphic xlink:href="091066_fig7.tif"/>
</fig>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure"><label>Figure 8:</label>
<caption><p>ROC curves for simulated data with heteroscedastic noise from three simultaneous sources (motion, motion derivative, activity), and different levels of heteroscedasticity. The homoscedastic model has a much lower performance, and fails to detect a large portion of the active voxels.</p></caption>
<graphic xlink:href="091066_fig8.tif"/>
</fig>
</sec>
<sec id="s5a2"><label>5.1.2.</label><title>GLMH vs weighted least squares</title>
<p>To compare the heteroscedastic model to the weighted least squares (WLS) approach proposed by Diedrichsen and Shadmehr [<xref ref-type="bibr" rid="c6">6</xref>], where a single weight is estimated for each volume, two additional datasets were simulated (using the same activity mask as above). For the first dataset, the same heteroscedastic noise was added to all voxels. For the second dataset, heteroscedastic noise was added to only 30&#x0025; of the voxels (using the same hetero mask as above). The simulation was performed to generate different types (motion, motion derivative) and levels (<italic>&#x03B3;</italic> = 1, 2 or 3) of heteroscedasticity. As the two approaches use different models for the temporal autocorrelation, the four AR parameters were set to 0, to focus solely on the heteroscedasticity. To mimic the analysis by Diedrichsen and Shadmehr [<xref ref-type="bibr" rid="c6">6</xref>], no motion regressors were used in the design matrix for the WLS approach. Bayesian t-scores (posterior mean / posterior standard deviation) were calculated for the heteroscedastic model, and compared to regular <italic>t</italic>-scores from the WLS approach. <xref ref-type="fig" rid="fig9">Figures 9</xref> &#x2013; <xref ref-type="fig" rid="fig12">12</xref> show ROC curves for the two approaches. Both approaches work well when the same heteroscedastic noise is present in all voxels, but the WLS approach fails to detect a large portion of the activity when the heteroscedastic noise is only present in 30&#x0025; of the voxels.</p>
<fig id="fig9" position="float" orientation="portrait" fig-type="figure"><label>Figure 9:</label>
<caption><p>ROC curves for simulated data with heteroscedastic noise in all voxels, generated by one motion covariate. Both approaches perform well for all levels of heteroscedasticity.</p></caption>
<graphic xlink:href="091066_fig9.tif"/>
</fig>
<fig id="fig10" position="float" orientation="portrait" fig-type="figure"><label>Figure 10:</label>
<caption><p>ROC curves for simulated data with heteroscedastic noise in all voxels, generated by the temporal derivative of one motion covariate. Both approaches perform well, but the hetero approach works better for higher levels of heteroscedasticity.</p></caption>
<graphic xlink:href="091066_fig10.tif"/>
</fig>
<fig id="fig11" position="float" orientation="portrait" fig-type="figure"><label>Figure 11:</label>
<caption><p>ROC curves for simulated data with heteroscedastic noise in 30&#x0025; of the voxels, generated by one motion covariate. Compared to heteroscedastic noise in all voxels, the WLS approach has a slightly lower performance.</p></caption>
<graphic xlink:href="091066_fig11.tif"/>
</fig>
<fig id="fig12" position="float" orientation="portrait" fig-type="figure"><label>Figure 12:</label>
<caption><p>ROC curves for simulated data with heteroscedastic noise in 30&#x0025; of the voxels, generated by the temporal derivative of one motion covariate. Compared to heteroscedastic noise in all voxels, the WLS approach fails to detect a large portion of the active voxels.</p></caption>
<graphic xlink:href="091066_fig12.tif"/>
</fig>
</sec>
</sec>
<sec id="s5b"><label>5.2.</label><title>Application to real data</title>
<p>Three datasets from the OpenfMRI project[<xref ref-type="bibr" rid="c24">24</xref>, <xref ref-type="bibr" rid="c25">25</xref>] were analyzed using both the homoscedastic and heteroscedastic noise models. The datasets include experiments on rhyme judgment<xref ref-type="fn" rid="fn1"><sup>1</sup></xref>, living-nonliving judgment<xref ref-type="fn" rid="fn2"><sup>2</sup></xref> and mixed gambles<xref ref-type="fn" rid="fn3"><sup>3</sup></xref>[<xref ref-type="bibr" rid="c33">33</xref>].</p>
<p>In the rhyme judgment task, stimuli were presented in pairs (consisting of either words or pseudo-words) and the subject was asked whether the pair of stimuli rhymed with one another. The dataset consists of 13 subjects and two different conditions: words and pseudo-words.</p>
<p>In the living/nonliving judgment task, subjects were presented with words in either plain or mirror-reversed format, and asked whether the stimulus referred to a living or nonliving object. The data set consists of 14 subjects and 4 different conditions: mirror-reversed trials preceded by a plain text trial, mirror-reversed trials preceded by a mirror-reversed trial, plain-text trials preceded by a mirror-reversed trial, and plain-text trials preceded by a plain-text trial. A fifth covariate is used to represent failed (junk) trials.</p>
<p>Finally, in the mixed gambles task, subjects were presented with gambles in which they have a 50&#x0025; chance of gaining and a 50&#x0025; chance of losing money, where the potential gain and loss varied across trials. The subject then decided whether or not to accept the gamble. The data set consists of 16 subjects and 4 different conditions: task, parametric gain, parametric loss, and distance from indifference point. For more details on the 3 datasets we refer to the OpenfMRI website (<ext-link ext-link-type="uri" xlink:href="https://openfmri.org">https://openfmri.org</ext-link>).</p>
<sec id="s5b1"><label>5.2.1.</label><title>Single subject analysis</title>
<p>Prior to statistical analysis, the BROCCOLI software [<xref ref-type="bibr" rid="c9">9</xref>] was used to perform motion correction and 6 mm FWHM smoothing. For each subject, the analysis was performed as described for the simulated data (16 covariates &#x002B; activity covariates, for both mean and variance). Only gray matter voxels were analyzed to lower processing time. All results were finally transformed to MNI space, by combining T1-MNI and fMRI-T1 transforms.</p>
<p><xref ref-type="fig" rid="fig13">Figure 13</xref> shows PPMs for one subject from the rhyme judgment dataset and one subject from the mixed gambles dataset; the heteroscedastic model tends to detect more brain activity compared to the homoscedastic model. <xref ref-type="fig" rid="fig14">Figures 14</xref> - <xref ref-type="fig" rid="fig16">16</xref> summarize the number of voxels where the difference between the heteroscedastic PPM and the homoscedastic PPM is larger than 0.5, for the three different datasets. The largest PPM differences are found in the rhyme judgment dataset, which contains the highest number of motion spikes. <xref ref-type="fig" rid="fig20">Figure 20</xref> shows a comparison between the estimated homoscedastic and heteroscedastic standard deviation for a single time series; the heteroscedastic standard deviation is much higher for time points close to motion spikes, but lower for time points with little head motion. The homoscedastic model struggles to find a single variance to fit both time points with and without motion, thereby ending up inflating the variance at times with little or no motion. The heteroscedastic model can have a lower variance at timeperiods with little motion, and is therefore able to detect more brain activity. <xref ref-type="fig" rid="fig17">Figures 17</xref> - <xref ref-type="fig" rid="fig19">19</xref> show the number of voxels, for each dataset, where the posterior inclusion probability is larger than 90&#x0025; for the variance covariates. The temporal derivative of the head motion parameters are clearly the most important covariates for modeling the variance.</p>
<fig id="fig13" position="float" orientation="portrait" fig-type="figure"><label>Figure 13:</label>
<caption><p>Single subject posterior probability maps (PPMs) for the rhyme judgment and mixed gambles datasets. From left to right: PPM for the heteroscedastic model, PPM for the homoscedastic model, PPM hetero - PPM homo. The hetero and the homo PPMs are thresholded at Pr = 0.95, while the difference is thresholded at 0.5. First row: Rhyme judgment dataset (subject 4, pseudo words contrast), Second row: Mixed gambles dataset (subject 3, parametric loss contrast). For subjects with one or several motion spikes, the heteroscedastic and the homoscedastic PPMs differ for a number of voxels. The reason for this is that the homoscedastic model overestimates the constant variance term, due to time points corresponding to motion spikes. The heteroscedastic model instead incorporates the head motion parameters, or the temporal derivative of them, to model these variance increases, and can thereby detect more brain activity.</p></caption>
<graphic xlink:href="091066_fig13.tif"/>
</fig>
<fig id="fig14" position="float" orientation="portrait" fig-type="figure"><label>Figure 14:</label>
<caption><p>Number of gray matter voxels where the difference between the heteroscedastic PPM and homoscedastic PPM is larger than 0.5, for the rhyme judgment dataset. The bars represent the average over all activity covariates.</p></caption>
<graphic xlink:href="091066_fig14.tif"/>
</fig>
<fig id="fig15" position="float" orientation="portrait" fig-type="figure"><label>Figure 15:</label>
<caption><p>Number of gray matter voxels where the difference between the heteroscedastic PPM and homoscedastic PPM is larger than 0.5, for the living nonliving dataset. The bars represent the average over all activity covariates.</p></caption>
<graphic xlink:href="091066_fig15.tif"/>
</fig>
<fig id="fig16" position="float" orientation="portrait" fig-type="figure"><label>Figure 16:</label>
<caption><p>Number of gray matter voxels where the difference between the heteroscedastic PPM and homoscedastic PPM is larger than 0.5, for the mixed gambles dataset. The bars represent the average over all activity covariates.</p></caption>
<graphic xlink:href="091066_fig16.tif"/>
</fig>
<fig id="fig17" position="float" orientation="portrait" fig-type="figure"><label>Figure 17:</label>
<caption><p>The use of variance covariates for the rhyme judgment dataset. Each bar represents the mean number of gray matter voxels, for each type of covariate (activity, trends, motion, motion derivative), for which the covariate is included to model the variance (posterior inclusion probability larger than 0.9). For subjects with motion spikes, one or several motion derivative covariates are used to model the heteroscedastic variance for a large number of voxels. The mean number of gray matter voxels is 15,600.</p></caption>
<graphic xlink:href="091066_fig17.tif"/>
</fig>
<fig id="fig18" position="float" orientation="portrait" fig-type="figure"><label>Figure 18:</label>
<caption><p>The use of variance covariates for the living nonliving dataset. Each bar represents the mean number of gray matter voxels, for each type of covariate (activity, trends, motion, motion derivative), for which the covariate is included to model the variance (posterior inclusion probability larger than 0.9). This dataset contains very few motion spikes, which explains why so few covariates are included in the variance. The mean number of gray matter voxels is 13,000.</p></caption>
<graphic xlink:href="091066_fig18.tif"/>
</fig>
<fig id="fig19" position="float" orientation="portrait" fig-type="figure"><label>Figure 19:</label>
<caption><p>The use of variance covariates for the mixed gambles dataset. Each bar represents the mean number of gray matter voxels, for each type of covariate (activity, trends, motion, motion derivative), for which the covariate is included to model the variance (posterior inclusion probability larger than 0.9). For subjects with motion spikes, one or several motion derivative covariates are used to model the heteroscedastic variance for a large number of voxels. The mean number of gray matter voxels is 15,500.</p></caption>
<graphic xlink:href="091066_fig19.tif"/>
</fig>
<fig id="fig20" position="float" orientation="portrait" fig-type="figure"><label>Figure 20:</label>
<caption><p>A comparison between the estimated homoscedastic and heteroscedastic standard deviation for one time series. The heteroscedastic standard deviation is much higher for the motion spikes, while it is lower for time points with little head motion. For this reason, the heteroscedastic model can automatically downweight time points close to motion spikes, and detect more brain activity by not over estimating the standard deviation for time points with little head motion.</p></caption>
<graphic xlink:href="091066_fig20.tif"/>
</fig>
</sec>
<sec id="s5b2"><label>5.2.2.</label><title>Convergence &#x0026; efficiency of MCMC</title>
<p>The MCMC convergence is in general excellent; the acceptance probabilities for the variance covariates are 85.4&#x0025; &#x00B1; 5.1&#x0025; for the rhyme judgment dataset, 89&#x0025; &#x00B1; 1.9&#x0025; for the living nonliving dataset and 87&#x0025; &#x00B1; 7.1&#x0025; for the mixed gambles dataset (standard deviation calculated over subjects). The efficiency of the MCMC chain in each voxel was investigated by calculating the inefficiency factor for each covariate for the mean and the variance, as well as for the four AR parameters. Since it is hard to estimate the inefficiency factor for variables with a low posterior inclusion probability (IPr), the inefficiency factor was only estimated if the IPr was larger than 0.3. To carefully investigate the MCMC efficiency in every voxel is difficult, due to the large number of voxels and covariates. An inefficiency factor of 1 is ideal, but very seldom achieved in practice. Inefficiency factors less than 10 - 20 are normally considered as acceptable. <xref ref-type="table" rid="tbl1">Tables 1</xref>, <xref ref-type="table" rid="tbl2">2</xref> and <xref ref-type="table" rid="tbl3">3</xref> therefore state the proportion of included voxels (IPr > 0.3) where the inefficiency factor is larger than 10, for the mean covariates (<italic>&#x00DF;</italic>), the variance covariates (<italic>&#x03B3;</italic>), and the auto regressive parameters (<italic>&#x03C1;</italic>), respectively. The efficiency is in general high for both the mean and the variance covariates; only a few voxels have inefficiency factors larger than 10. The efficiency is in general lower for the auto regressive parameters, which has two explanations. First, the stationarity restriction enforces the parameters to a certain region, and if a parameter is repeatedly close to the boundary the sampling efficiency will be low. Second, in some voxels the algorithm finds a new mode after a subset of all the draws, which indicates that the chain has not converged. Considering the fact that 1,000 draws are already used for burnin, and that the processing time is 10 - 40 hours per subject, increasing the number of burnin draws even further is not a realistic option.</p>
<table-wrap id="tbl1" orientation="portrait" position="float"><label>Table 1:</label>
<caption><p>Proportion of voxels with an inefficiency factor larger than 10 for the mean covariates (<italic>&#x00DF;</italic>), for the different datasets. The covariates in the design matrix have been grouped together to different types, and the numbers in the table represent the average over covariates (of each type) and subjects. The standard deviation was calculated over subjects.</p></caption>
<graphic xlink:href="091066_tbl1.tif"/>
</table-wrap>
<table-wrap id="tbl2" orientation="portrait" position="float"><label>Table 2:</label>
<caption><p>Proportion of voxels with an inefficiency factor larger than 10 for the variance covariates (<italic>&#x03B3;</italic>), for the different datasets. The covariates in the design matrix have been grouped together to different types, and the numbers in the table represent the average over covariates (of each type) and subjects. The standard deviation was calculated over subjects.</p></caption>
<graphic xlink:href="091066_tbl2.tif"/>
</table-wrap>
<table-wrap id="tbl3" orientation="portrait" position="float"><label>Table 3:</label>
<caption><p>Proportion of voxels with an inefficiency factor larger than 10 for the auto correlation parameters (<italic>p</italic>), for the different datasets. The standard deviation was calculated over subjects.</p></caption>
<graphic xlink:href="091066_tbl3.tif"/>
</table-wrap>
</sec>
<sec id="s5b3"><label>5.2.3.</label><title>Group analysis</title>
<p>Group analyses were performed using the full posterior of the task-related covariates from each subject (1,000 draws). To keep things simple, we perform each group analysis by computing the posterior for the sample mean: <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline20.gif"/></alternatives></inline-formula>, where <italic>&#x00DF;</italic><sup>(<italic>r</italic>)</sup> is the activity coefficient for the <italic>r</italic>th subject in the sample, and <italic>N</italic> is the number of subjects. For each draw, the mean brain activity over subjects was calculated, to form the posterior of the mean group activity, <italic>&#x00DF;</italic><sub><italic>grup</italic></sub>. In a second group analysis, each subject was weighted with the inverse posterior standard deviation, i.e. <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline21.gif"/></alternatives></inline-formula>. <xref ref-type="fig" rid="fig21">Figure 21</xref> shows hetero and homo group mean PPMs (unweighted and weighted) for the rhyme judgment dataset, minimal differences were found for the other two datasets. The difference between the two models is slightly larger for the weighted group analysis, which is natural as the GLMH approach mainly affects the variance of the posterior. The effect of using a heteroscedastic model would clearly be stronger at the group level if many subjects (e.g. children) in the group exhibit motion spikes.</p>
<fig id="fig21" position="float" orientation="portrait" fig-type="figure"><label>Figure 21:</label>
<caption><p>Group level posterior probability maps (PPMs) for the rhyme judgment dataset (contrast pseudo words). From left to right: PPM for the heteroscedastic model, PPM for the homoscedastic model, PPM hetero - PPM homo. The hetero and the homo PPMs are thresholded at Pr = 0.95, while the difference is thresholded at 0.5. Top row: group activity calculated without any subject specific weights. Bottom row: group activity calculated by weighting each subject with the inverse standard deviation.</p></caption>
<graphic xlink:href="091066_fig21.tif"/>
</fig>
</sec>
</sec>
</sec>
<sec id="s6"><label>6.</label><title>Discussion</title>
<p>We have presented a Bayesian heteroscedastic GLM for single subject fMRI analysis. The heteroscedastic GLM takes into consideration the fact that the variance is inflated for time points with a high degree of head motion, and thus provides more sensitive results, compared to its homoscedastic counterpart. Instead of discarding data with too much head motion, or applying different scrubbing or censoring techniques [<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c26">26</xref>, <xref ref-type="bibr" rid="c30">30</xref>], our heteroscedastic GLM automatically downweights the affected time points, and propagates the uncertainty to the group analysis by saving the full posterior. For the rhyme judgment dataset and the mixed gambles dataset, the temporal derivative of the head motion parameters are included as variance covariates for a large number of voxels. For heteroscedastic voxels in active brain areas, the difference between the homoscedastic PPM and the heteroscedastic PPM can be substantial. There will only be a sizeable PPM difference if the voxel belongs to an active brain area, and contains noise where the degree of heteroscedasticity is sufficiently high (see <xref ref-type="fig" rid="fig5">Figures 5</xref> - <xref ref-type="fig" rid="fig8">8</xref>). The difference between the two models is small for the living/nonliving dataset, mainly because that dataset contains very few motion spikes. This illustrates that our algorithm can be applied to any dataset, as using the heteroscedastic approach does not lead to a lower sensitivity when there are no motion spikes present.</p>
</sec>
<sec id="s6a"><label>6.1.</label><title>MCMC vs Variational Bayes</title>
<p>A drawback of using MCMC is the computational complexity; it takes 10 - 40 hours (depending on the number of covariates) to analyze a single subject using the heteroscedastic model, with an Intel Core i7 4790K quad-core CPU. One alternative is to use variational Bayes (VB), where a few iterations is normally sufficient to obtain a point estimate of the posterior [<xref ref-type="bibr" rid="c22">22</xref>]. It is, however, much harder to perform variable selection within VB, and variable selection is necessary in our case since 18 - 21 covariates are used for the mean as well as for the variance. Without variable selection the model would contain too many parameters, compared to the number of time points in a typical fMRI dataset, which would result in poor estimates. Another problem with VB is that the posterior standard deviation is often underestimated.</p>
<p>In theory, the proposed algorithm can run on a graphics processing unit (GPU), which can analyze some 30,000 voxels in parallel [<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>]. The pre-whitening step in each MCMC iteration is problematic from a GPU perspective, as a pre-whitened design matrix needs to be stored in each voxel / GPU thread. For 20 covariates and 200 time points, the design matrix requires 4,000 floats for storage. Modern Nvidia GPUs can, however, only store 255 floats per thread.</p>
</sec>
<sec id="s6b"><label>6.2.</label><title>GLMH vs weighted least squares</title>
<p>To make a fair comparison between our heteroscedastic model and the WLS approach proposed by Diedrichsen and Shadmehr [<xref ref-type="bibr" rid="c6">6</xref>] is difficult, as we use Bayesian inference. Nevertheless, the WLS approach seems to work well as long as the same heteroscedastic noise is present in all voxels, but fails to detect activity when the heteroscedastic noise is only present in 30&#x0025; of the voxels. Diedrichsen and Shadmehr [<xref ref-type="bibr" rid="c6">6</xref>] argue that the same weight should be used for all voxels, our results for real fMRI data (<xref ref-type="fig" rid="fig17">Figures 17</xref> - <xref ref-type="fig" rid="fig19">19</xref>) instead suggest that only a fraction of voxels have heteroscedastic noise. For some 13,000 - 15,600 voxels in gray matter, the derivative of the head motion parameters are included as covariates for the variance for 300 - 2,000 voxels (for subjects with motion spikes). Note that these numbers represent the average over each covariate type, meaning that if one of the six motion covariates is included for 12,000 voxels, the average over all six covariates will be 2,000 voxels.</p>
<p>The main drawback of the WLS approach is that it requires estimation of <italic>T</italic> weights from <italic>T</italic> time points, which results in extremely variable estimates unless the weights are averaged over many voxels. Our heteroscedastic GLM instead models the variance using a regression approach. Through the use of variable selection, a heteroscedastic model can be estimated independently in each voxel, even if the number of covariates is large.</p>
</sec>
<sec id="s6c"><label>6.3.</label><title>Multiple comparisons</title>
<p>In contrast to frequentistic statistics, there is no consensus in the fMRI field regarding if and how to correct for multiple comparisons for PPMs. In this paper we have mainly focused on looking at differences between the heteroscedastic and the homoscedastic models, for voxel inference. It is not obvious how to use Bayesian techniques for cluster inference [<xref ref-type="bibr" rid="c10">10</xref>], which for frequentistic statistics has a higher statistical power. One possible approach is to use theory on excursion sets [<xref ref-type="bibr" rid="c3">3</xref>], to work with the joint PPM instead of marginal PPMs. Such an approach, however, requires a spatially dependent posterior, while we independently estimate one posterior for each voxel.</p>
</sec>
<sec id="s6d"><label>6.4.</label><title>Future work</title>
<p>We have here only demonstrated the use of the heteroscedastic GLM for brain activity estimation, but it can also be used for estimating functional connectivity; for example by using a seed time series as a covariate in the design matrix. Although not investigated in this work, it is also possible to include additional covariates that may affect the variance, such as the global mean [<xref ref-type="bibr" rid="c27">27</xref>] or recordings of breathing and pulse [<xref ref-type="bibr" rid="c13">13</xref>]. Future work will also focus on adding a spatial model [<xref ref-type="bibr" rid="c23">23</xref>, <xref ref-type="bibr" rid="c29">29</xref>], instead of analyzing each voxel independently.</p>
</sec>
</body>
<back>
<ack><title>Acknowledgement</title>
<p>This work was financed by the Swedish Research council, grant 2013-5229 (&#x201C;Statistical analysis of fMRI data&#x201D;), and by the Information Technology for European Advancement (ITEA) 3 Project BENEFIT (better effectiveness and efficiency by measuring and modelling of interventional therapy). This research was also supported in part by NIH grants R01 EB016061 and P41 EB015909 from the National Institute of Biomedical Imaging. We thank Russ Poldrack and his colleagues for starting the OpenfMRI Project (supported by National Science Foundation Grant OCI-1131441) and all of the researchers who have shared their task-based data.</p>
</ack>
<ref-list><title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><surname>Adrian</surname>, <given-names>D. W.</given-names></string-name>, <string-name><surname>Maitra</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Rowe</surname>, <given-names>D. B.</given-names></string-name> (<year>2013</year>). <article-title>Ricean over Gaussian modelling in magnitude fMRI analysis - Added complexity with negligible practical benefits</article-title>. <source>Stat</source>, <volume>2</volume>(<issue>1</issue>):<fpage>303</fpage>&#x2013;<lpage>316</lpage>. <xref ref-type="sec" rid="s1">1</xref></mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><string-name><surname>Beckmann</surname>, <given-names>C. F.</given-names></string-name>, <string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name> (<year>2003</year>). <article-title>General multilevel linear modeling for group analysis in FMRI</article-title>. <source>NeuroImage</source>, <volume>20</volume>(<issue>2</issue>):<fpage>1052</fpage>&#x2013;<lpage>1063</lpage>. <xref ref-type="sec" rid="s1">1</xref></mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><surname>Bolin</surname>, <given-names>D.</given-names></string-name> and <string-name><surname>Lindgren</surname>, <given-names>F.</given-names></string-name> (<year>2015</year>). <article-title>Excursion and contour uncertainty regions for latent Gaussian models</article-title>. <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>, <volume>77</volume>:<fpage>85</fpage>&#x2013;<lpage>106</lpage>. <xref ref-type="sec" rid="s6c">6.3</xref></mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="book"><string-name><surname>Brooks</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gelman</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>G.</given-names></string-name>, and <string-name><surname>Meng</surname>, <given-names>X.-L.</given-names></string-name> (<year>2011</year>). <source>Handbook of Markov Chain Monte Carlo</source>. <publisher-name>Chapman and Hall/CRC</publisher-name>. <xref ref-type="sec" rid="s3">3</xref></mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><string-name><surname>Chen</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Saad</surname>, <given-names>Z. S.</given-names></string-name>, <string-name><surname>Nath</surname>, <given-names>A. R.</given-names></string-name>, <string-name><surname>Beauchamp</surname>, <given-names>M. S.</given-names></string-name>, and <string-name><surname>Cox</surname>, <given-names>R. W.</given-names></string-name> (<year>2012</year>). <article-title>FMRI group analysis combining effect estimates and their variances</article-title>. <source>NeuroImage</source>, <volume>60</volume>(<issue>1</issue>):<fpage>747</fpage>&#x2013;<lpage>765</lpage>. <xref ref-type="sec" rid="s1">1</xref></mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><string-name><surname>Diedrichsen</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Shadmehr</surname>, <given-names>R.</given-names></string-name> (<year>2005</year>). <article-title>Detecting and adjusting for artifacts in fMRI time series data</article-title>. <source>NeuroImage</source>, <volume>27</volume>(<issue>3</issue>):<fpage>624</fpage>&#x2013;<lpage>634</lpage>. <xref ref-type="sec" rid="s1">1</xref>, <xref ref-type="sec" rid="s1c">1.3</xref>, <xref ref-type="sec" rid="s5a2">5.1.2</xref>, <xref ref-type="sec" rid="s6b">6.2</xref></mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><string-name><surname>Eklund</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Andersson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Josephson</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Johannesson</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Knutsson</surname>, <given-names>H.</given-names></string-name> (<year>2012</year>). <article-title>Does parametric fMRI analysis with SPM yield valid results? - An empirical study of 1484 rest datasets</article-title>. <source>NeuroImage</source>, <volume>61</volume>:<fpage>565</fpage>&#x2013;<lpage>578</lpage>. <xref ref-type="sec" rid="s1">1</xref>, <xref ref-type="sec" rid="s1c">1.3</xref></mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><string-name><surname>Eklund</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Dufort</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Forsberg</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>LaConte</surname>, <given-names>S. M.</given-names></string-name> (<year>2013</year>). <article-title>Medical image processing on the GPU - Past, present and future</article-title>. <source>Medical Image Analysis</source>, <volume>17</volume>(<issue>8</issue>):<fpage>1073</fpage>&#x2013;<lpage>1094</lpage>. <xref ref-type="sec" rid="s6a">6.1</xref></mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><surname>Eklund</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Dufort</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Villani</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>LaConte</surname>, <given-names>S.</given-names></string-name> (<year>2014</year>). <article-title>BROCCOLI: Software for Fast fMRI Analysis on Many-Core CPUs and GPUs</article-title>. <source>Frontiers in Neuroinformatics</source>, <volume>8</volume>(<issue>24</issue>). <xref ref-type="sec" rid="s5b1">5.2.1</xref>, <xref ref-type="sec" rid="s6a">6.1</xref></mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><string-name><surname>Eklund</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Nichols</surname>, <given-names>T.</given-names></string-name>, and <string-name><surname>Knutsson</surname>, <given-names>H.</given-names></string-name> (<year>2016</year>). <article-title>Cluster failure: why fMRI inferences for spatial extent have inflated false-positive rates</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>113</volume>(<issue>28</issue>):<fpage>7900</fpage>&#x2013;<lpage>7905</lpage>. <xref ref-type="sec" rid="s6c">6.3</xref></mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Holmes</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Poline</surname>, <given-names>J.-B.</given-names></string-name>, <string-name><surname>Grasby</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Frackowiak</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Turner</surname>, <given-names>R.</given-names></string-name> (<year>1995</year>). <article-title>Analysis of fMRI time-series revisited</article-title>. <source>NeuroImage</source>, <volume>2</volume>(<issue>1</issue>):<fpage>45</fpage>&#x2013;<lpage>53</lpage>. <xref ref-type="sec" rid="s1">1</xref></mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name>, <string-name><surname>Holmes</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Worsley</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Poline</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Frith</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Frackowiak</surname>, <given-names>R.</given-names></string-name> (<year>1994</year>). <article-title>Statistical parametric maps in functional imaging: a general linear approach</article-title>. <source>Human brain mapping</source>, (<issue>2</issue>):<fpage>189</fpage>&#x2013;<lpage>210</lpage>. <xref ref-type="sec" rid="s1">1</xref></mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><string-name><surname>Glover</surname>, <given-names>G. H.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>T.-Q.</given-names></string-name>, and <string-name><surname>Ress</surname>, <given-names>D.</given-names></string-name> (<year>2000</year>). <article-title>Image-based method for retrospective correction of physiological motion effects in fMRI: RETROICOR</article-title>. <source>Magnetic Resonance in Medicine</source>, <volume>44</volume>(<issue>1</issue>):<fpage>162</fpage>&#x2013;<lpage>167</lpage>. <xref ref-type="sec" rid="s6d">6.4</xref></mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><surname>Grootoonk</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Hutton</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Ashburner</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Howseman</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Josephs</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Rees</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name>, and <string-name><surname>Turner</surname>, <given-names>R.</given-names></string-name> (<year>2000</year>). <article-title>Characterization and correction of interpolation effects in the realignment of fMRI time series</article-title>. <source>NeuroImage</source>, <volume>11</volume>:<fpage>49</fpage>&#x2013;<lpage>57</lpage>. <xref ref-type="sec" rid="s1a">1.1</xref></mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><string-name><surname>Gudbjartsson</surname>, <given-names>H.</given-names></string-name> and <string-name><surname>Patz</surname>, <given-names>S.</given-names></string-name> (<year>1995</year>). <article-title>The Rician distribution of noisy MRI data</article-title>. <source>Magnetic resonance in medicine</source>, <volume>34</volume>(<issue>6</issue>):<fpage>910</fpage>&#x2013;<lpage>914</lpage>. <xref ref-type="sec" rid="s1">1</xref></mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="website"><string-name><surname>Guennebaud</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Jacob</surname>, <given-names>B.</given-names></string-name>, <etal>et al.</etal> (<year>2010</year>). <source>Eigen v3</source>. <ext-link ext-link-type="uri" xlink:href="http://eigen.tuxfamily.org">http://eigen.tuxfamily.org</ext-link>. <xref ref-type="sec" rid="s4">4</xref></mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><string-name><surname>Kohn</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Smith</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Chan</surname>, <given-names>D.</given-names></string-name> (<year>2001</year>). <article-title>Nonparametric regression using linear combinations of basis functions</article-title>. <source>Statistics and Computing</source>, <volume>11</volume>(<issue>4</issue>):<fpage>313</fpage>&#x2013;<lpage>322</lpage>. <xref ref-type="sec" rid="s3b">3.2</xref></mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><string-name><surname>Lenoski</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Baxter</surname>, <given-names>L. C.</given-names></string-name>, <string-name><surname>Karam</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Maisog</surname>, <given-names>J.</given-names></string-name>, and <string-name><surname>Debbins</surname>, <given-names>J.</given-names></string-name> (<year>2008</year>). <article-title>On the performance of autocorrelation estimation algorithms for fMRI analysis</article-title>. <source>IEEE Journal of Selected Topics in Signal Processing</source>, <volume>2</volume>(<issue>6</issue>):<fpage>828</fpage>&#x2013;<lpage>838</lpage>. <xref ref-type="sec" rid="s1">1</xref></mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><string-name><surname>Lund</surname>, <given-names>T. E.</given-names></string-name>, <string-name><surname>Madsen</surname>, <given-names>K. H.</given-names></string-name>, <string-name><surname>Sidaros</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Luo</surname>, <given-names>W.-L.</given-names></string-name>, and <string-name><surname>Nichols</surname>, <given-names>T. E.</given-names></string-name> (<year>2006</year>). <article-title>Non-white noise in fMRI: Does modelling have an impact?</article-title> <source>NeuroImage</source>, <volume>29</volume>(<issue>1</issue>):<fpage>54</fpage>&#x2013;<lpage>66</lpage>. <xref ref-type="sec" rid="s1">1</xref></mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><string-name><surname>Luo</surname>, <given-names>W.-L.</given-names></string-name> and <string-name><surname>Nichols</surname>, <given-names>T. E.</given-names></string-name> (<year>2003</year>). <article-title>Diagnosis and exploration of massively univariate neuroimaging models</article-title>. <source>NeuroImage</source>, <volume>19</volume>(<issue>3</issue>):<fpage>1014</fpage>&#x2013;<lpage>1032</lpage>. <xref ref-type="sec" rid="s1">1</xref></mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><string-name><surname>Noh</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Solo</surname>, <given-names>V.</given-names></string-name> (<year>2011</year>). <article-title>Rician distributed FMRI: Asymptotic power analysis and Cramer-Rao lower bounds</article-title>. <source>IEEE Transactions on Signal Processing</source>, (<issue>59</issue>):<fpage>1322</fpage>&#x2013;<lpage>1328</lpage>. <xref ref-type="sec" rid="s1">1</xref></mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><string-name><surname>Penny</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Kiebel</surname>, <given-names>S.</given-names></string-name>, and <string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name> (<year>2003</year>). <article-title>Variational Bayesian inference for fMRI time series</article-title>. <source>NeuroImage</source>, <volume>19</volume>(<issue>3</issue>):<fpage>727</fpage>&#x2013;<lpage>741</lpage>. <xref ref-type="sec" rid="s6a">6.1</xref></mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><string-name><surname>Penny</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Trujillo-Barreto</surname>, <given-names>N.</given-names></string-name>, and <string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name> (<year>2005</year>). <article-title>Bayesian fMRI time series analysis with spatial priors</article-title>. <source>NeuroImage</source>, <volume>24</volume>(<issue>2</issue>):<fpage>350</fpage>&#x2013;<lpage>362</lpage>. <xref ref-type="sec" rid="s6d">6.4</xref></mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><string-name><surname>Poldrack</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Barch</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Mitchell</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wager</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Wagner</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Devlin</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Cumba</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Koyejo</surname>, <given-names>O.</given-names></string-name>, and <string-name><surname>Milham</surname>, <given-names>M.</given-names></string-name> (<year>2013</year>). <article-title>Toward open sharing of task-based fMRI data: the OpenfMRI project</article-title>. <source>Frontiers in Neuroinformatics</source>, <volume>7</volume>:<fpage>12</fpage>. <xref ref-type="sec" rid="s5b">5.2</xref></mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><string-name><surname>Poldrack</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>Gorgolewski</surname>, <given-names>K.</given-names></string-name> (<year>2014</year>). <article-title>Making big data open: data sharing in neuroimaging</article-title>. <source>Nature Neuroscience</source>, <volume>17</volume>:<fpage>1510</fpage>&#x2013;<lpage>1517</lpage>. <xref ref-type="sec" rid="s5b">5.2</xref></mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><string-name><surname>Power</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Mitra</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Laumann</surname>, <given-names>T. O.</given-names></string-name>, <string-name><surname>Snyder</surname>, <given-names>A. Z.</given-names></string-name>, <string-name><surname>Schlaggar</surname>, <given-names>B. L.</given-names></string-name>, and <string-name><surname>Petersen</surname>, <given-names>S. E.</given-names></string-name> (<year>2014</year>). <article-title>Methods to detect, characterize, and remove motion artifact in resting state fMRI</article-title>. <source>NeuroImage</source>, <volume>84</volume>:<fpage>320</fpage>&#x2013;<lpage>341</lpage>. <xref ref-type="sec" rid="s1c">1.3</xref>, <xref ref-type="sec" rid="s2">2</xref>, <xref ref-type="sec" rid="s6">6</xref></mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="other"><string-name><surname>Power</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Plitt</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Laumann</surname>, <given-names>T. O.</given-names></string-name>, and <string-name><surname>Martin</surname>, <given-names>A.</given-names></string-name> (<year>2016</year>). <article-title>Sources and implications of whole-brain fMRI signals in humans</article-title>. <source>NeuroImage</source>, page In press. <xref ref-type="sec" rid="s6d">6.4</xref></mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><string-name><surname>Satterthwaite</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Elliott</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Gerraty</surname>, <given-names>R. T.</given-names></string-name>, <string-name><surname>Ruparel</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Loughead</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Calkins</surname>, <given-names>M. E.</given-names></string-name>, <string-name><surname>Eickhoff</surname>, <given-names>S. B.</given-names></string-name>, <string-name><surname>Hakonarson</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Gur</surname>, <given-names>R. C.</given-names></string-name>, <string-name><surname>Gur</surname>, <given-names>R. E.</given-names></string-name>, and <string-name><surname>Wolf</surname>, <given-names>D. H.</given-names></string-name> (<year>2013</year>). <article-title>An improved framework for confound regression and filtering for control of motion artifact in the preprocessing of resting-state functional connectivity data</article-title>. <source>NeuroImage</source>, <volume>64</volume>:<fpage>240</fpage>&#x2013;<lpage>256</lpage>. <xref ref-type="sec" rid="s1c">1.3</xref>, <xref ref-type="sec" rid="s6">6</xref></mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><string-name><surname>Sid&#x00E9;n</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Eklund</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bolin</surname>, <given-names>D.</given-names></string-name>, and <string-name><surname>Villani</surname>, <given-names>M.</given-names></string-name> (<year>2017</year>). <article-title>Fast Bayesian whole-brain fMRI analysis with spatial 3D priors</article-title>. <source>NeuroImage</source>, <volume>146</volume>:<fpage>211</fpage>&#x2013;<lpage>225</lpage>. <xref ref-type="sec" rid="s4">4</xref>, <xref ref-type="sec" rid="s6d">6.4</xref></mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><string-name><surname>Siegel</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Power</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Dubis</surname>, <given-names>J. W.</given-names></string-name>, <string-name><surname>Vogel</surname>, <given-names>A. C.</given-names></string-name>, <string-name><surname>Church</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Schlaggar</surname>, <given-names>B. L.</given-names></string-name>, and <string-name><surname>Petersen</surname>, <given-names>S. E.</given-names></string-name> (<year>2014</year>). <article-title>Statistical improvements in functional magnetic resonance imaging analyses produced by censoring high-motion data points</article-title>. <source>Human Brain Mapping</source>, <volume>35</volume>(<issue>5</issue>):<fpage>1981</fpage>&#x2013;<lpage>1996</lpage>. <xref ref-type="sec" rid="s1c">1.3</xref>, <xref ref-type="sec" rid="s6">6</xref></mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><string-name><surname>Smith</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Kohn</surname>, <given-names>R.</given-names></string-name> (<year>1996</year>). <article-title>Nonparametric regression using Bayesian variable selection</article-title>. <source>Journal of Econometrics</source>, <volume>75</volume>(<issue>2</issue>):<fpage>317</fpage>&#x2013;<lpage>343</lpage>. <xref ref-type="sec" rid="s3c">3.3</xref></mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="book"><string-name><surname>Solo</surname>, <given-names>V.</given-names></string-name> and <string-name><surname>Noh</surname>, <given-names>J.</given-names></string-name> (<year>2007</year>). <chapter-title>An EM algorithm for Rician fMRI activation detection</chapter-title>. In <source>IEEE International Symposium on Biomedical Imaging (ISBI)</source>, pages <fpage>464</fpage>&#x2013;<lpage>467</lpage>. <publisher-name>IEEE</publisher-name>. <xref ref-type="sec" rid="s1">1</xref></mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><string-name><surname>Tom</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Fox</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Trepel</surname>, <given-names>C.</given-names></string-name>, and <string-name><surname>Poldrack</surname>, <given-names>R.</given-names></string-name> (<year>2007</year>). <article-title>The neural basis of loss aversion in decision-making under risk</article-title>. <source>Science</source>, <volume>315</volume>:<fpage>515</fpage>&#x2013;<lpage>518</lpage>. <xref ref-type="sec" rid="s5b">5.2</xref></mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><string-name><surname>Villani</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kohn</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Giordani</surname>, <given-names>P.</given-names></string-name> (<year>2009</year>). <article-title>Regression density estimation using smooth adaptive Gaussian mixtures</article-title>. <source>Journal of Econometrics</source>, <volume>153</volume>(<issue>2</issue>):<fpage>155</fpage>&#x2013;<lpage>173</lpage>. <xref ref-type="sec" rid="s3d">3.4</xref></mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><string-name><surname>Villani</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kohn</surname>, <given-names>R.</given-names></string-name>, and <string-name><surname>Nott</surname>, <given-names>D.</given-names></string-name> (<year>2012</year>). <article-title>Generalized smooth finite mixtures</article-title>. <source>Journal of Econometrics</source>, <volume>171</volume>(<issue>2</issue>):<fpage>121</fpage>&#x2013;<lpage>133</lpage>. <xref ref-type="sec" rid="s3d">3.4</xref></mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><string-name><surname>Woolrich</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Behrens</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Beckmann</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Smith</surname>, <given-names>S.</given-names></string-name> (<year>2004</year>a). <article-title>Multilevel linear modelling for FMRI group analysis using Bayesian inference</article-title>. <source>NeuroImage</source>, <volume>21</volume>:<fpage>1732</fpage>&#x2013;<lpage>1747</lpage>. <xref ref-type="sec" rid="s1">1</xref></mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Jenkinson</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Brady</surname>, <given-names>J. M.</given-names></string-name>, and <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name> (<year>2004</year>b). <article-title>Fully Bayesian spatio-temporal modeling of FMRI data</article-title>. <source>IEEE Transactions on Medical Imaging</source>, <volume>23</volume>(<issue>2</issue>):<fpage>213</fpage>&#x2013;<lpage>231</lpage>. <xref ref-type="sec" rid="s4">4</xref></mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><string-name><surname>Woolrich</surname>, <given-names>M. W.</given-names></string-name>, <string-name><surname>Ripley</surname>, <given-names>B. D.</given-names></string-name>, <string-name><surname>Brady</surname>, <given-names>M.</given-names></string-name>, and <string-name><surname>Smith</surname>, <given-names>S. M.</given-names></string-name> (<year>2001</year>). <article-title>Temporal autocorrelation in univariate linear modeling of FMRI data</article-title>. <source>NeuroImage</source>, <volume>14</volume>(<issue>6</issue>):<fpage>1370</fpage>&#x2013;<lpage>1386</lpage>. <xref ref-type="sec" rid="s1">1</xref></mixed-citation></ref>
</ref-list>
<app-group>
<app id="app1"><label>7. Appendix A</label><title>Implementation</title>
<p>Our heteroscedastic GLM can be launched from a terminal as
<disp-quote>
<p>HeteroGLM fmri.nii.gz -designfiles activitycovariates.txt</p>
<p>-gammacovariates gammacovariates.txt</p>
<p>-ontrialbeta trialbeta.txt</p>
<p>-ontrialgamma trialgamma.txt</p>
<p>-ontrialrho trialrho.txt</p>
<p>-mask mask.nii.gz</p>
<p>-regressmotion motion.txt</p>
<p>-regressmotionderiv motionderiv.txt</p>
<p>-draws 1000</p>
<p>-burnin 1000</p>
<p>-savefullposterior</p>
</disp-quote>
where &#x2018;activitycovariates.txt&#x2019; states the activity covariates for the design matrix (normally only used for the mean), &#x2018;gammacovariates.txt&#x2019; states the covariates being used to model the variance, &#x2018;ontrialbeta.txt&#x2019; states covariates for which variable selection is performed for the mean, &#x2018;ontri-algamma.txt&#x2019; states covariates for which variable selection is performed for the variance and &#x2018;ontrialrho.txt&#x2019; states variable selection parameters for the autocorrelation. Covariates for intercept and time trends are automatically added internally. A homoscedastic GLM can easily be obtained as a special case, using only a single covariate (the intercept) for the variance. The following nifti files are created; posterior mean of beta and Ibeta (for each covariate), posterior mean of gamma and Igamma (for each covariate), posterior mean of rho and Irho (for each AR parameter), and PPMs for each activity covariate. The full posterior of all beta, gamma and rho parameters can also be saved as nifti files.</p>
</app>
<app id="app2"><label>8. Appendix B</label><title>MCMC Details</title>
<sec><title>Variable selection by MCMC in the linear regression model</title>
<p>Let us assume a general multivariate prior <italic>&#x00DF;</italic><sub><italic>&#x1D4D8;</italic></sub>|<italic>&#x1D4D8;</italic> &#x007E; <italic>N</italic> (<italic>&#x00B5;</italic>, &#x03A9;<sub><italic>&#x1D4D8;</italic></sub>). Now,
<disp-formula><alternatives><graphic xlink:href="091066_uneqn4.gif"/></alternatives></disp-formula>
where <bold>X</bold><sub><italic>&#x1D4D8;</italic></sub> is the matrix formed by selecting the columns of <bold>X</bold> corresponding to <italic>&#x1D4D8;</italic>. The conditional likelihood exp <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline22.gif"/></alternatives></inline-formula> can be decomposed as
<disp-formula><alternatives><graphic xlink:href="091066_uneqn5.gif"/></alternatives></disp-formula>
Multiplying the conditional likelihood by the prior and completing the square<sup>4</sup> gives
<disp-formula><alternatives><graphic xlink:href="091066_uneqn6.gif"/></alternatives></disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline23.gif"/></alternatives></inline-formula>
and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="091066_inline24.gif"/></alternatives></inline-formula>. This shows that
<disp-formula><alternatives><graphic xlink:href="091066_uneqn7.gif"/></alternatives></disp-formula>
Integrating with respect to <italic>&#x03B2;</italic><sub><italic>&#x1D4D8;</italic></sub> gives
<disp-formula><alternatives><graphic xlink:href="091066_uneqn8.gif"/></alternatives></disp-formula></p>
</sec>
</app>
</app-group>
<fn-group>
<fn id="fn1"><label><sup>1</sup></label><p><ext-link ext-link-type="uri" xlink:href="http://openfmri.org/dataset/ds000003/">https://openfmri.org/dataset/ds000003/</ext-link></p></fn>
<fn id="fn2"><label><sup>2</sup></label><p><ext-link ext-link-type="uri" xlink:href="http://openfmri.org/dataset/ds000006/">https://openfmri.org/dataset/ds000006/</ext-link></p></fn>
<fn id="fn3"><label><sup>3</sup></label><p><ext-link ext-link-type="uri" xlink:href="http://openfmri.org/dataset/ds000005/">https://openfmri.org/dataset/ds000005/</ext-link></p></fn>
</fn-group>
</back>
</article>