<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/301036</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Bioinformatics</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Pattern-Driven Navigation in 2D Multiscale Visual Spaces with Scalable Insets</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8432-4835</contrib-id>
<name><surname>Lekschas</surname><given-names>Fritz</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1102-103X</contrib-id>
<name><surname>Behrisch</surname><given-names>Michael</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9201-7744</contrib-id>
<name><surname>Bach</surname><given-names>Benjami</given-names></name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0454-1803</contrib-id>
<name><surname>Kerpedjiev</surname><given-names>Peter</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0327-8297</contrib-id>
<name><surname>Gehlenborg</surname><given-names>Nils</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3620-2582</contrib-id>
<name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Harvard University</institution></aff>
<aff id="a2"><label>2</label><institution>Harvard Medical School</institution></aff>
<aff id="a3"><label>3</label><institution>University of Edinburgh</institution></aff>
</contrib-group>
<pub-date pub-type="epub">
<year>2018</year>
</pub-date>
<elocation-id>301036</elocation-id>
<history>
<date date-type="received">
<day>13</day>
<month>4</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>13</day>
<month>4</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>15</day>
<month>4</month>
<year>2018</year>
</date>
</history><permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license></permissions>
<self-uri xlink:href="301036.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>We present <italic>Scalable Insets</italic>, a technique for interactively exploring and navigating large numbers of annotated patterns in multiscale visual spaces such as gigapixel images, matrices, or maps. Exploration of many but sparsely-distributed patterns in multiscale visual spaces is challenging as visual representations change across zoom levels, context and navigational cues get lost upon zooming, and navigation is time consuming. Our technique visualizes annotated patterns too small to be identifiable at certain zoom levels using insets, i.e., magnified thumbnail views of the annotated pattern. Insets support users in searching, comparing, and contextualizing patterns, while reducing the amount of navigation needed. They are dynamically placed either within the viewport or along the boundary of the viewport to offer a compromise between locality and context preservation. Annotated patterns are interactively clustered by location and type. They are visually represented as an aggregated inset to provide scalable exploration within a single viewport. A controlled user study with 18 participants found improved performance in visual search (up to 45&#x0025; faster) and comparison of pattern types (up to 32 percentage points more accurate) compared to a baseline technique. A second study with 6 experts in the field of genomics showed that Scalable Insets are easy to learn and effective in a biological data exploration scenario.</p>
</abstract>
<counts>
<page-count count="28"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Many large data sets, such as gigapixel images, geographic maps, or networks, require exploration of annotations at different levels of detail. We call an annotation a region in the visualization that contains some visual patterns (called <italic>annotated pattern</italic> henceforth) as seen in <xref ref-type="fig" rid="fig1">Figure 1</xref>. These annotated patterns can either be generated by users or derived computationally. However, annotated patterns are often magnitudes smaller than the overview and too small to be visible. This makes tasks such as exploration, searching, comparing, or contextualizing challenging, as considerable navigation is needed to overcome the lack of overview or detail.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><p>Scalable Insets applied to a genome interaction matrix [<xref ref-type="bibr" rid="c47">47</xref>], gigapixel image [<xref ref-type="bibr" rid="c8">8</xref>], and geographic map from Mapbox [<xref ref-type="bibr" rid="c37">37</xref>] and OpenStreetMaps [<xref ref-type="bibr" rid="c42">42</xref>] (left to right). Various annotated patterns are highlighted in red.</p></caption>
<graphic xlink:href="301036_fig1.tif"/>
</fig>
<p>Exploring annotated patterns in context is often needed to assess the relevance of patterns and to dissect important from unimportant regions. For example, computational biologists study thousands of small patterns in large genome interaction matrices [<xref ref-type="bibr" rid="c9">9</xref>] to understand which physical interactions between regions on the genome are the driving factor that defines the D structure of the genome. In astronomy, researchers are exploring and comparing multiple heterogeneous galaxies and stars within super high-resolution imagery [<xref ref-type="bibr" rid="c44">44</xref>]. In either case, inspecting every potentially important region in detail is simply not feasible.</p>
<p>Exploring visual details of these annotated patterns in multiscale visual spaces requires a tradeoff between several conflicting criteria. Annotated patterns must be visible for inspection and comparison (D<sc>etail</sc>). Enough of the overview needs to be visible to provide context for the patterns (C<sc>ontext</sc>). And any detailed representation of an annotated pattern must be close to its actual position in the overview (L<sc>ocality</sc>). Current interactive navigation and visualization approaches, such as focus&#x002B;context, overview&#x002B;detail, or general highlighting techniques (<xref ref-type="sec" rid="s2">section 2</xref>), address some but not all of these criteria and become difficult as repeated viewport changes, multiple manual lenses, or separate views at different zoom levels are required, which stress the user&#x2019;s mental capacities.</p>
<p>In this paper, we describe <italic>Scalable Insets</italic>&#x2014;a scalable visualization and interaction technique for interactively exploring and navigating large numbers of annotated patterns in multiscale visual spaces. Scalable Insets support users in early exploration through multifocus guidance by dynamically placing magnified thumbnails of annotated patterns as insets within the viewport (<xref ref-type="fig" rid="fig1">Figure 1</xref>). The design of Scalable Insets is informed by interviews with genomics experts, who are engaged in exploring thousands of patterns in genome interaction matrices. To keep the number of insets stable as users navigate, we developed a dynamic grouping and interactive placement of insets within the viewport, clustering patterns based on their location, type, and the user&#x2019;s viewport (<xref ref-type="sec" rid="s4c">subsection 4.3</xref>). The degree of clustering constitutes a tradeoff between C<sc>ontext</sc> and D<sc>etail</sc>. Groups of patterns are visually represented as a single aggregated inset to accommodate for D<sc>etail</sc>. Details of aggregated patterns are gradually resolved as the user navigates into certain regions. We also present two dynamic mechanisms (<xref ref-type="sec" rid="s4b">subsection 4.2</xref>) for placing insets either within the overview (<xref ref-type="fig" rid="fig1">Figure 1</xref> left and center) or on the overview&#x2019;s boundary (<xref ref-type="fig" rid="fig1">Figure 1</xref> right) to allow flexible adjustment to L<sc>ocality</sc>. With Scalable Insets, the user can rapidly search, compare, and contextualize large pattern spaces in multiscale visualizations.</p>
<p>We implemented Scalable Insets as an extension to HiGlass [<xref ref-type="bibr" rid="c29">29</xref>], a flexible web application for viewing large tile-based datasets. The implementation currently supports gigapixel images, geographic maps, and genome interaction matrices. We present two usage scenarios for gigapixel images and geographic maps (<xref ref-type="sec" rid="s3a">subsection 3.1</xref>) to demonstrate the functionality of Scalable Insets. Feedback from a qualitative user study with six computational biologists who explored genome interaction matrices using Scalable Insets shows that our technique is easy to learn and effective in biological data exploration. Scalable Insets simplify the interpretation of computational results through identification and comparison of patterns in context and across zoom levels. Results of a controlled user study with 18 novice users comparing both placement mechanisms of Scalable Insets to a standard highlighting technique show that Scalable Insets reduced the time to find annotated patterns by up to 45&#x0025; at identical accuracy and improved the accuracy in comparing pattern types by up to 32 percentage points.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Related Work</title>
<p>In the following sections we review various techniques that focus on certain aspects of navigation in large-scale visualizations and highlight challenges for multifocus exploration of many small patterns.</p>
<sec id="s2a">
<title>Navigation</title>
<p>Pan and zoom [<xref ref-type="bibr" rid="c18">18</xref>] techniques are common for navigating large multiscale visual spaces. Although these techniques are often applied they can require a large amount of mental effort as either context or details are lost and navigating to distant regions can be time consuming [<xref ref-type="bibr" rid="c17">17</xref>]. Other techniques take advantage of the underlying data structure to simplify navigation. For example, Link Sliding and Bring &#x0026; Go developed by Moscovich et al. [<xref ref-type="bibr" rid="c41">41</xref>] implement navigation along an underlying network and provide context to navigational options through direct visualization of this network on demand. Similarly, Dynamic Insets [<xref ref-type="bibr" rid="c19">19</xref>] utilizes an underlying graph topology to dynamically place off-screen annotations as insets at the boundary of the view and thereby provides guidance on where to navigate next. However, many data sets do not provide such graph structures or navigation along links is not desirable.</p>
</sec>
<sec id="s2b">
<title>Highlighting</title>
<p>Highlighting the presence of details is commonly used to alleviate the lack of navigational cues [<xref ref-type="bibr" rid="c26">26</xref>]. For reviews on general highlighting techniques see [<xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c31">31</xref>,<xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c48">48</xref>,<xref ref-type="bibr" rid="c54">54</xref>]. Irrespective of the navigation technique or goal, knowing details about the outcome of an interaction upfront can prevent spending unnecessary time on interacting with an information space. For instance, Scented Widgets [<xref ref-type="bibr" rid="c57">57</xref>] embed visual cues and simple visualizations into user interface elements. This can shorten the interaction time given that some details about the outcome of the interaction are known upfront. Ip and Varshney [<xref ref-type="bibr" rid="c23">23</xref>] describe a salience-based technique for guiding users in gigapixel images. They utilize color to highlight regions of high salience. While this is very effective to provide visual cues, these cues do not enable the user to get an understanding of the detailed visual structure of the highlighted regions without having to manipulate the viewport. Scalable Insets draws on these ideas to display the details of annotated patterns across scales.</p>
</sec>
<sec id="s2c">
<title>Aggregation and Simplification</title>
<p>Scalability for large visualizations can be achieved through aggregation and simplification of sets of elements. For instance, ZAME [<xref ref-type="bibr" rid="c12">12</xref>] is a matrix visualization technique that presents a visual summary of multiple cells at a higher level. The summary is composed of an embedded visualization showing the distribution of aggregated cell values. Milo et al. [<xref ref-type="bibr" rid="c40">40</xref>] describe network motifs, which are repetitive network patterns, to facilitate a more concise view of large networks through visual simplification [<xref ref-type="bibr" rid="c10">10</xref>]. Van de Elzen and van Wijk [<xref ref-type="bibr" rid="c55">55</xref>] integrate the ideas of ZAME and Network Simplification into a very concise overview of large networks where nodes are aggregated into small statistical summary diagrams. This works well to gain an overview but the detailed visual structure of patterns is lost, which applies to many semantic zoom interfaces.</p>
</sec>
<sec id="s2d">
<title>Overview&#x002B;Detail</title>
<p>To address the problem of lost context or missing details [<xref ref-type="bibr" rid="c26">26</xref>] one can juxtapose multiple panels at different zoom levels. The separation between these panels provides flexibility but comes at the cost of divided user attention [<xref ref-type="bibr" rid="c39">39</xref>]. For example, in Poly-Zoom [<xref ref-type="bibr" rid="c25">25</xref>] different zoom levels are organized hierarchically and appear as separate panels, which limits the number of regions that the user can simultaneously focus on. TrailMap [<xref ref-type="bibr" rid="c58">58</xref>] shows thumbnails of previously visited locations in a map visualization to support rapid bookmarking and time traveling through the exploration history. Hereby, separate panels work well as the user has already seen each location in detail before it appears as a thumbnail, but it is not clear how efficient such an approach would be for guidance to new locations. HiPiler [<xref ref-type="bibr" rid="c33">33</xref>] supports exploration and comparisons of many patterns through interactive small multiples of many small patterns in a separated view. While this approach works well for out-of-context comparison, it has not been designed for guided navigation within the viewport of a multiscale visualization.</p>
</sec>
<sec id="s2e">
<title>Focus&#x002B;context</title>
<p>Focus&#x002B;context techniques show details in-place, while maintaining a continuous relationship to the context, often via distortion. The most common type of these techniques are lenses that can be moved and parameterized by the user (see [<xref ref-type="bibr" rid="c53">53</xref>] for a comprehensive review). For example, Table Lens [<xref ref-type="bibr" rid="c46">46</xref>] utilizes a degree of interest [<xref ref-type="bibr" rid="c17">17</xref>] approach to enlarge or shrink cells within a spreadsheet visualization. Similarly, Rubber Sheet [<xref ref-type="bibr" rid="c49">49</xref>] and JellyLens [<xref ref-type="bibr" rid="c45">45</xref>] are distortion-based techniques that enlarge areas of interest in a visualization. M&#x00E9;lange [<xref ref-type="bibr" rid="c13">13</xref>] takes a different approach by folding unfocused space into a 3rd dimension like paper. Unlike our method, these techniques benefit from maintaining a continuous view at the expense of direct context around a focus region and limiting the ability to support simultaneous focus on many regions.</p>
</sec>
<sec id="s2f">
<title>Detail-In-Context</title>
<p>Hybrid approaches magnify annotated patterns as insets within the image but offside their original location [<xref ref-type="bibr" rid="c5">5</xref>]. For example, Pad [<xref ref-type="bibr" rid="c43">43</xref>] and Pad&#x002B;&#x002B; [<xref ref-type="bibr" rid="c2">2</xref>] were one of the first tools to visualize details through magnified insets. Detail Lenses [<xref ref-type="bibr" rid="c27">27</xref>] put emphasis on several regions as insets that are arranged along the inner border of a map visualization. The insets are only loosely connected to their origin through sequential ordering and sparse leader lines. This ensures that the center of the map visualization is not occluded. This works well for up to a dozen insets without dynamic exploration. DragMag [<xref ref-type="bibr" rid="c56">56</xref>] extended the concept of Pad into a hybrid approach where magnified insets can either be placed within the image or on the outside border. Our new technique complements efforts in this area. In particular, we build upon the idea of DragMag [<xref ref-type="bibr" rid="c56">56</xref>] and extend it to multifocus scenarios that require automated grouping and placement to scale beyond a handful of insets.</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Scalable Insets: Overview</title>
<p>The design of Scalable Insets is driven by the three functional requirements for detail-in-context methods. It provides a dynamic tradeoff for DETAIL, C<sc>ontext</sc> and L<sc>ocality</sc> to overcome the issues one would run into with naive approaches (<xref ref-type="fig" rid="fig3">Figure 3</xref>). In the following we give an overview of the technique and demonstrate the visualization and guidance aspects. Technical details on how we achieve this tradeoff are given in <xref ref-type="sec" rid="s4">section 4</xref>.</p>
<p>In a multiscale visual space (<xref ref-type="fig" rid="fig2">Figure 2.1</xref>) that contains several <italic>annotated patterns</italic> (henceforth just called <italic>patterns</italic>), not all patterns will be identifiable at every zoom level. To this end, the notion of identifiability can be described as patterns being fully contained in the viewport and having a minimal output resolution, e.g., 24 &#x00D7; 24 pixels. Whenever a pattern is identifiable we are able to perceive its <italic>detailed visual structure</italic> (<xref ref-type="fig" rid="fig2">Figure 2.4</xref>). This setup lets us imagine a virtual pattern space (<xref ref-type="fig" rid="fig2">Figure 2.2</xref>), which describes when a pattern is visible or how much zooming is needed to identify its detailed visual structure. To reduce the interaction and navigation time to assess these visual details of patterns, we extract thumbnails of unidentifiable patterns at the zoom level that renders the pattern identifiable and place these thumbnails within the current viewport as insets. The number of displayed insets can be limited by clustering several closely-located patterns into a group (<xref ref-type="fig" rid="fig2">Figure 2.3</xref>) and representing this group as a single inset (<xref ref-type="fig" rid="fig2">Figure 2.4</xref>). Together with a dynamic placement strategy that avoids occlusion of insets and patterns, this enables Scalable Insets to provide guidance for high numbers of patterns while reducing navigation time.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><p>The core idea of the Scalable Inset. (1) A multiscale visual space with several annotated patterns, some of which are too small to be identifiable (indicated by &#x201C;???&#x201D;). (2) The virtual pattern space illustrated as a space-scale diagram, illustrating that some patterns are only identifiable at certain zoom levels. (3) To provide guidance, small patterns are placed as magnified insets into the current viewport. Scalability is ensured by dynamically grouping small patterns in close proximity and representing them as an aggregate as shown in (4).</p></caption>
<graphic xlink:href="301036_fig2.tif"/>
</fig>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><p>Three approaches exemplifying naive optimization of (1) L<sc>ocality</sc>, (2) C<sc>ontext</sc>, and (3) D<sc>etail</sc> only. The red rectangle in (C) indicates the size of the occluded image for reference.</p></caption>
<graphic xlink:href="301036_fig3.tif"/>
</fig>
<sec id="s3a">
<label>3.1</label>
<title>Usage Scenarios</title>
<p>The two following usage scenarios, on a gigapixel image and geographic map application, depict typical exploration tasks, such as searching, comparing, and contextualizing patterns, and focus likewise on overview and detail. A third, more complex exploration scenario on genome interaction matrices with computational biologists is presented in <xref ref-type="sec" rid="s6b">subsection 6.2</xref>.</p>
<sec id="s3a1">
<title>Exploring Gigapixel Images</title>
<p>We show a photograph of Rio de Janeiro [<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c52">52</xref>] with a resolution of 454330 &#x00D7; 149278 pixels (<xref ref-type="fig" rid="fig4">Figure 4.1</xref>) in which local users have annotated 924 patterns, including birds, people, or cars. Some of these patterns are close together, e.g., on the same street, while others are isolated in the sea. However, most of them are not identifiable without considerate pan and zoom. We follow a hypothetical journalist who is writing an article about unseen aspects of Rio de Janeiro, which requires finding, comparing, and localizing the annotated patterns to assess &#x201C;Which neighborhoods are particularly interesting to viewers?&#x201D;, &#x201C;What kind of patterns are most frequently annotated?&#x201D;, and &#x201C;Are there any unexpected patterns given their location?&#x201D;.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><p>Demonstration of the Scalable Insets approach on a gigapixel image of Rio de Janeiro [<xref ref-type="bibr" rid="c8">8</xref>] by <italic>The Rio de Janeiro</italic> - <italic>Hong Kong Connection</italic> [<xref ref-type="bibr" rid="c52">52</xref>] and ski areas around Utah and Colorado shown on a map from Mapbox [<xref ref-type="bibr" rid="c37">37</xref>] and OpenStreetMap [<xref ref-type="bibr" rid="c42">42</xref>]. The screenshots illustrate how Scalable Insets enables pattern-driven exploration and navigation at scale; details are explained in <xref ref-type="sec" rid="s3a">subsection 3.1</xref>.</p></caption>
<graphic xlink:href="301036_fig4.tif"/>
</fig>
<p>As shown in <xref ref-type="fig" rid="fig4">Figure 4.1</xref>, Scalable Insets places insets for annotated patterns too small to be identifiable within the viewport. Given their high number, Scalable Insets clusters many patterns that are in close proximity to each other, into groups, and presents these groups as aggregated insets. In this example, the number of insets ranges between 25-50 depending on the region being explored, which provides a good tradeoff between the D<sc>etail</sc> and C<sc>ontext</sc> criteria. The size of the insets ranges from 32 to 64 pixels (for the longer side) to provide a notion of depth and the popularity of patterns (determined by view statistics) is mapped onto the border size such that thicker borders indicate higher popularity.</p>
<p>The journalist starts by examining the entire picture to gain an overview. At a first glance, Scalable Insets reveals a relatively equal distribution of annotated patterns, with higher frequency in areas of man-made constructions (<xref ref-type="fig" rid="fig4">Figure 4.1</xref>) The journalist immediately finds popular pattern types as they are represented by the large image in an inset. The journalist notices a relatively high frequency of annotated swimming pools (<xref ref-type="fig" rid="fig4">Figure 4.2</xref>), which are scattered across the entire image but appear to be more frequently found in the left part of the image. Upon hovering over an inset, its origin is highlighted through small red dots and a red hull in the overview, which localizes the inset. Via a click, insets are scaled up and more details of the represented patterns are seen. This enables the journalist to quickly identify a bird (<xref ref-type="fig" rid="fig4">Figure 4.3i</xref>) sitting on a locally known rock and to find interesting street art (<xref ref-type="fig" rid="fig4">Figure 4.3ii</xref>), some of which are located in areas that are not easily accessible by foot. Also, with Scalable Insets, several patterns located in monotone regions (<xref ref-type="fig" rid="fig4">Figure 4.3iii</xref>), such as the sea, are explorable with minimal pan and zoom. As the journalist inspects a specific region more closely, aggregated insets of groups of patterns gradually split into smaller and smaller groups (<xref ref-type="fig" rid="fig4">Figure 4.4</xref>), presenting more details while maintaining a relatively fixed number of insets.</p>
</sec>
<sec id="s3a2">
<title>Exploring Geographic Maps</title>
<p>Our second scenario involves the exploration of ski areas in a geographic map fromMapbox [<xref ref-type="bibr" rid="c37">37</xref>] and OpenStreetMaps [<xref ref-type="bibr" rid="c42">42</xref>]. We obtain an estimation of the location of the world&#x2019;s ski areas by analyzing <italic>aerialway</italic> annotations [<xref ref-type="bibr" rid="c30">30</xref>] of the raw data from OpenStreetMap as these annotations describe potential paths of ski lifts, slopes, and other aerial ways.</p>
<p>The user sets out to find, compare, and localize <italic>interesting</italic> ski areas. In this scenario, interest is defined by the size of individual ski areas and the size of multiple ski areas within close proximity. Localization of ski areas is important to determine the accessibility and proximity by car between multiple closely-located ski areas. This time, insets are shown <italic>outside</italic> the map to provide full access of information in the map, such as streets, cities, mountains and other important geographical information needed for localization.</p>
<p>The user starts exploring around Utah and Colorado (<xref ref-type="fig" rid="fig4">Figure 4.E</xref>). The map shows two regions with several closely-located ski areas nearby Salt Lake City (<xref ref-type="fig" rid="fig4">Figure 4.5i</xref>) and Denver (<xref ref-type="fig" rid="fig4">Figure 4.5ii</xref>). Upon scaling up an inset, the user can explore size and shape of up to four representative ski areas among a group. This allows for fast comparison of the ski areas without the need to navigate. For example, the user compares three promising groups of several ski areas (<xref ref-type="fig" rid="fig4">Figure 4.5iii</xref>, <xref ref-type="fig" rid="fig5">5iv</xref>, and <xref ref-type="fig" rid="fig5">5v</xref>). Through hovering over different images in an aggregated inset, shown in <xref ref-type="fig" rid="fig4">Figure 4.5iii</xref>, the user identifies that this group contains only small ski areas as well as an outlier, i.e., an annotated region that does not correspond to a ski area. While the second group (<xref ref-type="fig" rid="fig4">Figure 4.5iv</xref>) indeed represents several large ski areas, close inspection (<xref ref-type="fig" rid="fig4">Figure 4.6</xref>) reveals that the road connecting these ski areas is closed during winter (<xref ref-type="fig" rid="fig4">Figure 4.2</xref>Fi). Zooming out again, the user finds a suitable region with several larger ski areas (<xref ref-type="fig" rid="fig4">Figure 4.7</xref>) that are conveniently accessible by car through the Interstate 70 nearby Vail (<xref ref-type="fig" rid="fig4">Figure 4.7i</xref>).</p>
</sec>
</sec>
</sec>
<sec id="s4">
<label>4</label>
<title>Scalable Insets: Technique</title>
<sec id="s4a">
<label>4.1</label>
<title>Inset and Leader Line Design</title>
<p>Insets are small, rectangular thumbnails of annotated patterns at increased magnification. The level of magnification is defined by the display size (in pixels) of the insets and zoom level. The display size of insets can depend on a continuous value, e.g., a confidence score or range between a user-defined minimum and maximum, but is invariant to the zoom level to give more control over D<sc>etail</sc> and C<sc>ontext</sc>. This comes at the cost of reduced awareness of the <italic>depth</italic> of annotated patterns, which we consider less important for Scalable Insets as it does not directly support finding, comparing, and contextualizing patterns. The thickness of the border can be used to encode an additional information and the hue of the border is adjustable to reflect different categories of annotated patterns. Both encodings are illustrated in <xref ref-type="fig" rid="fig5">Figure 5.1ii</xref> and <xref ref-type="fig" rid="fig5">Figure 5.1iii</xref>.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><p>Schematic design principals of Scalable Insets. (1) Inset design and information encoding. (2) Visual representation of aggregated insets. (3) Leader line styles. (4) The inset placement mechanism and stability considerations of Scalable Insets. (5) Aggregation procedure and stability considerations. (6) Interaction between insets applied in Scalable Insets.</p></caption>
<graphic xlink:href="301036_fig5.tif"/>
</fig>
<p>A leader line is drawn between insets and their origin in order to establish a mental connection as their positions may not coincide (<xref ref-type="sec" rid="s4b">subsection 4.2</xref>). We designed three different leader line styles. Plain and fading leader lines are static and only differ in their alpha values along the line (<xref ref-type="fig" rid="fig5">Figure 5.3i</xref> and <xref ref-type="fig" rid="fig5">Figure 5.3ii</xref>). Dynamic leader lines adjust their opacity depending on the distance of the mouse cursor to an inset or origin. Fading and dynamic leader lines minimize clutter in the event of leader line crossing to preserve context and have been shown to maintain a notion of connectedness [<xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c34">34</xref>]. We chose to limit Scalable Insets to straight leader lines for simplicity and performance reasons. While other techniques such as edge bundling [<xref ref-type="bibr" rid="c22">22</xref>] or boundary labeling [<xref ref-type="bibr" rid="c3">3</xref>] exist, the benefits are expected to be minimal. For inner-placement leader lines are usually very short, since insets are positioned as close as possible to their origin. Barth et al. [<xref ref-type="bibr" rid="c1">1</xref>] have shown that straight leader lines for boundary labeling, which is similar to our outer-placement (<xref ref-type="sec" rid="s4b">subsection 4.2</xref>), perform comparably to or even better than more sophisticated methods.</p>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>Inset Placement</title>
<p>We have developed an algorithm for positioning insets either within the view-port (inner-placement) or at the boundary of the viewport (outer-placement) based on simulated annealing. We chose simulated annealing for its simplicity, potential to produce high quality layouts [<xref ref-type="bibr" rid="c6">6</xref>], and generality [<xref ref-type="bibr" rid="c11">11</xref>]. Other methods can produce near-optimal layouts [<xref ref-type="bibr" rid="c27">27</xref>] but are not suitable for interactive adjustments in real time.</p>
<p>Our goal for placing insets is to maximize context preservation and locality. For both placement strategies, our algorithm optimizes three primary requirements. First, insets should not overlap with each other. At the same time, insets should be placed as close to their origin as possible. And finally, leader line crossing should be minimized. For the inner-placement, insets inevitably occlude some parts of the context. Therefore, our algorithm additionally tries to avoid occlusion of annotated patterns. <xref ref-type="fig" rid="fig5">Figure 5.4i</xref> and <xref ref-type="fig" rid="fig5">Figure 5.4iii</xref> illustrate both inner- and outer-placement strategies.</p>
<p>The inner-placement of insets is stable upon panning, i.e., the relative distance between insets and their origin is fixed. However, since the display size of insets is invariant to the zoom level, the algorithm is re-evaluating and potentially re-positioning insets upon zooming. Following all requirements strictly could lead to drastic changes in the positioning even upon subtle navigation. For example, in <xref ref-type="fig" rid="fig5">Figure 5.4ii</xref> a subtle zoom out would lead to occlusion of other annotated patterns (indicated as dashed, red boundaries), which would force the insets to <italic>jump</italic> to the next best position. Similarly, in <xref ref-type="fig" rid="fig5">Figure 5.4iv</xref> subtle panning to the left would change the closest available position on the boundary and make the inset flip from one side to the other. Since these phenomena would significantly harm the usability of Scalable Insets, we employ three additional requirements to stabilize placement across zoom levels. Position changes of insets are penalized to avoid marginal improvements to the placement, which would otherwise lead to unnecessary movements. Also, the Euclidean distance between an old and a new inset position is limited to avoid large changes. Finally, in the outer-placement approach, insets should not be moved to the opposing side, even if the leader line is getting long.</p>
</sec>
<sec id="s4c">
<label>4.3</label>
<title>Aggregation</title>
<p>To provide scalability beyond a handful of annotated patterns and preserve C<sc>ontext</sc>, we developed a density-based dynamic clustering algorithm that assigns every annotated pattern in the viewport to a particular group, called a cluster. Each each cluster is represented as a single visual entity, called an aggregated inset. Our clustering approach is based on the spatial distance between annotated patterns in the viewport. Starting with a randomly chosen pattern we find the closest cluster. Only if the distance between the selected pattern and the bounding box of the cluster is closer than a user-defined threshold and the area of the cluster combined with the selected pattern is smaller than a user-defined threshold do we assign the pattern to the given cluster (<xref ref-type="fig" rid="fig5">Figure 5.5i</xref>). Otherwise, we create a new cluster composed of the single, selected pattern. Upon adding patterns to clusters, we keep a sorted list of the nearest neighbors for each newly added pattern, which will help us to determine breakpoints upon zoom-out as explained next. Similar to the placement of insets, the clustering of pattern is re-evaluated upon navigation. To improve cluster stability between short, repeated zoom changes, the distance threshold, for deciding whether an inset should be assigned to a particular cluster, is dynamically adjusted as illustrated in <xref ref-type="fig" rid="fig5">Figure 5.5ii</xref>. During zoom-in, clusters remain unchanged until the distance between the farthest nearest neighbors is larger than 1.5 times the distance threshold. During zoom-out, clusters and insets will not be merged until their distance is less than half the distance threshold. This limits the changes to cluster composition upon navigation to facilitate the user&#x2019;s mental map of the pattern space. We chose to design a relatively simple clustering approach to ensure high performance in terms of rendering time. Our approach is inspired by DBSCAN [<xref ref-type="bibr" rid="c14">14</xref>] but we do not implement recursive scanning of nearby patterns as we strive for a spatially-equal partitioning rather than continuous clusters.</p>
<p>We designed two approaches to visually represent clusters. When exploring large matrices, clusters of annotated patterns are aggregated into piles and feature a 2D cover matrix together with 1D previews. The cover matrix represents a statistical summary of all the patterns on the cluster, e.g., the arithmetic mean or variance. 1D previews are averages across the Y dimension and provide a visual cue into the variability of patterns on the pile. To avoid arbitrary large piles in terms of their display size, the maximum number of previews is limited to a user-defined threshold. We employ KMeans clustering when the number of patterns on a pile is larger than the threshold and only represent 1D previews for the average patterns of each group. Clusters of patterns from photographic images and geographic maps are aggregated into a gallery of cluster representatives. A small number indicates the amount of patterns for clusters larger than four. Drawing on insights from work in exploration of the parameter, design, or ideation space [<xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c50">50</xref>,<xref ref-type="bibr" rid="c51">51</xref>], which suggests that diverse examples of groups are more efficient for representation than homogeneous examples, we chose a diverse set of patterns as the representatives. The largest pattern in the gallery is representing the most important pattern, according to a user-defined metric. The next representative is the pattern closest to the clusters centroid in Euclidean space. The last two representative patterns correspond to the two farthermost patterns in terms of their Euclidean distance. The pile-based aggregation is useful for patterns with well-alignable shapes, e.g., rectangles, lines, or points, and provides a concise representation of the average pattern and pattern variance. The gallery aggregation is preferable for patterns of diverse shapes as an average across shape boundaries does not provide meaningful insights.</p>
</sec>
<sec id="s4d">
<label>4.4</label>
<title>Inset Interaction</title>
<p>Scalable Insets add a minimal set of interaction to insets and is otherwise agnostic in terms of the navigation technique. Upon moving the mouse cursor over an inset the hue of its border and leader line are changed and a hull is drawn around the location of the annotated patterns represented by the inset. Upon scale-up (<xref ref-type="fig" rid="fig5">Figure 5.6i</xref>), it is possible to leaf through the 1D previews of a pile or the representatives of a gallery (<xref ref-type="fig" rid="fig5">Figure 5.6ii</xref>). Insets are draggable (<xref ref-type="fig" rid="fig5">Figure 5.6iii</xref>) to allow manual placement and uncover potentially occluded scenery in the overview (C<sc>ontext</sc>). Dragging disconnects insets from the locality criterion to avoid immediate re-positioning to the inset&#x2019;s previous position upon zooming. The disconnect is visualized with a small glyph indicating a broken link (<xref ref-type="fig" rid="fig5">Figure 5.6iii</xref>) and can be reversed through a click on this glyph.</p>
</sec>
</sec>
<sec id="s5">
<label>5</label>
<title>Implementation</title>
<p>To demonstrate the utility of Scalable Insets, we built a web-based prototype for gigapixel images, geographic maps, and genome interaction matrices. Scalable Insets are implemented as an extension to HiGlass [<xref ref-type="bibr" rid="c29">29</xref>], an open-source web-based viewer for large tile-based data sets. The Scalable Insets extension to HiGlass is implemented in JavaScript and integrates into the React [<xref ref-type="bibr" rid="c15">15</xref>] application framework. D3 [<xref ref-type="bibr" rid="c4">4</xref>] is used primarily for data mapping and matrices are rendered on canvas with PixiJS [<xref ref-type="bibr" rid="c36">36</xref>]. Scalable Insets utilizes HTML and CSS for positioning and styling insets and leader lines and implements flexible customization of nearly every parameter via a JSON configuration file. The server-site application, which extracts, aggregates, and serves the images for insets, is implemented in Python and built on top of Django [<xref ref-type="bibr" rid="c16">16</xref>]. Both, the front-end and back-end applications, are open-source, and freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/hms-dbmi/higlass">https://github.com/hms-dbmi/higlass</ext-link>.</p>
</sec>
<sec id="s6">
<label>6</label>
<title>Evaluation</title>
<p>We conducted two user studies (one quantiative, one qualitative) to assess whether people can quickly learn how to navigate and interact with Scalable Insets and if Scalable Insets increases performance (task completion-time, task accuracy) in exploring annotated patterns in large multiscale visualizations.</p>
<sec id="s6a">
<label>6.1</label>
<title>Study 1: Quantitative Evaluation</title>
<p>The goal of the first study was to compare the performance of Scalable Insets to traditional highlighting of the annotated patterns using boundary boxes and to assess the effects of context preservation when insets are located within the viewport and locality when insets are placed on the boundary of the viewport.</p>
<sec id="s6a1">
<title>Techniques</title>
<p>We compared the following three techniques and their configurations, illustrated in <xref ref-type="fig" rid="fig8">Figure 8</xref>.</p>
<list list-type="simple">
<list-item><p><bold>BB<sc>ox</sc></bold>: Annotated patterns are highlighted with boundary boxes.</p></list-item>
<list-item><p><bold>I<sc>nside</sc></bold>: Annotated patterns are highlighted with Scalable Insets placed <italic>inside</italic> the image viewport and mildly-translucent boundary boxes.</p></list-item>
<list-item><p><bold>O<sc>utside</sc></bold>: Annotated patterns are highlighted with Scalable Insets placed <italic>outside</italic> the image and mildly-translucent boundary boxes.</p></list-item>
</list>
</sec>
<sec id="s6a2">
<title>Data</title>
<p>For the study, we chose seven photographic gigapixel pictures<sup><xref ref-type="fn" rid="fn1">1</xref></sup> from Gigapan [<xref ref-type="bibr" rid="c20">20</xref>] showing different cities (e.g., <xref ref-type="fig" rid="fig6">Figure 6</xref>). We used two pictures for practice trials and used the remaining five for the final test trials. The annotated patterns in this study represent user-defined annotations from Gigapan. Image sizes ranged from 100,643 &#x00D7; 43,935 pixels to 454,330 &#x00D7; 149,278 and the number of patterns ranged from 82 to 924.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><p>Interface of the software, showing Rio de Janeiro [<xref ref-type="bibr" rid="c52">52</xref>], used in the first user study and example views for each task and technique. (1) Comparing the frequency of annotated patterns in two distinct regions with BBOX. (2) Finding a specific pattern that shows a Brazilian flag with INSIDE. (3) Comparing global frequency of patterns showing either &#x201C;player or sports field&#x201D; with &#x201C;Brazilian flag&#x201D; with O<sc>utside</sc>.</p></caption>
<graphic xlink:href="301036_fig6.tif"/>
</fig>
</sec>
<sec id="s6a3">
<title>Tasks</title>
<p>We defined the following three tasks for which we measured task-completion time (in seconds) and task accuracy (percentage of correct answers).</p>
<list list-type="bullet">
<list-item><p><bold>R<sc>egion</sc>:</bold> <italic>Which region contains more fully enclosed annotations; A or B?</italic> Participants had to chose between the two regions labeled A and B and click on either to confirm their answer. This task aimed to test if Scalable Insets cause clutter and if the aggregation prevents people from identifying dense regions.</p></list-item>
<list-item><p><bold>P<sc>attern</sc>:</bold> <italic>Find an annotation showing</italic> &#x003C;pattern&#x003E; <italic>and select it.</italic> The tag &#x003C;<italic>pattern</italic>&#x003E; was replaced with a description of a manually chosen pattern type that appears up to a few times in the image, e.g., &#x201C;a helicopter landing field&#x201D;. The goal of this task is to test our criteria of <sc>detail</sc>, i.e., whether showing patterns in detail is beneficial.</p></list-item>
<list-item><p><bold>T<sc>ype</sc>:</bold> <italic>Which visual pattern type appears more frequently; A or B?</italic> Where <italic>A</italic> and <italic>B</italic> were replaced with two generic pattern types appearing multiple times but not equally often, e.g., &#x201C;swimming pools&#x201D; and &#x201C;construction sites&#x201D;. Again, the participants had to choose between two options, which were collected via a click on either of two buttons. This task aimed at testing our criteria of <sc>locality</sc>.</p></list-item>
</list>
</sec>
<sec id="s6a4">
<title>Hypotheses</title>
<p>We formulated one hypothesis per task:
<list list-type="simple">
<list-item><p><bold>H1</bold> For <sc>region</sc>, there will be no difference in time and accuracy for any of the techniques as the detailed visual structure of annotated patterns is not important for R<sc>egion</sc>.</p></list-item>
<list-item><p><bold>H2</bold> For the P<sc>attern</sc> task, I<sc>nside</sc> will be faster than O<sc>utside</sc> and O<sc>utside</sc> will be faster than <sc>b</sc>B<sc>ox</sc>. We expect the inner-placement of insets to be most efficient as the detailed visual structure of annotated patterns is displayed spatially close to their original location, i.e., eye movement is minimized. We expect the outer-placement of insets to be slightly slower compared to <sc>inside</sc>, due to eye movement, but faster than the baseline technique as it still shows the detailed visual structure of annotated patterns.</p></list-item>
<list-item><p><bold>H3</bold> For the T<sc>ype</sc> task, I<sc>nside</sc> and O<sc>utside</sc> will be faster than BB<sc>ox</sc>. We expect Scalable Insets with both placement mechanisms to perform equally fast and better than BB<sc>ox</sc> as they both highlight the detailed visual structure of annotated patterns.</p></list-item>
</list></p>
</sec>
<sec id="s6a5">
<title>Participants</title>
<p>We recruited 18 participants (7 female and 11 male) from Harvard University. The majority (13) of participants were aged between 25 and 30. Three participants were aged between 18 and 25 and two between 31 and 35. The participants volunteered, had no visual impairments, and were not required to have any particular computer skills. Each participant received a &#x0024;15 gift card upon completion of the experiment.</p>
</sec>
<sec id="s6a6">
<title>Study Design</title>
<p>We used the following full-factorial within-subject study design with Latin square-randomized order of the techniques. Participants were split evenly between the three technique groups. Task order was kept constant across all participants and conditions. To avoid learning effects between images, the set of annotated patterns were split into three groups that covered an equally-sized region of the image. The order of these regions was kept constant, i.e., the first technique always operated on the first quadrant of the images. To avoid memory effects between R<sc>egion</sc> and P<sc>attern</sc> we excluded the patterns that we asked for in P<sc>attern</sc> from the trials on R<sc>egion</sc>. Furthermore, these specific patterns were chosen such that they are not contained in or in close proximity to the two regions that were compared in R<sc>egion</sc>. Finally, each trial is repeated five times on different images. Since we used real-world annotations the difficulty between images might differ. We ordered the images by size and amount of annotated patterns. The first image contains the fewest annotations and is the smallest in size. The annotation frequency, size, and structural difficulty increases with the last image being the largest and most frequently annotated. The order of the images was kept the same.</p>
</sec>
<sec id="s6a7">
<title>Setup</title>
<p>The study was conducted on a MacBook Pro with a 2.7 GHz quad-core processor and 16 GB RAM running MacOS X Sierra. The laptop featured a 15&#x201D; monitor with 2880 &#x00D7; 1800 pixels at an effective resolution of 1440 &#x00D7; 900 pixels, and was equipped with a standard two-button mouse that featured a scroll wheel.</p>
</sec>
<sec id="s6a8">
<title>Procedure</title>
<p>The study was conducted in individual sessions. Upon arrival, participants were informed about the general procedure of the experiment and consent was collected. We first introduced the data used in the study and briefly (2 minutes) demonstrated the functionality of the application interface. The participants then started a study software that guided them through each task. At the beginning, gender and age group was collected and participants had to solve a 12-image Ishihara color blindness test [<xref ref-type="bibr" rid="c24">24</xref>]. Prior to the actual test trials of each task, participants were shown detailed instructions and they had to complete two practice trials to familiarize themselves with the user interface and the respective task. The first out of five timed trials was started upon clicking on the start button located in the center of the screen. Once the user selected an answer the trial timer stopped and a click on the next button was awaited before starting the next trial. Participants were instructed to finish the trials <italic>as fast as possible and as accurately as possible</italic> but to also <italic>rely on their intuition</italic> when estimating frequencies (R<sc>egion</sc> and T<sc>ype</sc>). At the end, participants anonymously filled out a questionnaire on general improvements and rated insets approach in regard to its general impression, usefulness, and usability on a 5-point Likert scale.</p>
</sec>
</sec>
<sec id="s6b">
<title>Results</title>
<p>For each participant we collected measures from 810 timed trials, resulting in a total number of 14,580 trials. We found that completion time was not normally distributed after testing goodness-of-fit of a fitted normal distribution using a Shapiro-Wilk test. To remove outliers we visually inspected dot plots with individual data points and decided to remove trials that are more than 4 standard deviations away from the arithmetic mean time. These trials are most likely related to severe distraction. This resulted in the removal of 4 trials from REGION, 1 trial from PATTERN, and 1 trial from TYPE. Given the non-normal distribution of completion time and the unequal number of trials due to outlier removal, we report on non-parametric Kruskal-Wallis and Mann-Whitney U tests for assessing significance. For errors, we used a Chi-square test of independence. All <italic>p</italic>-values are reported for <italic>&#x03B1;</italic> = 0.05. In the following, we report on time (in seconds) and accuracy (in per) by task, summarized in <xref ref-type="fig" rid="fig7">Figure 7</xref>.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7:</label>
<caption><p>Mean completion time in seconds (lower is better) and mean accuracy in percent (higher is better) across tasks and techniques. Error bars indicate the 95&#x0025; confidence intervals. Note, due to non-normal distribution of completion time we report significance on the median using Kruskal-Wallis and Mann-Whitney U tests.</p></caption>
<graphic xlink:href="301036_fig7.tif"/>
</fig>
<sec id="s6b1">
<title>Results for R<sc>egion</sc></title>
<p>A pairwise post-hoc analysis revealed significant difference for completion time between BB<sc>ox</sc>-I<sc>nside</sc> (<italic>p</italic> = .0114) and BB<sc>ox</sc>-O<sc>utside</sc> (<italic>p</italic> = .0015) but not for I<sc>nside</sc>-O<sc>utside</sc>. The respective mean times are BB<sc>ox</sc>=13.4S (SD=14.2), I<sc>nside</sc>=14.1S (SD=10.4), O<sc>ut</sc>-SIDE=14.2S (SD=8.25). This constitutes an approximate speedup of 5.0&#x0025; for BB<sc>ox</sc> over I<sc>nside</sc> and 5.6&#x0025; for BB<sc>ox</sc> over O<sc>utside</sc>. These results let us reject H1 as BB<sc>ox</sc> was fastest. Given that in absolute numbers I<sc>nside</sc> and O<sc>utside</sc> are only 0.7s and 0.8s slower than BB<sc>ox</sc> suggests that overhead imposed by insets is fairly small and likely diminishes upon performance improvements to the current implementation of Scalable Insets as discussed later.</p>
<p>For accuracy, we found significant increase for BB<sc>ox</sc> (27 &#x0025;-points) and I<sc>nside</sc> (24 &#x0025;-points) over O<sc>utside</sc> (<italic>&#x03C7;</italic><sup>2</sup>(1, <italic>N</italic> = 177) = 30.4,<italic>p</italic> &#x003C; .0001 and <italic>&#x03C7;</italic><sup>2</sup> (1, <italic>N</italic> = 179) = 22.7, <italic>p</italic> &#x003C; .0001). We believe that this difference might be caused by misinterpretation or confusion of leader line stubs as they might have appeared like or occluded the bounding box around annotated patterns.</p>
</sec>
<sec id="s6b2">
<title>Results for P<sc>attern</sc></title>
<p>We found significant differences for completion time between BB<sc>ox</sc>-I<sc>nside</sc> (<italic>p</italic> = .0453) and BB<sc>ox</sc>-O<sc>utside</sc> (<italic>p</italic> = .0006). The mean times were BB<sc>ox</sc>=39.4 (SD=42.5), I<sc>nside</sc>=26.2 (SD=25.1), O<sc>ut</sc>-SIDE=21.8 (SD=22.9). This amounts for an approximate 44.9&#x0025; speed up for O<sc>utside</sc> over BB<sc>ox</sc> and 33&#x0025; speedup for I<sc>nside</sc> over BB<sc>ox</sc> and shows high potential for search task when the detailed visual structure of patterns and their location is important. While we could not confirm a superiority of I<sc>nside</sc> over O<sc>utside</sc>, we find a clear improvement of Scalable Insets over the baseline technique BB<sc>ox</sc>. We can thus partly accept H2. The speedup is very strong indicator that the core principal of Scalable Insets is efficient for pattern search. While the difference between I<sc>nside</sc> and O<sc>utside</sc> is not significant we are still surprised by the much stronger speedup for O<sc>utside</sc>. A possible explanation is that the alignment of insets in the outer-placement mechanism (O<sc>utside</sc>) is potentially beneficial for fast sequential scanning. This advantage might alleviate if contextual properties are included in the search task as well, e.g., find an annotated car located on an intersection, which we did not explicitly test for. For accuracy, we did not find any significant difference between the techniques.</p>
</sec>
<sec id="s6b3">
<title>Results for T<sc>ype</sc></title>
<p>We found only marginal significant (<italic>p</italic> = .0892) differences for completion time between BB<sc>ox</sc>-O<sc>utside</sc>. The mean times were BB<sc>ox</sc>=32.7 (SD=21.3), I<sc>nside</sc>=28.3 (SD=16.1), O<sc>utside</sc>=26.6 (SD=17.2). Although only marginally significant, we recognize an approximate 18.7&#x0025; speedup for O<sc>utside</sc> over BB<sc>ox</sc> on completion time. These results let us accept hypothesis H3. For accuracy, our results show pairwise significant differences between BB<sc>ox</sc>-I<sc>nside</sc> (<italic>&#x03C7;</italic><sup>2</sup>(1, <italic>N</italic> = 179) = 9.66, <italic>p</italic> = .0019) and BB<sc>ox</sc>-O<sc>utside</sc> (<italic>&#x03C7;</italic><sup>2</sup>(1, <italic>N</italic> = 180) = 23.5, <italic>p</italic> &#x003C; .0001). This describes an approximate improvement of 22 percentage points for I<sc>nside</sc> over BB<sc>ox</sc> and an approximate improvement of 32 percentage points for O<sc>utside</sc> over BB<sc>ox</sc>. While the completion time alone is not conclusive, the results for accuracy indicate that O<sc>utside</sc> and I<sc>nside</sc> provide a much better understanding of the distribution of pattern types. Participants with BB<sc>ox</sc> were only marginally significantly slower but made a lot more mistakes.</p>
</sec>
<sec id="s6b4">
<title>Qualitative feedback</title>
<p>In the closing questionnaire, participants were asked to rank general impression (Q1), usefulness of Scalable Insets (Q2), and simplicity to learn operating Scalable Insets (Q3) on a 5-point Likert scale ranging from 1: strongly disagree or negative to 5: strongly agree or positive. The results are presented in <xref ref-type="table" rid="tbl1">Table 1</xref>. overall, the participants perceived the Scalable Insets approach as a promising (Q1) and useful (Q2) for exploration. The exceptionally high ratings for the usability (Q3) indicate that the participants had no problem learning how to operate Scalable Insets. The complete questionnaire can be found in the supplementary Table S1. Two aspects were mentioned in the closing questionnaire multiple times: sudden disappearance of insets once the size of the original location is larger than a pre-defined threshold and the relatively low resolution of insets until scale-up. The first aspects highlight the need to adjust the dynamic rendering and placement of insets based on the user&#x2019;s region of interest, which could be inferred from the location of the mouse cursor. The relatively low resolution was due to performance reasons and can easily be mitigated through precomputation of image thumbnails.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><p>Results of the closing questionnaire in study 1 and 2. Note that Q3 of study 1 was only answered by 17 out of 18 participants and Q6 of study 2 was only answered by 3 out of 6 participants. Mean values are shaded by their values.</p></caption><graphic xlink:href="301036_tbl1.tif"/>
</table-wrap>
</sec>
</sec>
<sec id="s6c">
<label>6.2</label>
<title>Study 2: Qualitative Evaluation</title>
<p>The goal of our second study was to evaluate the usability and usefulness of Scalable Insets in the context of a scientific application. To that end we conducted six open-ended exploratory sessions with computational biologists, exploring large-scale genome interaction matrices in structural genomics (<xref ref-type="fig" rid="fig8">Figure 8</xref>) using both I<sc>nside</sc> and O<sc>utside</sc>. The apparatus was the same as in the first study.</p>
<fig id="fig8" position="float" orientation="portrait" fig-type="figure">
<label>Figure 8:</label>
<caption><p>Exploratory notes from the second user study. (1) Detailed inspection of an unexpected pattern through scale-up and leafing. (2) Example drill-down into the original location of a clustered pile of insets until they disperse. (3) Upon zoom-out a new pattern type was displayed as an inset (see little arrow) and immediately recognized. (4) Manual inspection of the context around the pile&#x2019;s original location (end of blue line). (5) Focus on a pile of two insets due to their location (corner of the larger rectangle).</p></caption>
<graphic xlink:href="301036_fig8.tif"/>
</fig>
<sec id="s6c1">
<title>Dataset</title>
<p>We obtained large-scale matrices at the order of 3 &#x00D7; 3 million rows and columns. The matrices were generated by biologists to study the probability of physical interaction between any two segments in the three-dimensional structure of a genome over time or across patients [<xref ref-type="bibr" rid="c9">9</xref>]. The matrix is visualized as a heatmap where darker cells indicate higher probability of phyical interaction [<xref ref-type="bibr" rid="c28">28</xref>]. Various algorithms are able to identify visual patterns inside the matrices that have potential biological implications for regulation of gene expression, replication, DNA repair, and other biological functions. The frequency of these patterns ranges from a few hundred to hundreds of thousands per matrix and analysts are interested in finding, comparing, and contextualizing patterns across many zoom levels (more details are described elsewhere [<xref ref-type="bibr" rid="c33">33</xref>]).</p>
</sec>
<sec id="s6c2">
<title>Participants</title>
<p>We recruited 6 computational biologists (2 female and 4 male) working in the field of structural genomics. 3 of the experts are PhD candidates and 3 are postdoctoral researchers. Each expert was already familiar with HiGlass but did not see its Scalable Insets extension before. All participants volunteered, had no visual impairments, and received &#x0024;15 upon completion of the experiment.</p>
</sec>
<sec id="s6c3">
<title>Tasks &#x0026; Study Design</title>
<p>The study consisted of individual open-ended sessions lasting between 20 and 30 minutes. Domain experts were asked to verbalize their thoughts and actions. Most participants started with I<sc>nside</sc> and some with O<sc>utside</sc>. In both cases the participants had to switch after half the time.</p>
</sec>
<sec id="s6c4">
<title>Procedure</title>
<p>Again, we first explained the study procedure and collected consent. Then, each participant was briefly introduced to Scalable Insets and the data that was to be explored (&#x003C;2 minutes). Next, we asked the participants to freely explore the data while verbalizing their thoughts. The screen content and audio was recorded for later analysis. At the end, each participant anonymously filled out a questionnaire on the usability, usefulness, and missing features.</p>
</sec>
</sec>
<sec id="s6d">
<title>Results</title>
<p>The results suggest that Scalable Insets is easy-to-learn. Participants immediately picked up the core concept of Scalable Insets and started exploring the data set as usual. Some participants started their exploration with slow pan and zoom, but all ended up using the interface in a fast manner. Having magnified and aggregated views of annotated patterns right inside the visual space was perceived useful for exploring genome interaction matrices. The domain experts were able to find and evaluate the detailed visual structure of the patterns early on without having to navigate to every location of an annotated pattern.</p>
<p>We notice that the inner- and outer-placement led to different behavior. For the outer-placement, people zoomed less often into the original location of an inset and instead compared the visual details of the patterns more frequently. P3 noticed <italic>&#x201C;It&#x2019;s easier to compare [the patterns] since they are all lined up nicely.&#x201D;</italic>. Some participants preferred one placement approach over the other but everyone noted that it would be useful to have the ability to switch between placement modes fluently. The complete protocol of actions can be found in the supplementary Table S2.</p>
<p>Some people needed more time to get used to the outer-placement approach with fading leader lines but identified the benefit for context preservation by themselves quickly. A drawback of the current implementation, mentioned by many participants, is the sudden disappearance of insets once their original location is larger than a pre-configured threshold (e.g., 24 &#x00D7; 24 pixels). Although it is necessary to remove insets, once the region of the annotated pattern becomes large enough, to release visual space for other patterns too small to be seen, it could be beneficial to adjust the threshold based on the region the user is zooming in or out. Insets in close proximity to the mouse cursor could remain visible until the user changes its focus through navigating elsewhere.</p>
<p>The closing questionnaire asked the participants to rate the usability and usefulness of the Scalable Insets approach for exploring genome interaction matrices. We asked participants to rate Scalable Insets on a 5-point Likert scale from 1: strongly disagree or negative to 5: strongly agree or positive on (Q1) their overall impression, (Q2) perceived usefulness of insets, (Q3) intuitiveness of the interface, (Q4) usefulness in its current form, (Q5) similar support by other tools, (Q6) performance comparison to other tools, (Q7) usefulness upon implementation of domain specific features, and (Q8) perceived usefulness outside the domain. The results are presented in <xref ref-type="table" rid="tbl1">Table 1</xref> and the complete questionnaire can be found in the supplementary Table S3.</p>
<p>The domain experts strongly indicated that they would use Scalable Insets to explore their annotated patterns, under the condition that additional application-specific and required features are added. For example, P4 would like to &#x201C;pin&#x201D; certain insets to compare and aggregate them with other annotated patterns. While this is easy to implement and already possible to some extent, it ultimately leads to different tasks which are already supported by complementary tools HiPiler [<xref ref-type="bibr" rid="c33">33</xref>], also available with HiGlass. Other participants requested to feature dynamically changing the color map, resolution, or size of an inset as well as adding and removing annotated patterns for highlighting.</p>
</sec>
</sec>
<sec id="s7">
<label>7</label>
<title>Discussion</title>
<p>We designed Scalable Insets as a guidance technique to improve exploration and navigation of high numbers of annotated patterns for tasks that involve awareness of the patterns&#x2019; detailed visual structure, context, and location. As the first study indicates, there is strong evidence that Scalable Insets support pattern search and comparison of pattern types. The second study found that the choice of placement depends on the importance of overview and context; inner-placement was preferred for contextualizing annotated patterns, while outer-placement was preferred for pattern comparison and gaining an overview.</p>
<sec id="s7a">
<title>Scalability and Limitations</title>
<p>Scalable Insets has been designed for dense multiscale visual spaces such as gigapixel images, maps, and large matrices, where every pixel is associated to some data point. The current prototypical implementation can simultaneously visualize and place about a hundred insets. The number of insets can be improved but it might require additional features such as filtering to cope with the cognitive challenge of too many patterns.</p>
</sec>
<sec id="s7b">
<title>Tradeoff Between Details, Context, and Locality</title>
<p>Scalable Insets set out to provide a tradeoff between D<sc>etail</sc>, C<sc>ontext</sc>, and L<sc>ocality</sc> to manage exploration of high numbers of annotated patterns but to this end the tradeoff is configured upfront by employing sensible defaults for the three use cases presented in this paper. An unsolved question beyond the Scalable Insets technique is what defines a &#x201C;good&#x201D; tradeoff and how could this tradeoff be adjusted interactively during navigation and exploration.</p>
</sec>
<sec id="s7c">
<title>Inset Design and Cluster Representation</title>
<p>The design of the insets content highly depends on the data type and specific tasks. We provide two generic approaches: piling for pattern types of homogeneous shape, such as dots or blocks in matrices, and a gallery view of cluster representatives for pattern types of high variance and diverging shape, such as patterns found in images and geographic maps. As participants in both studies noted, there are further possibilities for application specific cluster representations.</p>
</sec>
<sec id="s7d">
<title>Generalizability</title>
<p>Scalable Insets is not limited to the three multiscale data types we have presented throughout this paper. Our technique can be applied to any kind of multiscale visualization that exhibits a large number of sparsely-distributed patterns. Even monoscale visual spaces that entail a third dimension, such as magnetic resonance imaging, could be enhanced with Scalable Insets. The effectiveness of Scalable Insets depends on how important the detailed visual structure, context, and location of the annotated patterns is. Scalable Insets is designed as a guidance technique with a minimal interaction space to be compatible with a wide range of navigation techniques. Therefore, while our prototypical implementation of Scalable Insets in HiGlass exclusively relies on pan and zoom for navigation, other navigation techniques like PolyZoom [<xref ref-type="bibr" rid="c25">25</xref>] could easily be used as well.</p>
</sec>
</sec>
<sec id="s8">
<label>8</label>
<title>Conclusion and Future Work</title>
<p>Scalable Insets enable guided exploration and navigation of large numbers of sparsely-distributed annotated patterns 2D multiscale visual spaces. Scalable Insets visualizes annotated patterns as magnified thumbnails and dynamically places and aggregates these insets based on location and pattern type. While currently, Scalable Insets supports images, maps, and matrices, we plan for other data types and scenarios, investigate techniques to cope with dense regions of patterns, and support more free-form exploration, e.g., through pinning and manually grouping of insets.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgement</title>
<p>We wish to thank all participants from our two user studies who helped us evaluate Scalable Insets and provided useful feedback. This work was supported in part by the National Institutes of Health (U01 CA200059 and R00 HG007583).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="confproc"><string-name><given-names>L.</given-names> <surname>Barth</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Gemsa</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Niedermann</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>N&#x00F6;llenburg</surname></string-name>. <article-title>On the readability of boundary labeling</article-title>. In <conf-name>International Symposium on Graph Drawing and Network Visualization</conf-name>, pp. <fpage>515</fpage>&#x2013;<lpage>527</lpage>. <conf-sponsor>Springer</conf-sponsor>, <year>2015</year>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="confproc"><string-name><given-names>B. B.</given-names> <surname>Bederson</surname></string-name> and <string-name><given-names>J. D.</given-names> <surname>Hollan</surname></string-name>. <article-title>Pad&#x002B;&#x002B;: a zooming graphical interface for exploring alternate interface physics</article-title>. In <conf-name>Proceedings of the 7th annual ACM symposium on User interface software and technology</conf-name>, pp. <fpage>17</fpage>&#x2013;<lpage>26</lpage>. <conf-sponsor>ACM</conf-sponsor>, <year>1994</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><given-names>M. A.</given-names> <surname>Bekos</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Kaufmann</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Symvonis</surname></string-name>, and <string-name><given-names>A.</given-names> <surname>Wolff</surname></string-name>. <article-title>Boundary labeling: Models and efficient algorithms for rectangular maps</article-title>. <source>Computational Geometry</source>, <volume>36</volume>(<issue>3</issue>):<fpage>215</fpage>&#x2013;<lpage>236</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Bostock</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Ogievetsky</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Heer</surname></string-name>. <article-title>D<sup>3</sup> data-driven documents</article-title>. <source>IEEE transactions on visualization and computer graphics</source>, <volume>17</volume>(<issue>12</issue>):<fpage>2301</fpage>&#x2013;<lpage>2309</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="confproc"><string-name><given-names>M. S. T.</given-names> <surname>Carpendale</surname></string-name> and <string-name><given-names>C.</given-names> <surname>Montagnese</surname></string-name>. <article-title>A framework for unifying presentation space</article-title>. In <conf-name>Proceedings of the 14th annual ACM symposium on User interface software and technology</conf-name>, pp. <fpage>61</fpage>&#x2013;<lpage>70</lpage>. <conf-sponsor>ACM</conf-sponsor>, <year>2001</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Christensen</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Marks</surname></string-name>, and <string-name><given-names>S.</given-names> <surname>Shieber</surname></string-name>. <article-title>An empirical study of algorithms for point-feature label placement</article-title>. <source>ACM Transactions on Graphics (TOG)</source>, <volume>14</volume>(<issue>3</issue>):<fpage>203</fpage>&#x2013;<lpage>232</lpage>, <year>1995</year>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="confproc"><string-name><given-names>C.</given-names> <surname>Collins</surname></string-name>, <string-name><given-names>F. B.</given-names> <surname>Viegas</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Wattenberg</surname></string-name>. <article-title>Parallel tag clouds to explore and analyze faceted text corpora</article-title>. In <conf-name>Visual Analytics Science and Technology, 2009. VAST 2009. IEEE Symposium on</conf-name>, pp. <fpage>91</fpage>&#x2013;<lpage>98</lpage>. <conf-sponsor>IEEE</conf-sponsor>, <year>2009</year>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="website"><string-name><given-names>T. R.</given-names> <surname>de Janeiro</surname></string-name> <article-title>Hong Kong Connection</article-title>. <source>Corcovado</source>, <year>2018</year>. [Online]. Available: <ext-link ext-link-type="uri" xlink:href="http://www.gigapan.com/gigapans/66020">http://www.gigapan.com/gigapans/66020</ext-link> (<date-in-citation>Accessed: 31-March-2018</date-in-citation>).</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Dekker</surname></string-name>, <string-name><given-names>A. S.</given-names> <surname>Belmont</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Guttman</surname></string-name>, <string-name><given-names>V. O.</given-names> <surname>Leshyk</surname></string-name>, <string-name><given-names>J. T.</given-names> <surname>Lis</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Lomvardas</surname></string-name>, <string-name><given-names>L. A.</given-names> <surname>Mirny</surname></string-name>, <string-name><given-names>C. C.</given-names> <surname>O&#x2019;shea</surname></string-name>, <string-name><given-names>P. J.</given-names> <surname>Park</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Ren</surname></string-name>, <collab>and the 4D Nucleome Network</collab>. <article-title>The 4d nucleome project</article-title>. <source>Nature</source>, <volume>549</volume>(<issue>7671</issue>):<fpage>219</fpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="confproc"><string-name><given-names>C.</given-names> <surname>Dunne</surname></string-name> and <string-name><given-names>B.</given-names> <surname>Shneiderman</surname></string-name>. <article-title>Motif simplification: improving network visualization readability with fan, connector, and clique glyphs</article-title>. In <conf-name>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</conf-name>, pp. <fpage>3247</fpage>&#x2013;<lpage>3256</lpage>. <conf-sponsor>ACM</conf-sponsor>, <year>2013</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Edmondson</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Christensen</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Marks</surname></string-name>, and <string-name><given-names>S.</given-names> <surname>Shieber</surname></string-name>. <article-title>A general cartographic labelling algorithm</article-title>. <source>Cartographica: The International Journal for Geographic Information and Geovisualization</source>, <volume>33</volume>(<issue>4</issue>):<fpage>13</fpage>&#x2013;<lpage>24</lpage>, <year>1996</year>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="confproc"><string-name><given-names>N.</given-names> <surname>Elmqvist</surname></string-name>, <string-name><given-names>T.-N.</given-names> <surname>Do</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Goodell</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Henry</surname></string-name>, and <string-name><given-names>J.-D.</given-names> <surname>Fekete</surname></string-name>. <article-title>ZAME: Interactive large-scale graph visualization</article-title>. In <conf-name>Visualization Symposium, 2008. PacificVIS&#x2019;08. IEEE Pacific</conf-name>, pp. <fpage>215</fpage>&#x2013;<lpage>222</lpage>. <conf-sponsor>IEEE</conf-sponsor>, <year>2008</year>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="confproc"><string-name><given-names>N.</given-names> <surname>Elmqvist</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Henry</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Riche</surname></string-name>, and <string-name><given-names>J.-D.</given-names> <surname>Fekete</surname></string-name>. <article-title>Melange: space folding for multi-focus interaction</article-title>. In <conf-name>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</conf-name>, pp. <fpage>1333</fpage>&#x2013;<lpage>1342</lpage>. <conf-sponsor>ACM</conf-sponsor>, <year>2008</year>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Ester</surname></string-name>, <string-name><given-names>H.-P.</given-names> <surname>Kriegel</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Sander</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Xu</surname></string-name>, <etal>et al.</etal> <article-title>A density-based algorithm for discovering clusters in large spatial databases with noise</article-title>. In <source>Kdd</source>, vol. <volume>96</volume>, pp. <fpage>226</fpage>&#x2013;<lpage>231</lpage>, <year>1996</year>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="website"><string-name><given-names>I.</given-names> <surname>Facebook</surname></string-name>. <source>React - a javascript library for building user interfaces</source>, <year>2018</year>. [Online]. Available: <ext-link ext-link-type="uri" xlink:href="https://github.com/facebook/react/">https://github.com/facebook/react/</ext-link> (<date-in-citation>Accessed: 31-March-2018</date-in-citation>).</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="website"><string-name><given-names>D. S.</given-names> <surname>Foundation</surname></string-name> <etal>et al.</etal> <source>Django - the web framework for perfectionists with deadlines</source>, <year>2018</year>. [Online]. Available: <ext-link ext-link-type="uri" xlink:href="https://github.com/django/django">https://github.com/django/django</ext-link> (<date-in-citation>Accessed: 31-March-2018</date-in-citation>).</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="book"><string-name><given-names>G. W.</given-names> <surname>Furnas</surname></string-name>. <source>Generalizedfisheye views</source>, vol. <volume>17</volume>. <publisher-name>ACM</publisher-name>, <year>1986</year>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="confproc"><string-name><given-names>G. W.</given-names> <surname>Furnas</surname></string-name> and <string-name><given-names>B. B.</given-names> <surname>Bederson</surname></string-name>. <article-title>Space-scale diagrams: Understanding multiscale interfaces</article-title>. In <conf-name>Proceedings of the SIGCHI conference on Human factors in computing systems</conf-name>, pp. <fpage>234</fpage>&#x2013;<lpage>241</lpage>. <conf-sponsor>ACM Press/Addison-Wesley Publishing Co.</conf-sponsor>, <year>1995</year>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="book"><string-name><given-names>S.</given-names> <surname>Ghani</surname></string-name>, <string-name><given-names>N. H.</given-names> <surname>Riche</surname></string-name>, and <string-name><given-names>N.</given-names> <surname>Elmqvist</surname></string-name>. <chapter-title>Dynamic insets for context-aware graph navigation</chapter-title>. In <source>Computer Graphics Forum</source>, vol. <volume>30</volume>, pp. <fpage>861</fpage>&#x2013;<lpage>870</lpage>. <publisher-name>Wiley Online Library</publisher-name>, <year>2011</year>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="website"><collab>GigaPan Systems. GigaPan</collab>, <year>2018</year>. [Online]. Available: <ext-link ext-link-type="uri" xlink:href="http://www.gigapan.com">http://www.gigapan.com</ext-link> (<date-in-citation>Accessed: 31-March-2018</date-in-citation>).</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="book"><string-name><given-names>K. W.</given-names> <surname>Hall</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Perin</surname></string-name>, <string-name><given-names>P. G.</given-names> <surname>Kusalik</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Gutwin</surname></string-name>, and <string-name><given-names>S.</given-names> <surname>Carpendale</surname></string-name>. <chapter-title>Formalizing emphasis in information visualization</chapter-title>. In <source>Computer Graphics Forum</source>, vol. <volume>35</volume>, pp. <fpage>717</fpage>&#x2013;<lpage>737</lpage>. <publisher-name>Wiley Online Library</publisher-name>, <year>2016</year>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="book"><string-name><given-names>D.</given-names> <surname>Holten</surname></string-name> and <string-name><given-names>J. J.</given-names> <surname>Van Wijk</surname></string-name>. <chapter-title>Force-directed edge bundling for graph visualization</chapter-title>. In <source>Computer graphics forum</source>, vol. <volume>28</volume>, pp. <fpage>983</fpage>&#x2013;<lpage>990</lpage>. <publisher-name>Wiley Online Library</publisher-name>, <year>2009</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><string-name><given-names>C. Y.</given-names> <surname>Ip</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Varshney</surname></string-name>. <article-title>Saliency-assisted navigation of very large landscape images</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source>, <volume>17</volume>(<issue>12</issue>):<fpage>1737</fpage>&#x2013;<lpage>1746</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="book"><string-name><given-names>S.</given-names> <surname>Ishihara</surname></string-name>. <source>Tests for colour-blindness</source>. <publisher-name>Kanehara Shuppan Company</publisher-name>, <year>1960</year>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="confproc"><string-name><given-names>W.</given-names> <surname>Javed</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Ghani</surname></string-name>, and <string-name><given-names>N.</given-names> <surname>Elmqvist</surname></string-name>. <article-title>Polyzoom: multiscale and multifocus exploration in 2d visual spaces</article-title>. In <conf-name>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</conf-name>, pp. <fpage>287</fpage>&#x2013;<lpage>296</lpage>. <conf-sponsor>ACM</conf-sponsor>, <year>2012</year>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="confproc"><string-name><given-names>S.</given-names> <surname>Jul</surname></string-name> and <string-name><given-names>G. W.</given-names> <surname>Furnas</surname></string-name>. <article-title>Critical zones in desert fog: aids to multiscale navigation</article-title>. In <conf-name>Proceedings of the 11th annual ACM symposium on User interface software and technology</conf-name>, pp. <fpage>97</fpage>&#x2013;<lpage>106</lpage>. <conf-sponsor>ACM</conf-sponsor>, <year>1998</year>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><string-name><given-names>P.</given-names> <surname>Karnick</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Cline</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Jeschke</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Razdan</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Wonka</surname></string-name>. <article-title>Route visualization using detail lenses</article-title>. <source>IEEE transactions on visualization and computer graphics</source>, <volume>16</volume>(<issue>2</issue>):<fpage>235</fpage>&#x2013;<lpage>247</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="website"><string-name><given-names>P.</given-names> <surname>Kerpedjiev</surname></string-name>. <source>Fast contact matrix visualization for the web</source>, <year>2017</year>. [Online]. Available: <ext-link ext-link-type="uri" xlink:href="https://github.com/hms-dbmi/higlass">https://github.com/hms-dbmi/higlass</ext-link> (<date-in-citation>Accessed: 31-March-2018</date-in-citation>).</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><string-name><given-names>P.</given-names> <surname>Kerpedjiev</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Abdennur</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Lekschas</surname></string-name>, <string-name><given-names>C.</given-names> <surname>McCallum</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Dinkla</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Strobelt</surname></string-name>, <string-name><given-names>J. M.</given-names> <surname>Luber</surname></string-name>, <string-name><given-names>S. B.</given-names> <surname>Ouellette</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Ahzir</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Kumar</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Hwang</surname></string-name>, <string-name><given-names>B. H.</given-names> <surname>Alver</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Pfister</surname></string-name>, <string-name><given-names>L. A.</given-names> <surname>Mirny</surname></string-name>, <string-name><given-names>P. J.</given-names> <surname>Park</surname></string-name>, and <string-name><given-names>N.</given-names> <surname>Gehlenborg</surname></string-name>. <article-title>HiGlass: Web-based visual comparison and exploration of genome interaction maps</article-title>. <source>bioRxiv</source>, <year>2017</year>. doi: <pub-id pub-id-type="doi">10.1101/121889</pub-id></mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="website"><string-name><given-names>P.</given-names> <surname>Kerpedjiev</surname></string-name> <etal>et al.</etal> <source>Largest ski areas on each continent</source>, <year>2015</year>. [Online]. Available: <ext-link ext-link-type="uri" xlink:href="http://emptypipes.org/largest-ski-areas">http://emptypipes.org/largest-ski-areas</ext-link> (<date-in-citation>Accessed: 31-March-2018</date-in-citation>).</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="book"><string-name><given-names>H.-K.</given-names> <surname>Kong</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Liu</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Karahalios</surname></string-name>. <chapter-title>Internal and external visual cue preferences for visualizations in presentations</chapter-title>. In <source>Computer Graphics Forum</source>, vol. <volume>36</volume>, pp. <fpage>515</fpage>&#x2013;<lpage>525</lpage>. <publisher-name>Wiley Online Library</publisher-name>, <year>2017</year>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="confproc"><string-name><given-names>B.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Srivastava</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Kumar</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Brafman</surname></string-name>, and <string-name><given-names>S. R.</given-names> <surname>Klemmer</surname></string-name>. <article-title>Designing with interactive example galleries</article-title>. In <conf-name>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</conf-name>, pp. <fpage>2257</fpage>&#x2013;<lpage>2266</lpage>. <conf-sponsor>ACM</conf-sponsor>, <year>2010</year>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><string-name><given-names>F.</given-names> <surname>Lekschas</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Bach</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Kerpedjiev</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Gehlenborg</surname></string-name>, and <string-name><given-names>H.</given-names> <surname>Pfister</surname></string-name>. <article-title>HiPiler: Visual exploration of large genome interaction matrices with interactive small multiples</article-title>. <source>IEEE transactions on visualization and computer graphics</source>, <volume>24</volume>(<issue>1</issue>):<fpage>522</fpage>&#x2013;<lpage>531</lpage>, <year>2018</year>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Lex</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Partl</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Kalkofen</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Streit</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Gratzl</surname></string-name>, <string-name><given-names>A. M.</given-names> <surname>Wassermann</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Schmalstieg</surname></string-name>, and <string-name><given-names>H.</given-names> <surname>Pfister</surname></string-name>. <article-title>Entourage: Visualizing relationships between biological pathways using contextual subsets</article-title>. <source>IEEE transactions on visualization and computer graphics</source>, <volume>19</volume>(<issue>12</issue>):<fpage>2536</fpage>&#x2013;<lpage>2545</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="confproc"><string-name><given-names>J.</given-names> <surname>Liang</surname></string-name> and <string-name><given-names>M. L.</given-names> <surname>Huang</surname></string-name>. <article-title>Highlighting in information visualization: A survey</article-title>. In <conf-name>Information Visualisation (IV), 2010 14th International Conference</conf-name>, pp. <fpage>79</fpage>&#x2013;<lpage>85</lpage>. <conf-sponsor>IEEE</conf-sponsor>, <year>2010</year>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="website"><collab>G. D. Ltd</collab>. <source>The html5 creation engine: Create beautiful digital content with the fastest, most flexible 2d webgl renderer</source>, <year>2018</year>. [Online]. Available: <ext-link ext-link-type="uri" xlink:href="https://github.com/pixijs/pixi.js">https://github.com/pixijs/pixi.js</ext-link> (<date-in-citation>Accessed: 31-March-2018</date-in-citation>).</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="website"><collab>Mapbox, Inc. Mapbox</collab>, <year>2018</year>. [Online]. Available: <ext-link ext-link-type="uri" xlink:href="https://www.mapbox.com">https://www.mapbox.com</ext-link> (<date-in-citation>Accessed: 31-March-2018</date-in-citation>).</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="confproc"><string-name><given-names>J.</given-names> <surname>Marks</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Andalman</surname></string-name>, <string-name><given-names>P. A.</given-names> <surname>Beardsley</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Freeman</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Gibson</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Hodgins</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Kang</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Mirtich</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Pfister</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Ruml</surname></string-name>, <etal>et al.</etal> <article-title>Design galleries: A general approach to setting parameters for computer graphics and animation</article-title>. In <conf-name>Proceedings of the 24th annual conference on Computer graphics and interactive techniques</conf-name>, pp. <fpage>389</fpage>&#x2013;<lpage>400</lpage>. <conf-sponsor>ACM Press/Addison-Wesley Publishing Co.</conf-sponsor>, <year>1997</year>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="book"><string-name><given-names>R. E.</given-names> <surname>Mayer</surname></string-name>. <source>The Cambridge handbook of multimedia learning</source>. <publisher-name>Cambridge university press</publisher-name>, <year>2005</year>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><string-name><given-names>R.</given-names> <surname>Milo</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Shen-Orr</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Itzkovitz</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Kashtan</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Chklovskii</surname></string-name>, and <string-name><given-names>U.</given-names> <surname>Alon</surname></string-name>. <article-title>Network motifs: simple building blocks of complex networks</article-title>. <source>Science</source>, <volume>298</volume>(<issue>5594</issue>):<fpage>824</fpage>&#x2013;<lpage>827</lpage>, <year>2002</year>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="confproc"><string-name><given-names>T.</given-names> <surname>Moscovich</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Chevalier</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Henry</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Pietriga</surname></string-name>, and <string-name><given-names>J.-D.</given-names> <surname>Fekete</surname></string-name>. <article-title>Topology-aware navigation in large networks</article-title>. In <conf-name>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</conf-name>, pp. <fpage>2319</fpage>&#x2013;<lpage>2328</lpage>. <conf-sponsor>ACM</conf-sponsor>, <year>2009</year>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="website"><collab>OpenStreetMap contributors</collab>. <source>Planet dump retrieved from</source>. <ext-link ext-link-type="uri" xlink:href="https://www.openstreetmap.org">https://www.openstreetmap.org</ext-link>, <year>2017</year>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="confproc"><string-name><given-names>K.</given-names> <surname>Perlin</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Fox</surname></string-name>. <article-title>Pad: an alternative approach to the computer interface</article-title>. In <conf-name>Proceedings of the 20th annual conference on Computer graphics and interactive techniques</conf-name>, pp. <fpage>57</fpage>&#x2013;<lpage>64</lpage>. <conf-sponsor>ACM</conf-sponsor>, <year>1993</year>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="book"><string-name><given-names>E.</given-names> <surname>Pietriga</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Del Campo</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Ibsen</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Primet</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Appert</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Chapuis</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Hempel</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Mu&#x00F1;oz</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Eyheramendy</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Jordan</surname></string-name>, <etal>et al.</etal> <chapter-title>Exploratory visualization of astronomical data on ultra-high-resolution wall displays</chapter-title>. In <source>Software and Cyberinfrastructure for Astronomy IV</source>, vol. <volume>9913</volume>, p. <fpage>99130W</fpage>. <publisher-name>International Society for Optics and Photonics</publisher-name>, <year>2016</year>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="confproc"><string-name><given-names>C.</given-names> <surname>Pindat</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Pietriga</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Chapuis</surname></string-name>, and <string-name><given-names>C.</given-names> <surname>Puech</surname></string-name>. <article-title>Jellylens: content-aware adaptive lenses</article-title>. In <conf-name>Proceedings of the 25th annual ACM symposium on User interface software and technology</conf-name>, pp. <fpage>261</fpage>&#x2013;<lpage>270</lpage>. <conf-sponsor>ACM</conf-sponsor>, <year>2012</year>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="confproc"><string-name><given-names>R.</given-names> <surname>Rao</surname></string-name> and <string-name><given-names>S. K.</given-names> <surname>Card</surname></string-name>. <article-title>The table lens: merging graphical and symbolic representations in an interactive focus&#x002B; context visualization for tabular information</article-title>. In <conf-name>Proceedings of the SIGCHI conference on Human factors in computing systems</conf-name>, pp. <fpage>318</fpage>&#x2013;<lpage>322</lpage>. <conf-sponsor>ACM</conf-sponsor>, <year>1994</year>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><string-name><given-names>S. S.</given-names> <surname>Rao</surname></string-name>, <string-name><given-names>M. H.</given-names> <surname>Huntley</surname></string-name>, <string-name><given-names>N. C.</given-names> <surname>Durand</surname></string-name>, <string-name><given-names>E. K.</given-names> <surname>Stamenova</surname></string-name>, <string-name><given-names>I. D.</given-names> <surname>Bochkov</surname></string-name>, <string-name><given-names>J. T.</given-names> <surname>Robinson</surname></string-name>, <string-name><given-names>A. L.</given-names> <surname>Sanborn</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Machol</surname></string-name>, <string-name><given-names>A. D.</given-names> <surname>Omer</surname></string-name>, <string-name><given-names>E. S.</given-names> <surname>Lander</surname></string-name>, <etal>et al.</etal> <article-title>A 3D map of the human genome at kilobase resolution reveals principles of chromatin looping</article-title>. <source>Cell</source>, <volume>159</volume>(<issue>7</issue>):<fpage>1665</fpage>&#x2013;<lpage>1680</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><string-name><given-names>A. C.</given-names> <surname>Robinson</surname></string-name>. <article-title>Highlighting in geovisualization</article-title>. <source>Cartography and Geographic Information Science</source>, <volume>38</volume>(<issue>4</issue>):<fpage>373</fpage>&#x2013;<lpage>383</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="confproc"><string-name><given-names>M.</given-names> <surname>Sarkar</surname></string-name>, <string-name><given-names>S. S.</given-names> <surname>Snibbe</surname></string-name>, <string-name><given-names>O. J.</given-names> <surname>Tversky</surname></string-name>, and <string-name><given-names>S. P.</given-names> <surname>Reiss</surname></string-name>. <article-title>Stretching the rubber sheet: a metaphor for viewing large layouts on small screens</article-title>. In <conf-name>Proceedings of the 6th annual ACM symposium on User interface software and technology</conf-name>, pp. <fpage>81</fpage>&#x2013;<lpage>91</lpage>. <conf-sponsor>ACM</conf-sponsor>, <year>1993</year>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="confproc"><string-name><given-names>P.</given-names> <surname>Siangliulue</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Chan</surname></string-name>, <string-name><given-names>S. P.</given-names> <surname>Dow</surname></string-name>, and <string-name><given-names>K. Z.</given-names> <surname>Gajos</surname></string-name>. <article-title>Ideahound: improving large-scale collaborative ideation with crowd-powered real-time semantic modeling</article-title>. In <conf-name>Proceedings of the 29th Annual Symposium on User Interface Software and Technology</conf-name>, pp. <fpage>609</fpage>&#x2013;<lpage>624</lpage>. <conf-sponsor>ACM</conf-sponsor>, <year>2016</year>.</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><string-name><given-names>J. O.</given-names> <surname>Talton</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Gibson</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Hanrahan</surname></string-name>, and <string-name><given-names>V.</given-names> <surname>Koltun</surname></string-name>. <article-title>Exploratory modeling with collaborative design spaces</article-title>. <source>ACM Transactions on Graphics-TOG</source>, <volume>28</volume>(<issue>5</issue>):<fpage>167</fpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="website"><collab>The Rio de Janeiro - Hong Kong Connection</collab>. <source>The Rio de Janeiro - Hong Kong Connection</source>, <year>2018</year>. [Online]. Available: <ext-link ext-link-type="uri" xlink:href="https://www.visgraf.impa.br/riohk/">https://www.visgraf.impa.br/riohk/</ext-link> (<date-in-citation>Accessed: 31-March-2018</date-in-citation>).</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="book"><string-name><given-names>C.</given-names> <surname>Tominski</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Gladisch</surname></string-name>, <string-name><given-names>U.</given-names> <surname>Kister</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Dachselt</surname></string-name>, and <string-name><given-names>H.</given-names> <surname>Schumann</surname></string-name>. <chapter-title>Interactive lenses for visualization: An extended survey</chapter-title>. In <source>Computer Graphics Forum</source>, vol. <volume>36</volume>, pp. <fpage>173</fpage>&#x2013;<lpage>200</lpage>. <publisher-name>Wiley Online Library</publisher-name>, <year>2017</year>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="book"><string-name><given-names>M.</given-names> <surname>Trapp</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Beesk</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Pasewaldt</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>D&#x00F6;llner</surname></string-name>. <chapter-title>Interactive rendering techniques for highlighting in 3d geovirtual environments</chapter-title>. In <source>Advances in 3D Geo-Information Sciences</source>, pp. <fpage>197</fpage>&#x2013;<lpage>210</lpage>. <publisher-name>Springer</publisher-name>, <year>2011</year>.</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Van den Elzen</surname></string-name> and <string-name><given-names>J. J.</given-names> <surname>Van Wijk</surname></string-name>. <article-title>Multivariate network exploration and presentation: From detail to overview via selections and aggregations</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source>, <volume>20</volume>(<issue>12</issue>):<fpage>2310</fpage>&#x2013;<lpage>2319</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="book"><string-name><given-names>C.</given-names> <surname>Ware</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Lewis</surname></string-name>. <chapter-title>The dragmag image magnifier</chapter-title>. In <source>Conference companion on Human factors in computing systems</source>, pp. <fpage>407</fpage>&#x2013;<lpage>408</lpage>. <publisher-name>ACM</publisher-name>, <year>1995</year>.</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="journal"><string-name><given-names>W.</given-names> <surname>Willett</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Heer</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Agrawala</surname></string-name>. <article-title>Scented widgets: Improving navigation cues with embedded visualizations</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source>, <volume>13</volume>(<issue>6</issue>):<fpage>1129</fpage>&#x2013;<lpage>136</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c58"><label>[58]</label><mixed-citation publication-type="confproc"><string-name><given-names>J.</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Wigdor</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Balakrishnan</surname></string-name>. <article-title>Trailmap: facilitating information seeking in a multi-scale digital map via implicit bookmarking</article-title>. In <conf-name>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</conf-name>, pp. <fpage>3009</fpage>&#x2013;<lpage>3018</lpage>. <conf-sponsor>ACM</conf-sponsor>, <year>2013</year>.</mixed-citation></ref>
</ref-list>
<fn-group>
<fn id="fn1"><label><sup>1</sup></label><p>Gigapan IDs: 149705, 40280, 48635, 47373, 33411, 72802, and 66020</p></fn>
</fn-group>
</back>
</article>
