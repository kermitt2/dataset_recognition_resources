<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/211128</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Unsupervised discovery of demixed, low-dimensional neural dynamics across multiple timescales through tensor components analysis</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Williams</surname><given-names>Alex H.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Kim</surname><given-names>Tony Hyun</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Wang</surname><given-names>Forea</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Vyas</surname><given-names>Saurabh</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Ryu</surname><given-names>Stephen I.</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a11">11</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Shenoy</surname><given-names>Krishna V.</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="aff" rid="a7">7</xref>
<xref ref-type="aff" rid="a8">8</xref>
<xref ref-type="aff" rid="a9">9</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Schnitzer</surname><given-names>Mark</given-names></name>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a7">7</xref>
<xref ref-type="aff" rid="a9">9</xref>
<xref ref-type="aff" rid="a10">10</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Kolda</surname><given-names>Tamara G.</given-names></name>
<xref ref-type="aff" rid="a12">12</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Ganguli</surname><given-names>Surya</given-names></name>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="aff" rid="a7">7</xref>
<xref ref-type="aff" rid="a8">8</xref>
<xref ref-type="author-notes" rid="n2">&#x2020;</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Neurosciences Graduate Program</institution>, Stanford, CA 94305, <country>USA</country>.</aff>
<aff id="a2"><label>2</label><institution>Electrical Engineering Department</institution>, Stanford, CA 94305, <country>USA</country>.</aff>
<aff id="a3"><label>3</label><institution>Bioengineering Department</institution>, Stanford, CA 94305, <country>USA</country>.</aff>
<aff id="a4"><label>4</label><institution>Applied Physics Department</institution>, Stanford, CA 94305, <country>USA</country>.</aff>
<aff id="a5"><label>5</label><institution>Biology Department</institution>, Stanford, CA 94305, <country>USA</country>.</aff>
<aff id="a6"><label>6</label><institution>Neurobiology Department</institution>, Stanford, CA 94305, <country>USA</country>.</aff>
<aff id="a7"><label>7</label><institution>Bio-X Program</institution>, Stanford, CA 94305, <country>USA</country>.</aff>
<aff id="a8"><label>8</label><institution>Stanford Neurosciences Institute</institution>, Stanford, CA 94305, <country>USA</country>.</aff>
<aff id="a9"><label>9</label><institution>Howard Hughes Medical Institute</institution>, Stanford, CA 94305, <country>USA</country>.</aff>
<aff id="a10"><label>10</label><institution><italic>CNC</italic> Program, Stanford University</institution>, Stanford, CA 94305, <country>USA</country>.</aff>
<aff id="a11"><label>11</label><institution>Department of Neurosurgery, Palo Alto Medical Foundation</institution>, Palo Alto, CA 94301, <country>USA</country>.</aff>
<aff id="a12"><label>12</label><institution>Sandia National Laboratories</institution>, Livermore, CA 94551, <country>USA</country></aff>
</contrib-group>
<author-notes>
<fn id="n1"><label>&#x002A;</label><p><email>ahwillia@stanford.edu</email></p></fn>
<fn id="n2"><label>&#x2020;</label><p><email>sganguli@stanford.edu</email></p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2017</year>
</pub-date>
<elocation-id>211128</elocation-id>
<history>
<date date-type="received">
<day>29</day>
<month>10</month>
<year>2017</year>
</date>
<date date-type="rev-recd">
<day>29</day>
<month>10</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>30</day>
<month>10</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="211128.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Perceptions, thoughts and actions unfold over millisecond timescales, while learned behaviors can require many days to mature. While recent experimental advances enable large-scale and long-term neural recordings with high temporal fidelity, it remains a formidable challenge to extract unbiased and interpretable descriptions of how rapid single-trial circuit dynamics change slowly over many trials to mediate learning. We demonstrate a simple tensor components analysis (TCA) can meet this challenge by extracting three interconnected low dimensional descriptions of neural data: <italic>neuron factors</italic>, reflecting cell assemblies; <italic>temporal factors</italic>, reflecting rapid circuit dynamics mediating perceptions, thoughts, and actions within each trial; and <italic>trial factors</italic>, describing both long-term learning and trial-to-trial changes in cognitive state. We demonstrate the broad applicability of TCA by revealing insights into diverse datasets derived from artificial neural networks, large-scale calcium imaging of rodent prefrontal cortex during maze navigation, and multielectrode recordings of macaque motor cortex during brain machine interface learning.</p>
</abstract>
<counts>
<page-count count="38"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Two of the most challenging obstacles to understanding neural circuits are their diversity of dynamical timescales and the large number of neurons that contribute to their function. For instance, circuit dynamics mediating sensory perception, decision-making, attentional shifting, motor control, and higher cognition unfold over hundreds of milliseconds, while slower processes like motivation, long-term planning, and learning vary slowly, sometimes taking days or weeks to fully manifest [<xref ref-type="bibr" rid="c1">1</xref>&#x2013;<xref ref-type="bibr" rid="c3">3</xref>]. Moreover, every execution of a behavior can involve the coordinated activity of extremely large neural populations, often distributed across multiple brain regions.</p>
<p>Recent experimental advances enable us to monitor all aspects of this biological complexity by recording large numbers of neurons [<xref ref-type="bibr" rid="c4">4</xref>&#x2013;<xref ref-type="bibr" rid="c7">7</xref>] at high temporal precision [<xref ref-type="bibr" rid="c8">8</xref>] over long durations [<xref ref-type="bibr" rid="c9">9</xref>&#x2013;<xref ref-type="bibr" rid="c11">11</xref>]. The resulting datasets can contain thousands of neural activity traces collected over thousands of behavioral trials. The genesis of such complex, large scale datasets now present a major data-analytic challenge to the field of neuroscience. Namely, how can we develop general purpose algorithms to extract from such complex data, simple and interpretable descriptions of collective circuit dynamics that underly not only rapid sensory, motor and cognitive acts, but also describe slower signatures of long-term planning and learning? Moreover, how can these algorithms operate in an unsupervised manner, to enable the discovery of novel and unexpected cognitive states that can vary on a trial by trial basis?</p>
<p>Neuroscientists have often turned to unbiased dimensionality reduction methods to understand these complex datasets [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>]. However, commonly used methods focus on reducing the complexity of fast, within-trial firing rate dynamics instead of extracting interpretable slow, across-trial structure. A common approach is to average neural activity across trials [<xref ref-type="bibr" rid="c13">13</xref>&#x2013;<xref ref-type="bibr" rid="c15">15</xref>], thereby precluding the possibility of understanding of how cognition and behavior change on a trial by trial basis. More recent methods, including Gaussian Process Factor Analysis (GPFA) [<xref ref-type="bibr" rid="c16">16</xref>] and latent dynamical system models [<xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c18">18</xref>], identify low-dimensional firing rate trajectories <italic>within</italic> each trial, but do not reduce the dimensionality <italic>across</italic> trials by extracting analogous low-dimensional trajectories over trials. Other works have separately focused on trial-to-trial variability in neural responses [<xref ref-type="bibr" rid="c19">19</xref>&#x2013;<xref ref-type="bibr" rid="c22">22</xref>], and long-term trends across many trials [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c23">23</xref>&#x2013;<xref ref-type="bibr" rid="c26">26</xref>], but without an explicit focus on obtaining simple low-dimensional descriptions. Thus, while current experimental data can simultaneously capture neural dynamics underlying both fast cognitive processes as well as slower learning processes, we lack general-purpose methods for extracting unbiased descriptions of both fast cognition and slower learning.</p>
<p>The most common and fundamental method for dimensionality reduction of neural data is Principal Components Analysis (PCA) [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>]. Here, we explore a simple extension of PCA that enables multi-timescale dimensionality reduction of neural dynamics both within trials and across trials. The key idea is to organize neural firing rates into a third-order tensor (i.e., a three-dimensional data table) with three axes corresponding to individual neurons (index 1), time within trial (index 2), and trial number (index 3). We then fit a tensor decomposition model (CANDECOMP/PARAFAC) [<xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>] to identify a set of low-dimensional components describing variability along each of these three axes. We refer to this procedure as Tensor Components Analysis (TCA).</p>
<p>We demonstrate that TCA yields insightful descriptions of a variety of neural datasets. In particular, it enables us to move beyond trial averaging by simultaneously identifying separate low-dimensional features for rapid within-trial neural dynamics and slower across-trial neural dynamics. Furthermore, as described below, TCA possesses a set of favorable theoretical properties that translate into significant interpretational advantages when applied to neural data. In particular, the components returned by TCA are often unique [<xref ref-type="bibr" rid="c29">29</xref>], unlike PCA which requires a biologically unrealistic orthogonality constraint to yield unique components. Because of the uniqueness of TCA, it achieves a demixing of neural data in which <italic>individual</italic> components are often in one-to-one correspondence with biologically interpretable variables. For example, as we see below, in diverse datasets, individual components correspond to sensations, decisions, actions, rewards and performance.</p>
<p>Below, after introducing the method, we show that TCA is equivalent to a form of multi-dimensional gain control and so can be interpreted as a generalization of a well-studied model of cortical function [<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c31">31</xref>]. We then give three examples of its utility. First, in an artificial neural circuit trained to solve the well-studied motion discrimination task [<xref ref-type="bibr" rid="c32">32</xref>], we show that TCA yields a simple one-dimensional description of the evolving connectivity and dynamics of the circuit during learning. Next, in a maze navigation task in rodents, we show that TCA can recover several aspects of trial structure and behavior, including perceptions, decisions, rewards, and errors, in an unsupervised, data-driven fashion. Finally, for a monkey operating a brain machine interface (BMI), we show that TCA extracts a simple view of motor learning when the BMI is altered to change the relationship between neural activity and motor action.</p>
<p>Thus, this work introduces a simple and broadly applicable method for identifying interpretable structure in multi-trial neural data, thereby providing a way to attack two of the most challenging aspects of modern large-scale neural recordings: their multi-timescale nature, and their high dimensionality. While TCA is a general-purpose method [<xref ref-type="bibr" rid="c33">33</xref>], we provide specialized code and step-by-step instructions for applying TCA to neural data, and describe how to interpret the outcomes of TCA within the context of systems neuroscience.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Results</title>
<sec id="s2a">
<label>2.1.</label>
<title>Discovering multi-timescale structure through TCA</title>
<p>Before describing TCA, we first review the application of PCA for analyzing large-scale recordings. Consider a recording of <italic>N</italic> neurons over <italic>K</italic> experimental trials. We assume neural activity is recorded at <italic>T</italic> timepoints within each trial, but trials of variable duration can be aligned or temporally warped to accommodate this constraint (see, e.g., [<xref ref-type="bibr" rid="c34">34</xref>]). This dataset is naturally represented as an <italic>N &#x00D7; T &#x00D7; K</italic> array of firing rates, which is known in mathematics as a third-order tensor. Each element in this tensor, <italic>x</italic><sub><italic>ntk</italic></sub>, denotes the firing rate of neuron <italic>n</italic> at time <italic>t</italic> within trial <italic>k</italic>. Here, the index <italic>n</italic> ranges from 1 to <italic>N</italic>, <italic>t</italic> ranges from 1 to <italic>T</italic>, and <italic>k</italic> ranges from 1 to <italic>K</italic>.</p>
<p>These datasets are very challenging to interpret in their raw format. Even nominally identical trials (e.g., neural responses elicited by repeats of an identical sensory stimulus) can exhibit significant trial-to-trial variability [<xref ref-type="bibr" rid="c22">22</xref>]. Under the assumption that such variability is simply irrelevant noise, a common method to simplify the table is to average across trials, obtaining a two dimensional table, or matrix, <inline-formula><alternatives><inline-graphic xlink:href="211128_inline1.gif"/></alternatives></inline-formula>, which holds the trial-averaged neural firing rates for every neuron <italic>n</italic> and timepoint <italic>t</italic> (<xref rid="fig1" ref-type="fig">fig. 1a</xref>). Even such a matrix can be difficult to understand in large-scale experiments containing many neurons and rich temporal dynamics. PCA summarizes these data by performing a decomposition into <italic>R</italic> components such that</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig 1.</label>
<caption><title>Tensor representation of trial-structured neural data.</title>
<p><bold>(a)</bold> Schematic of trial-averaged PCA for spiking data. The raw data is represented as a sequence of <italic>N &#x00D7; T</italic> matrices (top). These matrices are averaged across trials to build a matrix representation of neural firing rates. PCA approximates the trial-averaged matrix as a sum of outer products of vectors (see eq. (1)). Each outer product contains a neuron factor (blue rectangles) and a temporal factor (red rectangles). <bold>(b)</bold> Schematic of trial-concatenated PCA for spiking data. Raw data are temporally smoothed by a Gaussian filter to estimate neural firing rates before concatenating all trials along the time axis. Applying PCA produces a separate set of temporal factors for each trial (subsets of the red vectors). <bold>(c)</bold> Schematic of TCA. Raw data are smoothed and collected into a third order tensor with dimensions <italic>N &#x00D7; T &#x00D7; K</italic>. TCA approximates the data as a sum of outer products of three vectors, producing a third set of low-dimensional factors (trial factors, green vectors) that describe how activity changes across trials.</p></caption>
<graphic xlink:href="211128_fig1.tif"/>
</fig>
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="211128_eqn1.gif"/></alternatives>
</disp-formula>
<p>This decomposition projects the high-dimensional data (with <italic>N</italic> or <italic>T</italic> dimensions) into a low-dimensional space (with <italic>R</italic> dimensions). Each component, indexed by <italic>r</italic>, contains a coefficient across neurons, <inline-formula><alternatives><inline-graphic xlink:href="211128_inline2.gif"/></alternatives></inline-formula>, and a coefficient across timepoints, <inline-formula><alternatives><inline-graphic xlink:href="211128_inline3.gif"/></alternatives></inline-formula>. These terms can be collected into vectors: <bold>w</bold><sup><italic>r</italic></sup>, of length <italic>N</italic>, which we call <italic>neuron factors</italic> (blue vectors in <xref rid="fig1" ref-type="fig">fig. 1</xref>), and <bold>b</bold><sup><italic>r</italic></sup>, of length <italic>T</italic>, which we call <italic>temporal factors</italic> (red vectors in <xref rid="fig1" ref-type="fig">fig. 1</xref>). The neuron factors can be thought of as an ensemble of cells that exhibit correlated firing. The temporal factors can be thought of as a trial-averaged dynamical activity pattern for each ensemble. Overall, this <italic>trial-averaged PCA</italic> procedure reduces the original <italic>N &#x00D7; T &#x00D7; K</italic> datapoints into <italic>R</italic>(<italic>N</italic> &#x002B; <italic>T</italic>) values, yielding a compact, and often insightful summary of the trial-averaged data [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>].</p>
<p>However, trial-averaging is motivated by the assumption that trial-to-trial variability is irrelevant noise, which is often at odds with our understanding of neural circuits and questions of experimental interest. For instance, even under repeated sensory stimuli, trial-to-trial variability may reflect fluctuations in interesting cognitive states, like attention or arousal [<xref ref-type="bibr" rid="c20">20</xref>, <xref ref-type="bibr" rid="c21">21</xref>]. Also, under situations in which animals are learning a task, there will be systematic changes in neural dynamics over many trials, which would be rendered invisible by trial averaging. Intriguingly, as the field moves to study more complex tasks, we may find completely unexpected structured variability across trials, corresponding to different internal brain states on different trials. Ideally, we would like unbiased, data-driven methods to extract such dynamics simply by analyzing the data tensor.</p>
<p>One approach to retain the variability across trials is to concatenate multiple trials rather than averaging, thereby transforming the data tensor into an <italic>N &#x00D7;TK</italic> matrix, and then applying PCA to this matrix (<xref rid="fig1" ref-type="fig">fig. 1b</xref>). This approach, which we call <italic>trial-concatenated PCA</italic>, is similar to Gaussian Process Factor Analysis (GPFA) [<xref ref-type="bibr" rid="c16">16</xref>], another specialized technique for neural data analysis. In trial-concatenated PCA, the <italic>R</italic> temporal factors are of length <italic>TK</italic> and do not enforce any commonality across trials. It therefore achieves a less significant reduction in the complexity of the data: the <italic>NTK</italic> numbers in the original data tensor are only reduced to <italic>R</italic>(<italic>N</italic> &#x002B; <italic>TK</italic>) numbers, which can be cumbersome in experiments consisting of thousands of trials.</p>
<p>Our proposal is to directly deal with neural data in its natural third-order tensor format by performing a dimensionality reduction of this tensor (<xref rid="fig1" ref-type="fig">fig. 1c</xref>), rather than first converting it to a matrix. This tensor components analysis (TCA) method then yields the <italic>R</italic>-component decomposition [<xref ref-type="bibr" rid="c27">27</xref>, <xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c33">33</xref>]</p>
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="211128_eqn2.gif"/></alternatives>
</disp-formula>
<p>In analogy to PCA, we can think of <bold>w</bold><sup><italic>r</italic></sup> as a prototypical firing rate pattern across neurons, and we can think of <bold>b</bold><sup><italic>r</italic></sup> as a temporal basis function across time within trials. These neuron factors and temporal factors constitute structure that is common across all trials. We call the third set of factors, <bold>a</bold><sup><italic>r</italic></sup>, <italic>trial factors</italic> (green vectors in <xref rid="fig1" ref-type="fig">fig. 1</xref>), which are new to TCA and not present in PCA. The trial factors can be thought of as trial-specific amplitudes for the within-trial activity patterns identified by the neuron and temporal factors. Thus, in TCA, the trial-to-trial fluctuations in neural activity are also embeded in <italic>R</italic>-dimensional space. TCA achieves a dramatic reduction of the original data tensor, reducing <italic>NTK</italic> datapoints to <italic>R</italic>(<italic>N</italic> &#x002B; <italic>T</italic> &#x002B; <italic>K</italic>) values, while still capturing trial-to-trial variability.</p>
<p>A subtle, but critical, difference between PCA and TCA is the uniqueness of the identified factors. In order to obtain unique factors, PCA constrains both the neuron and temporal factors to be orthogonal sets of vectors. This assumption is motivated by mathematical convenience rather than scientific principles. In real biological circuits, cell ensembles may overlap and temporal firing patterns may be correlated, producing non-orthogonal structure that is missed by PCA. In contrast, the TCA model often has a unique solution without further assumptions [<xref ref-type="bibr" rid="c29">29</xref>]. As we demonstrate below, TCA tends to extract non-orthogonal features of data that are more interpretable and meaningful than those extracted by PCA. In particular, we will see that TCA not only performs dimensionality reduction, but also demixing, by learning individual components that are in one-to-one correspondence with biologically interpretable variables.</p>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>TCA as a generalized cortical gain control model</title>
<p>Although TCA was originally developed as a statistical method [<xref ref-type="bibr" rid="c33">33</xref>], here we show that it concretely relates to a prominent theory of neural computation when applied to multi-trial datasets. In particular, performing TCA on neural data is equivalent to fitting a gain-modulated linear network model. In this network, <italic>N</italic> observed neurons (light gray circles, <xref rid="fig2" ref-type="fig">fig. 2a</xref>) are driven by a much smaller number of <italic>R</italic> unobserved, or latent, inputs (dark gray circles, <xref rid="fig2" ref-type="fig">fig. 2a</xref>) that have a fixed temporal profile but have varying amplitudes for each trial. The neuron factors of TCA, <inline-formula><alternatives><inline-graphic xlink:href="211128_inline4.gif"/></alternatives></inline-formula> in eq. (2), correspond to the synaptic <italic>weights</italic> from each latent input <italic>r</italic> to each neuron <italic>n</italic>. The temporal factors of TCA, <inline-formula><alternatives><inline-graphic xlink:href="211128_inline5.gif"/></alternatives></inline-formula>, correspond to <italic>basis functions</italic> or the activity of input <italic>r</italic> at time <italic>t</italic>. Finally, the trial factors of TCA, <inline-formula><alternatives><inline-graphic xlink:href="211128_inline6.gif"/></alternatives></inline-formula> correspond to <italic>amplitudes</italic>, or gain, of latent input <italic>r</italic> on trial <italic>k</italic>. Such trial-to-trial fluctuations in amplitude have been observed in a variety of sensory systems [<xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c35">35</xref>&#x2013;<xref ref-type="bibr" rid="c37">37</xref>], and are believed to be an important and ubiquitous feature of cortical circuits [<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c31">31</xref>]. Furthermore, plausible cellular mechanisms for gain modulation have been examined by a number of experimental and computational studies [<xref ref-type="bibr" rid="c38">38</xref>&#x2013;<xref ref-type="bibr" rid="c41">41</xref>]. The TCA model can be viewed as a higher, <italic>R</italic>-dimensional generalization of such theories. By allowing an <italic>R</italic>-dimensional space of possible gain modulations to different temporal factors, TCA can capture a rich diversity of changing multi-neuronal activity patterns across trials.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig 2.</label>
<caption><title>TCA precisely recovers the parameters of a linear model network.</title>
<p><bold>(a)</bold> Schematic of model network. Three input signals (dark gray) were delivered to a 1-layer, linear neural network with <italic>N</italic> &#x003D; 50 neurons (light gray). Gaussian noise was added to the output units. <bold>(b)</bold> Simulated activity of all neurons on example trials. <bold>(c)</bold> The factors identified by 3-component TCA precisely match the network parameters. <bold>(d-e)</bold> Applying PCA (in <bold>d</bold>) or ICA (in <bold>e</bold>) to each of the tensor unfoldings does not recover the network parameters. <bold>(f)</bold> Analysis pipeline for TCA. <bold>(f, inset 1)</bold> Error plots showing normalized reconstruction error (vertical axis) for TCA models with different numbers of components (horizontal axis). The red line tracks the minimum error (i.e., best-fit model). Each black dot denotes a model fit from different initial parameters. All models fit from different initializations had essentially identical performance. Reconstruction error did not improve after more than 3 components were included. <bold>(f, inset 2)</bold> Similarity plot showing similarity score (eq. (12); vertical axis) for TCA models with different numbers of components (horizontal axis). Similarity for each model (black dot) is computed with respect to the best-fit model with the same number of components. The red line tracks the mean similarity as a function of the number of components. Adding more than 3 components caused models to be less reliably identified.</p></caption>
<graphic xlink:href="211128_fig2.tif"/>
</fig>
<p>An important feature of TCA is that these network parameters can often be unambiguously identified from simulated data alone, due to the previously mentioned uniqueness property of TCA [<xref ref-type="bibr" rid="c29">29</xref>]. We confirmed this in a simple simulation with three latent inputs/components. In this example, the first component grows in amplitude across trials, the second component shrinks, and the third component grows and then shrinks in amplitude (<xref rid="fig2" ref-type="fig">fig. 2a</xref>). This model generates rich simulated population activity patterns across neurons, time, and trials as shown in (<xref rid="fig2" ref-type="fig">fig. 2b</xref>), where we have added Gaussian white noise to demonstrate the robustness of the method. When applied to noisy multi-neuronal traces, TCA with <italic>R</italic> &#x003D; 3 components precisely extracted the network parameters (<xref rid="fig2" ref-type="fig">fig. 2c</xref>).</p>
<p>In contrast, neither PCA nor independent components analysis (ICA) [<xref ref-type="bibr" rid="c42">42</xref>] can recover the network parameters, as demonstrated in <xref rid="fig2" ref-type="fig">fig. 2d</xref> and <xref rid="fig2" ref-type="fig">fig. 2e</xref> respectively. Unlike TCA, both PCA and ICA are fundamentally matrix, not tensor, decomposition methods. Therefore they cannot be applied directly to the data tensor, but instead must be applied to three different matrices obtained by <italic>tensor unfolding</italic> (<xref rid="fig2" ref-type="fig">fig. 2</xref> supp. 1; [<xref ref-type="bibr" rid="c33">33</xref>]). In essence, the unfolding procedure generalizes the trial-concatenated representation of the data tensor (<xref rid="fig1" ref-type="fig">fig. 1b</xref>) to allow concatenation across neurons or timepoints. This unfolding destroys natural structure across neurons, time, and trials in the third-order data tensor, thereby precluding the possibility of finding the ground truth synaptic weights, temporal basis functions, and trial amplitudes that actually generated observed neural activity patterns.</p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Choosing the number of components</title>
<p>A schematic view of the process of applying TCA to neural data is shown in <xref rid="fig2" ref-type="fig">fig. 2f</xref> (see <italic>Methods</italic> for more details). As in PCA and many other dimensionality reduction methods, a critical issue is the choice of the number of components, or dimensions <italic>R</italic>. We employ two methods to inform this choice. First, we inspect an <italic>error plot</italic> (<xref rid="fig2" ref-type="fig">fig. 2f</xref>, inset), which displays the model reconstruction error as a function of the number of components <italic>R</italic>. We normalize the reconstruction error to range between zero and one as described in <xref ref-type="sec" rid="s4e1">section 4.5.1</xref>. This provides a metric analogous to the fraction of unexplained variance, which is used in PCA. As in PCA, a kink or leveling out in this plot indicates a point of diminishing returns for including more components. Unlike PCA, we run the optimization algorithm underlying TCA at each value of <italic>R</italic> multiple times from random initial conditions, and plot the normalized reconstruction error for all such models. Such repeated optimization runs enable us to check whether some runs converge to suboptimal solutions with high reconstruction error. As shown in (<xref rid="fig2" ref-type="fig">fig. 2f</xref>, inset), the error plot reveals that all runs at fixed <italic>R</italic> yield the same error, and moreover, the kink in the plot unambiguously reveals <italic>R</italic> &#x003D; 3 as the true number of components in the generated data, in agreement with the ground truth.</p>
<p>A second method to assess the number of components involves generating a <italic>similarity plot</italic> (<xref rid="fig2" ref-type="fig">fig. 2f</xref>, inset), which displays how sensitive the recovered factors are to the initialization of the optimization procedure underlying TCA. For each component, we compute the similarity of all fitted models to the model with lowest reconstruction error by a similarity score bounded between zero (orthogonal factors) and one (identical factors). See <xref ref-type="sec" rid="s4e1">section 4.5.1</xref> for more details. Adding more components to the model can produce lower similarity scores, which complicates exploratory analysis since multiple low-dimensional descriptions may be consistent with the data. Like the error plot, the similarity plot unambiguously reveals <italic>R</italic> &#x003D; 3 as the correct number of components, as decompositions with <italic>R &#x003E;</italic> 3 are less consistent with each other (<xref rid="fig2" ref-type="fig">fig. 2f</xref>, inset). Notably, all models with <italic>R</italic> &#x003D; 3 converge to <italic>identical</italic> components (up to permutations and re-scalings of factors), suggesting that only a single low-dimensional description, corresponding to the ground truth network parameters, achieves minimal reconstruction error. TCA consistently identifies this solution across multiple optimization runs.</p>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>TCA elucidates learning dynamics, circuit connectivity and computational mechanism in a nonlinear network</title>
<p>While TCA corresponds to a linear gain-modulated network, it can nevertheless reveal insights into the operation of more complex nonlinear networks, analogous to how PCA, a linear dimensionality reduction technique, allows visualization of low-dimensional nonlinear neural trajectories [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c13">13</xref>]. We examine the application of TCA to nonlinear recurrent neural networks (RNNs), a powerful class of models that can learn to approximate any dynamical system [<xref ref-type="bibr" rid="c43">43</xref>]. RNNs have achieved success both in machine learning applications [<xref ref-type="bibr" rid="c44">44</xref>] and in modeling neural dynamics and behavior [<xref ref-type="bibr" rid="c45">45</xref>&#x2013;<xref ref-type="bibr" rid="c48">48</xref>]. However, such models are so complex that they are often viewed as &#x201C;black boxes.&#x201D; Statistical methods that shed light on the function of RNNs and other complex computational models are therefore of great interest [<xref ref-type="bibr" rid="c49">49</xref>, <xref ref-type="bibr" rid="c50">50</xref>]. Notably, while previous studies have focused on reverse-engineering RNNs with static parameters [<xref ref-type="bibr" rid="c51">51</xref>], few works have attempted to characterize how computational mechanisms in RNNs emerge over the process of learning, or optimization, of network parameters. Here we show TCA can naturally yield such a characterization for an RNN that learns to solve a simple sensory discrimination task, analogous to the well-known random dots direction-discrimination task [<xref ref-type="bibr" rid="c32">32</xref>].</p>
<p>Specifically, we trained an RNN with 50 neurons to estimate whether a noisy input signal had net positive or negative activity over a short time window, and indicate this estimate by exciting or inhibiting an output neuron (<xref rid="fig3" ref-type="fig">fig. 3a</xref>). We call trials with a net positive input <italic>(&#x002B;)-trials</italic> and trials with a net negative input <italic>(-)-trials</italic>. The average amplitude of the input can be viewed as a proxy for the average motion energy of moving dots along a directional axis, with &#x002B;/-corresponding to left/right, for example. The synaptic weights were updated by a simple gradient descent rule using backpropagation through time on a logistic loss function [<xref ref-type="bibr" rid="c52">52</xref>]. Within 750 trials the network performed the task with virtually 100&#x0025; accuracy (<xref rid="fig3" ref-type="fig">fig. 3b</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig 3.</label>
<caption><title>Unsupervised discovery of low-dimensional learning dynamics and mechanism in an model RNN.</title>
<p><bold>(a)</bold> Model schematic. A noisy input signal is broadcast to a recurrent population of neurons with all-to-all connectivity (yellow oval). On <italic>(&#x002B;)-trials</italic> the input is net positive (black traces), while on <italic>(-)-trials</italic> the input is net negative (red traces). The network is trained to output the sign of the input signal with a large magnitude. <bold>(b)</bold> Learning curve for the model, showing the objective value on each trial over learning. <bold>(c)</bold> Scree plot showing the improvement in normalized reconstruction error as more components are added to the model. <bold>(d)</bold> An example <italic>(&#x002B;)-cell</italic> and <italic>(-)-cell</italic> before and after training on both trial types. Black traces indicate <italic>(&#x002B;)-trials</italic>, and red traces indicate <italic>(-)-trials</italic>. <bold>(e)</bold> Factors discovered by a one-component TCA applied simulated neuron activity over training. The neuron factor identifies <italic>(&#x002B;)-cells</italic> (black bars) and <italic>(-)-cells</italic> (red bars), which have opposing correlations with the input signal. These two populations naturally exist in a randomly initialized network (trial 0), but become separated after during training, as described by the trial factor. <bold>(f)</bold> The neuron factor identified by TCA closely matches the principal eigenvector of the synaptic connectivity matrix post-learning. <bold>(g)</bold> The recurrent synaptic connectivity matrix post-learning. Resorting the neurons by their order in the neuron factor in <bold>(e)</bold> uncovers competitive connectivity between the <italic>(&#x002B;)-cells</italic> and <italic>(-)-cells</italic>. <bold>(h)</bold> Simplified diagram of the learned mechanism for this network.</p></caption>
<graphic xlink:href="211128_fig3.tif"/>
</fig>
<p>Remarkably, TCA needed only a single component to capture <italic>both</italic> the within-trial multi-neuronal circuit dynamics of decision making <italic>and</italic> the across-trial dynamics of learning. Adding more components led to negligible improvements in reconstruction error (<xref rid="fig3" ref-type="fig">fig. 3c</xref>). A single-component TCA model makes two strong predictions about this dataset. First, within all trials, the time course of evidence integration is shared across all neurons and is not substantially effected by training. Second, across trials, the amplitude of single cell responses are simply scaled by a common factor during learning. In essence, prior to learning, all cells have some small, random preference for one of the two input types, and learning corresponds to simply amplifying these initial tunings. We visually confirmed this prediction by examining single trial responses of individual cells. We observed two cell types within this model network: <italic>(&#x002B;)-cells</italic> which were excited on (&#x002B;)-trials and inhibited on (-)-trials (<xref rid="fig3" ref-type="fig">fig. 3d</xref>, left), and <italic>(-)-cells</italic> which were excited on (-)-trials and inhibited on (&#x002B;)-trials (<xref rid="fig3" ref-type="fig">fig. 3d</xref>, right). The response amplitudes of both cell types magnified over learning, and typically the initial tuning (pale lines) aligned with the final tuning (dark lines). These trends are verified across the full population of cells in <xref rid="fig3" ref-type="fig">fig. 3</xref> Supplement 1a-b.</p>
<p>We then visualized the three factors of the single-component TCA model (<xref rid="fig3" ref-type="fig">fig. 3e</xref>). We sorted the cells by their weight in the neuron factor, and plotted this factor, <bold>w</bold><sup>1</sup>, as a bar plot (<xref rid="fig3" ref-type="fig">fig. 3e</xref>; left). Neurons with a positive weight are precisely the (&#x002B;)-cells (black bars) defined earlier, while neurons with a negative weight were (-)-cells (red bars). While it is conceptually helpful to discretely categorize cells, the neuron factor illustrates that the model cells actually fall along a continuous spectrum rather than two discrete groups. The temporal basis function extracted by TCA, <bold>b</bold><sup>1</sup>, reveals a common dynamical pattern within all trials corresponding to integration to a bound (<xref rid="fig3" ref-type="fig">fig. 3e</xref>; middle), similar to the example cells shown in <xref rid="fig3" ref-type="fig">Figure 3d</xref>. Finally, the trial factor of TCA, <bold>a</bold><sup>1</sup>, recovered two important aspects of the neural dynamics (<xref rid="fig3" ref-type="fig">fig. 3e</xref>; right). First, the trial amplitude is positive for (&#x002B;)-trials (black points) and negative for (-)-trials (red points), thereby providing a direct readout of the input on each trial. Second, over the course of learning, these two trial types become more separated, reflecting stronger internal responses to the stimulus and a more confident prediction at the output neuron. Intriguingly, this analysis reveals that the process of learning simply involves monotonically amplifying small but random initial selectivity for the &#x002B;/-stimulus into a strong final selectivity.</p>
<p>This analysis also sheds light on the synaptic connectivity and computational mechanism of the RNN. To perform the task, the network must integrate evidence for the sign of the noisy stimulus over time. Linear model networks achieve this when the synaptic weight matrix has a single eigenvalue equal to one, and the remaining eigenvalues close to zero [<xref ref-type="bibr" rid="c53">53</xref>]. The eigenvector associated with this eigenvalue corresponds to a pattern of activity across neurons along which the network integrates evidence. The nonlinear RNN converged to a similar solution where one eigenvalue of the connectivity matrix is close to one, and the remaining eigenvalues are smaller and correspond to random noise in the synaptic connections (<xref rid="fig3" ref-type="fig">fig. 3</xref>, supp. 1a). Although the TCA model was fit only to the activity of the network, the prototypical firing pattern extracted by TCA in (<xref rid="fig3" ref-type="fig">fig. 3e</xref>; left) closely matched the principal eigenvector of the network&#x2019;s synaptic connectivity matrix (<xref rid="fig3" ref-type="fig">fig. 3f</xref>). Thus, TCA extracted an important aspect of the network&#x2019;s connectome from the raw simulated activity.</p>
<p>The neuron factor can also be used to better visualize and interpret the weight matrix itself. Since the original order of the neurons is arbitrary, the raw synaptic connectivity matrix appears to be unstructured noise (<xref rid="fig3" ref-type="fig">fig. 3g</xref>, left). However, re-sorting the neurons based on the neuron factor extracted by TCA, reveals a competitive connectivity between the (&#x002B;)-cells and (-)-cells (<xref rid="fig3" ref-type="fig">fig. 3g</xref>, right). Specifically, neurons tend to send excitatory connections to cells in their same class, and inhibitory connections to cells of the opposite class. We also observed positive correlations between the neuron factor and the input and output synaptic weights of the network (<xref rid="fig3" ref-type="fig">fig. 3</xref> supp. 1b-c). Taken together, these results provide a simple account of network function in which the input signal excites (&#x002B;)-cells and inhibits (-)-cells on (&#x002B;)-trials, and vice versa on (-)-trials. The two cell populations then compete for dominance in a winner-take-all fashion. Finally, the decision of the network is broadcast to the output cell by excitatory projections from the (&#x002B;)-cells and inhibitory projections from the (-)-cells (<xref rid="fig3" ref-type="fig">fig. 3h</xref>).</p>
<p>In summary, TCA extracts a simple one-dimensional description of the activity of all neurons over all trials in this nonlinear network. Moreover, each of the three factors extracted by TCA have a simple neurobiological interpretation: the neuron factor <bold>w</bold><sup>1</sup> reveals a continuum of neurons interpolating between two cell assemblies, the temporal factor <bold>b</bold><sup>1</sup> describes the dominant neural activity underlying decision making, namely integration to a bound, and the trial amplitudes <bold>a</bold><sup>1</sup> reflect the trial-by-trial decisions of the network, as well as the long term amplification of stimulus selectivity underlying learning. Finally, even though the low-dimensional TCA factors were found in an unsupervised fashion from the raw neural activity, they provide direct insights into the synaptic connectivity and emergent computational mechanism underlying the network&#x2019;s ability to learn and decide.</p>
</sec>
<sec id="s2e">
<label>2.5</label>
<title>TCA compactly represents prefrontal activity during spatial navigation</title>
<p>Given the demonstrated success of TCA on an artificial nonlinear network, we next examined the performance of TCA on large-scale neurobiological datasets. We first examined the activity of cortical cells in mice performing a spatial navigation task with variable reward contingencies. A miniature microendoscope [<xref ref-type="bibr" rid="c54">54</xref>] was used to record fluorescence in GCaMP6m-expressing excitatory neurons in the medial prefrontal cortex while mice navigated a four-armed maze. Mice began each trial in either the east or west arm and chose to visit either the north or south arm, at which point a water reward was either dispensed or witheld (<xref rid="fig4" ref-type="fig">fig. 4a</xref>-b). We examined a dataset from a mouse containing <italic>N</italic> &#x003D; 282 neurons recorded at <italic>T</italic> &#x003D; 111 timepoints (at 10 Hz) on <italic>K</italic> &#x003D; 600 behavioral trials, collected over a five day period. The rewarded navigational rules were switched periodically, prompting the mouse to explore different actions from each starting arm. Fluorescence traces for each neuron were shifted and scaled to range between zero and one in each session, and organized into a <italic>N &#x00D7; T &#x00D7; K</italic> tensor.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig 4.</label>
<caption><title>Reconstruction of single-cell activity during spatial navigation by unconstrained and nonnegative TCA.</title>
<p><bold>(a)</bold> All four possible combinations of starting and ending position on a trial. <bold>(b)</bold> Color scheme for three binary task variables (start location, end location, and reward). Each trial involves a sequential selection of these three variables. <bold>(c)</bold> Median fluorescence of example neurons that encode the starting location. Dashed lines denote upper and lower quartiles of the fluorescence. <bold>(d-e)</bold> Same as <bold>(d)</bold> but showing neurons that encode the ending location and the presence/absence of water reward. <bold>(f)</bold> Scree plot showing normalized reconstruction error for unconstrained (blue) and nonnegative (red) TCA, and the condition-averaged baseline model (black dashed line). Models were optimized from multiple initial parameters; each dot corresponds to a different optimization run. <bold>(g)</bold> Median coefficient of determination (<italic>R</italic><sup>2</sup>) for neurons as a function of the number of model components for unconstrained TCA (blue), nonnegative TCA (red), and the condition-averaged baseline (black). Dots show the median <italic>R</italic><sup>2</sup> and the extent of the lines shows the first and third quartiles of the distribution. <bold>(h)</bold> Model similarity (<xref ref-type="sec" rid="s4e1">section 4.5.1</xref>) as a function of model components for unconstrained (blue) and nonnegative (red) TCA. Each dot shows the similarity of a single optimization run compared to the best-fit model within each category. <bold>(i)</bold> Sparsity (proportion of zero elements) in the neuron factors of unconstrained (blue) and nonnegative decompositions. For each decomposition type, only the best-fit model is shown. <bold>(j)</bold> Neuron dimensionality (<xref ref-type="sec" rid="s4e2">section 4.5.2</xref>) plotted against variance in activity. The size and color of the dots represent the <italic>R</italic><sup>2</sup> of a nonnegative decomposition with 15 components. <bold>(k)</bold> Normalized reconstruction error plotted against number of free parameters for trial-averaged PCA, trial-concatenated PCA, and TCA.</p></caption>
<graphic xlink:href="211128_fig4.tif"/>
</fig>
<p>Neural firing in prefrontal cortical areas have previously been found to encode task variables, outcomes, value judgments, and cognitive strategies [<xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c55">55</xref>&#x2013;<xref ref-type="bibr" rid="c59">59</xref>]. We observed that many neurons selectively correlated with individual task variables on each trial: the initial arm of the maze (<xref rid="fig4" ref-type="fig">fig. 4c</xref>), the final arm (<xref rid="fig4" ref-type="fig">fig. 4d</xref>), and whether the mouse received a reward (<xref rid="fig4" ref-type="fig">fig. 4e</xref>). Notably, many of these neurons &#x2014; particularly those with strong and robust coding properties &#x2014; varied most strongly in amplitude across trials, suggesting that low-dimensional gain modulation is a reasonable model for these data. A TCA model with 15 components accurately modeled the activity of these individual cells and recovered their coding properties (<xref rid="fig4" ref-type="fig">fig. 4c</xref>-e; middle column; <italic>R</italic><sup>2</sup> between 0.44 and 0.91).</p>
<p>Since the fluorescence traces were normalized to be nonnegative, we also investigated the performance of <italic>non-negative TCA</italic>. This variant of TCA constrains the neuron, temporal, and trial factors to have nonnegative elements but is otherwise identical to standard TCA. Nonnegative TCA can produce more interpretable models, since the model is constrained to reconstruct the original data only through adding, but not subtracting, components, similar to nonnegative matrix factorization [<xref ref-type="bibr" rid="c60">60</xref>]. Despite this additional constraint, nonnegative TCA with 15 components reconstructed the activity of individual neurons with similar fidelity to an unconstrained TCA model (<xref rid="fig4" ref-type="fig">fig. 4c</xref>-e; right column; <italic>R</italic><sup>2</sup> between 0.42 and 0.89).</p>
<p>We then characterized the performance of TCA and nonnegative TCA across the full population of neurons. We compared both methods to a <italic>condition-average baseline model</italic>, which predicts the neural activity on each trial to be the trial-average population activity conditioned on the same trial trajectory (same starting arm and ending arm) and trial outcome (reward vs. error). That is, we computed the mean activity within each of the eight possible combinations of trial conditions, decisions, and outcomes, as used this to predict single-trial data. In essence, this baseline captures the average effect of all task variables, but does not account for trial-to-trial variability within each combination of task variables.</p>
<p>An error plot for unconstrained and nonnegative TCA showed three important findings (<xref rid="fig4" ref-type="fig">fig. 4f</xref>). First, nonnegative TCA had similar predictive performance to unconstrained TCA in terms of reconstruction error across all numbers of latent components (small gap between red and blue lines, <xref rid="fig4" ref-type="fig">fig. 4f</xref>). Second, both forms of TCA converged to very similar reconstruction error from twenty different random initializations, suggesting that the models did not get caught in highly suboptimal local minima during optimization (all blue points and all red points reached similar error, <xref rid="fig4" ref-type="fig">fig. 4f</xref>). Third, TCA models with more than 6 components matched or surpassed the condition-average baseline model, suggesting that relatively few components were needed to explain a substantial fraction of explainable variance in the dataset (dashed black line, <xref rid="fig4" ref-type="fig">fig. 4f</xref>). We also examined the performance of nonnegative and unconstrained TCA in terms of the <italic>R</italic><sup>2</sup> of individual neurons. Again, nonnegative TCA performed similarly to unconstrained TCA as judged by the median and upper/lower quartiles of the single neuron <italic>R</italic><sup>2</sup>, and both models surpassed the simple condition-average baseline if they included more than 7 components (<xref rid="fig4" ref-type="fig">fig. 4g</xref>).</p>
<p>In addition to achieving similar accuracy to unconstrained TCA, nonnegative TCA possesses two important advantages. First, a similarity plot showed that nonnegative models converged more consistently to a similar set of low-dimensional components (<xref rid="fig4" ref-type="fig">fig. 4h</xref>). Second, the components recovered by nonnegative TCA were more sparse, meaning that each neuron&#x2019;s activity across all trials was reconstructed by a smaller and more interpretable subset of components (<xref rid="fig4" ref-type="fig">fig. 4i</xref>).</p>
<p>While TCA could reconstruct the activity of many neurons very well (<xref rid="fig4" ref-type="fig">fig. 4c</xref>-e), other neurons were more difficult to fit (<xref rid="fig4" ref-type="fig">fig. 4</xref>, supp. 1). However, we observed that neurons with low <italic>R</italic><sup>2</sup> had firing patterns that were unreliably timed across trials and did not correlate with task variables (<xref rid="fig4" ref-type="fig">fig. 4</xref>, supp. 1b). To visualize this, we plotted the total variance and the dimensionality of each cell&#x2019;s activity against the fit of a nonnegative TCA model with 15 components (<xref rid="fig4" ref-type="fig">fig. 4j</xref>). The dimensionality of each cell&#x2019;s activity (see <italic>Methods</italic>, <xref ref-type="sec" rid="s4e2">section 4.5.2</xref>) measures the trial-to-trial reliability of a cell&#x2019;s firing: cells that fire consistently at the same time in each trial will be low-dimensional relative to cells that fire at different time points in each trial. First, this plot shows a negative correlation between variance and dimensionality: cells with higher variance (larger dynamic ranges in fluorescence) tended to be lower dimensional and thus more reliably timed across trials. Second, this plot shows these low-dimensional cells were well fit by TCA, suggesting that TCA summarizes the information encoded most reliably and strongly by this neural population. Moreover, outlier cells that defy a simple statistical characterization can be algorithmically identified and flagged for secondary analysis by sorting neurons by their <italic>R</italic><sup>2</sup> score under TCA.</p>
<p>TCA&#x2019;s performance in summarizing neural population activity with very few parameters far exceeds that of trial-averaged PCA, which has sub-par performance, and trial-concatenated PCA, which requires many more parameters to achieve similar performance. This comparison is summarized in <xref rid="fig4" ref-type="fig">Figure 4k</xref>, which plots reconstruction error against the number of free parameters over 1 to 20 low-dimensional components for each class of models. Trial-averaged PCA (<xref rid="fig4" ref-type="fig">fig. 4k</xref>, gray line) has fewer parameters than TCA, but cannot account for trial-to-trial changes in activity, cannot achieve much lower than 60&#x0025; error, and by construction entirely misses trial-to-trial fluctuations in neural firing that encode task variables. In contrast, trial-concatenated PCA (<xref rid="fig4" ref-type="fig">fig. 4k</xref>, black line) achieved comparable reconstruction error to TCA but required roughly 100x more free parameters, and is therefore much less interpretable. A TCA model with 15 components reduces the complexity of the data by 3 orders of magnitude, from &#x223C;10<sup>7</sup> datapoints to &#x223C;10<sup>4</sup> parameters; whereas a trial-concatenated PCA model with a comparable number of components only reduces the number of parameters to &#x223C;10<sup>6</sup>.</p>
</sec>
<sec id="s2f">
<label>2.6</label>
<title>Individual TCA components selectively correlate with individual task variables</title>
<p>These results demonstrate that TCA accurately describes the firing rates of single cells in a highly compact manner. We then examined whether this model identified an interpretable set of low-dimensional components. <xref rid="fig5" ref-type="fig">Figure 5</xref> shows eight components from a 15-component nonnegative TCA model (the remaining seven factors carry similar information and are shown in <xref rid="fig5" ref-type="fig">fig. 5</xref>, supp. 1). Each nonnegative TCA component identified a sub-population, or assembly of cells (neuron factor; left column) with a common intra-trial temporal dynamics (temporal factor; middle column) that was differentially activated across trials (trial factor; right column).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Fig 5.</label>
<caption><title>Nonnegative TCA of prefrontal cortical activity during spatial navigation.</title>
<p>Eight low-dimensional components, each containing a neuron factor (left column), temporal factor (middle column), and trial factor (right column) are shown from a 15-component model (see <xref rid="fig5" ref-type="fig">fig. 5</xref>, supp. 1 for the remaining seven components). For each component, the trial factor is color-coded by the task variable it is most highly correlated with.</p></caption>
<graphic xlink:href="211128_fig5.tif"/>
</fig>
<p>In contrast, PCA identified factors that contained complex mixtures of coding for the mouse&#x2019;s position, choice, and reward on each trial (<xref rid="fig5" ref-type="fig">fig. 5</xref>, supp. 2), hampering interpretability [<xref ref-type="bibr" rid="c34">34</xref>]. TCA on the other hand, isolated each of these task variables into separate components: each trial factor selectively correlated with a <italic>single</italic> task variable, as indicated by the color-coded scatterplots in <xref rid="fig5" ref-type="fig">fig. 5</xref>. Overall, the TCA model uncovers, in a completely unsupervised manner, a compelling qualitative view of prefrontal dynamics in which largely distinct subsets of neurons (<xref rid="fig5" ref-type="fig">fig. 5</xref>, left columns) are active at successive times within a trial (<xref rid="fig5" ref-type="fig">fig. 5</xref> middle column) and whose variation across trials (<xref rid="fig5" ref-type="fig">fig. 5</xref> right column) encodes a highly interpretable single task variable.</p>
<p>Specifically, components 1-2 uncover neurons that encode the starting location (component 1, east trials; component 2, west trials), components 5-6 encode the destination arm (component 5, north trials; component 6, south trials), and components 7-8 encode the trial outcome (component 7, rewarded trials; component 8, error trials). Interestingly, the temporal factors indicate that these components are sequentially activated in each trial: components 1-2 activated before components 5-6, which in turn activated before components 7-8, in agreement with the schematic flow diagram shown in <xref rid="fig4" ref-type="fig">fig. 4b</xref>.</p>
<p>Intriguingly, TCA also uncovers unexpected components, like components 3-4 which activate prior to the destination and outcome-related components (i.e., components 5-8). Component 4 displays systematic reductions in activity across trials within each day, while component 3 is active on nearly every single trial. Component 4 could potentially correspond, for example, to a novelty or arousal signal that wanes over trials within a day. While further experiments will be required to ascertain whether this interpretation is correct, the extraction of these components illustrates the potential power of TCA as an unbiased exploratory data analysis technique to extract unobserved cognitive states and separate them from observable aspects of trial-to-trial variations in behavior.</p>
<p>It is important to emphasize that TCA is an unsupervised method that only has access to the neural data tensor, and does not receive any information about task variables like starting location, ending location, and reward. Therefore, the correspondence between TCA trial factors and behavioral information demonstrated in <xref rid="fig5" ref-type="fig">fig. 5</xref>, constitutes an unbiased revelation of task structure directly from neural data. Moreover, individual components extracted by TCA are in one-to-one correspondence with meaningful aspects of task structure and behavior, a property not shared by many other dimensionality reduction algorithms.</p>
</sec>
<sec id="s2g">
<label>2.7</label>
<title>TCA reveals two-dimensional learning dynamics in macaque motor cortex after a BMI perturbation</title>
<p>In the previous section we validated TCA on a dataset where the animal&#x2019;s behavior decomposed into a set of discrete experimental conditions, choices, and trial outcomes. We next applied this method to a brain-machine interface (BMI) learning task, in which the behavior on each trial was quantified by a continuous path of a computer cursor. The cursor movement on each trial is never identical and is difficult to summarize concisely in a principled manner. In these more unstructured scenarios, supervised methods such as classification and regression can be difficult to construct, making unsupervised dimensionality reduction methods an important tool to explore hypotheses and let the low-dimensional structure of the data &#x201C;speak for itself.&#x201D;</p>
<p>Specifically, we collected multi-unit data from the pre-motor and primary motor cortices of a Rhesus macaque (<italic>Macaca mulatta</italic>) controlling a computer cursor in a 2D plane through a brain-machine interface (<xref rid="fig6" ref-type="fig">fig. 6a</xref>). Spikes were recorded when the voltage signal crossed below &#x2212;4.5 times the root-mean-square voltage. The monkey was trained to make point-to-point reaches from a central position to one of eight radial targets. For simplicity, we initially investigated neural activity during 45&#x00B0; outward reaches. The cursor velocity was controlled by a velocity Kalman filter decoder, which was driven by non-sorted multi-unit activity (-4.5 root-mean-square threshold crossings) and fit using relations between neural activity and reaches by the monkey&#x2019;s contralateral arm at the beginning of the experiment [<xref ref-type="bibr" rid="c61">61</xref>]. We analyzed multi-unit activity during subsequent reaches, which used this decoder as a BMI interface directly from neural activity to cursor motion. These initial reaches were accurate (<xref rid="fig6" ref-type="fig">fig. 6b</xref>, left) and took less than one second to execute (<xref rid="fig6" ref-type="fig">fig. 6c</xref>, first 30 trials).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Fig 6.</label>
<caption><title>TCA reveals two-dimensional learning dynamics in primate motor cortex during BMI cursor control.</title>
<p><bold>(a)</bold> Schematic of monkey making center-out, point-to-point reaches in BMI task. <bold>(b)</bold> Cursor trajectories to a 45&#x00B0; target position. Twenty trials are shown at three stages of the behavioral session showing initial performance (left), performance immediately after a 30&#x00B0; counterclockwise visuomotor perturbation (middle), and performance after learning, at the end of the behavioral session. Cyan and magenta points respectively denote the cursor position at the beginning and end of the trial. <bold>(c)</bold> Time for the cursor to reach target for each trial in seconds. The visuomotor perturbation was introduced after 31 trials (red line). <bold>(d)</bold> An optimal 3-component nonnegative TCA on smoothed multi-unit spike trains recorded from motor cortex during virtual reaches reveals two components (2-3) that capture learning after the BMI perturbation.</p></caption>
<graphic xlink:href="211128_fig6.tif"/>
</fig>
<p>We then perturbed the BMI decoder by rotating the output cursor velocities counterclockwise by 30&#x00B0; (a visuomotor rotation). Thus, the same neural activity pattern that originally caused a motion of the cursor towards the 45&#x00B0; direction, now caused a maladaptive motion in the 75&#x00B0; direction, yielding an immediate drop in performance: the cursor trajectories were biased in the counterclockwise direction (<xref rid="fig6" ref-type="fig">fig. 6b</xref>, middle), and took longer to reach the target (<xref rid="fig6" ref-type="fig">fig. 6c</xref>, trials following perturbation). These deficits were partially recovered within a single training session as the monkey adapted to the new decoder. By the end of the session, the monkey made more direct cursor movements (<xref rid="fig6" ref-type="fig">fig. 6b</xref>, right) and achieved the target more quickly (<xref rid="fig6" ref-type="fig">fig. 6c</xref>).</p>
<p>We applied TCA and nonnegative TCA to the raw spike trains smoothed with a Gaussian filter with a standard devation of 50 ms [<xref ref-type="bibr" rid="c34">34</xref>]. We again found that nonnegative TCA fit the data with similar reconstruction error and higher reliability than unconstrained TCA (<xref rid="fig6" ref-type="fig">fig. 6</xref> supp. 1). To examine a simple account of learning dynamics, we examined a nonnegative TCA model with 3 components. Models with fewer than 3 components had substantially worse reconstruction error, while models with more components had only moderately better performance and occasionally converged to dissimilar parameters during optimization (<xref rid="fig6" ref-type="fig">fig. 6</xref> supp. 1).</p>
<p>The neuron, temporal, and trial factors of the nonnegative TCA model are shown in <xref rid="fig6" ref-type="fig">Figure 6d</xref>. Component 1 (red) described multi-units that were active at the beginning of each trial, and were consistently active over all trials. The other two components described multi-units that were inactive before the BMI perturbation, and became active only after the perturbation, thereby capturing motor learning. Component 2 (blue) became active on trials immediately after the BMI perturbation, but then slowly decayed over successive trials. Within a single trial, this component was only active at late stages in the reach. Component 3 (green) on the other-hand was not active on trials immediately following the BMI perturbation, but did activate slowly across successive trials. Within a single trial, this component was active earlier in the reach. These results suggest a mode of motor learning in which a suboptimal, late reaching-stage correction is initially used to perform the task (component 2). Over time, this component is slowly traded for a more optimal early reaching-stage correction (component 3). Interestingly, motor learning did not involve extinguishing neural dynamics present before the perturbation (component 1), even though this component is maladaptive after the perturbation.</p>
<p>We were able to confirm this intuition by relating each of these components to a different phase of motor execution and learning. <xref rid="fig7" ref-type="fig">Figure 7a</xref> plots cursor trajectories on individual reaches before the perturbation (left), immediately following the perturbation (middle), and at the end of the behavioral session (right). Every 50ms the trajectory was colored based on the component with the largest activation at that timepoint and trial. Prior to the perturbation, component 1 (red) dominated; the other two components were nearly inactive since their TCA trial factor amplitudes were near zero before the perturbation (see <xref rid="fig6" ref-type="fig">fig. 6d</xref>). Immediately following the perturbation, component 1 still dominated in the early phase of each trial, producing a counterclockwise off-target trajectory. However, component 2 dominated the second half of each trial at which point the monkey performed a &#x201C;corrective&#x201D; horizontal movement to compensate for the initial error. Finally, near the end of the training session, component 3 was most active at many stages of the reach. Typically, the cursor moved directly towards the 45&#x00B0; target when component 3 was active, suggesting that component 3 captured learned neural dynamics that were correctly adapted to the perturbed visuomotor environment.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Fig 7.</label>
<caption><title>TCA tracks performance and uncovers &#x201C;corrective&#x201D; dynamics in BMI adaptation task.</title>
<p><bold>(a)</bold> Cursor trajectories for 45&#x00B0; cursor reaches. Every 50 ms, the trajectory is colored by the TCA component with the strongest activation at that timepoint and trial. Components were colored according to the definition in panel (b). Three example trajectories are shown at three stages of the experiment: reaches before the visuomotor perturbation (left), reaches immediately following the perturbation (middle), and reaches at the end of the behavioral session. <bold>(b)</bold> Average low-dimensional temporal factors identified by nonnegative TCA across all eight reach angles. The <italic>early component</italic> had the earliest active temporal factor (red). The <italic>corrective component</italic> had the last active temporal factor (blue). The <italic>learned component</italic> was the second active temporal factor (green). Solid and dashed lines denote mean &#x002B;/- standard deviation. <bold>(c)</bold> Preferred cursor angles for each component type after the visuomotor perturbation. All data were rotated so that the target reach angle was at 0&#x00B0; (solid black line). Dashed black lines denote &#x002B;/- 30&#x00B0; for reference, which was the magnitude of the visuomotor perturbation. On average, the <italic>early component</italic> was associated with a cursor angle misaligned counterclockwise from the target (red). The <italic>corrective component</italic> preferred angle was aligned clockwise from the target (blue) by about 30&#x00B0;, in a way that could compensate for the 30&#x00B0; counterclockwise misalignment of the <italic>early component</italic>. The <italic>learned component</italic> preferred angle not significantly different from that of the actual target. <bold>(d)</bold> Smoothed trial factors for the <italic>early component</italic> and <italic>learned component</italic>. Colored lines denote averages across all reach angles; gray lines denote the factors for each of the eight reach conditions. Factors were smoothed with a Gaussian filter with 1.5 standard deviation for visualization purposes. <bold>(e)</bold> Smoothed trial factor for the <italic>corrective component</italic> (blue) and smoothed behavioral performance (black) quantified by seconds to reach target. Each subplot shows data for a different reach angle. All signals were smoothed with a Gaussian filter with 1.5 standard deviation for visualization.</p></caption>
<graphic xlink:href="211128_fig7.tif"/>
</fig>
<p>Based on these observations, we called the component active at the beginning of each trial the <italic>early component</italic> (#1 in <xref rid="fig6" ref-type="fig">fig. 6</xref>), the component active at the end of each trial the <italic>corrective component</italic> (#2 in <xref rid="fig6" ref-type="fig">fig. 6</xref>), and the component active in the middle of each trial the <italic>learned component</italic> (#3 in <xref rid="fig6" ref-type="fig">fig. 6</xref>). These components are colored red, blue, and green respectively in both <xref rid="fig6" ref-type="fig">Figure 6</xref> and <xref rid="fig7" ref-type="fig">Figure 7</xref>. We then fit 3-component TCA models separately to each of the eight reach angles, and operationally defined the components as <italic>early</italic>, <italic>corrective</italic>, and <italic>learned</italic> based on the peak magnitude of their associated within-trial temporal basis functions (<xref rid="fig7" ref-type="fig">fig. 7b</xref>). This very simple definition yielded similar interpretations for low-dimensional components separately fit across different reach angles.</p>
<p>Similar to computing a directional tuning curve for an individual neuron [<xref ref-type="bibr" rid="c62">62</xref>], we examined the <italic>preferred cursor angles</italic> of each low-dimensional component by computing the average cursor velocity weighted by activity of the component (see <italic>Methods</italic>, <xref ref-type="sec" rid="s4e2">section 4.4.7</xref>). To compare across all target reach angles, we rotated the preferred angles so that the target was situated at 0&#x00B0; (black line, <xref rid="fig7" ref-type="fig">fig. 7c</xref>). All preferred angles were computed on post-perturbation trials. When the <italic>early component</italic> was active, the cursor typically moved at an angle counterclockwise to the target (<italic>p &#x003C;</italic> 0.05, one sample test for the mean angle), reflecting our previous observation that the early component encodes pre-perturbation dynamics that are maladaptive post-perturbation (<xref rid="fig7" ref-type="fig">fig. 7c</xref>, left). When the <italic>corrective component</italic> was active, the cursor typically moved at an angle clockwise to the target (<italic>p &#x003C;</italic> 0.01, one sample test for the mean angle), reflecting a late-trial compensation for the error introduced by the early component (<xref rid="fig7" ref-type="fig">fig. 7c</xref>, middle). Finally, the <italic>learned component</italic> was not significantly different from the target angle, reflecting a tuning that was better adapted for the perturbed visuomotor environment.</p>
<p>Having established a within-trial interpretation for each component, we next examined across-trial learning dynamics. For visualization purposes, we gently smoothed all TCA trial factors by a Gaussian filter with a standard deviation of 1.5 trials. Across all reach angles, the <italic>early component</italic> was typically flat and insensitive to the visuomotor perturbation (<xref rid="fig7" ref-type="fig">fig. 7d</xref>, left). In contrast, the <italic>learned component</italic> activated soon after the perturbation was applied, although the rapidness of this onset varied across reach angles (<xref rid="fig7" ref-type="fig">fig. 7d</xref>, right). Together, this reinforces our earlier observation that adaptation to the visuomotor rotation typically involves the production of new neural dynamics (captured by the <italic>learned component</italic>), rather than the suppression of maladaptive dynamics (captured by the <italic>early component</italic>).</p>
<p>Finally, the <italic>corrective component</italic> was consistently correlated with the animal&#x2019;s behavioral performance on all reach angles (<italic>p &#x003C;</italic> 0.05, Spearman&#x2019;s rho test). Since performance differed across reach angles, we separately plotted the <italic>corrective component</italic> (blue) against the time to acquire the target (black) for each reach angle (<xref rid="fig7" ref-type="fig">fig. 7e</xref>). Remarkably, in many cases, the corrective component provided an accurate trial-by-trial prediction of the reach duration, meaning that trials with a large corrective movement took longer to execute.</p>
<p>Together, these results demonstrate that TCA can identify, in a purely unsupervised manner, both learning dynamics across trials and single trial neural dynamics. Indeed, each trial factor can be related to within-trial behaviors, such as error-prone cursor movements and their subsequent correction. Furthermore, these basic interpretations largely replicate across all eight reach angles, despite differences in the learning rate within each of these conditions. Most intriguingly, a <italic>single</italic> trial factor, extracted only from neural data, can directly predict execution time on a trial by trial basis, without ever having direct access to this aspect of behavior (<xref rid="fig7" ref-type="fig">fig. 7e</xref>).</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Discussion</title>
<p>Recent experimental technologies enable us to record from more neurons, at higher temporal precision, and for much longer time periods than ever before [<xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c11">11</xref>], thereby simultaneously increasing the size and complexity of datasets along three distinct modes. However, methods for multi-timescale dimensionality reduction that describe both rapid neural dynamics within trials and long-term changes in neural dynamics across-trials are still lacking. As a result, experimental investigations of neural circuits are often confined to a single timescale, even though bridging our understanding across multiple timescales is of great interest [<xref ref-type="bibr" rid="c9">9</xref>]. Here we demonstrated a unified approach, TCA, that simultaneously recovers low-dimensional and interpretable structure across neurons, time within trials, and trials.</p>
<p>TCA and other tensor decomposition techniques have been extensively studied from a theoretical perspective [<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c63">63</xref>&#x2013;<xref ref-type="bibr" rid="c65">65</xref>], and have been applied to a variety of biomedical problems [<xref ref-type="bibr" rid="c66">66</xref>&#x2013;<xref ref-type="bibr" rid="c68">68</xref>]. Several studies have applied tensor decompositions to EEG and fMRI data, most typically to model differences across subjects or Fourier/wavelet transformed signals [<xref ref-type="bibr" rid="c69">69</xref>&#x2013;<xref ref-type="bibr" rid="c72">72</xref>], rather than across trials [<xref ref-type="bibr" rid="c73">73</xref>]. A recent study examined trial-averaged neural data across multiple neurons, conditions, and time within trials as a tensor, but they did not study trial-to-trial variability, and only examined different unfoldings of the data tensor into matrices, rather than applying TCA directly to the data tensor [<xref ref-type="bibr" rid="c74">74</xref>]. Other studies have modeled the receptive fields of neurons in auditory and visual cortex as third-order tensors with low-rank structure [<xref ref-type="bibr" rid="c75">75</xref>, <xref ref-type="bibr" rid="c76">76</xref>]. We go beyond these previous studies by applying TCA to a broader class of artificial and experimental datasets, drawing a novel connection between TCA and theories of gain modulation, and demonstrating that visualization and analysis of the TCA trial factors can directly yield functional clustering of neural populations (i.e., cell assemblies) as well reveal learning dynamics on trial-by-trial basis.</p>
<p>In particular, we demonstrated that TCA reveals a simple description of learning in an artificial nonlinear neural network trained to solve the analog of a motion discrimination task [<xref ref-type="bibr" rid="c32">32</xref>]. TCA discovered a one-dimensional learning process in which initial, small random selectivity is monotonically amplified over time to yield the final learned decision making dynamics. Moreover, cell-type information extracted by TCA in an unsupervised manner enabled us to re-organize the network&#x2019;s connectome, thereby yielding conceptual insights into how this connectome gives rise to mechanisms for decision making. Also, in calcium imaging data recorded from rodent pre-frontal cortex during a maze navigation task, TCA uncovered functional subsets of neurons that fired sequentially within trials, and whose amplitude on each trial selectively mapped onto task-relevant variables, including starting location, ending location and reward (in that order).</p>
<p>Finally, in electrophysiological recordings from macaque motor and premotor cortex, TCA revealed a simple two-dimensional learning process in response to a BMI perturbation. Interestingly, this learning process did not involve extinguishing maladaptive dynamics that were established in the pre-perturbation period (i.e., the &#x201C;early component&#x201D; identified by TCA). Rather, it involved the addition of two components that compensated for the maladaptive dynamics. The first &#x201C;corrective&#x201D; component was a suboptimal, late stage within-trial correction that was active in trials soon after the perturbation, which extinguished over trials to give rise to a second &#x201C;learned&#x201D; component that implemented a more optimal early stage within-trial correction. Moreover, the late stage correction could predict time to target acquisition on a trial-by-trial basis. Importantly, all of these results were discovered purely from the neural data and not behavioral measurements, suggesting that TCA can uncover unexpected and otherwise unobservable neural dynamics in a data-driven, unsupervised manner.</p>
<p>In addition to the empirical success of TCA in diverse scenarios presented here, there are three other reasons we expect TCA to have widespread utility in neuroscience. First, TCA is arguably the simplest generalization of PCA that can handle trial-to-trial variability. Given the widespread adoption of PCA, we believe that TCA may also enjoy widespread adoption and success, especially as technologies enabling long-term and large-scale recordings become more accessible. Second, as we have shown, TCA has an intriguing interpretation as a network model with low-dimensional gain-modulated inputs. This model is supported by experimental evidence in many contexts [<xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c37">37</xref>, <xref ref-type="bibr" rid="c76">76</xref>&#x2013;<xref ref-type="bibr" rid="c78">78</xref>], and underlies influential theories of cortical computation [<xref ref-type="bibr" rid="c30">30</xref>, <xref ref-type="bibr" rid="c31">31</xref>] and perceptual learning [<xref ref-type="bibr" rid="c79">79</xref>].</p>
<p>Third, while TCA is a simple generalization of PCA, its theoretical properties are strikingly more favorable. A fundamental limitation of PCA is that the components it recovers are restricted to be orthogonal to each other, and moreover these components can be rotated amongst each other without changing the reconstruction error. This invariance to rotations in PCA leads to a fundamental ambiguity, and so the factors identified by PCA are unlikely to be directly interpretable as biological signals (see <italic>Methods</italic>, <xref ref-type="sec" rid="s4e2">section 4.4.2</xref>). In contrast, the factors identified by</p>
<p>TCA are not invariant to many transformations [<xref ref-type="bibr" rid="c29">29</xref>], yielding more interpretable results. This advantage was first demonstrated in <xref rid="fig2" ref-type="fig">fig. 2</xref> where the factors recovered from neural firing rates, matched the underlying parameters of the model neural network in a one-to-one fashion. Similarly, in the rodent prefrontal analysis, TCA uncovers demixed factors that individually correlate with interpretable task variables, whereas PCA does not (compare <xref rid="fig5" ref-type="fig">fig. 5</xref> to <xref rid="fig5" ref-type="fig">fig. 5</xref> supp. 1). And finally, when applied to neural activity during BMI learning, TCA consistently found, across multiple reach angles, a &#x201C;corrective factor&#x201D; that significantly correlated with behavioral performance on a trial-by-trial basis (<xref rid="fig7" ref-type="fig">fig. 7</xref>).</p>
<p>In this paper, we examined the simplest form of TCA by making no assumptions about the temporal dynamics of neural activity within trials or the dynamics of learning across trials. As a result, we obtain extreme flexibility: for example, trial factors could be discretely activated or inactivated on each trial (<xref rid="fig5" ref-type="fig">fig. 5</xref>), or they might emerge incrementally over longer timescales (<xref rid="fig6" ref-type="fig">fig. 6</xref>). However, future work could augment TCA with additional structure and assumptions, such as a smoothness penalty or dynamical systems structure within trials [<xref ref-type="bibr" rid="c16">16</xref>]. Intriguingly, a dynamical system could just as easily be incorporated along the trials axis of the data tensor to potentially relate high-dimensional neural activity to low-dimensional models of learning [<xref ref-type="bibr" rid="c80">80</xref>].</p>
<p>Further work in this direction could connect TCA to a large body of work on fitting latent dynamical systems to reproduce within-trial firing patterns. In particular, single trial neural activity has been modeled with linear dynamics [<xref ref-type="bibr" rid="c81">81</xref>&#x2013;<xref ref-type="bibr" rid="c84">84</xref>], switched linear dynamics [<xref ref-type="bibr" rid="c85">85</xref>, <xref ref-type="bibr" rid="c86">86</xref>], linear dynamics with nonlinear observations [<xref ref-type="bibr" rid="c17">17</xref>], and nonlinear dynamics [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c87">87</xref>]. In practice, these methods require many modeling choices, validation procedures, and post-hoc analyses. Simple linear models have a relatively constrained dynamical repertoire [<xref ref-type="bibr" rid="c12">12</xref>], while models with nonlinear elements often have greater predictive abilities [<xref ref-type="bibr" rid="c17">17</xref>, <xref ref-type="bibr" rid="c18">18</xref>], but at the expense of interpretability. In all cases, the learned representation of each trial (e.g., the initial condition to a nonlinear dynamical system) is not transparently related to single trial data. In contrast, the trial factors identified by TCA have an extremely simple interpretation as introducing trial-specific linear gain modulation. Overall, we view TCA as a simple and complementary technique to identifying a full dynamical model, as has been previously suggested for PCA [<xref ref-type="bibr" rid="c12">12</xref>].</p>
<p>An important property of TCA is that it extracts salient features of a dataset in a data-driven, unbiased fashion. Such unsupervised methods are a critical counterpart to supervised methods, such as regression, which can directly assess whether a dependent variable of interest is represented in population activity. Recently developed methods like <italic>demixed PCA</italic> [<xref ref-type="bibr" rid="c34">34</xref>] combine regression with dimensionality reduction to isolate linear subspaces that selectively code for variables of interest. Again, we view TCA as a complementary approach, with at least three points of difference. First, like trial-concatenated PCA and GPFA, demixed PCA only reduces dimensionality within trials by identifying a different low-dimensional temporal trajectory for each trial. In contrast, TCA identifies a common low-dimensional temporal trajectory (temporal factors) for all trials, which are modulated by different amplitudes (trial factors) on each trial. Second, demixed PCA can separate neural dynamics in cases where trials have discrete conditions and labels, such as in the rodent prefrontal analysis in <xref rid="fig5" ref-type="fig">fig. 5</xref>; however, it is not designed to handle continuous dependent variables, such as those describing learning dynamics (see <xref rid="fig3" ref-type="fig">fig. 3</xref> and <xref rid="fig7" ref-type="fig">fig. 7</xref>). Furthermore, unsupervised techniques like TCA can identify unexpected cognitive states and dynamics corresponding to unknown or difficult to measure dependent variables. Finally, the same rotation invariance of PCA is present within the linear subspaces identified by demixed PCA. Thus, both PCA and demixed PCA are fundamentally subspace identification algorithms, while TCA can often extract directly meaningful features from data, such as clusters of functional cell types or neural populations that grow or shrink in magnitude across trials.</p>
<p>An intriguing direction for future research is to expand TCA to higher-order tensors beyond those encoding neurons, timepoints, and trials. For example, we can also record across multiple subjects learning to solve the same task, yielding a fourth order data tensor, with individual subjects as the fourth index. Similarly, if an individual subject is taught multiple learning tasks, one could encode experimental task or condition as a fourth index. However, directly applying TCA to these tensors may be undesirable, since we record from different neural populations in different subjects and the learning rate may vary from subject-to-subject or from task-to-task. Instead, we could model such data via coupled tensor factorizations [<xref ref-type="bibr" rid="c88">88</xref>] which allow some measured tensor axes to be fit as common factors, while others are fit in a separate and unconstrained fashion. For instance, we could assign separate neuron and trial factors for each subject, but use shared temporal factors across subjects if they are hypothesized to share similar low-dimensional within trial cognitive dynamics. This scheme could extract common circuit dynamics from small numbers of neurons through increased statistical power obtained via pooling across multiple subjects. Moreover, the separate neuron factors would then provide &#x201C;translations&#x201D; between subjects, by revealing how the same cognitive variable is encoded in different population activity patterns in different subjects. In essence, while moving from second to third-order tensor methods provides a new window into how circuit dynamics changes across trials to mediate learning, moving additionally to fourth order tensor methods may provide new insights into how the learning dynamics itself changes across subjects and tasks.</p>
<p>Overall, this work highlights the prevalence of tensor structure in neural datasets and demonstrates that exploiting this structure can provide extremely useful insights into complex, multi-timescale, high-dimensional neural data, including the unsupervised discovery of cell assemblies, within trial neural dynamics underlying perceptions, actions and thoughts, and across trial learning dynamics. Just as PCA has become part of the standard canon of neural data analyses for trial-averaged neural recordings, the combined simplicity and power of TCA suggests it may have widespread utility in the analysis of multineuronal data at the level of single trials.</p>
</sec>
<sec id="s4">
<label>4</label>
<title>Methods</title>
<sec id="s4a">
<label>4.1</label>
<title>Key Resources Table</title>
</sec>
<sec id="s4b">
<label>4.2</label>
<title>Contact for Reagent and Resource Sharing</title>
<p>Further requests for resources should be directed to and will be fulfilled by the Lead Contact, Alex H. Williams (<email>ahwillia@stanford.edu</email>)</p>
</sec>
<sec id="s4c" sec-type="data-availability">
<label>4.3</label>
<title>Data and Software Availability</title>
<p>We provide specialized tools for fitting and visualizing TCA in <ext-link ext-link-type="uri" xlink:href="https://github.com/ahwillia/tensortools">https://github.com/ahwillia/tensortools</ext-link>. Other resources for fitting tensor decompositions include [<xref ref-type="bibr" rid="c93">93</xref>&#x2013;<xref ref-type="bibr" rid="c95">95</xref>].</p>
<table-wrap id="utbl1" orientation="portrait" position="float">
<graphic xlink:href="211128_utbl1.tif"/>
</table-wrap>
</sec>
<sec id="s4d">
<label>4.4</label>
<title>Method Details</title>
<sec id="s4d1">
<label>4.4.1</label>
<title>Notation and Terminology</title>
<p>Colloquially, a tensor is a data array or table with multiple axes or dimensions. More formally, the axes are called <italic>modes</italic> of the tensor, while the <italic>dimensions</italic> of the tensor are the lengths of each mode. Throughout this paper we consider a tensor with three modes with dimensions <italic>N</italic> (number of neurons), <italic>T</italic> (number of timepoints in a trial), and <italic>K</italic> (number of trials).</p>
<p>The number of modes is called the <italic>order</italic> of the tensor. We denote vectors (order-one tensors) with lowercase boldface letters, e.g., <bold>x</bold>. We denote matrices (order-two tensors) with uppercase boldface letters, e.g., <bold>X</bold>. We denote higher-order tensors (order-three and higher) with boldface calligraphic letters, e.g., <inline-formula><alternatives><inline-graphic xlink:href="211128_inline7.gif"/></alternatives></inline-formula>. Scalars are denoted by non-boldface letters, e.g., <italic>x</italic> or <italic>X</italic>. We use <bold>X</bold><sup><italic>T</italic></sup> to denote the transpose of <bold>X</bold>. We aim to keep other notation light and introduce as it is first used &#x2014; readers may refer to [<xref ref-type="bibr" rid="c33">33</xref>] for notational conventions.</p>
</sec>
<sec id="s4d2">
<label>4.4.2</label>
<title>Matrix and Tensor models</title>
<p>Neural population activity is commonly represented as a matrix with each row holding a neuron&#x2019;s activity trace [<xref ref-type="bibr" rid="c12">12</xref>]. Let <bold>X</bold> denote an <italic>N &#x00D7; T</italic> matrix dataset in which <italic>N</italic> neurons are recorded over <italic>T</italic> time steps. For spiking data, <bold>X</bold> may denote trial-averaged spike counts or a single-trial spike train smoothed with a Gaussian filter. If fluorescence microscopy is used in conjunction with voltage or calcium indicators, the data entries could be normalized fluorescence (&#x2206;F<italic>/</italic>F).</p>
<p>PCA is a special case of <italic>matrix decomposition</italic>. A matrix decomposition model approximates the data <bold>X</bold> as a rank-<italic>R</italic> matrix, <inline-formula><alternatives><inline-graphic xlink:href="211128_inline8.gif"/></alternatives></inline-formula>, yielding <italic>R</italic> components. This approximation can be expressed as the product of an <italic>N &#x00D7; R</italic> matrix <bold>W</bold> and a <italic>T &#x00D7; R</italic> matrix <bold>B</bold>:</p>
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="211128_eqn3.gif"/></alternatives>
</disp-formula>
<p>We call the columns of <bold>W</bold> neuron factors, denoted <bold>w</bold><sup><italic>r</italic></sup>, and the columns of <bold>B</bold> temporal factors, denoted <bold>b</bold><sup><italic>r</italic></sup>. The rows of <bold>W</bold>, denoted <bold>w</bold><sub><italic>n</italic></sub>, provide an <italic>R</italic>-dimensional description of each neuron&#x2019;s activity trace. Likewise the rows of <bold>B</bold>, denoted <bold>b</bold><sub><italic>t</italic></sub>, provide an <italic>R</italic>-dimensional description of the full neural population activity pattern at each timepoint. In order to reduce the dimensionality of the data we chose <italic>R &#x003C; N</italic> and <italic>R &#x003C; T</italic>. Note that eq. (3) is equivalent to eq. (1) in the <italic>Results</italic>.</p>
<p>Perhaps the simplest matrix decomposition problem is to identify a rank-<italic>R</italic> decomposition that minimizes the squared reconstruction error:</p>
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="211128_eqn4.gif"/></alternatives>
</disp-formula>
<p>Here, <inline-formula><alternatives><inline-graphic xlink:href="211128_inline9.gif"/></alternatives></inline-formula> denotes the squared <italic>Frobenius norm</italic> of a matrix, which is simply the sum of squared matrix elements:</p>
<disp-formula>
<alternatives><graphic xlink:href="211128_ueqn1.gif"/></alternatives>
</disp-formula>
<p>PCA provides <italic>one</italic> solution to eq. (4). Most critically, the PCA solution constrains the neuron factors and temporal factors to be orthogonal, meaning that <bold>W</bold><sup><italic>T</italic></sup> <bold>W</bold> and <bold>B</bold><sup><italic>T</italic></sup> <bold>B</bold> are diagonal matrices. However, this solution does not uniquely minimize the squared reconstruction error. In fact, there is a continuous manifold of matrix decompositions that solve eq. (4), since any invertible linear transformation <bold>F</bold> can produce a new set of parameters, <bold>W</bold>&#x2032; &#x003D; <bold>WF</bold><sup>&#x2212;1</sup> and <bold>B</bold>&#x2032; &#x003D; <bold>BF</bold><sup><italic>T</italic></sup> that produce an equivalent reconstruction of the data:</p>
<disp-formula id="eqn5">
<alternatives><graphic xlink:href="211128_eqn5.gif"/></alternatives>
</disp-formula>
<p>This result &#x2014; sometimes called the <italic>rotation problem</italic> &#x2014; has a fundamental consequence: if the data were truly generated as a combination of <italic>R</italic> low-dimensional components, then PCA <italic>cannot not recover these ground truth components</italic>. At best, PCA can only be expected to recover the same linear subspace of the true components.</p>
<p>In essence, after fitting a PCA model, one might be tempted to interpret the columns of <bold>W</bold> as identifying subpopulations of neurons with firing patterns given by the columns in <bold>B</bold>. However, eq. (5) shows that these putative sub-populations can be linearly mixed by a broad class of transformations, so long as the components are mixed by the appropriate inverse transformation. Thus, the latent factors identified by PCA are poorly constrained, and it is better to interpret PCA as finding an orthogonal coordinate basis for visualizing data. As reviewed below, the optimization problem addressed by TCA has superior uniqueness properties relative to eq. (4), which gives us greater license to directly interpret the TCA factors as potentially biologically meaningful neural populations and activity patterns.</p>
<p>TCA is a natural generalization of PCA to higher-order tensors. Let <inline-formula><alternatives><inline-graphic xlink:href="211128_inline10.gif"/></alternatives></inline-formula> denote a <italic>N</italic> &#x00D7; <italic>T</italic> &#x00D7; <italic>K</italic> data tensor, and let <italic>x</italic><sub><italic>ntk</italic></sub> represent the activity of neuron <italic>n</italic> at time <italic>t</italic> on trial <italic>k</italic>. For a third-order tensor, TCA finds a set of three factor matrices, <bold>W</bold>, <bold>B</bold>, and <bold>A</bold>, with dimensions <italic>N &#x00D7; R</italic>, <italic>T &#x00D7; R</italic>, and <italic>K &#x00D7; R</italic>, respectively. As before, the columns of <bold>W</bold> are the neuron factors, the columns of <bold>B</bold> are the temporal factors. Analogously, the columns of <bold>A</bold> are the trial factors, denoted <bold>a</bold><sup><italic>r</italic></sup>, and the rows of <bold>A</bold>, denoted <bold>a</bold><sub><italic>k</italic></sub>, embed each trial into an <italic>R</italic>-dimensional space.</p>
<p>To reformulate eq. (2) into an equivalent matrix equation, let <bold>X</bold><sub><italic>k</italic></sub> denote an <italic>N &#x00D7; T</italic> matrix holding the data from trial <italic>k</italic>. TCA models each trial of neural data as:
<disp-formula id="eqn6">
<alternatives><graphic xlink:href="211128_eqn6.gif"/></alternatives>
</disp-formula>
where Diag(<bold>a</bold><sub><italic>k</italic></sub>) embeds <bold>a</bold><sub><italic>k</italic></sub> as the diagonal entries of an <italic>R &#x00D7; R</italic> matrix. Again, eq. (6) is equivalent to eq. (2) in the <italic>Results</italic>. In this paper, we also employed the <italic>nonnegative</italic> TCA model, which simply adds a constraint that all factor matrices have nonnegative elements:</p>
<disp-formula>
<alternatives><graphic xlink:href="211128_ueqn2.gif"/></alternatives>
</disp-formula>
<p>Nonnegative TCA has been previously studied in the tensor decomposition literature [<xref ref-type="bibr" rid="c64">64</xref>, <xref ref-type="bibr" rid="c96">96</xref>&#x2013;<xref ref-type="bibr" rid="c98">98</xref>], and is a higher-order generalization of nonnegative matrix factorization (NNMF) [<xref ref-type="bibr" rid="c60">60</xref>, <xref ref-type="bibr" rid="c99">99</xref>]. Similar to eq. (3), in this paper both unconstrained and nonnegative TCA were fit to minimize the squared reconstruction error:</p>
<disp-formula id="eqn7">
<alternatives><graphic xlink:href="211128_eqn7.gif"/></alternatives>
</disp-formula>
<p>Both PCA and TCA can be extended to incorporate different loss functions, such as a Poisson negative log-likelihood [<xref ref-type="bibr" rid="c100">100</xref>], however we do not consider these models in this paper.</p>
<p>Fitting TCA to data is a nonconvex problem. Unlike PCA, there is no efficient procedure for achieving a certifiably optimal solution [<xref ref-type="bibr" rid="c65">65</xref>]. We use established optimization algorithms to minimize eq. (7) from an initial guess (see <xref ref-type="sec" rid="s4e3">section 4.4.3</xref>). Although this approach may converge to local minima in the objective function, our results empirically suggest that this is not a major practical concern. Indeed, as long we does not choose too many factors (too large an <italic>R</italic>) and use nonnegative factors, we find that the multiple local minima yield similar parameter values and similar reconstruction error.</p>
<p>An important advantage of TCA is that the low-dimensional components it uncovers are often &#x201C;essentially unique,&#x201D; up to permutations and scalings. More precisely, in [<xref ref-type="bibr" rid="c29">29</xref>], it was proven that every local minimum of the TCA objective function is isolated in parameter space; it is not part of a continuous manifold of parameters that achieve <italic>exactly</italic> the same reconstruction error, as in matrix factorization described above. Instead, this continuous degeneracy, or ambiguity is replaced by a much more benign ambiguity, namely a set of solutions with the same reconstruction error related to each other simply by permutations and rescalings. For instance, the columns of <bold>W</bold>, <bold>B</bold>, and <bold>A</bold> can be jointly permuted without affecting the model. Also, the columns of any pair of <bold>W</bold>, <bold>B</bold>, and <bold>A</bold> can be jointly rescaled. For example, if the <italic>r</italic><sup>th</sup> column of <bold>W</bold> is multiplied by a scalar <italic>s</italic>, then the <italic>r</italic><sup>th</sup> column of either <bold>B</bold> or <bold>A</bold> can be divided by <italic>s</italic> without affecting the model&#x2019;s prediction. These transformations, which are also present in PCA, are inconsequential since the direction of the latent factors and total size of any set of factors, rather than their order, are of primary interest. Thus the parameter set corresponding to the global minimum of TCA is essentially unique, up to permutations and scalings. Of course, in general we are not guaranteed to find this global minimum, but as we have shown in the main text, in situations where we do not choose too many factors, all the local minima we find using multiple runs of TCA achieve similarly low reconstruction error, and moreover are close to each other in parameter space. In such a situation, all the local minima likely cluster near the global minimum, and the resultant parameter values are likely to be biologically meaningful, or interpretable.</p>
<p>In summary, when the factors are all linearly independent (i.e., <bold>W</bold>, <bold>B</bold>, and <bold>A</bold> have full column rank), TCA is, in the sense described above, provably unique up to rescalings and permutations [<xref ref-type="bibr" rid="c29">29</xref>]. TCA can nevertheless be difficult to optimize if latent factors are approximately linearly dependent [<xref ref-type="bibr" rid="c101">101</xref>]. To quantify and monitor this possibility, we computed a similarity score between TCA models based on the angles between the extracted factors (see <xref ref-type="sec" rid="s4e1">section 4.5.1</xref>). In practice, we did not find this to be a critical problem.</p>
</sec>
<sec id="s4d3">
<label>4.4.3</label>
<title>Model optimization</title>
<p>TCA can be applied to neural data by a series of simple steps (<xref rid="fig2" ref-type="fig">Figure 2f</xref>). First, to incorporate the common assumption that latent neural firing rates are smooth in time [<xref ref-type="bibr" rid="c16">16</xref>], spiking data can be temporally smoothed (e.g., with a Gaussian filter). The width of this smoothing filter affects the smoothness of the latent temporal factors recovered by TCA. Analogous smoothness hyperparameters are present in other dimensionality reduction methods. For example, in GPFA, the timescale of latent dynamics are set by the autocorrelation in the prior&#x2019;s covariance matrix [<xref ref-type="bibr" rid="c16">16</xref>]. Depending on the dataset, it may be important to apply other common preprocessing steps, such as z-scoring the activity traces of neurons, or applying variance-stabilizing transformations such as taking the square root of spike counts [<xref ref-type="bibr" rid="c12">12</xref>].</p>
<p>Like many dimensionality reduction methods, TCA can only be fit by iterative optimization algorithms. While these procedures may get stuck in sub-optimal local minima, in practice we found that all optimization fits converged to similar reconstruction errors. Other techniques, such as nonnegative matrix factorization [<xref ref-type="bibr" rid="c60">60</xref>], also demonstrate practical success while being NP-hard in terms of worst-case analysis [<xref ref-type="bibr" rid="c102">102</xref>].</p>
<p>Specialized algorithms for fitting TCA are an area of active research. We used the classic method of <italic>alternating least-squares</italic> (ALS) to obtain estimates of the factor matrices. ALS is motivated by the observation that fixing two of the factor matrices and optimizing over the third in eq. (7) is a least-squares subproblem that is convex and has a closed-form solution. For illustration, consider optimizing the neuron factors <bold>W</bold>, while temporarily fixing the within-trial factors, <bold>B</bold>, and the trial factors <bold>A</bold>. This yields the following update rule:
<disp-formula id="eqn8">
<alternatives><graphic xlink:href="211128_eqn8.gif"/></alternatives>
</disp-formula>
which can be solved as a linear least-squares matrix problem. In particular, with some manipulation of the indices, eq. (8) can be rearranged into a matrix equation (see [<xref ref-type="bibr" rid="c33">33</xref>]) and solved by standard matrix library routines. This procedure is then cyclically repeated: the temporal factors <bold>B</bold> are updated while fixing <bold>W</bold> and <bold>A</bold>, then the trial factors <bold>A</bold> are updated while fixing <bold>W</bold> and <bold>B</bold> and so on until the objective function converges. The ALS algorithm is available in several open-source packages [<bold>tensortoolbox2.6</bold>, 94, 95], and is reviewed in [<xref ref-type="bibr" rid="c33">33</xref>]. For nonnegative TCA, we solved each sub-problem using a specialized nonnegative least squares solver [<xref ref-type="bibr" rid="c103">103</xref>], instead of standard least-squares.</p>
</sec>
<sec id="s4d4">
<label>4.4.4</label>
<title>Linear gain-modulated model network</title>
<p>In <xref rid="fig2" ref-type="fig">fig. 2</xref>, we constructed a linear network model with three input neurons connected to <italic>N</italic> &#x003D; 50 observed neurons by random Gaussian weights. The outgoing weights of each input neuron were normalized to unit Euclidean length. Each input neuron had a different temporal firing pattern lasting <italic>T</italic> &#x003D; 150 time steps, parameterized as probability density functions of Gamma distributions. The trial-specific amplitude of the first two input neurons were respectively parameterized as increasing and decreasing logarithmically spaced points over <italic>K</italic> &#x003D; 100 trials. The amplitude of the third input neuron linearly increased for <italic>K &#x003C;</italic> 50 and then linearly decreased to the same starting value. All within-trial waveforms and across-trial amplitude vectors were normalized to unit Euclidean length. As described in the <italic>Results</italic>, the activity of all neurons is modeled by the same equations as TCA (eq. (2)). Independent and identically distributed Gaussian noise with a standard deviation of 0.01 was added to the simulated data. ICA and PCA were performed on this simulated dataset via the scikit-learn Python package [<xref ref-type="bibr" rid="c91">91</xref>].</p>
</sec>
<sec id="s4d5">
<label>4.4.5</label>
<title>Nonlinear recurrent neural network model</title>
<p>We simulated a discrete-time recurrent neural network with a hyperbolic tangent nonlinearity.</p>
<disp-formula id="eqn9">
<alternatives><graphic xlink:href="211128_eqn9.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn10">
<alternatives><graphic xlink:href="211128_eqn10.gif"/></alternatives>
</disp-formula>
<p>Here, <bold>x</bold><sub><italic>t</italic></sub> is a vector of <italic>N</italic> neural firing rates of the recurrently connected neural population at time <italic>t</italic>, <bold>u</bold><sub><italic>t</italic></sub> and <bold>y</bold><sub><italic>t</italic></sub> are the inputs and outputs of the network, <bold>J</bold><sub>rec</sub>, <bold>J</bold><sub>in</sub>, <bold>J</bold><sub>out</sub> are synaptic weight matrices for the recurrent, input, and output connections, and <italic><bold>&#x03B2;</bold></italic> is a <italic>N</italic>-dimensional vector of bias terms. The input and output of the were one-dimensional signals, as illustrated in <xref rid="fig3" ref-type="fig">fig. 3a</xref>. Thus, the recurrent synaptic weights were held in a <italic>N &#x00D7; N</italic> matrix, <bold>J</bold><sub>rec</sub>, the input weights were held in a <italic>N &#x00D7;</italic> 1 matrix, <bold>J</bold><sub>in</sub>, and the output weights were held in a <italic>N &#x00D7;</italic> 1 matrix, <bold>J</bold><sub>out</sub>.</p>
<p>On each trial, the input signal to the network consisted of <italic>T</italic> &#x003D; 40 independent draws from a standard normal distribution with mean <italic>&#x00B5;</italic> &#x003D; 1 or <italic>&#x00B5;</italic> &#x003D; <italic>&#x2212;</italic>1 (chosen randomly with equal probability on each trial). The goal of the network was to produce a positive output (<italic>y<sub>t</sub> &#x003E;</italic> 0) when the input was net-positive, and produce a negative output (<italic>y<sub>t</sub> &#x003C;</italic> 0) when the input was net-negative. The performance of the network on each trial was measured by a logistic loss function (applied to the output on the final time step, <italic>y</italic><sub><italic>T</italic></sub>):</p>
<disp-formula>
<alternatives><graphic xlink:href="211128_ueqn3.gif"/></alternatives>
</disp-formula>
<p>For each simulated trial, we used the deep learning framework PyTorch to compute the gradient of this loss function with respect to all network parameters {<bold>J</bold><sub>rec</sub>, <bold>J</bold><sub>in</sub>, <bold>J</bold><sub>out</sub>, <bold>&#x03B2;</bold>} via the backpropagation through time algorithm. A small parameter update in the direction of the negative gradient for each weight matrix was applied after each trial (stochastic gradient descent, with a learning rate of 0.005). This was repeated for <italic>K</italic> &#x003D; 750 trials. The activity of the recurrent units (<bold>x</bold><sub><italic>t</italic></sub> in eq. (9)) over all timepoints and trials was collected into a <italic>N &#x00D7; T &#x00D7; K</italic> tensor for analysis.</p>
</sec>
<sec id="s4d6">
<label>4.4.6</label>
<title>Mouse spatial navigation task</title>
<p>We injected 500 nL of AAV2/5-CaMKII<italic>&#x03B1;</italic>-GCaMP6m into the medial prefrontal cortex (AP: 1.9, ML: 0.95, DV: 2.25, relative to bregma) into mice aged &#x223C;8 weeks. Approximately one week after virus injection, we installed glass-bottom stainless steel guide tubes into the prefrontal cortex to enable deep brain optical imaging using a 1 mm diameter GRIN microendoscope (1050-002176, Inscopix). Two weeks following guide tube surgery, we checked for cellular Ca<sup>2&#x002B;</sup> signals with a miniaturized fluorescence microscope (nVista HD, Inscopix). Animals with robust Ca<sup>2&#x002B;</sup> responses were selected for further behavioral study. Mice selected for behavioral training underwent water restriction (1 mL per day) to reach &#x223C;85&#x0025; of their <italic>ad libitum</italic> weight.</p>
<p>Mice performed spatial navigation on a custom-built elevated plus maze. The center-to-end arm length of the maze was 38 cm. By blocking one of the arms with an opaque barrier, the plus maze could be converted into a T-maze with any of the four arms as the stem. Additional gates on each of the arms (at &#x223C; 15 cm from the end) could be used to confine the mouse at the arms. At the end of each arm, a proximity sensor enabled detection of the mouse and a water spout allowed for reward delivery. The maze was placed in a rectangular housing whose four side walls were uniquely defined by distinctly patterned curtains.</p>
<p>The mice performed 100-150 trials on each session. At the beginning of each trial, the experimenter placed the mouse in the stem arm of the T-maze with the corresponding gate closed. After 5 s holding time in the stem arm, the &#x201C;start&#x201D; gate was opened to allow the mouse to run to either end of the T-maze. Once the mouse was detected in one of the ends, the &#x201C;end&#x201D; gate was closed behind the mouse to confine it in the chosen arm for another 5 s. If the mouse&#x2019;s choice was consistent with the reward contingency, 5-10 <italic>&#x00B5;</italic>L of water was delivered to the spout. Trained mice typically made the run in 2 s; hence the typical trial was &#x223C;12 s long. At the end of each trial, the experimenter retrieved the mouse and wiped the maze with ethanol.</p>
<p>During trials, we recorded prefrontal Ca<sup>2&#x002B;</sup> activity at 20 Hz using the miniature fluorescence microscope. An overhead camera (DMK 23FV024, The Imaging Source) mounted above the behavioral apparatus synchronously recorded the position of the mouse on the maze. To extract cells and their activity traces from the Ca<sup>2&#x002B;</sup> movies, we followed a procedure previously described in [<xref ref-type="bibr" rid="c5">5</xref>], and we then tracked individual neurons across sessions using previously described methods [<xref ref-type="bibr" rid="c104">104</xref>].</p>
<p>The tensor representation of neural activity requires that the number of samples within each trial be the same for all trials, whereas the mice took a variable amount of time to complete each trial. Hence, we used the largest number of intra-trial samples common to all trials (or, equivalently, the duration of the shortest trial) as the length of the intra-trial time dimension. We chose to temporally align trials to the end of each trial, because the mice showed more consistent behavior across trials at the ends (i.e. approaching the choice arm and consuming reward, if available) rather than the beginnings (where mice could take variable time to initiate motion after opening of the start gate).</p>
<p>Along the trial dimension of the tensor, we simply concatenated trials across days. However, all Ca<sup>2&#x002B;</sup> activity traces were normalized to the range [0, 1] based on the cell&#x2019;s minimum and maximum fluorescence values on each day. This normalization procedure was crucial for forming across-day tensors, since the exact amplitude of a Ca<sup>2&#x002B;</sup> trace was dependent on precise, micron-level axial positioning of the microscope &#x2014; which could vary randomly from session to session.</p>
</sec>
<sec id="s4d7">
<label>4.4.7</label>
<title>Primate BMI task</title>
<p>The monkey&#x2019;s hands were restrained for the full duration of the experiment. Voltage signals were band-pass filtered from each electrode (250 Hz - 7.5 KHz). A spike was recorded whenever these filtered signals crossed below a threshold of &#x2212;4.5 times the root-mean-square voltage.</p>
<p>The neural recordings from PMd and M1 were used jointly and without distinction to train a BMI decoder by the recalibrated feedback-intention trained Kalman filter (ReFIT) procedure [<xref ref-type="bibr" rid="c61">61</xref>]. At the start of each session, the monkey observed 600 trials of automated cursor movements from the center of the workspace to one of 8 radially arranged targets at a distance of 12 cm. During these observation trials, the cursor velocity began at 8 cm/s, and increased by 2 cm/s every 200 trials. Under the premise that the monkey is imagining the intended task during these observation trials, we used the neural activity and cursor kinematics to fit a Kalman filter decoder. The velocity gain of the decoder was calibrated by the experimenter to help the monkey achieve fast reaches (improved by high gain) while still holding the cursor steady (improved by low gain).</p>
<p>The monkey then executed instructed-delay cursor movements to indicated radial target locations, before returning to the center position and repeating the cursor movement to another target. This essential behavioral paradigm has been previously described [<xref ref-type="bibr" rid="c105">105</xref>]. Each target position and the center position were indicated on the screen. Monkeys started by holding the cursor on the central target continuously for 500 ms. After a randomized delay (sampled uniformly from 400-800 ms), monkeys moved the cursor within a 4 &#x00D7; 4 cm acceptance window of the cued target. This target also had to be held continuously for 500 ms. The target changed color to signify the hold period. If the cursor left the acceptance window, the timer was reset, but the trial was not immediately failed. Monkeys had 2 s to acquire the target. Success was accompanied with a liquid reward, along with a success tone. Failure resulted in no reward, and a failure tone. The center target was then presented, which the monkeys also had to acquire and hold.</p>
<p>For our analysis, we collected the non-sorted spiking activity of all <italic>N</italic> &#x003D; 192 multiunit recordings during all center to outward cursor reaches (reaches back to the center were not analyzed). Spike times were aligned to the end of the delay period (<italic>t</italic> &#x003D; 0) and ended at the time of first target acquisition or after two seconds had elapsed and the target was still not required. The data tensor was zero padded to ensure a consistent trial length of two seconds. Data were smoothed within each trial with a Gaussian filter with a standard deviation of 50 ms (same as in [<xref ref-type="bibr" rid="c34">34</xref>]). Using a smaller filter did not qualitatively effect the trial factors extracted by TCA, but resulted in less smooth temporal factors.</p>
</sec>
</sec>
<sec id="s4e">
<label>4.5</label>
<title>Quantification and Statistical Analysis</title>
<sec id="s4e1">
<label>4.5.1</label>
<title>TCA model analysis</title>
<p>Unlike PCA (but similar to ICA and other methods), TCA needs to be iteratively optimized to minimize a cost function. In theory, each optimization run may converge to a sub-optimal local minimum. Additionally, the number of components in the model can affect the final result [<xref ref-type="bibr" rid="c63">63</xref>]. This is different from PCA where the largest components do not change by adding additional components (a consequence of the Eckert-Young theorem; [<xref ref-type="bibr" rid="c106">106</xref>]). Thus, we fit all TCA models from multiple initial parameters and with different numbers of low-dimensional factors. We then inspect this ensemble of models for a consistent and interpretable summary of the data.</p>
<p>The most basic metric to compare models is the squared reconstruction error, since this is what TCA aims to minimize. For interpretability, we normalize the reconstruction error on a scale of zero to one:</p>
<disp-formula id="eqn11">
<alternatives><graphic xlink:href="211128_eqn11.gif"/></alternatives>
</disp-formula>
<p>We typically visualize reconstruction error as a function of the number of model components (see, e.g., <xref rid="fig4" ref-type="fig">fig. 4g</xref>), which we call an &#x201C;error plot.&#x201D;</p>
<p>As discussed in <xref ref-type="sec" rid="s4e2">section 4.4.2</xref>, TCA is invariant to permutations and rescalings of the factors. In PCA, the components are often normalized to unit Euclidean length and ordered by variance explained. An analogous procedure exists for TCA [<xref ref-type="bibr" rid="c33">33</xref>]. First, rescale the columns of <bold>W</bold>, <bold>B</bold>, and <bold>A</bold> to be unit length, and absorb these scalings into <italic>&#x03BB;</italic><sub><italic>r</italic></sub> for each component <italic>r</italic>. Then the estimate of the data becomes:</p>
<disp-formula>
<alternatives><graphic xlink:href="211128_ueqn4.gif"/></alternatives>
</disp-formula>
<p>If desired, the components can be sorted by decreasing <italic>&#x03BB;</italic><sub><italic>r</italic></sub>.</p>
<p>To quantify the similarity of two fitted TCA models, we used a similarity score based on the angles between latent factors [<xref ref-type="bibr" rid="c107">107</xref>]. Formally, for two TCA models, {<bold>W</bold>, <bold>B</bold>, <bold>A</bold>} and {<bold>W</bold>&#x2032;, <bold>B</bold>&#x2032;, <bold>A</bold>&#x2032;}, the similarity score is:</p>
<disp-formula id="eqn12">
<alternatives><graphic xlink:href="211128_eqn12.gif"/></alternatives>
</disp-formula>
<p>Where &#x2126; denotes the set of all permutations of the factors, and <italic>&#x03C9;</italic> is a particular permuation. For example, for a three component model (<italic>R</italic> &#x003D; 3) the score is computed for all possible permutations, <italic>&#x03C9;</italic> &#x003D; {1, 2, 3}, {2, 1, 3}, {3, 2, 1}, and {3, 1, 2}, and the lowest score is taken. For TCA models with more than 10 components, enumerating all permutations can be computationally prohibitive. In these cases we match factors in a greedy fashion to identify a permutation that provides a good (though not certifiably optimal) alignment of the models. Note that this measurement of model similarity is quite severe, since the distance of each pair of factors are multiplied &#x2013; if any single dimension is orthogonal, For our datasets, models with similarity scores above 0.8 were qualitatively similar and led to similar quantitative results in post-hoc analyses. Models with similarity scores within the 0.6&#x2212;0.8 range also appeared quite similar in our applications.</p>
</sec>
<sec id="s4e2">
<label>4.5.2</label>
<title>Mouse spatial navigation task</title>
<p>We quantified the <italic>dimensionality</italic> of a single neuron across trials by the following quantity:
<disp-formula id="eqn13">
<alternatives><graphic xlink:href="211128_eqn13.gif"/></alternatives>
</disp-formula>
where <italic>&#x03BB;</italic><sub><italic>i</italic></sub> are the eigenvalues of the covariance matrix; i.e., <inline-formula><alternatives><inline-graphic xlink:href="211128_inline11.gif"/></alternatives></inline-formula> where <italic>&#x03C3;</italic><sub><italic>i</italic></sub> are the singular values of <bold>X</bold><sup>(<italic>n</italic>)</sup>, which is a <italic>K &#x00D7; T</italic> matrix holding the activity of neuron <italic>n</italic> across all trials. This is a continuous measure of dimensionality used in condensed matter physics, and was previously applied to analyze neural circuits [<xref ref-type="bibr" rid="c108">108</xref>]. For example, (13) reduces to <italic>N</italic> when all the <italic>&#x03BB;</italic><sub><italic>i</italic></sub> are evenly distributed and take the same value, and reduces to 1 if only one <italic>&#x03BB;</italic><sub><italic>i</italic></sub> is nonzero. For uneven distributions of <italic>&#x03BB;</italic><sub><italic>i</italic></sub>, this measure sensibly interpolates between these two extremes.</p>
</sec>
<sec id="s4e3">
<label>4.5.3</label>
<title>Primate BMI task</title>
<p>In <xref rid="fig7" ref-type="fig">fig. 7</xref>, statistical tests on the mean preferred angle of TCA components were performed using PyCircStat (<ext-link ext-link-type="uri" xlink:href="https://github.com/circstat/pycircstat">https://github.com/circstat/pycircstat</ext-link>). Statistical tests on Spearman&#x2019;s rho were computed using the SciPy statistics module (<ext-link ext-link-type="uri" xlink:href="https://docs.scipy.org/doc/scipy-0.14.0/reference/stats.html">https://docs.scipy.org/doc/scipy-0.14.0/reference/stats.html</ext-link>).</p>
</sec>
</sec>
<sec id="s4f">
<label>4.6</label>
<title>Experimental model and subject details</title>
<sec id="s4f1">
<label>4.6.1</label>
<title>Mice</title>
<p>The Stanford Administrative Panel on Laboratory Animal Care approved all mouse procedures. We used male C57BL/6 mice, aged &#x223C;8 weeks at start. Throughout the entire protocol, we monitored the weight daily and looked for signs of distress (e.g., unkempt fur, hunched posture). Mice were habituated to experimenter handling and the behavioral apparatus for &#x223C;2 weeks prior to the five day behavioral protocol.</p>
</sec>
<sec id="s4f2">
<label>4.6.2</label>
<title>Monkey</title>
<p>Recordings were made from motor cortical areas of an adult male monkey, R (<italic>Macaca mulatta</italic>, 15 kg, 12 years old), performing an instructed delay cursor task. The monkey had two chronic 96-electrode arrays (1 mm electrodes, spaced 400 <italic>&#x00B5;</italic>m apart, Blackrock Microsystems), one implanted in the dorsal aspect of the premotor cortex (PMd) and one implanted in the primary motor cortex (M1). The arrays were implanted 5 years prior to these experiments. Animal protocols were approved by the Stanford University Institutional Animal Care and Use Committee.</p>
</sec>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>The authors thank Jeff Seely (Cognescent Corporation) and Casey Battaglino (Georgia Tech) for discussions pertaining to this work. A.H.W. was supported by the Department of Energy Computational Science Graduate Fellowship program. T.H.K. was supported by a Stanford Graduate Fellowship in Science &#x0026; Engineering. F.W. was supported by a National Science Foundation Graduate Research Fellowship. S.V. was supported by a National Science Foundation Graduate Research Fellowship, a Ric Weiland Stanford Graduate Fellowship, National Institutes of Health F31 training grant, and the Stanford Center for Mind, Brain and Computation. K.V.S. was supported by the US National Institutes of Health Director&#x2019;s Pioneer Award 8DP1HD075623, US National Institutes of Health Director&#x2019;s Transformative Research Award (TR01) from the NIMH #5R01MH09964703, Defense Advanced Research Projects Agency NeuroFAST award from BTO #W911NF-14-2-0013, the Simons Foundation, and the Howard Hughes Medical Institute. M.S. was supported by the National Institutes of Health (#1R21NS104833-01), the National Science Foundation (#1707261), and the Howard Hughes Medical Institute. Work by T.G.K. was supported by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program in a grant to Sandia National Laboratories, a multimission laboratory managed and operated by National Technology and Engineering Solutions of Sandia, LLC., a wholly owned subsidiary of Honeywell International, Inc., for the U.S. Department of Energy&#x2019;s National Nuclear Security Administration under contract DE-NA-0003525. S.G. was supported by the Burroughs Wellcome Foundation, the McKnight Foundation, the James S. McDonnnell Foundation, the Simons Foundation, and the Office of Naval Research.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><given-names>JA</given-names> <surname>Kleim</surname></string-name>, <string-name><given-names>S</given-names> <surname>Barbay</surname></string-name>, and <string-name><given-names>RJ</given-names> <surname>Nudo</surname></string-name>. <article-title>&#x201C;Functional Reorganization of the Rat Motor Cortex Following Motor Skill Learning&#x201D;</article-title>. <source>J Neurophysiol</source> <volume>80.6</volume> (<year>1998</year>), pp. <fpage>3321</fpage>&#x2013;<lpage>3325</lpage>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><string-name><given-names>CT</given-names> <surname>Law</surname></string-name> and <string-name><given-names>JI</given-names> <surname>Gold</surname></string-name>. <article-title>&#x201C;Neural correlates of perceptual learning in a sensory-motor, but not a sensory, cortical area&#x201D;</article-title>. <source>Nat Neurosci</source> <volume>11.4</volume> (<year>2008</year>), pp. <fpage>505</fpage>&#x2013;<lpage>513</lpage>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><given-names>AJ</given-names> <surname>Peters</surname></string-name>, <string-name><given-names>SX</given-names> <surname>Chen</surname></string-name>, and <string-name><given-names>T</given-names> <surname>Komiyama</surname></string-name>. <article-title>&#x201C;Emergence of reproducible spatiotemporal activity during motor learning&#x201D;</article-title>. <source>Nature</source> <volume>510.7504</volume> (<year>2014</year>), pp. <fpage>263</fpage>&#x2013;<lpage>267</lpage>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><string-name><given-names>A</given-names> <surname>Marblestone</surname></string-name>, <string-name><given-names>B</given-names> <surname>Zamft</surname></string-name>, <string-name><given-names>Y</given-names> <surname>Maguire</surname></string-name>, <string-name><given-names>M</given-names> <surname>Shapiro</surname></string-name>, <string-name><given-names>T</given-names> <surname>Cybulski</surname></string-name>, <string-name><given-names>J</given-names> <surname>Glaser</surname></string-name>, <string-name><given-names>D</given-names> <surname>Amodei</surname></string-name>, <string-name><given-names>PB</given-names> <surname>Stranges</surname></string-name>, <string-name><given-names>R</given-names> <surname>Kalhor</surname></string-name>, <string-name><given-names>D</given-names> <surname>Dalrymple</surname></string-name>, <string-name><given-names>D</given-names> <surname>Seo</surname></string-name>, <string-name><given-names>E</given-names> <surname>Alon</surname></string-name>, <string-name><given-names>M</given-names> <surname>Maharbiz</surname></string-name>, <string-name><given-names>J</given-names> <surname>Carmena</surname></string-name>, <string-name><given-names>J</given-names> <surname>Rabaey</surname></string-name>, <string-name><given-names>E</given-names> <surname>Boyden</surname></string-name>&#x002A;&#x002A;, <string-name><given-names>G</given-names> <surname>Church</surname></string-name>&#x002A;&#x002A;, and <string-name><given-names>K</given-names> <surname>Kording</surname></string-name>&#x002A;&#x002A;. <article-title>&#x201C;Physical principles for scalable neural recording&#x201D;</article-title>. <source>Front Comput Neurosci</source> <volume>7</volume> (<year>2013</year>), p. <fpage>137</fpage>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><string-name><given-names>TH</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>Y</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>J</given-names> <surname>Lecoq</surname></string-name>, <string-name><given-names>JC</given-names> <surname>Jung</surname></string-name>, <string-name><given-names>J</given-names> <surname>Li</surname></string-name>, <string-name><given-names>H</given-names> <surname>Zeng</surname></string-name>, <string-name><given-names>CM</given-names> <surname>Niell</surname></string-name>, and <string-name><given-names>MJ</given-names> <surname>Schnitzer</surname></string-name>. <article-title>&#x201C;Long-Term Optical Access to an Estimated One Million Neurons in the Live Mouse Cortex&#x201D;</article-title>. <source>Cell Reports</source> <volume>17.12</volume> (<year>2016</year>), pp. <fpage>3385</fpage>&#x2013;<lpage>3394</lpage>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><string-name><given-names>JP</given-names> <surname>Seymour</surname></string-name>, <string-name><given-names>F</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>KD</given-names> <surname>Wise</surname></string-name>, and <string-name><given-names>E</given-names> <surname>Yoon</surname></string-name>. <article-title>&#x201C;State-of-the-art MEMS and microsystem tools for brain research&#x201D;</article-title>. <source>Microsys Nanoeng</source> <volume>3</volume> (<year>2017</year>),</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>Pachitariu</surname></string-name>, <string-name><given-names>C</given-names> <surname>Stringer</surname></string-name>, <string-name><given-names>M</given-names> <surname>Dipoppa</surname></string-name>, <string-name><given-names>S</given-names> <surname>Schr&#x00F6;der</surname></string-name>, <string-name><given-names>LF</given-names> <surname>Rossi</surname></string-name>, <string-name><given-names>H</given-names> <surname>Dalgleish</surname></string-name>, <string-name><given-names>M</given-names> <surname>Carandini</surname></string-name>, and <string-name><given-names>KD</given-names> <surname>Harris</surname></string-name>. <article-title>&#x201C;Suite2p: beyond 10,000 neurons with standard two-photon microscopy&#x201D;</article-title>. <source>bioRxiv</source> (<year>2017</year>).</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><string-name><given-names>MZ</given-names> <surname>Lin</surname></string-name> and <string-name><given-names>MJ</given-names> <surname>Schnitzer</surname></string-name>. <article-title>&#x201C;Genetically encoded indicators of neuronal activity&#x201D;</article-title>. <source>Nat Neurosci</source> <volume>19.9</volume> (<year>2016</year>), pp. <fpage>1142</fpage>&#x2013;<lpage>1153</lpage>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><given-names>H</given-names> <surname>L&#x00FC;tcke</surname></string-name>, <string-name><given-names>DJ</given-names> <surname>Margolis</surname></string-name>, and <string-name><given-names>F</given-names> <surname>Helmchen</surname></string-name>. <article-title>&#x201C;Steady or changing? Long-term monitoring of neuronal population activity&#x201D;</article-title>. <source>Trends in Neurosciences</source> <volume>36.7</volume> (<year>2013</year>), pp. <fpage>375</fpage>&#x2013;<lpage>384</lpage>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><string-name><given-names>AK</given-names> <surname>Dhawale</surname></string-name>, <string-name><given-names>R</given-names> <surname>Poddar</surname></string-name>, <string-name><given-names>E</given-names> <surname>Kopelowitz</surname></string-name>, <string-name><given-names>V</given-names> <surname>Normand</surname></string-name>, <string-name><given-names>S</given-names> <surname>Wolff</surname></string-name>, and <string-name><given-names>B</given-names> <surname>Olveczky</surname></string-name>. <article-title>&#x201C;Automated long-term recording and analysis of neural activity in behaving animals&#x201D;</article-title>. <source>bioRxiv</source> (<year>2016</year>).</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><string-name><given-names>R</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>A</given-names> <surname>Canales</surname></string-name>, and <string-name><given-names>P</given-names> <surname>Anikeeva</surname></string-name>. <article-title>&#x201C;Neural recording and modulation technologies&#x201D;</article-title>. <source>Nature Reviews Materials</source> <volume>2</volume> (<year>2017</year>),</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><string-name><given-names>JP</given-names> <surname>Cunningham</surname></string-name> and <string-name><given-names>BM</given-names> <surname>Yu</surname></string-name>. <article-title>&#x201C;Dimensionality reduction for large-scale neural recordings&#x201D;</article-title>. <source>Nat Neurosci</source> <volume>17.11</volume> (<year>2014</year>), pp. <fpage>1500</fpage>&#x2013;<lpage>1509</lpage>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><string-name><given-names>P</given-names> <surname>Gao</surname></string-name> and <string-name><given-names>S</given-names> <surname>Ganguli</surname></string-name>. <article-title>&#x201C;On simplicity and complexity in the brave new world of large-scale neuroscience&#x201D;</article-title>. <source>Curr Opin Neurbiol</source> <volume>32</volume> (<year>2015</year>), pp. <fpage>148</fpage>&#x2013;<lpage>155</lpage>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><given-names>MB</given-names> <surname>Ahrens</surname></string-name>, <string-name><given-names>JM</given-names> <surname>Li</surname></string-name>, <string-name><given-names>MB</given-names> <surname>Orger</surname></string-name>, <string-name><given-names>DN</given-names> <surname>Robson</surname></string-name>, <string-name><given-names>AF</given-names> <surname>Schier</surname></string-name>, <string-name><given-names>F</given-names> <surname>Engert</surname></string-name>, and <string-name><given-names>R</given-names> <surname>Portugues</surname></string-name>. <article-title>&#x201C;Brain-wide neuronal dynamics during motor adaptation in zebrafish&#x201D;</article-title>. <source>Nature</source> <volume>485.7399</volume> (<year>2012</year>), pp. <fpage>471</fpage>&#x2013;<lpage>477</lpage>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><string-name><given-names>MM</given-names> <surname>Churchland</surname></string-name>, <string-name><given-names>JP</given-names> <surname>Cunningham</surname></string-name>, <string-name><given-names>MT</given-names> <surname>Kaufman</surname></string-name>, <string-name><given-names>JD</given-names> <surname>Foster</surname></string-name>, <string-name><given-names>P</given-names> <surname>Nuyujukian</surname></string-name>, <string-name><given-names>SI</given-names> <surname>Ryu</surname></string-name>, and <string-name><given-names>KV</given-names> <surname>Shenoy</surname></string-name>. <article-title>&#x201C;Neural population dynamics during reaching&#x201D;</article-title>. <source>Nature</source> <volume>487.7405</volume> (<year>2012</year>), pp. <fpage>51</fpage>&#x2013;<lpage>56</lpage>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><string-name><given-names>BM</given-names> <surname>Yu</surname></string-name>, <string-name><given-names>JP</given-names> <surname>Cunningham</surname></string-name>, <string-name><given-names>G</given-names> <surname>Santhanam</surname></string-name>, <string-name><given-names>SI</given-names> <surname>Ryu</surname></string-name>, <string-name><given-names>KV</given-names> <surname>Shenoy</surname></string-name>, and <string-name><given-names>M</given-names> <surname>Sahani</surname></string-name>. <article-title>&#x201C;Gaussian-Process Factor Analysis for Low-Dimensional Single-Trial Analysis of Neural Population Activity&#x201D;</article-title>. <source>J Neurophysiol</source> <volume>102.1</volume> (<year>2009</year>), pp. <fpage>614</fpage>&#x2013;<lpage>635</lpage>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="book"><string-name><given-names>Y</given-names> <surname>Gao</surname></string-name>, <string-name><given-names>EW</given-names> <surname>Archer</surname></string-name>, <string-name><given-names>L</given-names> <surname>Paninski</surname></string-name>, and <string-name><given-names>JP</given-names> <surname>Cunningham</surname></string-name>. <chapter-title>&#x201C;Linear dynamical neural population models through nonlinear embeddings&#x201D;</chapter-title>. <source>Advances in Neural Information Processing Systems</source> <volume>29</volume>. Ed. by <person-group person-group-type="editor"><string-name><given-names>DD</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>M</given-names> <surname>Sugiyama</surname></string-name>, <string-name><given-names>UV</given-names> <surname>Luxburg</surname></string-name>, <string-name><given-names>I</given-names> <surname>Guyon</surname></string-name>, and <string-name><given-names>R</given-names> <surname>Garnett</surname></string-name></person-group>. <publisher-name>Curran Associates, Inc.</publisher-name>, <year>2016</year>, pp. <fpage>163</fpage>&#x2013;<lpage>171</lpage>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="other"><string-name><given-names>C</given-names> <surname>Pandarinath</surname></string-name>, <string-name><given-names>DJ</given-names> <surname>O&#x2019;Shea</surname></string-name>, <string-name><given-names>J</given-names> <surname>Collins</surname></string-name>, <string-name><given-names>R</given-names> <surname>Jozefowicz</surname></string-name>, <string-name><given-names>SD</given-names> <surname>Stavisky</surname></string-name>, <string-name><given-names>JC</given-names> <surname>Kao</surname></string-name>, <string-name><given-names>EM</given-names> <surname>Trautmann</surname></string-name>, <string-name><given-names>MT</given-names> <surname>Kaufman</surname></string-name>, <string-name><given-names>SI</given-names> <surname>Ryu</surname></string-name>, <string-name><given-names>LR</given-names> <surname>Hochberg</surname></string-name>, <string-name><given-names>JM</given-names> <surname>Henderson</surname></string-name>, <string-name><given-names>KV</given-names> <surname>Shenoy</surname></string-name>, <string-name><given-names>LF</given-names> <surname>Abbott</surname></string-name>, and <string-name><given-names>D</given-names> <surname>Sussillo</surname></string-name>. <article-title>&#x201C;Inferring single-trial neural population dynamics using sequential auto-encoders&#x201D;</article-title>. <source>bioRxiv</source> (<year>2017</year>).</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><string-name><given-names>BB</given-names> <surname>Averbeck</surname></string-name>, <string-name><given-names>PE</given-names> <surname>Latham</surname></string-name>, and <string-name><given-names>A</given-names> <surname>Pouget</surname></string-name>. <article-title>&#x201C;Neural correlations, population coding and computation&#x201D;</article-title>. <source>Nat Rev Neurosci</source> <volume>7.5</volume> (<year>2006</year>), pp. <fpage>358</fpage>&#x2013;<lpage>366</lpage>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><string-name><given-names>MR</given-names> <surname>Cohen</surname></string-name> and <string-name><given-names>JHR</given-names> <surname>Maunsell</surname></string-name>. <article-title>&#x201C;A Neuronal Population Measure of Attention Predicts Behavioral Performance on Individual Trials&#x201D;</article-title>. <source>J Neurosci</source> <volume>30.45</volume> (<year>2010</year>), pp. <fpage>15241</fpage>&#x2013;<lpage>15253</lpage>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><string-name><given-names>MR</given-names> <surname>Cohen</surname></string-name> and <string-name><given-names>JHR</given-names> <surname>Maunsell</surname></string-name>. <article-title>&#x201C;When Attention Wanders: How Uncontrolled Fluctuations in Attention Affect Performance&#x201D;</article-title>. <source>J Neurosci</source> <volume>31.44</volume> (<year>2011</year>), pp. <fpage>15802</fpage>&#x2013;<lpage>15806</lpage>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><string-name><given-names>RLT</given-names> <surname>Goris</surname></string-name>, <string-name><given-names>JA</given-names> <surname>Movshon</surname></string-name>, and <string-name><given-names>EP</given-names> <surname>Simoncelli</surname></string-name>. <article-title>&#x201C;Partitioning neuronal variability&#x201D;</article-title>. <source>Nat Neurosci</source> <volume>17.6</volume> (<year>2014</year>), pp. <fpage>858</fpage>&#x2013;<lpage>865</lpage>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><string-name><given-names>K</given-names> <surname>Ganguly</surname></string-name> and <string-name><given-names>JM</given-names> <surname>Carmena</surname></string-name>. <article-title>&#x201C;Emergence of a Stable Cortical Map for Neuroprosthetic Control&#x201D;</article-title>. <source>PLOS Biol</source> <volume>7.7</volume> (<year>2009</year>), pp. <fpage>1</fpage>&#x2013;<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><string-name><given-names>KB</given-names> <surname>Clancy</surname></string-name>, <string-name><given-names>AC</given-names> <surname>Koralek</surname></string-name>, <string-name><given-names>RM</given-names> <surname>Costa</surname></string-name>, <string-name><given-names>DE</given-names> <surname>Feldman</surname></string-name>, and <string-name><given-names>JM</given-names> <surname>Carmena</surname></string-name>. <article-title>&#x201C;Volitional modulation of optically recorded calcium signals during neuroprosthetic learning&#x201D;</article-title>. <source>Nat Neurosci</source> <volume>17.6</volume> (<year>2014</year>), pp. <fpage>807</fpage>&#x2013;<lpage>809</lpage>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="other"><string-name><given-names>MJ</given-names> <surname>Siniscalchi</surname></string-name>, <string-name><given-names>V</given-names> <surname>Phoumthipphavong</surname></string-name>, <string-name><given-names>F</given-names> <surname>Ali</surname></string-name>, <string-name><given-names>M</given-names> <surname>Lozano</surname></string-name>, and <string-name><given-names>AC</given-names> <surname>Kwan</surname></string-name>. <article-title>&#x201C;Fast and slow transitions in frontal ensemble activity during flexible sensorimotor behavior&#x201D;</article-title>. <source>Nat Neurosci advance online publication</source> (<year>2016</year>),</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><string-name><given-names>LN</given-names> <surname>Driscoll</surname></string-name>, <string-name><given-names>NL</given-names> <surname>Pettit</surname></string-name>, <string-name><given-names>M</given-names> <surname>Minderer</surname></string-name>, <string-name><given-names>SN</given-names> <surname>Chettih</surname></string-name>, and <string-name><given-names>CD</given-names> <surname>Harvey</surname></string-name>. <article-title>&#x201C;Dynamic Reorganization of Neuronal Activity Patterns in Parietal Cortex&#x201D;</article-title>. <source>Cell</source> (<year>2017</year>).</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><string-name><given-names>JD</given-names> <surname>Carroll</surname></string-name> and <string-name><given-names>JJ</given-names> <surname>Chang</surname></string-name>. <article-title>&#x201C;Analysis of individual differences in multidimensional scaling via an n-way generalization of &#x201C;Eckart-Young&#x201D; decomposition&#x201D;</article-title>. <source>Psychometrika</source> <volume>35.3</volume> (<year>1970</year>), pp. <fpage>283</fpage>&#x2013;<lpage>319</lpage>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><string-name><given-names>RA</given-names> <surname>Harshman</surname></string-name>. <article-title>&#x201C;Foundations of the PARAFAC procedure: Models and conditions for an explanatory multi-modal factor analysis&#x201D;</article-title>. <source>UCLA Working Papers in Phonetics</source> <volume>16</volume> (<year>1970</year>), pp. <fpage>1</fpage>&#x2013;<lpage>84</lpage>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><string-name><given-names>JB</given-names> <surname>Kruskal</surname></string-name>. <article-title>&#x201C;Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics&#x201D;</article-title>. <source>Linear Algebra and its Applications</source> <volume>18.2</volume> (<year>1977</year>), pp. <fpage>95</fpage>&#x2013;<lpage>138</lpage>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><string-name><given-names>E</given-names> <surname>Salinas</surname></string-name> and <string-name><given-names>P</given-names> <surname>Thier</surname></string-name>. <article-title>&#x201C;Gain Modulation: A Major Computational Principle of the Central Nervous System&#x201D;</article-title>. <source>Neuron</source> <volume>27.1</volume> (<year>2000</year>), pp. <fpage>15</fpage>&#x2013;<lpage>21</lpage>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>Carandini</surname></string-name> and <string-name><given-names>DJ</given-names> <surname>Heeger</surname></string-name>. <article-title>&#x201C;Normalization as a canonical neural computation&#x201D;</article-title>. <source>Nat Rev Neurosci</source> <volume>13.1</volume> (<year>2012</year>), pp. <fpage>51</fpage>&#x2013;<lpage>62</lpage>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><string-name><given-names>K</given-names> <surname>Britten</surname></string-name>, <string-name><given-names>M</given-names> <surname>Shadlen</surname></string-name>, <string-name><given-names>W</given-names> <surname>Newsome</surname></string-name>, and <string-name><given-names>J</given-names> <surname>Movshon</surname></string-name>. <article-title>&#x201C;The analysis of visual motion: a comparison of neuronal and psychophysical performance&#x201D;</article-title>. <source>J Neurosci</source> <volume>12.12</volume> (<year>1992</year>), pp. <fpage>4745</fpage>&#x2013;<lpage>4765</lpage>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><string-name><given-names>TG</given-names> <surname>Kolda</surname></string-name> and <string-name><given-names>BW</given-names> <surname>Bader</surname></string-name>. <article-title>&#x201C;Tensor Decompositions and Applications&#x201D;</article-title>. <source>SIAM Review</source> <volume>51.3</volume> (<year>2009</year>), pp. <fpage>455</fpage>&#x2013;<lpage>500</lpage>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><string-name><given-names>D</given-names> <surname>Kobak</surname></string-name>, <string-name><given-names>W</given-names> <surname>Brendel</surname></string-name>, <string-name><given-names>C</given-names> <surname>Constantinidis</surname></string-name>, <string-name><given-names>CE</given-names> <surname>Feierstein</surname></string-name>, <string-name><given-names>A</given-names> <surname>Kepecs</surname></string-name>, <string-name><given-names>ZF</given-names> <surname>Mainen</surname></string-name>, <string-name><given-names>XL</given-names> <surname>Qi</surname></string-name>, <string-name><given-names>R</given-names> <surname>Romo</surname></string-name>, <string-name><given-names>N</given-names> <surname>Uchida</surname></string-name>, and <string-name><given-names>CK</given-names> <surname>Machens</surname></string-name>. <article-title>&#x201C;Demixed principal component analysis of neural population data&#x201D;</article-title>. <source>eLife</source> <volume>5</volume> (<year>2016</year>), e10989.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><string-name><given-names>I</given-names> <surname>Dean</surname></string-name>, <string-name><given-names>NS</given-names> <surname>Harper</surname></string-name>, and <string-name><given-names>D</given-names> <surname>McAlpine</surname></string-name>. <article-title>&#x201C;Neural population coding of sound level adapts to stimulus statistics&#x201D;</article-title>. <source>Nat Neurosci</source> <volume>8.12</volume> (<year>2005</year>), pp. <fpage>1684</fpage>&#x2013;<lpage>1689</lpage>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><string-name><given-names>CM</given-names> <surname>Niell</surname></string-name> and <string-name><given-names>MP</given-names> <surname>Stryker</surname></string-name>. <article-title>&#x201C;Modulation of visual responses by behavioral state in mouse visual cortex&#x201D;</article-title>. <source>Neuron</source> <volume>65.4</volume> (<year>2010</year>), pp. <fpage>472</fpage>&#x2013;<lpage>479</lpage>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><string-name><given-names>HK</given-names> <surname>Kato</surname></string-name>, <string-name><given-names>SN</given-names> <surname>Gillet</surname></string-name>, <string-name><given-names>AJ</given-names> <surname>Peters</surname></string-name>, <string-name><given-names>JS</given-names> <surname>Isaacson</surname></string-name>, and <string-name><given-names>T</given-names> <surname>Komiyama</surname></string-name>. <article-title>&#x201C;Parvalbumin-Expressing Interneurons Linearly Control Olfactory Bulb Output&#x201D;</article-title>. <source>Neuron</source> <volume>80.5</volume> (<year>2013</year>), pp. <fpage>1218</fpage>&#x2013;<lpage>1231</lpage>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><string-name><given-names>FS</given-names> <surname>Chance</surname></string-name>, <string-name><given-names>L</given-names> <surname>Abbott</surname></string-name>, and <string-name><given-names>AD</given-names> <surname>Reyes</surname></string-name>. <article-title>&#x201C;Gain modulation from background synaptic input&#x201D;</article-title>. <source>Neuron</source> <volume>35.4</volume> (<year>2002</year>), pp. <fpage>773</fpage>&#x2013;<lpage>782</lpage>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><string-name><given-names>SA</given-names> <surname>Prescott</surname></string-name> and <string-name><given-names>Y</given-names> <surname>De Koninck</surname></string-name>. <article-title>&#x201C;Gain control of firing rate by shunting inhibition: Roles of synaptic noise and dendritic saturation&#x201D;</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>100.4</volume> (<year>2003</year>), pp. <fpage>2076</fpage>&#x2013;<lpage>2081</lpage>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><string-name><given-names>JA</given-names> <surname>Cardin</surname></string-name>, <string-name><given-names>LA</given-names> <surname>Palmer</surname></string-name>, and <string-name><given-names>D</given-names> <surname>Contreras</surname></string-name>. <article-title>&#x201C;Cellular mechanisms underlying stimulus-dependent gain modulation in primary visual cortex neurons in vivo&#x201D;</article-title>. <source>Neuron</source> <volume>59.1</volume> (<year>2008</year>), pp. <fpage>150</fpage>&#x2013;<lpage>160</lpage>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><string-name><given-names>FR</given-names> <surname>Fernandez</surname></string-name> and <string-name><given-names>JA</given-names> <surname>White</surname></string-name>. <article-title>&#x201C;Gain control in CA1 pyramidal cells using changes in somatic conductance&#x201D;</article-title>. <source>J Neurosci</source> <volume>30.1</volume> (<year>2010</year>), pp. <fpage>230</fpage>&#x2013;<lpage>241</lpage>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="journal"><string-name><given-names>AJ</given-names> <surname>Bell</surname></string-name> and <string-name><given-names>TJ</given-names> <surname>Sejnowski</surname></string-name>. <article-title>&#x201C;An information-maximization approach to blind separation and blind deconvolution&#x201D;</article-title>. <source>Neural computation</source> <volume>7.6</volume> (<year>1995</year>), pp. <fpage>1129</fpage>&#x2013;<lpage>1159</lpage>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><string-name><given-names>K</given-names> <surname>Funahashi</surname></string-name> and <string-name><given-names>Y</given-names> <surname>Nakamura</surname></string-name>. <article-title>&#x201C;Approximation of dynamical systems by continuous time recurrent neural networks&#x201D;</article-title>. <source>Neural Networks</source> <volume>6.6</volume> (<year>1993</year>), pp. <fpage>801</fpage>&#x2013;<lpage>806</lpage>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="other"><string-name><given-names>A</given-names> <surname>Graves</surname></string-name>, <string-name><given-names>A r.</given-names> <surname>Mohamed</surname></string-name>, and <string-name><given-names>G</given-names> <surname>Hinton</surname></string-name>. <article-title>&#x201C;Speech recognition with deep recurrent neural networks&#x201D;</article-title>. <source>2013 IEEE International Conference on Acoustics, Speech and Signal Processing</source>. <year>2013</year>, pp. <fpage>6645</fpage>&#x2013;<lpage>6649</lpage>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><string-name><given-names>V</given-names> <surname>Mante</surname></string-name>, <string-name><given-names>D</given-names> <surname>Sussillo</surname></string-name>, <string-name><given-names>KV</given-names> <surname>Shenoy</surname></string-name>, and <string-name><given-names>WT</given-names> <surname>Newsome</surname></string-name>. <article-title>&#x201C;Context-dependent computation by recurrent dynamics in prefrontal cortex&#x201D;</article-title>. <source>Nature</source> <volume>503.7474</volume> (<year>2013</year>), pp. <fpage>78</fpage>&#x2013;<lpage>84</lpage>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><string-name><given-names>R</given-names> <surname>Laje</surname></string-name> and <string-name><given-names>DV</given-names> <surname>Buonomano</surname></string-name>. <article-title>&#x201C;Robust timing and motor patterns by taming chaos in recurrent neural networks&#x201D;</article-title>. <source>Nat Neurosci</source> <volume>16.7</volume> (<year>2013</year>), pp. <fpage>925</fpage>&#x2013;<lpage>933</lpage>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="journal"><string-name><given-names>HF</given-names> <surname>Song</surname></string-name>, <string-name><given-names>GR</given-names> <surname>Yang</surname></string-name>, and <string-name><given-names>XJ</given-names> <surname>Wang</surname></string-name>. <article-title>&#x201C;Training Excitatory-Inhibitory Recurrent Neural Networks for Cognitive Tasks: A Simple and Flexible Framework&#x201D;</article-title>. <source>PLOS Comput Biol</source> <volume>12.2</volume> (<year>2016</year>), pp. <fpage>1</fpage>&#x2013;<lpage>30</lpage>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><string-name><given-names>HF</given-names> <surname>Song</surname></string-name>, <string-name><given-names>GR</given-names> <surname>Yang</surname></string-name>, and <string-name><given-names>XJ</given-names> <surname>Wang</surname></string-name>. <article-title>&#x201C;Reward-based training of recurrent neural networks for cognitive and value-based tasks&#x201D;</article-title>. <source>eLife</source> <volume>6</volume> (<year>2017</year>). Ed. by <person-group person-group-type="editor"><string-name><given-names>TE</given-names> <surname>Behrens</surname></string-name></person-group>, <fpage>e21492</fpage>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><string-name><given-names>D</given-names> <surname>Sussillo</surname></string-name>. <article-title>&#x201C;Neural circuits as computational dynamical systems&#x201D;</article-title>. <source>Curr Opin Neurbiol</source> <volume>25</volume> (<year>2014</year>), pp. <fpage>156</fpage>&#x2013;<lpage>163</lpage>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><string-name><given-names>E</given-names> <surname>Jonas</surname></string-name> and <string-name><given-names>KP</given-names> <surname>Kording</surname></string-name>. <article-title>&#x201C;Could a Neuroscientist Understand a Microprocessor?&#x201D;</article-title> <source>PLOS Comput Biol</source> <volume>13.1</volume> (<year>2017</year>), pp. <fpage>1</fpage>&#x2013;<lpage>24</lpage>.</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><string-name><given-names>D</given-names> <surname>Sussillo</surname></string-name> and <string-name><given-names>O</given-names> <surname>Barak</surname></string-name>. <article-title>&#x201C;Opening the Black Box: Low-Dimensional Dynamics in High-Dimensional Recurrent Neural Networks&#x201D;</article-title>. <source>Neural Comput</source> <volume>25.3</volume> (<year>2013</year>), pp. <fpage>626</fpage>&#x2013;<lpage>649</lpage>.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="journal"><string-name><given-names>PJ</given-names> <surname>Werbos</surname></string-name>. <article-title>&#x201C;Backpropagation through time: what it does and how to do it&#x201D;</article-title>. <source>Proceedings of the IEEE</source> <volume>78.10</volume> (<year>1990</year>), pp. <fpage>1550</fpage>&#x2013;<lpage>1560</lpage>.</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="journal"><string-name><given-names>HS</given-names> <surname>Seung</surname></string-name>. <article-title>&#x201C;How the brain keeps the eyes still&#x201D;</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>93.23</volume> (<year>1996</year>), pp. <fpage>13339</fpage>&#x2013;<lpage>13344</lpage>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="journal"><string-name><given-names>KK</given-names> <surname>Ghosh</surname></string-name>, <string-name><given-names>LD</given-names> <surname>Burns</surname></string-name>, <string-name><given-names>ED</given-names> <surname>Cocker</surname></string-name>, <string-name><given-names>A</given-names> <surname>Nimmerjahn</surname></string-name>, <string-name><given-names>Y</given-names> <surname>Ziv</surname></string-name>, <string-name><given-names>AE</given-names> <surname>Gamal</surname></string-name>, and <string-name><given-names>MJ</given-names> <surname>Schnitzer</surname></string-name>. <article-title>&#x201C;Miniaturized integration of a fluorescence microscope&#x201D;</article-title>. <source>Nat Meth</source> <volume>8.10</volume> (<year>2011</year>), pp. <fpage>871</fpage>&#x2013;<lpage>878</lpage>.</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="journal"><string-name><given-names>EL</given-names> <surname>Rich</surname></string-name> and <string-name><given-names>M</given-names> <surname>Shapiro</surname></string-name>. <article-title>&#x201C;Rat Prefrontal Cortical Neurons Selectively Code Strategy Switches&#x201D;</article-title>. <source>J Neurosci</source> <volume>29.22</volume> (<year>2009</year>), pp. <fpage>7208</fpage>&#x2013;<lpage>7219</lpage>.</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="journal"><string-name><given-names>D</given-names> <surname>Durstewitz</surname></string-name>, <string-name><given-names>NM</given-names> <surname>Vittoz</surname></string-name>, <string-name><given-names>SB</given-names> <surname>Floresco</surname></string-name>, and <string-name><given-names>JK</given-names> <surname>Seamans</surname></string-name>. <article-title>&#x201C;Abrupt Transitions between Prefrontal Neural Ensemble States Accompany Behavioral Transitions during Rule Learning&#x201D;</article-title>. <source>Neuron</source> <volume>66.3</volume> (<year>2010</year>), pp. <fpage>438</fpage>&#x2013;<lpage>448</lpage>.</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="journal"><string-name><given-names>JD</given-names> <surname>Wallis</surname></string-name> and <string-name><given-names>SW</given-names> <surname>Kennerley</surname></string-name>. <article-title>&#x201C;Heterogeneous reward signals in prefrontal cortex&#x201D;</article-title>. <source>Curr Opin Neurbiol</source> <volume>20.2</volume> (<year>2010</year>), pp. <fpage>191</fpage>&#x2013;<lpage>198</lpage>.</mixed-citation></ref>
<ref id="c58"><label>[58]</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>Rigotti</surname></string-name>, <string-name><given-names>O</given-names> <surname>Barak</surname></string-name>, <string-name><given-names>MR</given-names> <surname>Warden</surname></string-name>, <string-name><given-names>XJ</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>ND</given-names> <surname>Daw</surname></string-name>, <string-name><given-names>EK</given-names> <surname>Miller</surname></string-name>, and <string-name><given-names>S</given-names> <surname>Fusi</surname></string-name>. <article-title>&#x201C;The importance of mixed selectivity in complex cognitive tasks&#x201D;</article-title>. <source>Nature</source> <volume>497.7451</volume> (<year>2013</year>), pp. <fpage>585</fpage>&#x2013;<lpage>590</lpage>.</mixed-citation></ref>
<ref id="c59"><label>[59]</label><mixed-citation publication-type="journal"><string-name><given-names>MG</given-names> <surname>Stokes</surname></string-name>, <string-name><given-names>M</given-names> <surname>Kusunoki</surname></string-name>, <string-name><given-names>N</given-names> <surname>Sigala</surname></string-name>, <string-name><given-names>H</given-names> <surname>Nili</surname></string-name>, <string-name><given-names>D</given-names> <surname>Gaffan</surname></string-name>, and <string-name><given-names>J</given-names> <surname>Duncan</surname></string-name>. <article-title>&#x201C;Dynamic Coding for Cognitive Control in Prefrontal Cortex&#x201D;</article-title>. <source>Neuron</source> <volume>78.2</volume> (<year>2013</year>), pp. <fpage>364</fpage>&#x2013;<lpage>375</lpage>.</mixed-citation></ref>
<ref id="c60"><label>[60]</label><mixed-citation publication-type="journal"><string-name><given-names>DD</given-names> <surname>Lee</surname></string-name> and <string-name><given-names>HS</given-names> <surname>Seung</surname></string-name>. <article-title>&#x201C;Learning the parts of objects by non-negative matrix factorization&#x201D;</article-title>. <source>Nature</source> <volume>401.6755</volume> (<year>1999</year>), pp. <fpage>788</fpage>&#x2013;<lpage>791</lpage>.</mixed-citation></ref>
<ref id="c61"><label>[61]</label><mixed-citation publication-type="journal"><string-name><given-names>V</given-names> <surname>Gilja</surname></string-name>, <string-name><given-names>P</given-names> <surname>Nuyujukian</surname></string-name>, <string-name><given-names>CA</given-names> <surname>Chestek</surname></string-name>, <string-name><given-names>JP</given-names> <surname>Cunningham</surname></string-name>, <string-name><given-names>BM</given-names> <surname>Yu</surname></string-name>, <string-name><given-names>JM</given-names> <surname>Fan</surname></string-name>, <string-name><given-names>MM</given-names> <surname>Churchland</surname></string-name>, <string-name><given-names>MT</given-names> <surname>Kaufman</surname></string-name>, <string-name><given-names>JC</given-names> <surname>Kao</surname></string-name>, <string-name><given-names>SI</given-names> <surname>Ryu</surname></string-name>, and <string-name><given-names>KV</given-names> <surname>Shenoy</surname></string-name>. <article-title>&#x201C;A high-performance neural prosthesis enabled by control algorithm design&#x201D;</article-title>. <source>Nat Neurosci</source> <volume>15.12</volume> (<year>2012</year>), pp. <fpage>1752</fpage>&#x2013;<lpage>1757</lpage>.</mixed-citation></ref>
<ref id="c62"><label>[62]</label><mixed-citation publication-type="journal"><string-name><given-names>A</given-names> <surname>Georgopoulos</surname></string-name>, <string-name><given-names>J</given-names> <surname>Kalaska</surname></string-name>, <string-name><given-names>R</given-names> <surname>Caminiti</surname></string-name>, and <string-name><given-names>J</given-names> <surname>Massey</surname></string-name>. <source>&#x201C;On the relations between the direction of two-dimensional arm movements and cell discharge in primate motor cortex&#x201D;</source>. <string-name><given-names>J</given-names> <surname>Neurosci</surname></string-name> <volume>2.11</volume> (<year>1982</year>), pp. <fpage>1527</fpage>&#x2013;<lpage>1537</lpage>.</mixed-citation></ref>
<ref id="c63"><label>[63]</label><mixed-citation publication-type="journal"><string-name><given-names>TG</given-names> <surname>Kolda</surname></string-name>. <article-title>&#x201C;A Counterexample to the Possibility of an Extension of the Eckart&#x2013;Young Low-Rank Approximation Theorem for the Orthogonal Rank Tensor Decomposition&#x201D;</article-title>. <source>SIAM J Matrix Anal Appl</source> <volume>24.3</volume> (<year>2003</year>), pp. <fpage>762</fpage>&#x2013;<lpage>767</lpage>.</mixed-citation></ref>
<ref id="c64"><label>[64]</label><mixed-citation publication-type="journal"><string-name><given-names>LH</given-names> <surname>Lim</surname></string-name> and <string-name><given-names>P</given-names> <surname>Comon</surname></string-name>. <article-title>&#x201C;Nonnegative approximations of nonnegative tensors&#x201D;</article-title>. <source>J Chemometrics</source> <volume>23.7-8</volume> (<year>2009</year>), pp. <fpage>432</fpage>&#x2013;<lpage>441</lpage>.</mixed-citation></ref>
<ref id="c65"><label>[65]</label><mixed-citation publication-type="journal"><string-name><given-names>CJ</given-names> <surname>Hillar</surname></string-name> and <string-name><given-names>LH</given-names> <surname>Lim</surname></string-name>. <article-title>&#x201C;Most Tensor Problems Are NP-Hard&#x201D;</article-title>. <source>J. ACM</source> 60.6 (<year>2013</year>), <volume>45</volume>:<issue>1&#x2013;45</issue>:<fpage>39</fpage>.</mixed-citation></ref>
<ref id="c66"><label>[66]</label><mixed-citation publication-type="journal"><string-name><given-names>L</given-names> <surname>Omberg</surname></string-name>, <string-name><given-names>GH</given-names> <surname>Golub</surname></string-name>, and <string-name><given-names>O</given-names> <surname>Alter</surname></string-name>. <article-title>&#x201C;A tensor higher-order singular value decomposition for integrative analysis of DNA microarray data from different studies&#x201D;</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>104.47</volume> (<year>2007</year>), pp. <fpage>18371</fpage>&#x2013;<lpage>18376</lpage>.</mixed-citation></ref>
<ref id="c67"><label>[67]</label><mixed-citation publication-type="journal"><string-name><given-names>DA</given-names> <surname>Cartwright</surname></string-name>, <string-name><given-names>SM</given-names> <surname>Brady</surname></string-name>, <string-name><given-names>DA</given-names> <surname>Orlando</surname></string-name>, <string-name><given-names>B</given-names> <surname>Sturmfels</surname></string-name>, and <string-name><given-names>PN</given-names> <surname>Benfey</surname></string-name>. <article-title>&#x201C;Reconstructing spatiotemporal gene expression data from partial observations&#x201D;</article-title>. <source>Bioinformatics</source> <volume>25.19</volume> (<year>2009</year>), p. <fpage>2581</fpage>.</mixed-citation></ref>
<ref id="c68"><label>[68]</label><mixed-citation publication-type="other"><string-name><given-names>V</given-names> <surname>Hore</surname></string-name>, <string-name><given-names>A</given-names> <surname>Vinuela</surname></string-name>, <string-name><given-names>A</given-names> <surname>Buil</surname></string-name>, <string-name><given-names>J</given-names> <surname>Knight</surname></string-name>, <string-name><given-names>MI</given-names> <surname>McCarthy</surname></string-name>, <string-name><given-names>K</given-names> <surname>Small</surname></string-name>, and <string-name><given-names>J</given-names> <surname>Marchini</surname></string-name>. <article-title>&#x201C;Tensor decomposition for multiple-tissue gene expression experiments&#x201D;</article-title>. <source>Nat Genet advance online publication</source> (<year>2016</year>),</mixed-citation></ref>
<ref id="c69"><label>[69]</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>M&#x00F8;rup</surname></string-name>, <string-name><given-names>LK</given-names> <surname>Hansen</surname></string-name>, <string-name><given-names>CS</given-names> <surname>Herrmann</surname></string-name>, <string-name><given-names>J</given-names> <surname>Parnas</surname></string-name>, and <string-name><given-names>SM</given-names> <surname>Arnfred</surname></string-name>. <article-title>&#x201C;Parallel factor analysis as an exploratory tool for wavelet transformed event-related EEG&#x201D;</article-title>. <source>NeuroImage</source> <volume>29.3</volume> (<year>2006</year>), pp. <fpage>938</fpage>&#x2013;<lpage>947</lpage>.</mixed-citation></ref>
<ref id="c70"><label>[70]</label><mixed-citation publication-type="journal"><string-name><given-names>E</given-names> <surname>Acar</surname></string-name>, <string-name><given-names>C</given-names> <surname>Aykut-Bingol</surname></string-name>, <string-name><given-names>H</given-names> <surname>Bingol</surname></string-name>, <string-name><given-names>R</given-names> <surname>Bro</surname></string-name>, and <string-name><given-names>B</given-names> <surname>Yener</surname></string-name>. <article-title>&#x201C;Multiway analysis of epilepsy tensors&#x201D;</article-title>. <source>Bioinformatics</source> <volume>23.13</volume> (<year>2007</year>), pp. <fpage>i10</fpage>&#x2013;<lpage>i18</lpage>.</mixed-citation></ref>
<ref id="c71"><label>[71]</label><mixed-citation publication-type="journal"><string-name><given-names>F</given-names> <surname>Cong</surname></string-name>, <string-name><given-names>QH</given-names> <surname>Lin</surname></string-name>, <string-name><given-names>LD</given-names> <surname>Kuang</surname></string-name>, <string-name><given-names>XF</given-names> <surname>Gong</surname></string-name>, <string-name><given-names>P</given-names> <surname>Astikainen</surname></string-name>, and <string-name><given-names>T</given-names> <surname>Ristaniemi</surname></string-name>. <article-title>&#x201C;Tensor decomposition of EEG signals: A brief review&#x201D;</article-title>. <source>J Neurosci Methods</source> <volume>248</volume> (<year>2015</year>), pp. <fpage>59</fpage>&#x2013;<lpage>69</lpage>.</mixed-citation></ref>
<ref id="c72"><label>[72]</label><mixed-citation publication-type="journal"><string-name><given-names>B</given-names> <surname>Hunyadi</surname></string-name>, <string-name><given-names>P</given-names> <surname>Dupont</surname></string-name>, <string-name><given-names>W</given-names> <surname>Van Paesschen</surname></string-name>, and <string-name><given-names>S</given-names> <surname>Van Huffel</surname></string-name>. <article-title>&#x201C;Tensor decompositions and data fusion in epileptic electroencephalography and functional magnetic resonance imaging data&#x201D;</article-title>. <source>Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</source> <volume>7.1</volume> (<year>2017</year>), <fpage>e1197</fpage>&#x2013;n/a.</mixed-citation></ref>
<ref id="c73"><label>[73]</label><mixed-citation publication-type="journal"><string-name><given-names>AH</given-names> <surname>Andersen</surname></string-name> and <string-name><given-names>WS</given-names> <surname>Rayens</surname></string-name>. <article-title>&#x201C;Structure-seeking multilinear methods for the analysis of fMRI data&#x201D;</article-title>. <source>NeuroImage</source> <volume>22.2</volume> (<year>2004</year>), pp. <fpage>728</fpage>&#x2013;<lpage>739</lpage>.</mixed-citation></ref>
<ref id="c74"><label>[74]</label><mixed-citation publication-type="journal"><string-name><given-names>JS</given-names> <surname>Seely</surname></string-name>, <string-name><given-names>MT</given-names> <surname>Kaufman</surname></string-name>, <string-name><given-names>SI</given-names> <surname>Ryu</surname></string-name>, <string-name><given-names>KV</given-names> <surname>Shenoy</surname></string-name>, <string-name><given-names>JP</given-names> <surname>Cunningham</surname></string-name>, and <string-name><given-names>MM</given-names> <surname>Churchland</surname></string-name>. <article-title>&#x201C;Tensor Analysis Reveals Distinct Population Structure that Parallels the Different Computational Roles of Areas M1 and V1&#x201D;</article-title>. <source>PLoS Comput Biol</source> <volume>12.11</volume> (<year>2016</year>), pp. <fpage>1</fpage>&#x2013;<lpage>34</lpage>.</mixed-citation></ref>
<ref id="c75"><label>[75]</label><mixed-citation publication-type="journal"><string-name><given-names>MB</given-names> <surname>Ahrens</surname></string-name>, <string-name><given-names>JF</given-names> <surname>Linden</surname></string-name>, and <string-name><given-names>M</given-names> <surname>Sahani</surname></string-name>. <article-title>&#x201C;Nonlinearities and Contextual Influences in Auditory Cortical Responses Modeled with Multilinear Spectrotemporal Methods&#x201D;</article-title>. <source>J Neurosci</source> <volume>28.8</volume> (<year>2008</year>), pp. <fpage>1929</fpage>&#x2013;<lpage>1942</lpage>.</mixed-citation></ref>
<ref id="c76"><label>[76]</label><mixed-citation publication-type="journal"><string-name><given-names>NC</given-names> <surname>Rabinowitz</surname></string-name>, <string-name><given-names>RL</given-names> <surname>Goris</surname></string-name>, <string-name><given-names>M</given-names> <surname>Cohen</surname></string-name>, and <string-name><given-names>EP</given-names> <surname>Simoncelli</surname></string-name>. <article-title>&#x201C;Attention stabilizes the shared gain of V4 populations&#x201D;</article-title>. <source>eLife</source> <volume>4</volume> (<year>2015</year>). Ed. by <person-group person-group-type="editor"><string-name><given-names>M</given-names> <surname>Carandini</surname></string-name></person-group>, <fpage>e08998</fpage>.</mixed-citation></ref>
<ref id="c77"><label>[77]</label><mixed-citation publication-type="journal"><string-name><given-names>JJ</given-names> <surname>Letzkus</surname></string-name>, <string-name><given-names>SBE</given-names> <surname>Wolff</surname></string-name>, <string-name><given-names>EMM</given-names> <surname>Meyer</surname></string-name>, <string-name><given-names>P</given-names> <surname>Tovote</surname></string-name>, <string-name><given-names>J</given-names> <surname>Courtin</surname></string-name>, <string-name><given-names>C</given-names> <surname>Herry</surname></string-name>, and <string-name><given-names>A</given-names> <surname>Luthi</surname></string-name>. <article-title>&#x201C;A disinhibitory microcircuit for associative fear learning in the auditory cortex&#x201D;</article-title>. <source>Nature</source> <volume>480.7377</volume> (<year>2011</year>), pp. <fpage>331</fpage>&#x2013;<lpage>335</lpage>.</mixed-citation></ref>
<ref id="c78"><label>[78]</label><mixed-citation publication-type="journal"><string-name><given-names>BV</given-names> <surname>Atallah</surname></string-name>, <string-name><given-names>W</given-names> <surname>Bruns</surname></string-name>, <string-name><given-names>M</given-names> <surname>Carandini</surname></string-name>, and <string-name><given-names>M</given-names> <surname>Scanziani</surname></string-name>. <article-title>&#x201C;Parvalbumin-Expressing Interneurons Linearly Transform Cortical Responses to Visual Stimuli&#x201D;</article-title>. <source>Neuron</source> <volume>73.1</volume> (<year>2012</year>), pp. <fpage>159</fpage>&#x2013;<lpage>170</lpage>.</mixed-citation></ref>
<ref id="c79"><label>[79]</label><mixed-citation publication-type="journal"><string-name><given-names>H</given-names> <surname>Makino</surname></string-name>, <string-name><given-names>EJ</given-names> <surname>Hwang</surname></string-name>, <string-name><given-names>NG</given-names> <surname>Hedrick</surname></string-name>, and <string-name><given-names>T</given-names> <surname>Komiyama</surname></string-name>. <article-title>&#x201C;Circuit Mechanisms of Sensorimotor Learning&#x201D;</article-title>. <source>Neuron</source> <volume>92.4</volume> (<year>2016</year>), pp. <fpage>705</fpage>&#x2013;<lpage>721</lpage>.</mixed-citation></ref>
<ref id="c80"><label>[80]</label><mixed-citation publication-type="book"><string-name><given-names>RS</given-names> <surname>Sutton</surname></string-name> and <string-name><given-names>AG</given-names> <surname>Barto</surname></string-name>. <source>Reinforcement learning: An introduction</source>. <volume>Vol. 1</volume>. <fpage>1</fpage>. <publisher-name>MIT press Cambridge</publisher-name>, <year>1998</year>.</mixed-citation></ref>
<ref id="c81"><label>[81]</label><mixed-citation publication-type="journal"><string-name><given-names>AC</given-names> <surname>Smith</surname></string-name> and <string-name><given-names>EN</given-names> <surname>Brown</surname></string-name>. <article-title>&#x201C;Estimating a State-Space Model from Point Process Observations&#x201D;</article-title>. <source>Neural Comput</source> <volume>15.5</volume> (<year>2003</year>), pp. <fpage>965</fpage>&#x2013;<lpage>991</lpage>.</mixed-citation></ref>
<ref id="c82"><label>[82]</label><mixed-citation publication-type="book"><string-name><given-names>JH</given-names> <surname>Macke</surname></string-name>, <string-name><given-names>L</given-names> <surname>Buesing</surname></string-name>, <string-name><given-names>JP</given-names> <surname>Cunningham</surname></string-name>, <string-name><given-names>BM</given-names> <surname>Yu</surname></string-name>, <string-name><given-names>KV</given-names> <surname>Shenoy</surname></string-name>, and <string-name><given-names>M</given-names> <surname>Sahani</surname></string-name>. <chapter-title>&#x201C;Empirical models of spiking in neural populations&#x201D;</chapter-title>. <source>Advances in Neural Information Processing Systems</source> <volume>24</volume>. Ed. by <person-group person-group-type="editor"><string-name><given-names>J</given-names> <surname>Shawe-Taylor</surname></string-name>, <string-name><given-names>RS</given-names> <surname>Zemel</surname></string-name>, <string-name><given-names>PL</given-names> <surname>Bartlett</surname></string-name>, <string-name><given-names>F</given-names> <surname>Pereira</surname></string-name>, and <string-name><given-names>KQ</given-names> <surname>Weinberger</surname></string-name></person-group>. <publisher-name>Curran Associates, Inc.</publisher-name>, <year>2011</year>, pp. <fpage>1350</fpage>&#x2013;<lpage>1358</lpage>.</mixed-citation></ref>
<ref id="c83"><label>[83]</label><mixed-citation publication-type="journal"><string-name><given-names>L</given-names> <surname>Buesing</surname></string-name>, <string-name><given-names>JH</given-names> <surname>Macke</surname></string-name>, and <string-name><given-names>M</given-names> <surname>Sahani</surname></string-name>. <article-title>&#x201C;Spectral learning of linear dynamics from generalised-linear observations with application to neural population data&#x201D;</article-title>. <source>Advances in neural information processing systems</source>. <year>2012</year>, pp. <fpage>1682</fpage>&#x2013;<lpage>1690</lpage>.</mixed-citation></ref>
<ref id="c84"><label>[84]</label><mixed-citation publication-type="journal"><string-name><given-names>JC</given-names> <surname>Kao</surname></string-name>, <string-name><given-names>P</given-names> <surname>Nuyujukian</surname></string-name>, <string-name><given-names>SI</given-names> <surname>Ryu</surname></string-name>, <string-name><given-names>MM</given-names> <surname>Churchland</surname></string-name>, <string-name><given-names>JP</given-names> <surname>Cunningham</surname></string-name>, and <string-name><given-names>KV</given-names> <surname>Shenoy</surname></string-name>. <article-title>&#x201C;Single-trial dynamics of motor cortex and their applications to brain-machine interfaces&#x201D;</article-title>. <source>Nat Commun</source> <volume>6</volume> (<year>2015</year>),</mixed-citation></ref>
<ref id="c85"><label>[85]</label><mixed-citation publication-type="book"><string-name><given-names>B</given-names> <surname>Petreska</surname></string-name>, <string-name><given-names>BM</given-names> <surname>Yu</surname></string-name>, <string-name><given-names>JP</given-names> <surname>Cunningham</surname></string-name>, <string-name><given-names>G</given-names> <surname>Santhanam</surname></string-name>, <string-name><given-names>SI</given-names> <surname>Ryu</surname></string-name>, <string-name><given-names>KV</given-names> <surname>Shenoy</surname></string-name>, and <string-name><given-names>M</given-names> <surname>Sahani</surname></string-name>. <chapter-title>&#x201C;Dynamical segmentation of single trials from population neural data&#x201D;</chapter-title>. <source>Advances in Neural Information Processing Systems</source> <volume>24</volume>. Ed. by <person-group person-group-type="editor"><string-name><given-names>J</given-names> <surname>Shawe-Taylor</surname></string-name>, <string-name><given-names>RS</given-names> <surname>Zemel</surname></string-name>, <string-name><given-names>PL</given-names> <surname>Bartlett</surname></string-name>, <string-name><given-names>F</given-names> <surname>Pereira</surname></string-name>, and <string-name><given-names>KQ</given-names> <surname>Weinberger</surname></string-name></person-group>. <publisher-name>Curran Associates, Inc.</publisher-name>, <year>2011</year>, pp. <fpage>756</fpage>&#x2013;<lpage>764</lpage>.</mixed-citation></ref>
<ref id="c86"><label>[86]</label><mixed-citation publication-type="book"><string-name><given-names>S</given-names> <surname>Linderman</surname></string-name>, <string-name><given-names>M</given-names> <surname>Johnson</surname></string-name>, <string-name><given-names>A</given-names> <surname>Miller</surname></string-name>, <string-name><given-names>R</given-names> <surname>Adams</surname></string-name>, <string-name><given-names>D</given-names> <surname>Blei</surname></string-name>, and <string-name><given-names>L</given-names> <surname>Paninski</surname></string-name>. <chapter-title>&#x201C;Bayesian Learning and Inference in Recurrent Switching Linear Dynamical Systems&#x201D;</chapter-title>. <source>Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</source>. Ed. by <person-group person-group-type="editor"><string-name><given-names>A</given-names> <surname>Singh</surname></string-name> and <string-name><given-names>J</given-names> <surname>Zhu</surname></string-name></person-group>. <volume>Vol. 54</volume>. <publisher-name>Proceedings of Machine Learning Research</publisher-name>. <publisher-loc>Fort Lauderdale, FL, USA: PMLR</publisher-loc>, <year>2017</year>, pp. <fpage>914</fpage>&#x2013;<lpage>922</lpage>.</mixed-citation></ref>
<ref id="c87"><label>[87]</label><mixed-citation publication-type="books"><string-name><given-names>Y</given-names> <surname>Zhao</surname></string-name> and <string-name><given-names>IM</given-names> <surname>Park</surname></string-name>. <article-title>&#x201C;Interpretable Nonlinear Dynamic Modeling of Neural Trajectories&#x201D;</article-title>. <source>Advances in Neural Information Processing Systems</source> <volume>29</volume>. Ed. by <person-group person-group-type="editor"><string-name><given-names>DD</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>M</given-names> <surname>Sugiyama</surname></string-name>, <string-name><given-names>UV</given-names> <surname>Luxburg</surname></string-name>, <string-name><given-names>I</given-names> <surname>Guyon</surname></string-name>, and <string-name><given-names>R</given-names> <surname>Garnett</surname></string-name></person-group>. <publisher-name>Curran Associates, Inc.</publisher-name>, <year>2016</year>, pp. <fpage>3333</fpage>&#x2013;<lpage>3341</lpage>.</mixed-citation></ref>
<ref id="c88"><label>[88]</label><mixed-citation publication-type="journal"><string-name><given-names>E</given-names> <surname>Acar</surname></string-name>, <string-name><given-names>R</given-names> <surname>Bro</surname></string-name>, and <string-name><given-names>AK</given-names> <surname>Smilde</surname></string-name>. <article-title>&#x201C;Data Fusion in Metabolomics Using Coupled Matrix and Tensor Factorizations&#x201D;</article-title>. <source>Proceedings of the IEEE</source> <volume>103.9</volume> (<year>2015</year>), pp. <fpage>1602</fpage>&#x2013;<lpage>1620</lpage>.</mixed-citation></ref>
<ref id="c89"><label>[89]</label><mixed-citation publication-type="journal"><string-name><given-names>E</given-names> <surname>Jones</surname></string-name>, <string-name><given-names>T</given-names> <surname>Oliphant</surname></string-name>, <string-name><given-names>P</given-names> <surname>Peterson</surname></string-name>, <etal>et al.</etal> <source>SciPy: Open source scientific tools for Python</source>. <fpage>2001</fpage>&#x2013;.</mixed-citation></ref>
<ref id="c90"><label>[90]</label><mixed-citation publication-type="journal"><string-name><given-names>JD</given-names> <surname>Hunter</surname></string-name>. <article-title>&#x201C;Matplotlib: A 2D Graphics Environment&#x201D;</article-title>. <source>Computing in Science Engineering</source> <volume>9.3</volume> (<year>2007</year>), pp. <fpage>90</fpage>&#x2013;<lpage>95</lpage>.</mixed-citation></ref>
<ref id="c91"><label>[91]</label><mixed-citation publication-type="journal"><string-name><given-names>F</given-names> <surname>Pedregosa</surname></string-name>, <string-name><given-names>G</given-names> <surname>Varoquaux</surname></string-name>, <string-name><given-names>A</given-names> <surname>Gramfort</surname></string-name>, <string-name><given-names>V</given-names> <surname>Michel</surname></string-name>, <string-name><given-names>B</given-names> <surname>Thirion</surname></string-name>, <string-name><given-names>O</given-names> <surname>Grisel</surname></string-name>, <string-name><given-names>M</given-names> <surname>Blondel</surname></string-name>, <string-name><given-names>P</given-names> <surname>Prettenhofer</surname></string-name>, <string-name><given-names>R</given-names> <surname>Weiss</surname></string-name>, <string-name><given-names>V</given-names> <surname>Dubourg</surname></string-name>, <string-name><given-names>J</given-names> <surname>Vanderplas</surname></string-name>, <string-name><given-names>A</given-names> <surname>Passos</surname></string-name>, <string-name><given-names>D</given-names> <surname>Cournapeau</surname></string-name>, <string-name><given-names>M</given-names> <surname>Brucher</surname></string-name>, <string-name><given-names>M</given-names> <surname>Perrot</surname></string-name>, and <string-name><given-names>E</given-names> <surname>Duchesnay</surname></string-name>. <article-title>&#x201C;Scikit-learn: Machine Learning in Python&#x201D;</article-title>. <source>Journal of Machine Learning Research</source> <volume>12</volume> (<year>2011</year>), pp. <fpage>2825</fpage>&#x2013;<lpage>2830</lpage>.</mixed-citation></ref>
<ref id="c92"><label>[92]</label><mixed-citation publication-type="journal"><string-name><given-names>TW</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>TJ</given-names> <surname>Wardill</surname></string-name>, <string-name><given-names>Y</given-names> <surname>Sun</surname></string-name>, <string-name><given-names>SR</given-names> <surname>Pulver</surname></string-name>, <string-name><given-names>SL</given-names> <surname>Renninger</surname></string-name>, <string-name><given-names>A</given-names> <surname>Baohan</surname></string-name>, <string-name><given-names>ER</given-names> <surname>Schreiter</surname></string-name>, <string-name><given-names>RA</given-names> <surname>Kerr</surname></string-name>, <string-name><given-names>MB</given-names> <surname>Orger</surname></string-name>, <string-name><given-names>V</given-names> <surname>Jayaraman</surname></string-name>, <string-name><given-names>LL</given-names> <surname>Looger</surname></string-name>, <string-name><given-names>K</given-names> <surname>Svoboda</surname></string-name>, and <string-name><given-names>DS</given-names> <surname>Kim</surname></string-name>. <article-title>&#x201C;Ultrasensitive fluorescent proteins for imaging neuronal activity&#x201D;</article-title>. <source>Nature</source> <volume>499.7458</volume> (<year>2013</year>), pp. <fpage>295</fpage>&#x2013;<lpage>300</lpage>.</mixed-citation></ref>
<ref id="c93"><label>[93]</label><mixed-citation publication-type="other"><string-name><given-names>BW</given-names> <surname>Bader</surname></string-name>, <string-name><given-names>TG</given-names> <surname>Kolda</surname></string-name>, <etal>et al.</etal> <source>MATLAB Tensor Toolbox</source>. <year>2017</year>. url: <ext-link ext-link-type="uri" xlink:href="http://www.tensortoolbox.org">www.tensortoolbox.org</ext-link>.</mixed-citation></ref>
<ref id="c94"><label>[94]</label><mixed-citation publication-type="other"><string-name><given-names>N</given-names> <surname>Vervliet</surname></string-name>, <string-name><given-names>O</given-names> <surname>Debals</surname></string-name>, <string-name><given-names>L</given-names> <surname>Sorber</surname></string-name>, <string-name><given-names>M</given-names> <surname>Van Barel</surname></string-name>, and <string-name><given-names>L</given-names> <surname>De Lathauwer</surname></string-name>. <source>Tensorlab 3.0</source>. <year>2016</year>. url: <ext-link ext-link-type="uri" xlink:href="http://www.tensorlab.net">http://www.tensorlab.net</ext-link>.</mixed-citation></ref>
<ref id="c95"><label>[95]</label><mixed-citation publication-type="other"><string-name><given-names>J</given-names> <surname>Kossaifi</surname></string-name>, <string-name><given-names>Y</given-names> <surname>Panagakis</surname></string-name>, and <string-name><given-names>M</given-names> <surname>Pantic</surname></string-name>. <article-title>&#x201C;TensorLy: Tensor Learning in Python&#x201D;</article-title>. <source>ArXiv e-print</source> (<year>2016</year>).</mixed-citation></ref>
<ref id="c96"><label>[96]</label><mixed-citation publication-type="journal"><string-name><given-names>R</given-names> <surname>Bro</surname></string-name> and <string-name><given-names>S</given-names> <surname>De Jong</surname></string-name>. <article-title>&#x201C;A fast non-negativity-constrained least squares algorithm&#x201D;</article-title>. <source>J Chemometrics</source> <volume>11.5</volume> (<year>1997</year>), pp. <fpage>393</fpage>&#x2013;<lpage>401</lpage>.</mixed-citation></ref>
<ref id="c97"><label>[97]</label><mixed-citation publication-type="journal"><string-name><given-names>P</given-names> <surname>Paatero</surname></string-name>. <article-title>&#x201C;A weighted non-negative least squares algorithm for three-way &#x2018;PARAFAC&#x2019; factor analysis&#x201D;</article-title>. <source>Chemometrics and Intelligent Laboratory Systems</source> <volume>38.2</volume> (<year>1997</year>), pp. <fpage>223</fpage>&#x2013;<lpage>242</lpage>.</mixed-citation></ref>
<ref id="c98"><label>[98]</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>Welling</surname></string-name> and <string-name><given-names>M</given-names> <surname>Weber</surname></string-name>. <article-title>&#x201C;Positive Tensor Factorization&#x201D;</article-title>. <source>Pattern Recognition Letters</source> <volume>22.12</volume> (<year>2001</year>), pp. <fpage>1255</fpage>&#x2013;<lpage>1261</lpage>.</mixed-citation></ref>
<ref id="c99"><label>[99]</label><mixed-citation publication-type="journal"><string-name><given-names>P</given-names> <surname>Paatero</surname></string-name> and <string-name><given-names>U</given-names> <surname>Tapper</surname></string-name>. <article-title>&#x201C;Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data values&#x201D;</article-title>. <source>Environmetrics</source> <volume>5.2</volume> (<year>1994</year>), pp. <fpage>111</fpage>&#x2013;<lpage>126</lpage>.</mixed-citation></ref>
<ref id="c100"><label>[100]</label><mixed-citation publication-type="journal"><string-name><given-names>EC</given-names> <surname>Chi</surname></string-name> and <string-name><given-names>TG</given-names> <surname>Kolda</surname></string-name>. <article-title>&#x201C;On Tensors, Sparsity, and Nonnegative Factorizations&#x201D;</article-title>. <source>SIAM J Matrix Anal Appl</source> <volume>33.4</volume> (<year>2012</year>), pp. <fpage>1272</fpage>&#x2013;<lpage>1299</lpage>.</mixed-citation></ref>
<ref id="c101"><label>[101]</label><mixed-citation publication-type="journal"><string-name><given-names>P</given-names> <surname>Comon</surname></string-name>, <string-name><given-names>X</given-names> <surname>Luciani</surname></string-name>, and <string-name><given-names>ALF</given-names> <surname>de Almeida</surname></string-name>. <article-title>&#x201C;Tensor decompositions, alternating least squares and other tales&#x201D;</article-title>. <source>J Chemometrics</source> <volume>23.7-8</volume> (<year>2009</year>), pp. <fpage>393</fpage>&#x2013;<lpage>405</lpage>.</mixed-citation></ref>
<ref id="c102"><label>[102]</label><mixed-citation publication-type="journal"><string-name><given-names>SA</given-names> <surname>Vavasis</surname></string-name>. <article-title>&#x201C;On the Complexity of Nonnegative Matrix Factorization&#x201D;</article-title>. <source>SIAM Journal on Optimization</source> <volume>20.3</volume> (<year>2010</year>), pp. <fpage>1364</fpage>&#x2013;<lpage>1377</lpage>.</mixed-citation></ref>
<ref id="c103"><label>[103]</label><mixed-citation publication-type="journal"><string-name><given-names>J</given-names> <surname>Kim</surname></string-name> and <string-name><given-names>H</given-names> <surname>Park</surname></string-name>. <article-title>&#x201C;Fast Nonnegative Matrix Factorization: An Active-Set-Like Method and Comparisons&#x201D;</article-title>. <source>SIAM Journal on Scientific Computing</source> <volume>33.6</volume> (<year>2011</year>), pp. <fpage>3261</fpage>&#x2013;<lpage>3281</lpage>.</mixed-citation></ref>
<ref id="c104"><label>[104]</label><mixed-citation publication-type="journal"><string-name><given-names>BF</given-names> <surname>Grewe</surname></string-name>, <string-name><given-names>J</given-names> <surname>Gr&#x00FC;ndemann</surname></string-name>, <string-name><given-names>LJ</given-names> <surname>Kitch</surname></string-name>, <string-name><given-names>JA</given-names> <surname>Lecoq</surname></string-name>, <string-name><given-names>JG</given-names> <surname>Parker</surname></string-name>, <string-name><given-names>JD</given-names> <surname>Marshall</surname></string-name>, <string-name><given-names>MC</given-names> <surname>Larkin</surname></string-name>, <string-name><given-names>PE</given-names> <surname>Jercog</surname></string-name>, <string-name><given-names>F</given-names> <surname>Grenier</surname></string-name>, <string-name><given-names>JZ</given-names> <surname>Li</surname></string-name>, <string-name><given-names>A</given-names> <surname>L&#x00FC;thi</surname></string-name>, and <string-name><given-names>MJ</given-names> <surname>Schnitzer</surname></string-name>. <article-title>&#x201C;Neural ensemble dynamics underlying a long-term associative memory&#x201D;</article-title>. <source>Nature</source> <volume>543.7647</volume> (<year>2017</year>), pp. <fpage>670</fpage>&#x2013;<lpage>675</lpage>.</mixed-citation></ref>
<ref id="c105"><label>[105]</label><mixed-citation publication-type="journal"><string-name><given-names>KV</given-names> <surname>Shenoy</surname></string-name>, <string-name><given-names>M</given-names> <surname>Sahani</surname></string-name>, and <string-name><given-names>MM</given-names> <surname>Churchland</surname></string-name>. <article-title>&#x201C;Cortical Control of Arm Movements: A Dynamical Systems Perspective&#x201D;</article-title>. <source>Annual Review of Neuroscience</source> <volume>36.1</volume> (<year>2013</year>), pp. <fpage>337</fpage>&#x2013;<lpage>359</lpage>.</mixed-citation></ref>
<ref id="c106"><label>[106]</label><mixed-citation publication-type="journal"><string-name><given-names>C</given-names> <surname>Eckart</surname></string-name> and <string-name><given-names>G</given-names> <surname>Young</surname></string-name>. <article-title>&#x201C;The approximation of one matrix by another of lower rank&#x201D;</article-title>. <source>Psychometrika</source> <volume>1.3</volume> (<year>1936</year>), pp. <fpage>211</fpage>&#x2013;<lpage>218</lpage>.</mixed-citation></ref>
<ref id="c107"><label>[107]</label><mixed-citation publication-type="journal"><string-name><given-names>G</given-names> <surname>Tomasi</surname></string-name> and <string-name><given-names>R</given-names> <surname>Bro</surname></string-name>. <article-title>&#x201C;A comparison of algorithms for fitting the PARAFAC model&#x201D;</article-title>. <source>Computational Statistics &#x0026; Data Analysis</source> <volume>50.7</volume> (<year>2006</year>), pp. <fpage>1700</fpage>&#x2013;<lpage>1734</lpage>.</mixed-citation></ref>
<ref id="c108"><label>[108]</label><mixed-citation publication-type="journal"><string-name><given-names>A</given-names> <surname>Litwin-Kumar</surname></string-name>, <string-name><given-names>KD</given-names> <surname>Harris</surname></string-name>, <string-name><given-names>R</given-names> <surname>Axel</surname></string-name>, <string-name><given-names>H</given-names> <surname>Sompolinsky</surname></string-name>, and <string-name><given-names>LF</given-names> <surname>Abbott</surname></string-name>. <article-title>&#x201C;Optimal Degrees of Synaptic Connectivity&#x201D;</article-title>. <source>Neuron</source> <volume>93.5</volume> (<year>2017</year>), <fpage>1153</fpage>&#x2013;<lpage>1164.e7</lpage>.</mixed-citation></ref>
</ref-list>
<sec>
<title>Figure Supplements</title>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure 2, Supplement 1.</label>
<caption><p>Illustration of <italic>tensor unfolding</italic> for applying matrix decompositions to tensor datasets. A <italic>N &#x00D7; T &#x00D7; K</italic> dimensional tensor can be reshaped into three different matrices: a &#x201C;neurons unfolding&#x201D; with dimensions <italic>N &#x00D7; T K</italic>, a &#x201C;time unfolding&#x201D; with dimensions <italic>T &#x00D7; NK</italic>, and a &#x201C;trials unfolding&#x201D; with dimensions <italic>K &#x00D7; NT</italic>. Applying PCA or other matrix decomposition methods to each unfolding yields a different set of low-dimensional factors.</p></caption>
<graphic xlink:href="211128_figS2.tif"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure 3, Supplement 1.</label>
<caption><p>Cell tuning and synaptic connectivity properties in a nonlinear RNN trained on a stimulus discrimination task. <bold>(a)</bold> Activity of all cells on (&#x002B;)-trials and (-)-trials before and after training. Cells were sorted by the low-dimensional neuron factor, <inline-formula><alternatives><inline-graphic xlink:href="211128_inline12.gif"/></alternatives></inline-formula>, as in <xref rid="fig3" ref-type="fig">fig. 3e</xref>. <bold>(b)</bold> Cell tuning quantified as peak activity on (&#x002B;)-trials minus peak activity on (-)-trials before and after training (averaged over ten trials). Cells with positive tuning scores are (&#x002B;)-cells, while cells with negative tuning scores are (-)-cells. The initial tuning was positively correlated with final tuning for each cell. <bold>(c)</bold> Eigenvalues of the synaptic connectivity matrix after training. Similar to the solution in linear networks [53], the connectivity matrix has a single eigenvalue near 1 &#x002B; 0<italic>i</italic>; and all other eigenvalues are small in magnitude. <bold>(d-e)</bold> The neuron factor identified by a 1-component TCA model is positively correlated with the input-to-network synaptic weights <bold>(d)</bold>, and the network-to-output weights <bold>(e)</bold>.</p></caption>
<graphic xlink:href="211128_figS3.tif"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Figure 4, Supplement 1.</label>
<caption><p>An example cell with low <italic>R</italic><sup>2</sup>. <bold>(a)</bold> Raster heatmaps showing the cell&#x2019;s fluorescence over the 200 most active trials (left), and the estimate of a 15-component nonnegative TCA model on these trials (right). On a small subset of trials the cell is active, but at variable phases of the trial. Note that on the remaining trials, the cell was hardly active at all (not shown). <bold>(b)</bold> Median fluorescence traces averaged over various task variables (start location, end location, and reward delivery). The cell does not, on average, show a preference for any task variable. Dashed lines denote the first and third quartiles of the fluorescence trace. <bold>(c)</bold> Median estimated fluorescence of the 15-component nonnegative TCA model for this cell. The estimate is closely matched to the median firing rates shown in panel b. Dashed lines denote first and third quartiles.</p></caption>
<graphic xlink:href="211128_figS4.tif"/>
</fig>
<fig id="figS5a" position="float" fig-type="figure">
<label>Figure 5, Supplement 1.</label>
<caption><p>Additional detail on the decomposition of mouse prefrontal cortex dynamics. <bold>(a)</bold> Remaining seven TCA factors from the 15-component decomposition shown in <xref rid="fig5" ref-type="fig">fig. 5</xref>. <bold>(b)</bold> The magnitude (Euclidean length) of each factor in the decomposition, a metric analogous to the variance explained by each component (see <italic>Methods</italic>, <xref ref-type="sec" rid="s4e1">section 4.5.1</xref>).</p></caption>
<graphic xlink:href="211128_figS5a.tif"/>
</fig>
<fig id="figS5b" position="float" fig-type="figure">
<label>Figure 5, Supplement 2.</label>
<caption><p>PCA components in trial-space do not cleanly encode individual task variables, in line with previous observations [34]. Each row shows a principal component, ordered by variance explained. Each column shows a different coloring of that principal component by a different task variable. With few exceptions (notably the top component), any single coloring does not yield a simple interpretation of the component.</p></caption>
<graphic xlink:href="211128_figS5b.tif"/>
</fig>
<fig id="figS6" position="float" fig-type="figure">
<label>Figure 6, Supplement 1.</label>
<caption><p>Diagnostic plots for TCA models fit to 45&#x00B0; reaches in the primate BMI dataset. <bold>(a)</bold> Scree plot for unconstrained (blue) and nonnegative (red) TCA. As elsewhere in this manuscript, each dot denotes a model fit from different initial parameters, demonstrating that neither model got caught in appreciably sub-optimal local minima during optimization. Nonnegative decomposition provided similar explanatory power to unconstrained decompositions. <bold>(a)</bold> Similarity plot for unconstrained (blue) and nonnegative (red) CP decompositions. As elsewhere in this manuscript, each dot denotes the similarity score between a model and the best-fit model with the same number of components. Nonnegative decomposition had larger similarity scores, suggesting that the latent factors were more reliably identified and less sensitive to initialization.</p></caption>
<graphic xlink:href="211128_figS6.tif"/>
</fig>
</sec>
</back>
</article>
