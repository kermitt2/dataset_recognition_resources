<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/157628</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Compressive Temporal Summation in Human Visual Cortex</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Zhou</surname>
<given-names>Jingyang</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Benson</surname>
<given-names>Noah C.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6604-9155</contrib-id>
<name>
<surname>Kay</surname>
<given-names>Kendrick</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7475-5586</contrib-id>
<name>
<surname>Winawer</surname>
<given-names>Jonathan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Psychology, New York University</institution>, 10003</aff>
<aff id="a2"><label>2</label><institution>Department of Radiology, University of Minnesota</institution>, Twin Cities, 55414</aff>
<aff id="a3"><label>3</label><institution>Center for Neural Science New York University</institution>, 10003</aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label>Corresponding Author, <email>jingyang.zhou@nyu.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2017</year>
</pub-date>
<elocation-id>157628</elocation-id>
<history>
<date date-type="received">
<day>29</day>
<month>6</month>
<year>2017</year>
</date>
<date date-type="rev-recd">
<day>29</day>
<month>6</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>30</day>
<month>6</month>
<year>2017</year>
</date>
</history><permissions><copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license></permissions>
<self-uri xlink:href="157628.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Combining sensory inputs over space and time is fundamental to vision. Population receptive field models have been highly successful in characterizing spatial encoding throughout the human visual pathways. A parallel question&#x2014;how visual areas in the human brain process information distributed over time&#x2014;has received less attention. One challenge is that the most widely used neuroimaging method&#x2013;fMRI&#x2013;has coarse temporal resolution compared to the time scale of neural dynamics. Here, via carefully controlled temporally modulated stimuli, we show that information about temporal processing can be readily derived from fMRI signal amplitudes. We find that all visual areas exhibit subadditive summation, whereby responses to longer stimuli are less than the linear prediction from briefer stimuli. We also find fMRI evidence that the neural response to two stimuli is reduced for brief interstimulus intervals (indicating adaptation). These effects are more pronounced in anterior visual areas than V1-V3. Finally, we develop a general model that shows how all of these effects can be captured with two simple operations: temporal summation followed by a compressive nonlinearity. This model operates for arbitrary temporal stimulation patterns and provides a simple and interpretable set of computations that can be used to characterize neural response properties across the visual hierarchy. Importantly, compressive temporal summation directly parallels earlier findings of compressive spatial summation in human visual cortex describing responses to stimuli distributed across space. This indicates that for space and time, cortex uses a similar processing strategy to achieve higher-level and increasingly invariant representations of the visual world.</p>
<sec id="s1">
<title>Significance statement</title>
<p>Combining sensory inputs over time is fundamental to seeing. Two important temporal phenomena are <italic>summation</italic>, the accumulation of sensory inputs over time, and <italic>adaptation</italic>, a response reduction for repeated or sustained stimuli. We investigated these phenomena in the human visual system using fMRI. We built predictive models that operate on arbitrary temporal patterns of stimulation using two simple computations: temporal summation followed by a compressive nonlinearity. Our new temporal compressive summation model captures (1) subadditive temporal summation, and (2) adaptation. We show that the model accounts for systematic differences in these phenomena across visual areas. Finally, we show that for space and time, the visual system uses a similar strategy to achieve increasingly invariant representations of the visual world.</p>
</sec>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Functional MRI, Population Receptive Fields, Temporal Summation, Visual Cortex, Adaptation, visual hierarchy</kwd>
</kwd-group>
<counts>
<page-count count="28"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s2">
<label>1.</label>
<title>Introduction</title>
<p>A fundamental task of the visual system is to combine sensory information distributed across space and time. How neural responses sum inputs across space has been well characterized, with several robust phenomena. First, spatial summation in visual cortex is subadditive: the response to two stimuli presented in different locations at the same time is less than the sum of the responses to the stimuli presented separately. This phenomenon is observed in all cortical areas studied and has been measured with both fMRI (<xref ref-type="bibr" rid="c26">Kastner et al., 2001</xref>; <xref ref-type="bibr" rid="c28">Kay et al., 2013a</xref>) and electrophysiology (<xref ref-type="bibr" rid="c40">Rolls and Tovee, 1995</xref>; <xref ref-type="bibr" rid="c8">Britten and Heuer, 1999</xref>; <xref ref-type="bibr" rid="c22">Heuer and Britten, 2002</xref>; <xref ref-type="bibr" rid="c57">Winawer et al., 2013</xref>); such nonlinearities may reflect an adaptation to achieve efficient encoding of natural images (<xref ref-type="bibr" rid="c43">Schwartz and Simoncelli, 2001</xref>). In addition, in higher visual areas, receptive field size increases (<xref ref-type="bibr" rid="c34">Maunsell and Newsome, 1987</xref>) and sub-additive summation becomes more pronounced (<xref ref-type="bibr" rid="c28">Kay et al., 2013a</xref>; <xref ref-type="bibr" rid="c30">Kay et al., 2013c</xref>). As the subadditivity becomes more pronounced in later areas and receptive fields get larger, a stimulus that occupies only a small fraction of a neural receptive field can produce a large response. As a result, responses in higher visual areas become increasingly insensitive to changes in the size and position of stimuli (<xref ref-type="bibr" rid="c49">Tovee et al., 1994</xref>; <xref ref-type="bibr" rid="c14">Grill-Spector et al., 2001</xref>; <xref ref-type="bibr" rid="c28">Kay et al., 2013a</xref>). The tendency towards increasing tolerance for size and position in higher areas trades off with the increasing specificity of tuning to higher level stimulus information (<xref ref-type="bibr" rid="c41">Rust and Dicarlo, 2010</xref>, <xref ref-type="bibr" rid="c42">2012</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Parallels between spatial and temporal processing.</title>
<p>It is well established that spatial receptive fields are small in V1 (left) and grow larger in later visual areas such as the temporal occipital maps (&#x2018;TO&#x2019;, right). It was recently shown that there is also a gradient of an increasingly pronounced compressive summation over space from early to later areas (<xref ref-type="bibr" rid="c28">Kay et al., 2013a</xref>). Here, we hypothesize that temporal summation, as well as the temporal receptive field size, follows a similar pattern, with increasingly long temporal windows and more compressive summation over time in the more anterior visual areas. We propose that the combination of larger spatiotemporal windows and more compressive nonlinearities is part of a coding strategy whereby higher visual areas achieve increasing invariance to changes in stimulus size, position, and duration.</p>
</caption>
<graphic xlink:href="157628_fig1.tif"/>
</fig>
<p>Here, we hypothesize that the same organizational principles for the visual cortex apply in the temporal domain. Just as natural images tend to vary slowly over space, image sequences typically vary slowly over time (<xref ref-type="bibr" rid="c11">Dong and Atick, 1995</xref>; <xref ref-type="bibr" rid="c55">Weiss and Adelson, 1998</xref>). As a result, an efficient code would prioritize abrupt changes in time over sustained or repeated stimuli (<xref ref-type="bibr" rid="c46">Snow et al., 2016</xref>); this would result in sub-additive temporal summation for sustained or repeated stimuli (also referred to as adaptation or repetition suppression). Evidence for such temporal non-linearities are abundant in single cell recordings of primary visual cortex (for example, <xref ref-type="bibr" rid="c48">Tolhurst et al., 1980</xref>), but have not been systemically characterized across visual areas or with a forward model. At longer time scales, the fMRI BOLD signal sums contrast patterns close to, but slightly less than, linearly (<xref ref-type="bibr" rid="c5">Boynton et al., 1996</xref>; <xref ref-type="bibr" rid="c4">Boynton et al., 2012</xref>). We hypothesize that (1) at the time scale of neuronal dynamics in sensory cortex (tens to hundreds of ms), temporal summation will be substantially subadditive, and (2) that more anterior visual areas will show greater subadditivity. This greater subadditivity in later areas will make these responses less sensitive to the precise duration and timing of a stimulus, paralleling size and position tolerance in the spatial domain. This prediction is consistent with the logic that later visual areas trade off position and duration specificity for increased tuning for high level stimulus properties.</p>
<p>In this paper, we used fMRI to study temporal summation and adaptation. We characterized responses to brief stimuli (tens to hundreds of ms) in many visual areas, measured with fMRI, which has the advantage of being non-invasive and recording from many visual areas in parallel. To quantify and understand how temporal information is encoded across visual cortex, we implemented a temporal population receptive field (&#x201C;pRF&#x201D;) model which predicts the fMRI response amplitude to arbitrary stimulus time courses.</p>
</sec>
<sec id="s3">
<label>2.</label>
<title>Materials and methods</title>
<sec id="s3a">
<label>2.1</label>
<title>fMRI procedure</title>
<sec id="s3a1">
<title>Participants</title>
<p>Data from six experienced fMRI participants (one male, age range 21-48, mean age 31) were collected at the Center for Brain Imaging (CBI) at NYU. All participants had normal or corrected-to-normal visual acuity. The experimental protocol was approved by the University Committee on Activities Involving Human Subjects, and informed written consents were obtained from all participants prior to the study. For each participant, we conducted a 1-hour session for visual field map identification and high-resolution anatomical volumes, and either one or two 1.5-hour sessions to study temporal summation. Two of the six participants were included in both the main temporal summation experiment and the self-replication experiment (hence two 1.5-hour sessions). The other four participants were included in only the main temporal summation experiment or the self-replication experiment.</p>
</sec>
<sec id="s3a2">
<title>Visual Stimuli</title>
<sec id="s3a2a">
<title>Stimuli</title>
<p>For the main experiment, stimuli were large field (24&#xB0; diameter) band-pass noise patterns (centered at 3 cycles per degree), independently generated for each trial. The pattern was chosen because it was previously shown to be effective in eliciting responses in most visual areas (<xref ref-type="bibr" rid="c30">Kay et al., 2013c</xref>). (See <xref ref-type="bibr" rid="c30">Kay et al (2013c)</xref> for details on stimulus construction). A second experiment replicated all aspects of the main experiments except that the stimulus patterns differed. For this experiment, the patterns were either pink noise (1/f amplitude spectrum, random phase), or a front-view face image embedded in the pink noise background. The face stimuli were the front-facing subset of the faces used by <xref ref-type="bibr" rid="c27">Kay et al (2015)</xref>. For both experiments, stimuli were windowed with a circular aperture (24&#xB0; diameter, 768 &#x00D7; 768 pixels) with a raised cosine boundary (2.4 deg). All stimuli were gray scale. Stimulus generation, presentation and response recording were coded using Psychophysics Toolbox (<xref ref-type="bibr" rid="c7">Brainard, 1997</xref>; <xref ref-type="bibr" rid="c37">Pelli, 1997</xref>) and vistadisp (<ext-link ext-link-type="uri" xlink:href="https://github.com/vistalab/vistadisp">https://github.com/vistalab/vistadisp</ext-link>). We used a MacBook Air computer to control stimulus presentation and record responses from the participants (button presses) during the experiment.</p>
</sec>
<sec id="s3a2b">
<title>Display</title>
<p>Stimuli were displayed via an LCD projector (Eiki LC_XG250; resolution: 1024 &#x00D7; 768 pixels; refresh rate: 60 Hz) onto a back-projection screen in the bore of the magnet. Participants, at a viewing distance of &#x007E;58 cm, viewed the screen (field of view, horizontal: &#x007E;32&#xB0;, vertical: &#x007E;24&#xB0;) through an angled mirror. The images were confined to a circular region with a radius of 12&#xB0;. The display was calibrated and gamma corrected using a linearized lookup table.</p>
</sec>
<sec id="s3a2c">
<title>Fixation task</title>
<p>To stabilize attention level across scans and across participants during the main experiment, all participants were instructed to do a one-back digit task at the center of fixation throughout the experiment, as in previous publications (<xref ref-type="bibr" rid="c28">Kay et al., 2013a</xref>; <xref ref-type="bibr" rid="c30">Kay et al., 2013c</xref>). The digit (0.24&#xB0; x 0.24&#xB0;) was presented at the center of a neutral gray disk (0.47&#xB0; diameter). Within a scan, each digit (randomly selected from 0 to 9) was on for 0.5 second, off for 0.167 second before the next digit appeared at the same location. Participants were asked to press a button when a digit repeated. Digit repetition occurred around 2-3&#x0025;, with no more than two identical digits being presented successively. To reduce visual adaptation, all digits alternated between black and white, and on average participants pressed a button every 30 seconds. During the retinotopy task, the fixation alternated pseudo-randomly between red and green (switches on average every 3s), and the participant pressed a button to indicate color changes.</p>
</sec>
</sec>
<sec id="s3a3">
<title>Experimental Design</title>
<p>We used a randomized event-related experimental design (<xref ref-type="fig" rid="fig2">Figure 2A-B</xref>) to prevent participants from anticipating the stimulus conditions. An event is a stimulus presented according to one of thirteen distinct time courses (&#x003C; 800 ms in total), either a single pulse with variable duration or a double pulse with fixed duration and variable inter-stimulus interval (ISI). Durations and ISIs were powers of 2 times the monitor dwell time (1/60 s). Each pulse in the double-pulse stimuli lasted 134 ms. The 0-ms stimulus was a blank (zero-contrast, mean luminance, and hence identical to the preceding and subsequent blank screen between stimulus events). For the main experiment, each participant completed seven scans, and within a scan, each temporal event repeated 4 times. A temporal event started with the onset of a pattern image, and the inter-trial interval (stimulus plus subsequent blank) was always 4.5 seconds. For stimuli with two pulses, the two noise patterns were identical. The design was identical for the self-replication experiment, except that each time course repeated three times per scan instead of 4, and each participant completed 6 scans.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Experimental design and analysis.</title>
<p><italic>(A)</italic> Participants were presented with one or two pulses of large field (24&#xB0;) spatial contrast patterns. One-pulse stimuli were of varying durations and two-pulse stimuli were of varying ISI (with each pulse lasting 134ms). <italic>(B)</italic> The temporal conditions were presented in random order, indicated by the white bars in the 13-column design matrix (one column per temporal condition). To analyze the data, we extracted a &#x00DF;-weight for each temporal condition per area using a variant of the general linear model, GLM denoise. <italic>(C)</italic> Nine visual field maps or visual field maps pairs were bilaterally identified for each participant (V1; V2; V3; hV4; VO-1/2; V3A/B; IPS-0/1; LO-1/2; TO-1/2).</p></caption>
<graphic xlink:href="157628_fig2.tif"/>
</fig>
</sec>
<sec id="s3a4">
<title>MRI Data Acquisition</title>
<p>All fMRI data were acquired at NYU Center for Brain Imaging (CBI) using a Siemens Allegra 3T head-only scanner with a Nova Medical phased array, 8-channel receive surface coil (NMSC072). For each participant, we collected functional images (1500 ms TR, 30 ms TE, and 72-degree flip angle). Voxels were 2.5mm<sup>3</sup> isotopic, with 24 slices. The slice prescription covered most of the occipital lobe, and the posterior part of both the temporal and parietal lobes. Images were corrected for B0 field inhomogeneity using CBI algorithms during offline image reconstruction.</p>
<p>In a separate session, we acquired two to three T1-weighted whole brain anatomical scans (MPRAGE sequence; 1mm<sup>3</sup>). Additionally, a T1-weighted &#x201C;inplane&#x2019;&#x2019; image was collected with the same slice prescription as the functional scans to aid alignment of the functional images to the high-resolution T1-weighted anatomical images. This scan had an inplane resolution of 1.25 &#x00D7; 1.25 mm and a slice thickness of 2.5 mm.</p>
</sec>
<sec id="s3a5">
<title>Data Preprocessing and Analysis</title>
<sec id="s3a5a">
<title>Data preprocessing</title>
<p>We co-registered and segmented the T1-weighted whole brain anatomical images into gray and white matter voxels using FreeSurfer&#x2019;s auto-segmentation algorithm (<ext-link ext-link-type="uri" xlink:href="surfer.nmr.mgh.havard.edu">surfer.nmr.mgh.havard.edu</ext-link>). Using custom software, vistasoft (<ext-link ext-link-type="uri" xlink:href="https://github.com/vistalab/vistasoft">https://github.com/vistalab/vistasoft</ext-link>), the functional data were slice-time corrected by resampling the time series in each slice to the center of each 1.5s volume. Data were then motion-corrected by co-registering all volumes of all scans to the first volume of the first scan. The first 8 volumes (12 seconds) of each scan were discarded for analysis to allow longitudinal magnetization and stabilized hemodynamic response.</p>
</sec>
<sec id="s3a5b">
<title>GLM analysis</title>
<p>For analysis of the temporal summation functional data, we used a variant of the GLM procedure&#x2014;GLM denoise (<xref ref-type="bibr" rid="c29">Kay et al., 2013b</xref>), a technique that improves signal-to-noise ratios by entering noise regressors into the GLM analysis. Noise regressors were selected by performing principle component analysis on voxels whose activities were unrelated to the task. The optimal number of noise regressors was selected based on cross-validated R<sup>2</sup> improvement (coefficient of determination). The input to GLM denoise was the pre-processed EPI data and a design matrix for each scan (13 distinct temporal profiles &#x00D7; number of volumes per scan), and the output was &#x00DF;-weights for each temporal profile for each voxel, bootstrapped 100 times across scans (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). For analysis, we normalized all 13 &#x00DF;-weights per voxel by the vector length and selected a subset of voxels (see <italic>Voxel selection)</italic>. We then averaged the &#x00DF;-weights for a given temporal condition from the first bootstrap across voxels within each ROI and across all participants to get a mean; this gives one estimate of the mean response per ROI for a given condition. This was repeated for each condition, and then repeated for each of the 100 bootstraps, yielding a matrix of 100 &#x00D7; 13 for each ROI (bootstraps by temporal condition). GLM denoise was not applied to the visual field map measurements, since these experiments did not have an event-related design, and hence are not amenable to a GLM analysis.</p>
</sec>
<sec id="s3a5c">
<title>ROI identification</title>
<p>We fitted a linear population (&#x2018;pRF&#x2019;) model (<xref ref-type="bibr" rid="c12">Dumoulin and Wandell, 2008</xref>) to each subject&#x2019;s retinotopy data (average of two scans). We made an initial guess of ROI locations by first projecting the maximum likelihood probabilistic atlas from <xref ref-type="bibr" rid="c51">Wang et al (2015)</xref> onto the cortical surface. Then we visualized eccentricity and polar angle maps derived from the pRF model fits and modified ROI boundaries based on visual inspection. For each participant, we defined nine bilateral ROIs (V1, V2, V3, hV4, VO-1/2, LO-1/2, TO-1/2, IPS-0/1) (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). For the second experiment (self-replication), in addition to the nine ROIs from the main experiment, we also identified a bilateral face-selective region of interest. This ROI included face-selective voxels in the inferior occipital gyrus (&#x2018;IOG-faces, or &#x2018;Occipital Face Area&#x2019;) and in the posterior fusiform (pFus, or &#x2018;FFA-1&#x2019;) (<xref ref-type="bibr" rid="c13">Gauthier et al., 2000</xref>; <xref ref-type="bibr" rid="c53">Weiner and Grill-Spector, 2010</xref>). We identified these areas by taking the difference between the mean fMRI response to all face images and the mean response to all noise images, and then thresholding the difference for voxels at the two anatomical locations (IOG and pFus), as described previously (<xref ref-type="bibr" rid="c53">Weiner and Grill-Spector, 2010</xref>; <xref ref-type="bibr" rid="c27">Kay et al., 2015</xref>).</p>
</sec>
<sec id="s3a5d">
<title>Voxel selection</title>
<p>All analyses were restricted to voxels that satisfy the following three criteria. First voxels must be located within 2-10&#xB0; (eccentricity) based on the pRF model. Second, voxels must have a positive &#x00DF;-weight for the average across all non-blank temporal conditions (and averaged across bootstraps). The bootstraps, computed by GLM denoise, were derived by sampling with replacement from the repeated scans. Third, voxels must have &#x003E; 3&#x0025; GLM R<sup>2</sup>. Voxels that satisfy all criteria were averaged within a participant to yield 13 beta weights per ROI per participant per 100 bootstraps. The data were then averaged across participants. Averaging within a participant prior to averaging across participants ensures that the contribution for each participant has the same weight, irrespective of the numbers of voxels per participant.</p>
</sec>
</sec>
</sec>
<sec id="s3b">
<label>2.2</label>
<title>Temporal pRF Models</title>
<p>We used two variants of a temporal pRF model, one linear and one non-linear, to predict neuronal summation measured using fMRI. All model forms take the time course of a spatial contrast pattern as input (<italic>T</italic><sub>input</sub>), and produce a predicted neuronal response time course as output. To predict the fMRI data (BOLD), we summed the predicted time course within a trial (&#x003C; 1 s) to yield one number per temporal condition. For model fitting, these numbers were compared to the fMRI &#x00DF;-weights, derived from the GLM denoise analysis.</p>
<sec id="s3b1">
<title>Models</title>
<sec id="s3b1a">
<title>Linear model</title>
<p>The linear model prediction is computed by convolving a neuronal impulse response function (IRF) with the stimulus time course (T<sub>input</sub>), and scaling by a gain factor (<italic>g</italic>)
<disp-formula id="ueqn1"><alternatives><graphic xlink:href="157628_ueqn1.gif"/></alternatives></disp-formula>
The time course is then summed for the fMRI predictions (plus an error term, <italic>e</italic>):
<disp-formula id="ueqn2"><alternatives><graphic xlink:href="157628_ueqn2.gif"/></alternatives></disp-formula>
For the IRF, we assumed a gamma function, parameterized by &#x03C4;<sub>1</sub>, of the form,
<disp-formula id="ueqn3"><alternatives><graphic xlink:href="157628_ueqn3.gif"/></alternatives></disp-formula>
Because the IRF was assumed to have unit area, the specific shape of the IRF has no effect on the predictions of the linear model, and the prediction reduces to:
<disp-formula id="ueqn4"><alternatives><graphic xlink:href="157628_ueqn4.gif"/></alternatives></disp-formula>
and the only value solved for is the gain factor, <italic>g</italic>.</p>
</sec>
<sec id="s3b1b">
<title>Compressive temporal summation model (CTS)</title>
<p>The CTS model is computed with a linear convolution, followed by a divisive normalization. The linear step is identical to the linear model. For the divisive normalization:
<disp-formula id="ueqn5"><alternatives><graphic xlink:href="157628_ueqn5.gif"/></alternatives></disp-formula>
we solved for &#x03C4;, &#x03C3;, and gain factor <italic>g</italic> for the CTS model.</p>
</sec>
<sec id="s3b1c">
<title>Compressive summation model (CTS) with the power law implementation</title>
<p>To compute the model predicted neuronal response, we first computed the linear response by convolving an IRF (gamma function with variable time to peak &#x03C4;) with an input stimulus time course. Then an exponent e is applied point-wise to the predicted linear output.</p>
<disp-formula id="ueqn6"><alternatives><graphic xlink:href="157628_ueqn6.gif"/></alternatives></disp-formula>
<p>To fit the CTS model to the fMRI data, we again summed the predicted response time series:
<disp-formula id="ueqn7"><alternatives><graphic xlink:href="157628_ueqn7.gif"/></alternatives></disp-formula>
and solved for &#x03C4;<sub>1</sub>, &#x03B5; and <italic>g</italic>.</p>
<p>Because of the nonlinearity, the specific shape of the impulse response function does matter, in contrast to the linear model. This version of the CTS model is identical to the prior one, except that the shape of the compressive non-linearity due to the power law is slightly different from the shape obtained using divisive normalization.</p>
</sec>
</sec>
<sec id="s3b2">
<title>Parameter estimation and model fitting</title>
<sec id="s3b2a">
<title>CTS model</title>
<p>Models were fit in two steps, a grid fit followed by a search fit.</p>
<p>For the grid fit, we computed the model responses to the 13 temporal conditions for 10,000 combinations of &#x03C4;<sub>1</sub> and &#x03B5; (&#x03C4;<sub>1</sub> values from 0.01 to 1 in steps of 0.01; &#x03B5; values from 0 to 1 in steps of 0.01). For each ROI, the parameter pair generating the predictions with the highest correlation to the data was then used as a seed for the search fit. This was repeated 100 times per ROI, once for each bootstrap of the data.</p>
<p>We then did a search fit using Matlab&#x2019;s <italic>fminsearch</italic>, 100 times per ROI, using the 100 sets of bootstrapped &#x03B2;-weights, and the 100 seed values as derived above. The search finds the parameters which minimize the squared error between predicted and measured &#x03B2;-weights. This gave us 100 estimates of each model parameter for each ROI, which we summarized by the median and 50&#x0025; confidence interval.</p>
</sec>
<sec id="s3b2b">
<title>Linear model</title>
<p>The linear model does not require a search or seeds. Instead, we fit the 100 bootstrapped data sets per ROI by linear regression, giving us 100 estimates of the gain factor, <italic>g</italic>, per ROI.</p>
</sec>
</sec>
<sec id="s3b3">
<title>Statistical analysis</title>
<p>We compared model accuracy of the CTS and the linear model. Because the models have different numbers of free parameters, it is important to obtain an unbiased estimated of model accuracy, which we did by leave-one-out cross validation. For each ROI, and for each of the 100 bootstrapped sets of &#x00DF;-weights, we fit 13 linear models and 13 CTS models by leaving out each of the 13 temporal stimuli. For each bootstrap, we thus obtained 13 left-out predictions, which were concatenated and compared to the 13 &#x00DF;-weights by coefficient of determination, R<sup>2</sup>:
<disp-formula id="ueqn8"><alternatives><graphic xlink:href="157628_ueqn8.gif"/></alternatives></disp-formula>
This yielded 100 R<sup>2</sup>&#x2019;s per ROI, and we summarized model accuracy as the median and 50&#x0025; confidence interval derived from these 100 values.</p>
<p>Note that the coefficient of determination, R<sup>2</sup>, is bounded by [&#x2212;&#x221E;, 1], as the residuals between model and data can be larger than the data. In contrast, r<sup>2</sup> is bounded by [0, 1].</p>
<sec id="s3b3a">
<title>Noise ceilings</title>
<p>The noise ceiling represents the highest accuracy a model can achieve given the signal- to-noise ratio in the data, irrespective of the specific model used. We computed noise ceilings stochastically based on the mean and standard error of the GLM-&#x00DF; weights from bootstrapped estimates, as implemented in (<xref ref-type="bibr" rid="c28">Kay et al., 2013a</xref>).</p>
</sec>
<sec id="s3b3b">
<title>Flat model</title>
<p>We computed a model that assumes the BOLD response to all stimuli are identical (&#x2018;flat model&#x2019;) as a further basis of comparison to the CTS and linear models. Like the CTS and linear models, the accuracy of the flat model was computed by leave-one-out cross validation. (The cross-validated predictions from the flat model are not quite identical across conditions, because the mean is affected by the left-out data.)</p>
</sec>
<sec id="s3b3c">
<title>Parameter Recovery</title>
<p>To estimate how well model parameters are specified, for each visual area, we simulated the CTS model responses by first generating the predicted fMRI &#x00DF;-weight for each temporal condition. The parameters used for simulation were the median of each of the parameters from the bootstrapped fits to the data. We then added noise to each &#x00DF;-weight by randomly sampling from a normal distribution whose standard deviation was matched to the standard error in the bootstrapped data, averaged across the temporal conditions. We added noise 1,000 time per ROI, and then solved the CTS model for the 1,000 simulated responses using the same procedure used with the actual data. The parameters recovered from this fitting procedure provide an estimate of how well specified each parameter is given the form of the model (including the parameters) and the noise level in the data.</p>
</sec>
</sec>
<sec id="s3b4">
<title>Public Data Sets and Software Code</title>
<p>To ensure that our computational methods are reproducible, all data and all software will be made publicly available via an open science framework site, <ext-link ext-link-type="uri" xlink:href="https://osf.io/v843t/">https://osf.io/v843t/</ext-link>. The software repository will include scripts of the form <italic>trf_mkFigure2</italic> to reproduce <xref ref-type="fig" rid="fig2">figure 2</xref>, etc., as in prior publications (e.g., <xref ref-type="bibr" rid="c56">Winawer and Parvizi, 2016</xref>).</p>
</sec>
</sec>
</sec>
<sec id="s4">
<label>3.</label>
<title>Results</title>
<sec id="s4a">
<label>3.1</label>
<title>Measuring temporal summation in visual cortex</title>
<p>In each trial of the experiment, participants viewed either one or two pulses of a static spatial contrast pattern. The pattern was an independently generated band-pass noise image (24&#xB0; diameter), used in prior studies of spatial encoding (<xref ref-type="bibr" rid="c28">Kay et al., 2013a</xref>; <xref ref-type="bibr" rid="c30">Kay et al., 2013c</xref>). For the two-pulse trials, the two spatial patterns were identical. Each trial used one of thirteen distinct time courses (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). The durations of the one-pulse stimuli and the ISIs of the two-pulse stimuli were the same: 0, 1, 2, 4, 8,16, 32, or 64 video frames of a 60 Hz monitor (i.e., 0, 17, 33, 67, 134, 267, 533 ms). Each pulse in the 2-pulse stimuli was 8 frames (134 ms). The 0-ms one-pulse stimulus was a blank (mean luminance), and the two-pulse stimulus with 0 ISI was identical to the one-pulse stimulus of twice the length (16 frames, 267 ms). Four participants were scanned, and data were binned into nine bilateral, eccentricity-restricted (2-10&#xB0;) visual areas defined from a separate retinotopy scan (<xref ref-type="fig" rid="fig2">Figure 2C</xref>).</p>
<p>The fMRI data were analyzed in two stages. First, we extracted the amplitude (&#x00DF;-weight) for each of the 13 temporal conditions using a variation of the general linear model, &#x201C;GLM denoise&#x201D; (<xref ref-type="bibr" rid="c29">Kay et al., 2013b</xref>), a technique that improves the signal-to-noise ratio by including noise regressors in the GLM (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Second, we fitted the temporal pRF model to the GLM &#x00DF;-weights, averaged across voxels within ROIs.</p>
</sec>
<sec id="s4b">
<label>3.2</label>
<title>Temporal summation in visual cortex is subadditive</title>
<p>We tested the linearity of the fMRI BOLD signal in each visual area. To do so, we assume a time-invariant linear system such that the BOLD amplitude (GLM &#x00DF;-weight) is proportional to the total stimulus duration within the trial. Due to the linearity assumption, the form of the neural impulse response function does not affect the pattern of the predicted BOLD amplitudes. For example, the linear prediction is that a stimulus of duration <italic>2t</italic> produces twice the amplitude as a stimulus of duration <italic>t</italic>, and the same amplitude as a two-pulse stimulus, with total duration 2t (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). This prediction is not borne out by the data. The response to a stimulus of length <italic>2t</italic> is about 75&#x0025; of the linear prediction in V1 and 50&#x0025; in area TO (a homolog of macaque areas MT and MST) (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, <bold>left panel</bold>). This systematic failure of linearity is found in all visual areas measured, with temporal summation ratios below 0.8 for all ROIs, and a tendency toward lower ratios in later areas (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). The BOLD amplitudes to all stimuli are low (&#x003C;1&#x0025;) because the stimuli are brief, compared to measurements of visual cortex using moving stimuli or a block design with multiple static stimuli, where percent BOLD changes can be several percent.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Sub-linear temporal summation in visual cortex.</title>
<p><italic>(A) Linear temporal summation prediction</italic>. The sum of the responses to two separate events (top) is equal to the response to the two events in the same trial, with or without a brief gap between them (bottom). <italic>(B) Sub-linear temporal summation</italic>. Gray dots are the measured responses to a 134-ms pulse, a 268-ms pulse, and two 134-ms pulses, with either a 17-ms or 134-ms gap between pulses. Plots show the mean across participants and 50&#x0025; CI (bootstrapped across repeated scans within each participant). The green circles and dotted lines are the linear prediction based on the response to the single 134-ms pulse. For V1, the measured responses are less than the linear prediction except when there is a long gap. For area TO, all responses are less than the linear prediction<italic>. (C) Temporal summation ratio</italic>. Temporal summation ratio is the response to a stimulus of length 2x divided by twice the response to a single pulse stimulus of length x, averaged across 5 stimulus pairs (17 and 34 ms, 34 and 68 ms, etc.). Linear summation occurs when the temporal summation ratio is 1. The temporal summation ratio is less than 1 in all visual areas, indicating compressive temporal summation. The ratio is higher in early visual areas (&#x007E;0.7 in V1-V3), and lower in later areas (&#x007E;0.5 to 0.65). Error bars represent the 50&#x0025; CI (bootstrapped across scans). The ROIs on the x-axis are arranged in order of increasing spatial pRF size at 5 deg eccentricity, as a proxy for order in the visual hierarchy.</p></caption>
<graphic xlink:href="157628_fig3.tif"/>
</fig>
<p>A further failure of linearity occurs for trials with two pulses and variable ISI: the response is larger when the ISI is longer, especially in V1. The linear prediction is that the amplitudes are the same, and double the response to the one-pulse stimulus (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, <bold>right</bold>). When the ISI is relatively long (528 ms), the response in V1 is close to the linear prediction made from the one-pulse stimulus. In TO, even with a long ISI the response is still well below the linear prediction. This pattern, whereby the response to a second stimulus is reduced for short ISIs, and larger for longer ISIs, is often called adaptation and recovery (<xref ref-type="bibr" rid="c38">Priebe et al., 2002</xref>; <xref ref-type="bibr" rid="c32">Kohn, 2007</xref>). For TO, the recovery time is longer than V1, and longer than the longest ISI we tested.</p>
</sec>
<sec id="s4c">
<label>3.3</label>
<title>The temporal subadditivity is captured by a compressive temporal summation model (CTS)</title>
<p>We modeled the temporal subadditivity with a compressive temporal summation model (&#x201C;CTS&#x201D;) (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The CTS model has a Linear-Nonlinear (LN) structure. The linear stage convolves the stimulus time course with a temporal impulse response function (parameterized by the time constant t). The nonlinear stage passes the linear output through a static nonlinearity, divisive normalization. The normalization is implemented by squaring the linear response at each time point (as in (<xref ref-type="bibr" rid="c17">Heeger, 1992</xref>)), and dividing this by the sum of two terms, a semi-saturation constant (&#x03C3;<sup>2</sup>) and the squared linear response (<xref ref-type="bibr" rid="c17">Heeger, 1992</xref>). Squaring is widely used for modeling neural computations such as color (<xref ref-type="bibr" rid="c21">Helmholtz, 1886</xref>; <xref ref-type="bibr" rid="c31">Koenderink et al., 1972</xref>) and motion (<xref ref-type="bibr" rid="c1">Adelson and Bergen, 1985</xref>; <xref ref-type="bibr" rid="c45">Simoncelli and Heeger, 1998</xref>). The normalization model was developed to describe responses at the level of single neurons. However, we can generalize it to an fMRI voxel by assuming that the neurons within a voxel share a normalization pool, and the voxel sums across neurons within it. In this case, the normalization equation has the same term in the numerator and denominator, as implemented in the CTS model. (See also: <italic>Relationship to Divisive Normalization</italic> in (<xref ref-type="bibr" rid="c28">Kay et al., 2013a</xref>).)<sup><xref ref-type="fn" rid="fn1">1</xref></sup> To relate the CTS model output to the BOLD signal, we summed the predicted CTS output for a trial, and scaled this by a gain parameter, <italic>g</italic>, to convert to units of percent BOLD change. We sum the CTS output to give a single value per temporal condition, which can be compared to the &#x00DF;-weight in each condition, fit from the GLM. If we instead convolve the time-varying CTS model prediction with an hRF, rather than convolving the summed CTS model prediction with the hRF, the predicted BOLD response is nearly identical (<xref ref-type="fig" rid="fig4">Figure 4B</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Compressive temporal summation (CTS) model.</title>
<p>(<italic>A</italic>) <italic>The CTS model</italic>. The CTS model takes the binary stimulus time course for a trial as input (1 whenever the contrast pattern is present, 0 whenever it is absent). The input is convolved with an impulse response function parameterized by &#x03C4; to produce a linear prediction. The second stage is a divisive normalization computation. The numerator is the linear prediction raised point-wise and squared. The denominator is the sum of the semisaturation constant, &#x03C3;<sup>2</sup>, and the squared linear response. A smaller s makes the output response more sublinear. Finally, the time-varying CTS prediction is summed and scaled (<italic>g</italic>) to predict the percent BOLD response. The CTS model was fit for each ROI by finding the values of &#x03C4;, &#x03C3;, and <italic>g</italic> that minimized the squared error between the model predictions and the GLM &#x00DF;-weights. <italic>(B) Predicted BOLD time series from CTS model predictions</italic>. CTS model predictions were computed for two stimuli, a 128-ms and a 512-ms single pulse stimulus (assuming tau &#x003D; 0.05; sigma &#x003D; 0.01). The CTS model predictions were then passed through a hemodynamic response function (hRF) in one of two ways, either by convolving the CTS model prediction with the hRF (dashed lines), or by convolving the hRF with a single number for each stimulus, the sum of the CTS model prediction (solid lines). For each stimulus, the predicted response to the summed CTS response and to the time-varying CTS response is almost identical. Further, the BOLD response to the longer stimulus is the same shape as the response to the briefer stimulus, just scaled in amplitude.</p>
</caption>
<graphic xlink:href="157628_fig4.tif"/>
</fig>
<p>We compared the CTS model to a linear model by measuring cross-validated accuracy. The CTS model is more accurate than the linear model for all areas (<xref ref-type="fig" rid="fig5">Figure 5</xref>). The linear model substantially underpredicts responses to short durations and overpredicts responses to long durations, whereas the CTS model does not. Further, the predictions of the linear model do not depend on ISI, whereas the CTS model correctly predicts that the response amplitude increases with longer ISI. The cross-validated CTS model predicts the left-out fMRI responses with accuracy between 81&#x0025; and 98&#x0025; across the 9 ROIs. This represents a large improvement compared to the linear model for every area (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). The improvement is especially pronounced in later than early areas (LO/TO/IPS vs. V1-V3).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>CTS model fits to BOLD data across visual areas.</title>
<p><italic>(A) Data and predictions</italic>. BOLD responses to each temporal condition averaged across participants are plotted as circles. The temporal conditions on the x-axis show increasing durations of one-pulse stimuli (0 to 533 ms; left) and increasing ISI of two-pulse stimuli (0 to 533 ms, right). Stimulus temporal conditions are as in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. Error bars show the 50&#x0025; CI bootstrapped across repeated scans. Predictions for the linear (green) and CTS (red) model fits are computed by leave-out-one-condition cross-validation. Shaded regions represent the 50&#x0025; CI of the predictions across bootstraps (not visible for most of the linear fit because the CI is narrow). <italic>(B)</italic> The cross-validated accuracy (R<sup>2</sup>) is higher for the CTS model, compared to the linear model in each area. Each circle represents the median crossvalidated R<sup>2</sup> for each model and the error bar is the 50&#x0025; CI across bootstraps.</p>
</caption>
<graphic xlink:href="157628_fig5.tif"/>
</fig>
<p>The CTS model is also more accurate than a flat model (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). The flat model predicts the same response amplitude to all stimuli. This indicates that although the BOLD responses are relatively small (low percent signal change) and compressive (similar for different duration stimuli), there are nonetheless meaningful differences in the response amplitudes to different temporal conditions. Importantly, the CTS model accurately captures these differences, as it is substantially more accurate than the flat model. One notable exception is area TO, where the BOLD responses are most compressive: here the CTS model is only slightly more accurate than the flat model (and both are much more accurate than the linear model). In contrast, the linear model is more accurate than the flat model only in early visual areas (V1-V3) and less accurate in higher visual areas.</p>
</sec>
<sec id="s4d">
<label>3.4</label>
<title>The CTS model fits capture systematic differences between areas</title>
<p>The CTS model is parameterized by &#x03C4;, &#x03C3;, and a gain factor, <italic>g</italic>. &#x03C4; is the time to peak in the temporal impulse response function, and therefore is related to temporal summation window length; &#x03C3;<sup>2</sup> is the semisaturation constant, and reflects how much the CTS prediction deviates from the linear prediction. When &#x03C3; is lower, the response is more compressive. In later visual areas (hV4 &#x2013; IPS), &#x03C3; is about 10 times lower than earlier areas (V1 &#x2013; V3ab) (&#x007E;0.003 v &#x007E;0.03; <xref ref-type="fig" rid="fig6">Figure 6A</xref>, right), consistent with temporal summation being more sub-linear in the model-free summary (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). The more pronounced sub-linearity later in the visual hierarchy is qualitatively similar to the pattern found for spatial summation (<xref ref-type="bibr" rid="c28">Kay et al., 2013a</xref>). A consequence of more compressive temporal summation is that the response amplitude varies less with minor changes in stimulus duration, just as greater compression of spatial summation predicts more tolerance to changes in size and position (<xref ref-type="bibr" rid="c28">Kay et al., 2013a</xref>). From the current fMRI data set, there is also a tendency toward shorter time constants (t) in earlier areas, with some exceptions (except for VO, V1-V3 have the smallest &#x03C4;, &#x007E;50 ms; <xref ref-type="fig" rid="fig6">Figure 6A</xref>).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>CTS model parameters and summary metrics.</title>
<p><italic>(A) CTS model parameter estimates</italic>. The estimated parameter &#x03C3; is smaller for later (&#x007E;0.003, hV4 &#x2013; IPS), compared to earlier visual areas (&#x007E;0.3, V1 &#x2013; V3ab), indicating that the temporal sum in the later visual areas deviates more from the linear sum. The time constant &#x03C4; is short in V1-V3, compared to most of the later visual areas. <italic>(B) CTS parameter recovery</italic>. The precision with which parameters can be fit depends on the noise level in the data and the specific parameter values. We simulated data using the median &#x03C4; and &#x03C3; from V1, V3ab, and LO, and the noise levels estimated from these areas. The analysis shows that under these conditions, &#x03C4;(x-axes) is specified most precisely in V1 and least precisely in V3ab; the opposite pattern is found for &#x03C3;(<italic>y</italic>-axes). <italic>(C) Summary metrics</italic>. Two summary metrics of the CTS model reveal a pattern across ROIs. R<sub>double</sub> is the ratio of the predicted response to a 200-ms pulse divided by twice the response to a 100-ms pulse. R<sub>double</sub> is below 1 for all ROIs, indicating sub-additivity, and decreases along the visual hierarchy (V1-V3, &#x007E;0.67, LO-IPS, &#x003C; 0.6). T<sub>ISI</sub> is the length of ISI required for the response to two 100-ms pulses to approach the linear prediction. T<sub>ISI</sub> is short in the earlier areas (V1-V3, &#x007E;100 ms) compared to most of the later areas.</p>
</caption>
<graphic xlink:href="157628_fig6.tif"/>
</fig>
<p>The precision of our parameter estimates in each area depends on the BOLD noise level (the confidence interval of the &#x00DF;-weights), as well as the specific parameters estimated for that area. To understand how these factors interact, we simulated 1,000 data sets for each of 3 areas-V1, V3ab, and LO. The simulations used the median parameter fits for each area (&#x03C4; and &#x03C3;) to generate a noiseless prediction. We then added noise independently for each of the 1,000 predictions, according to the noise level in the fMRI measures for that area. Finally, we solved the CTS model for each of the predicted set of responses and analyzed the parameters. This parameter recovery analysis reveals two important results. First, it shows that the parameters for the different areas are distinguishable: models solved from simulations matched to V1, say, are not confusable with models solved from simulations matched to V3ab or LO (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Second, the analysis shows that the precision of the parameter estimates differs across areas. For example, for V1, &#x03C4; is more precisely specified than &#x03C3;, whereas for LO, &#x03C3; is more precisely specified than &#x03C4; (<xref ref-type="fig" rid="fig6">Figure 6B</xref>, insets). V3ab is intermediate. These simulations are consistent with the observation that model solutions on the bootstrapped data show a smaller confidence interval for &#x03C4; than for &#x03C3; in V1, and the reverse for LO (<xref ref-type="fig" rid="fig6">Figure 6A</xref>).</p>
<p>To further examine the differences in temporal processing between ROIs, we summarized the CTS model predictions to each ROI response in terms of two metrics that have more directly interpretable units: R<sub>double</sub> and T<sub>ISI</sub> (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). R<sub>double</sub> is the ratio between the CTS-predicted BOLD response to a 100-ms stimulus and a 200-ms stimulus. Lower R<sub>double</sub> means more compressive temporal summation. Later visual areas have lower R<sub>double</sub> than earlier ones. T<sub>ISI</sub> is the minimal duration separating two 100-ms pulses such that the response to the paired stimuli is close to the linear prediction from the single stimulus. Similar to previous measurements at longer time scales (<xref ref-type="bibr" rid="c54">Weiner et al., 2010</xref>; <xref ref-type="bibr" rid="c33">Mattar et al., 2016</xref>), the recovery time is longer for later than earlier visual areas.</p>
</sec>
<sec id="s4e">
<label>3.5</label>
<title>Alternative implementation of CTS model</title>
<p>The nonlinear component of the CTS model was implemented as a divisive normalization. This model fit the data much more accurately than a linear model, and divisive normalization is a good descriptor of a wide range of neural phenomena (<xref ref-type="bibr" rid="c9">Carandini and Heeger, 2012</xref>). However, there are many choices of static nonlinearities. In prior work, a power law static nonlinearity was used to model compressive spatial summation in fMRI (<xref ref-type="bibr" rid="c28">Kay et al., 2013a</xref>; <xref ref-type="bibr" rid="c57">Winawer et al., 2013</xref>). Although the form of the two nonlinearities differ, we found that refitting the fMRI data with the power law nonlinearity produced highly similar results (<xref ref-type="fig" rid="fig7">Figure 7</xref>). The model accuracy was not distinguishable when using a power law vs divisive normalization, and the two derived parameters, R<sub>double</sub> and T<sub>isi</sub>, showed the same pattern. Hence the fMRI data in these experiments do not distinguish the two forms of the compressive nonlinearity. The power law nonlinearity has the advantage of ease of interpretation &#x2013; the exponent indicates how much the response deviates from linear. Divisive normalization has the advantage of strong support from many neural systems (<xref ref-type="bibr" rid="c9">Carandini and Heeger, 2012</xref>). It might be possible to distinguish the two by measuring responses to stimuli with very brief durations or very low contrasts.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Figure 7.</label>
<caption><title>CTS model implemented with a power-law nonlinearity.</title>
<p><italic>(A) Model description</italic>. The model is identical to that shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>, except that the static nonlinearity is a power-law (parameterized by e) rather than a divisive normalization. If e is 1, the model is linear, and if e is less than 1, it is compressive. <italic>(B) Cross-validated model fit to the data from the main fMRI experiment</italic>. The model describes the data with high accuracy. See <xref ref-type="fig" rid="fig4">figure 4A</xref> for plotting conventions. <italic>(C) Model parameters</italic>. The estimated exponent e is below 1 in each area, and is lower (more sub-linear) in later areas (&#x007E;0.15, hV4-IPS versus &#x007E;0.25, V1-V3ab). The time constant &#x03C4; is similar to that observed for the normalization fit. <italic>(D) Summary metrics</italic>. R<sub>double</sub> is below 1 for all ROIs, indicating sub-additivity, and decreases along the visual hierarchy (V1-V3, &#x007E;0.67, LO-IPS, &#x003C; 0.62). T<sub>ISI</sub> is short in the earlier areas (V1-V3, &#x007E;100 ms) compared to most of the later areas.</p>
</caption>
<graphic xlink:href="157628_fig7.tif"/>
</fig>
<p>One difference between the two nonlinearities is the precision in which parameters are recovered. For example, &#x03C4; is recovered with high precision for most visual areas in the divisive normalization implementation; for the power-law implementation, the exponent e is recovered more accurately than &#x03C4; (simulations not shown).</p>
</sec>
<sec id="s4f">
<label>3.6</label>
<title>Eccentricity</title>
<p>Prior work has shown that temporal encoding in V1 differs between fovea and periphery (<xref ref-type="bibr" rid="c25">Horiguchi et al., 2009</xref>). In a separate analysis, we asked whether the CTS model parameters differed as a function of eccentricity. We did not find reliable differences for parafovea (2-5 deg) versus periphery (5-10 deg), either in the response amplitude (<xref ref-type="fig" rid="fig8">Figure 8A</xref>) or in the summary metrics (<xref ref-type="fig" rid="fig8">Figure 8B</xref>). This may be due to the limited range of eccentricities: <xref ref-type="bibr" rid="c25">Horiguchi et al (2009)</xref> found the biggest difference in temporal sensitivity between fovea and the far periphery (20-60 deg), with only minimal differences between the low-to-mid eccentricity bins we tested.</p>
<fig id="fig8" position="float" fig-type="figure">
<label>Figure 8.</label>
<caption><title>CTS model fits by eccentricity.</title>
<p>Data from the main fMRI experiment are re-plotted separating each ROI into 2 eccentricity bins. <italic>(A) CTS model fit to low and high eccentricity bins</italic>. The left panels are the data and CTS model fits restricted to voxels with population receptive field centers within 2 &#x2013; 5&#xB0;. The right panels are data and CTS model fits restricted to voxels with 5 &#x2013; 10&#xB0; eccentricity. <italic>(B) Summarized metrics for different eccentricity bins</italic>. The summarized metrics do not differ systematically between the two eccentricity ranges. . Each dot represents the median of the metrics summarized for 100 bootstraps of data (across scans), error bars represent 50&#x0025; confidence interval.</p>
</caption>
<graphic xlink:href="157628_fig8.tif"/>
</fig>
</sec>
<sec id="s4g">
<label>3.7</label>
<title>Self-replication (experiment 2)</title>
<p>We conducted a separate experiment with the identical temporal profiles and two different classes of images &#x2013; pink noise and faces embedded in pink noise (<xref ref-type="fig" rid="fig9">Figure 9A</xref>). This experiment tests the generalizability across spatial pattern. Because faces were used as one of the textures in this experiment, we included an additional region of interest &#x2013; a face-selective area, which is a combination of the occipital face area (OFA) and the fusiform face area (FFA). A single model was fit to each ROI for each participant, assuming independent gain parameters for the two stimulus classes, and the same time constant and semi-saturation constant. The results from the main experiment hold: all visual areas in the second experiment sum sub-linearly in time, with the CTS model fitting the data more accurately than the linear model (<xref ref-type="fig" rid="fig9">Figure 9B</xref>). Moreover, as with the main experiment, later areas tended to sum more sub-linearly compared to the earlier ones (<xref ref-type="fig" rid="fig9">Figure 9C</xref>). The response amplitudes are slightly lower than those in the main experiment due to stimulus selectivity, and the responses are noisier due to fewer trials per condition; otherwise the pattern of responses is highly similar.</p>
<fig id="fig9" position="float" fig-type="figure">
<label>Figure 9.</label>
<caption><title>FMRI data and model fits from a second experiment.</title>
<p><italic>(A) Stimuli and V1 responses</italic>. The two stimulus classes &#x2013; noise patterns and faces embedded in noise patterns, were randomly interleaved within runs. Temporal conditions were identical to those in <xref ref-type="fig" rid="fig4">Figure 4</xref>. The general pattern of responses and model fits are highly similar to those in the main experiment, with the CTS model fitting the data much more accurately than the linear model. <italic>(B) CTS model fit to extrastriate visual areas</italic>. The CTS model (red) fit the data more accurately than the linear model in all visual areas. <italic>(C) Parameters derived using the CTS model fit</italic>. The derived metrics, R<sub>double</sub> and T<sub>ISI</sub> show similar patterns as in the main experiment: decreased R<sub>double</sub> and increased T<sub>ISI</sub> in higher visual areas.</p>
</caption>
<graphic xlink:href="157628_fig9.tif"/>
</fig>
</sec>
</sec>
<sec id="s5">
<label>4.</label>
<title>DISCUSSION</title>
<sec id="s5a">
<label>4.1</label>
<title>Summation and adaptation in visual cortex</title>
<p>We report that temporal summation is subadditive throughout human visual cortex. Across 10 visual areas, BOLD responses to long stimuli were less than the linear prediction from briefer stimuli. This sub-additivity was especially pronounced in areas anterior to V1-V3. We captured this effect with a new temporal receptive field model, comprised of a linear stage followed by a static non-linearity. This compressive temporal summation model made highly accurate predictions for the fMRI data, and in all visual areas was substantially more accurate than a linear model. A single model accurately predicted two phenomena: subadditivity in the duration-response function and adaptation over short time scales (interstimulus intervals ranging from 0 to 528 ms). This indicates that both effects&#x2013;the subadditivity with respect to duration and the response reduction to repeated stimuli&#x2013;may arise from the same underlying processes.</p>
<p>A wide range of prior experimental measures are consistent with temporal subadditivities in visual cortex. For example, at the scale of 3 to 24 s, the fMRI response in V1 to a long presentation of a reversing contrast pattern is less than the prediction from a shorter presentation (<xref ref-type="bibr" rid="c5">Boynton et al., 1996</xref>); the fMRI response to repeated contrast patterns is larger for 1-s ISIs than 3-s ISIs (<xref ref-type="bibr" rid="c16">Heckman et al., 2007</xref>); the response of a V1 neuron to a steady flash is not predicted by its temporal frequency tuning and decreases over time (<xref ref-type="bibr" rid="c48">Tolhurst et al., 1980</xref>); the response of a neuron to a repeated stimulus is less than the response to the first stimulus (<xref ref-type="bibr" rid="c38">Priebe et al., 2002</xref>; <xref ref-type="bibr" rid="c35">Motter, 2006</xref>). Here we both quantified temporal subadditivities across the cortical visual hierarchy and account for the effects with a forward model. The model generalizes from the observed effects, as it takes arbitrary temporal patterns as inputs. The two operations &#x2013; linear summation and a compressive nonlinearity &#x2013; provide a simple and interpretable set of computations that can be used to characterize neural response properties across visual areas. For example, an implication of the t<sub>ISI</sub> measures is that when designing an fMRI experiment, stimuli must be spaced by at least 100 ms to avoid significant interactions in V1 responses, and at least 1 &#x03C3; in TO or IPS.</p>
</sec>
<sec id="s5b">
<label>4.2</label>
<title>Subadditivities in fMRI</title>
<p>In principle, the subadditivity could arise from the neuronal responses, coupling between neuronal processes and the BOLD signal, or a combination of both. There are several reasons to believe that at least a significant part of the observed non-linearity is neuronal in origin. First, single unit measurements of cortical neurons show temporal sub-additivities (<xref ref-type="bibr" rid="c48">Tolhurst et al., 1980</xref>; <xref ref-type="bibr" rid="c35">Motter, 2006</xref>), and it is more parsimonious to attribute subadditivities in the single unit and BOLD measurements to a single cause. Second, we find greater subadditivities in later than earlier visual areas, consistent with a cascade architecture in which later areas add additional non-linearities to the outputs from earlier areas (<xref ref-type="bibr" rid="c19">Heeger et al., 1996</xref>; <xref ref-type="bibr" rid="c45">Simoncelli and Heeger, 1998</xref>; <xref ref-type="bibr" rid="c10">DiCarlo et al., 2012</xref>; <xref ref-type="bibr" rid="c28">Kay et al., 2013a</xref>; <xref ref-type="bibr" rid="c30">Kay et al., 2013c</xref>); in contrast, there is no reason to expect that the coupling between neuronal signals and the hemodynamic response would become increasingly compressive along the visual hierarchy. Third, because even our longest stimuli were brief (&#x2264; 528 ms), thereby eliciting relatively small BOLD signals (&#x007E;0.5&#x0025;), it is unlikely that saturation of the BOLD signal for longer stimulus durations could explain the compressive response. For example, when similar stimuli are presented in a sequence of several images, the fMRI responses are several times larger (1-4&#x0025;) (<xref ref-type="bibr" rid="c28">Kay et al., 2013a</xref>; <xref ref-type="bibr" rid="c30">Kay et al., 2013c</xref>), indicating that the BOLD signal measured here was well below saturation. Therefore, overall our results indicate that the neuronal response underlying the BOLD signal shows significant temporal subadditivities, and that the subadditivity is more pronounced in later visual areas.</p>
<p>Multiple studies are consistent with the possibility that the linear approximation of the neural-to-BOLD transform is reasonably good (<xref ref-type="bibr" rid="c6">Boynton et al., 1989</xref>; <xref ref-type="bibr" rid="c20">Heeger et al., 2000</xref>; <xref ref-type="bibr" rid="c39">Rees et al., 2000</xref>). However, our interpretation of temporal compressive summation in the neural response does not rely on the assumption that the BOLD signal is <italic>exactly</italic> a linear transform of local neuronal activity. If, for example, the coupling reflects an approximately square root compression, as recently suggested by one group (<xref ref-type="bibr" rid="c3">Bao et al., 2015</xref>), then the stimulus-to-BOLD nonlinearity we observed would still imply a highly compressive neural response. This is easiest to appreciate for the power-law implementation of the CTS model. For example, the median exponent fit to the BOLD signal across ROIs ranged from 0.1 (IPS) to 0.28 (V1). If we assume that this includes a neurovascular compressive exponent of 0.5, then the stimulus-to-neural response would have exponents ranging from 0.2 (IPS) to 0.56 (V1), still highly compressive.</p>
</sec>
<sec id="s5c">
<label>4.3</label>
<title>Spatial and temporal subadditivities</title>
<p>Subadditive temporal summation is likely to have important functional consequences. The two ways we documented temporal subadditivities, a compressive function of duration for single stimuli, and a reduced response for paired stimuli with short ISIs, are consistent with neural adaptation: a reduced response to prolonged or repeated stimuli. These phenomena are thought to reflect adaptive changes to the local environment, rather than being a passive by-product of neural responses (<xref ref-type="bibr" rid="c52">Webster, 2015</xref>). For example, adaptation may serve to prioritize new information or act as gain control (<xref ref-type="bibr" rid="c47">Solomon and Kohn, 2014</xref>). An interesting consequence of subadditive temporal summation is that responses to stimuli of different durations are more similar to one another than they would be if summation were linear. This may be thought of as a form of <italic>duration or timing tolerance</italic>, analogous to size and position tolerance in spatial encoding, which are increasingly prominent in higher visual areas (<xref ref-type="bibr" rid="c28">Kay et al., 2013a</xref>). For example, in V1, as the stimulus size increases or the stimulus duration lengthens, the response amplitude increases substantially, whereas in area TO, the response amplitudes increase only slightly, indicating greater tolerance for size and duration (<xref ref-type="fig" rid="fig10">Figure 10</xref>).</p>
<fig id="fig10" position="float" fig-type="figure">
<label>Figure 10.</label>
<caption><title>Sub-additive spatial and temporal summation.</title>
<p>(A) BOLD responses pooled across voxels in V1 (left) and in TO (right) are plotted as a function of stimulus size. Circles and error bars are means and standard errors across bootstrapped estimates. A compressive spatial summation model (red), fit to separate data, predicts the responses slightly more accurately than a linear model (green) in V1, and substantially more accurately in TO. Adapted from <xref ref-type="fig" rid="fig8">figure 8</xref> in (<xref ref-type="bibr" rid="c28">Kay et al., 2013a</xref>). (B) A similar pattern is observed for duration, replotted from <xref ref-type="fig" rid="fig4">Figure 4A</xref>.</p>
</caption>
<graphic xlink:href="157628_fig10.tif"/>
</fig>
<p>While spatial and temporal subadditivities share some properties, they are independent findings and differ in detail. For example, V2 shows substantially more spatial subadditivity than V1 (<xref ref-type="fig" rid="fig9">fig 9b</xref> in (<xref ref-type="bibr" rid="c30">Kay et al., 2013c</xref>); <xref ref-type="fig" rid="fig7">fig 7b</xref> in (<xref ref-type="bibr" rid="c28">Kay et al., 2013a</xref>)), but a similar degree of temporal subadditivity (<xref ref-type="fig" rid="fig6">Figures 6</xref> and <xref ref-type="fig" rid="fig7">7</xref>). Moreover, temporal subadditivities are directional: the future cannot affect the past, whereas responses to two spatial locations can affect each other. Further, a system which is space-time separable could, in principle, exhibit saturation with space but be linear in time, or vice versa. It will be important in future work to develop an integrated model which accounts for spatial and temporal nonlinearities.</p>
</sec>
<sec id="s5d">
<label>4.4</label>
<title>Temporal window length</title>
<p>Our finding that time scales lengthen across the visual hierarchy is consistent with measurements of temporal dynamics at a larger scale. For example, temporal receptive window length was studied by measuring response reliability to scrambled movie segments (<xref ref-type="bibr" rid="c15">Hasson et al., 2008</xref>; <xref ref-type="bibr" rid="c24">Honey et al., 2012</xref>): In visual cortex, responses depended on information accumulated over &#x007E;1 s, whereas in anterior temporal, parietal and frontal areas the time scale ranged from &#x007E;12-36 s. Similarly, in event-related fMRI, the influence of prior trials was modeled with an exponential decay, with longer time constants in later areas: <xref ref-type="bibr" rid="c5">Boynton et al (1996)</xref> reported a time constant of &#x007E;1s in V1 for contrast reversing checkerboards, and <xref ref-type="bibr" rid="c33">Mattar et al (2016)</xref>, using static face images, reported short time constants in V1 (&#x007E;0.6s) and much longer constants in face areas (&#x007E;5s). In macaque, the timescale of autocorrelations in spike counts was longer for areas higher in the hierarchy (&#x007E;300 ms) compared to sensory areas (&#x007E; 75-100 ms; (<xref ref-type="bibr" rid="c36">Murray et al., 2014</xref>)). These studies used very different methods and resulted in a wide range of time-scale estimates. It will be important in future work to ask whether a forward model can account for the range of values.</p>
<p>Analyzing visual information at multiple temporal scales has benefits. First, accumulating information in the past is necessary for predicting the future, and a hierarchy of temporal windows may be useful for predictions over different time-scales (<xref ref-type="bibr" rid="c18">Heeger, 2017</xref>). Second, signal-to-noise ratios are optimized when the temporal scale of analysis is matched to the temporal scale of the event of interest (a &#x201C;matched filter&#x201D;); different visual areas extract information about different image properties, which in turn are likely to have different temporal (or spatiotemporal) distributions in natural viewing. Conversely, the time-scale of cortical areas may set the time-scale of integration for behavior. For example, words, faces, and global motion patterns are integrated over periods 5-10 times longer than textures and local motion patterns (<xref ref-type="bibr" rid="c23">Holcombe, 2009</xref>). These effects have not been connected to a neural model; modeling the time-scale of cortical areas critical for these tasks may help explain these large behavioral effects.</p>
</sec>
<sec id="s5e">
<label>4.6</label>
<title>Generalization and future directions</title>
<p>The CTS model parameters estimated from our main experiment are similar to those from the second experiment (self-replication), in which we used different stimulus images. Yet, just as with spatial pRF models, it is likely that our model will fail for certain tasks or stimuli (<xref ref-type="bibr" rid="c50">Wandell and Winawer, 2015</xref>). For example, sustained attention to the stimulus (<xref ref-type="bibr" rid="c44">Self et al., 2016</xref>), presence of a surround (<xref ref-type="bibr" rid="c2">Bair et al., 2003</xref>), non-separable spatiotemporal patterns (motion), and stimulus history of many seconds or more (<xref ref-type="bibr" rid="c54">Weiner et al., 2010</xref>), can all affect the time course, hence subadditivity of the response. By formulating a forward model of responses to large-field contrast stimuli during passive viewing, we provide a quantitative benchmark that can be used to guide interpretation of how other factors influence response dynamics, and a platform upon which to extend the model to new stimulus or task features. An important goal for future work is to develop a space-time model that simultaneously accounts for nonlinearities in spatial (<xref ref-type="bibr" rid="c28">Kay et al., 2013a</xref>) and temporal summation. Finally, our fMRI model contains a static nonlinearity. Measurements with finer temporal resolution such as intracranial EEG will be informative for understanding the time scale of the nonlinearities.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We thank David Heeger, Brian Wandell, and Mike Landy for comments on an earlier draft of this manuscript. We also thank Bosco Tjan, David Heeger, XJ Wang, Denis Pelli and Rachel Denison for helpful discussions and feedback as we developed our models and analyses. The research was supported by NIH grants R00-EY022116 and R01-MH111417 (J.W.)</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Adelson</surname> <given-names>EH</given-names></string-name>, <string-name><surname>Bergen</surname> <given-names>JR</given-names></string-name> (<year>1985</year>) <article-title>Spatiotemporal energy models for the perception of motion</article-title>. <source>J Opt Soc Am A</source> <volume>2</volume>:<fpage>284</fpage>&#x2013;<lpage>299</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Bair</surname> <given-names>W</given-names></string-name>, <string-name><surname>Cavanaugh</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Movshon</surname> <given-names>JA</given-names></string-name> (<year>2003</year>) <article-title>Time course and time-distance relationships for surround suppression in macaque V1 neurons</article-title>. <source>J Neurosci</source> <volume>23</volume>:<fpage>7690</fpage>&#x2013;<lpage>7701</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="other"><string-name><surname>Bao</surname> <given-names>P</given-names></string-name>, <string-name><surname>Purington</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Tjan</surname> <given-names>BS</given-names></string-name> (<year>2015</year>) <article-title>Using an achiasmic human visual system to quantify the relationship between the fMRI BOLD signal and neural response</article-title>. <source>Elife</source> <fpage>4</fpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Boynton</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Engel</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name> (<year>2012</year>) <article-title>Linear systems analysis of the fMRI signal</article-title>. <source>Neuroimage</source> <volume>62</volume>:<fpage>975</fpage>&#x2013;<lpage>984</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Boynton</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Engel</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Glover</surname> <given-names>GH</given-names></string-name>, <string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name> (<year>1996</year>) <article-title>Linear systems analysis of functional magnetic resonance imaging in human V1</article-title>. <source>J Neurosci</source> <volume>16</volume>:<fpage>4207</fpage>&#x2013;<lpage>4221</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Boynton</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Fargo</surname> <given-names>L</given-names></string-name>, <string-name><surname>Olson</surname> <given-names>CX</given-names></string-name>, <string-name><surname>Smallman</surname> <given-names>HS</given-names></string-name> (<year>1989</year>) <article-title>Category Effects in Color Memory</article-title>. <source>Color Research and Application</source> <volume>14</volume>:<fpage>229</fpage>&#x2013;<lpage>234</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Brainard</surname> <given-names>DH</given-names></string-name> (<year>1997</year>) <article-title>The Psychophysics Toolbox</article-title>. <source>Spat Vis</source> <volume>10</volume>:<fpage>433</fpage>&#x2013;<lpage>436</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Britten</surname> <given-names>KH</given-names></string-name>, <string-name><surname>Heuer</surname> <given-names>HW</given-names></string-name> (<year>1999</year>) <article-title>Spatial summation in the receptive fields of MT neurons</article-title>. <source>J Neurosci</source> <volume>19</volume>:<fpage>5074</fpage>&#x2013;<lpage>5084</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Carandini</surname> <given-names>M</given-names></string-name>, <string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name> (<year>2012</year>) <article-title>Normalization as a canonical neural computation</article-title>. <source>Nature reviews Neuroscience</source> <volume>13</volume>:<fpage>51</fpage>&#x2013;<lpage>62</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>DiCarlo</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Zoccolan</surname> <given-names>D</given-names></string-name>, <string-name><surname>Rust</surname> <given-names>NC</given-names></string-name> (<year>2012</year>) <article-title>How does the brain solve visual object recognition?</article-title> <source>Neuron</source> <volume>73</volume>:<fpage>415</fpage>&#x2013;<lpage>434</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Dong</surname> <given-names>DW</given-names></string-name>, <string-name><surname>Atick</surname> <given-names>JJ</given-names></string-name> (<year>1995</year>) <article-title>Statistics of Natural Time-Varying Images</article-title>. <source>Network-Comp Neural</source> <volume>6</volume>:<fpage>345</fpage>&#x2013;<lpage>358</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Dumoulin</surname> <given-names>SO</given-names></string-name>, <string-name><surname>Wandell</surname> <given-names>BA</given-names></string-name> (<year>2008</year>) <article-title>Population receptive field estimates in human visual cortex</article-title>. <source>Neuroimage</source> <volume>39</volume>:<fpage>647</fpage>&#x2013;<lpage>660</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Gauthier</surname> <given-names>I</given-names></string-name>, <string-name><surname>Skudlarski</surname> <given-names>P</given-names></string-name>, <string-name><surname>Gore</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Anderson</surname> <given-names>AW</given-names></string-name> (<year>2000</year>) <article-title>Expertise for cars and birds recruits brain areas involved in face recognition</article-title>. <source>Nat Neurosci</source> <volume>3</volume>:<fpage>191</fpage>&#x2013;<lpage>197</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Grill-Spector</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kourtzi</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name> (<year>2001</year>) <article-title>The lateral occipital complex and its role in object recognition</article-title>. <source>Vision research</source> <volume>41</volume>:<fpage>1409</fpage>&#x2013;<lpage>1422</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Hasson</surname> <given-names>U</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>E</given-names></string-name>, <string-name><surname>Vallines</surname> <given-names>I</given-names></string-name>, <string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Rubin</surname> <given-names>N</given-names></string-name> (<year>2008</year>) <article-title>A hierarchy of temporal receptive windows in human cortex</article-title>. <source>J Neurosci</source> <volume>28</volume>:<fpage>2539</fpage>&#x2013;<lpage>2550</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Heckman</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Bouvier</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Carr</surname> <given-names>VA</given-names></string-name>, <string-name><surname>Harley</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Cardinal</surname> <given-names>KS</given-names></string-name>, <string-name><surname>Engel</surname> <given-names>SA</given-names></string-name> (<year>2007</year>) <article-title>Nonlinearities in rapid event-related fMRI explained by stimulus scaling</article-title>. <source>Neuroimage</source> <volume>34</volume>:<fpage>651</fpage>&#x2013;<lpage>660</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name> (<year>1992</year>) <article-title>Normalization of cell responses in cat striate cortex</article-title>. <source>Vis Neurosci</source> <volume>9</volume>:<fpage>181</fpage>&#x2013;<lpage>197</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="confproc"><string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name> (<year>2017</year>) <source>Theory of cortical function</source>. <conf-name>Proc Natl Acad Sci</conf-name> <conf-loc>U S A</conf-loc>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="confproc"><string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name>, <string-name><surname>Movshon</surname> <given-names>JA</given-names></string-name> (<year>1996</year>) <source>Computational models of cortical visual processing</source>. <conf-name>Proc Natl Acad Sci</conf-name> <conf-loc>U S A</conf-loc> <volume>93</volume>:<fpage>623</fpage>&#x2013;<lpage>627</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Huk</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Geisler</surname> <given-names>WS</given-names></string-name>, <string-name><surname>Albrecht</surname> <given-names>DG</given-names></string-name> (<year>2000</year>) <article-title>Spikes versus BOLD: what does neuroimaging tell us about neuronal activity?</article-title> <source>Nat Neurosci</source> <volume>3</volume>:<fpage>631</fpage>&#x2013;<lpage>633</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="book"><string-name><surname>Helmholtz</surname> <given-names>Hv</given-names></string-name> (<year>1886</year>) <source>Helmholtz&#x2019;s treatise on physiological optics</source>. <publisher-loc>[Rochester, N.Y.]</publisher-loc>: <publisher-name>The Optical Society of America</publisher-name>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Heuer</surname> <given-names>HW</given-names></string-name>, <string-name><surname>Britten</surname> <given-names>KH</given-names></string-name> (<year>2002</year>) <article-title>Contrast dependence of response normalization in area MT of the rhesus macaque</article-title>. <source>Journal of neurophysiology</source> <volume>88</volume>:<fpage>3398</fpage>&#x2013;<lpage>3408</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Holcombe</surname> <given-names>AO</given-names></string-name> (<year>2009</year>) <article-title>Seeing slow and seeing fast: two limits on perception</article-title>. <source>Trends Cogn Sci</source> <volume>13</volume>:<fpage>216</fpage>&#x2013;<lpage>221</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Honey</surname> <given-names>CJ</given-names></string-name>, <string-name><surname>Thesen</surname> <given-names>T</given-names></string-name>, <string-name><surname>Donner</surname> <given-names>TH</given-names></string-name>, <string-name><surname>Silbert</surname> <given-names>LJ</given-names></string-name>, <string-name><surname>Carlson</surname> <given-names>CE</given-names></string-name>, <string-name><surname>Devinsky</surname> <given-names>O</given-names></string-name>, <string-name><surname>Doyle</surname> <given-names>WK</given-names></string-name>, <string-name><surname>Rubin</surname> <given-names>N</given-names></string-name>, <string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Hasson</surname> <given-names>U</given-names></string-name> (<year>2012</year>) <article-title>Slow cortical dynamics and the accumulation of information over long timescales</article-title>. <source>Neuron</source> <volume>76</volume>:<fpage>423</fpage>&#x2013;<lpage>434</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Horiguchi</surname> <given-names>H</given-names></string-name>, <string-name><surname>Nakadomari</surname> <given-names>S</given-names></string-name>, <string-name><surname>Misaki</surname> <given-names>M</given-names></string-name>, <string-name><surname>Wandell</surname> <given-names>BA</given-names></string-name> (<year>2009</year>) <article-title>Two temporal channels in human V1 identified using fMRI</article-title>. <source>Neuroimage</source> <volume>47</volume>:<fpage>273</fpage>&#x2013;<lpage>280</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Kastner</surname> <given-names>S</given-names></string-name>, <string-name><surname>De Weerd</surname> <given-names>P</given-names></string-name>, <string-name><surname>Pinsk</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Elizondo</surname> <given-names>MI</given-names></string-name>, <string-name><surname>Desimone</surname> <given-names>R</given-names></string-name>, <string-name><surname>Ungerleider</surname> <given-names>LG</given-names></string-name> (<year>2001</year>) <article-title>Modulation of sensory suppression: implications for receptive field sizes in the human visual cortex</article-title>. <source>Journal of neurophysiology</source> <volume>86</volume>:<fpage>1398</fpage>&#x2013;<lpage>1411</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="other"><string-name><surname>Kay</surname> <given-names>KN</given-names></string-name>, <string-name><surname>Weiner</surname> <given-names>KS</given-names></string-name>, <string-name><surname>Grill-Spector</surname> <given-names>K</given-names></string-name> (<year>2015</year>) <article-title>Attention Reduces Spatial Uncertainty in Human Ventral Temporal Cortex</article-title>. <source>Curr Biol</source>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Kay</surname> <given-names>KN</given-names></string-name>, <string-name><surname>Winawer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Mezer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wandell</surname> <given-names>BA</given-names></string-name> (<year>2013a</year>) <article-title>Compressive spatial summation in human visual cortex</article-title>. <source>Journal of neurophysiology</source> <volume>110</volume>:<fpage>481</fpage>&#x2013;<lpage>494</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Kay</surname> <given-names>KN</given-names></string-name>, <string-name><surname>Rokem</surname> <given-names>A</given-names></string-name>, <string-name><surname>Winawer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Dougherty</surname> <given-names>RF</given-names></string-name>, <string-name><surname>Wandell</surname> <given-names>BA</given-names></string-name> (<year>2013b</year>) <article-title>GLMdenoise: a fast, automated technique for denoising task-based fMRI data</article-title>. <source>Frontiers in neuroscience</source> <volume>7</volume>:<fpage>247</fpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Kay</surname> <given-names>KN</given-names></string-name>, <string-name><surname>Winawer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Rokem</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mezer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wandell</surname> <given-names>BA</given-names></string-name> (<year>2013c</year>) <article-title>A two-stage cascade model of BOLD responses in human visual cortex</article-title>. <source>PLoS Comput Biol</source> <volume>9</volume>:<fpage>e1003079</fpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Koenderink</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>van de Grind</surname> <given-names>WA</given-names></string-name>, <string-name><surname>Bouman</surname> <given-names>MA</given-names></string-name> (<year>1972</year>) <article-title>Opponent color coding: A mechanistic model and a new metric for color space</article-title>. <source>Kybernetik</source> <volume>10</volume>:<fpage>78</fpage>&#x2013;<lpage>98</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Kohn</surname> <given-names>A</given-names></string-name> (<year>2007</year>) <article-title>Visual adaptation: physiology, mechanisms, and functional benefits</article-title>. <source>Journal of neurophysiology</source> <volume>97</volume>:<fpage>3155</fpage>&#x2013;<lpage>3164</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Mattar</surname> <given-names>MG</given-names></string-name>, <string-name><surname>Kahn</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Thompson-Schill</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Aguirre</surname> <given-names>GK</given-names></string-name> (<year>2016</year>) <article-title>Varying timescales of stimulus integration unite neural adaptation and prototype formation</article-title>. <source>Current Biology</source> <volume>26</volume>:<fpage>1669</fpage>&#x2013;<lpage>1676</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Maunsell</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Newsome</surname> <given-names>WT</given-names></string-name> (<year>1987</year>) <article-title>Visual processing in monkey extrastriate cortex</article-title>. <source>Annual Reviews of Neuroscience</source> <volume>10</volume>:<fpage>363</fpage>&#x2013;<lpage>401</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Motter</surname> <given-names>BC</given-names></string-name> (<year>2006</year>) <article-title>Modulation of transient and sustained response components of V4 neurons by temporal crowding in flashed stimulus sequences</article-title>. <source>J Neurosci</source> <volume>26</volume>:<fpage>9683</fpage>&#x2013;<lpage>9694</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Murray</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Bernacchia</surname> <given-names>A</given-names></string-name>, <string-name><surname>Freedman</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Romo</surname> <given-names>R</given-names></string-name>, <string-name><surname>Wallis</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Cai</surname> <given-names>X</given-names></string-name>, <string-name><surname>Padoa-Schioppa</surname> <given-names>C</given-names></string-name>, <string-name><surname>Pasternak</surname> <given-names>T</given-names></string-name>, <string-name><surname>Seo</surname> <given-names>H</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>D</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>XJ</given-names></string-name> (<year>2014</year>) <article-title>A hierarchy of intrinsic timescales across primate cortex</article-title>. <source>Nat Neurosci</source> <volume>17</volume>:<fpage>1661</fpage>&#x2013;<lpage>1663</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Pelli</surname> <given-names>DG</given-names></string-name> (<year>1997</year>) <article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title>. <source>Spat Vis</source> <volume>10</volume>:<fpage>437</fpage>&#x2013;<lpage>442</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Priebe</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>Churchland</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Lisberger</surname> <given-names>SG</given-names></string-name> (<year>2002</year>) <article-title>Constraints on the source of short-term motion adaptation in macaque area MT.I. the role of input and intrinsic mechanisms</article-title>. <source>Journal of neurophysiology</source> <volume>88</volume>:<fpage>354</fpage>&#x2013;<lpage>369</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Rees</surname> <given-names>G</given-names></string-name>, <string-name><surname>Friston</surname> <given-names>K</given-names></string-name>, <string-name><surname>Koch</surname> <given-names>C</given-names></string-name> (<year>2000</year>) <article-title>A direct quantitative relationship between the functional properties of human and macaque V5</article-title>. <source>Nat Neurosci</source> <volume>3</volume>:<fpage>716</fpage>&#x2013;<lpage>723</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Rolls</surname> <given-names>ET</given-names></string-name>, <string-name><surname>Tovee</surname> <given-names>MJ</given-names></string-name> (<year>1995</year>) <article-title>The responses of single neurons in the temporal visual cortical areas of the macaque when more than one stimulus is present in the receptive field</article-title>. <source>Exp Brain Res</source> <volume>103</volume>:<fpage>409</fpage>&#x2013;<lpage>420</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Rust</surname> <given-names>NC</given-names></string-name>, <string-name><surname>Dicarlo</surname> <given-names>JJ</given-names></string-name> (<year>2010</year>) <article-title>Selectivity and tolerance (&#x201C;invariance&#x201D;) both increase as visual information propagates from cortical area V4 to IT</article-title>. <source>J Neurosci</source> <volume>30</volume>:<fpage>12978</fpage>&#x2013;<lpage>12995</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Rust</surname> <given-names>NC</given-names></string-name>, <string-name><surname>DiCarlo</surname> <given-names>JJ</given-names></string-name> (<year>2012</year>) <article-title>Balanced increases in selectivity and tolerance produce constant sparseness along the ventral visual stream</article-title>. <source>J Neurosci</source> <volume>32</volume>:<fpage>10170</fpage>&#x2013;<lpage>10182</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Schwartz</surname> <given-names>O</given-names></string-name>, <string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name> (<year>2001</year>) <article-title>Natural signal statistics and sensory gain control</article-title>. <source>Nat Neurosci</source> <volume>4</volume>:<fpage>819</fpage>&#x2013;<lpage>825</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Self</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Peters</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Possel</surname> <given-names>JK</given-names></string-name>, <string-name><surname>Reithler</surname> <given-names>J</given-names></string-name>, <string-name><surname>Goebel</surname> <given-names>R</given-names></string-name>, <string-name><surname>Ris</surname> <given-names>P</given-names></string-name>, <string-name><surname>Jeurissen</surname> <given-names>D</given-names></string-name>, <string-name><surname>Reddy</surname> <given-names>L</given-names></string-name>, <string-name><surname>Claus</surname> <given-names>S</given-names></string-name>, <string-name><surname>Baayen</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Roelfsema</surname> <given-names>PR</given-names></string-name> (<year>2016</year>) <article-title>The Effects of Context and Attention on Spiking Activity in Human Early Visual Cortex</article-title>. <source>PLoS Biol</source> <volume>14</volume>:<fpage>e1002420</fpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name>, <string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name> (<year>1998</year>) <article-title>A model of neuronal responses in visual area MT</article-title>. <source>Vision research</source> <volume>38</volume>:<fpage>743</fpage>&#x2013;<lpage>761</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Snow</surname> <given-names>M</given-names></string-name>, <string-name><surname>Coen-Cagli</surname> <given-names>R</given-names></string-name>, <string-name><surname>Schwartz</surname> <given-names>O</given-names></string-name> (<year>2016</year>) <article-title>Specificity and timescales of cortical adaptation as inferences about natural movie statistics</article-title>. <source>J Vis 16</source>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Solomon</surname> <given-names>SG</given-names></string-name>, <string-name><surname>Kohn</surname> <given-names>A</given-names></string-name> (<year>2014</year>) <article-title>Moving sensory adaptation beyond suppressive effects in single neurons</article-title>. <source>Curr Biol</source> <volume>24</volume>:<fpage>R1012</fpage>&#x2013;<lpage>1022</lpage>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Tolhurst</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Walker</surname> <given-names>NS</given-names></string-name>, <string-name><surname>Thompson</surname> <given-names>ID</given-names></string-name>, <string-name><surname>Dean</surname> <given-names>AF</given-names></string-name> (<year>1980</year>) <article-title>Non-linearities of temporal summation in neurones in area 17 of the cat</article-title>. <source>Exp Brain Res</source> <volume>38</volume>:<fpage>431</fpage>&#x2013;<lpage>435</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Tovee</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Rolls</surname> <given-names>ET</given-names></string-name>, <string-name><surname>Azzopardi</surname> <given-names>P</given-names></string-name> (<year>1994</year>) <article-title>Translation invariance in the responses to faces of single neurons in the temporal visual cortical areas of the alert macaque</article-title>. <source>Journal of neurophysiology</source> <volume>72</volume>:<fpage>1049</fpage>&#x2013;<lpage>1060</lpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Wandell</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Winawer</surname> <given-names>J</given-names></string-name> (<year>2015</year>) <article-title>Computational neuroimaging and population receptive fields</article-title>. <source>Trends Cogn Sci</source> <volume>19</volume>:<fpage>349</fpage>&#x2013;<lpage>357</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Wang</surname> <given-names>L</given-names></string-name>, <string-name><surname>Mruczek</surname> <given-names>RE</given-names></string-name>, <string-name><surname>Arcaro</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Kastner</surname> <given-names>S</given-names></string-name> (<year>2015</year>) <article-title>Probabilistic Maps of Visual Topography in Human Cortex</article-title>. <source>Cereb Cortex</source> <volume>25</volume>:<fpage>3911</fpage>&#x2013;<lpage>3931</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Webster</surname> <given-names>MA</given-names></string-name> (<year>2015</year>) <article-title>Visual Adaptation</article-title>. <source>Annu Rev Vis Sci</source> <volume>1</volume>:<fpage>547</fpage>&#x2013;<lpage>567</lpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Weiner</surname> <given-names>KS</given-names></string-name>, <string-name><surname>Grill-Spector</surname> <given-names>K</given-names></string-name> (<year>2010</year>) <article-title>Sparsely-distributed organization of face and limb activations in human ventral temporal cortex</article-title>. <source>Neuroimage</source> <volume>52</volume>:<fpage>1559</fpage>&#x2013;<lpage>1573</lpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Weiner</surname> <given-names>KS</given-names></string-name>, <string-name><surname>Sayres</surname> <given-names>R</given-names></string-name>, <string-name><surname>Vinberg</surname> <given-names>J</given-names></string-name>, <string-name><surname>Grill-Spector</surname> <given-names>K</given-names></string-name> (<year>2010</year>) <article-title>fMRI-adaptation and category selectivity in human ventral temporal cortex: regional differences across time scales</article-title>. <source>Journal of neurophysiology</source> <volume>103</volume>:<fpage>3349</fpage>&#x2013;<lpage>3365</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Weiss</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Adelson</surname> <given-names>EH</given-names></string-name> (<year>1998</year>) <source>Slow and smooth: A Bayesian theory for the combination of local motion signals in human vision</source>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Winawer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Parvizi</surname> <given-names>J</given-names></string-name> (<year>2016</year>) <article-title>Linking Electrical Stimulation of Human Primary Visual Cortex, Size of Affected Cortical Area, Neuronal Responses, and Subjective Experience</article-title>. <source>Neuron</source> <volume>92</volume>:<fpage>1213</fpage>&#x2013;<lpage>1219</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Winawer</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kay</surname> <given-names>KN</given-names></string-name>, <string-name><surname>Foster</surname> <given-names>BL</given-names></string-name>, <string-name><surname>Rauschecker</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Parvizi</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wandell</surname> <given-names>BA</given-names></string-name> (<year>2013</year>) <article-title>Asynchronous broadband signals are the principal source of the BOLD response in human visual cortex</article-title>. <source>Curr Biol</source> <volume>23</volume>:<fpage>1145</fpage>&#x2013;<lpage>1153</lpage>.</mixed-citation></ref>
</ref-list>
<fn-group>
<fn id="fn1">
<label><sup>1</sup></label>
<p>Technically, the normalization model amplifies the response when the linear response amplitude is low due to the squaring, and compresses the response when the amplitude is high; however, for all temporal conditions we tested, the model output is compressive (less than the linear prediction from a briefer stimulus).</p>
</fn>
</fn-group>
</back>
</article>