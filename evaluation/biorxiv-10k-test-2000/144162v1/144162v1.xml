<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/144162</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Bioinformatics</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Accurate and Fast feature selection workflow for high-dimensional omics data</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6579-6941</contrib-id>
<name>
<surname>Perez-Riverol</surname>
<given-names>Yasset</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kun</surname>
<given-names>Max</given-names>
</name>
<xref ref-type="aff" rid="a2">b</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Vizca&#x00ED;no</surname>
<given-names>Juan Antonio</given-names>
</name>
<xref ref-type="aff" rid="a1">a</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9894-6897</contrib-id>
<name>
<surname>Hitz</surname>
<given-names>Marc-Phillip</given-names>
</name>
<xref ref-type="aff" rid="a3">c</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Audain</surname>
<given-names>Enrique</given-names>
</name>
<xref ref-type="aff" rid="a3">c</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<aff id="a1"><label>a</label><institution>European Molecular Biology Laboratory, European Bioinformatics Institute (EMBL-EBI), Wellcome Trust Genome Campus</institution>, Hinxton, Cambridge, CB10 1SD, <country>UK</country>.</aff>
<aff id="a2"><label>b</label><institution>RStudio</institution>, Boston MA</aff>
<aff id="a3"><label>c</label><institution>Department of Congenital Heart Disease and Pediatric Cardiology, Universit&#x00E4;tsklinikum Schleswig&#x2013;Holstein Kiel</institution>, Kiel, <country>Germany</country>.</aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label>Corresponding author: <email>yperez@ebi.ac.uk</email>, European Molecular Biology Laboratory, EMBL-EBI, Wellcome Trust Genome Campus, Hinxton, Cambridge, CB10 1SD, UK.</corresp>
<fn id="n1"><p><email>enrique.audain@uksh.de</email>, Department of Congenital Heart Disease and Pediatric Cardiology, Universit&#x00E4;tsklinikum Schleswig&#x2013;Holstein Kiel, Kiel, Germany.</p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2017</year>
</pub-date>
<elocation-id>144162</elocation-id>
<history>
<date date-type="received">
<day>01</day>
<month>6</month>
<year>2017</year>
</date>
<date date-type="rev-recd">
<day>01</day>
<month>6</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>02</day>
<month>6</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="144162.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>We are moving into the age of &#x2018;Big Data&#x2019; in biomedical research and bioinformatics. This trend could be encapsulated in this simple formula: D &#x003D; S &#x00D7; F, where the volume of data generated (D) increases in both dimensions: the number of samples (S) and the number of sample features (F). Frequently, a typical bioinformatics problem (e.g. classification) includes redundant and irrelevant features that can result, in the worst-case scenario, in false positive results. Then, Feature Selection (FS) constitutes an enormous challenge. Despite the number and diversity of algorithms available, the proper choice of an approach for facing a specific problem often falls in a &#x2018;grey zone&#x2019;. In this study, we select a subset of FS methods to develop an efficient workflow and an R package for bioinformatics machine learning problems. We cover relevant issues concerning FS, ranging from domain&#x2019;s problems to algorithm solutions and computational tools. Finally, we use seven different proteomics and gene expression datasets to evaluate the workflow and guide the FS process.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Feature Selection</kwd>
<kwd>R programming</kwd>
<kwd>Bioinformatics</kwd>
<kwd>Big Data</kwd>
<kwd>Proteomics</kwd>
<kwd>Genomics</kwd>
<kwd>Principal Component Analysis</kwd>
<kwd>Support Vector Machine</kwd>
</kwd-group>
<counts>
<page-count count="24"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>INTRODUCTION</title>
<p>The term &#x2018;Big Data&#x2019; is often used to describe the huge volumes of information produced by modern systems such as mobile devices, tracking tools and sensors [<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c2">2</xref>]. In biomedical research, the growth of high-throughput (omics) technologies has resulted in an exponential growth in the dimensionality and sample size. This increase has two major directions: i) the number of samples processed, powered by novels machines (i.e. sequencers and mass spectrometers); and ii) the features, attributes and variables collected alongside each sample [<xref ref-type="bibr" rid="c3">3</xref>]. This high-dimensional environment becomes a challenge to many modelling tasks used in bioinformatics, ranging from sequence analysis to spectral analyses as well as literature mining. Reducing data complexity is therefore crucial for data analysis tasks, knowledge inference using machine learning algorithms, and data visualization [<xref ref-type="bibr" rid="c4">4</xref>&#x2013;<xref ref-type="bibr" rid="c6">6</xref>].</p>
<p>The &#x2018;curse of dimensionality&#x2019; (term first introduced by Bellman in 1957) [<xref ref-type="bibr" rid="c7">7</xref>] described the problem caused by the exponential increase in volume associated with adding extra dimensions to an Euclidean space. In this context, the typical bioinformatics problem involves both: relevant and redundant features. Therefore, a Feature Selection (FS) approach becomes a crucial and non-trivial task because: i) it provides a deeper insight into the underlying processes that are the foundation of the data; ii) it improves the performance (CPU-time and memory) of the machine learning (ML) step, by reducing the number of variables; and iii) it produces better model results avoiding overfitting. However, a FS algorithm brings an important decision in any machine learning workflow (e.g. classification of protein/gene expression profiles): are there redundant features (e.g. proteins or genes) in the dataset that are irrelevant and/or redundant for the biological study?</p>
<p>The most-common attempt to address the FS problem (the so-called univariate filtering approach) is to use a variable ranking method to filter out the least promising variables before using a multivariate method [<xref ref-type="bibr" rid="c8">8</xref>]. These methods have been used extensively in computational biology for cancer classification using microarray data [<xref ref-type="bibr" rid="c9">9</xref>, <xref ref-type="bibr" rid="c10">10</xref>]. However, correlation filters could prompt some loss of relevant features that are meaningless by themselves but that can be useful in combination. To overcome this effect, a set of algorithms has been proposed to combine the original variables into a new and smaller subset of features, such as Principal Component Analysis (PCA) and Linear Discriminant Analysis. In PCA [<xref ref-type="bibr" rid="c11">11</xref>], new orthogonal features (latent variables or principal components) are obtained by maximizing the variation of the original features. The number of the latent features (factors) can be much lower than the number of original features, so that the data can be visualized in a much lower-dimensional space. As correlation filters, PCA methods can reduce the number of variables by looking into the feature dependencies without taking into account the final learning model. In 1997, a powerful strategy emerged that combines a FS algorithm with a learning/classification step: the so-called <italic>wrapper</italic> methods [<xref ref-type="bibr" rid="c12">12</xref>]. These wrapper approaches (e.g. <italic>forward selection</italic> and <italic>backward elimination</italic>) can use the prediction performance of a given machine learning approach to assess the relative usefulness of different subsets of variables. An exhaustive search can be performed if the number of variables is not too large.</p>
<p>Due to the diversity of FS methods available, it is hard to choose the correct approach needed to accomplish a specific task beforehand (e.g. regression or classification). In 2007, Saeys and co-workers published an introduction to FS in bioinformatics [<xref ref-type="bibr" rid="c3">3</xref>]. Also, several reviews have focused on the application in computational biology of particular methods such as PCA [<xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c14">14</xref>] or Support Vector Machines (SVM) [<xref ref-type="bibr" rid="c15">15</xref>]. However, most of this work has been done to describe current methods in isolation and not to evaluate how they could be combined. In this manuscript, we developed a FS workflow and an R package for high-dimensional omics data analysis. The workflow combined univariate/multivariate correlation filters with wrapper feature backward elimination and it was applied to regression and classification problems. We benchmarked the individual steps of the described workflow, highlighting the optimal steps in different scenarios, using seven different omics datasets. Finally, we discuss major challenges when applying the described workflow to classification problems of high-dimensional omics data.</p>
</sec>
<sec id="s2">
<title>MATERIALS AND METHODS</title>
<sec id="s2a">
<title>Transcriptomic dataset of breast tumor samples (Dataset 1)</title>
<p>We first used a gene expression dataset (GEO (Gene Expression Omnibus) accession number: GSE5325) from <italic>Saal et al</italic>. [<xref ref-type="bibr" rid="c16">16</xref>], which has already been extensively studied before [<xref ref-type="bibr" rid="c13">13</xref>]. The authors performed a study using microarrays to measure the expression of 27,648 genes in 105 breast tumor samples. The dataset includes the estrogen receptor alpha status (0&#x003D;negative, 1&#x003D;positive), a transcription factor recognized as being important for stimulating the growth of a large proportion of breast cancers and used to explore co-expression [<xref ref-type="bibr" rid="c17">17</xref>].</p>
</sec>
<sec id="s2b">
<title>High-resolution isoelectric focusing proteomics dataset (Dataset 2)</title>
<p>The second dataset is the result of an electrophoresis experiment on peptide samples [<xref ref-type="bibr" rid="c18">18</xref>]. A total of 7,391 peptides were identified in 12 fractions, where each fraction corresponded to an experimental isoelectric point. This dataset has been used before to develop a machine learning model that can accurately predict the theoretical isoelectric points for peptides and proteins based on the amino acid sequence properties [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c19">19</xref>].</p>
</sec>
<sec id="s2c">
<title>Triple-Negative Breast Cancer (TNBC) dataset (Dataset 3)</title>
<p>A third dataset containing protein quantification data using a label free technique was included [<xref ref-type="bibr" rid="c20">20</xref>]. The dataset assembles a panel of 44 (including samples and technical replicates) human breast cell lines and clinical tumors for analyzing the proteomics landscape of TNBC. The studied cell lines cover mesenchymal-, luminal-, and basal-like subtypes, as well as three receptor-positive and one non-tumorigenic cell lines. Thus, the idea behind including this dataset was to evaluate the ability of the proposed FS workflow to classify subtypes of cellular lines.</p>
</sec>
<sec id="s2d">
<title>Transcriptomics analysis of left ventricles of mouse hearts (Dataset 4)</title>
<p>A fourth dataset included the results of a transcriptomics analysis of left ventricles of mouse hearts subjected to an isoproterenol challenge [<xref ref-type="bibr" rid="c21">21</xref>]. In the study, the authors utilized expression arrays from left ventricular (LV) tissues, with and without an isoproterenol treatment, to understand the genetic control of gene expression and its relationship with heart failure. Then, the issue arising here suggests a binary classification problem where the researcher could be interested in, in order to know the optimal feature subset which could best discriminate between both classes (treated and non-treated samples).</p>
</sec>
<sec id="s2e">
<title>Expression data from normal and prostate tumor tissues (Datasets 5, 6, and 7)</title>
<p>Recently, Li <italic>et al</italic>. have used several gene expression datasets to benchmark different FS algorithms [<xref ref-type="bibr" rid="c22">22</xref>]. From the original microarray datasets, we have selected three of those datasets (GEO accession number: GSE6919), to compare the FS workflow with the results obtained by <italic>Li et al</italic>. <bold>Note 1 (Supplemental Information 1)</bold> summarizes the main characteristics of the datasets described previously.</p>
</sec>
</sec>
<sec id="s3">
<title>RESULTS AND DISCUSSION</title>
<p>A good feature subset can be defined as one that contains features highly correlated with (predictive of) outcome, yet uncorrelated (independent) with (not predictive of) each other. Nevertheless, the existing diversity of FS methods makes it challenging to choose the correct one for the task at hand (<bold>Supplementary Information 1, Note 2</bold>). <xref ref-type="fig" rid="fig1">Figure 1</xref> represents the proposed overall workflow to perform FS in high-dimensional omics big data. First, a univariate correlation filter can be used before applying any wrapper approach, to determine the relation between each feature and the class or predicted variable. Then, a second filtering step (Correlation Matrix (CM) or PCA), can follow, in order to determine the dependencies between the different dataset features. Finally, backward elimination is achieved by wrapping a machine learning (ML) method, such as Random Forest and SVM around each example.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><p>Proposed workflow for FS including a filtering step with univariate and/or multivariate approaches, followed by a wrapper approach (recursive feature elimination).</p></caption>
<graphic xlink:href="144162_fig1.tif"/>
</fig>
<sec id="s3a">
<title>Workflow R-package</title>
<p>An R-package has been developed to reproduce the proposed workflow (<underline><ext-link ext-link-type="uri" xlink:href="https://github.com/enriquea/feseR">https://github.com/enriquea/feseR</ext-link></underline>). For its development five main R packages were used: i) <bold>Caret</bold> [<xref ref-type="bibr" rid="c23">23</xref>] (<bold>C</bold>lassification <bold>A</bold>nd <bold>RE</bold>gression <bold>T</bold>raining) (<underline><ext-link ext-link-type="uri" xlink:href="http://topepo.github.io/caret">http://topepo.github.io/caret</ext-link></underline>), containing a set of functions that attempt to streamline the process for creating predictive models; ii) <bold>randomForest</bold> [<xref ref-type="bibr" rid="c24">24</xref>] is a package enabling Random Forest analysis (<underline><ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/web/packages/randomForest/">https://cran.r-project.org/web/packages/randomForest/</ext-link></underline>); iii) <bold>prcomp</bold> is a native function included in the R package <italic>stats</italic>; iv) <italic><bold>Kernlab</bold></italic> [<xref ref-type="bibr" rid="c25">25</xref>] (<underline><ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/web/packages/kernlab/">https://cran.r-project.org/web/packages/kernlab/</ext-link></underline>) provides the user with basic kernel functionality (e.g., computing a kernel matrix), along with some utility functions, commonly used in kernel-based methods; and v) the <bold>FSelector</bold> package [<xref ref-type="bibr" rid="c26">26</xref>] (<underline><ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/web/packages/FSelector/">https://cran.r-project.org/web/packages/FSelector/</ext-link></underline>), which offers algorithms for filtering attributes (e.g. chi-squared, information gain, and linear correlation).</p>
<p>We have used the current FS workflow and R-package in combination two different machine learning (regression/classification) problems. Six of the datasets represent classification of (protein/gene) expression profiles and the last one a regression problem for the accurate estimation of isoelectric point of peptides and proteins. In the following sections, we discuss the results of combining the different steps of the FS workflow depending of the ML problem.</p>
</sec>
<sec id="s3b">
<title>Removing irrelevant features: Univariate correlation filtering</title>
<p>The univariate correlation filtering step removes all features that are not directly related to their class variables. When we applied this approach to <bold>Dataset 1</bold> it removed those genes with a non-correlated expression to the presence or absence of estrogen receptor alpha, reducing the number of genes from 8,534 (only those genes showing expression in all samples were considered) to 1,697. In <bold>Dataset 2</bold>, we used the univariate filter to remove features (amino acid properties) unrelated with the isoelectric point. <xref ref-type="fig" rid="fig2">Figure 2a</xref> shows the high-correlation found among the original 545 physicochemical peptides properties considered for the 7,391 peptides. We implemented a univariate correlation filter to remove all features that were not correlated with the isoelectric point, reducing the number of variables to 81 features (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). When we extended the analysis to the remaining benchmarking datasets, we observed that, in general, univariate correlation filtering removed more than 80&#x0025; of the original features that were not related to the predicted variable. As previously discussed by other authors [<xref ref-type="bibr" rid="c8">8</xref>], univariate correlation filtering should be always applied at early stages of any classification and/or regression process. However, univariate correlation filtering can only be used to study the relationship of each feature with a class variable, but cannot be applied to find the relationships among them. For this reason, a multivariate step (e.g. correlation matrix) was used (<xref ref-type="fig" rid="fig1">Figure 1</xref>) to remove the redundancy among highly-correlated features.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><p>(<bold>a</bold>) Correlation matrix for the 544 physicochemical (features) of the 7,391 peptides (samples) included in Dataset 2; (<bold>b</bold>) 81 relevant features after univariate correlation filtering; (<bold>c</bold>) the final 21 variables after the correlation-matrix filtering steps.</p></caption>
<graphic xlink:href="144162_fig2.tif"/>
</fig>
</sec>
<sec id="s3c">
<title>Reducing feature Complexity: CM or PCA</title>
<p>We implemented two different strategies (depending on the classification or regression problem) to reduce the number of variables, while keeping most of the original and relevant information: CM and PCA. <bold>Dataset 2</bold> is a good example of a dataset containing regression related problems. In this particular case, the aim was to predict more accurately the isoelectric point of peptides and proteins, using other physicochemical features of the peptides. Therefore, the final model should be based on, or be correlated to, the original features (because they would be used in the future to make a predictor that could be applied for other datasets). One of the simplest and most powerful filtering approaches to remove feature redundancy, while keeping original features, is the use of a CM filter. For example, peptides properties such as aromatic rings, bond and carbon atom counts are strongly correlated [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c27">27</xref>]. Therefore, any of these variables could be used as a proxy for all the others. <xref ref-type="fig" rid="fig2">Figure 2b</xref> shows the correlation matrix of the feature space after the univariate filtering step (81 variables). It should be noted that several features clustered together, suggesting a high-redundancy in the feature set. By applying the CM filter, it is possible to remove those that are redundant (or irrelevant) and to keep only a reduced feature set for subsequent analysis steps (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). The present workflow keeps only 21 variables (out of the original 545 features) for the final machine learning step (<xref ref-type="fig" rid="fig1">Figure 1</xref>). The current approach also reuses the final model in new datasets because the filtering steps preserve the original variables by only removing the redundant ones.</p>
<p>Opposite to <bold>Dataset 2</bold>, the other datasets constitute good examples of classification related problems. In addition to the MC filter approach, we implemented and studied the use of Principal Component Analysis (PCA) as a multivariate filter to reduce the number of features. PCA reduces the dimensionality of the data while retaining most of the variation in the predictor variables [<xref ref-type="bibr" rid="c13">13</xref>]. Thus, by using a few components, PCA can represent each sample by using relatively few (new) variables instead of (potentially) thousands of them. <xref ref-type="fig" rid="fig3">Figure 3</xref> shows the PCA performed in <bold>Dataset 1</bold>. The proportion of the variation present in all genes is encompassed within each of the principal components, with the first few components representing most of it (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). The cumulative variance analysis shows that most of the variance is contained in the first 30 principal components (75&#x0025;), where only 76 components reach a 95&#x0025; of variance (<xref ref-type="fig" rid="fig3">Figure 3b</xref>) and 104 components are enough to retain all the original variance. This number of variables is 10-fold smaller than the original 1,697 features obtained after applying the univariate correlation filter.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><p>(<bold>a</bold>) Proportion of variance and (<bold>b</bold>) cumulative variance of principal components for the analysis of <bold>Dataset 1</bold>.</p></caption>
<graphic xlink:href="144162_fig3.tif"/>
</fig>
<p>When the number of variables is larger than the number of samples, PCA can reduce the dimensionality of the samples to, at most, the number of samples, without losing information [<xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c28">28</xref>]. We obtained the same results when PCA was applied to the other relevant datasets (<bold>Dataset 3</bold> to <bold>Dataset 7</bold>, those with a classification problem, <bold>Supplementary Information 2</bold>). However, since the principal components are linear combinations of the original data, it is not obvious how model parameter estimates can relate back to the original variables. Thus, this method is not suitable for problems where it is required to keep the primary information (e.g. in the case of regression problems, <bold>Dataset 2</bold>).</p>
</sec>
<sec id="s3d">
<title>Optimizing the Feature selection: Wrapper Recursive Feature Elimination</title>
<p>All filtering FS approaches previously shown (e.g. correlation-based or PCA) are relatively easy to implement and computationally fast. Therefore, these algorithms represent a suitable choice in the first stage of any given FS pipeline. However, wrapper methods should be used in the last steps to find the &#x201C;optimal&#x201D; feature subsets, by iteratively selecting features based on classifier performance (<xref ref-type="fig" rid="fig1">Figure 1</xref>). The wrapper methods should be combined with cross-validation steps to improve the final results [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c29">29</xref>]. These cross-validation steps can be used to assess the results of the learning analysis (e.g. regression or classification) and help to generalize these steps to an independent dataset. The goal of cross-validation is to define a dataset to &#x201C;test&#x201D; the model in the training phase (i.e., the validation dataset), in order to limit problems like overfitting [<xref ref-type="bibr" rid="c29">29</xref>]. In the proposed workflow, we used a recursive feature elimination (backward elimination) approach in combination with two machine learning models (Random Forest and SVM) to systematically increase each machine learning step. The number of cross-validation iterations should be evaluated in detail because it could significantly increase the running time without improving the performance of the model prediction.</p>
<p>We implemented the wrapper backward elimination step in combination with the SVM radial kernel, in order to predict the isoelectric point using <bold>Dataset 2.</bold> <xref ref-type="table" rid="tbl1">Table 1</xref> shows the performance (regarding running time and model prediction accuracy) of the feature workflow for <bold>Dataset 2</bold>. We benchmarked all the FS combinations with the SVM model by removing each of them. Applying the SVM model alone (<bold>SVM</bold>) without FS or cross-validation helps to predict the isoelectric point with a high root-mean-square error (RMSE) of 0.88. In contrast, when both correlation filters (<bold>X2-CM-SVM</bold>) were applied, RMSE and running time decreased to 0.57 and 0.50 min, respectively. When the complete workflow (<bold>X2-CM-RFE-SVM-CV3</bold>) was used RMSE decreased to 0.33 min (<xref ref-type="table" rid="tbl1">Table 1</xref>). It should be noted that when pre-filtering was applied (<bold>RFE-SVM-CV3</bold>), RMSE decreased to 0.32 and two new variables were added to the SVM model. However, this improvement in performance (e.g. low RMSE) decreased the overall efficiency of the workflow by increasing the execution time three-fold. Also, we observed no changes where the number of cross-validation steps was increased (<xref ref-type="table" rid="tbl1">Table 1</xref>).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><p>Benchmark of the SVM regression model for <bold>Dataset 2</bold> applying different FS methods (<bold>SVM</bold>), no feature selection, (<bold>X2</bold>) univariate correlation alone, (<bold>CM</bold>) correlation matrix filtering, (<bold>RFE</bold>) and wrapper feature elimination. The figures indicated using the prefixes CV3, CV7 and CV10 correspond to the number of interactions in the cross-validation steps during the RFE feature selection.</p></caption>
<graphic xlink:href="144162_tbl1.tif"/>
</table-wrap>
<p>Wrapper backward elimination step provided a powerful method to optimize the final subset of variables in response to the regression SVM model. <xref ref-type="fig" rid="fig4">Figure 4</xref> shows the final results of the isoelectric point prediction (<bold>Dataset 2</bold>) for all FS combinations. Backward selection in combination with the cross-validation step enables a better estimation of the variable prediction (isoelectric point) in the regions where less experimental evidences exist (basic pH range). This workflow has been used in a recent approach to predict the isoelectric point and it has proven to predict the isoelectric point more accurately than any other algorithm so far. A similar implementation was applied to the remaining datasets (1,3-7) where a Random Forest model was wrapped around, using a recursive approach to evaluate the performance and the variable weight following different FS workflows. We first evaluated the Random Forest approach for FS without any filtering and parameter tuning as discussed before by <italic>D&#x00ED;az-Uriarte et al</italic>. [<xref ref-type="bibr" rid="c30">30</xref>]. In addition, four recursive feature elimination methods, wrapped with Random Forest, were combined as follows: <bold>RFE-RF</bold> without any pre-filtering step (i.e. other FS methods), PCA combined with <bold>RFE-RF</bold>, univariate correlation filtering (<bold>X2</bold>) combined with <bold>RFE-RF</bold>, and finally, all methods were used sequentially: <bold>X2-PCA-RFE-RF</bold> or <bold>X2-MC-RFE-RF</bold></p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><p>Error plot of predicted isoelectric point vs the experimental isoelectric point (<bold>Dataset 2</bold>): (<bold>SVM</bold>) applying FS or cross-correlation step; (<bold>X2-CM-SVM</bold>) adding correlation filters as the only steps for feature selection; (<bold>RFE-SVM-CV3</bold>) recursive feature elimination, three interactions of cross-validation combined with <bold>SVM</bold>; (<bold>X2-CM-RFE-SVM-CV3</bold>) considering the full FS workflow.</p></caption>
<graphic xlink:href="144162_fig4.tif"/>
</fig>
<p><xref ref-type="fig" rid="fig5">Figure 5</xref> shows the performance evaluation (for the expression datasets 1, 3-7) of each complete FS combination (<bold>X2-PCA-RFE-RF</bold> and <bold>X2-MC-RFE-RF</bold>) and the random forest classification without FS step. We use the approach previously reported by <italic>Pochet et al</italic>. [<xref ref-type="bibr" rid="c31">31</xref>], where 20-fold randomized test data were used to summarize the accuracy in the prediction (see detailed description in <bold>Supplementary Information 2</bold>). Also, we kept a 10-fold internal crossvalidation step in all implementations of recursive feature selection trials. The results shown that when any of the full FS approaches are applied the average accuracy is higher compare with the results when not FS is used (red box plots). Only, in <bold>Dataset 3</bold> the workflow using PCA is less efficient than the random forest without FS step which can be related with the low number of samples analyzed (44). Importantly, even when RF perform very well it retains all the original features on each making difficult to decided which features are more relevant for the classification (<bold>Supplementary Information 1</bold>, <xref ref-type="table" rid="tbl2">Table 2</xref>). Both FS workflows reduce the number of variables in all cases in more than 90&#x0025; (<bold>Supplementary Information 1</bold>, <xref ref-type="table" rid="tbl2">Table 2</xref>), with average accuracy always above 70&#x0025; (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Because both workflow shows similar performance and some users may want to select PCA (less variables) or MC (original features), the R-package allows to define which multivariate option use during the FS.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><p>Accuracy vs. Feature Selection combination for Expression datasets (1, 3, 4, 5, 6 and 7). (<bold>RF</bold>) Random Forest without previous feature selection step; (<bold>X2-CM-RFE-RF</bold>), random forest classification after the feature selection step using univariate correlation filter with matrix correlation and recursive feature elimination; (<bold>X2-PCA-RFE-RF</bold>), random forest classification after the feature selection step using univariate correlation filter with principal component analysis and recursive feature elimination. All methods include an internal cross-validation 10-fold step. All accuracy metrics were estimated following the approach previously reported by <italic>Pochet et al</italic>. [<xref ref-type="bibr" rid="c31">31</xref>], where 20-fold randomized test data were used to summarize the accuracy of the FS combination.</p></caption>
<graphic xlink:href="144162_fig5.tif"/>
</fig>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><p>Benchmarking of the Random Forest model (classification) for <bold>Dataset 1</bold>, when different FS methods are applied: (<bold>RF</bold>) Random Forest only, (<bold>RFE</bold>) wrapper recursive feature elimination with 10-times internal cross-validation, (<bold>PCA</bold>) Principal Component Analysis with (<bold>X2</bold>) univariate correlation filtering. Each method is applied 20 times with randomized and class-balanced training datasets. The accuracy values provided correspond to the average value.</p></caption>
<graphic xlink:href="144162_tbl2.tif"/>
</table-wrap>
<p><xref ref-type="table" rid="tbl2">Table 2</xref> summarizes the benchmark metrics (accuracy, standard deviation, number of final features and time) for each evaluated FS workflow (in <bold>Dataset 1</bold>). While all methods kept the accuracy in the range 83-88&#x0025;, when all methods were combined (proposed workflow) a lower standard deviation was obtained. Using a Random Forest model without FS, the classification process was faster than in the case of any other combination, keeping all the relevant features (1,969 of them). Including PCA and Recursive Feature Elimination (<bold>PCA-RFE-RF</bold>), we observed a strong feature reduction (7-10 components) and a better standard deviation (5.4). Selecting a univariate correlation filter (<bold>X2-RFE-RF</bold>), a lowest standard deviation was obtained (3.2).</p>
<p><xref ref-type="fig" rid="fig6">Figure 6</xref> visualizes the results of the Random Forest classification algorithm without (panels a, c, f) and with (panels b, d, e) a FS step; for Datasets 1, 3, and 4, respectively. The results show that the remaining features obtained allow to &#x2018;discriminate&#x2019; between the different samples classes or groups (see detailed description in <bold>Supplementary Information 2</bold>). It can be concluded that for those classification problems where the original features are needed, the PCA step could be removed without sacrificing general performance (accuracy, standard deviation, or CPU time). In contrast, univariate correlation filtering FS steps had a key impact on the final results of the Random Forest model by increasing the performance in all the studied combinations. As we pointed out earlier, PCA &#x2018;obfuscates&#x2019; the primary information, and thus, can potentially result in problems. When it is desirable to keep the &#x201C;initial nature&#x201D; of the variables, filtering methods (e.g. univariate correlation filter) exhibit a good performance (<bold><xref ref-type="table" rid="tbl1">Table 1</xref>-<xref ref-type="table" rid="tbl2">2</xref></bold>) with a considerable lower number of features (25 final features).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><p>Visualization of the classification process using the first two Principal Components (PC1 and PC2) from the original data before (panels a, c, and e) and after (panels b, d, and f), to apply the following FS workflow: Univariate correlation (X2) with Recursive Feature Elimination (RFE) wrapped with Random Forest (RF). The figure shows the classes distribution for <bold>Dataset 1</bold> (panels a-b), <bold>Dataset 3</bold> (panels c-d) and <bold>Dataset 4</bold> (panels e-f).</p></caption>
<graphic xlink:href="144162_fig6.tif"/>
</fig>
</sec>
<sec id="s3e">
<title>Summary of the benchmarking process</title>
<p>We have demonstrated the impact of the FS workflow in the classification and/or regression results as well as in the performance of the machine learning algorithm (CPU time and memory). Finally, we applied the same FS workflow to gene expression data from normal and prostate tumor tissues (<bold>Datasets 5, 6</bold> and <bold>7</bold>), and compared them with the results obtained by <italic>Li et al</italic>. [<xref ref-type="bibr" rid="c22">22</xref>], who used a similar approach on the same datasets (see Table 9 in [<xref ref-type="bibr" rid="c22">22</xref>]). Even though we observe a slight improvement in the classification accuracy in these three datasets (<xref ref-type="table" rid="tbl3">Table 3</xref>), the most notable differences were found in the number of features obtained by the final models and in the total runtime, using a similar computational platform. Thus, the results from the comparison reinforce our previous observations and validate the effectiveness of the FS workflow proposed in this manuscript.</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3:</label>
<caption><p>Performance comparison between the proposed approach (<bold>X2-PCA-RFE-RF</bold>) and the method reported by Li <italic>et al</italic>. [<xref ref-type="bibr" rid="c22">22</xref>]. The computer used in the original manuscript was an Intel(R) Core(TM) i5-4690 @ 3.5 GHz CPU, with 16 GB of RAM. In this study, we used an Intel(R) Core(TM) i5-4200 @ 2.5 GHz CPU, with 16 GB of RAM.</p></caption>
<graphic xlink:href="144162_tbl3.tif"/>
</table-wrap>
</sec>
<sec id="s3f">
<title>Conclusions</title>
<p>FS selection algorithms are playing a major role to select correct variables for different classification and regression problems. Nevertheless, choosing the appropriate algorithm (or combination of algorithms) is not a trivial task. Different studies have highlighted methods to perform FS, but unfortunately, a thorough comparison including proper benchmarking is still lacking. Another major challenge remains: how to efficiently combine different FS methods to improve the final results. The developed FS workflow shown in this manuscript combines major strengths of univariate filtering methods, with CM and PCA strategies, as well as recursive feature elimination in two well-known learning problems: classification and regression. When univariate filtering was used in both types of problems the number of features was reduced by 80&#x0025; without compromising the accuracy of the final model, and decreasing the CPU time of the learning model steps. The introduction of a wrapper method (recursive feature elimination) in combination with the learning model improved the accuracy in both cases. If the wrapper method is applied without a previous filtering step, the CPU-time becomes too high. Finally, we demonstrated that the use of an intermediate FS step to remove redundancy between variables and features can significantly increase the accuracy of the learning model. This can be achieved by transforming the original variables into new components (retaining most of the variability in the original values) using PCA or by removing redundant highly correlated variables.</p>
<p>Large efforts have taken place in recent years to adopt individual FS methods. However, in our opinion, a multiple FS step workflow offers more promising results. Future developments should focus on other fields where the number of samples is growing considerably (e.g. clinical genomics, text and literature mining), and on the combination of heterogeneous datasets from different sources.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>Y.P.-R. is supported by BBSRC &#x2018;PROCESS&#x2019; grant (BB/K01997X/1). J.A.V. acknowledges the Wellcome Trust (grant number WT101477MA) and EMBL core funding.</p>
</ack>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item><term>CM</term><def><p>Correlation Matrix</p></def></def-item>
<def-item><term>FS</term><def><p>Feature Selection</p></def></def-item>
<def-item><term>ML</term><def><p>Machine Learning</p></def></def-item>
<def-item><term>PCA</term><def><p>Principal Component Analysis</p></def></def-item>
<def-item><term>RFE</term><def><p>Recursive Feature Elimination</p></def></def-item>
<def-item><term>RMSE</term><def><p>Root Mean Square Error</p></def></def-item>
<def-item><term>RF</term><def><p>Random Forest</p></def></def-item>
<def-item><term>SVM</term><def><p>Support Vector Machine</p></def></def-item>
<def-item><term>TNBC</term><def><p>Triple-Negative Breast Cancer</p></def></def-item>
<def-item><term>X2</term><def><p>Univariate Correlation</p></def></def-item>
</def-list>
</glossary>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Lynch</surname> <given-names>C.</given-names></string-name> <article-title>Big data: How do your data grow?</article-title> <source>Nature</source>. <year>2008</year>;<volume>455</volume>(<issue>7209</issue>):<fpage>28</fpage>&#x2013;<lpage>9</lpage>. doi:<pub-id pub-id-type="doi">10.1038/455028a</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">18769419</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Perez-Riverol</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Bai</surname> <given-names>M</given-names></string-name>, <string-name><surname>da Veiga Leprevost</surname> <given-names>F</given-names></string-name>, <string-name><surname>Squizzato</surname> <given-names>S</given-names></string-name>, <string-name><surname>Park</surname> <given-names>YM</given-names></string-name>, <string-name><surname>Haug</surname> <given-names>K</given-names></string-name>, <etal>et al.</etal> <article-title>Discovering and linking public omics data sets using the Omics Discovery Index</article-title>. <source>Nature biotechnology</source>. <year>2017</year>;<volume>35</volume>(<issue>5</issue>):<fpage>406</fpage>&#x2013;<lpage>9</lpage>. doi: <pub-id pub-id-type="doi">10.1038/nbt.3790</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">28486464</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Saeys</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Inza</surname> <given-names>I</given-names></string-name>, <string-name><surname>Larranaga</surname> <given-names>P.</given-names></string-name> <article-title>A review of feature selection techniques in bioinformatics</article-title>. <source>Bioinformatics</source>. <year>2007</year>;<volume>23</volume>(<issue>19</issue>):<fpage>2507</fpage>&#x2013;<lpage>17</lpage>. doi:<pub-id pub-id-type="doi">10.1093/bioinformatics/btm344</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">17720704</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Barbu</surname> <given-names>A</given-names></string-name>, <string-name><surname>She</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Ding</surname> <given-names>L</given-names></string-name>, <string-name><surname>Gramajo</surname> <given-names>G.</given-names></string-name> <article-title>Feature Selection with Annealing for Computer Vision and Big Data Learning</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2017</year>;<volume>39</volume>(<issue>2</issue>):<fpage>272</fpage>&#x2013;<lpage>86</lpage>. doi:<pub-id pub-id-type="doi">10.1109/TPAMI.2016.2544315</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">27019473</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Perez-Riverol</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Audain</surname> <given-names>E</given-names></string-name>, <string-name><surname>Millan</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ramos</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Sanchez</surname> <given-names>A</given-names></string-name>, <string-name><surname>Vizcaino</surname> <given-names>JA</given-names></string-name>, <etal>et al.</etal> <article-title>Isoelectric point optimization using peptide descriptors and support vector machines</article-title>. <source>Journal of proteomics</source>. <year>2012</year>;<volume>75</volume>(<issue>7</issue>):<fpage>2269</fpage>&#x2013;<lpage>74</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jprot.2012.01.029</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">22326964</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname> <given-names>R</given-names></string-name>, <string-name><surname>Perez-Riverol</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Hermjakob</surname> <given-names>H</given-names></string-name>, <string-name><surname>Vizcaino</surname> <given-names>JA.</given-names></string-name> <article-title>Open source libraries and frameworks for biological data visualisation: a guide for developers</article-title>. <source>Proteomics</source>. <year>2015</year>;<volume>15</volume>(<issue>8</issue>):<fpage>1356</fpage>&#x2013;<lpage>74</lpage>. doi:<pub-id pub-id-type="doi">10.1002/pmic.201400377</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">25475079</pub-id>; PubMed Central PMCID: <pub-id pub-id-type="pub">PMCPMC4409855</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Bellman</surname> <given-names>R.</given-names></string-name> <article-title>Dynamic programming and Lagrange multipliers</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>1956</year>;<volume>42</volume>(<issue>10</issue>):<fpage>767</fpage>&#x2013;<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Michalak</surname> <given-names>K</given-names></string-name>, <string-name><surname>Kwa&#x015B;nicka</surname> <given-names>H.</given-names></string-name> <article-title>Correlation-based feature selection strategy in classification problems</article-title>. <source>International Journal of Applied Mathematics and Computer Science</source>. <year>2006</year>;<volume>16</volume>:<fpage>503</fpage>&#x2013;<lpage>11</lpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Tetko</surname> <given-names>IV</given-names></string-name>, <string-name><surname>Hall</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Frank</surname> <given-names>E</given-names></string-name>, <string-name><surname>Facius</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mayer</surname> <given-names>KF</given-names></string-name>, <etal>et al.</etal> <article-title>Gene selection from microarray data for cancer classification&#x002D;&#x002D;a machine learning approach</article-title>. <source>Computational biology and chemistry</source>. <year>2005</year>;<volume>29</volume>(<issue>1</issue>):<fpage>37</fpage>&#x2013;<lpage>46</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.compbiolchem.2004.11.001</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">15680584</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Makedon</surname> <given-names>F</given-names></string-name>, <string-name><surname>Pearlman</surname> <given-names>J.</given-names></string-name> <article-title>Tumor classification based on DNA copy number aberrations determined using SNP arrays</article-title>. <source>Oncol Rep</source>. <year>2006</year>;<volume>15</volume> Spec no.:<fpage>1057</fpage>&#x2013;<lpage>9</lpage>. PubMed PMID: <pub-id pub-id-type="pub">16525700</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="other"><string-name><surname>Jolliffe</surname> <given-names>I.</given-names></string-name> <article-title>Principal component analysis: Wiley Online Library</article-title>; <year>2002</year>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Kohavi</surname> <given-names>R</given-names></string-name>, <string-name><surname>John</surname> <given-names>GH.</given-names></string-name> <article-title>Wrappers for feature subset selection</article-title>. <source>Artificial intelligence</source>. <year>1997</year>;<volume>97</volume>(<issue>1&#x2013;2</issue>):<fpage>273</fpage>&#x2013;<lpage>324</lpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Ringner</surname> <given-names>M.</given-names></string-name> <article-title>What is principal component analysis?</article-title> <source>Nature biotechnology</source>. <year>2008</year>;<volume>26</volume>(<issue>3</issue>):<fpage>303</fpage>&#x2013;<lpage>4</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nbt0308-303</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">18327243</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Yeung</surname> <given-names>KY</given-names></string-name>, <string-name><surname>Ruzzo</surname> <given-names>WL.</given-names></string-name> <article-title>Principal component analysis for clustering gene expression data</article-title>. <source>Bioinformatics</source>. <year>2001</year>;<volume>17</volume>(<issue>9</issue>):<fpage>763</fpage>&#x2013;<lpage>74</lpage>. PubMed PMID: <pub-id pub-id-type="pub">11590094</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Yang</surname> <given-names>ZR.</given-names></string-name> <article-title>Biological applications of support vector machines</article-title>. <source>Brief Bioinform</source>. <year>2004</year>;<volume>5</volume>(<issue>4</issue>):<fpage>328</fpage>&#x2013;<lpage>38</lpage>. PubMed PMID: <pub-id pub-id-type="pub">15606969</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Saal</surname> <given-names>LH</given-names></string-name>, <string-name><surname>Johansson</surname> <given-names>P</given-names></string-name>, <string-name><surname>Holm</surname> <given-names>K</given-names></string-name>, <string-name><surname>Gruvberger-Saal</surname> <given-names>SK</given-names></string-name>, <string-name><surname>She</surname> <given-names>QB</given-names></string-name>, <string-name><surname>Maurer</surname> <given-names>M</given-names></string-name>, <etal>et al.</etal> <article-title>Poor prognosis in carcinoma is associated with a gene expression signature of aberrant PTEN tumor suppressor pathway activity</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2007</year>; <volume>104</volume>(<issue>18</issue>):<fpage>7564</fpage>&#x2013;<lpage>9</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.0702507104</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">17452630</pub-id>; PubMed Central PMCID: <pub-id pub-id-type="pub">PMCPMC1855070</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Duffy</surname> <given-names>MJ.</given-names></string-name> <article-title>Estrogen receptors: role in breast cancer</article-title>. <source>Crit Rev Clin Lab Sci</source>. <year>2006</year>;<volume>43</volume>(<issue>4</issue>):<fpage>325</fpage>&#x2013;<lpage>47</lpage>. doi:<pub-id pub-id-type="doi">10.1080/10408360600739218</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">16769596</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Perez-Riverol</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Sanchez</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ramos</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Schmidt</surname> <given-names>A</given-names></string-name>, <string-name><surname>Muller</surname> <given-names>M</given-names></string-name>, <string-name><surname>Betancourt</surname> <given-names>L</given-names></string-name>, <etal>et al.</etal> <article-title>In silico analysis of accurate proteomics, complemented by selective isolation of peptides</article-title>. <source>Journal of proteomics</source>. <year>2011</year>;<volume>74</volume>(<issue>10</issue>):<fpage>2071</fpage>&#x2013;<lpage>82</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jprot.2011.05.034</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">21658481</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Audain</surname> <given-names>E</given-names></string-name>, <string-name><surname>Ramos</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Hermjakob</surname> <given-names>H</given-names></string-name>, <string-name><surname>Flower</surname> <given-names>DR</given-names></string-name>, <string-name><surname>Perez-Riverol</surname> <given-names>Y.</given-names></string-name> <article-title>Accurate estimation of isoelectric point of protein and peptide based on amino acid sequences</article-title>. <source>Bioinformatics</source>. <year>2016</year>;<volume>32</volume>(<issue>6</issue>):<fpage>821</fpage>&#x2013;<lpage>7</lpage>. doi:<pub-id pub-id-type="doi">10.1093/bioinformatics/btv674</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">26568629</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Lawrence</surname> <given-names>RT</given-names></string-name>, <string-name><surname>Perez</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Hernandez</surname> <given-names>D</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>CP</given-names></string-name>, <string-name><surname>Haas</surname> <given-names>KM</given-names></string-name>, <string-name><surname>Irie</surname> <given-names>HY</given-names></string-name>, <etal>et al.</etal> <article-title>The proteomic landscape of triple-negative breast cancer</article-title>. <source>Cell Rep</source>. <year>2015</year>;<volume>11</volume>(<issue>4</issue>):<fpage>630</fpage>&#x2013;<lpage>44</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.celrep.2015.03.050</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">25892236</pub-id>; PubMed Central PMCID: <pub-id pub-id-type="pub">PMCPMC4425736</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Rau</surname> <given-names>C</given-names></string-name>, <string-name><surname>Avetisyan</surname> <given-names>R</given-names></string-name>, <string-name><surname>Ren</surname> <given-names>S</given-names></string-name>, <string-name><surname>Romay</surname> <given-names>MC</given-names></string-name>, <string-name><surname>Stolin</surname> <given-names>G</given-names></string-name>, <etal>et al.</etal> <article-title>Genetic Dissection of Cardiac Remodeling in an Isoproterenol-Induced Heart Failure Mouse Model</article-title>. <source>PLoS genetics</source>. <year>2016</year>;<volume>12</volume>(<issue>7</issue>):<fpage>e1006038</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pgen.1006038</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">27385019</pub-id>; PubMed Central PMCID: <pub-id pub-id-type="pub">PMCPMC4934852</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Li</surname> <given-names>S</given-names></string-name>, <string-name><surname>Oh</surname> <given-names>S.</given-names></string-name> <article-title>Improving feature selection performance using pairwise pre-evaluation</article-title>. <source>BMC bioinformatics</source>. <year>2016</year>;<volume>17</volume>:<fpage>312</fpage>. doi:<pub-id pub-id-type="doi">10.1186/s12859-016-1178-3</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">27544506</pub-id>; PubMed Central PMCID: <pub-id pub-id-type="pub">PMCPMC4992252</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Kuhn</surname> <given-names>M.</given-names></string-name> <article-title>Caret package</article-title>. <source>Journal of Statistical Software</source>. <year>2008</year>;<volume>28</volume>(<issue>5</issue>):<fpage>1</fpage>&#x2013;<lpage>26</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Liaw</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wiener</surname> <given-names>M.</given-names></string-name> <article-title>Classification and regression by randomForest</article-title>. <source>R news</source>. <year>2002</year>;<volume>2</volume>(<issue>3</issue>):<fpage>18</fpage>&#x2013;<lpage>22</lpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Zeileis</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hornik</surname> <given-names>K</given-names></string-name>, <string-name><surname>Smola</surname> <given-names>A</given-names></string-name>, <string-name><surname>Karatzoglou</surname> <given-names>A.</given-names></string-name> <source>kernlab-an S4 package for kernel methods in R. Journal of statistical software</source>. <year>2004</year>;<volume>11</volume>(<issue>9</issue>):<fpage>1</fpage>&#x2013;<lpage>20</lpage>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="website"><string-name><surname>Romanski</surname> <given-names>P</given-names></string-name>, <string-name><surname>Kotthoff</surname> <given-names>L</given-names></string-name>, <string-name><surname>Kotthoff</surname> <given-names>ML.</given-names></string-name> <article-title>Package &#x2018;FSelector&#x2019;</article-title>. URL <ext-link ext-link-type="uri" xlink:href="https://cran/.r-project.org/web/packages/FSelector/index.html">http://cran/.r-project.org/web/packages/FSelector/index.html</ext-link>; <year>2013</year>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Audain</surname> <given-names>E</given-names></string-name>, <string-name><surname>Sanchez</surname> <given-names>A</given-names></string-name>, <string-name><surname>Vizcaino</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Perez-Riverol</surname> <given-names>Y.</given-names></string-name> <article-title>A survey of molecular descriptors used in mass spectrometry based proteomics</article-title>. <source>Current topics in medicinal chemistry</source>. <year>2014</year>;<volume>14</volume>(<issue>3</issue>):<fpage>388</fpage>&#x2013;<lpage>97</lpage>. PubMed PMID: <pub-id pub-id-type="pub">24304317</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Chambers</surname> <given-names>SE</given-names></string-name>, <string-name><surname>Hoskins</surname> <given-names>PR</given-names></string-name>, <string-name><surname>Haddad</surname> <given-names>NG</given-names></string-name>, <string-name><surname>Johnstone</surname> <given-names>FD</given-names></string-name>, <string-name><surname>McDicken</surname> <given-names>WN</given-names></string-name>, <string-name><surname>Muir</surname> <given-names>BB.</given-names></string-name> <article-title>A comparison of fetal abdominal circumference measurements and Doppler ultrasound in the prediction of small-for-dates babies and fetal compromise</article-title>. <source>Br J Obstet Gynaecol</source>. <year>1989</year>;<volume>96</volume>(<issue>7</issue>):<fpage>803</fpage>&#x2013;<lpage>8</lpage>. PubMed PMID: <pub-id pub-id-type="pub">2669932</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Varma</surname> <given-names>S</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>R.</given-names></string-name> <article-title>Bias in error estimation when using cross-validation for model selection</article-title>. <source>BMC bioinformatics</source>. <year>2006</year>;<volume>7</volume>:<fpage>91</fpage>. doi:<pub-id pub-id-type="doi">10.1186/1471-2105-7-91</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">16504092</pub-id>; PubMed Central PMCID: <pub-id pub-id-type="pub">PMCPMC1397873</pub-id>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Diaz-Uriarte</surname> <given-names>R</given-names></string-name>, <string-name><surname>Alvarez de Andres</surname> <given-names>S.</given-names></string-name> <article-title>Gene selection and classification of microarray data using random forest</article-title>. <source>BMC bioinformatics</source>. <year>2006</year>;<volume>7</volume>:<fpage>3</fpage>. doi:<pub-id pub-id-type="doi">10.1186/1471-2105-7-3</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">16398926</pub-id>; PubMed Central PMCID: <pub-id pub-id-type="pub">PMCPMC1363357</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Pochet</surname> <given-names>N</given-names></string-name>, <string-name><surname>De Smet</surname> <given-names>F</given-names></string-name>, <string-name><surname>Suykens</surname> <given-names>JA</given-names></string-name>, <string-name><surname>De Moor</surname> <given-names>BL.</given-names></string-name> <article-title>Systematic benchmarking of microarray data classification: assessing the role of non-linearity and dimensionality reduction</article-title>. <source>Bioinformatics</source>. <year>2004</year>;<volume>20</volume>(<issue>17</issue>):<fpage>3185</fpage>&#x2013;<lpage>95</lpage>. doi:<pub-id pub-id-type="doi">10.1093/bioinformatics/bth383</pub-id>. PubMed PMID: <pub-id pub-id-type="pub">15231531</pub-id>.</mixed-citation></ref>
</ref-list>
</back>
</article>