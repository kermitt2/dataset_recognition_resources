<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="EN" article-type="research-article">
  <?origin publisher?>
  <?properties no_embargo?>
  <?properties open_access?>
  <?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
  <?DTDIdentifier.IdentifierType public?>
  <?SourceDTD.DTDName A++V2.4.dtd?>
  <?SourceDTD.Version 2.4?>
  <?ConverterInfo.XSLTName springer2nlm.xsl?>
  <?ConverterInfo.Version 1?>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Psychometrika</journal-id>
      <journal-title>Psychometrika</journal-title>
      <issn pub-type="ppub">0033-3123</issn>
      <issn pub-type="epub">1860-0980</issn>
      <publisher>
        <publisher-name>Springer-Verlag</publisher-name>
        <publisher-loc>New York</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">2792348</article-id>
      <article-id pub-id-type="pmid">20037635</article-id>
      <article-id pub-id-type="publisher-id">9075</article-id>
      <article-id pub-id-type="doi">10.1007/s11336-008-9075-y</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Theory and Methods</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>A Multivariate Multilevel Approach to the Modeling of Accuracy and Speed of Test Takers</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" corresp="yes">
          <name name-style="western">
            <surname>Klein&#xA0;Entink</surname>
            <given-names>R. H.</given-names>
          </name>
          <address>
            <phone>+31-53-4893650</phone>
            <email>r.h.kleinentink@gw.utwente.nl</email>
          </address>
          <xref ref-type="aff" rid="Aff1"/>
        </contrib>
        <contrib contrib-type="author">
          <name name-style="western">
            <surname>Fox</surname>
            <given-names>J.-P.</given-names>
          </name>
          <xref ref-type="aff" rid="Aff1"/>
        </contrib>
        <contrib contrib-type="author">
          <name name-style="western">
            <surname>van&#xA0;der Linden</surname>
            <given-names>W. J.</given-names>
          </name>
          <xref ref-type="aff" rid="Aff1"/>
        </contrib>
        <aff id="Aff1">Department of Research Methodology, Measurement and Data Analysis, University of Twente, P.O. Box 217, 7500 AE Enschede, The Netherlands </aff>
      </contrib-group>
      <pub-date pub-type="epub">
        <day>23</day>
        <month>8</month>
        <year>2008</year>
      </pub-date>
      <pub-date pub-type="ppub">
        <month>3</month>
        <year>2009</year>
      </pub-date>
      <volume>74</volume>
      <issue>1</issue>
      <fpage>21</fpage>
      <lpage>48</lpage>
      <history>
        <date date-type="received">
          <day>16</day>
          <month>2</month>
          <year>2007</year>
        </date>
        <date date-type="rev-recd">
          <day>28</day>
          <month>6</month>
          <year>2008</year>
        </date>
        <date date-type="accepted">
          <day>9</day>
          <month>7</month>
          <year>2008</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>&#xA9; The Author(s) 2008</copyright-statement>
      </permissions>
      <abstract xml:lang="EN">
        <p>Response times on test items are easily collected in modern computerized testing. When collecting both (binary) responses and (continuous) response times on test items, it is possible to measure the accuracy and speed of test takers. To study the relationships between these two constructs, the model is extended with a multivariate multilevel regression structure which allows the incorporation of covariates to explain the variance in speed and accuracy between individuals and groups of test takers. A Bayesian approach with Markov chain Monte Carlo (MCMC) computation enables straightforward estimation of all model parameters. Model-specific implementations of a Bayes factor (BF) and deviance information criterium (DIC) for model selection are proposed which are easily calculated as byproducts of the MCMC computation. Both results from simulation studies and real-data examples are given to illustrate several novel analyses possible with this modeling framework.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>speed</kwd>
        <kwd>accuracy</kwd>
        <kwd>IRT</kwd>
        <kwd>response times</kwd>
      </kwd-group>
      <custom-meta-wrap>
        <custom-meta>
          <meta-name>issue-copyright-statement</meta-name>
          <meta-value>&#xA9; The Psychometric Society 2009</meta-value>
        </custom-meta>
      </custom-meta-wrap>
    </article-meta>
  </front>
  <body>
    <sec id="Sec1" sec-type="introduction">
      <title>1. Introduction</title>
      <p>Response times (RTs) on test items can be a valuable source of information on test takers and test items, for example, when analyzing the speededness of the test, calibrating test items, detecting cheating, and designing a test (e.g., <xref ref-type="bibr" rid="CR9">Bridgeman &amp; Cline, 2004</xref>; <xref ref-type="bibr" rid="CR60">Wise &amp; Kong, 2005</xref>; <xref ref-type="bibr" rid="CR56">van der Linden &amp; Guo, 2008</xref>; <xref ref-type="bibr" rid="CR57">van der Linden, Breithaupt, Chuah, &amp; Zang, 2007</xref>; <xref ref-type="bibr" rid="CR54">van der Linden, 2007</xref>). With the introduction of computerized testing, their collection has become straightforward.</p>
      <p>It is important to make a distinction between the RTs on the test items and the speed at which a test taker operates throughout the test, especially when each person takes a different selection of items, as in adaptive testing. For two different test takers, it is possible to operate at the same speed, but produce entirely different RTs because the problems formulated in their items require different amounts of information to be processed, different problem-solving strategies, etc. Models for RTs should therefore have separate parameters for the test takers&#x2019; speed and the time intensities of the items.</p>
      <p>Another potential confounding relationship is that between speed and accuracy. It is well known that on complex tasks, these two are different constructs (see, for instance, <xref ref-type="bibr" rid="CR24">Kennedy, 1930</xref>; <xref ref-type="bibr" rid="CR40">Schnipke &amp; Scrams, 2002</xref>). <xref ref-type="bibr" rid="CR50">Tate (1948)</xref> was one of the first to examine the relationship between speed and accuracy on different tests. He concluded that for a controlled level of accuracy, each test taker worked at a constant speed. Furthermore, test takers working at a certain speed do not necessarily demonstrate the same accuracy.</p>
      <p>Some of these findings can be explained by the well-known speed-accuracy trade-off (e.g., <xref ref-type="bibr" rid="CR28">Luce, 1986</xref>). The trade-off reflects the fact that speed and accuracy are main determinants of each other. Also, they are negatively related. When a person chooses to increase his speed, then his accuracy decreases. But once his speed is fixed, his accuracy remains constant. Observe that this trade-off involves a within-person constraint only; it does not enable us to predict the speed or accuracy of one person from another taking the same test. In order to model the relationship between speed and accuracy adequately, we therefore need a model with different levels. This multilevel perspective has not yet been dominant in the psychometric literature on RT modeling. Instead, attempts have been made to integrate speed parameters or RTs into traditional single-level response models (<xref ref-type="bibr" rid="CR59">Verhelst, Verstralen, &amp; Jansen, 1997</xref>) or, reversely, response parameters into RT models (<xref ref-type="bibr" rid="CR51">Thissen, 1983</xref>). However, a hierarchical framework for modeling responses and RTs was introduced in <xref ref-type="bibr" rid="CR55">van der Linden (2008)</xref>. The framework has separate first-level models for the responses and RTs. For the response model, a traditional item-response theory (IRT) model was chosen. For the RTs, a lognormal model with separate person and item parameters was adopted, which has nice statistical properties and fitted actual response time data very well (<xref ref-type="bibr" rid="CR53">van der Linden, 2006</xref>). At the second level, the joint distributions of the person and item parameters in the two first-level models were modeled separately.</p>
      <p>Observe that because the framework in this paper does not model a speed-accuracy tradeoff, it can be used just as well to analyze responses and RTs to instruments for noncognitive domains, such as attitudes scales or personality questionnaires.</p>
      <p>Because the first-level parameters capture all systematic variation in the RTs, they can be assumed to be conditionally independent given the speed parameter. Likewise, the responses and RTs are assumed to be conditionally independent given the ability and speed parameter. Such assumptions of conditional independence are quite common in hierarchical modeling but may seem counterintuitive in the current context, where the speed-accuracy trade-off is often taken to suggest that the frequency of the correct responses increases if the RTs go up. However, this confusion arises when the earlier distinction between speed and RT is overlooked. The trade-off controls the choice of the levels of speed and accuracy by the individual test taker whereas the conditional independence assumptions address what happens with his response and RT distributions after the levels of speed and accuracy have been fixed.</p>
      <p>Besides being a nice implementation of the assumptions of local independence for RTs and responses, this framework allows for the incorporation of explanatory variables to identify factors that explain variation in speed and accuracy between individuals who may be nested within groups. The current paper addresses this possibility; its goal is to extend the framework with a third level with regression and group effects and to make this result statistically tractable. The result is a multivariate multilevel model for mixed response variables (binary responses and continuous RTs). At the person level, just as in the original framework, it allows us to measure both accuracy and speed. Test takers can therefore be compared to each other with respect to these measures. But at the higher levels, the extended framework also allows us to identify covariates and group memberships that explain the measures as well as their relationships. Also, the item parameters are allowed to correlate.</p>
      <p>Analysis of the extended model is performed in a fully Bayesian way. The motivation for the Bayesian treatment is its capability of handling complex models with many parameters that take all possible sources of variation into account. A new Gibbs sampling procedure (<xref ref-type="bibr" rid="CR18">Geman &amp; Geman, 1984</xref>; <xref ref-type="bibr" rid="CR16">Gelfand &amp; Smith, 1990</xref>) was developed which applies not only to the current framework but to the entire class of nonlinear multivariate multilevel models for mixed responses with balanced and unbalanced designs. All parameters can be estimated simultaneously without the need to fine-tune any parameters to guarantee convergence, for instance, as in a Metropolis- Hastings (MH) algorithm. Proper prior distributions can be specified that can be used both to incorporate a set of identifying restrictions for the model and to reflect the researcher&#x2019;s ideas about the parameter values and uncertainties. The estimation method can also handle incomplete designs with data missing at random.</p>
      <p>A model-specific implementation of the Bayes factor (<xref ref-type="bibr" rid="CR23">Kass &amp; Raftery, 1995</xref>) and the deviance information criterion (DIC) (<xref ref-type="bibr" rid="CR49">Spiegelhalter, Best, Carlin, &amp; van der Linde, 2002</xref>) is given, which can be used (i) to test specific assumptions about the distribution of speed and accuracy in a population of test takers and (ii) to iteratively build a structural multivariate multilevel component for the latent person parameters with fixed and random effects. Both statistics can be computed as by-products of the proposed Gibbs sampler. The DIC requires an analytic expression of the deviance associated with the likelihood of interest. Such an expression is offered for the multivariate multilevel model given the complete data, which includes augmented continuous data given the binary responses (<xref ref-type="bibr" rid="CR2">Albert, 1992</xref>), integrating out both random person parameters and other random regression effects at the level of groups of respondents. The posterior expectation of this complete DIC is taken over the augmented data using the output from the MCMC algorithm. Properties of the DIC, as well as the Bayes factor, were analyzed in a study with simulated data.</p>
      <p>In the next sections, we describe the entire model, specify the prior distributions, discuss the Gibbs sampler, and show how to apply the Bayes factor and the DIC to the current model. Then in a simulation study, the performance of the Gibbs sampler is addressed, whereby our interest is particularly in estimating the parameters in the structural component of the model. In a second simulation study, the relationships between the person parameters and the tests of multivariate hypotheses using the Bayes factor and the DIC are explored. Finally, the results from real-data examples are given and a few suggestions for extensions of the model are presented.</p>
    </sec>
    <sec id="Sec2">
      <title>2. A Multivariate Multilevel Model</title>
      <p>Various sources contribute to the variation between responses and RTs on test items. The total variation can be partitioned into variation due to (i) the sampling of persons and items, (ii) the nesting of responses within persons and items, and (iii) the nesting of persons within groups.</p>
      <p>Two measurement models describe the distributions of the binary responses and continuous RTs at level 1 of the framework. At level 2, two correlation structures are posited to allow for the dependencies between the level 1 model parameters. First, the person parameters for ability and speed, denoted as <bold><italic>&#x3B8;</italic></bold>} and <bold><italic>&#x3B6;</italic></bold>, respectively, are modeled to have a multivariate normal regression on covariates <bold>x</bold>, while group differences between these parameters are explained as a function of group-level covariates <bold>w</bold> at a third level. By specifying a higher-level regression structure for these random person parameters, it becomes possible to partition their total variance into within-group and between-group components. As a result, we are able to draw inferences about the person parameters for different groups simultaneously. Second, a correlation structure for the item parameters in the two measurement models is specified.</p>
      <p>The model can be used for various analyses. First, the analysis might focus on the item parameters; more specifically, the relationships between the characteristics of the items in the domain covered by the test. For example, we may want to know the correlation between the time intensity and difficulty parameters of the items. Second, the analysis could be about the structural relationships between explanatory information at the individual and/or group levels and the test takers&#x2019; ability and speed. For example, the variance components of the structural model help us to explore the partitioning of the variance of the speed parameters across the different levels of analysis. Third, the interest might be in the random effects in the model, e.g., to identify atypical individuals or groups with respect to their ability or speed.</p>
      <sec id="Sec3">
        <title>2.1. Level-1 Measurement Models for the Responses and RTs</title>
        <p>The probability of person <italic>i</italic> = 1, &#x2026;, <italic>n</italic><sub><italic>j</italic></sub> in group <italic>j</italic> = 1, &#x2026;, <italic>J</italic> answering item <italic>k</italic> = 1, &#x2026;, <italic>K</italic> correctly (<italic>y</italic><sub><italic>ijk</italic></sub> = 1) is assumed to follow the three-parameter normal ogive model:
<disp-formula id="Equ1"><tex-math id="M1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$P({y_{ijk}} = 1\left| {{\theta _{ij}},{a_k},{b_k},{c_k}) = {c_k} + (1 - {c_k})\Phi ({a_k}{\theta _{ij}} - {b_k})} \right.$$
\end{document}</tex-math></disp-formula>, where <bold><italic>&#x3A6;</italic></bold>(.) denotes the normal distribution function, <italic>&#x3D1;</italic><sub><italic>ij</italic></sub> the ability parameter of test taker <italic>ij</italic>, and <italic>a</italic><sub><italic>k</italic></sub>, <italic>b</italic><sub><italic>k</italic></sub>, and <italic>c</italic><sub><italic>k</italic></sub> the discrimination, difficulty and guessing parameters of item <italic>k</italic>, respectively.</p>
        <p>Typically, as the result of a natural lower bound at zero, RT distributions are skewed to the right. A family that describes this characteristic well is the log-normal distribution (<xref ref-type="bibr" rid="CR53">van der Linden, 2006</xref>; <xref ref-type="bibr" rid="CR39">Schnipke &amp; Scrams, 1997</xref>). Let <italic>t</italic><sub><italic>ijk</italic></sub> denote the log-response time of person <italic>i</italic> in group <italic>j</italic> on item <italic>k</italic>. We apply a normal model for <italic>t</italic><sub><italic>ijk</italic></sub>, with a mean depending on the speed at which the person works, denoted as <italic>&#x3B6;</italic><sub><italic>ij</italic></sub>, and the time intensity of the item, <italic>&#x3BB;</italic><sub><italic>k</italic></sub>. A higher <italic>&#x3BB;</italic><sub><italic>k</italic></sub> represents an item that is expected to consume more time. On the other hand, a higher <italic>&#x3B6;</italic><sub><italic>ij</italic></sub> means that the person works faster and a lower RT is expected. A parameter <italic>&#x3D5;</italic><sub><italic>k</italic></sub> is introduced, which can be interpreted as a time discrimination parameter.</p>
        <p>The response-time model at level 1 is given by:
<disp-formula id="Equ2"><tex-math id="M2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${t_{ijk}} = - {\phi _k}{\xi _{ij}} + {\lambda _k} + {\varepsilon _{\xi ijk}}$$
\end{document}</tex-math></disp-formula>, where <inline-formula id="IEqa"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqa.gif --><tex-math id="M3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${ \in _{{\xi _{ijk}}}} \sim N(0,\tau _k^2)$$
\end{document}</tex-math></inline-formula>. Notice that the interpretation of the model parameters in (<xref rid="Equ2" ref-type="">2</xref>) results in a different location of the minus sign compared to the IRT model. Also, there is a correspondence of the RT model with IRT models for continuous responses; for the latter, see, for instance, <xref ref-type="bibr" rid="CR32">Mellenbergh (1994)</xref> and <xref ref-type="bibr" rid="CR44">Shi &amp; Lee (1998)</xref>.</p>
      </sec>
      <sec id="Sec4">
        <title>2.2. Multivariate Two-Level Model for the Person Parameters</title>
        <p>The interest is in the relationships between the person parameters and the effects of potential explanatory variables. For convenience, we use the same set of explanatory variables for both types of person parameters; the generalization to the case of different variables is straightforward. Let <bold>x</bold><sub><italic>j</italic></sub> denote a known <italic>n</italic><sub><italic>j</italic></sub> &#xD7; <italic>Q</italic> covariate matrix (with ones in the first column for the intercept) and <inline-formula id="IEqb"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqb.gif --><tex-math id="M4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\beta _j} = ({\beta _{1j}},{\beta _{2j}})$$
\end{document}</tex-math></inline-formula> a <italic>Q</italic> &#xD7; 2 matrix of regression coefficients for group <italic>j</italic> = 1, &#x2026;, <italic>J</italic>. The coefficients are treated as random but they can be restricted to be common to all groups, leading to the case of one fixed effect.</p>
        <p>The regression of the two sets of person parameters at the individual level is defined by:
<disp-formula id="Equ3"><tex-math id="M5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\theta _{ij}} = {\rm{x}}_{ij}^t{\beta _{1j}} + {e_\theta }_{_{ij}}$$
\end{document}</tex-math></disp-formula>,
<disp-formula id="Equ4"><tex-math id="M6">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\xi _{ij}} = {\rm{x}}_{ij}^t{\beta _{2j}} + {e_\xi }_{_{ij}}$$
\end{document}</tex-math></disp-formula>. The two sets of regression equations are allowed to have correlated error terms; <inline-formula id="IEqc"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqc.gif --><tex-math id="M7">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$({e_{{\theta _{ij}}}},{e_{{\xi _{ij}}}})$$
\end{document}</tex-math></inline-formula> is taken to be bivariate normal with zero means and covariance matrix &#x3A3;<sub><italic>P</italic></sub>:
<disp-formula id="Equ5"><tex-math id="M8">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\sum {_P = \left[ {\matrix{ {\sigma _\theta ^2} &amp; \rho \cr \rho &amp; {\sigma _\xi ^2} \cr  } } \right]} $$
\end{document}</tex-math></disp-formula>.</p>
        <p>It is straightforward to extend the random effects model to explain variance in the &#x3B2;&#x2019;s by group level covariates (<xref ref-type="bibr" rid="CR48">Snijders &amp; Bosker, 1999</xref>). For instance, test takers can be grouped according to their social economic background or because they are nested within different schools. Although different covariates can be included for the <italic>Q</italic> intercept and slope parameters, for convenience, it will be assumed that the same covariate matrix is used for <bold><italic>&#x3B2;</italic></bold><sub>1</sub> and <bold><italic>&#x3B2;</italic></bold><sub>2</sub>. The covariates for the <italic>Q</italic> parameters of group <italic>j</italic> are contained in a matrix <bold>w</bold><sub><italic>j</italic></sub> of dimension <italic>Q</italic> &#xD7; <italic>S</italic>. That is, in total there are <italic>S</italic> covariates for each group, including the ones for the intercepts. The random effects <bold><italic>&#x3B2;</italic></bold><sub>1<italic>j</italic></sub> and <bold><italic>&#x3B2;</italic></bold><sub>2<italic>j</italic></sub> are then modeled as:
<disp-formula id="Equ6"><tex-math id="M9">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\beta _{1j}} = {{\rm{w}}_j}{\gamma _1} + {{\rm{u}}_{1j}}$$
\end{document}</tex-math></disp-formula>,
<disp-formula id="Equ7"><tex-math id="M10">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\beta _{2j}} = {{\rm{w}}_j}{\gamma _2} + {{\rm{u}}_{2j}}$$
\end{document}</tex-math></disp-formula>, where <bold><italic>&#x3B3;</italic></bold><sub>1</sub> and <bold><italic>&#x3B3;</italic></bold><sub>2</sub> are the vectors of regression coefficients of length <italic>S</italic>. The group-level error terms, (<bold>u</bold><sub>1<italic>j</italic></sub>,<bold>u</bold><sub>2<italic>j</italic></sub>), are assumed to be multivariate normally distributed with means zero and covariance matrix <bold>V</bold>. More stable parameter estimates can be obtained by restricting this covariance matrix to be block-diagonal with diagonal matrices <bold>V</bold><sub>1</sub> and <bold>V</bold><sub>2</sub>, each of dimension <italic>Q</italic> &#xD7; <italic>Q</italic>. In this case, the random effects in the regression of <bold><italic>&#x3B8;</italic></bold> on <bold>x</bold> are allowed to correlate but they are independent of those in the regression of <bold><italic>&#x3B6;</italic></bold> on <bold>x</bold>. This choice will be made throughout this paper. Note that when <inline-formula id="IEqd"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqd.gif --><tex-math id="M11">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$({\rm{x}}{\beta _1},{\rm{x}}{\beta _2}) = ({\mu _\theta },{\mu _\xi }) = {\mu _P}$$
\end{document}</tex-math></inline-formula>, the model as proposed by <xref ref-type="bibr" rid="CR55">van der Linden (2008)</xref> is obtained as a special case.</p>
        <p>Let <bold><italic>&#x3B8;</italic></bold><sub><italic>j</italic></sub> and <bold><italic>&#x3B6;</italic></bold><sub><italic>j</italic></sub> denote the vectors of length <italic>n</italic><sub><italic>j</italic></sub> of the person parameters of group <italic>j</italic>. The entire structural multivariate multilevel model can now be presented as:
<disp-formula id="Equ8"><tex-math id="M12">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\rm{vec}}({\theta _j},{\xi _j}) = ({{\rm{I}}_2} \otimes {\rm{x}}_j^t){\rm{vec}}({\beta _j}) + {\rm{vec}}({e_{{\theta _j}}},{e_{{\xi _j}}})$$
\end{document}</tex-math></disp-formula>,
<disp-formula id="Equ9"><tex-math id="M13">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\rm{vec}}({\beta _j}) = ({{\rm{I}}_2} \otimes {{\rm{w}}_j}){\rm{vec}}({\gamma _1},{\gamma _2}) + {\rm{vec}}({{\rm{u}}_{1j}},{{\rm{u}}_{2j}})$$
\end{document}</tex-math></disp-formula>, where vec denotes the operation of vectorizing a matrix. We refer to these two models as level 2 and 3 models, respectively. Marginalizing over the random regression effects in (<xref rid="Equ8" ref-type="">8</xref>) and (<xref rid="Equ9" ref-type="">9</xref>), the distribution of vec(<bold><italic>&#x3B8;</italic></bold><sub><italic>j</italic></sub>, <bold><italic>&#x3B6;</italic></bold><sub><italic>j</italic></sub>) becomes
<disp-formula id="Equ10"><tex-math id="M14">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\rm{vec}}({\theta _j},{\xi _j}) \sim N({{\rm{I}}_2} \otimes {{\rm{x}}_j}{{\rm{w}}_j})\gamma ,({{\rm{I}}_2} \otimes {{\rm{x}}_j}){\rm{V}}{({{\rm{I}}_2} \otimes {{\rm{x}}_j})^t} + \sum {_P \otimes {{\rm{I}}_{nj}})} $$
\end{document}</tex-math></disp-formula>.</p>
        <p>The structural component of the model allows a simultaneous regression analysis of all person parameters on explanatory variables at the individual and group levels while taking into account the dependencies between the individuals within each group. As a result, among other things, conclusions can be drawn as to the size of the effects of the explanatory variables on the test takers&#x2019; ability and speed as well as the correlation between these person parameters. Note that hypotheses on these effects can be tested simultaneously.</p>
      </sec>
      <sec id="Sec5">
        <title>2.3. Multivariate Model for the Item Parameters</title>
        <p>An empirical distribution for the item parameters is specified such that for each item the vector <bold><italic>&#x3B6;</italic></bold><sub><italic>k</italic></sub> = (<italic>a</italic><sub><italic>k</italic></sub>, <italic>b</italic><sub><italic>k</italic></sub>, <italic>&#x3D5;</italic><sub><italic>k</italic></sub>, <italic>&#x3BB;</italic>{<sub>itk</sub>}) is assumed to follow a multivariate normal distribution with mean vector <bold><italic>&#x3BC;</italic></bold><sub><italic>I</italic></sub> = (<italic>&#x3BC;</italic><sub><italic>a</italic></sub>, <italic>&#x3BC;</italic><sub><italic>b</italic></sub>, <italic>&#x3BC;</italic><sub><italic>&#x3D5;</italic></sub>, &#x3BC;{<sub>it&#x3BB;</sub>})}:
<disp-formula id="Equ11"><tex-math id="M15">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\xi _k} = {\mu _I} + {e_I},{e_I} \sim N(0,\sum {_I)} $$
\end{document}</tex-math></disp-formula>, where <bold><italic>&#x3A3;</italic></bold><sub><italic>I</italic></sub> specifies the covariance structure.</p>
        <p>The assumption introduces a correlation structure between the item parameters. For example, it may be expected that easy items require less time to be solved than more difficult items. If so, the time intensity parameter correlates positively with the item difficulty parameter. The guessing parameter of the response model has no analogous parameter in the RT measurement model (since there is no guessing aspect for the RTs). Therefore, it does not serve a purpose to include it in this multivariate model and an independent prior for this parameter is specified below.</p>
      </sec>
    </sec>
    <sec id="Sec6">
      <title>3. Exploring the Multivariate Normal Structure</title>
      <p>The observed response data are augmented using a procedure that facilitates the statistical inferences. Besides, as will be shown in the next section, these augmentation steps allow for a fully Gibbs sampling approach for estimation of the model.</p>
      <p>First, an augmentation step is introduced according to <xref ref-type="bibr" rid="CR5">Beguin &amp; Glas (2001)</xref>. A variable <italic>s</italic><sub><italic>ijk</italic></sub> = 1 when a person <italic>ij</italic> knows the correct answer to question <italic>k</italic> and is <italic>s</italic>{<sub>itijk</sub>} = 0 otherwise. Its conditional probabilities are given by:
<disp-formula id="Equ12"><tex-math id="M16">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$P({s_{ijk}} = 1\left| {{y_{ijk}} = 1,{\theta _{ij}},{a_k},{b_k},{c_k}) = {{\Phi ({a_k}{\theta _{ij}} - {b_k})} \over {\Phi ({a_k}{\theta _{ij}} - {b_k}) + {c_k}(1 - \Phi ({a_k}{\theta _{ij}} - {b_k}))}}} \right.$$
\end{document}</tex-math></disp-formula>,
<disp-formula id="Equ13"><tex-math id="M17">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$P({s_{ijk}} = 0\left| {{y_{ijk}} = 1,{\theta _{ij}},{a_k},{b_k},{c_k}) = {{{c_k}(1 - \Phi ({a_k}{\theta _{ij}} - {b_k}))} \over {\Phi ({a_k}{\theta _{ij}} - {b_k}) + {c_k}(1 - \Phi ({a_k}{\theta _{ij}} - {b_k}))}}} \right.$$
\end{document}</tex-math></disp-formula>,
<disp-formula id="Equ14"><tex-math id="M18">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$P({s_{ijk}} = 1\left| {{y_{ijk}} = 0,{\theta _{ij}},{a_k},{b_k},{c_k}) = 0} \right.$$
\end{document}</tex-math></disp-formula>,
<disp-formula id="Equ15"><tex-math id="M19">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$P({s_{ijk}} = 0\left| {{y_{ijk}} = 0,{\theta _{ij}},{a_k},{b_k},{c_k}) = 1} \right.$$
\end{document}</tex-math></disp-formula>. Second, following <xref ref-type="bibr" rid="CR2">Albert (1992)</xref>, continuous latent responses <italic>z</italic><sub><italic>ij k</italic></sub> are defined:
<disp-formula id="Equ16"><tex-math id="M20">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${z_{ijk}} = {a_k}{\theta _{ij}} - {b_k} + {\varepsilon _{{\theta _{ijk}}}}$$
\end{document}</tex-math></disp-formula>, where the error terms are standard normally distributed and <bold>s</bold> is taken to be a matrix of indicator variables for the events of the components of <bold>z</bold> being positive. When the guessing parameters are restricted to be zero, it follows immediately that <italic>s</italic><sub><italic>ijk</italic></sub> = <italic>y</italic><sub><italic>ijk</italic></sub> with probability one and the 2-parameter IRT model is obtained.</p>
      <p>Statistical inferences can be made from the complete data due to the following factorization:
<disp-formula id="Equ17"><tex-math id="M21">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$p({\rm{y}},{\rm{z}},{\rm{s}},{\rm{t}}\left| {{\rm{a}},{\rm{b}},{\rm{c}},\phi ,\lambda ,\gamma ,\sum {_P,{\rm{V}}) = p(} } \right.{\rm{y}}\left| {{\rm{z}},{\rm{s}})p({\rm{s}}\left| {{\rm{c}})p({\rm{z}},{\rm{t}}\left| {{\rm{a}},{\rm{b}},\phi ,\lambda ,\gamma ,\sum {_P,{\rm{V}})} } \right.} \right.} \right.$$
\end{document}</tex-math></disp-formula>. Our interest is in exploring the structural relationships between ability and speed. Therefore, the term on the far right-hand side of (<xref rid="Equ17" ref-type="">17</xref>) will be explored in more detail now. This likelihood can be taken to be that of a normal multivariate multilevel model,
<disp-formula id="Equ18"><tex-math id="M22">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$p({\rm{z}},{\rm{t}}\left| {{\rm{a}},{\rm{b}},\phi ,\lambda ,\gamma ,\sum {_P,{\rm{V}}) = \int\!\!\!\int\!\!\!\int {p{\rm{(z}}\left| {\theta ,{\rm{a}},{\rm{b}})p(t\left| {\xi {\rm{,}}\phi {\rm{,}}\lambda )p(\xi ,\theta \left| {\beta ,\sum {_P)p(\beta \left| {\gamma ,{\rm{V}})d\theta d\xi d\beta } \right.} } \right.} \right.} \right.} } } \right.$$
\end{document}</tex-math></disp-formula>. Therefore, all factors in this decomposition are multivariate normal densities. The first two factors occur because of the independence of the responses and response times given the latent person parameters. The last two factors represent levels 2 and 3 of the model.</p>
      <p>Inference from this multivariate hierarchical model simplifies when taking advantage of some of the properties of the multivariate normal distribution. For example, let us assume for a moment that the item parameters are fixed and known and define <inline-formula id="IEqe"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqe.gif --><tex-math id="M23">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$({{{\rm{\tilde z}}}_{ij}},{{{\rm{\tilde t}}}_{ij}}) = ({{\rm{z}}_{ij}} + {\rm{b}},{{\rm{t}}_{ij}} - \lambda )$$
\end{document}</tex-math></inline-formula>. Levels 1 and 2 of the model can then be represented by the following multivariate hierarchical structure: <inline-graphic xlink:href="11336_2008_Article_9075_Fig4.gif" id="N0x1e24000N0x2dc6530"/>. (19) This representation provides insight in the complex correlational structure hidden in the data and entails several possible inferences. It also helps us to derive some of the conditional posterior distributions for the Gibbs sampling algorithm (e.g., the conditional posterior distributions of the latent person parameters given the augmented data). For a general treatment of the derivation of conditional from multivariate normal distributions, see, for instance, <xref ref-type="bibr" rid="CR42">Searle, Casella, and McCulloch (1992)</xref>.</p>
      <p>Parameter <italic>&#x3C1;</italic>, which controls the covariance between the <italic>&#x3B8;</italic><sub>s</sub> and <italic>&#x3B6;</italic><sub>s</sub>, plays an important role in the model. It can be considered to be the bridge between the separate measurement models for ability and speed. Therefore, its role within the hierarchical structure will be explored in more detail.</p>
      <p>The conditional covariance between the latent response variables and RTs on items <italic>k</italic> = 1, &#x2026;, <italic>K</italic> is equal to <inline-formula id="IEqf"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqf.gif --><tex-math id="M24">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\mathop{\rm cov}} ({a_k}{\theta _{ij}} - {b_k} + {\varepsilon _{{\theta _{ijk}}}}, - {\phi _k}{\zeta _{ij}} + {\lambda _k} + {\varepsilon _{{\xi _{ijk}}}}) = - {a_k}\rho {\phi _k}$$
\end{document}</tex-math></inline-formula>, due to independence between the residuals as well as the residuals and the person parameters. Since <italic>a</italic><sub><italic>k</italic></sub> and <italic>&#x3D5;</italic><sub><italic>k</italic></sub> are positive, the latent response variables and RTs, and hence the responses and RTs, correlate negatively when <italic>&#x3C1;</italic> is positive. So, in spite of conditional independence between the responses and RTs given the person parameters, their correlation is negative.</p>
      <p>The conditional distribution of <italic>&#x3B8;</italic><sub><italic>ij</italic></sub> given <italic>&#x3B6;</italic><sub><italic>ij</italic></sub> is normal:
<disp-formula id="Equ20"><tex-math id="M25">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\theta _{ij}}{\left| {{\zeta _{ij}},{\beta _j},\sigma _\theta ^2,\sigma _\zeta ^2,\rho \sim N({\rm{x}}_{ij}^t\beta } \right._{1j}} + \rho \sigma _\zeta ^{ - 2}({\zeta _{ij}} - {\rm{x}}_{ij}^t{\beta _{2j}}),\sigma _\theta ^2 - {\rho ^2}\sigma _\zeta ^{ - 2})$$
\end{document}</tex-math></disp-formula>. A greater covariance <italic>&#x3C1;</italic> between the person parameters gives a greater reduction of the conditional variance of <italic>&#x3B8;</italic><sub><italic>ij</italic></sub> given <italic>&#x3B6;</italic><sub><italic>ij</italic></sub>. The expression also shows that the amount of information about <italic>&#x3B8;</italic><sub><italic>ij</italic></sub> in <italic>&#x3B6;</italic><sub><italic>ij</italic></sub> depends both on the precision of measuring the speed parameter and its correlation with the ability parameter.</p>
      <p>From (19), it also follows that the conditional expected value of <italic>&#x3B8;</italic><sub><italic>ij</italic></sub> given the complete data is equal to
<disp-formula id="Equ21"><tex-math id="M26">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$E({\theta _{ij}}|{\beta _j},{\zeta _{ij}},{{\rm{\tilde z}}_{ij}},\sum {_P,{\rm{a}},{\rm{b}})} = {\rm{x}}_{ij}^t{\beta _{1j}} + \rho \sigma _\zeta ^{ - 2}({\zeta _{ij}} - {\rm{x}}_{ij}^t{\beta _{2j}}) + \sigma _\theta ^2{{\rm{a}}^t}{({\rm{a}}\sigma _\theta ^2{{\rm{a}}^t} + {{\rm{I}}_K})^{ - 1}}({{\rm{\tilde z}}_{ij}} - {\rm{ax}}_{ij}^t{\beta _{1j}}) = {({{\rm{a}}^t}{\rm{a}} + \sigma _\theta ^{ - 2})^{ - 1}}({{\rm{a}}^t}{{\rm{\tilde z}}_{ij}} + \sigma _\theta ^{ - 2}({\rm{x}}_{ij}^t{\beta _{1j}} + \rho \sigma _\zeta ^{ - 2}({\zeta _{ij}} - {\rm{x}}_{ij}^t{\beta _{2j}})))$$
\end{document}</tex-math></disp-formula>. The conditional expected value of <italic>&#x3B8;</italic><sub><italic>ij</italic></sub> consists of two parts: one part representing the information about <italic>&#x3B8;</italic><sub><italic>ij</italic></sub> in the (augmented) response data and another the information through the multivariate regression on <bold>x</bold><sub><italic>ij</italic></sub>. For <italic>&#x3C1;</italic> = 0, (<xref rid="Equ21" ref-type="">21</xref>) reduces to
<disp-formula id="Equ22"><tex-math id="M27">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$E({\theta _{ij}}|{\beta _{1j}},{{\rm{\tilde z}}_{ij}},\sigma _\theta ^2{\rm{,a}},{\rm{b}}) = {({{\rm{a}}^t}{\rm{a}} + \sigma _\theta ^{ - 2})^{ - 1}}({{\rm{a}}^t}{{\rm{\tilde z}}_{ij}} + \sigma _\theta ^{ - 2}{\rm{x}}_{ij}^t{\beta _{1j}})$$
\end{document}</tex-math></disp-formula>. This expression can be recognized as the precision-weighted mean of the predictions of <italic>&#x3B8;</italic><sub><italic>ij</italic></sub> from the (augmented) response data and from the linear regression of <italic>&#x3B8;</italic> on <bold>x</bold> (see, for instance, <xref ref-type="bibr" rid="CR15">Fox &amp; Glas, 2001</xref>). Comparing (<xref rid="Equ22" ref-type="">22</xref>) with (<xref rid="Equ21" ref-type="">21</xref>), it can be seen that when <italic>&#x3C1;</italic> &gt; 0, the expected value of <italic>&#x3B8;</italic><sub><italic>ij</italic></sub> increases for test takers who work at a greater than average speed; that is, a test taker&#x2019;s ability is predicted to be higher when the same response pattern is obtained at a higher speed (i.e., in a shorter expected time on the same set of items).</p>
      <p>In (19), in addition to the responses and RTs, the random test takers were the only extra source of heterogeneity. But another level of heterogeneity was added in (<xref rid="Equ9" ref-type="">9</xref>), where the test takers were assumed to be nested within groups and the regression effects were allowed to vary randomly across them. Also, the item parameters correlate in (<xref rid="Equ11" ref-type="">11</xref>). Because of these random effects and correlations, the marginal covariances between the measurements change.</p>
      <p>We conclude this discussion with the following comments:
<list list-type="bullet"><list-item><p>In (19), a special structure (compound symmetry) for the covariance matrix of the residuals at the level of individuals was shown to exist. This structure may lead to more efficient inference. For a general discussion of possible parameterizations and estimation methods for multivariate random effects structures, see, for instance, <xref ref-type="bibr" rid="CR21">Harville (1977)</xref>, <xref ref-type="bibr" rid="CR35">Rabe-Hesketh and Skrondal (2001)</xref>, and <xref ref-type="bibr" rid="CR36">Reinsel (1983)</xref>.</p></list-item><list-item><p>Linear multivariate three-level structures for continuous responses are discussed, among others, in <xref ref-type="bibr" rid="CR20">Goldstein (2003)</xref>, and <xref ref-type="bibr" rid="CR48">Snijders and Bosker (1999)</xref>. As already indicated, the covariance structure of the level-3 random regression effects is assumed to be block diagonal. This means that the parameters in the regression of <bold><italic>&#x3B8;</italic></bold> on <bold>x</bold> are conditionally independent of those in the regression of <bold><italic>&#x3B6;</italic></bold> on <bold>x</bold>. It is possible to allow these parameters to correlate but this option is unattractive when the dimension of the covariance matrix becomes large. Typically, the covariance matrix is then poorly estimated (<xref ref-type="bibr" rid="CR25">Laird &amp; Ware, 1982</xref>).</p></list-item><list-item><p>For the same reason, the covariance matrix of the fixed effects in (<xref rid="Equ9" ref-type="">9</xref>) is assumed to be block diagonal. The Bayesian approach in the next sections allows us to specify different levels of prior information about this matrix.</p></list-item></list></p>
    </sec>
    <sec id="Sec7">
      <title>4. Bayesian Estimation Using Gibbs Sampling</title>
      <p>In Bayesian statistics, inferences are made from the posterior distribution of the model parameters. Markov chain Monte Carlo (MCMC) methods enable us to simulate random draws from this distribution. Summary statistics can then be used to estimate the parameters or functionals of interest. A useful feature of MCMC methods is that they remain straightforward and easy to implement when the complexity of the model increases. Also, they allow for the simultaneous estimation of all model parameters. Since the current model is quite complex and has many parameters, we need these advantages to estimate the model. For a general introduction to Gibbs sampling, see <xref ref-type="bibr" rid="CR17">Gelman, Carlin, Stern, and Rubin (2004)</xref> and <xref ref-type="bibr" rid="CR16">Gelfand &amp; Smith (1990)</xref>. MCMC methods for IRT models are discussed by <xref ref-type="bibr" rid="CR2">Albert (1992)</xref> and <xref ref-type="bibr" rid="CR34">Patz &amp; Junker (1999)</xref>.</p>
      <p>A new Gibbs sampling scheme was developed to deal with the extension of the model. Further, the scheme differs from that in <xref ref-type="bibr" rid="CR56">van der Linden (2008)</xref> by its increased efficiency; it samples both types of person parameters in one step, taking into account the identifying restrictions, and avoids an MH step in the sampling of the item parameters due to better capitalization on the regression structure of the model. The full conditional distributions of all model parameters for the scheme are given in the <xref rid="AppA" ref-type="app">Appendix</xref>.</p>
      <p>The remainder of this section discusses the priors and identifying restrictions we use.</p>
      <sec id="Sec8">
        <title>4.1. Prior Distributions</title>
        <p>The parameter <italic>c</italic><sub><italic>k</italic></sub> is the success probability in the Binomial distribution for the number of correct guesses on item <italic>k</italic>. A Beta prior with parameters <italic>B</italic>(<italic>b</italic>&#x2032;<sub>1</sub>, <italic>b</italic>&#x2032;<sub>2</sub>) is chosen, which is the conjugate for the Binomial likelihood, and thus leads to a Beta posterior.</p>
        <p>For the residual variance <italic>&#x3C4;</italic><sub arrange="stack"><italic>k</italic></sub><sup arrange="stack">2</sup>, a conjugate inverse Gamma prior is assumed with parameters <italic>g</italic><sub>1</sub> and <italic>g</italic><sub>2</sub>.</p>
        <p>A normal inverse-Wishart prior is chosen for the mean vector <bold><italic>&#x3BC;</italic></bold><sub><italic>I</italic></sub> and covariance matrix <bold><italic>&#x3A3;</italic></bold><sub><italic>I</italic></sub> of the item parameters. The family of priors is conjugate for the multivariate normal distribution (<xref ref-type="bibr" rid="CR17">Gelman et al., 2004</xref>). Thus,
<disp-formula id="Equ23"><tex-math id="M28">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\sum {_I \sim {\rm&lt;Subscript&gt;verse&lt;/Subscript&gt;} - {\rm{Wishart}}(\sum {_{{I_0}}^{ - 1},{v_{{I_0}}})} } $$
\end{document}</tex-math></disp-formula>,
<disp-formula id="Equ24"><tex-math id="M29">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\mu _I}|\sum {_I \sim N({\mu _{I0}},\sum {_I/{\kappa _{I0}})} } $$
\end{document}</tex-math></disp-formula>. A vague proper prior follows if <italic>&#x3BD;</italic><sub><italic>I</italic>0</sub> is set equal to the minimum value for the degrees-of-freedom parameter and a diagonal variance matrix with large values is chosen.</p>
        <p>Likewise, a normal inverse-Wishart prior is chosen for the fixed parameters <bold><italic>&#x3B3;</italic></bold> of the multivariate random-effects structure of the person parameters in (<xref rid="Equ9" ref-type="">9</xref>),
<disp-formula id="Equ25"><tex-math id="M30">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\sum {_\gamma \sim {\rm&lt;Subscript&gt;verse&lt;/Subscript&gt;} - {\rm{Wishart}}(\sum {_{{\gamma _0}}^{ - 1},{v_{{\gamma _0}}})} } $$
\end{document}</tex-math></disp-formula>,
<disp-formula id="Equ26"><tex-math id="M31">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\gamma |\sum {_\gamma \sim N({\gamma _0},\sum {_\gamma /{\kappa _{\gamma 0}})} } $$
\end{document}</tex-math></disp-formula>.</p>
        <p>The covariance matrix <bold>V</bold> of the level-3 random group effects (<bold>u</bold><sub>1<italic>j</italic></sub>,<bold>u</bold><sub>2<italic>j</italic></sub>) is assumed to also have an inverse-Wishart prior with scale matrix <bold>V</bold><sub>0</sub> and degrees of freedom <italic>&#x3BD;</italic><sub>V 0</sub>.</p>
        <p>The prior for the covariance matrix of the person parameters, <bold><italic>&#x3A3;</italic></bold><sub><italic>P</italic></sub>, is chosen to give special treatment because the model is not yet identified.</p>
      </sec>
      <sec id="Sec9">
        <title>4.2. Prior for <bold><italic>&#x3A3;</italic></bold><sub><italic>P</italic></sub> with Identifying Restrictions</title>
        <p>The model can be identified by fixing the scales of the two latent person parameters. A straightforward way of fixing the scale of the ability parameter is to set the mean equal to zero and the variance to one. To avoid a tradeoff between <bold><italic>&#x3D5;</italic></bold> and <bold><italic>&#x3B6;</italic></bold> the time discrimination parameters are restricted to <inline-formula id="IEqg"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqg.gif --><tex-math id="M32">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\Pi _{k = 1}^K{\phi _k} = 1$$
\end{document}</tex-math></inline-formula>. When these are restricted to <bold><italic>&#x3D5;</italic></bold> = <bold>1</bold> the lognormal RT model as proposed by <xref ref-type="bibr" rid="CR53">van der Linden (2006)</xref> is obtained. Then for the speed parameter, since RTs have a natural unit, we only have to fix the origin of its scale and set it equal to its population mean. Note that a multivariate probit model is identified by fixing the diagonal elements of the covariance matrix (<xref ref-type="bibr" rid="CR11">Chib &amp; Greenberg, 1998</xref>) but that because of the special nature of the RTs, in the current case only one element of <bold><italic>&#x3A3;</italic></bold><sub><italic>P</italic></sub> has to be fixed.</p>
        <p>Generally, two issues arise when restricting a covariance structure. First, defining proper priors for a restricted covariance matrix is rather difficult. For example, for the conjugate inverse-Wishart prior, there is no choice of parameter values that reflects a restriction on the variance of the ability parameter such as that above. For the multinomial probit model, <xref ref-type="bibr" rid="CR30">McCulloch and Rossi (1994)</xref> tackled this problem by specifying proper diffuse priors for the unidentified parameters and reporting the marginal posterior distributions of the identified parameters. However, it is hard to specify prior beliefs about unidentified parameters. Second, for a Gibbs sampler, sampling from a restricted covariance matrix requires extra attention. <xref ref-type="bibr" rid="CR11">Chib and Greenberg (1998)</xref> defined individual priors on the free covariance parameters, but as a result, the augmented data had to be sampled from a special truncated region and the values of the free covariance parameter could only be sampled using an MH step. However, such steps involve the specification of an effective proposal density with tuning parameters that can only be fixed through a cumbersome process. A general approach for sampling from a restricted covariance matrix can be found in <xref ref-type="bibr" rid="CR10">Browne (2006)</xref> but this is also based on an MH algorithm.</p>
        <p>Here, a different approach is taken that allows us to specify proper informative priors and facilitate the implementation of the Gibbs sampler. A prior is chosen such that <italic>&#x3C3;</italic><sub arrange="stack"><italic>&#x3B8;</italic></sub><sup arrange="stack">2</sup> = 1 with probability one. Hence, covariance matrix <bold><italic>&#x3A3;</italic></bold><sub><italic>P</italic></sub> always equals:
<disp-formula id="Equ27"><tex-math id="M33">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\sum {_P = \left[ {\matrix{ 1 &amp; \rho \cr \rho &amp; {\sigma _\zeta ^2} \cr  } } \right]} $$
\end{document}</tex-math></disp-formula>. Using (<xref rid="Equ8" ref-type="">8</xref>) and (<xref rid="Equ27" ref-type="">27</xref>), the conditional distribution of <italic>&#x3B6;</italic><sub><italic>ij</italic></sub> given <italic>&#x3B8;</italic><sub><italic>ij</italic></sub> has density
<disp-formula id="Equa"><tex-math id="M34">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\zeta _{ij}}|{\theta _{ij}},{\beta _j},\rho ,\sigma _\zeta ^2 \sim N({\rm{x}}_{ij}^t{\beta _{2j}} + \rho ({\theta _{ij}} - {\rm{x}}_{ij}^t{\beta _{1j}}),\tilde \sigma _\zeta ^2)$$
\end{document}</tex-math></disp-formula> where <inline-formula id="IEqh"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqh.gif --><tex-math id="M35">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\tilde \sigma _\zeta ^2 = \sigma _\zeta ^2 - {\rho ^2}$$
\end{document}</tex-math></inline-formula>. Parameter <italic>&#x3C1;</italic> can be viewed as the slope parameter in a normal regression problem of <italic>&#x3B6;</italic><sub><italic>ij</italic></sub> on <italic>&#x3B8;</italic><sub><italic>ij</italic></sub> with variance <inline-formula id="IEqi"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqi.gif --><tex-math id="M36">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\tilde \sigma _\zeta ^2 = \sigma _\zeta ^2 - {\rho ^2}$$
\end{document}</tex-math></inline-formula>. Specifying a normal and inverse gamma as conjugate priors for these parameters,
<disp-formula id="Equ28"><tex-math id="M37">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\rho \sim N({\rho _0},\sigma _\rho ^2)$$
\end{document}</tex-math></disp-formula>,
<disp-formula id="Equ29"><tex-math id="M38">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\tilde \sigma _\zeta ^{ - 2} \sim {\rm{Gamma}}({g_1},{g_2})$$
\end{document}</tex-math></disp-formula>, their full conditional posterior distributions become
<disp-formula id="Equ30"><tex-math id="M39">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\rho |\theta ,\zeta ,\beta ,{\rho _0},\sigma _\rho ^2 \sim N(\Delta ({\rho _0}\sigma _\rho ^{ - 2} + {(\theta - {\rm{x}}{\beta _1})^t}(\zeta - {\rm{x}}{\beta _2})),\Delta )$$
\end{document}</tex-math></disp-formula>,
<disp-formula id="Equ31"><tex-math id="M40">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\tilde \sigma _\zeta ^{ - 2}|\theta ,\zeta ,\beta ,\rho \sim {\rm{Gamma}}({g_1} + N/2,{g_2} + {\Xi ^t}\Xi /2)$$
\end{document}</tex-math></disp-formula>, where
<disp-formula id="Equb"><tex-math id="M41">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\Delta = {({(\theta - {\rm{x}}{\beta _1})^t}(\theta - {\rm{x}}{\beta _1}) + \sigma _\rho ^{ - 2})^{ - 1}}$$
\end{document}</tex-math></disp-formula> and
<disp-formula id="Equc"><tex-math id="M42">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\Xi = (\zeta - {\rm{x}}{\beta _2}) - \rho (\theta - {\rm{x}}{\beta _1})$$
\end{document}</tex-math></disp-formula>.</p>
        <p>Since <inline-formula id="IEqj"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqj.gif --><tex-math id="M43">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$|{\Sigma _P}| = \sigma _\zeta ^2 - {\rho ^2} = \tilde \sigma _\zeta ^2$$
\end{document}</tex-math></inline-formula> and <inline-formula id="IEqk"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqk.gif --><tex-math id="M44">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\tilde \sigma _\zeta ^2 &gt; 0$$
\end{document}</tex-math></inline-formula>, it follows that the determinant |<bold><italic>&#x3A3;</italic></bold><sub><italic>P</italic></sub> | &gt; 0. The latter is sufficient to guarantee matrix <bold><italic>&#x3A3;</italic></bold><sub><italic>P</italic></sub> to be positive semi-definite.</p>
        <p>When implementing a Gibbs sampler, the random draws of the elements of covariance matrix <bold><italic>&#x3A3;</italic></bold><sub><italic>P</italic></sub> in (<xref rid="Equ27" ref-type="">27</xref>) can be constructed from the samples drawn from (<xref rid="Equ30" ref-type="">30</xref>)&#x2013;(<xref rid="Equ31" ref-type="">31</xref>). These draws will show greater autocorrelation due to this new parametrization. This implies that more MCMC iterations are needed to cover the support of the posterior distribution adequately, a measure that only involves a (linear) increase in the running time of the sampler. On the other hand, convergence of the algorithm is easily established without having to specify any tuning parameter. Finally, this procedure also enables straightforward implementation of the data augmentation procedure since the <bold>z</bold>s can be drawn from a normal distribution truncated at zero, where <bold>s</bold> indicates when <bold>z</bold> is positive.</p>
        <p>The key element of the present approach is the specification of a proper prior distribution for the covariance matrix with one fixed diagonal element and the construction of random draws from the matrix from the corresponding conditional posterior distribution. For the multinomial probit model, the approach was also followed by <xref ref-type="bibr" rid="CR31">McCulloch, Polson, and Rossi (2000)</xref>. For completeness, we also mention an alternative approach. <xref ref-type="bibr" rid="CR3">Barnard, McCullogh, and Meng (2000)</xref> formulated a prior directly for the identified parameters. In order to do so, they factored the covariance matrix into a diagonal matrix with standard deviations and a correlation matrix, and specified an informative prior for the latter. This prior was then incorporated into a Griddy-Gibbs sampler. However, such algorithms can be slow and require the choices of a grid size and boundaries. <xref ref-type="bibr" rid="CR7">Boscardin and Zhang (2004)</xref> followed a comparable approach but used a parameter-extended MH algorithm for sampling values from the conditional distribution of the correlation matrix.</p>
      </sec>
    </sec>
    <sec id="Sec10">
      <title>5. Model Selection Methods</title>
      <p>A model comparison method is often based on a measure of fit and some penalty function based on the number of free parameters for the complexity of the model. A bias-variance tradeoff exists between these two quantities since a more complex model often leads to less bias but a less complex model involves more accurate estimation. Two well-known criteria of model selection based on a deviance fit measure are the Bayesian information criterion (BIC) (<xref ref-type="bibr" rid="CR41">Schwarz, 1978</xref>) and Akaike&#x2019;s information criterion (AIC) (<xref ref-type="bibr" rid="CR1">Akaike, 1973</xref>). These criteria depend on the effective number of parameters in the model as a measure of model complexity. A drawback of these measures is that they are often difficult to calculate for hierarchical models: Although the nominal number of parameters follows directly from the likelihood, the prior distribution imposes additional restrictions on the parameter space and reduces its effective dimension. In a random-effects model, the effective number of parameters depends strongly on the higher-level variance parameters. When the variance of the random effects approaches zero, all random effects are equal and the model reduces to a simple linear model with one mean parameter. But when the variance goes to infinity, the number of free parameters approaches the number of random effects.</p>
      <p><xref ref-type="bibr" rid="CR49">Spiegelhalter et al. (2002)</xref> proposed the deviance information criterion (DIC) for model comparison when the number of parameters is not clearly defined. The DIC is defined as the sum of a deviance measure and a penalty term for the effective number of parameters based on a measure of model complexity described below.</p>
      <p>An alternative method for model selection that can handle complex hierarchical models is the Bayes factor; for a review, see <xref ref-type="bibr" rid="CR23">Kass and Raftery (1995)</xref>. The Bayes factor is based on a comparison of marginal likelihoods but its implementation is hampered by its critical dependence on the prior densities assigned to the model parameters. It is known that the Bayes factor tends to favor models with reasonably vague proper priors; see, for instance, <xref ref-type="bibr" rid="CR6">Berger and Delampady (1987)</xref> and <xref ref-type="bibr" rid="CR46">Sinharay and Stern (2002)</xref>. An advantage of the Bayes factor is its clear interpretation as the change in the odds in favor of the model when moving from the prior to the posterior distribution (<xref ref-type="bibr" rid="CR26">Lavine &amp; Schervish, 1999</xref>).</p>
      <p>In one of the empirical examples below, the focus is on the structural multivariate model for the person parameters. It will be shown that a DIC can be formulated for choosing between models that differ in the fixed and/or random part of the structural model. In addition, a Bayes factor for selecting between the IRT measurement model for binary responses and the model extended with the hierarchical structure for responses and RTs is presented.</p>
      <sec id="Sec11">
        <title>5.1. Model Selection Using the DIC</title>
        <p>The DIC requires a closed-form likelihood. Our interest is focused on the likelihood of the structural parameters in the model; accordingly, all random effect parameters can be intregated out. Besides, the variances, covariances, and items parameters are considered as nuisance parameters, and their values are assumed to be known. So, a DIC will be derived for the complete-data likelihood with the random effects integrated out. Subsequently, the posterior expectation of the DIC over the augmented data will be taken. The same procedure was proposed for mixture models by <xref ref-type="bibr" rid="CR12">DeIorio and Robert (2002)</xref>.</p>
        <p>Let <bold>z</bold><sub arrange="stack"><italic>ij</italic></sub><sup arrange="stack">*</sup> = vec(<bold>z</bold><sub><italic>ij</italic></sub> + <bold>b</bold>, <bold>t</bold><sub><italic>ij</italic></sub> &#x2212; <bold><italic>&#x3BB;</italic></bold>) and <bold>H</bold><sub><italic>P</italic></sub> = (<bold>a</bold>&#x2295;&#x2212;<bold><italic>&#x3D5;</italic></bold>). From (19), Conditional on <bold>s</bold>, the measurement models for ability and speed can be summarized as
<disp-formula id="Equ32"><tex-math id="M45">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\rm{z}}_{ij}^* = {{\rm{H}}_p}{\Omega _{ij}} + {{\rm{e}}_{ij}}$$
\end{document}</tex-math></disp-formula>, where <bold>e</bold><sub><italic>ij</italic></sub> &#x223C; <italic>N</italic>(0,<bold><italic>C</italic></bold>), with <inline-formula id="IEql"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEql.gif --><tex-math id="M46">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$C = ({{\rm{I}}_K} \oplus {{\rm{I}}_K}{\tau ^2})$$
\end{document}</tex-math></inline-formula> a diagonal matrix with in the left upper square <bold>1</bold> and in the right lower square <bold><italic>&#x3C4;</italic></bold> on its diagonal, and <bold><italic>&#x3A9;</italic></bold><sub><italic>ij</italic></sub> = vec(<bold><italic>&#x3B8;</italic></bold><sub><italic>ij</italic></sub>, <bold><italic>&#x3B6;</italic></bold><sub><italic>ij</italic></sub>). The focus is on the structure of <bold><italic>&#x3A9;</italic></bold>. Using the factorization in (<xref rid="Equ17" ref-type="">17</xref>), the standardized deviance is
<disp-formula id="Equ33"><tex-math id="M47">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$D(\Omega ) = \sum\limits_{ij} {{{({\rm{z}}_{ij}^*{\rm{ - }}{{\rm{H}}_p}{\Omega _{ij}})}^t}{C^{ - 1}}({\rm{z}}_{ij}^* - {{\rm{H}}_p}{\Omega _{ij}})} $$
\end{document}</tex-math></disp-formula>. The DIC is defined as
<disp-formula id="Equ34"><tex-math id="M48">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\rm{DIC}} = \smallint \left[ {{\rm{DIC}}|{\rm{z}}} \right]p({\rm{z}}|{\rm{y}})d{\rm{z}}$$
\end{document}</tex-math></disp-formula><disp-formula id="Equ35"><tex-math id="M49">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\rm{ = }}\smallint {\rm{[}}D{\rm{(}}\bar \Omega {\rm{) + }}{{\rm{2}}_{{p_D}}}{\rm{]}}p{\rm{(z|y)}}d{\rm{z}}$$
\end{document}</tex-math></disp-formula><disp-formula id="Equ36"><tex-math id="M50">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\rm{ = }}{E_{\rm{z}}}{\rm{[}}D{\rm{(}}\bar \Omega ) + {2_{{p_D}}}|{\rm{y}}]$$
\end{document}</tex-math></disp-formula>, where <inline-formula id="IEqm"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqm.gif --><tex-math id="M51">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\bar \Omega }$$
\end{document}</tex-math></inline-formula> equals the posterior mean and <italic>p</italic><sub><italic>D</italic></sub> is the effective number of parameters given the augmented data. The latter can be shown to be equal to the mean deviance minus the deviance of the mean. Hence,
<disp-formula id="Equ37"><tex-math id="M52">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\eqalign{ &amp; {p_D} = \overline {D(\Omega )} - D(\bar \Omega ) \cr &amp; = {E_\Omega }[D(\Omega )|{\rm{z}}*] - D(E(\Omega |{\rm{z}}*)) \cr &amp; = {E_\Omega }{[\sum\limits_{ij} {({\rm{z}}_{ij}^*} - {H_p}{\Omega _{ij}})^t}{C^{ - 1}}({\rm{z}}_{ij}^* - {H_p}{\Omega _{ij}})] - D(E(\Omega |{\rm{z}}*)) \cr &amp; = {\rm{tr}}[\sum\limits_{ij} {{E_\Omega }(} {\rm{z}}_{ij}^* - {{\rm{H}}_p}{\Omega _{ij}}){({\rm{z}}_{ij}^* - {{\rm{H}}_p}{\Omega _{ij}})^t}{C^{ - 1}}] - {\rm{tr}}[\sum\limits_{ij} ( {\rm{z}}_{ij}^* - {{\rm{H}}_p}E(\Omega |{\rm{z}}*)){({\rm{z}}_{ij}^* - {{\rm{H}}_p}E(\Omega |{\rm{z}}*))^t}{C^{ - 1}}] \cr &amp; = \sum\limits_{ij} {{\rm{tr}}[{E_\Omega }(} {\rm{z}}_{ij}^* - {{\rm{H}}_p}{\Omega _{ij}}){({\rm{z}}_{ij}^* - {{\rm{H}}_p}{\Omega _{ij}})^t}{C^{ - 1}} - ({\rm{z}}_{ij}^* - {{\rm{H}}_p}E(\Omega |{\rm{z}}*)){({\rm{z}}_{ij}^* - {{\rm{H}}_p}E(\Omega |{\rm{z}}*))^t}{C^{ - 1}} \cr} $$
\end{document}</tex-math></disp-formula><disp-formula id="Equ38"><tex-math id="M53">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$ = \sum\limits_{ij} {{\rm{tr}}[{C^{ - 1}}{\mathop{\rm var}} ({{\rm{e}}_{ij}}|{\rm{z}}_{ij}^*)]} $$
\end{document}</tex-math></disp-formula><disp-formula id="Equ39"><tex-math id="M54">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$ = \sum\limits_{ij} {{\rm{tr}}[{C^{ - 1}}{\mathop{\rm var}} ({{\rm{e}}_{ij}}) - {\mathop{\rm cov}} ({{\rm{e}}_{ij}},{\rm{z}}_{ij}^*){\mathop{\rm var}} {{({\rm{z}}_{ij}^*)}^{ - 1}}{\mathop{\rm cov}} ({{\rm{e}}_{ij}},{\rm{z}}_{ij}^*)]} $$
\end{document}</tex-math></disp-formula><disp-formula id="Equ40"><tex-math id="M55">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\eqalign{ &amp; = \sum\limits_{ij} {{\rm{tr}}[{C^{ - 1}}{\mathop{\rm var}} {{({\rm{z}}_{ij}^*)}^{ - 1}}{\mathop{\rm var}} ({{\rm{H}}_p}{\Omega _{ij}})]} \cr &amp; = \sum\limits_{ij} {{\rm{tr}}[{C^{ - 1}}{{({{\rm{H}}_P}{{\rm{x}}_{ij}}{{\rm{w}}_j}{\Sigma _\gamma }{\rm{w}}_j^t{\rm{x}}_{ij}^t{\rm{H}}_P^t + {{\rm{H}}_P}{{\rm{x}}_{ij}}{\rm{Vx}}_{ij}^t{\rm{H}}_P^t + {{\rm{H}}_P}{\Sigma _P}{\rm{H}}_P^t + C)}^{ - 1}} \times ({{\rm{H}}_P}{{\rm{x}}_{ij}}{{\rm{w}}_j}{\Sigma _\gamma }{\rm{w}}_j^t{\rm{x}}_{ij}^t{\rm{H}}_P^t + {{\rm{H}}_P}{{\rm{x}}_{ij}}{\rm{Vx}}_{ij}^t{\rm{H}}_P^t + {{\rm{H}}_P}{\Sigma _P}{\rm{H}}_P^t)]} \cr} $$
\end{document}</tex-math></disp-formula><disp-formula id="Equ41"><tex-math id="M56">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$ = \sum\limits_{ij} {{\rm{tr}}[{C^{ - 1}}{{({{\rm{H}}_P}{{\rm{x}}_{ij}}{{\rm{w}}_j}{\Sigma _\gamma }{\rm{w}}_j^t{\rm{x}}_{ij}^t{\rm{H}}_P^t + {{\rm{H}}_P}{{\rm{x}}_{ij}}{\rm{Vx}}_{ij}^t{\rm{H}}_P^t + {{\rm{H}}_P}{\Sigma _P}{\rm{H}}_P^t + C)}^{ - 1}} \times ({{\rm{H}}_P}{{\rm{x}}_{ij}}{{\rm{w}}_j}{\Sigma _\gamma }{\rm{w}}_j^t{\rm{x}}_{ij}^t{\rm{H}}_P^t + {{\rm{H}}_P}{{\rm{x}}_{ij}}{\rm{Vx}}_{ij}^t{\rm{H}}_P^t + {{\rm{H}}_P}{\Sigma _P}{\rm{H}}_P^t)]} $$
\end{document}</tex-math></disp-formula>, where tr(&#xB7;) denotes the trace function, i.e., the sum of the diagonal elements. The expectation is taken with respect to the posterior distribution of <bold><italic>&#x3A9;</italic></bold>. The terms in (<xref rid="Equ38" ref-type="">38</xref>) can be recognized as the posterior variances of the residuals whereas those in (<xref rid="Equ40" ref-type="">40</xref>) follow from the fact that because of independence, the variance of <bold>z</bold><sub arrange="stack"><italic>ij</italic></sub><sup arrange="stack">*</sup> equals the sum of the variance of <bold>H</bold><sub><italic>P</italic></sub><bold><italic>&#x3A9;</italic></bold><sub><italic>ij</italic></sub> and <bold>e</bold><sub><italic>ij</italic></sub>.</p>
        <p>DICs of nested models are computed by restricting one or more variance parameters in (<xref rid="Equ41" ref-type="">41</xref>) to zero. Also, (<xref rid="Equ41" ref-type="">41</xref>) can be estimated as a by-product of the MCMC algorithm; that is, the output of the algorithm can be used to estimate the posterior means of the model parameters in the second term of (<xref rid="Equ37" ref-type="">37</xref>) and to integrate the DIC over the item parameters to obtain the first term. (In the current application, the item parameters are the nuisance parameters.)</p>
        <p>Usually the variance parameters are unknown. Then the DIC has to be integrated over their marginal distribution, too. In fact, the correct Bayesian approach would be to integrate the joint posterior over the nuisance parameters to obtain the marginal posterior of interest. However, this approach is not possible since no closed-form expression of the DIC can be obtained for this marginal posterior. Thus, our proposal does not account for the unknown variances. Equation (<xref rid="Equ41" ref-type="">41</xref>) reflects the effective number of parameters of the proposed model without the additional variability in the posterior because of the unknown covariance parameters. The more general case with unknown covariance parameters is complex, and no simple correction seems available. But <xref ref-type="bibr" rid="CR52">Vaida and Blanchard (2005)</xref> showed that for a mixed-effects model, the correction for unknown covariance parameters is negligible asymptotically. So, it seems safe to assume that their effect on the estimate of (<xref rid="Equ37" ref-type="">37</xref>) becomes only apparent when the covariance parameters are estimated less precisely.</p>
      </sec>
      <sec id="Sec12">
        <title>5.2. Model Selection Using the Bayes Factor</title>
        <p>The question we address is if the use of the RTs in the hierarchical model proves to be beneficial for making inferences about the ability parameter. As no benefits can be obtained when the correlation <italic>r</italic>(<italic>&#x3B8;</italic>, <italic>&#x3B6;</italic>) = &#x3F1; = 0 (i.e., independence between <italic>&#x3B8;</italic> and <italic>&#x3B6;</italic>), a Bayes factor is defined to test whether the data support fitting a model <italic>M</italic><sub>1</sub> between <italic>&#x3B8;</italic> and <italic>&#x3B6;</italic> or the null model <italic>M</italic><sub>0</sub> &#x2282; <italic>M</italic><sub>1</sub> with independence. For an introduction to Bayes factors, see <xref ref-type="bibr" rid="CR6">Berger and Delampady (1987)</xref>, <xref ref-type="bibr" rid="CR23">Kass and Raftery (1995)</xref>.</p>
        <p>Both models are given equal prior weight. Therefore, the Bayes factor can be presented as
<disp-formula id="Equ42"><tex-math id="M57">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\rm{BF}} = {{p({\rm{y}},{\rm{t}}|{M_0})} \over {p({\rm{y}},{\rm{t}}|{M_1})}}$$
\end{document}</tex-math></disp-formula><disp-formula id="Equ43"><tex-math id="M58">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$ = {{\smallint p({\rm{y}}|{\rm{z}})p({\rm{z}},{\rm{t}}|{M_0})d{\rm{z}}} \over {\smallint p({\rm{y}}|{\rm{z}})p({\rm{z}},{\rm{t}}|{M_1})d{\rm{z}}}}$$
\end{document}</tex-math></disp-formula><disp-formula id="Equ44"><tex-math id="M59">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$ = {{\smallint p({\rm{y}}|{\rm{z}})p({\rm{z}},{\rm{t}}|\rho = 0)d{\rm{z}}} \over {\smallint \smallint p({\rm{y}}|{\rm{z}})p({\rm{z}},{\rm{t}}|\rho )\pi (\rho )d\rho d{\rm{z}}}}$$
\end{document}</tex-math></disp-formula>. A popular family of conjugate priors for the correlation coefficient has the form (1 &#x2212; <italic>&#x3F1;</italic><sup>2</sup>)<sup>&#x3BD;</sup> on its support, 0 &#x2264; &#x3F1; &#x2264; 1 (<xref ref-type="bibr" rid="CR27">Lee, 2004</xref>). For <italic>&#x3BD;</italic> = 0, a uniform distribution is obtained. For <italic>&#x3BD;</italic> = 5, half-normal distribution is approximated. For <italic>&#x3BD;</italic>&#x2192;&#x221E;, the prior assigns probability 1 to <italic>&#x3F1;</italic> = 0, which yields model <italic>M</italic><sub>0</sub>. To assess the sensitivity of the Bayes factor to the specification of the prior density, a variety of members from the family can be chosen.</p>
      </sec>
    </sec>
    <sec id="Sec13">
      <title>6. Simulation Study</title>
      <p>In the first study, different data sets were simulated and the parameters were re-estimated to check the performance of the Gibbs sampler. In the second study, the properties of the proposed Bayes factor in (<xref rid="Equ44" ref-type="">44</xref>) were investigated for datasets generated for different values of <italic>&#x3F1;</italic> and different choices of prior distributions.We also checked the rejection region for the null hypothesis. In the third study, the characteristics of the proposed DIC test were analyzed.</p>
      <sec id="Sec14">
        <title>6.1. Parameter Recovery</title>
        <p>Datasets were simulated for the following structural component of the model:
<disp-formula id="Equd"><tex-math id="M60">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$
\left( {\begin{array}{*{20}c}
   {\theta _{ij} }  \\
   {\zeta _{ij} }  \\

 \end{array} } \right) = \left( {\begin{array}{*{20}c}
   {\gamma _{00}  + u_{0i}^{(\theta )} }  \\
   {\gamma _{10}  + u_{1j}^{(\zeta )} }  \\

 \end{array} } \right) + \left( {\begin{array}{*{20}c}
   {x_{ij} (w_j \gamma _{01}  + u_{1j}^{(\theta )} )}  \\
   {x_{ij} (w_j \gamma _{11}  + u_{2j}^{(\zeta )} )}  \\

 \end{array} } \right) + \left( {\begin{array}{*{20}c}
   {e_{1ij} }  \\
   {e_{2ij} }  \\

 \end{array} } \right)
$$\end{document}</tex-math></disp-formula>
, where <bold>e</bold><sub><italic>ij</italic></sub> &#x223C; <italic>N</italic>(0,<bold><italic>&#x3A3;</italic></bold><sub><italic>P</italic></sub>), <bold>u</bold><sup>(<italic>&#x3B8;</italic>)</sup> &#x223C; <italic>N</italic>(0,<bold>V</bold><sub>1</sub>) and <bold>u</bold><sup>(<italic>&#x3B6;</italic>)</sup> &#x223C; <italic>N</italic>(0,<bold>V</bold><sub>2</sub>). The model had the same set of explanatory variables in the regression of each latent parameter and had random intercepts and slopes. The intercepts and slopes were taken to be independent of the residuals and across the person parameters. The true values of the structural parameters used in the study are given in Table <xref rid="Tab1" ref-type="table">1</xref>. The values of the explanatory variables <bold>x</bold> and <bold>w</bold> were drawn from a standard normal distribution. For the responses, the 2PL model was assumed and the item parameters were drawn from a multivariate normal distribution with mean <bold><italic>&#x3BC;</italic></bold><sub><italic>I</italic></sub> = (1, 0, 1, 0) and a diagonal covariance matrix <bold><italic>&#x3A3;</italic></bold><sub><italic>I</italic></sub> with all variances equal to 0.5. Negative values of <bold><italic>&#x3D5;</italic></bold> and <bold>a</bold> were simply ignored. Responses and RTs were simulated for <italic>N</italic> = 1,000 persons nested in 50 groups each taking 20 items.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Simulated and estimated values of the structural parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th/><th>True value</th><th>EAP</th><th>SD</th></tr></thead><tbody><tr><td colspan="2">Fixed parameters</td><td/><td/><td/></tr><tr><td/><td>&#x3B3;<sub>00</sub></td><td>0.00</td><td>0.00</td><td>&#x2013;</td></tr><tr><td/><td>&#x3B3;<sub>01</sub></td><td>4.00</td><td>3.77</td><td>0.23</td></tr><tr><td/><td>&#x3B3;<sub>10</sub></td><td>0.00</td><td>0.00</td><td>&#x2013;</td></tr><tr><td/><td>&#x3B3;<sub>11</sub></td><td>3.00</td><td>2.99</td><td>0.12</td></tr><tr><td colspan="2">Variance components</td><td/><td/><td/></tr><tr><td>&#x3A3;<sub><italic>P</italic></sub></td><td>&#x3A3;<sub>11</sub></td><td>1.00</td><td>1.00</td><td>&#x2013;</td></tr><tr><td/><td>&#x3C3;<sub>12</sub></td><td>0.50</td><td>0.55</td><td>0.04</td></tr><tr><td/><td>&#x3A3;<sub>22</sub></td><td>1.00</td><td>1.07</td><td>0.06</td></tr><tr><td>V<sub>1</sub></td><td><italic>V</italic><sub>11</sub></td><td>1.00</td><td>1.00</td><td>0.25</td></tr><tr><td/><td><italic>V</italic><sub>12</sub></td><td>0.50</td><td>0.48</td><td>0.22</td></tr><tr><td/><td><italic>V</italic><sub>22</sub></td><td>1.00</td><td>1.13</td><td>0.35</td></tr><tr><td>V<sub>2</sub></td><td><italic>V</italic><sub>11</sub></td><td>1.00</td><td>1.07</td><td>0.23</td></tr><tr><td/><td><italic>V</italic><sub>12</sub></td><td>0.50</td><td>0.47</td><td>0.17</td></tr><tr><td/><td><italic>V</italic><sub>22</sub></td><td>1.00</td><td>0.86</td><td>0.19</td></tr></tbody></table></table-wrap></p>
        <p>In the estimation procedure, the following hyperparameters were used: Scale matrices <italic>&#x3A3;</italic><sub><italic>I</italic>0</sub> and <italic>&#x3A3;</italic><sub><italic>&#x3B3;</italic>0</sub> were chosen to be diagonal with elements 0.01 to indicate vague proper prior information, and we set <bold><italic>&#x3BC;</italic></bold><sub><italic>I</italic>0</sub> = (1, 0, 1, 0) and <bold><italic>&#x3B3;</italic></bold><sub>0</sub> = <bold>0</bold>. Besides, a vague normal prior with parameters <italic>&#x3BC;</italic><sub>&#x3C1;</sub> = 0 and <italic>&#x3C3;</italic><sub arrange="stack"><italic>&#x3C1;</italic></sub><sup arrange="stack">2</sup> = 10 was specified for <italic>&#x3C1;</italic>.</p>
        <p>The MCMC procedure was iterated 50,000 times and the first 5,000 iterations were discarded when the means and posterior standard deviations of the parameters were estimated.</p>
        <p>The accuracy of the parameter estimates was investigated by comparing them to their true values. The results for the parameters in the structural model are given in Table <xref rid="Tab1" ref-type="table">1</xref>. Both the estimates of the fixed parameters and the variance components are in close agreement with the true values. (Note that <italic>&#x3B3;</italic><sub>00</sub> and <italic>&#x3B3;</italic><sub>10</sub> are zero due to the identifying restrictions.) Although not shown here, the same close agreement was observed for the item parameter estimates.</p>
      </sec>
      <sec id="Sec15">
        <title>6.2. Sensitivity of the Bayes Factor</title>
        <p>Usually, we will have little prior information about the correlation of the person parameters. Therefore, it is important to know how the Bayes factor behaves for a relatively vague prior distribution of the correlation <inline-formula id="IEqn"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqn.gif --><tex-math id="M61">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\rho = {\rho ^2}/\sqrt {\sigma _\theta ^2\sigma _\zeta ^2} $$
\end{document}</tex-math></inline-formula>. In total, 500 data sets were simulated for different values of <italic>&#x3F1;</italic> &#x2208; [0, 1] and an empty structural model for the person parameters. All other specifications were identical to those in the preceding study. The Bayes factor in (<xref rid="Equ44" ref-type="">44</xref>) was calculated using an importance sampling method (<xref ref-type="bibr" rid="CR33">Newton &amp; Raftery, 1994</xref>). For each dataset, the calculations were repeated for different priors for the correlation parameter.</p>
        <p>Following <xref ref-type="bibr" rid="CR27">Lee (2004)</xref>, a reference prior for <italic>&#x3F1;</italic> was used, which led to
<disp-formula id="Equ45"><tex-math id="M62">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\rm{BF}}(v) = {{\smallint p({\rm{y}}|{\rm{z}})p({\rm{z}},{\rm{t}}|\rho = 0)d{\rm{z}}} \over {\smallint \smallint p({\rm{y}}|{\rm{z}})p({\rm{z}},{\rm{t}}|\rho )C{{(1 - {\rho ^2})}^v}d\rho d{\rm{z}}}}$$
\end{document}</tex-math></disp-formula>, with <italic>C</italic> the normalizing constant. According to Jefreys&#x2019; scheme (<xref ref-type="bibr" rid="CR23">Kass &amp; Raftery, 1995</xref>), 1/BF(<italic>&#x3BD;</italic>) &gt; 3 implies evidence against the null hypothesis of <italic>&#x3F1;</italic> = 0 given the value of <italic>&#x3BD;</italic>.</p>
        <p>The results are shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, where the dotted line indicates log(BF) = 0. For true values of <italic>&#x3F1;</italic> close to zero or larger than 0.35, the Bayes factor yielded the same conclusion for all chosen priors. More specifically, it favored the null model for all values of <italic>&#x3F1;</italic> below 0.1 but the alternative model for all values larger than 0.35. It can also be seen that the estimated Bayes factors are higher (and thus favor the null model more frequently) for lower values of <italic>&#x3BD;</italic>, which correspond to the less informative priors. For <italic>&#x3F1;</italic> &#x2208; [0.20, 0.35], the prior distribution of <italic>&#x3F1;</italic> was the major determinant of the Bayes factor favoring the null or the alternative model.
<fig id="Fig1"><label>Figure 1</label><caption><p>Log(BF) as a function of the correlation between accuracy and speed for 3 different priors for &#x3F1;</p></caption><graphic position="anchor" xlink:href="11336_2008_Article_9075_Fig1" id="MO1"/></fig></p>
        <p>It can be concluded that the Bayes factor is sensitive to the prior choice for <italic>&#x3F1;</italic>. Figure <xref rid="Fig1" ref-type="fig">1</xref> gives a clear idea about the variation of the Bayes factor for a class of prior distributions. This information can be used in real-world applications when a prior for <italic>&#x3F1;</italic> needs to be selected but the information about this parameter is poor.</p>
      </sec>
      <sec id="Sec16">
        <title>6.3. Iterative Model Building Using the DIC</title>
        <p>In this study, it was investigated whether the DIC can be used to choose between models with different fixed and/or random terms in the structural component of the model for the person parameters. Data were simulated for 1,000 persons nested in 20 groups each taking 20 items using a model that is explained below. The setup was the same as in the earlier parameter recovery study; the only difference was that <italic>w</italic><sub><italic>j</italic></sub> was set equal to one for all <italic>j</italic></p>
        <p>Table <xref rid="Tab2" ref-type="table">2</xref> summarizes the calculations of the DIC for four different models. <inline-formula id="IEqo"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqo.gif --><tex-math id="M63">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\bar D$$
\end{document}</tex-math></inline-formula> is the estimated posterior mean deviance; <inline-formula id="IEqp"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqp.gif --><tex-math id="M64">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$D(\hat \Omega )$$
\end{document}</tex-math></inline-formula> is the deviance for the posterior mean of the parameter values.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Deviance summaries for the four models in the simulation study.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th><inline-formula id="IEqq"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqq.gif --><tex-math id="M65">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\bar D$$
\end{document}</tex-math></inline-formula></th><th><inline-formula id="IEqr"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqr.gif --><tex-math id="M66">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$D(\hat \Omega )$$\end{document}</tex-math></inline-formula></th><th><italic>p</italic><sub><italic>D</italic></sub></th><th>DIC</th></tr></thead><tbody><tr><td>1, Two-level, fixed parameters</td><td>40,168</td><td>38,184</td><td>1,984</td><td>42,152</td></tr><tr><td>2, Empty two-level</td><td>40,161</td><td>38,194</td><td>1,967</td><td>42,129</td></tr><tr><td>3, Two-level+\gw \vb x</td><td>40,172</td><td>38,206</td><td>1,966</td><td>42,139</td></tr><tr><td>4, Three-level + \gw \vb x</td><td>40,165</td><td>38,290</td><td>1,875</td><td>42,039</td></tr></tbody></table></table-wrap></p>
        <p>Model 1 was an empty two-level model with fixed parameters for <bold><italic>&#x3B8;</italic></bold> and <bold><italic>&#x3B6;</italic></bold>, which was obtained by setting <italic>&#x3C1;</italic> to 0 and the variances of each person parameters equal to 1,000. Model 2 was an empty two-level model that ignored any group structure for the test takers. In Model 3, the two-level structure was extended with a covariate of <bold><italic>&#x3B6;</italic></bold> and <bold><italic>&#x3B8;</italic></bold> but no group structure was assumed. Model 4 was the true model under which the data were simulated; this model did have both the covariate and the group structure for the test takers. Identification of the models was obtained via the restriction <inline-formula id="IEqs"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqs.gif --><tex-math id="M67">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\Pi _{k = 1}^K{a_k} = 1$$
\end{document}</tex-math></inline-formula> on the item parameters. This choice is motivated as follows: In Model 4, the variability of the person parameters is explained by <bold><italic>V</italic></bold> and the covariates. When estimating Model 1, 2, or 3 from the data simulated under Model 4, this variability should be captured by <bold><italic>&#x3A3;</italic></bold><sub><italic>P</italic></sub> as extra residual variation. Therefore, <bold><italic>&#x3A3;</italic></bold><sub><italic>P</italic></sub> was left unrestricted; otherwise, the variance would have been traded with the estimated <bold><italic>a</italic></bold> parameters, which might have led to misinterpretation of the results.</p>
        <p>As expected, the DIC values we found suggested that Model 4 was best performing. Particularly, it can be seen that when the grouping of the test takers was ignored, this led to an increase in the effective number of parameters. Note that the optimal model choice is not necessarily the best fitting model, but a tradeoff between model fit and the number of parameters used.</p>
      </sec>
    </sec>
    <sec id="Sec17">
      <title>7. Empirical Examples</title>
      <p>In this section, two empirical examples illustrate the use of several developments that were presented in this paper.</p>
      <sec id="Sec18">
        <title>7.1. First Example</title>
        <p>A data set of 286 persons who had taken a computerized version of a 22-item personality questionnaire was analyzed. The respondents were Psychology and Social Sciences undergraduates from a university in Spain. The majority of the students was between 18 and 30 years old (age variable), and this group consisted of 215 girls and 71 boys (gender variable). The questionnaire consisted of two scales of 11 dichotomous items measuring neuroticism and extraversion. The neuroticism dimension assesses whether a person is prone to experience unpleasant emotions and is emotionally unstable and the extraversion dimension measures sociability, enthusiasm, and arousal of pleasure. According to the five factor model, these two dimensions summarize part of the covariation among personality traits.</p>
        <p>As already indicated, because it does not assume anything about the relationship between the speed at which the individual test takers work and the latent trait represented in the response model, the modeling framework can also be applied to personality questionnaires. For this domain, it is also interesting to study the responses and RTs simultaneously. In addition to the statistical advantages of multivariate modeling of the data over separate univariate modeling, such a study would allow us to infer, for instance, how differences in speed levels between subgroups of test takers correlate with differences in their personality traits.</p>
        <p>From earlier results, it was known that there is a moderate negative dependency between neuroticism and extraversion (<xref ref-type="bibr" rid="CR4">Becker, 1999</xref>; <xref ref-type="bibr" rid="CR29">McCrea &amp; Costa, 1997</xref>). Here, interest was focused on the differences between students with respect to these two personality dimensions given age and gender. Additionally, variation in the respondent&#x2019;s speed-levels with respect to neuroticism and extraversion was explored. This part of the study involved the estimation of the covariances between the personality traits and the latent speed-levels.</p>
        <p>Since this test consisted of yes-no personality questions, the 2PL model was chosen as the measurement model for the responses. For the RTs, the measurement part was specified by (<xref rid="Equ2" ref-type="">2</xref>). The following structural part was specified to explore the variation in speed and the traits as a function of both background variables:
<disp-formula id="Equ46"><tex-math id="M68">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\eqalign{ &amp; = {{\smallint p({\rm{y}}|{\rm{z}})p({\rm{z}},{\rm{t}}|\rho = 0)d{\rm{z}}} \over {\smallint \smallint p({\rm{y}}|{\rm{z}})p({\rm{z}},{\rm{t}}|\rho )C{{(1 - {\rho ^2})}^v}d\rho d{\rm{z}}}} \cr &amp; \left( {\matrix{ {{\theta _i}} \cr {{\zeta _i}} \cr  } } \right) = \left( {\matrix{ {{\gamma _{00}}} \cr {{\gamma _{10}}} \cr  } } \right) + \left( {\matrix{ {{\gamma _{01}}{\rm{Mal}}{{\rm{e}}_i} + {\gamma _{02}}{\rm{Ag}}{{\rm{e}}_i}} \cr {{\gamma _{11}}{\rm{Mal}}{{\rm{e}}_i} + {\gamma _{12}}{\rm{Ag}}{{\rm{e}}_i}} \cr  } } \right) + \left( {\matrix{ {{e_{1i}}} \cr {{e_{2i}}} \cr  } } \right) \cr} $$
\end{document}</tex-math></disp-formula>, where <bold><italic>e</italic></bold> &#x223C; <italic>N</italic>(0,<bold><italic>&#x3A3;</italic></bold><sub><italic>P</italic></sub>). Further, <italic>&#x3B3;</italic><sub>00</sub> and <italic>&#x3B3;</italic><sub>10</sub> denote the intercepts, <italic>&#x3B3;</italic>01 and <italic>&#x3B3;</italic>11 represent the effects of being male, and <italic>&#x3B3;</italic><sub>02</sub> and <italic>&#x3B3;</italic><sub>12</sub> represent the effects of age. The age vector contained the age of the test takers on a continuous scale.</p>
        <p>Four models were fitted to the data: (1) null model without covariates, (2) and (3) structural multivariate model with age and gender as a covariate, respectively, and (4) full structural multivariate model with both age and gender as covariates. For estimation, proper noninformative priors were specified, with all prior variance components set at 100 and the covariances at 0. The MCMC algorithm was iterated 50,000 times; the first 10,000 iterations were discarded as the burn-in period.</p>
        <p>Posterior predictive checks were used to evaluate several assumptions of the model. An important assumption of the model is that of local independence. Therefore, an odds ratio statistic was used to test for possible dependencies between response patterns of items. For an impression of overall fit of the response model, an observed score statistic was estimated to assess if the model was able to replicate the observed score patterns. For a detailed description of these two statistics, see <xref ref-type="bibr" rid="CR45">Sinharay (2005)</xref> and <xref ref-type="bibr" rid="CR47">Sinharay, Johnson, and Stern (2006)</xref>. To assess the fit of the RT model, <xref ref-type="bibr" rid="CR56">van der Linden and Guo (2008)</xref> proposed a Bayesian residual analysis. That is, by evaluating the actual observation <italic>t</italic><sub><italic>ik</italic></sub> under the posterior density, the probability of observing a value smaller than <italic>t</italic><sub><italic>ik</italic></sub> can be approximated by <inline-formula id="IEqt"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqt.gif --><tex-math id="M69">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$p \approx \Sigma _{m = 1}^M\Phi ({t_{ik}}|\zeta _i^{(m)},\phi _k^{(m)},\lambda _k^{(m)})/M$$
\end{document}</tex-math></inline-formula>, from <italic>M</italic> iterations from the MCMC chain. According to the probability integral transform theorem, under a good fitting model, these probabilities should be distributed <italic>U</italic>(0, 1). Model fit can then be checked graphically by plotting the posterior <italic>p</italic>-values against their expected values under the <italic>U</italic>(0, 1) distribution.</p>
        <p>The posterior checks of the model were based on 1,000 replicated data sets from the posterior distribution. The fitted IRT model replicated the responses well, as the observed sum score statistic did not point at any significant flaws for neither of the two scales. The odds ratio statistic indicated that for two item combinations on the neuroticism scale and for four item combinations on the extraversion scale, a violation of local independence might exist. However, given all possible item combinations, these possible violations of local independence did not give any reason to doubt the unidimensionality of the scales. As indicated by minor deviations in de lower tail and in the middle of the <italic>U</italic>(0, 1) distributions, the RT model tended to slightly underpredict the quicker responses (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>). Overall, however, model fit was satisfactory.
<fig id="Fig2"><label>Figure 2</label><caption><p>Probabilities of <italic>P</italic>(t*<sub><italic>ik</italic></sub> &lt; <italic>t</italic><sub><italic>ik</italic></sub>|<bold>y, t</bold>) against their expected values under the <italic>U</italic>(0,1) distribution for the 11 items of the neuroticism scale, Example 2.</p></caption><graphic position="anchor" xlink:href="11336_2008_Article_9075_Fig2" id="MO2"/></fig></p>
        <p>Table <xref rid="Tab3" ref-type="table">3</xref> gives the calculated DIC values for the four models and the two scales. Comparing the results for model (1) and model (3), the DIC criterium yielded no significant difference in performance between boys and girls on both scales. That is, for the neuroticism as well as the extraversion scale, no mean significant difference between boys and girls was found, neither in the latent personality traits, nor in the speed of working on the test. Neither did the age of the test takers explain any significant amount of variation in the personality traits and speed levels.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>DIC values for 4 models fitted to the neuroticism and extraversion scales.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th colspan="3">Neuroticism</th><th colspan="3">Extraversion</th></tr><tr><th/><th><italic>p</italic><sub><italic>D</italic></sub></th><th><inline-formula id="IEqu"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqu.gif --><tex-math id="M70">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$D(\hat \Omega )$$
\end{document}</tex-math></inline-formula></th><th>DIC</th><th><italic>p</italic><sub><italic>D</italic></sub></th><th><inline-formula id="IEqv"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqv.gif --><tex-math id="M71">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$D(\hat \Omega )$$
\end{document}</tex-math></inline-formula></th><th>DIC</th></tr></thead><tbody><tr><td>1 (null)</td><td>572</td><td>3,839</td><td>4,983</td><td>572</td><td>3,767</td><td>4,911</td></tr><tr><td>2 (gender)</td><td>572</td><td>3,846</td><td>4,990</td><td>572</td><td>3,774</td><td>4,918</td></tr><tr><td>3 (age)</td><td>572</td><td>3,941</td><td>5,085</td><td>572</td><td>3,838</td><td>4,982</td></tr><tr><td>4 (full)</td><td>572</td><td>3,943</td><td>5,087</td><td>572</td><td>3,827</td><td>4,971</td></tr></tbody></table></table-wrap></p>
        <p>Next, the respondents were clustered with respect to their estimated extraversion scores. The clustering was such that the intervals of respondents&#x2019; scores in each cluster had equal probability mass under a normal model for the population distribution. The sample size of 286 respondents allowed a grouping of respondents in eight different clusters of extraversion levels. Note that the clusters were obtained from an estimated population model, and that they jointly represented the entire score range.</p>
        <p>It was investigated whether the grouping of respondents with respect to the extraversion scores explained any variation in respondents&#x2019; neuroticism scores. Further, the influence of the background variables was explored. The following multivariate random effects structural model was specified:
<disp-formula id="Equ47"><label>(47)</label><tex-math id="M72">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}$$
\left( {\begin{array}{*{20}c}
   {\theta _{ij} }  \\
   {\zeta _{ij} }  \\

 \end{array} } \right) = \left( {\begin{array}{*{20}c}
   {\gamma _{00}  + u_{0i}^{(\theta )} }  \\
   {\gamma _{10}  + u_{1j}^{(\zeta )} }  \\

 \end{array} } \right) + \left( {\begin{array}{*{20}c}
   {Male_{ij} (\gamma _{01}  + u_{01j}^{(\theta )}  + Age_{ij} (\gamma _{02}  + u_{02j}^{(\theta )} )}  \\
   {Male_{ij} (\gamma _{11}  + u_{11j}^{(\zeta )} ) + Age_{ij} (\gamma _{12}  + u_{12j}^{(\zeta )} )}  \\

 \end{array} } \right) + \left( {\begin{array}{*{20}c}
   {e_{1ij} }  \\
   {e_{2ij} }  \\

 \end{array} } \right)
$$\end{document}</tex-math></disp-formula>
, where <bold>e</bold><sub><italic>ij</italic></sub> &#x223C; <italic>N</italic>(0,<bold><italic>&#x3A3;</italic></bold><sub><italic>P</italic></sub>), <bold>u</bold><sup>(<italic>&#x3B8;</italic>)</sup> &#x223C; <italic>N</italic>(0,<bold>V</bold><sub>1</sub>) and <bold>u</bold><sup>(<italic>&#x3B6;</italic>)</sup> &#x223C; <italic>N</italic>(0,<bold>V</bold><sub>2</sub>). In (<xref rid="Equ47" ref-type="">47</xref>), the intercepts and slope coefficients for the regression on the neuroticism scores and the speed levels were treated as random across clusters of extraversion levels. These random effects were allowed to correlate both within the regression on the neuroticism scores and within the regression on the speed levels. Also, the error terms at the individual level were allowed to correlate since the speed levels and neuroticism scores were clustered within individuals.</p>
        <p>Five models were fitted to the neuroticism scale by restricting one or more parameters to zero: (1) the null model with fixed intercepts by restricting <bold>V</bold><sub>1</sub> and <bold>V</bold><sub>2</sub> to be zero; (2) the empty multivariate random effects model (without covariates) with free covariance parameters; (3)&#x2013;(4) a multivariate random effects model including a random regression effect for gender and age, respectively, and (5) the full model as specified in (<xref rid="Equ47" ref-type="">47</xref>).</p>
        <p>Using the proper noninformative priors described earlier, the models were estimated using 50,000 iterations of the Gibbs sampler, where 10,000 iterations were discarded because of the burn-in. The DIC value for each of the five models was estimated using (<xref rid="Equ36" ref-type="">36</xref>) since our interest was focused primarily on the structural model on the speed levels and neuroticism scores. The estimated DIC values are given in Table <xref rid="Tab4" ref-type="table">4</xref>.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>DIC values for five models fitted to the neuroticism scale, accounting for a grouping of respondents in extraversion levels.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th><italic>p</italic><sub><italic>D</italic></sub></th><th><inline-formula id="IEqw"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqw.gif --><tex-math id="M73">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$D(\hat \Omega )$$
\end{document}</tex-math></inline-formula></th><th>DIC</th></tr></thead><tbody><tr><td>1 (null)</td><td>572</td><td>3,839</td><td>4,983</td></tr><tr><td>2 (empty)</td><td>521</td><td>3,858</td><td>4,899</td></tr><tr><td>3 (gender)</td><td>536</td><td>3,866</td><td>4,938</td></tr><tr><td>4 (age)</td><td>572</td><td>3,860</td><td>5,004</td></tr><tr><td>5 (full)</td><td>572</td><td>3,873</td><td>5,017</td></tr></tbody></table></table-wrap><table-wrap id="Tab5"><label>Table 5</label><caption><p>Estimated covariance components and correlations.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="2">Variance components</th><th>EAP</th><th>SD</th><th>Cor</th></tr></thead><tbody><tr><td>&#x3A3;<sub><italic>p</italic></sub></td><td>&#x3A3;<sub>11</sub></td><td>1.00</td><td>&#x2013;</td><td>1.00</td></tr><tr><td/><td>&#x3A3;<sub>12</sub></td><td>&#x2212;0.380</td><td>0.02</td><td>&#x2212;0.76</td></tr><tr><td/><td>&#x3A3;<sub>22</sub></td><td>0.25</td><td>0.02</td><td>1.00</td></tr><tr><td/><td>&#x3A3;<sub>11</sub></td><td>0.15</td><td>0.04</td><td>1.00</td></tr><tr><td/><td>&#x3A3;<sub>12</sub></td><td>&#x2212;0.110</td><td>0.04</td><td>&#x2212;0.53</td></tr><tr><td/><td>&#x3A3;<sub>13</sub></td><td>0.05</td><td>0.02</td><td>0.41</td></tr><tr><td/><td>&#x3A3;<sub>14</sub></td><td>0.02</td><td>0.04</td><td>0.09</td></tr><tr><td/><td>&#x3A3;<sub>22</sub></td><td>0.33</td><td>0.07</td><td>1.00</td></tr><tr><td/><td>&#x3A3;<sub>23</sub></td><td>0.06</td><td>0.03</td><td>0.34</td></tr><tr><td/><td>&#x3A3;<sub>24</sub></td><td>0.07</td><td>0.05</td><td>0.21</td></tr><tr><td/><td>&#x3A3;<sub>33</sub></td><td>0.10</td><td>0.02</td><td>1.00</td></tr><tr><td/><td>&#x3A3;<sub>34</sub></td><td>0.10</td><td>0.03</td><td>0.51</td></tr><tr><td/><td>&#x3A3;<sub>44</sub></td><td>0.35</td><td>0.06</td><td>1.00</td></tr></tbody></table></table-wrap><table-wrap id="Tab6"><label>Table 6</label><caption><p>Estimated Bayes factors and regression parameters for the structural models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Hypothesis</th><th/><th>log(BF)</th></tr></thead><tbody><tr><td>H<sub>01</sub></td><td/><td>&#x2212;5.0</td></tr><tr><td>H<sub>02</sub></td><td/><td>&#x2212;21.3</td></tr><tr><td>H<sub>03</sub></td><td/><td>16.4</td></tr><tr><td>H<sub>04</sub></td><td/><td>16.3</td></tr><tr><td>H<sub>05</sub></td><td/><td>16.2</td></tr><tr><td>Fixed parameters</td><td>EAP</td><td>SD</td></tr><tr><td>&#x3B3;<sub>01</sub></td><td>0.25</td><td>0.03</td></tr><tr><td>&#x3B3;<sub>02</sub></td><td>0.25</td><td>0.03</td></tr><tr><td>&#x3B3;<sub>12</sub></td><td>&#x2212;0.22</td><td>0.02</td></tr></tbody></table></table-wrap></p>
        <p>It can be seen that the empty multivariate random-effects model had a smaller effective number of model parameters relative to the null model and was to be preferred given the DIC values of both models. The estimated deviance increased slightly for models 3&#x2013;5, which can be attributed to additional sampling variance introduced by the covariates. Note that in the empty multivariate random-effects model, the individual random-effect parameters were modeled as group-specific random effects at the level of the clusters of extraversion scores (a third level in the model) and that this led to a serious reduction in the effective number of model parameters. It can be concluded that the grouping of respondents according to their extraversion levels explained a substantial amount of variation in the speed levels as well as the neuroticism scores. The estimated correlation between the neuroticism scores and the speed levels was 0.30 (with a standard deviation of 0.07), which justified the multivariate modeling approach. Intraclass correlation coefficients were calculated to asses the amount of variability in the individual neuroticism scores and the speed levels due to the grouping of respondents in clusters of extraversion levels. The intraclass correlation estimates for neuroticism and the speed trait were based on the MCMC output for the empty multivariate random effects model. The estimates were
<disp-formula id="Eque"><tex-math id="M74">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\rm{IC}}{{\rm{C}}_\theta } \approx {1 \over M}\sum\limits_{m = 1}^M {{{V_{11}^{(m)}} \over {V_{11}^{(m)} + \sigma _\theta ^{{2^{(m)}}}}} = 0.12} $$
\end{document}</tex-math></disp-formula>,
<disp-formula id="Equf"><tex-math id="M75">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\rm{IC}}{{\rm{C}}_\zeta } \approx {1 \over M}\sum\limits_{m = 1}^M {{{V_{22}^{(m)}} \over {V_{22}^{(m)} + \sigma _\zeta ^{{2^{(m)}}}}} = 0.07} $$
\end{document}</tex-math></disp-formula>, where <italic>m</italic> = 1, &#x2026;, <italic>M</italic> denotes the number of iterations after burn-in. It follows that 12% of the variability in the neuroticism trait could be explained by the grouping of the respondents by their extraversion levels. It is surprising that 7% of the variability in speed levels was located at the group level. This means that the clustering of the respondents via the estimated extraversion levels explained a significant amount of variation in the individual speed levels corresponding to the neuroticism test. The explanation is supported by the estimated correlation between both speed parameters, which was 0.76. Note that this relatively high correlation between the individual speed levels on the two tests also supports the assumption of stationary speed during testing. Finally, the DIC values show that the covariates did not explain any variation in the trait or speed levels. It can be seen that the introduction of random regression parameters for the background variables did not lead to any reduction in the effective number of parameters since the covariates did not explain any variation within the grouped neuroticism scores. Neither did they for the entire sample of neuroticism scores.</p>
      </sec>
      <sec id="Sec19">
        <title>7.2. Second Example</title>
        <p>In this example, the data set studied earlier by <xref ref-type="bibr" rid="CR61">Wise, Kong, and Pastor (2007)</xref> was analyzed. This data set included 388 test takers who each answered 65 items of a computer-based version of the <italic>Natural World Assessment Test</italic> (NAW-8). This test is used to assess the quantitative and scientific reasoning proficiencies of college students. It was part of a required education assessment for mid-year sophomores by a medium-sized university. Covariates for the test takers such as their SAT scores, gender (GE), a self-report measure of citizenship (CS) and a self-report measure of test effort (TE) were available. Citizenship was a measure of a test taker&#x2019;s willingness to help the university collecting its assessment data, whereas test effort reflected the importance of the test to the test taker. The number of response options for the items varied between 2 and 6.</p>
        <p>The 3PL model was chosen as the measurement model for the responses. In the estimation procedure, the same hyperparameter values as in the simulation study above were used to specify vague proper prior knowledge. The model was estimated with 20,000 iterations of the Gibbs sampler, and the first 10,000 iterations were discarded as the burn-in. The odds ratio statistic indicated that for less than 4% of the possible item combinations there was a significant dependency between two items. The replicated response patterns under the posterior distribution matched the observed data quite well, as shown by the observed sum score statistic. From the posterior residual check, it followed that the RT model described the data well. The estimated time discrimination parameters varied over [0.25, 1.65], indicating that the items discriminated substantially between test takers of different speed. This result was verified by testing the RT model with <bold><italic>&#x3D5;</italic></bold> = 1 against the RT model where <bold><italic>&#x3D5;</italic></bold> &#x2260; 1 using the DIC. The estimated DIC&#x2019;s were 85,780 and 84,831 for the restricted and for the unrestricted RT model, respectively.</p>
        <p>Table <xref rid="Tab5" ref-type="table">5</xref> gives the estimated covariance components and correlations between the level 1 parameters. The correlation between the person parameters was estimated to be &#x2212;0.76. The Bayes factor for testing the null hypothesis of this correlation being zero, clearly favored the alternative for the range of possible priors given in the simulation study above. Therefore, for this data set, fitting the hierarchical model has to be favored over the alternative of independence between the two constructs. An explanation for this strong negative dependency might be that higher-ability candidates have more insight in their test behavior and, therefore, are better at time management. A negative correlation between speed and ability also often suggests a nonspeeded test, because it implies that higher ability test takers who take their time do not run out of time toward the end of the test.</p>
        <p>As shown earlier by <xref ref-type="bibr" rid="CR54">van der Linden et al. (2007)</xref>, response times can be a valuable tool for diagnosing differential speededness. Thereby, checks on the assumption of stationary speed during the test are particularly useful. For each test taker, the standardized residuals <inline-formula id="IEqx"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqx.gif --><tex-math id="M76">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${e_{ijk}} = ({t_{ijk}} - ({\lambda _k} - {\phi _k}{\zeta _{ij}}))/{\tau _k}$$
\end{document}</tex-math></inline-formula> were calculated. When the stationary speed assumption holds, a test taker&#x2019;s residual pattern shows randomly varying residuals which almost all will lie between [&#x2212;2,2]. However, a test taker running out of time will show a deviation of this assumption toward the end of the test. In such a case, this result is misfit of the RT model, because of larger residuals for the test taker on these last items. In Fig. <xref rid="Fig3" ref-type="fig">3</xref>, residual patterns of the RT model for 16 test takers are shown. An aberrancy can be seen in the last figure, where for some items the test taker responded unusually fast. However, a graphical check of the residual patterns of all the test takers did not reveal any structural aberrancies. Therefore, there were no indications of speededness for this test.
<fig id="Fig3"><label>Figure 3</label><caption><p>Standardized residual patterns for the RT model for 16 selected test takers.</p></caption><graphic position="anchor" xlink:href="11336_2008_Article_9075_Fig3" id="MO3"/></fig></p>
        <p>Subsequently, the following structural model on the person parameters was specified to identify possible relationships of ability and speed with the covariates:
<disp-formula id="Equ48"><tex-math id="M77">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\left( {\matrix{ {{\theta _i}} \cr {{\xi _i}} \cr  } } \right) = \left( {\matrix{ {{\gamma _{00}}} \cr {{\gamma _{10}}} \cr  } } \right) + \left( {\matrix{ {{\rm{SA}}{{\rm{T}}_{i{\gamma _{01}}}} + {\rm{T}}{{\rm{E}}_{_i{\gamma _{02}}}} + {\rm{G}}{{\rm{E}}_{i{\gamma _{03}}}} + {\rm{C}}{{\rm{S}}_{i{\gamma _{04}}}}} \cr {{\rm{SA}}{{\rm{T}}_{i{\gamma _{11}}}} + {\rm{T}}{{\rm{E}}_{_i{\gamma _{12}}}} + {\rm{G}}{{\rm{E}}_{i{\gamma _{13}}}} + {\rm{C}}{{\rm{S}}_{i{\gamma _{14}}}}} \cr  } } \right) + \left( {\matrix{ {{e_{1i}}} \cr {{e_{2i}}} \cr  } } \right)$$
\end{document}</tex-math></disp-formula>, where <bold>e</bold><sub><italic>i</italic></sub> &#x223C; <italic>N</italic>(0,<bold><italic>&#x3A3;</italic></bold><sub><italic>P</italic></sub>). Several hypotheses about this model were tested. First, the composite null hypothesis <italic>H</italic><sub>01</sub> of both <italic>&#x3B3;</italic><sub>01</sub> and <italic>&#x3B3;</italic><sub>11</sub> equal zero was tested. Second, the null hypotheses <italic>H</italic>{ia02} that <italic>&#x3B3;</italic><sub>02</sub> and <italic>&#x3B3;</italic><sub>12</sub> equal zero were evaluated and, similarly, the hypotheses <italic>H</italic><sub>03</sub> (<italic>&#x3B3;</italic><sub>03</sub> and <italic>&#x3B3;</italic><sub>13</sub> equal zero) and <italic>H</italic><sub>04</sub> (<italic>&#x3B3;</italic><sub>04</sub> and <italic>&#x3B3;</italic><sub>14</sub> equal zero) were evaluated. Finally, by iterative model building, the composite hypothesis <italic>H</italic><sub>05</sub> of the effects <italic>&#x3B3;</italic><sub>03</sub>, <italic>&#x3B3;</italic>04, <italic>&#x3B3;</italic><sub>12</sub>, <italic>&#x3B3;</italic>13 and <italic>&#x3B3;</italic>14 equal to zero was tested. Testing these hypotheses corresponds to comparing models that differ only in their fixed part. This can be easily done via a Bayes factor because by using a result known as the Savage-Dickey density ratio (<xref ref-type="bibr" rid="CR13">Dickey, 1971</xref>; <xref ref-type="bibr" rid="CR58">Verdinelli &amp; Wasserman, 1995</xref>), these Bayes factors are easy to obtain in reduced computation time.</p>
        <p>The hypotheses that gender and citizenship had no effect on ability and speed were con- firmed. Also, the estimated 0.95 HPD regions of their effects (and their 0.90 HPD regions too) included 0, which was another indication that these covariates did not have any explanatory power in speed and ability. However, the SAT scores and test effort explained a significant amount of variation between the person parameters. This result implies the following reduced model:
<disp-formula id="Equ49"><tex-math id="M78">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\left( {\matrix{ {{\theta _i}} \cr {{\zeta _i}} \cr  } } \right) = \left( {\matrix{ {{\gamma _{00}}} \cr {{\gamma _{10}}} \cr  } } \right) + \left( {\matrix{ {{\rm{SA}}{{\rm{T}}_{i{\gamma _{01}}}} + {\rm{T}}{{\rm{E}}_{_i{\gamma _{02}}}}} \cr {{\rm{T}}{{\rm{E}}_{_i{\gamma _{12}}}}} \cr  } } \right) + \left( {\matrix{ {{e_{1i}}} \cr {{e_{2i}}} \cr  } } \right)$$
\end{document}</tex-math></disp-formula>, where <bold>e</bold><sub><italic>i</italic></sub> &#x223C; <italic>N</italic>(0,<bold><italic>&#x3A3;</italic></bold><sub><italic>P</italic></sub>). The Bayes factors for the several nested models and the final estimates of the regression parameters are given in Table <xref rid="Tab6" ref-type="table">6</xref>.</p>
        <p>Intuitively, a positive relationship of TE with ability should have been expected. That is, test takers scoring higher on the TE-scale should have been expected to differ from test takers who care less about their results. Also, when the test is relatively more important to the candidate, he/she can be expected to try harder and spend more time on each item to get better results. The negative relationship of TE with speed is also in agreement with this hypothesis since a lower speed results in higher expected RTs. As expected, the SAT score shows a positive relationship with ability. However, there was no significant effect for SAT with respect to the speed of working of the test takers.</p>
      </sec>
    </sec>
    <sec id="Sec20" sec-type="discussion">
      <title>8. Discussion</title>
      <p>A framework for a multivariate multilevel modeling approach was given in which the latent response parameters are measured using conjoint IRT models for the response and response time data. The IRT models for speed and ability are based on the assumption of conditional independence. This means that the ability parameter (speed parameter) is the assumed underlying construct for the response data (response time data). As a result, for each individual, at the level of measurements the responses and response times are local independent given the latent person parameters. The correlation structure between the person parameters is specified at a higher level. The correlation between speed and accuracy in the population of respondents can be tested via a Bayes factor. As the empirical examples showed, the correlation between ability and speed is not necessarily positive. The sign of this relationship will probably depend on the type of test and the test conditions. That is, sometimes hard work will pay off (e.g., a test with strict time limit) while for another setting &#x201C;take your time&#x201D; might be the best advice. RTs can give insight about the best strategy of test taking, which is useful information for both test takers and test developers. Other model selection issues related to the structural model on the person parameters can be handled via the proposed DIC which can be computed as a by product of the MCMC algorithm. It was shown that the MCMC algorithm performed well and enabled simultaneous estimation.</p>
      <p>The class of multivariate mixture models has not received much attention in the literature. <xref ref-type="bibr" rid="CR38">Schafer and Yucel (2002)</xref> developed an MCMC implementation for the linear multivariate mixed model with incomplete data that does converge rapidly for a small number of large groups but it is limited to two levels of nesting. <xref ref-type="bibr" rid="CR43">Shah, Laird, and Schoenfeld (1997)</xref> extended the EM-algorithm of <xref ref-type="bibr" rid="CR25">Laird and Ware (1982)</xref> to deal with linear bivariate mixed models. Also, for some applications, it may be possible to stack the columns of the response matrix and apply standard software for univariate mixed models (e.g., <italic>SAS Proc Mixture; S-Plus Nlme</italic>). However, this approach quickly becomes impossible when the number of individuals per group and/or the number of variables grows. The MCMC algorithm developed in this project, which is available in <italic>R</italic> from the authors upon request, may help researchers to analyze nonlinear multivariate multilevel mixed response data. This implementation is not limited to small numbers of variables or responses and can handle multiple random effects.</p>
      <p>The model in this paper can easily be extended, for example, to deal with polytomous response data. MCMC algorithms for polytomous IRT models can be found in <xref ref-type="bibr" rid="CR14">Fox (2005)</xref>, <xref ref-type="bibr" rid="CR34">Patz and Junker (1999)</xref>, and <xref ref-type="bibr" rid="CR22">Johnson and Albert (1999)</xref>, among others. The necessary adjustment of the MCMC algorithm consists of replacing the random draws from the parameters in the threeparameter normal-ogive IRT model with those in a polytomous model. Although several studies have shown the log-normal model to yield satisfactory fit to RTs on test items, the hierarchical framework can be used with other measurement models for RTs, for example, to deal with RT distributions with a different skewed or that require heavier tails to be more robust against outliers.</p>
      <p>If subpopulations of test takers follow different strategies to solve the items, differences in the joint distribution of accuracy and speed can be expected. To model them, a mixture modeling approach with different latent classes for different strategies can be used (see, for instance, <xref ref-type="bibr" rid="CR37">Rost, 1990</xref>). This procedure can also be used to relate the popularity of different strategies to covariates.</p>
      <p>Finally, the relationship between accuracy and speed may differ across groups of items, for instance, when they are organized in families of cloned items (<xref ref-type="bibr" rid="CR19">Glas &amp; van der Linden, 2003</xref>) or are presented with a testlet structure (<xref ref-type="bibr" rid="CR8">Bradlow, Wainer, &amp; Wang, 1999</xref>). In order to deal with such cases, the hierarchical framework has to be extended with a group structure for items. The consequences of this extension and the extensions above for the MCMC method of estimation and hypothesis testing still have to be explored.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Open Access</title>
      <p>This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.</p>
    </ack>
    <app-group>
      <app id="AppA">
        <title>Appendix: Gibbs Sampling Scheme</title>
        <p>The Gibbs sampler iteratively samples from the full conditional distributions of all parameters. The full conditional distributions are specified below.</p>
        <p><italic>Sampling of Structural Model Parameters</italic> The sampling of the augmented data is described in (<xref rid="Equ16" ref-type="">16</xref>).</p>
        <p>As for the person parameters, observe that (19) can be written as
<disp-formula id="Equ50"><tex-math id="M79">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\left[ {\matrix{ {{\Omega _{ij}}} \cr {{\rm{z}}_{ij}^*} \cr  } } \right] \sim N\left( {\left[ {\matrix{ {{\rm{x}}_{ij}^t} &amp; {{\beta _j}} \cr {{{\rm{H}}_P}} &amp; {\Omega _{ij}^t} \cr  } } \right],\left[ {\matrix{ {{\Sigma _P}} &amp; {{\Sigma _P}{\rm{H}}_P^t} \cr {{{\rm{H}}_P}{\Sigma _P}} &amp; {{{\rm{H}}_P}{\Sigma _P}{\rm{H}}_P^t + {\rm{I}}} \cr  } } \right]} \right)$$
\end{document}</tex-math></disp-formula>, where the matrix notation is the same as in (<xref rid="Equ32" ref-type="">32</xref>), with <bold>z</bold><sub arrange="stack"><italic>ij</italic></sub><sup arrange="stack">*</sup> = vec(<italic>z</italic><sub><italic>ij k</italic></sub> + <italic>b</italic><sub><italic>k</italic></sub>, <italic>t</italic><sub><italic>ijk</italic></sub> &#x2212; <italic>&#x3BB;</italic><sub><italic>k</italic></sub>) and <bold>H</bold><sub><italic>P</italic></sub> = (<bold>a</bold>&#x2295;&#x2212;<bold><italic>&#x3D5;</italic></bold>). From the fact that (<xref rid="Equ50" ref-type="">50</xref>) is multivariate normal, it follows for the full conditional distribution of the person parameters that
<disp-formula id="Equ51"><tex-math id="M80">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\Omega _{ij}}|{\rm{z}}_{ij}^*,{\Sigma _P},\beta \sim N(E({\Omega _{ij}}|{\rm{z}}_{ij}^*),{\mathop{\rm var}} ({\Omega _{ij}}|{\rm{z}}_{ij}^*))$$
\end{document}</tex-math></disp-formula>, with
<disp-formula id="Equ52"><tex-math id="M81">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$E({\Omega _{ij}}|{\rm{z}}_{ij}^*,{\Sigma _P},\beta ) = {\rm{x}}_{ij}^t{\beta _j} + {{\rm{H}}_P}{\Sigma _P}{({{\rm{H}}_P}{\Sigma _P}{\rm{H}}_P^t + {\rm{I}})^{ - 1}}({\rm{z}}_{ij}^* - {{\rm{H}}_P}{({\rm{x}}_{ij}^t{\beta _j})^t})$$
\end{document}</tex-math></disp-formula> and
<disp-formula id="Equ53"><tex-math id="M82">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\mathop{\rm var}} ({\Omega _{ij}}|{\rm{z}}_{ij}^*,{\Sigma _P},\beta ) = {\Sigma _P} - {\Sigma _P}{\rm{H}}_P^t{({{\rm{H}}_P}{\Sigma _P}{\rm{H}}_P^t + {\rm{I}})^{ - 1}}{{\rm{H}}_P}{\Sigma _P}$$
\end{document}</tex-math></disp-formula>.</p>
        <p>This result involves an efficient sampling scheme since the values of both person parameters are obtained in just one step.</p>
        <p>The derivation of the full conditional distribution of regression coefficients <bold><italic>&#x3B2;</italic></bold> and <bold><italic>&#x3B3;</italic></bold> is analogous. From (<xref rid="Equ50" ref-type="">50</xref>) and (<xref rid="Equ9" ref-type="">9</xref>), it follows that the <bold><italic>&#x3B2;</italic></bold><sub><italic>j</italic></sub>s are multivariate normal with mean
<disp-formula id="Equ54"><tex-math id="M83">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$E({\beta _j}|{\Omega _j},{\Sigma _P},{\rm{V}},\gamma ) = {{\rm{w}}_j}\gamma + {{\rm{x}}_j}{\rm{V}}{({{\rm{x}}_j}{\rm{Vx}}_j^t + {\Sigma _P})^{ - 1}}({\Omega _j} - {{\rm{x}}_j}{{\rm{w}}_j}\gamma )$$
\end{document}</tex-math></disp-formula>, and variance,
<disp-formula id="Equ55"><tex-math id="M84">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\mathop{\rm var}} ({\beta _j}|{\Omega _j},{\Sigma _P},{\rm{V}}) = {\rm{V}} - {\rm{Vx}}_j^t{({{\rm{x}}_j}{\rm{Vx}}_j^t + {\Sigma _P})^{ - 1}}{{\rm{x}}_j}{\rm{V}}$$
\end{document}</tex-math></disp-formula>.</p>
        <p>Likewise, from (<xref rid="Equ9" ref-type="">9</xref>) and (<xref rid="Equ26" ref-type="">26</xref>), the (fixed) coefficients <bold><italic>&#x3B3;</italic></bold> are multivariate normal distributed with mean
<disp-formula id="Equ56"><tex-math id="M85">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$E(\gamma |\beta ,{\rm{V}},{{\rm{V}}_0},{\kappa _{{V_0}}},{\gamma _0}) = {\gamma _0} + {\rm{wV}}*{({\rm{wV}}*{{\rm{w}}^t} + {\rm{V}})^{ - 1}}(\beta - {{\rm{w}}_{{\gamma _0}}})$$
\end{document}</tex-math></disp-formula>, and variance
<disp-formula id="Equ57"><tex-math id="M86">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\mathop{\rm var}} (\gamma |\beta ,{\rm{V}},{{\rm{V}}_0},{\kappa _{{V_0}}}) = {\rm{V}}* - {\rm{V}}*{{\rm{w}}^t}{({\rm{wV}}*{{\rm{w}}^t} + {\rm{V}})^{ - 1}}{\rm{wV}}*$$
\end{document}</tex-math></disp-formula>, where <inline-formula id="IEqy"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqy.gif --><tex-math id="M87">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${{\rm{V}}^*} = {\rm{V}}/{\kappa _{{V_0}}}$$
\end{document}</tex-math></inline-formula>.</p>
        <p>The full conditional distribution of covariance matrix <bold><italic>&#x3A3;</italic></bold><italic>P</italic> was already introduced in the section on the identifying prior structure.</p>
        <p><italic>Sampling of Remaining Parameters</italic> As for the item parameters, a regression structure analogous to that of the person parameters in (19) can be found. Let &#x39B;<italic>k</italic> = (<italic>a</italic><sub><italic>k</italic></sub>, <italic>b</italic><sub><italic>k</italic></sub>, <italic>&#x3D5;</italic><sub><italic>k</italic></sub>, <italic>&#x3BB;</italic><sub><italic>k</italic></sub>)<sup><italic>t</italic></sup> and <bold>H</bold><sub><italic>I</italic></sub> = (<bold><italic>&#x3B8;</italic></bold>,&#x2212;<bold>1</bold><sub><italic>N</italic></sub>) &#x2295; (&#x2212;<bold><italic>&#x3B6;</italic></bold>, <bold>1</bold><sub><italic>N</italic></sub>). The item parameters are the coefficients of the regression of <bold><italic>z</italic></bold><sub arrange="stack"><italic>k</italic></sub><sup arrange="stack">*</sup> on <bold>H</bold><sub><italic>I</italic></sub>. Combined with the prior in (<xref rid="Equ11" ref-type="">11</xref>), this observation leads to a multivariate normal posterior distribution of the item parameters with mean,
<disp-formula id="Equ58"><tex-math id="M88">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$E({\Lambda _k}|{\rm{z}}_k^*,\Omega ,{\Sigma _I}) = {\mu _I} + {{\rm{H}}_I}{\Sigma _I}{({{\rm{H}}_I}{\Sigma _I}{\rm{H}}_I^t + {\rm{I}})^{ - 1}}({\Lambda _k} - {{\rm{H}}_I}{\mu _I})$$
\end{document}</tex-math></disp-formula> and variance,
<disp-formula id="Equ59"><tex-math id="M89">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\mathop{\rm var}} ({\Lambda _k}|{\rm{z}}_k^*,\Omega ,{\Sigma _I}) = {\Sigma _I} - {\Sigma _I}{\rm{H}}_I^t{({{\rm{H}}_I}{\Sigma _I}{\rm{H}}_I^t + {\rm{I}})^{ - 1}}{{\rm{H}}_I}{\Sigma _I}$$
\end{document}</tex-math></disp-formula>.</p>
        <p>The parameters of the distribution of the item parameters follow a multivariate normal distribution; see (<xref rid="Equ11" ref-type="">11</xref>). The normal inverse-Wishart prior in (<xref rid="Equ23" ref-type="">23</xref>) and (<xref rid="Equ24" ref-type="">24</xref>) is conjugate for the multivariate normal distribution (<xref ref-type="bibr" rid="CR17">Gelman et al., 2004</xref>). The resulting posterior distribution also belongs to the normal inverse-Wishart family:
<disp-formula id="Equ60"><tex-math id="M90">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$p({\mu _I},{\Sigma _I}|\Lambda ,{\mu _{{I_0}}},{\Sigma _{{I_0}}},{\kappa _{{I_0}}},{\nu _{{I_0}}}) \sim N - {\rm&lt;Subscript&gt;v&lt;/Subscript&gt;} - {\rm{Wishart}}({\mu _*},\Lambda *,\kappa ,\nu )$$
\end{document}</tex-math></disp-formula> with parameters:
<disp-formula id="Equg"><tex-math id="M91">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\eqalign{ &amp; {\mu _*} = {{{\kappa _{{I_0}}}} \over {{\kappa _{{I_0}}} + K}}{\mu _{{I_0}}} + {K \over {{\kappa _{{I_0}}} + K}}\bar \Lambda , \cr &amp; \kappa = {\kappa _{{I_0}}} + K, \cr &amp; \nu = {\nu _{{I_0}}} + K, \cr &amp; {\Lambda ^*} = {\Sigma _{{I_0}}} + {\rm{S}} + {{{\kappa _{{I_0}}}K} \over {{\kappa _{{I_0}}} + K}}(\bar \Lambda - {\mu _{{I_0}}}){(\bar \Lambda - {\mu _{{I_0}}})^t} \cr} $$
\end{document}</tex-math></disp-formula>, where <inline-formula id="IEqz"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqz.gif --><tex-math id="M92">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\rm{S}} = \Sigma _{k = 1}^K({\Lambda _k} - \bar \Lambda ){({\Lambda _k} - \bar \Lambda )^t}$$
\end{document}</tex-math></inline-formula>.</p>
        <p>Likewise, for the fixed parameters <bold><italic>&#x3B3;</italic></bold>, an Inverse-Wishart prior was specified. So, the posterior is an Inverse-Wishart
<disp-formula id="Equ61"><tex-math id="M93">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\rm{V}}|{\gamma _0},\beta ,{\nu _{{V_0}}},{\kappa _{{V_0}}},{{\rm{V}}_0} \sim {\rm&lt;Subscript&gt;v&lt;/Subscript&gt;} - {\rm{Wishar}}{{\rm{t}}_{{\nu _V}}}({\psi ^{ - 1}})$$
\end{document}</tex-math></disp-formula> with parameters
<disp-formula id="Equh"><tex-math id="M94">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\eqalign{ &amp; {\nu _V} = {\nu _{{V_0}}} + J, \cr &amp; \psi = {{\rm{V}}_0} + S + {{{\kappa _{{V_0}}}J} \over {{\kappa _{{V_0}}} + J}}({\rm{w}}\gamma - {\rm{w}}{\gamma _0}){({\rm{w}}\gamma - {\rm{w}}{\gamma _0})^t}, \cr &amp; S = \sum\limits_{j = 1}^J {({\beta _j} - {{\rm{w}}_j}\gamma ){{({\beta _j} - {{\rm{w}}_j}\gamma )}^t}} \cr} $$
\end{document}</tex-math></disp-formula>.</p>
        <p>For the 3PL model, an additional augmentation step is introduced according to <xref ref-type="bibr" rid="CR5">Beguin and Glas (2001)</xref>. A variable <italic>s</italic><sub><italic>ijk</italic></sub> = 1 when a person <italic>ij</italic> knows the correct answer to question <italic>k</italic> and is <italic>s</italic><sub><italic>ijk</italic></sub> = 0 otherwise. Its conditional probabilities are given by (<xref rid="Equ12" ref-type="">12</xref>). Subsequently, <italic>z</italic><sub><italic>ijk</italic></sub> &#x223C; <italic>N</italic>(<italic>a</italic><sub><italic>k</italic></sub>&#x3B8;<sub><italic>ij</italic></sub> &#x2212;<italic>b</italic><sub><italic>k</italic></sub>, 1), truncated at the left of 0 when <italic>sijk</italic> = 0 and truncated at the right when <italic>s</italic><sub><italic>ijk</italic></sub> = 1.</p>
        <p>It was already noted that the posterior of the guessing parameters is a Beta distribution:
<disp-formula id="Equ62"><tex-math id="M95">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${c_k} \sim {\rm{Beta}}({{b'}_1} + {s_k},{{b'}_2} + {n_k} - {s_k})$$
\end{document}</tex-math></disp-formula>, where <italic>n</italic><sub><italic>k</italic></sub> is the number of people who do not know the answer and <italic>s</italic><sub><italic>k</italic></sub> is the number of people who guessed the answer correctly.</p>
        <p>For the residual variance of the RT model <italic>&#x3C4;</italic><sub arrange="stack"><italic>k</italic></sub><sup arrange="stack">2</sup>, with an Inverse-Gamma prior, the posterior is again an Inverse-Gamma distribution with parameter <italic>g</italic><sub>1</sub> + <italic>N</italic>/2 and scale parameter <inline-formula id="IEqaa"><!-- Alternate image not processed: 11336_2008_Article_9075_TeX2GIFIEqaa.gif --><tex-math id="M96">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$g2 + {({{\rm{t}}_k} - ( - {\phi _{k\zeta }} + {\lambda _k}))^t}({{\rm{t}}_k} - ( - {\phi _{k\zeta }} + {\lambda _k}))/2$$
\end{document}</tex-math></inline-formula>.</p>
      </app>
    </app-group>
    <ref-list id="Bib1">
      <title>References</title>
      <ref id="CR1">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Akaike</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <person-group person-group-type="editor">
            <name>
              <surname>Petrov</surname>
              <given-names>B.N.</given-names>
            </name>
            <name>
              <surname>Csaki</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Information theory and an extension of the maximum likelihood principle</article-title>
          <source>2nd international symposium on information theory</source>
          <year>1973</year>
          <publisher-loc>Budapest</publisher-loc>
          <publisher-name>Akademiai Kiado</publisher-name>
          <fpage>267</fpage>
          <lpage>281</lpage>
        </citation>
        <citation citation-type="display-unstructured">
Akaike, H. (1973). Information theory and an extension of the maximum likelihood principle. In B.N. Petrov &amp; F. Csaki (Eds.), 2nd international symposium on information theory (pp. 267&#x2013;281). Budapest: Akademiai Kiado.
 </citation>
      </ref>
      <ref id="CR2">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Albert</surname>
              <given-names>J.H.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian estimation of normal ogive item response curves using Gibbs sampling</article-title>
          <source>Journal of Educational Statistics</source>
          <year>1992</year>
          <volume>17</volume>
          <fpage>251</fpage>
          <lpage>269</lpage>
          <pub-id pub-id-type="doi">10.2307/1165149</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Albert, J.H. (1992). Bayesian estimation of normal ogive item response curves using Gibbs sampling. Journal of Educational Statistics, 17, 251&#x2013;269.
 </citation>
      </ref>
      <ref id="CR3">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Barnard</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>McCullogh</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Meng</surname>
              <given-names>X.-L.</given-names>
            </name>
          </person-group>
          <article-title>Modelling covariance matrices in terms of standard deviations and correlations, with application to shrinkage</article-title>
          <source>Statistica Sinica</source>
          <year>2000</year>
          <volume>10</volume>
          <fpage>1281</fpage>
          <lpage>1311</lpage>
        </citation>
        <citation citation-type="display-unstructured">
Barnard, J., McCullogh, R., &amp; Meng, X.-L. (2000). Modelling covariance matrices in terms of standard deviations and correlations, with application to shrinkage. Statistica Sinica, 10, 1281&#x2013;1311.
 </citation>
      </ref>
      <ref id="CR4">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Becker</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Beyond the big five</article-title>
          <source>Personality and Individual Differences</source>
          <year>1999</year>
          <volume>26</volume>
          <fpage>511</fpage>
          <lpage>30</lpage>
          <pub-id pub-id-type="doi">10.1016/S0191-8869(98)00168-8</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Becker, P. (1999). Beyond the big five. Personality and Individual Differences, 26, 511&#x2013;530.
 </citation>
      </ref>
      <ref id="CR5">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Beguin</surname>
              <given-names>A.A.</given-names>
            </name>
            <name>
              <surname>Glas</surname>
              <given-names>C.A.W.</given-names>
            </name>
          </person-group>
          <article-title>MCMC estimation and some model-fit analysis of multidimensional IRT models</article-title>
          <source>Psychometrika</source>
          <year>2001</year>
          <volume>66</volume>
          <fpage>541</fpage>
          <lpage>562</lpage>
          <pub-id pub-id-type="doi">10.1007/BF02296195</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Beguin, A.A., &amp; Glas, C.A.W. (2001). MCMC estimation and some model-fit analysis of multidimensional IRT models. Psychometrika, 66, 541&#x2013;562.
 </citation>
      </ref>
      <ref id="CR6">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Berger</surname>
              <given-names>J.O.</given-names>
            </name>
            <name>
              <surname>Delampady</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Testing precise hypotheses</article-title>
          <source>Statistical Science</source>
          <year>1987</year>
          <volume>2</volume>
          <fpage>317</fpage>
          <lpage>352</lpage>
          <pub-id pub-id-type="doi">10.1214/ss/1177013238</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Berger, J.O., &amp; Delampady, M. (1987). Testing precise hypotheses. Statistical Science, 2, 317&#x2013;352.
 </citation>
      </ref>
      <ref id="CR7">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Boscardin</surname>
              <given-names>W.J.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <person-group person-group-type="editor">
            <name>
              <surname>Gelman</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Meng</surname>
              <given-names>X.-L.</given-names>
            </name>
          </person-group>
          <article-title>Modeling the covariance and correlation matrix of repeated measures</article-title>
          <source>Applied Bayesian modeling and causal inference from incomplete-data perspectives</source>
          <year>2004</year>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>Wiley</publisher-name>
          <fpage>215</fpage>
          <lpage>226</lpage>
        </citation>
        <citation citation-type="display-unstructured">
Boscardin, W.J., &amp; Zhang, X. (2004). Modeling the covariance and correlation matrix of repeated measures. In A. Gelman &amp; X.-L. Meng (Eds.), Applied Bayesian modeling and causal inference from incomplete-data perspectives (pp. 215&#x2013;226). New York: Wiley.
 </citation>
      </ref>
      <ref id="CR8">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bradlow</surname>
              <given-names>E.T.</given-names>
            </name>
            <name>
              <surname>Wainer</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <article-title>A Bayesian random effects model for testlets</article-title>
          <source>Psychometrika</source>
          <year>1999</year>
          <volume>64</volume>
          <fpage>153</fpage>
          <lpage>168</lpage>
          <pub-id pub-id-type="doi">10.1007/BF02294533</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Bradlow, E.T., Wainer, H., &amp; Wang, X. (1999). A Bayesian random effects model for testlets. Psychometrika, 64, 153&#x2013;168.
 </citation>
      </ref>
      <ref id="CR9">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bridgeman</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Cline</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Effects of differentially time-consuming tests on computer-adaptive test scores</article-title>
          <source>Journal of Educational Measurement</source>
          <year>2004</year>
          <volume>41</volume>
          <fpage>137</fpage>
          <lpage>148</lpage>
          <pub-id pub-id-type="doi">10.1111/j.1745-3984.2004.tb01111.x</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Bridgeman, B., &amp; Cline, F. (2004). Effects of differentially time-consuming tests on computer-adaptive test scores. Journal of Educational Measurement, 41, 137&#x2013;148.
 </citation>
      </ref>
      <ref id="CR10">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Browne</surname>
              <given-names>W.J.</given-names>
            </name>
          </person-group>
          <article-title>MCMC algorithms for constrained variance matrices</article-title>
          <source>Computational Statistics &amp; Data Analysis</source>
          <year>2006</year>
          <volume>50</volume>
          <fpage>1655</fpage>
          <lpage>1677</lpage>
          <pub-id pub-id-type="doi">10.1016/j.csda.2005.02.008</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Browne, W.J. (2006). MCMC algorithms for constrained variance matrices. Computational Statistics &amp; Data Analysis, 50, 1655&#x2013;1677.
 </citation>
      </ref>
      <ref id="CR11">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chib</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Greenberg</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Analysis of multivariate probit models</article-title>
          <source>Biometrika</source>
          <year>1998</year>
          <volume>85</volume>
          <fpage>347</fpage>
          <lpage>361</lpage>
          <pub-id pub-id-type="doi">10.1093/biomet/85.2.347</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Chib, S., &amp; Greenberg, E. (1998). Analysis of multivariate probit models. Biometrika, 85, 347&#x2013;361.
 </citation>
      </ref>
      <ref id="CR12">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>DeIorio</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Robert</surname>
              <given-names>C.P.</given-names>
            </name>
          </person-group>
          <article-title>Discussion of Spiegelhalter et al</article-title>
          <source>Journal of the Royal Statistical Society, Series B</source>
          <year>2002</year>
          <volume>64</volume>
          <fpage>629</fpage>
          <lpage>630</lpage>
        </citation>
        <citation citation-type="display-unstructured">
DeIorio, M., &amp; Robert, C.P. (2002). Discussion of Spiegelhalter et al. Journal of the Royal Statistical Society, Series B, 64, 629&#x2013;630.
 </citation>
      </ref>
      <ref id="CR13">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Dickey</surname>
              <given-names>J.M.</given-names>
            </name>
          </person-group>
          <article-title>The weighted likelihood ratio, linear hypothesis on normal location parameters</article-title>
          <source>The Annals of Mathematical Statistics</source>
          <year>1971</year>
          <volume>42</volume>
          <fpage>204</fpage>
          <lpage>223</lpage>
          <pub-id pub-id-type="doi">10.1214/aoms/1177693507</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Dickey, J.M. (1971). The weighted likelihood ratio, linear hypothesis on normal location parameters. The Annals of Mathematical Statistics, 42, 204&#x2013;223.
 </citation>
      </ref>
      <ref id="CR14">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fox</surname>
              <given-names>J.-P.</given-names>
            </name>
          </person-group>
          <article-title>Multilevel IRT using dichotomous and polytomous items</article-title>
          <source>British Journal of Mathematical and Statistical Psychology</source>
          <year>2005</year>
          <volume>58</volume>
          <fpage>145</fpage>
          <lpage>172</lpage>
          <pub-id pub-id-type="doi">10.1348/000711005X38951</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Fox, J.-P. (2005). Multilevel IRT using dichotomous and polytomous items. British Journal of Mathematical and Statistical Psychology, 58, 145&#x2013;172.
 <pub-id pub-id-type="pmid">15969844</pub-id></citation>
      </ref>
      <ref id="CR15">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fox</surname>
              <given-names>J.-P.</given-names>
            </name>
            <name>
              <surname>Glas</surname>
              <given-names>C.A.W.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian estimation of a multilevel IRT model using Gibbs sampling</article-title>
          <source>Psychometrika</source>
          <year>2001</year>
          <volume>66</volume>
          <fpage>269</fpage>
          <lpage>286</lpage>
          <pub-id pub-id-type="doi">10.1007/BF02294839</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Fox, J.-P., &amp; Glas, C.A.W. (2001). Bayesian estimation of a multilevel IRT model using Gibbs sampling. Psychometrika, 66, 269&#x2013;286.
 </citation>
      </ref>
      <ref id="CR16">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gelfand</surname>
              <given-names>A.E.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>A.F.M.</given-names>
            </name>
          </person-group>
          <article-title>Sampling-based approaches to calculating marginal densities</article-title>
          <source>Journal of the American Statistical Association</source>
          <year>1990</year>
          <volume>85</volume>
          <fpage>398</fpage>
          <lpage>409</lpage>
          <pub-id pub-id-type="doi">10.2307/2289776</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Gelfand, A.E., &amp; Smith, A.F.M. (1990). Sampling-based approaches to calculating marginal densities. Journal of the American Statistical Association, 85, 398&#x2013;409.
 </citation>
      </ref>
      <ref id="CR17">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Gelman</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Carlin</surname>
              <given-names>J.B.</given-names>
            </name>
            <name>
              <surname>Stern</surname>
              <given-names>H.S.</given-names>
            </name>
            <name>
              <surname>Rubin</surname>
              <given-names>D.B.</given-names>
            </name>
          </person-group>
          <source>Bayesian data analysis</source>
          <year>2004</year>
          <edition>2</edition>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>Chapman &amp; Hall/CRC</publisher-name>
        </citation>
        <citation citation-type="display-unstructured">
Gelman, A., Carlin, J.B., Stern, H.S., &amp; Rubin, D.B. (2004). Bayesian data analysis (2nd ed.). New York: Chapman &amp; Hall/CRC.
 </citation>
      </ref>
      <ref id="CR18">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Geman</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Geman</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images</article-title>
          <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
          <year>1984</year>
          <volume>6</volume>
          <fpage>721</fpage>
          <lpage>741</lpage>
          <pub-id pub-id-type="doi">10.1109/TPAMI.1984.4767596</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Geman, S., &amp; Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6, 721&#x2013;741.
 </citation>
      </ref>
      <ref id="CR19">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Glas</surname>
              <given-names>C.A.W.</given-names>
            </name>
            <name>
              <surname>Linden</surname>
              <given-names>W.J.</given-names>
            </name>
          </person-group>
          <article-title>Computerized adaptive testing with item cloning</article-title>
          <source>Applied Psychological Measurement</source>
          <year>2003</year>
          <volume>23</volume>
          <fpage>249</fpage>
          <lpage>263</lpage>
        </citation>
        <citation citation-type="display-unstructured">
Glas, C.A.W., &amp; van&#xA0;der Linden, W.J. (2003). Computerized adaptive testing with item cloning. Applied Psychological Measurement, 23, 249&#x2013;263.
 </citation>
      </ref>
      <ref id="CR20">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Goldstein</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <source>Multilevel statistical models</source>
          <year>2003</year>
          <edition>3</edition>
          <publisher-loc>London</publisher-loc>
          <publisher-name>Arnold</publisher-name>
        </citation>
        <citation citation-type="display-unstructured">
Goldstein, H. (2003). Multilevel statistical models (3rd ed.). London: Arnold.
 </citation>
      </ref>
      <ref id="CR21">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Harville</surname>
              <given-names>D.A.</given-names>
            </name>
          </person-group>
          <article-title>Maximum likelihood approaches to variance component estimation and related problems</article-title>
          <source>Journal of the American Statistical Association</source>
          <year>1977</year>
          <volume>72</volume>
          <fpage>320</fpage>
          <lpage>338</lpage>
          <pub-id pub-id-type="doi">10.2307/2286796</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Harville, D.A. (1977). Maximum likelihood approaches to variance component estimation and related problems. Journal of the American Statistical Association, 72, 320&#x2013;338.
 </citation>
      </ref>
      <ref id="CR22">
        <citation citation-type="other">
Johnson, V.E., &amp; Albert, J.H. (1999). <italic>Ordinal data modeling</italic>.
</citation>
      </ref>
      <ref id="CR23">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kass</surname>
              <given-names>R.E.</given-names>
            </name>
            <name>
              <surname>Raftery</surname>
              <given-names>A.E.</given-names>
            </name>
          </person-group>
          <article-title>Bayes factors</article-title>
          <source>Journal of the American Statistical Association</source>
          <year>1995</year>
          <volume>90</volume>
          <fpage>773</fpage>
          <lpage>795</lpage>
          <pub-id pub-id-type="doi">10.2307/2291091</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Kass, R.E., &amp; Raftery, A.E. (1995). Bayes factors. Journal of the American Statistical Association, 90, 773&#x2013;795.
 </citation>
      </ref>
      <ref id="CR24">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kennedy</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Speed as a personality trait</article-title>
          <source>Journal of Social Psychology</source>
          <year>1930</year>
          <volume>1</volume>
          <fpage>286</fpage>
          <lpage>298</lpage>
        </citation>
        <citation citation-type="display-unstructured">
Kennedy, M. (1930). Speed as a personality trait. Journal of Social Psychology, 1, 286&#x2013;298.
 </citation>
      </ref>
      <ref id="CR25">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Laird</surname>
              <given-names>N.M.</given-names>
            </name>
            <name>
              <surname>Ware</surname>
              <given-names>J.H.</given-names>
            </name>
          </person-group>
          <article-title>Random-effects models for longitudinal data</article-title>
          <source>Biometrics</source>
          <year>1982</year>
          <volume>38</volume>
          <fpage>963</fpage>
          <lpage>974</lpage>
          <pub-id pub-id-type="doi">10.2307/2529876</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Laird, N.M., &amp; Ware, J.H. (1982). Random-effects models for longitudinal data. Biometrics, 38, 963&#x2013;974.
 <pub-id pub-id-type="pmid">7168798</pub-id></citation>
      </ref>
      <ref id="CR26">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lavine</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Schervish</surname>
              <given-names>M.J.</given-names>
            </name>
          </person-group>
          <article-title>Bayes factors: What they are and what they are not</article-title>
          <source>The American Statistician</source>
          <year>1999</year>
          <volume>53</volume>
          <fpage>119</fpage>
          <lpage>122</lpage>
          <pub-id pub-id-type="doi">10.2307/2685729</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Lavine, M., &amp; Schervish, M.J. (1999). Bayes factors: What they are and what they are not. The American Statistician, 53, 119&#x2013;122.
 </citation>
      </ref>
      <ref id="CR27">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Lee</surname>
              <given-names>P.M.</given-names>
            </name>
          </person-group>
          <source>Bayesian statistics, an introduction</source>
          <year>2004</year>
          <edition>3</edition>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>Arnold</publisher-name>
        </citation>
        <citation citation-type="display-unstructured">
Lee, P.M. (2004). Bayesian statistics, an introduction (3rd ed.). New York: Arnold.
 </citation>
      </ref>
      <ref id="CR28">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Luce</surname>
              <given-names>D.R.</given-names>
            </name>
          </person-group>
          <source>Response times: Their role in inferring elementary mental organization</source>
          <year>1986</year>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>Oxford University Press</publisher-name>
        </citation>
        <citation citation-type="display-unstructured">
Luce, D.R. (1986). Response times: Their role in inferring elementary mental organization. New York: Oxford University Press.
 </citation>
      </ref>
      <ref id="CR29">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>McCrea</surname>
              <given-names>R.R.</given-names>
            </name>
            <name>
              <surname>Costa</surname>
              <given-names>P.T.</given-names>
            </name>
          </person-group>
          <article-title>Personality trait structure as a human universal</article-title>
          <source>American Psychologist</source>
          <year>1997</year>
          <volume>52</volume>
          <fpage>509</fpage>
          <lpage>516</lpage>
          <pub-id pub-id-type="doi">10.1037/0003-066X.52.5.509</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
McCrea, R.R., &amp; Costa, P.T. (1997). Personality trait structure as a human universal. American Psychologist, 52, 509&#x2013;516.
 <pub-id pub-id-type="pmid">9145021</pub-id></citation>
      </ref>
      <ref id="CR30">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>McCulloch</surname>
              <given-names>R.E.</given-names>
            </name>
            <name>
              <surname>Rossi</surname>
              <given-names>P.E.</given-names>
            </name>
          </person-group>
          <article-title>An exact likelihood analysis of the multinomial probit model</article-title>
          <source>Journal of Econometrics</source>
          <year>1994</year>
          <volume>64</volume>
          <fpage>207</fpage>
          <lpage>240</lpage>
          <pub-id pub-id-type="doi">10.1016/0304-4076(94)90064-7</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
McCulloch, R.E., &amp; Rossi, P.E. (1994). An exact likelihood analysis of the multinomial probit model. Journal of Econometrics, 64, 207&#x2013;240.
 </citation>
      </ref>
      <ref id="CR31">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>McCulloch</surname>
              <given-names>R.E.</given-names>
            </name>
            <name>
              <surname>Polson</surname>
              <given-names>N.G.</given-names>
            </name>
            <name>
              <surname>Rossi</surname>
              <given-names>P.E.</given-names>
            </name>
          </person-group>
          <article-title>A Bayesian analysis of the multinomial probit model with fully identified parameters</article-title>
          <source>Journal of Econometrics</source>
          <year>2000</year>
          <volume>99</volume>
          <fpage>173</fpage>
          <lpage>193</lpage>
          <pub-id pub-id-type="doi">10.1016/S0304-4076(00)00034-8</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
McCulloch, R.E., Polson, N.G., &amp; Rossi, P.E. (2000). A Bayesian analysis of the multinomial probit model with fully identified parameters. Journal of Econometrics, 99, 173&#x2013;193.
 </citation>
      </ref>
      <ref id="CR32">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mellenbergh</surname>
              <given-names>G.J.</given-names>
            </name>
          </person-group>
          <article-title>A unidimensional latent trait model for continuous item responses</article-title>
          <source>Multivariate Behavioral Research</source>
          <year>1994</year>
          <volume>29</volume>
          <fpage>223</fpage>
          <lpage>236</lpage>
          <pub-id pub-id-type="doi">10.1207/s15327906mbr2903_2</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Mellenbergh, G.J. (1994). A unidimensional latent trait model for continuous item responses. Multivariate Behavioral Research, 29, 223&#x2013;236.
 </citation>
      </ref>
      <ref id="CR33">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Newton</surname>
              <given-names>M.A.</given-names>
            </name>
            <name>
              <surname>Raftery</surname>
              <given-names>A.E.</given-names>
            </name>
          </person-group>
          <article-title>Approximate Bayesian inference with the weighted likelhood bootstrap</article-title>
          <source>Journal of the Royal Statistical Society Series B</source>
          <year>1994</year>
          <volume>56</volume>
          <fpage>3</fpage>
          <lpage>58</lpage>
        </citation>
        <citation citation-type="display-unstructured">
Newton, M.A., &amp; Raftery, A.E. (1994). Approximate Bayesian inference with the weighted likelhood bootstrap. Journal of the Royal Statistical Society Series B, 56, 3&#x2013;58.
 </citation>
      </ref>
      <ref id="CR34">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Patz</surname>
              <given-names>R.J.</given-names>
            </name>
            <name>
              <surname>Junker</surname>
              <given-names>B.W.</given-names>
            </name>
          </person-group>
          <article-title>A straightforward approach to Markov chain Monte Carlo methods for item response models</article-title>
          <source>Journal of Educational and Behavioral Statistics</source>
          <year>1999</year>
          <volume>24</volume>
          <fpage>146</fpage>
          <lpage>178</lpage>
        </citation>
        <citation citation-type="display-unstructured">
Patz, R.J., &amp; Junker, B.W. (1999). A straightforward approach to Markov chain Monte Carlo methods for item response models. Journal of Educational and Behavioral Statistics, 24, 146&#x2013;178.
 </citation>
      </ref>
      <ref id="CR35">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rabe-Hesketh</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Skrondal</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Parameterization of multivariate random effects models for categorical data</article-title>
          <source>Biometrics</source>
          <year>2001</year>
          <volume>57</volume>
          <fpage>1256</fpage>
          <lpage>1264</lpage>
          <pub-id pub-id-type="doi">10.1111/j.0006-341X.2001.1256_1.x</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Rabe-Hesketh, S., &amp; Skrondal, A. (2001). Parameterization of multivariate random effects models for categorical data. Biometrics, 57, 1256&#x2013;1264.
 <pub-id pub-id-type="pmid">11764269</pub-id></citation>
      </ref>
      <ref id="CR36">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Reinsel</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Some results on multivariate autoregressive index models</article-title>
          <source>Biometrika</source>
          <year>1983</year>
          <volume>70</volume>
          <fpage>145</fpage>
          <lpage>156</lpage>
          <pub-id pub-id-type="doi">10.1093/biomet/70.1.145</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Reinsel, G. (1983). Some results on multivariate autoregressive index models. Biometrika, 70, 145&#x2013;156.
 </citation>
      </ref>
      <ref id="CR37">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rost</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Rasch models in latent classes: An integration of two approaches to item analysis</article-title>
          <source>Applied Psychological Measurement</source>
          <year>1990</year>
          <volume>14</volume>
          <fpage>271</fpage>
          <lpage>282</lpage>
          <pub-id pub-id-type="doi">10.1177/014662169001400305</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Rost, J. (1990). Rasch models in latent classes: An integration of two approaches to item analysis. Applied Psychological Measurement, 14, 271&#x2013;282.
 </citation>
      </ref>
      <ref id="CR38">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schafer</surname>
              <given-names>J.L.</given-names>
            </name>
            <name>
              <surname>Yucel</surname>
              <given-names>R.C.</given-names>
            </name>
          </person-group>
          <article-title>Computational strategies for multivariate linear mixed-effects models with missing values</article-title>
          <source>Journal of Computational and Graphical Statistics</source>
          <year>2002</year>
          <volume>11</volume>
          <fpage>437</fpage>
          <lpage>457</lpage>
          <pub-id pub-id-type="doi">10.1198/106186002760180608</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Schafer, J.L., &amp; Yucel, R.C. (2002). Computational strategies for multivariate linear mixed-effects models with missing values. Journal of Computational and Graphical Statistics, 11, 437&#x2013;457.
 </citation>
      </ref>
      <ref id="CR39">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schnipke</surname>
              <given-names>D.L.</given-names>
            </name>
            <name>
              <surname>Scrams</surname>
              <given-names>D.J.</given-names>
            </name>
          </person-group>
          <article-title>Modeling item response times with a two-state mixture model: A new method for measuring speededness</article-title>
          <source>Journal of Educational Measurement</source>
          <year>1997</year>
          <volume>34</volume>
          <fpage>213</fpage>
          <lpage>232</lpage>
          <pub-id pub-id-type="doi">10.1111/j.1745-3984.1997.tb00516.x</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Schnipke, D.L., &amp; Scrams, D.J. (1997). Modeling item response times with a two-state mixture model: A new method for measuring speededness. Journal of Educational Measurement, 34, 213&#x2013;232.
 </citation>
      </ref>
      <ref id="CR40">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Schnipke</surname>
              <given-names>D.L.</given-names>
            </name>
            <name>
              <surname>Scrams</surname>
              <given-names>D.J.</given-names>
            </name>
          </person-group>
          <person-group person-group-type="editor">
            <name>
              <surname>Mills</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Fremer</surname>
              <given-names>M.P.J.</given-names>
            </name>
            <name>
              <surname>Ward</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>Exploring issues of examinee behavior: Insights gained from response-time analyses</article-title>
          <source>Computer-based testing: Building the foundation for future assessments</source>
          <year>2002</year>
          <publisher-loc>Mahwah</publisher-loc>
          <publisher-name>Lawrence Erlbaum Associates</publisher-name>
          <fpage>237</fpage>
          <lpage>266</lpage>
        </citation>
        <citation citation-type="display-unstructured">
Schnipke, D.L., &amp; Scrams, D.J. (2002). Exploring issues of examinee behavior: Insights gained from response-time analyses. In C. Mills, M.P.J. Fremer, &amp; W. Ward (Eds.), Computer-based testing: Building the foundation for future assessments (pp. 237&#x2013;266). Mahwah: Lawrence Erlbaum Associates.
 </citation>
      </ref>
      <ref id="CR41">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schwarz</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Estimating the dimension of a model</article-title>
          <source>The Annals of Statistics</source>
          <year>1978</year>
          <volume>6</volume>
          <fpage>461</fpage>
          <lpage>464</lpage>
          <pub-id pub-id-type="doi">10.1214/aos/1176344136</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics, 6, 461&#x2013;464.
 </citation>
      </ref>
      <ref id="CR42">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Searle</surname>
              <given-names>S.R.</given-names>
            </name>
            <name>
              <surname>Casella</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>McCulloch</surname>
              <given-names>C.E.</given-names>
            </name>
          </person-group>
          <source>Variance components</source>
          <year>1992</year>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>Wiley</publisher-name>
        </citation>
        <citation citation-type="display-unstructured">
Searle, S.R., Casella, G., &amp; McCulloch, C.E. (1992). Variance components. New York: Wiley.
 </citation>
      </ref>
      <ref id="CR43">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Shah</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Laird</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Schoenfeld</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Random-effects model for multiple characteristics with possibly missing data</article-title>
          <source>Journal of the American Statistical Association</source>
          <year>1997</year>
          <volume>92</volume>
          <fpage>775</fpage>
          <lpage>779</lpage>
          <pub-id pub-id-type="doi">10.2307/2965726</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Shah, A., Laird, N., &amp; Schoenfeld, D. (1997). Random-effects model for multiple characteristics with possibly missing data. Journal of the American Statistical Association, 92, 775&#x2013;779.
 </citation>
      </ref>
      <ref id="CR44">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Shi</surname>
              <given-names>J.Q.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>S.Y.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian sampling based approach for factor analysis models with continuous and polytomous data</article-title>
          <source>British Journal of Mathematical and Statistical Psychology</source>
          <year>1998</year>
          <volume>51</volume>
          <fpage>233</fpage>
          <lpage>252</lpage>
        </citation>
        <citation citation-type="display-unstructured">
Shi, J.Q., &amp; Lee, S.Y. (1998). Bayesian sampling based approach for factor analysis models with continuous and polytomous data. British Journal of Mathematical and Statistical Psychology, 51, 233&#x2013;252.
 </citation>
      </ref>
      <ref id="CR45">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sinharay</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Assessing fit of unidimensional item response theory models using a Bayesian approach</article-title>
          <source>Journal of Educational Measurement</source>
          <year>2005</year>
          <volume>42</volume>
          <fpage>375</fpage>
          <lpage>394</lpage>
          <pub-id pub-id-type="doi">10.1111/j.1745-3984.2005.00021.x</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Sinharay, S. (2005). Assessing fit of unidimensional item response theory models using a Bayesian approach. Journal of Educational Measurement, 42, 375&#x2013;394.
 </citation>
      </ref>
      <ref id="CR46">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sinharay</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Stern</surname>
              <given-names>H.S.</given-names>
            </name>
          </person-group>
          <article-title>On the sensitivity of Bayes factors to the prior distributions</article-title>
          <source>The American Statistician</source>
          <year>2002</year>
          <volume>56</volume>
          <fpage>196</fpage>
          <lpage>201</lpage>
          <pub-id pub-id-type="doi">10.1198/000313002137</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Sinharay, S., &amp; Stern, H.S. (2002). On the sensitivity of Bayes factors to the prior distributions. The American Statistician, 56, 196&#x2013;201.
 </citation>
      </ref>
      <ref id="CR47">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sinharay</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Johnson</surname>
              <given-names>M.S.</given-names>
            </name>
            <name>
              <surname>Stern</surname>
              <given-names>H.S.</given-names>
            </name>
          </person-group>
          <article-title>Posterior predictive assessment of item response theory models</article-title>
          <source>Applied Psychological Measurement</source>
          <year>2006</year>
          <volume>30</volume>
          <fpage>298</fpage>
          <lpage>321</lpage>
          <pub-id pub-id-type="doi">10.1177/0146621605285517</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Sinharay, S., Johnson, M.S., &amp; Stern, H.S. (2006). Posterior predictive assessment of item response theory models. Applied Psychological Measurement, 30, 298&#x2013;321.
 </citation>
      </ref>
      <ref id="CR48">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Snijders</surname>
              <given-names>T.A.B.</given-names>
            </name>
            <name>
              <surname>Bosker</surname>
              <given-names>R.J.</given-names>
            </name>
          </person-group>
          <source>Multilevel analysis, an introduction to basic and advanced multilevel modeling</source>
          <year>1999</year>
          <publisher-loc>London</publisher-loc>
          <publisher-name>Sage Publishers</publisher-name>
        </citation>
        <citation citation-type="display-unstructured">
Snijders, T.A.B., &amp; Bosker, R.J. (1999). Multilevel analysis, an introduction to basic and advanced multilevel modeling. London: Sage Publishers.
 </citation>
      </ref>
      <ref id="CR49">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Spiegelhalter</surname>
              <given-names>D.J.</given-names>
            </name>
            <name>
              <surname>Best</surname>
              <given-names>N.G.</given-names>
            </name>
            <name>
              <surname>Carlin</surname>
              <given-names>B.P.</given-names>
            </name>
            <name>
              <surname>Linde</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian measures of model complexity and fit</article-title>
          <source>Journal of the Royal Statistical Society Series B</source>
          <year>2002</year>
          <volume>64</volume>
          <fpage>583</fpage>
          <lpage>639</lpage>
          <pub-id pub-id-type="doi">10.1111/1467-9868.00353</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Spiegelhalter, D.J., Best, N.G., Carlin, B.P., &amp; van&#xA0;der Linde, A. (2002). Bayesian measures of model complexity and fit. Journal of the Royal Statistical Society Series B, 64, 583&#x2013;639.
 </citation>
      </ref>
      <ref id="CR50">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tate</surname>
              <given-names>M.W.</given-names>
            </name>
          </person-group>
          <article-title>Individual differences in speed of response in mental test materials of varying degrees of difficulty</article-title>
          <source>Educational and Psychological Measurement</source>
          <year>1948</year>
          <volume>8</volume>
          <fpage>353</fpage>
          <lpage>374</lpage>
        </citation>
        <citation citation-type="display-unstructured">
Tate, M.W. (1948). Individual differences in speed of response in mental test materials of varying degrees of difficulty. Educational and Psychological Measurement, 8, 353&#x2013;374.
 <pub-id pub-id-type="pmid">18885688</pub-id></citation>
      </ref>
      <ref id="CR51">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Thissen</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <person-group person-group-type="editor">
            <name>
              <surname>Weiss</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Timed testing: An approach using item response theory</article-title>
          <source>New horizons in testing: Latent trait test theory and computerized adaptive testing</source>
          <year>1983</year>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>Academic Press</publisher-name>
          <fpage>179</fpage>
          <lpage>203</lpage>
        </citation>
        <citation citation-type="display-unstructured">
Thissen, D. (1983). Timed testing: An approach using item response theory. In D. Weiss (Ed.), New horizons in testing: Latent trait test theory and computerized adaptive testing (pp. 179&#x2013;203). New York: Academic Press.
 </citation>
      </ref>
      <ref id="CR52">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Vaida</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Blanchard</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Conditional Akaike information for mixed-effects models</article-title>
          <source>Biometrika</source>
          <year>2005</year>
          <volume>92</volume>
          <fpage>351</fpage>
          <lpage>370</lpage>
          <pub-id pub-id-type="doi">10.1093/biomet/92.2.351</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Vaida, F., &amp; Blanchard, S. (2005). Conditional Akaike information for mixed-effects models. Biometrika, 92, 351&#x2013;370.
 </citation>
      </ref>
      <ref id="CR53">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Linden</surname>
              <given-names>W.J.</given-names>
            </name>
          </person-group>
          <article-title>A lognormal model for response times on test items</article-title>
          <source>Journal of Educational and Behavioural Statistics</source>
          <year>2006</year>
          <volume>31</volume>
          <fpage>181</fpage>
          <lpage>204</lpage>
          <pub-id pub-id-type="doi">10.3102/10769986031002181</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
van der Linden, W.J. (2006). A lognormal model for response times on test items. Journal of Educational and Behavioural Statistics, 31, 181&#x2013;204.
 </citation>
      </ref>
      <ref id="CR54">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Linden</surname>
              <given-names>W.J.</given-names>
            </name>
          </person-group>
          <article-title>Using response times for item selection in adaptive testing</article-title>
          <source>Journal of Educational and Behavioral Statistics</source>
          <year>2007</year>
        </citation>
        <citation citation-type="display-unstructured">
van der Linden, W.J. (2007). Using response times for item selection in adaptive testing. Journal of Educational and Behavioral Statistics doi:10.3102/1076998607302626 </citation>
      </ref>
      <ref id="CR55">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Linden</surname>
              <given-names>W.J.</given-names>
            </name>
          </person-group>
          <article-title>A hierarchical framework for modeling speed and accuracy on test items</article-title>
          <source>Psychometrika</source>
          <year>2008</year>
          <volume>72</volume>
          <fpage>287</fpage>
          <lpage>308</lpage>
          <pub-id pub-id-type="doi">10.1007/s11336-006-1478-z</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
van der Linden, W.J. (2008). A hierarchical framework for modeling speed and accuracy on test items. Psychometrika, 72, 287&#x2013;308.
 </citation>
      </ref>
      <ref id="CR56">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Linden</surname>
              <given-names>W.J.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Bayesian procedures for identifying aberrant response-time patterns in adaptive testing</article-title>
          <source>Psychometrika</source>
          <year>2008</year>
        </citation>
        <citation citation-type="display-unstructured">
van der Linden, W.J., &amp; Guo, F. (2008). Bayesian procedures for identifying aberrant response-time patterns in adaptive testing. Psychometrika doi:10.1007/S11336-007-9046-8 </citation>
      </ref>
      <ref id="CR57">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Linden</surname>
              <given-names>W.J.</given-names>
            </name>
            <name>
              <surname>Breithaupt</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Chuah</surname>
              <given-names>S.C.</given-names>
            </name>
            <name>
              <surname>Zang</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Detecting differential speededness in multistage testing</article-title>
          <source>Journal of Educational Measurement</source>
          <year>2007</year>
          <volume>44</volume>
          <fpage>117</fpage>
          <lpage>130</lpage>
          <pub-id pub-id-type="doi">10.1111/j.1745-3984.2007.00030.x</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
van der Linden, W.J., Breithaupt, K., Chuah, S.C., &amp; Zang, Y. (2007). Detecting differential speededness in multistage testing. Journal of Educational Measurement, 44, 117&#x2013;130.
 </citation>
      </ref>
      <ref id="CR58">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Verdinelli</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Wasserman</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Computing Bayes factors using a generalization of the Savage&#x2013;Dickey density ratio</article-title>
          <source>Journal of the American Statistical Association</source>
          <year>1995</year>
          <volume>90</volume>
          <fpage>614</fpage>
          <lpage>618</lpage>
          <pub-id pub-id-type="doi">10.2307/2291073</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Verdinelli, I., &amp; Wasserman, L. (1995). Computing Bayes factors using a generalization of the Savage&#x2013;Dickey density ratio. Journal of the American Statistical Association, 90, 614&#x2013;618.
 </citation>
      </ref>
      <ref id="CR59">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Verhelst</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Verstralen</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Jansen</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <person-group person-group-type="editor">
            <name>
              <surname>Linden</surname>
              <given-names>W.J.</given-names>
            </name>
            <name>
              <surname>Hambleton</surname>
              <given-names>R.K.</given-names>
            </name>
          </person-group>
          <article-title>Models for time-limit tests</article-title>
          <source>Handbook of modern item response theory</source>
          <year>1997</year>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>Springer</publisher-name>
          <fpage>169</fpage>
          <lpage>185</lpage>
        </citation>
        <citation citation-type="display-unstructured">
Verhelst, N., Verstralen, H., &amp; Jansen, M. (1997). Models for time-limit tests. In W.J. van der Linden &amp; R.K. Hambleton (Eds.), Handbook of modern item response theory (pp. 169&#x2013;185). New York: Springer.
 </citation>
      </ref>
      <ref id="CR60">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wise</surname>
              <given-names>S.L.</given-names>
            </name>
            <name>
              <surname>Kong</surname>
              <given-names>X.J.</given-names>
            </name>
          </person-group>
          <article-title>Response time effort: A new measure of examinee motivation in computer-based tests</article-title>
          <source>Applied Measurement in Education</source>
          <year>2005</year>
          <volume>18</volume>
          <fpage>163</fpage>
          <lpage>183</lpage>
          <pub-id pub-id-type="doi">10.1207/s15324818ame1802_2</pub-id>
        </citation>
        <citation citation-type="display-unstructured">
Wise, S.L., &amp; Kong, X.J. (2005). Response time effort: A new measure of examinee motivation in computer-based tests. Applied Measurement in Education, 18, 163&#x2013;183.
 </citation>
      </ref>
      <ref id="CR61">
        <citation citation-type="other">
Wise, S.L., Kong, X.J., &amp; Pastor, D.A. (2007). Understanding correlates of rapid-guessing behavior in low-stakes testing: Implications for test development and measurement practice. Paper presented at the 2007 <italic>anual meeting of the National Council on Measurement in Education</italic>, Chicago, IL.
</citation>
      </ref>
    </ref-list>
    <fn-group>
      <fn>
        <p>The authors thank Steven Wise, James Madison University, and Pere Joan Ferrando, Universitat Rovira i Virgili, for generously making available their data sets for the empirical examples in this paper.</p>
      </fn>
    </fn-group>
  </back>
</article>
