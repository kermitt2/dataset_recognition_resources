<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v3.0 20080202//EN" "archivearticle3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
  <?properties open_access?>
  <?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
  <?DTDIdentifier.IdentifierType public?>
  <?SourceDTD.DTDName A++V2.4.dtd?>
  <?SourceDTD.Version 2.4?>
  <?ConverterInfo.XSLTName springer2nlmx2.xsl?>
  <?ConverterInfo.Version 2?>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Multimed Tools Appl</journal-id>
      <journal-title-group>
        <journal-title>Multimedia Tools and Applications</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1380-7501</issn>
      <issn pub-type="epub">1573-7721</issn>
      <publisher>
        <publisher-name>Springer US</publisher-name>
        <publisher-loc>Boston</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">3066164</article-id>
      <article-id pub-id-type="pmid">21461317</article-id>
      <article-id pub-id-type="publisher-id">638</article-id>
      <article-id pub-id-type="doi">10.1007/s11042-010-0638-4</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>SHIRAZ: an automated histology image annotation system for zebrafish phenomics</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" corresp="yes">
          <name>
            <surname>Canada</surname>
            <given-names>Brian A.</given-names>
          </name>
          <address>
            <email>brian.canada@gmail.com</email>
          </address>
          <xref ref-type="aff" rid="Aff1">1</xref>
          <bio>
            <p><bold>Brian A. Canada</bold> earned his BS degree (with Highest Distinction) in Chemical Engineering from The Pennsylvania State University in 1999. After working as an engineer in industry, he returned to Penn State for graduate school in 2004, where he was named an Academic Computing Fellow from 2006&#x2013;2009. He will complete his PhD in Integrative Biosciences with an option in Bioinformatics and Genomics as well as a graduate minor in Computational Sciences in December 2010, and will be starting as an Assistant Professor of Computational Science at the University of South Carolina-Beaufort in January 2011.
<graphic position="anchor" xlink:href="11042_2010_638_Figa_HTML" id="d28e167"/></p>
          </bio>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Thomas</surname>
            <given-names>Georgia K.</given-names>
          </name>
          <address>
            <email>gkt612@gmail.com</email>
          </address>
          <xref ref-type="aff" rid="Aff2">2</xref>
          <bio>
            <p><bold>Georgia K. Thomas</bold> is a 2003 graduate of Carnegie Mellon University&#x2019;s program in Biological Sciences with a minor in Biomedical Engineering. She completed her PhD in Cell and Molecular Biology at the Penn State College of Medicine in 2009, and is presently in her second year of medical school at the State University of New York (SUNY) Upstate Medical College in Syracuse, NY.
<graphic position="anchor" xlink:href="11042_2010_638_Figb_HTML" id="d28e187"/></p>
          </bio>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Cheng</surname>
            <given-names>Keith C.</given-names>
          </name>
          <address>
            <email>kcheng76@gmail.com</email>
          </address>
          <xref ref-type="aff" rid="Aff3">3</xref>
          <bio>
            <p><bold>Keith C. Cheng</bold> is presently Chief of Experimental Pathology as well as Professor of Pathology, of Biochemistry &amp; Molecular Biology, and of Pharmacology in the Jake Gittlen Cancer Research Foundation at the Penn State College of Medicine. He received a Bachelor&#x2019;s degree in Biochemical Sciences from Harvard and an M.D. from New York University School of Medicine. He trained in Anatomic Pathology at Brigham &amp; Women&#x2019;s Hospital in Boston and University of Washington, and while engaged in the latter, earned a Ph.D. in phage genetics at the Fred Hutchinson Cancer Research Institute and the University of Washington in Seattle. During his postdoctoral training at the University of Washington, Dr. Cheng studied mechanisms of mutation.
<graphic position="anchor" xlink:href="11042_2010_638_Figc_HTML" id="d28e207"/></p>
          </bio>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>James Z.</given-names>
          </name>
          <address>
            <email>jwang@ist.psu.edu</email>
          </address>
          <xref ref-type="aff" rid="Aff4">4</xref>
          <bio>
            <p><bold>James Z. Wang</bold> received the BS degree (summa cum laude) in Mathematics and Computer Science from the University of Minnesota, Twin Cities, in 1994, the MSc degree in Mathematics and the MSc degree in Computer Science from Stanford University, Stanford, California, in 1997, and the PhD degree in Medical Information Sciences from the Stanford University Biomedical Informatics Program and Computer Science Database Group in 2000. He is currently Professor of Information Sciences and Technology at The Pennsylvania State University, University Park. He was the holder of the endowed PNC Technologies Career Development Professorship from 2000 to 2006, a recipient of a US National Science Foundation Career award in 2004, a Visiting Professor of the Robotics Institute at Carnegie Mellon University from 2007 to 2008, and the lead guest editor for IEEE Transactions on Pattern Analysis and Machine Intelligence Special Issue on Real-World Image Annotation and Retrieval in 2008. He is a senior member of the IEEE and the ACM.
<graphic position="anchor" xlink:href="11042_2010_638_Figd_HTML" id="d28e227"/></p>
          </bio>
        </contrib>
        <aff id="Aff1"><label>1</label>Department of Science and Mathematics, University of South Carolina, Beaufort, SC USA </aff>
        <aff id="Aff2"><label>2</label>SUNY Upstate Medical University, Syracuse, NY USA </aff>
        <aff id="Aff3"><label>3</label>Penn State College of Medicine, Hershey, PA USA </aff>
        <aff id="Aff4"><label>4</label>College of Information Sciences &amp; Technology, The Pennsylvania State University, University Park, PA USA </aff>
      </contrib-group>
      <pub-date pub-type="epub">
        <day>24</day>
        <month>10</month>
        <year>2010</year>
      </pub-date>
      <pub-date pub-type="pmc-release">
        <day>24</day>
        <month>10</month>
        <year>2010</year>
      </pub-date>
      <pub-date pub-type="ppub">
        <month>1</month>
        <year>2011</year>
      </pub-date>
      <volume>51</volume>
      <issue>2</issue>
      <fpage>401</fpage>
      <lpage>440</lpage>
      <permissions>
        <copyright-statement>&#xA9; The Author(s) 2010</copyright-statement>
      </permissions>
      <abstract id="Abs1">
        <p>Histological characterization is used in clinical and research contexts as a highly sensitive method for detecting the morphological features of disease and abnormal gene function. Histology has recently been accepted as a phenotyping method for the forthcoming Zebrafish Phenome Project, a large-scale community effort to characterize the morphological, physiological, and behavioral phenotypes resulting from the mutations in all known genes in the zebrafish genome. In support of this project, we present a novel content-based image retrieval system for the automated annotation of images containing histological abnormalities in the developing eye of the larval zebrafish.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Automatic image annotation</kwd>
        <kwd>High-throughput phenotyping</kwd>
        <kwd>Information-based similarity metrics</kwd>
        <kwd>Computational symmetry</kwd>
      </kwd-group>
      <custom-meta-group>
        <custom-meta>
          <meta-name>issue-copyright-statement</meta-name>
          <meta-value>&#xA9; Springer Science+Business Media, LLC 2011</meta-value>
        </custom-meta>
      </custom-meta-group>
    </article-meta>
  </front>
  <body>
    <sec id="Sec1">
      <title>Introduction</title>
      <p>The function of uncharacterized human genes can be elucidated by studying phenotypes associated with deficiencies in homologous genes in model organisms such as the mouse, fruit fly, and zebrafish. The zebrafish has proven to be an excellent model organism for studying vertebrate development and human disease for a number of reasons. Its transparent, readily accessible embryo develops ex vivo (outside the mother&#x2019;s body), and most organ systems are well differentiated by seven days post-fertilization&#xA0;[<xref ref-type="bibr" rid="CR27">27</xref>], which allows mutant phenotypes (i.e., observable traits) to be readily identified in a relatively short amount of time compared with other vertebrate model systems, such as the rat and mouse. In addition, its millimeter-length scale allows whole-body phenotyping at cell resolutions, which is unique among well-characterized vertebrate model systems (Cheng lab, unpublished).</p>
      <p>The emerging field of phenomics, or more specifically, <italic>high-throughput phenomics</italic>&#xA0;[<xref ref-type="bibr" rid="CR23">23</xref>], addresses the problem of collecting, integrating, and mining phenotypic data from genetic manipulation experiments across animal models. The <italic>Zebrafish Phenome Project</italic>&#xA0;[<xref ref-type="bibr" rid="CR42">42</xref>], currently in the planning stages, has a goal of functionally annotating the zebrafish genome, which presently is believed to encompass at least 20,000&#x2013;25,000 genes&#xA0;[<xref ref-type="bibr" rid="CR41">41</xref>], by systematically phenotyping mutants for each of these genes. One of the key phenotyping areas proposed by researchers participating in the Zebrafish Phenome Project is that of early development&#x2014;that is, from 0 to 8 days post-fertilization (dpf). Recognizing that conventional visual morphological defect detection by gross observation using stereo-microscopy has limited sensitivity for phenotype detection at the cellular level, Zebrafish Phenome Project researchers have reached a consensus (through community meetings) that high-resolution histological approaches should be used to characterize subtle cellular and tissue-level defects that are only apparent at sub-micron resolution (see Figs.&#xA0;<xref rid="Fig1" ref-type="fig">1</xref> and <xref rid="Fig2" ref-type="fig">2</xref>, respectively, for examples of gross versus histological images of both normal and abnormal zebrafish larvae).
<fig id="Fig1"><label>Fig.&#xA0;1</label><caption><p>Example of gross images of wild-type (normal) and mutant zebrafish at age 5 dpf (days post fertilization). Image source: <ext-link ext-link-type="uri" xlink:href="http://web.mit.edu/hopkins/images/full/1532B-1.jpg">http://web.mit.edu/hopkins/images/full/1532B-1.jpg</ext-link></p></caption><graphic xlink:href="11042_2010_638_Fig1_HTML" id="d28e336"/></fig><fig id="Fig2"><label>Fig.&#xA0;2</label><caption><p>Example of histological images of wild-type (normal) and mutant zebrafish at age 7 dpf. Images taken from Penn State Zebrafish Atlas&#xA0;[<xref ref-type="bibr" rid="CR28">28</xref>]; direct link to virtual slide comparison tool at <ext-link ext-link-type="uri" xlink:href="http://zfatlas.psu.edu/comparison.php?s[]=264,1,382,172&amp;s[]=265,1,474,386">http://zfatlas.psu.edu/comparison.php?s[]=264,1,382,172&amp;s[]=265,1,474,386</ext-link></p></caption><graphic xlink:href="11042_2010_638_Fig2_HTML" id="d28e352"/></fig></p>
      <p>Compared to gross phenotypic characterization using stereomicroscopy, the microscopic analysis of histological samples is more expensive in terms of time and effort. While recent technological advancements&#xA0;[<xref ref-type="bibr" rid="CR32">32</xref>, <xref ref-type="bibr" rid="CR36">36</xref>] have enabled the largely automatic collection of high-throughput histological data in the form of libraries of high-resolution &#x201C;virtual slides&#x201D; of zebrafish larval histology&#xA0;[<xref ref-type="bibr" rid="CR28">28</xref>], the annotation and scoring of histological data has remained a manual, subjective, and relatively low-throughput process. While useful clinically, the qualitative aspects of current histological assessments can be associated with intra- and inter-observer variability&#xA0;[<xref ref-type="bibr" rid="CR5">5</xref>] due to variations in observer training, ability, timing, experience, and alertness. Therefore, in order to maximize the effectiveness of histological studies as applied to animal model systems, it is critical that some form of automatic, quantitative method be developed for the analysis of histology images.</p>
      <p>We have been actively researching methods in content-based image retrieval (CBIR) and annotation with the goal of developing a fully automated system that is not only capable of scoring defects in zebrafish histology, but is also compatible with the high-throughput histology workflow that we have previously proposed&#xA0;[<xref ref-type="bibr" rid="CR32">32</xref>] (see Fig.&#xA0;<xref rid="Fig3" ref-type="fig">3</xref>). In 2007, we developed the first system&#xA0;[<xref ref-type="bibr" rid="CR2">2</xref>] for automated segmentation, feature extraction, and classification of zebrafish histological abnormalities, with a pilot study focusing specifically on the larval-stage eye and gut organs, which were chosen because of their inherent polarity and &#x201C;directional&#x201D; organization that, when disrupted, results in mutant phenotypes that are relatively easy for human experts to detect (see Fig.&#xA0;<xref rid="Fig4" ref-type="fig">4</xref> for examples of normal versus mutant eye histology). Our prototype system proved to be a successful proof-of-concept, demonstrating that even conventional approaches to image segmentation and classification had the potential not only to distinguish normal (or &#x201C;wild-type&#x201D;) tissue from abnormal, but also to classify the abnormality according to different levels of severity, yielding a &#x201C;semi-quantitative&#x201D; result.
<fig id="Fig3"><label>Fig.&#xA0;3</label><caption><p>An overview of the high-throughput histology workflow (modified from Sabaliauskas et&#xA0;al. [<xref ref-type="bibr" rid="CR32">32</xref>])</p></caption><graphic xlink:href="11042_2010_638_Fig3_HTML" id="d28e392"/></fig><fig id="Fig4"><label>Fig.&#xA0;4</label><caption><p>Example of histological images of wild-type (normal) and mutant zebrafish eyes at age 5 dpf (days post fertilization)</p></caption><graphic xlink:href="11042_2010_638_Fig4_HTML" id="d28e401"/></fig></p>
      <p>While this prototype system was comparatively slow and not necessarily designed for high-throughput or high-resolution use, it nonetheless provided the foundation upon which we have developed the present work, which we call SHIRAZ (System of Histological Image Retrieval and Annotation for Zoomorphology). Presently, SHIRAZ is designed to process high-resolution images of larval zebrafish arrays directly at 20&#xD7; magnification (presently corresponding to approximately 0.5 &#x3BC;m per pixel). The current prototype system enables the automatic extraction of individual specimens from the array (comprised of histological sections of up to 50 larval specimens, all on a single slide), followed by extraction of the eyes, which are then automatically annotated according to the presence of histological abnormalities. SHIRAZ has the potential to discriminate between different types of abnormalities, whether they be morphological phenotypes resulting from genetic mutations or tissue artifacts that may have arisen during the high-throughput histology workflow. The work described herein thus makes the following contributions to the biological and multimedia retrieval research communities:
<list list-type="order"><list-item><p>The first content-based image retrieval system designed to rapidly annotate both histological phenotypes and potentially confounding artifacts;</p></list-item><list-item><p>A novel similarity scheme for multi-label classification, based on a combination of information-based and probabilistic approaches; and</p></list-item><list-item><p>A proof-of-principle demonstration of a prototype system enabling greater phenotyping efficiency to benefit large-scale phenotyping projects, such as the Zebrafish Phenome Project described above.</p></list-item></list></p>
      <p>Before describing in detail the technical challenges and our proposed solutions, however, it is prudent to briefly review here the CBIR field and its relevance to the biological community.</p>
    </sec>
    <sec id="Sec2">
      <title>Related work</title>
      <p>Conventional image retrieval methods in use today, such as Yahoo! Image Search [<xref ref-type="bibr" rid="CR40">40</xref>], are often described as description- or text-based image retrieval systems [<xref ref-type="bibr" rid="CR24">24</xref>]. In such systems, images in a database must be annotated manually with a series of keywords (&#x201C;metadata&#x201D;) so that the images may be found in a text-based search query. However, manual annotation is time-consuming and may ignore key features depending on the annotator&#x2019;s expertise and diligence. Context-based indexing methods attempt to automatically assign keywords to images on the Web based on the proximity of keywords or other textual metadata to an image filename in the context of the document in which the image and text appear, but the success of such an approach is limited by, in part, the accuracy (and disambiguity) of the textual metadata themselves, which in many cases were entered manually anyway, potentially introducing the same difficulties for retrieval precision as direct manual annotation in a database. On the other hand, in CBIR systems, images are searched and retrieved (and in some cases, automatically annotated) according to their <italic>visual</italic> content&#xA0;[<xref ref-type="bibr" rid="CR24">24</xref>] rather than on manually entered keywords or metadata. In general, there are three main parts to any CBIR system: feature extraction, indexing, and retrieval. When an image is processed, the visual features of an image&#x2014;such as texture, color, and shape&#x2014;must first be extracted. These low-level features are represented by a multidimensional index or &#x201C;feature vector&#x201D; and then stored in a database. Relevant images in the database are returned as matches to a query based on some measure of &#x201C;distance&#x201D; between the feature vectors of the query and database images.</p>
      <p>The idea of using image or video content to query a database is nothing new. The QBIC (Query By Image Content) method, introduced in 1993 by IBM&#xA0;[<xref ref-type="bibr" rid="CR26">26</xref>], inspired a number of related CBIR systems, including the SIMPLIcity system [<xref ref-type="bibr" rid="CR38">38</xref>], which made novel use of semantically-adaptive searching methods. When a query image is submitted to SIMPLIcity, the image is segmented into similar regions and then classified as either a photograph or an illustration, for example. This semantic classification thus dictates the choice of algorithm to be used for feature extraction.</p>
      <p>In a related project, the Automatic Linguistic Indexing of Pictures (ALIP) system [<xref ref-type="bibr" rid="CR19">19</xref>] is used to train computers to automatically recognize and annotate images by extracting their features and comparing them to categories of images that have previously been annotated. The demonstrated high accuracy of the ALIP system&#x2019;s potential to index photographic images compelled the development of a computationally much more efficient statistical modeling algorithm and a Web-based user interface, ALIPR, to permit users to upload images and see the results of the annotation in real time [<xref ref-type="bibr" rid="CR20">20</xref>]. For details on these and other technical approaches and progress made in the CBIR field, the surveys by Smeulders et al.&#xA0;[<xref ref-type="bibr" rid="CR34">34</xref>] (covering the pre-year 2000 period) and Datta et al. [<xref ref-type="bibr" rid="CR7">7</xref>] (2000&#x2013;2008) together provide a comprehensive review.</p>
      <p><bold>Computational symmetry for novel image retrieval and machine vision applications</bold> A largely unexplored approach to image characterization and retrieval is based on the recognition of symmetry patterns in certain images. Patterns that exhibit 1-D symmetry and repetition are known as &#x201C;frieze&#x201D; patterns, and their 2-D counterparts are known as &#x201C;wallpaper&#x201D; patterns. Studies involving the application of group theory to pattern recognition show that there are only seven unique frieze symmetry groups and only 17 unique wallpaper groups, enabling the possibility of systematic methods for pattern characterization, even in noisy or distorted images&#xA0;[<xref ref-type="bibr" rid="CR13">13</xref>]. Each frieze pattern corresponds to particular rotational symmetry group, whether cyclic or dihedral, as demonstrated by Lee et al.&#xA0;[<xref ref-type="bibr" rid="CR18">18</xref>], who recently proposed a new algorithm for rotational symmetry detection using a &#x201C;frieze-expansion&#x201D; approach, in which 2-D images consisting of potential centers of rotational symmetry are transformed from polar to rectangular coordinates with respect to each potential symmetry center. The local regions of rotational symmetry are then characterized based on the frieze groups detected in the rectangular patterns that result from frieze-expansion. Most real-world images, however, do not immediately exhibit the type of regularity that is easily characterized by frieze or wallpaper symmetry groups. Consequently, near-regular texture (NRT) has been defined&#xA0;[<xref ref-type="bibr" rid="CR22">22</xref>] as minor geometric, photometric, and topological deformations from an otherwise translationally symmetric 2D wallpaper pattern. Much progress has been made in the automated detection of NRT patterns, whether static&#xA0;[<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR22">22</xref>] or in motion&#xA0;[<xref ref-type="bibr" rid="CR21">21</xref>]. In addition, the application of computational symmetry methods to the detection and characterization of patterns in histology images was explored in some of our earlier work, with results promising enough to warrant further investigation and more advanced algorithm development, including [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR4">4</xref>], and the current work.</p>
      <p><bold>Recent efforts in automated histological image analysis and retrieval</bold> As noted above, histopathological assessments may suffer from intra- and inter-observer variability&#xA0;[<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR31">31</xref>], which has inspired the development of automated cancer diagnosis systems that make use of advances in machine vision. Several reviews cover the literature related to CBIR systems as applied to biomedical applications, including a general survey by M&#xFC;ller et al.&#xA0;[<xref ref-type="bibr" rid="CR25">25</xref>] as well as a more recent perspective by Zhou et al.&#xA0;[<xref ref-type="bibr" rid="CR44">44</xref>] on the role of semantics in medical CBIR systems. In addition, Demir and Yener&#xA0;[<xref ref-type="bibr" rid="CR9">9</xref>] offer a specialized review of cancer diagnosis systems using automated histopathology.In one early example of such a system, Hamilton et al.&#xA0;[<xref ref-type="bibr" rid="CR15">15</xref>] used texture feature extraction and a linear classifier to identify possible locations of dysplastic mucosa in colorectal histology images. In another study, Zhao et al.&#xA0;[<xref ref-type="bibr" rid="CR43">43</xref>] noted that classification performance might be improved by using a &#x201C;hierarchy&#x201D; of classifiers. For instance, one scheme might be used to classify a feature as <italic>gland</italic> or <italic>not-gland</italic>, and then depending on the result, a different method would be used to further analyze the image and extract the appropriate features. This paradigm is not unlike the semantically-adaptive searching methods used in the SIMPLIcity method mentioned above&#xA0;[<xref ref-type="bibr" rid="CR38">38</xref>].The idea of retrieving histological images based on semantic content analysis was also explored by Tang et al.&#xA0;[<xref ref-type="bibr" rid="CR35">35</xref>] who used a hierarchy of &#x201C;coarse&#x201D; and &#x201C;fine&#x201D; analyzers for detection of visual features and semantic labeling of the histological structures and substructures, demonstrating the concept of associating &#x201C;high-level&#x201D; semantic content and reasoning with &#x201C;low-level&#x201D; visual features and properties, thus laying the groundwork for the development of histological image retrieval systems that not only recognize features of such images, but can also produce meaningful annotations. The concept of a hierarchical, &#x201C;coarse-to-fine&#x201D; multiresolution analysis was also investigated by Doyle et al.&#xA0;[<xref ref-type="bibr" rid="CR10">10</xref>] and by Gurcan et al.&#xA0;[<xref ref-type="bibr" rid="CR14">14</xref>], who noted that such a framework more closely emulates the histology image process used by human pathologists, and that execution times using this approach can be reduced by an order of magnitude versus single-resolution image retrieval systems.</p>
      <p><bold>CBIR systems for the biological community</bold> Beyond the more clinically-oriented CBIR systems mentioned above, the scientific community has begun to embrace CBIR as means for the navigation and high-throughput population of biological databases. A prime example of this effort comes from the laboratory of Eric Xing, who developed a system&#xA0;[<xref ref-type="bibr" rid="CR12">12</xref>] for querying databases of <italic>Drosophila melanogaster</italic> (fruit fly) embryo gene expression images using multiple modes, allowing users to query not only by gene name or keyword (as with most biological databases) but also by directly uploading images of gene expression. The system automatically annotates the uploaded image using a list of ranked gene expression terms and returns both a set of similar images as well as a list of relevant genes that produce similar expression patterns. This vastly reduces the complexity of user queries required to find genes that match a given expression pattern (which often involves inputting a complicated series of filtering conditions), making this is an important contribution to bioinformatics. The fact that a gene is expressed in a given tissue, however, does not necessarily mean that the morphology of the tissue will be affected if the underlying gene expression is disrupted. This motivates the need to develop CBIR systems that can be used to annotate images with histological phenotypes. The present work, SHIRAZ, is intended to be the first such system that can do so at high-resolution and with the potential for high-throughput. This scalability would serve the needs of large-scale phenotyping efforts such as the Zebrafish Phenome&#xA0;Project.</p>
    </sec>
    <sec id="Sec6" sec-type="methods">
      <title>Methods</title>
      <sec id="Sec7">
        <title>Slide preparation and image preprocessing</title>
        <p>Wild-type (i.e., free of genetic manipulation) and mutant zebrafish larvae, at ages ranging from 2 to 5 dpf (dpf = days post-fertilization) were collected, fixed, and embedded as described in&#xA0;[<xref ref-type="bibr" rid="CR32">32</xref>, <xref ref-type="bibr" rid="CR36">36</xref>]. Individual slides containing up to 50 hematoxylin and eosin (H&amp;E)-stained larval zebrafish sections were digitally captured at 20&#xD7; magnification using an Aperio ScanScope&#x2122; slide scanner in the Penn State Zebrafish Functional Genomics Core Imaging Facility. The resulting images are stored in the proprietary SVS (ScanScope Virtual Slide) format. The SVS format is similar to BigTiff or Zoomify formats in that the image files consist of several image layers, with each layer corresponding to a specific resolution or magnification. Generally speaking, the 20&#xD7; layer alone has a size of approximately 90,000 &#xD7; 45,000 pixels, roughly corresponding to a &#x223C;150&#x2013;200 MB uncompressed TIFF file, although many virtual slides in our laboratory have also been digitized at 40&#xD7; magnification, resulting in even larger (up to &#x223C;50 GB) file sizes (Cheng lab, unpublished).</p>
        <p>For our proof-of-principle prototype system, we elected to train SHIRAZ to perform automatic phenotypic annotation of 5 dpf specimens of larval eye histology. For training and testing purposes, a total of 176 pre-selected eye images were manually extracted at full 20&#xD7; magnification (yielding 768 &#xD7; 768 single-layer RGB images in TIFF format) using Aperio ImageScope&#x2122; software. To enable compatibility with the Web-based SHIRAZ demo site, these images were then converted from TIFF to 24-bit PNG (Portable Network Graphics) format using the ImageMagick <italic>mogrify</italic> utility.</p>
      </sec>
      <sec id="Sec8">
        <title>Preparation of eye images for texture feature extraction</title>
        <sec id="Sec9">
          <title>Extraction of individual histological specimen images</title>
          <p>The original 20X SVS file must be split into separate TIFF images. Here, this is accomplished using the <italic>tiffsplit</italic> command line utility, which itself is part of the <italic>libtiff</italic> library included with most Linux distributions. Splitting the SVS file up into separate images allows us to perform automated extraction of individual specimens at the lowest possible resolution, with much faster results than would be achieved by processing the original 20X image directly.</p>
          <p>The specimens are extracted at low resolution using a fully automated, fiduciary marker-free lattice detection algorithm that we developed previously&#xA0;[<xref ref-type="bibr" rid="CR3">3</xref>] (see Fig.&#xA0;<xref rid="Fig5" ref-type="fig">5</xref> for a typical &#x201C;before-and-after&#x201D; result example). The coordinates of the lattice are scaled up to 20X to enable extraction of individual specimen images at their original resolution. The lattice detection algorithm proceeds in three stages; in stage 1 (Fig.&#xA0;<xref rid="Fig6" ref-type="fig">6</xref>a and b), the rotation offset of the array image, if any, is corrected so that all specimens are oriented horizontally. In stage 2 (Fig.&#xA0;<xref rid="Fig6" ref-type="fig">6</xref>c and d), we use the detected number of cells in the array to divide the bounding box circumscribing the specimens into an initial lattice, taking advantage of the upper bounds of five columns and ten rows imposed by the histology array apparatus design. In practice, we have observed that is it sufficient to assume that the lattice will contain ten rows, even if some rows are partially or totally unoccupied. The number of columns in the field of view, however, varied from 1 to 5, depending on the particular experimental question. Finally, in stage 3 (Fig.&#xA0;<xref rid="Fig6" ref-type="fig">6</xref>e), we optimize the initial lattice by varying its height until we maximize the symmetry of each lattice cell image about its horizontal reflection axis. The algorithm pseudocode is provided as follows:</p>
          <p>
            <italic>Stage 1: Global orientation correction</italic>
            <list list-type="order">
              <list-item>
                <p><inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$A_{\rm convhull}\gets$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq1.gif"/></alternatives></inline-formula> area of convex hull</p>
              </list-item>
              <list-item>
                <p><inline-formula id="IEq2"><alternatives><tex-math id="M2">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$A_{\rm box}\gets$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq2.gif"/></alternatives></inline-formula> area of bounding box around convex hull</p>
              </list-item>
              <list-item>
                <p>
                  <inline-formula id="IEq3">
                    <alternatives>
                      <tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\Theta_{\rm corrected}\gets {{\rm argmax}({A_{\rm convhull} \over A_{\rm box}})}$\end{document}</tex-math>
                      <inline-graphic xlink:href="11042_2010_638_Article_IEq3.gif"/>
                    </alternatives>
                  </inline-formula>
                </p>
              </list-item>
            </list>
          </p>
          <p>
            <italic>Stage 2: Rigid lattice placement</italic>
            <list list-type="order">
              <list-item>
                <p>Perform morphological closing (&#x2295;) in vertical direction</p>
              </list-item>
              <list-item>
                <p><inline-formula id="IEq4"><alternatives><tex-math id="M4">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$n_{\rm columns}\gets$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq4.gif"/></alternatives></inline-formula> number of connected components after &#x2295;</p>
              </list-item>
              <list-item>
                <p><inline-formula id="IEq5"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$n_{\rm rows}\gets$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq5.gif"/></alternatives></inline-formula> 10 (assumed)</p>
              </list-item>
              <list-item>
                <p><inline-formula id="IEq6"><alternatives><tex-math id="M6">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$L_{\rm initial}\gets$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq6.gif"/></alternatives></inline-formula> result of dividing bounding box around foreground pixels into lattice containing (<italic>n</italic><sub>rows</sub><italic>n</italic><sub>columns</sub>) equally sized cells</p>
              </list-item>
            </list>
          </p>
          <p>
            <italic>Stage 3: Deformable lattice optimization</italic>
            <list list-type="order">
              <list-item>
                <p><bold>foreach</bold> grayscale cell image <italic>C</italic> in lattice <italic>L</italic></p>
              </list-item>
              <list-item>
                <p>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;<inline-formula id="IEq7"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$S_{\rm corr}\gets corr\bigl(C, mirrorimage(C)\bigr)$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq7.gif"/></alternatives></inline-formula></p>
              </list-item>
              <list-item>
                <p>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;<inline-formula id="IEq8"><alternatives><tex-math id="M8">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$L_{\rm f\/inal}\gets {\rm argmax}(S)$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq8.gif"/></alternatives></inline-formula> for all <italic>C</italic></p>
              </list-item>
            </list>
            <fig id="Fig5">
              <label>Fig.&#xA0;5</label>
              <caption>
                <p>Typical example of a histological section of a larval zebrafish array (<italic>left</italic>) and its computationally detected underlying lattice structure (<italic>right</italic>)</p>
              </caption>
              <graphic xlink:href="11042_2010_638_Fig5_HTML" id="d28e747"/>
            </fig>
            <fig id="Fig6">
              <label>Fig.&#xA0;6</label>
              <caption>
                <p>Illustration of the array image lattice detection procedure</p>
              </caption>
              <graphic xlink:href="11042_2010_638_Fig6_HTML" id="d28e756"/>
            </fig>
          </p>
          <p>The final lattice (<italic>L</italic><sub>final</sub>) is scaled up from the low resolution image to the full 20X image to enable the full-resolution extraction of specimens; however, any rotation correction applied to the low-resolution image must also be applied to the 20X image as well before the lattice can be applied. We used the VIPS image processing libraries [<xref ref-type="bibr" rid="CR37">37</xref>] for handling the requisite rotation transformations (using package <italic>im_affinei_all</italic>) and extractions of those image areas corresponding to individual specimens (using package <italic>im_extract_area</italic>).</p>
        </sec>
        <sec id="Sec10">
          <title>Eye extraction</title>
          <p>After the whole-larva specimens are cropped from the source array image, the organs of interest (here, the left and right eyes) are automatically extracted from each specimen image. Relative to the major axis and mouth of the zebrafish larva, the positions of the eyes are highly consistent from one fish to the next. Figure&#xA0;<xref rid="Fig7" ref-type="fig">7</xref> indicates the distribution of overlapping areas of zebrafish left and right eyes (at age 5 dpf) for approximately 45 manually-labeled histological specimens. Because the positions of the eyes are highly consistent for informative specimens, we can use the position of the overlapping areas to crop a 768 &#xD7; 768 region from each 20X larval specimen image in which the eye histology is contained (assuming that the eye tissue is present in the histological section of interest).
<fig id="Fig7"><label>Fig.&#xA0;7</label><caption><p>When each 5-dpf larval zebrafish image is rotated to align to a common origin point (here, the mouth, located at the leftmost position on the midline in the above images) and also along a common horizontal axis, we find that the positions of the eyes across all images are largely overlapping, which allows us to use a relatively simple location-based method for extracting a 768 &#xD7; 768 square region around each eye for input to the SHIRAZ system</p></caption><graphic xlink:href="11042_2010_638_Fig7_HTML" id="d28e789"/></fig></p>
        </sec>
        <sec id="Sec11">
          <title>&#x201C;Frieze-like&#x201D; expansion of eye histology images</title>
          <p>Upon inspection of any typical zebrafish histology image, one can observe that certain tissue structures exhibit a certain degree of repetition or symmetry (at least in the normal or wild-type state). One can see that the zebrafish larval retina possesses partial rotational symmetry about the lens, as indicated in Fig.&#xA0;<xref rid="Fig8" ref-type="fig">8</xref>. The repeating pattern implied within the retina might become more obvious (and more easily detected computationally) if the image could be transformed from its rotationally symmetric appearance to a more regular, linear shape, thereby reducing the pattern complexity to only one dimension. This would facilitate the generation of methods for detecting and characterizing the implicit symmetry patterns as well as defects and local deviations that disrupt the pattern continuity. We refer to the transformation of the original eye image into a rectangular shape as <italic>frieze-like expansion</italic> after the previously mentioned algorithm (see Section&#xA0;<xref rid="Sec2" ref-type="sec">2</xref>) by Lee et al.&#xA0;[<xref ref-type="bibr" rid="CR18">18</xref>] that was designed to locate and characterize the underlying frieze (1-D) symmetry group patterns within real-world images exhibiting local rotational symmetries.
<fig id="Fig8"><label>Fig.&#xA0;8</label><caption><p>Labeled layers of the 5-dpf zebrafish eye. In addition, the <italic>arrow</italic> indicates the direction of the implicit &#x201C;rotational symmetry&#x201D; in certain histology images such as this, where we typically find a constant and repeating pattern of cellular layers that revolve about the lens</p></caption><graphic xlink:href="11042_2010_638_Fig8_HTML" id="d28e818"/></fig></p>
          <p>The zebrafish eye consists of several distinct layers, including the lens, the ganglion cell layer, the inner plexiform later, the inner nuclear layer (which itself consists of the amacrine and bipolar cell layers), the outer plexiform layer, the photoreceptor layer, and finally the retinal pigmented epithelium (RPE) and choroidal melanocytes (which at this developmental stage appear together and have no clear boundary between them)&#xA0;[<xref ref-type="bibr" rid="CR27">27</xref>] (see Fig.&#xA0;<xref rid="Fig8" ref-type="fig">8</xref>). For the purposes of expanding the retina into a 1-D frieze-like pattern, we only need to provide an outer perimeter, which will correspond the top edge of the frieze-like expansion pattern, and an inner perimeter, corresponding to the bottom edge (see Fig.&#xA0;<xref rid="Fig9" ref-type="fig">9</xref>). Thus we need only extract the RPE and the lens, and fortunately, they are relatively easy to identify. The RPE, by virtue of its melanin pigmentation, is generally the darkest continuous segment of the retina, and the lens is generally the largest object near the center of the eye image whose shape most closely approximates that of a circle. The image processing operations required for segmentation of the lens and RPE are thus relatively straightforward, consisting primarily of gray-level thresholding, edge detection, and shape identification based on the ratio of the shape&#x2019;s area to its perimeter (i.e., one possible measure of shape &#x201C;compactness&#x201D;).
<fig id="Fig9"><label>Fig.&#xA0;9</label><caption><p>Overview of frieze-like expansion of zebrafish retina. Note that because of the distortion resulting from re-scaling the shorter line segments (generally found near the marginal zones of the retina, on either side of the lens), we extract only the middle 50% of pixels (<italic>dashed line</italic>) from the frieze-expanded eye image</p></caption><graphic xlink:href="11042_2010_638_Fig9_HTML" id="d28e841"/></fig></p>
          <p>The perimeter points are used as a basis for extracting line segments of pixels from the original eye histology image (Fig.&#xA0;<xref rid="Fig9" ref-type="fig">9</xref>a and b). Ideally, each line segment would start at the outer perimeter and be oriented more or less perpendicular to the tangent line at the starting point. The intersecting point on the inner perimeter would thus be the endpoint of the line segment. However, owing to the irregular shape of both perimeters, the line segment may not necessarily end at the desired point on the inner perimeter; in fact, in some cases the line perpendicular to the tangent line at the starting point may not even intersect with the inner perimeter at all. To remedy this, the algorithm identifies a set of initial &#x201C;control points&#x201D; evenly spaced around the outer perimeter (in practice, 12 control points usually provides good results). Line segments are extracted starting from these control points and ending at the nearest point on the inner perimeter as measured by Euclidean distance. The remaining line segments are then extracted by interpolation between the control points. Following extraction of all line segments, we normalize their lengths to a pre-specified number of pixels. The normalized line segments are then rearranged as columns of the transformed image (Fig.&#xA0;<xref rid="Fig9" ref-type="fig">9</xref>c). Since the layers of the larval retina exhibit only partial rotational symmetry around the lens, the resulting image will be distorted at either end of the frieze expansion. Generally speaking, the central part of the transformed image is most highly representative of the retina&#x2019;s partial rotational symmetry around the lens, so we perform our subsequent analysis only on this central region (we chose the middle 50% of pixels), and ignore the more highly distorted sections at either end of the original transformed image.</p>
        </sec>
      </sec>
      <sec id="Sec12">
        <title>Extraction of texture features from image blocks</title>
        <sec id="Sec13">
          <title>Blockwise processing</title>
          <p>Our feature extraction process follows a block-based scheme, in which we divide the frieze-expanded eye image into a series of 64 &#xD7; 64 blocks (see Fig.&#xA0;<xref rid="Fig10" ref-type="fig">10</xref>). Although the height of each frieze-expanded image is set at a fixed 128 pixels, the width of the image will vary from image to image depending on the number of line segments extracted during the frieze-like expansion process. In practice, the width typically ranges from about 400&#x2013;700 pixels.
<fig id="Fig10"><label>Fig.&#xA0;10</label><caption><p>Illustration of subdivision of frieze-expanded eye &#x201C;parent&#x201D; image into 64 &#xD7; 64 &#x201C;child&#x201D; block images (Selected image blocks from the subdivision of the parent image have been marked with dotted borders to show their corresponding positions in the &#x201C;exploded&#x201D; view)</p></caption><graphic xlink:href="11042_2010_638_Fig10_HTML" id="d28e869"/></fig></p>
          <p>Dividing the &#x201C;parent&#x201D; frieze-expanded image into smaller &#x201C;child&#x201D; blocks offers several benefits over extracting features from the parent image as a whole. First, because the local image texture properties will vary throughout the image, extracting features from smaller blocks permits us to more precisely identify where the local texture aberrations occur. If we were to extract features from the image as a whole, it is likely that local variations would be &#x201C;masked&#x201D; by the properties of the whole image, thus making it difficult to discriminate or differentially classify two images that are largely identical save for local texture variations.</p>
          <p>The blockwise operation also allows us to identify local features in a regional context. The blocks are not necessarily image patches with discrete boundaries&#x2014;rather, we intend for these blocks to overlap, so that we can approximately capture the &#x201C;continuum&#x201D; of local texture variation from one image region to the next, which is particularly important when there are local texture irregularities in the parent image (see Fig.&#xA0;<xref rid="Fig11" ref-type="fig">11</xref>). In the vertical direction, where the parent image is 128 pixels high, we extract three 64 &#xD7; 64 blocks. The top and bottom blocks do have a discrete boundary, but the middle block now overlaps one-half of each of the top and bottom blocks. Because the texture features extracted from the middle block will capture the local variation about the interface between the top and bottom blocks, the three-dimensional &#x201C;vector&#x201D; of features representing the three 64 &#xD7; 64 blocks will be more representative of continuous local variation in the vertical direction than one would obtain by only extracting features from non-overlapping blocks. Conceptually, the idea is similar to taking a &#x201C;moving average&#x201D; of variation along the vertical direction, but here we are using a 64 &#xD7; 64 &#x201C;window&#x201D; in which the &#x201C;moving average&#x201D; is computed.
<fig id="Fig11"><label>Fig.&#xA0;11</label><caption><p>An example to illustrate the rationale for using overlapping child block images</p></caption><graphic xlink:href="11042_2010_638_Fig11_HTML" id="d28e885"/></fig></p>
          <p>In the horizontal direction, a similar approach is taken, but because the width varies from one image to the next, the number and location of the individual 64 &#xD7; 64 blocks or &#x201C;windows&#x201D; will also vary. In other words, the image width is usually not a multiple of 64, and so dividing up the image into non-overlapping patches will result in a &#x201C;remainder&#x201D; set of pixels that is less than 64 pixels wide. Allowing for overlap as was done in the vertical direction, where the overlapping blocks cover one-half of each of their neighbors, would only work if the parent image width were a multiple of 32. To ensure account for all pixels in the parent image, we still allow for overlap from one block to the next, but the extent of overlap&#x2014;in terms of the number of pixels along the horizontal&#x2014;is now a function of the total width of the parent image. To compute both the degree of overlap and the number of &#x201C;child&#x201D; blocks to be extracted in the horizontal direction while ensuring that the extent of pixel overlap is identical for all blocks and that no pixels are left unaccounted for, we devised the following procedure:
<disp-formula id="Equa"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \begin{array}{l} \textit{width} \gets \mbox{ width of frieze-expanded parent image} \\ \\ \textit{height} \gets \mbox{ height of frieze-expanded parent image}\\ \\ \textit{blocksize} = {\it f\/loor}\left(\frac{\textit{height}}{2}\right)\\ \\ \beta = {ceil\left({\frac{\textit{width}} {\textit{blocksize}}}\right)} \\ \\ \alpha = \textit{blocksize} - {\frac{(\textit{blocksize} \times \beta) - \textit{width}} {\beta - 1} } \end{array} $$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p>
          <p>In the above algorithm, <italic>&#x3B2;</italic> represents the number of blocks to be extracted, while <italic>&#x3B1;</italic> represents the degree of horizontal overlap between blocks, expressed in number of pixels. (The degree of overlap in the vertical direction is always 32, assuming that the parent image height is always 128.) For each 64 &#xD7; 64 image block, we extract a total of 54 features, comprised of a combination of the following:
<list list-type="bullet"><list-item><p><italic>Gray-level co-occurrence features</italic></p></list-item><list-item><p><italic>Lacunarity</italic></p></list-item><list-item><p><italic>Gray-level morphology features</italic></p></list-item><list-item><p><italic>Markov Random Field model parameters</italic></p></list-item><list-item><p><italic>Daubechies wavelet packet features</italic></p></list-item></list></p>
          <p>We describe each of the above five types of features in more detail below.</p>
        </sec>
        <sec id="Sec14">
          <title>Gray-level co-occurrence features</title>
          <p>A common method for characterizing texture in images involves descriptors derived from an image&#x2019;s gray level co-occurrence matrix, or GLCM, which is a representation of the second-order (joint) probability of pairs of pixels having certain gray level values. For a given pixel (<italic>i</italic>, <italic>j</italic>) which has a gray-level value <italic>m</italic>, the GLCM provides a method of representing the frequency of pixels at some distance <italic>d</italic> from (<italic>i</italic>, <italic>j</italic>) that have a gray-level value <italic>n</italic>. Formally, a rotationally-invariant GLCM (that is, one that only depends on the distance (&#x394;<italic>x</italic>, &#x394;<italic>y</italic>) between pixels in a pair, and not on the angle of the line segment connecting them) may be defined as
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M10">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ C(m,n,{\Delta}x,{\Delta}y) = \sum\limits_i \sum\limits_j I\bigl(m-g(i,j)\bigr)I\bigl(n-g(i+{\Delta}x,j+{\Delta}y)\bigr) $$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>C</italic> is the GLCM, <italic>g</italic>(<italic>i</italic>, <italic>j</italic>) represents the gray level value at position (<italic>i</italic>, <italic>j</italic>), &#x394;<italic>x</italic> and &#x394;<italic>y</italic> are respectively the offset distances in the <italic>x</italic> and <italic>y</italic> directions, and the binary indicator function <italic>I</italic>(<italic>a</italic>&#x2009;&#x2212;&#x2009;<italic>b</italic>) equals 1 if <italic>a</italic>&#x2009;=&#x2009;<italic>b</italic>, but equals zero if <italic>a</italic>&#x2009;&#x2260;&#x2009;<italic>b</italic>.</p>
          <p>The GLCM is a rich representation of the texture contained in an image, but by itself it is large and often sparse, and thus cumbersome to work with. In practice, one can compute a series of second-order statistics from the GLCM that provide a more compact representation of image texture. The five most common descriptors computed from the GLCM and which we use in our analysis are listed below, shown here as defined in&#xA0;[<xref ref-type="bibr" rid="CR29">29</xref>] but originally derived by Haralick et al. in&#xA0;[<xref ref-type="bibr" rid="CR16">16</xref>]:
<list list-type="bullet"><list-item><p>Energy:
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \displaystyle\sum\limits_{m=0}^{L-1}\sum\limits_{n=0}^{L-1}p(m,n)^2 $$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p></list-item><list-item><p>Entropy:
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M12">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \displaystyle\sum\limits_{m=0}^{L-1}\sum\limits_{n=0}^{L-1}p(m,n) \log p(m,n) $$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p></list-item><list-item><p>Contrast:
<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {1\over{(L-1)^2}}\displaystyle\sum\limits_{m=0}^{L-1}\sum\limits_{n=0}^{L-1} (m-n)^2 p(m,n) $$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p></list-item><list-item><p>Correlation:
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M14">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\sum\nolimits_{m=0}^{L-1}\sum\nolimits_{n=0}^{L-1}mnp(m,n)-{\mu}_x{\mu}_y}\over{\sigma_x\sigma_y} $$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where
<disp-formula id="Equb"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mu_x = \displaystyle\sum\limits_{m=0}^{L-1}m\sum\limits_{n=0}^{L-1}p(m,n) \quad\quad\quad\quad\quad \mu_y = \displaystyle\sum\limits_{n=0}^{L-1}n\sum\limits_{m=0}^{L-1}p(m,n) $$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equb.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equc"><alternatives><tex-math id="M16">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \sigma_x = \displaystyle\sum\limits_{m=0}^{L-1}(m - \mu_x)^2\sum\limits_{n=0}^{L-1}p(m,n) \quad\quad \sigma_y = \displaystyle\sum\limits_{n=0}^{L-1}(n - \mu_y)^2\sum\limits_{m=0}^{L-1}p(m,n) $$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equc.gif" position="anchor"/></alternatives></disp-formula></p></list-item><list-item><p>Homogeneity:
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \displaystyle\sum\limits_{m=0}^{L-1}\sum\limits_{n=0}^{L-1} {{p(m,n)}\over{1 + |m-n|}} $$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p></list-item></list></p>
          <p>In each of the above formulae, <italic>L</italic> refers to the total number of gray levels (256 for an 8-bit image), and <italic>p</italic>(<italic>m</italic>, <italic>n</italic>) is the joint probability density function derived by normalizing the co-occurrence matrix <italic>C</italic> via division by the number of all paired pixel occurrences, i.e.:
<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M18">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ p(m,n) = {{C(m,n)}\over{\rm total\ number\ of\ paired\ occurrences}} $$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p>
        </sec>
        <sec id="Sec15">
          <title>Lacunarity</title>
          <p>To enhance the description of an image&#x2019;s texture beyond the Haralick texture features described above, we also employ a measure known as <italic>lacunarity</italic>, which is perhaps more commonly used in geometry as a measure of fractal texture&#xA0;[<xref ref-type="bibr" rid="CR30">30</xref>], but we use it here as a means of representing the heterogeneity of image data. Mathematically, it is similar to the so-called &#x201C;dispersion index&#x201D; in that it depends on the ratio of the variance of a function divided by its mean. Lacunarity can be described by any of the following expressions (as defined in&#xA0;[<xref ref-type="bibr" rid="CR29">29</xref>]):
<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{L}_s \equiv {{1\over NM} \sum\nolimits_{i=1}^N \sum\nolimits_{j=1}^M g(i,j)^2 \over {\left({1\over NM} \sum\nolimits_{k=1}^N \sum\nolimits_{l=1}^M g(k,l)\right)^2}} - 1} $$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M20">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{L}_a \equiv {1\over NM} \displaystyle\sum\limits_{i=1}^N \displaystyle\sum\limits_{j=1}^M {\Bigg|{ g(i,j) \over {\left({1\over NM} \sum\nolimits_{k=1}^N \sum\nolimits_{l=1}^M g(k,l)\right)}} - 1}\Bigg|} $$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{L}_p \equiv {\Bigg({1\over NM} \displaystyle\sum\limits_{i=1}^N \displaystyle\sum\limits_{j=1}^M {\Bigg({ g(i,j) \over {\left({1\over NM} \sum\nolimits_{k=1}^N \sum\nolimits_{l=1}^M g(k,l)\right)}} - 1}\Bigg)^p}\Bigg)}^{1\over p} $$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula></p>
          <p>Respectively, <inline-formula id="IEq9"><alternatives><tex-math id="M22">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal{L}_s$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq9.gif"/></alternatives></inline-formula>, <inline-formula id="IEq10"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal{L}_a$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq10.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq11"><alternatives><tex-math id="M24">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal{L}_p$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq11.gif"/></alternatives></inline-formula> refer to the squared, absolute, and power-mean representations of lacunarity, while <italic>g</italic>(<italic>i</italic>, <italic>j</italic>) refers to the gray-level value of the pixel located at (<italic>i</italic>, <italic>j</italic>). In our analysis, we extract lacunarity features for <inline-formula id="IEq12"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal{L}_s$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq12.gif"/></alternatives></inline-formula>, <inline-formula id="IEq13"><alternatives><tex-math id="M26">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal{L}_a$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq13.gif"/></alternatives></inline-formula>, as well as <inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal{L}_p$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq14.gif"/></alternatives></inline-formula> at three arbitrarily chosen levels: <italic>p</italic>&#x2009;=&#x2009;2, <italic>p</italic>&#x2009;=&#x2009;4, and <italic>p</italic>&#x2009;=&#x2009;6.</p>
        </sec>
        <sec id="Sec16">
          <title>Gray-level morphology</title>
          <p>In image processing, morphological operations such as <italic>dilation, erosion, opening, and closing</italic> are often used to generate representations of the shapes of image regions, which may then be used in the segmentation of images into regions of similar color and/or texture. Typically, morphological operations are performed on black-and-white or &#x201C;binarized&#x201D; images&#x2014;that is, images that have been thresholded at a particular gray level value such that image pixels at or above the threshold value are re-mapped as &#x201C;foreground&#x201D; pixels (with a value of 1), and all other pixels are re-mapped as &#x201C;background&#x201D; pixels (with a value of 0). Morphological operations are applied to each pixel in the image in order to add or remove pixels inscribed within in a &#x201C;moving neighborhood&#x201D; (known as a <italic>structuring element</italic>) centered on each pixel in the source image. In <italic>dilation</italic>, for example, the source image is modified by placing the structuring element over each foreground pixel and then setting all pixels within the contained area to a value of 1. The effect is that all objects in the foreground are expanded or &#x201C;dilated,&#x201D; as the name suggests. By contrast, in <italic>erosion</italic>, the foreground objects are modified by placing the structuring element over each object pixel and then removing or &#x201C;eroding&#x201D; object pixels that do not allow the structuring element to fit within the object. For example, if we place a 3 &#xD7; 3 square structuring element over a given foreground object pixel, the resulting output image will only keep that object pixel if all of its eight-connected neighbors are foreground pixels. If even one of its eight-connected neighbors are background pixels (i.e., having a value of zero), then the central object pixel is removed, or &#x201C;eroded,&#x201D; and will not appear in the resulting output image.</p>
          <p>Dilation and erosion are typically used in combination when the goal is to remove object details that are smaller than a given size while leaving larger details alone&#xA0;[<xref ref-type="bibr" rid="CR29">29</xref>]. For example, to remove spurious, &#x201C;hair-like&#x201D; projections from an object, we would first erode the image to remove the spurious details, and then perform dilation on the eroded image to restore the more &#x201C;solid&#x201D; portions of the image objects to their original sizes. The process of erosion followed by dilation is more commonly known as <italic>opening</italic>. In contrast, we may choose to perform <italic>closing</italic> on an image if there are spurious or otherwise undesired &#x201C;gaps&#x201D; within an image object. To &#x201C;close&#x201D; an image, we first perform dilation on the foreground pixels to &#x201C;cover up&#x201D; the gaps, and then we perform erosion on the result to restore the gap-filled objects back to their original sizes. The success or failure of opening and closing in accomplishing the user&#x2019;s objective depends, of course, on the type and size of structuring element used. If there are wide gaps within an object, the user would want to employ a structuring element whose radius was wide enough to ensure gap closure, but not so wide as to merge two or more distant objects that are unrelated (i.e., not intended to be part of the same closed region).</p>
          <p>Morphological operations may also be applied to the gray-level images themselves, without the need for thresholding or &#x201C;binarization,&#x201D; with the main difference being that when placing the structuring element over the central pixel, only the central pixel will be modified. In the output image, the central pixel takes on the value of the maximum (if dilation) or minimum (if erosion) gray level of the pixels within the neighborhood covered by the structuring element, while the remaining pixels under the structuring element are ignored. This difference in the operation ensures that the same output image is obtained regardless of how the structuring element &#x201C;moves&#x201D; around the source image. In other words, if all pixels under the structuring element were set to the value of the maximum- (or minimum-) gray value pixel, then every time the structuring element moved to a new position, the output values of non-central pixels covered by the structuring element in the previous position may change. A structuring element that moves, for instance, from left-to-right will produce a different output image than one that moves from right-to-left. By allowing only the central pixel to be modified, the same output image will be produced regardless of the path followed by the structuring element.</p>
          <p>The results of applying morphological operations on a gray-level image may be useful in characterizing the texture of the image by <italic>granulometry</italic>, using the following method as described in&#xA0;[<xref ref-type="bibr" rid="CR29">29</xref>]:
<list list-type="bullet"><list-item><p><italic>S</italic><sub><italic>D</italic></sub> = result of <italic>dilating</italic> gray level source image <italic>S</italic> using a structuring element of radius <italic>m</italic></p></list-item><list-item><p><italic>S</italic><sub><italic>E</italic></sub> = result of <italic>eroding</italic> gray level source image <italic>S</italic> using a structuring element of radius <italic>m</italic></p></list-item><list-item><p><italic>N</italic><sub>1</sub> = sum of the pixel values of the difference image produced by subtracting <italic>S</italic><sub><italic>E</italic></sub> from <italic>S</italic><sub><italic>D</italic></sub></p></list-item><list-item><p><italic>N</italic><sub>2</sub> = sum of the pixel values of the original image <italic>S</italic>, excluding the values at pixels that were &#x201C;ignored&#x201D; by the dilation and erosion operations (i.e., the pixels around the image perimeter)</p></list-item><list-item><p><italic>N</italic><sub>1</sub>/<italic>N</italic><sub>2</sub> = <italic>granulometry</italic> of image <italic>S</italic></p></list-item></list></p>
        </sec>
        <sec id="Sec17">
          <title>Markov random fields</title>
          <p>Markov random fields (MRFs) have been used to model image textures; some of the earliest work in the area comes from Cross and Jain&#xA0;[<xref ref-type="bibr" rid="CR6">6</xref>]. A random field, in its most general sense, may be represented by a series of random values (such as the outcomes of a coin tossing or other random number generation experiment) mapped onto some <italic>n</italic>-dimensional space. In the case of two-dimensional images, <italic>n</italic>&#x2009;=&#x2009;2, and the random field might consist of a set of randomly selected gray levels placed on the two-dimensional grid.</p>
          <p>A <italic>Markov random field</italic> is similar, but instead of each pixel&#x2019;s gray level value being determined by an unbiased random experiment, the gray-level value of each pixel possesses a so-called &#x201C;Markovian&#x201D; property in that the gray level value is directly dependent on or influenced by the pixels in its immediate neighborhood (but not directly dependent on pixels outside of that neighborhood). By implicit determination of Markov random field parameters, we may be able to model the gray levels that constitute a particular image&#x2019;s texture.</p>
          <p>In our analysis, we use the parameters of a Gaussian MRF model as texture features. A Gaussian MRF has the following properties:
<list list-type="bullet"><list-item><p>The value of each pixel is determined by a random sample from a Gaussian distribution</p></list-item><list-item><p>For a given pixel value, the parameters of the Gaussian distribution are functions of the values of the neighboring pixels</p></list-item></list>For simplicity, the standard deviation of the Gaussian may be assumed to be independent of the gray values of the neighbors about the central pixel (but which is still characteristic of the Markov process), and so a general model for the Gaussian MRF can be represented by the following probability density function:
<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M28">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ p\bigl(g_{ij}|g_{i'j'},(i',j') \in N_{ij}\bigr) = {1 \over {\sqrt{2\pi}\sigma}}\exp\left({{{-{\bigl(g_{ij}-\sum\nolimits_{l=1}^{L}a_l g_{i'j';l}\bigr)^2}}}\over{2{\sigma}^2}}\right) $$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula>in which <inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$p\bigl(g_{ij}|g_{i'j'},(i',j') \in N_{ij}\bigr)$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq15.gif"/></alternatives></inline-formula> represents the probability of pixel (<italic>i</italic>, <italic>j</italic>) having gray-level value <italic>g</italic><sub><italic>ij</italic></sub> given the corresponding gray-level values of the neighboring pixels, <italic>L</italic> is the number of pixels in the neighborhood defined by <italic>N</italic><sub><italic>ij</italic></sub> that influence the pixel (<italic>i</italic>, <italic>j</italic>) in the context of the MRF, <italic>a</italic><sub><italic>l</italic></sub> is a MRF parameter indicating the level of influence that a neighbor pixel has on the pixel at (<italic>i</italic>, <italic>j</italic>), and <italic>g</italic><sub><italic>i</italic>&#x2032;<italic>j</italic>&#x2032;;<italic>l</italic></sub> is the value of the neighbor pixel at the position (<italic>i</italic>&#x2032;, <italic>j</italic>&#x2032;). Finally, the standard deviation <italic>&#x3C3;</italic> is an expression of the degree of uncertainty in the Gaussian MRF model and is also used to characterize the random experiment performed in each location that the model uses to assign a value to pixel (<italic>i</italic>, <italic>j</italic>)&#xA0;[<xref ref-type="bibr" rid="CR29">29</xref>].</p>
          <p>One method of inferring the parameters of a Gaussian MRF is by least-squared-error (LSE) estimation. We follow the LSE estimation procedure outlined in&#xA0;[<xref ref-type="bibr" rid="CR29">29</xref>]. If, for example, we assume a second-order Markov neighborhood in which the neighboring pixels are immediately adjacent to the central pixel (i.e., an eight-connected neighborhood), then the above equation becomes
<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M30">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ p\bigl(g_{ij}|g_{i'j'},(i',j') \in N_{ij}\bigr) = {1 \over {\sqrt{2\pi}\sigma}}exp\left({{{-{\bigl(g_{ij}-\sum\nolimits_{l=1}^{4}a_l s_{ij;l}\bigr)^2}}}\over{2{\sigma}^2}}\right) $$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula>where:
<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{lll} &amp;&amp; s_{ij;1} \equiv g_{i-1,j} + g_{i+1,j} \nonumber \\ &amp;&amp; s_{ij;2} \equiv g_{i,j-1} + g_{i,j+1} \nonumber \\ &amp;&amp; s_{ij;3} \equiv g_{i-1,j-1} + g_{i+1,j+1} \nonumber \\ &amp;&amp; s_{ij;4} \equiv g_{i-1,j+1} + g_{i+1,j-1} \end{array}$$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>and thus five parameters must be determined implicitly: <italic>a</italic><sub>1</sub>, <italic>a</italic><sub>2</sub>, <italic>a</italic><sub>3</sub>, <italic>a</italic><sub>4</sub>, and <italic>&#x3C3;</italic>. These five parameters can thus be used as properties of the image texture modeled by the Gaussian MRF. However, a proper solution of the model can only take place simultaneously for those pixels that have non-overlapping neighborhoods of pixels that influence the central value according to the model. In the case of the second-order (eight-connected) neighborhood used here, it can be shown that an image can be divided up into four possible sets of pixels whose influencing neighbors do not overlap&#xA0;[<xref ref-type="bibr" rid="CR29">29</xref>]. Thus, we can compute the five parameters <italic>a</italic><sub>1</sub>, <italic>a</italic><sub>2</sub>, <italic>a</italic><sub>3</sub>, <italic>a</italic><sub>4</sub>, and <italic>&#x3C3;</italic> for each of these four sets of pixels, and also take their respective means, and then use all of these results as texture features. For the LSE-MRF texture modeling approach alone, this yields a fairly robust 25-feature set for each 64 &#xD7; 64 image block.</p>
        </sec>
        <sec id="Sec18">
          <title>Wavelet packet features for texture characterization</title>
          <p>More recently, <italic>wavelets</italic> have become a popular form of multiresolution representation of image texture. In the context of signal and image processing, wavelets are used to break down a function or signal into components at multiple scales, yielding insight not only into an image&#x2019;s frequency characteristics, but its spatial characteristics as well&#xA0;[<xref ref-type="bibr" rid="CR11">11</xref>].</p>
          <p>In practice, the multiresolution analysis is performed by applying low- and high-pass filters along the vertical and horizontal axes of a full-resolution image (a low-pass filter passes lower-frequency signals and reduces higher-frequency signal amplitudes, while a high-pass filter does the opposite). This results in a &#x201C;splitting&#x201D; or &#x201C;expansion&#x201D; of the original signal into four component subbands: Low-Low (LL), Low-High (LH), High-Low (HL), and High-High (HH). The LH, HL, and HH subbands provide information about texture variation in the vertical, horizontal, and diagonal directions, respectively&#xA0;[<xref ref-type="bibr" rid="CR38">38</xref>]. Each of the four subbands can then be filtered again at additional levels of resolution; the expansions from one scale to the next can be visualized as a tree-like structure (see Fig.&#xA0;<xref rid="Fig12" ref-type="fig">12</xref>). For the purposes of texture analysis, we wish to have as much redundant information as is computationally feasible to provide a robust representation of image texture, and so it makes sense to expand all signals into their LL, LH, HL, and HH subband components. This &#x201C;full&#x201D; expansion is typically referred to as <italic>packet wavelet analysis</italic>, which stands in contrast to <italic>tree wavelet analysis</italic>, which focuses only on the lowest frequency subbands and thus is useful for compressing an image into a form where it can be reconstructed from the minimum number of bits&#xA0;[<xref ref-type="bibr" rid="CR29">29</xref>]. Because we are interested here in the characterization of image texture and not on the compression of image file size, we use the packet wavelet approach in our analysis. Further, rather than always choosing the lowest-frequency channels for signal expansion at each scale, we preferentially choose the subband with the highest &#x201C;energy,&#x201D; which is computed by taking the summation of the squares of the gray level values of the pixels making up each subband image. The &#x201C;energies&#x201D; at each level are thus used as features for describing the texture of the original image.
<fig id="Fig12"><label>Fig.&#xA0;12</label><caption><p>Example of wavelet packet expansion for a 64 &#xD7; 64 &#x201C;child&#x201D; image block taken from a frieze-expanded &#x201C;parent&#x201D; image. The expansion proceeds through four steps, and following the initial expansion, the image corresponding to the highest-energy band is chosen for the next level of expansion. A total of 16 energy values are thus used as texture features for each child image block</p></caption><graphic xlink:href="11042_2010_638_Fig12_HTML" id="d28e1554"/></fig></p>
          <p>A number of filters have been developed for wavelet analysis, each having different properties that result in a trade-off between computation time and the ability to capture local texture variation at longer resolutions. The Daubechies-4 filter&#xA0;[<xref ref-type="bibr" rid="CR8">8</xref>] is a common filter that yields a good combination of localization properties and speed of computation&#xA0;[<xref ref-type="bibr" rid="CR38">38</xref>], and we thus use it here. In our tests, we found that using the Daubechies-4 filter to expand a 64 &#xD7; 64 block into four scale levels (preferentially choosing the highest-energy subband at each level) produced acceptable results for generating a large number of texture features (i.e., the &#x201C;energy&#x201D; levels of each channel at each level of expansion) in a relatively short period of time.</p>
        </sec>
        <sec id="Sec19">
          <title>Summary of feature extraction procedure</title>
          <p>To summarize, we extract a total of 54 texture features for each child image block, including five GLCM-derived or &#x201C;Haralick&#x201D; features, five lacunarity features, three gray-level morphology features (<italic>N</italic><sub>1</sub>, <italic>N</italic><sub>2</sub>, and granulometry), twenty-five LSE-estimated MRF properties, and sixteen energy features computed by packet wavelet analysis. In our tests, using feature extraction implementation binaries (compiled for Linux) as provided in&#xA0;[<xref ref-type="bibr" rid="CR29">29</xref>], a typical frieze-expanded &#x201C;parent&#x201D; image yields a full set of texture features (54 features per 64 &#xD7; 64 child block image, computed for approximately 50&#x2013;60 blocks depending, for a total of about 3,000 features per parent image) in about 15&#x2013;20 s. Figure&#xA0;<xref rid="Fig13" ref-type="fig">13</xref> shows an example of the values of features extracted from two images representative of obviously different textures.
<fig id="Fig13"><label>Fig.&#xA0;13</label><caption><p>Examples of actual values of texture features extracted from two different image blocks taken from the same frieze-expanded parent image</p></caption><graphic xlink:href="11042_2010_638_Fig13_HTML" id="d28e1593"/></fig></p>
          <p>Once the 54 features are extracted across all 3 &#xD7; 3 neighborhoods centered on each of the &#x201C;central blocks,&#x201D; we re-shape the resulting feature matrix into a set of nine-element vectors, with each vector corresponding to the same texture descriptor extracted across the nine blocks making up each 3 &#xD7; 3 neighborhood (see Fig.&#xA0;<xref rid="Fig14" ref-type="fig">14</xref>). The values in each nine-element vector are sorted in ascending order, somewhat resembling a &#x201C;cumulative distribution&#x201D; or other sigmoidal curve, though such a description is given for illustration purposes only. The reason for the sorting of the 9-element vectors is to facilitate the calculation of the degree of <italic>betweenness</italic> of a &#x201C;query vector&#x201D; with respect to two &#x201C;boundary vectors&#x201D; that are determined in the training phase, described below in Section&#xA0;<xref rid="Sec22" ref-type="sec">3.4.2</xref>.
<fig id="Fig14"><label>Fig.&#xA0;14</label><caption><p>Illustration describing how the feature matrix extracted from each frieze-expanded image is reshaped into a series of sorted nine-element feature vectors, one for each feature-neighborhood correspondence</p></caption><graphic xlink:href="11042_2010_638_Fig14_HTML" id="d28e1613"/></fig></p>
        </sec>
      </sec>
      <sec id="Sec20">
        <title>Feature subset selection and classifier training</title>
        <sec id="Sec21">
          <title>Overview of phenotype and artifact annotation concepts</title>
          <p>One of the goals of the SHIRAZ system is that it will be able to automatically annotate the phenotype observed in a larval zebrafish histology image based on its similarity to classes of images that have previously been characterized. In practice, however, most larval histology images will contain some sort of artifact that arises during the slide preparation or digitization processes. Typical artifacts may include separation or tearing of cell layers that result from sectioning errors, or the tissue appearance being grossly distorted as a result of poor fixation, or that sectioning may have occurred at a skewed angle (producing a so-called &#x201C;eccentric&#x201D; section) because the larval specimen was not properly oriented during the embedding process. A trained observer may be able to account for these artifacts when scoring a histological phenotype, but in certain cases, the artifact may in fact obfuscate the underlying phenotype, and the affected section should not be used. We have, therefore, elected to allow the annotation of larval histology images based not only on a detected phenotype, but also on the detection of artifacts that indicate errors in histological preparation. For the purposes of our demonstration, we have chosen a total of ten possible labels or &#x201C;concepts&#x201D; for annotation of images of the larval zebrafish retina:
<list list-type="order"><list-item><p>Phenotype&#x2014;absent (i.e., normal or &#x201C;wild-type&#x201D;)</p></list-item><list-item><p>Phenotype&#x2014;necrosis&#x2014;minimal to mild</p></list-item><list-item><p>Phenotype&#x2014;necrosis&#x2014;moderate to severe</p></list-item><list-item><p>Phenotype&#x2014;disorganization&#x2014;minimal to mild</p></list-item><list-item><p>Phenotype&#x2014;disorganization&#x2014;moderate to severe</p></list-item><list-item><p>Phenotype&#x2014;possible hypotrophy</p></list-item><list-item><p>Artifact&#x2014;lens detachment or similar tissue &#x201C;shrinkage&#x201D; artifact</p></list-item><list-item><p>Artifact&#x2014;lens is abnormal in appearance (dark, small, &#x201C;chattered,&#x201D; or undetectable, but can possibly indicate a malformation phenotype)</p></list-item><list-item><p>Artifact&#x2014;specimen appears to have fixation problems</p></list-item><list-item><p>Optic nerve or hyaloid artery appears to be present (neither a phenotype nor an artifact)</p></list-item></list></p>
          <p>Examples of potential phenotype and artifact annotation labels are given in Fig.&#xA0;<xref rid="Fig15" ref-type="fig">15</xref>. For feature selection and classification, we wanted to allow for an image to be annotated with multiple labels when appropriate and possible. For example, a retinal image may exhibit a normal phenotype, but the lens may happen to be detached due to a so-called &#x201C;shrinkage&#x201D; artifact resulting from improper preparation of the tissue sample.
<fig id="Fig15"><label>Fig.&#xA0;15</label><caption><p>Typical example images spanning the range of histological phenotypes and artifacts that SHIRAZ is trained to recognize</p></caption><graphic xlink:href="11042_2010_638_Fig15_HTML" id="d28e1696"/></fig></p>
          <p>For training, we do not assign labels to the parent images as a whole, but rather to the &#x201C;child&#x201D; block images&#x2014;or more specifically, to each of the &#x201C;central&#x201D; child blocks and their eight-connected neighbor blocks. For example, let us say that a parent image has been &#x201C;divided&#x201D; into three rows of 19 blocks each (for a total of 57 blocks); the &#x201C;central&#x201D; child blocks would consist of the set of 17 blocks that are not located along the perimeter (or, put another way, the set of blocks in the middle row, with exception of the two end blocks). Each of the 17 central blocks has eight neighbor blocks, and these neighborhoods will overlap from one central block to the next. Just as we allow the blocks themselves to overlap with each other to provide a more continuous representation of image texture (as in Fig.&#xA0;<xref rid="Fig11" ref-type="fig">11</xref>), the overlapping of the 3 &#xD7; 3 neighborhoods also contributes to a model of such a &#x201C;texture continuum,&#x201D; as described above and illustrated in Fig.&#xA0;<xref rid="Fig14" ref-type="fig">14</xref>.</p>
          <p>An additional benefit to extracting features nine blocks at a time in a 3 &#xD7; 3 fashion is that we can preserve, to a certain extent, the location-specific context of the image texture information. From a biological point of view, this enables us to discriminate between different degrees of abnormality that may only manifest themselves in local texture aberrations. For example, an image depicting &#x201C;minimal to mild necrosis&#x201D; might exhibit a texture anomaly in only one or two blocks in a nine-block neighborhood, while an image of &#x201C;moderate to severe necrosis&#x201D; might exhibit similar texture anomalies across a greater number of blocks.</p>
        </sec>
        <sec id="Sec22">
          <title>Determination of &#x201C;feature signatures&#x201D; for each class</title>
          <p>During the training phase, a set of classification &#x201C;rules&#x201D; are identified for each annotation concept. We refer to a given concept&#x2019;s collective set of &#x201C;rules&#x201D; as its <italic>feature signature,</italic> which consists of the subset of 54 features (along with the upper and lower boundaries of the values of such features in the subset) which have been determined to be, statistically speaking, more enriched within that concept than in the other concepts.</p>
          <p>For each feature &#x3A6; we compute the <italic>pairwise information agreement score S</italic> between any two image neighborhoods <italic>i</italic> and <italic>j</italic> as follows:
<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M32">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ S(i,j,\Phi) \equiv \log_2{\mathcal{N} \over {\displaystyle\sum\limits_{k=1}^{\mathcal{N}}}\Bigg[\displaystyle\bigcap\limits_{v=1}^{9}\bigl[\min(s_{{\Phi}iv},s_{{\Phi}jv}) \leq s_{{\Phi}kv} \leq \max(s_{{\Phi}iv},s_{{\Phi}jv})\bigr]_{i{\neq}j}\Bigg]} $$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula>where <italic>s</italic> refers to a single element of the <italic>sorted</italic> (in ascending order) nine-element vector extracted for a given 3 &#xD7; 3 neighborhood. The expression min (<italic>s</italic><sub>&#x3A6;<italic>iv</italic></sub>, <italic>s</italic><sub>&#x3A6;<italic>jv</italic></sub>) (or, respectively, max (<italic>s</italic><sub>&#x3A6;<italic>iv</italic></sub>, <italic>s</italic><sub>&#x3A6;<italic>jv</italic></sub>)) means that we are taking the lesser (greater) of the two values contributed by the feature vector for the neighborhood pair (<italic>i</italic>, <italic>j</italic>) at element position <italic>v</italic>. Strictly speaking, the use of the intersection notation (&#x2229;) would require that all nine elements of the feature vector <italic>s</italic><sub>&#x3A6;<italic>kv</italic></sub> must lie between the min and max &#x201C;boundary vectors&#x201D; defined by neighborhoods <italic>i</italic> and <italic>j</italic>, but in practice we allow for up to two of the elements of the feature vector for neighborhood <italic>k</italic> to fall outside of these limits (similar to the example shown in Fig.&#xA0;<xref rid="Fig16" ref-type="fig">16</xref>). <inline-formula id="IEq16"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal{N}$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq16.gif"/></alternatives></inline-formula> refers to the total number of neighborhoods in the entire database.
<fig id="Fig16"><label>Fig.&#xA0;16</label><caption><p>Illustration of agreement test for a nine-element &#x201C;query vector&#x201D; (that is, corresponding to a given 3 &#xD7; 3 child block neighborhood, as in Fig.&#xA0;<xref rid="Fig12" ref-type="fig">12</xref>) and its degree of match to the statistically determined <italic>feature signature</italic> corresponding to that texture feature (see Sections&#xA0;<xref rid="Sec22" ref-type="sec">3.4.2</xref> and <xref rid="Sec23" ref-type="sec">3.4.3</xref>). In practice, we allow for up to two elements to be outside the boundaries before the query vector fails the &#x201C;betweenness test &#x201D; for that texture feature</p></caption><graphic xlink:href="11042_2010_638_Fig16_HTML" id="d28e1825"/></fig></p>
          <p>The above formula is derived from the so-called &#x201C;loss-of-function agreement score&#x201D; previously been proposed by Weirauch et al.&#xA0;[<xref ref-type="bibr" rid="CR39">39</xref>] for characterizing the frequency of phenotypic presence (and/or absence) among a set of genetic mutants. Our variant allows for the possibility of quantitative feature values to be used in the pairwise comparisons of feature vectors, unlike the original metric, which requires that the phenotypic presence be expressed as a logical (that is, the phenotype is simply &#x201C;present&#x201D; or &#x201C;absent&#x201D;). Mathematically, however, both the above equation and the original metric by Weirauch et al. are based on the concept of the <italic>inverse document frequency</italic> used in information retrieval&#xA0;[<xref ref-type="bibr" rid="CR33">33</xref>].</p>
          <p>The probability computed from the cumulative hypergeometric distribution function may be used to determine the extent to which a given feature &#x3A6; is enriched in the so-called &#x201C;feature signature&#x201D; for a class (i.e., the subset of features that tends to belong to a particular class, but not to others). Generally speaking, for each feature &#x3A6;, we want to see if the &#x201C;random sampling&#x201D; (without replacement) of <italic>N</italic> elements contains at least <italic>x</italic> elements that meet some pre-specified criterion for &#x201C;success.&#x201D; <italic>M</italic> refers to the total number of elements in the whole population, while <italic>K</italic> refers to the number of elements in the whole population that also meet the criterion for &#x201C;success.&#x201D; The probability of the event <italic>X</italic>&#x2009;&#x2265;&#x2009;<italic>x</italic> (i.e., that at least <italic>x</italic> samples drawn meet the &#x201C;success&#x201D; criterion) is computed as
<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M34">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\label{eq15} P(X \geq x) = 1 - P\bigl(X \leq (x - 1)\bigr) = 1 - \sum\limits_{i=0}^{x-1} {{\dbinom{K}{i}\dbinom{M - K}{N - i}}\over\dbinom{M}{N}} $$\end{document}</tex-math><graphic xlink:href="11042_2010_638_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula></p>
          <p>The resulting <italic>P</italic>-value tells us the probability that the <italic>x</italic> successes are due to random chance. If we compute this <italic>P</italic>-value for every feature &#x3A6; and for a given class, then those features with a significant <italic>P</italic>-value (typically &#x2264; 0.05, but can be adjusted for stringency) would be considered part of the subset or <italic>signature</italic> of features that are enriched in that class (and which are less likely to be observed in other classes).</p>
        </sec>
        <sec id="Sec23">
          <title>Training procedure</title>
          <p>Here we summarize the steps involved in training the SHIRAZ annotation system:
<list list-type="order"><list-item><p>Divide each frieze-expanded parent image into 64 &#xD7; 64 child block images, then capture vector of 54 texture features for the 3 &#xD7; 3 neighborhood about each of the <italic>central</italic> child blocks.</p></list-item><list-item><p>Sort each neighborhood&#x2019;s feature vector as described in Section&#xA0;<xref rid="Sec18" ref-type="sec">3.3.6</xref>, then merge all feature vectors into a &#x201C;feature matrix&#x201D; <inline-formula id="IEq17"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal{M}$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq17.gif"/></alternatives></inline-formula>, which will have a total of (54 &#xD7; 9&#x2009;=&#x2009;) 486 features for each row (corresponding to the neighborhood about each of the central child block images). Each neighborhood <italic>i</italic> of <inline-formula id="IEq18"><alternatives><tex-math id="M36">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal{M}$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq18.gif"/></alternatives></inline-formula> is then labeled with a relevant histological phenotype/artifact annotation concept <italic>&#x3BA;</italic>.</p></list-item><list-item><p>Determine the <italic>feature signature</italic><italic>&#x3A8;</italic><sub><italic>&#x3BA;</italic></sub> associated with a given labeled concept <italic>&#x3BA;</italic>, as follows:
<list list-type="order"><list-item><p>Compute the pairwise information agreement score (using (<xref rid="Equ14" ref-type="">14</xref>)) for neighborhoods <italic>i</italic> and <italic>j</italic> and store as <italic>S</italic>(<italic>i</italic>, <italic>j</italic>, &#x3A6;). The total size of <italic>S</italic> is <inline-formula id="IEq19"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal{N} \times \mathcal{N} \times \mathcal{F}$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq19.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq20"><alternatives><tex-math id="M38">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal{F}$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq20.gif"/></alternatives></inline-formula> is the total number of features = 54.</p></list-item><list-item><p>In each symmetric matrix <italic>S</italic><sub>&#x3A6;</sub> (that is, the square <inline-formula id="IEq21"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal{N} \times \mathcal{N}$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq21.gif"/></alternatives></inline-formula> matrix corresponding to each feature &#x3A6;), each column <italic>S</italic><sub><italic>i</italic>,&#x3A6;</sub> corresponds to all <italic>pairs</italic> of neighborhoods containing neighborhood <italic>i</italic>, each of which has been assigned a specific annotation concept. Thus, we repeat the following steps for each feature &#x3A6; and for each concept <italic>&#x3BA;</italic>, while ignoring all pairwise self-comparisons (<italic>i</italic>, <italic>i</italic>):
<list list-type="order"><list-item><p>Determine a minimum agreement score value <italic>S</italic><sub>min</sub> (e.g., 80th or 90th percentile of all values in <italic>S</italic><sub>&#x3A6;</sub>) required for any two neighborhoods <italic>i</italic> and <italic>j</italic> to be considered &#x201C;similar.&#x201D;</p></list-item><list-item><p>Let <italic>x</italic> = the number of neighborhood pairs in <italic>S</italic><sub><italic>i</italic>,&#x3A6;</sub> that belong to annotation concept <italic>&#x3BA;</italic> but for which the criterion <italic>S</italic>(<italic>i</italic>, <italic>j</italic>, &#x3A6;)&#x2009;&#x2265;&#x2009;<italic>S</italic><sub>min</sub> has been met</p></list-item><list-item><p>Let <italic>K</italic> = the number of neighborhood pairs in <italic>S</italic><sub><italic>i</italic>,&#x3A6;</sub> where <italic>S</italic>(<italic>i</italic>, <italic>j</italic>, &#x3A6;)&#x2009;&#x2265;&#x2009;<italic>S</italic><sub>min</sub></p></list-item><list-item><p>Let <italic>N</italic> = the number of neighborhood pairs in <italic>S</italic><sub><italic>i</italic>,&#x3A6;</sub> that belong to annotation concept <italic>&#x3BA;</italic>, regardless of the value of <italic>S</italic>(<italic>i</italic>, <italic>j</italic>, &#x3A6;)</p></list-item><list-item><p>Let <italic>M</italic> = the <italic>total</italic> number of neighborhood pairs in <italic>S</italic><sub><italic>i</italic>,&#x3A6;</sub></p></list-item><list-item><p>Use <italic>x</italic>, <italic>K</italic>, <italic>N</italic>, and <italic>M</italic> as inputs to the cumulative hypergeometric distribution function (<xref rid="Equ15" ref-type="">15</xref>) to determine the probability that the number of samples in the class that meet the criteria of interest is due only to random chance (i.e., the <italic>P</italic>-value). Note that the &#x201C;random sample&#x201D; (that is, the set of <italic>N</italic> elements selected from <italic>S</italic><sub><italic>i</italic>,&#x3A6;</sub> above) is not actually random, but corresponds to those samples that have been labeled with the current annotation concept <italic>&#x3BA;</italic> whose feature signature <italic>&#x3A8;</italic><sub><italic>&#x3BA;</italic></sub> we are trying to determine.</p></list-item><list-item><p>If a minimum fraction (we used 80%) of all <italic>P</italic>-values computed for each class sample are significant (e.g., <italic>P</italic>&#x2009;&#x2264;&#x2009;0.05), then assign the current feature to the <italic>feature signature</italic><italic>&#x3A8;</italic><sub><italic>&#x3BA;</italic></sub> for the concept <italic>&#x3BA;</italic>.</p></list-item><list-item><p>Determine the <italic>betweenness rules</italic> that must be met for a <italic>new</italic> test sample neighborhood to be assigned with the current concept (see below). In other words, if the current feature &#x3A6; belongs to the current annotation concept&#x2019;s feature signature <italic>&#x3A8;</italic><sub><italic>&#x3BA;</italic></sub>, then we let <italic>&#x3A8;</italic><sub><italic>&#x3BA;</italic></sub>(&#x3A6;)<sub>max</sub> and <italic>&#x3A8;</italic><sub><italic>&#x3BA;</italic></sub>(&#x3A6;)<sub>min</sub> equal, respectively, vectors containing the maximum and minimum values with respect to the set of the original nine-element vectors corresponding to the neighborhoods belonging to the current annotation concept <italic>&#x3BA;</italic>.</p></list-item></list></p></list-item></list></p></list-item></list></p>
        </sec>
        <sec id="Sec24">
          <title>Automated image annotation</title>
          <p>A previously uncharacterized eye image <italic>I</italic> will be annotated as follows:
<list list-type="order"><list-item><p>Extract the image features for each 3 &#xD7; 3 child block image neighborhood <italic>i</italic> in the parent image <italic>I</italic>. Let <italic>X</italic><sub><italic>i</italic>,&#x3A6;</sub> = the nine-element vector (sorted in ascending order) for each feature.</p></list-item><list-item><p>Initialize <italic>&#x3A8;</italic><sub><italic>&#x3BA;</italic>,matches</sub>&#x2009;=&#x2009;0</p></list-item><list-item><p>For each child image neighborhood in <italic>I</italic>, for each annotation concept <italic>&#x3BA;</italic>, and for each feature/phenotype &#x3A6;:
<list list-type="order"><list-item><p>If seven out of nine elements in <italic>X</italic><sub><italic>i</italic>,&#x3A6;</sub> fall within the betweenness limit vectors <italic>&#x3A8;</italic><sub><italic>&#x3BA;</italic></sub>(&#x3A6;)<sub>max</sub> and <italic>&#x3A8;</italic><sub><italic>&#x3BA;</italic></sub>(&#x3A6;)<sub>min</sub> of the current annotation concept&#x2019;s feature signature <italic>&#x3A8;</italic><sub><italic>&#x3BA;</italic></sub>, then increment <italic>&#x3A8;</italic><sub><italic>&#x3BA;</italic>,matches</sub> by 1</p></list-item><list-item><p>Let <italic>n</italic><sub><italic>&#x3BA;</italic></sub> = the total number of features in the feature signature for concept <italic>&#x3BA;</italic></p></list-item><list-item><p>Let <italic>&#x3A6;</italic><sub><italic>&#x3BA;</italic></sub> = the normalized fraction of matches computed over all features in the feature signature <italic>&#x3A8;</italic><sub><italic>&#x3BA;</italic></sub> for the current neighborhood = <inline-formula id="IEq22"><alternatives><tex-math id="M40">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\Psi_{\kappa,{\rm matches}} \over n_\kappa^2$\end{document}</tex-math><inline-graphic xlink:href="11042_2010_638_Article_IEq22.gif"/></alternatives></inline-formula> (Note: the denominator term is squared in order to account for the variability in the number of features included in each concept&#x2019;s feature signature and to mitigate the effect of any annotation bias towards concepts that have signatures with a greater number of features than other concepts)</p></list-item></list></p></list-item><list-item><p>For each neighborhood, we take the list of matching concepts and sort from highest to lowest <italic>&#x3A6;</italic><sub><italic>&#x3BA;</italic></sub> score.</p></list-item><list-item><p>Let &#x3A9; = a matrix in which each row (corresponding to each neighborhood in <italic>I</italic>) contains a list of concept numbers (from 1 to <italic>&#x3BA;</italic>), sorted according to the corresponding <italic>&#x3A6;</italic><sub><italic>&#x3BA;</italic></sub> values for each concept. (In other words, the concept number in column 1 will be that which has the highest <italic>&#x3A6;</italic><sub><italic>&#x3BA;</italic></sub> score for the current neighborhood.)</p></list-item><list-item><p>For each concept <italic>&#x3BA;</italic>, count the number of times in which the number of the concept appears in each column &#x3A9; and multiply times a pre-specified &#x201C;voting factor&#x201D; for that column. (In our case, any &#x201C;vote&#x201D; for a concept in column 1 gets multiplied by 10, any &#x201C;vote&#x201D; in column 2 gets multiplied by 9, column 3 by 8, and so on.)</p></list-item><list-item><p>Report the results as a ranked list of annotation concepts for each image <italic>I</italic>, where the the first reported annotation is that which has the highest number of &#x201C;votes&#x201D; as totaled across all neighborhoods. (For compactness and for evaluation purposes, only the three highest-ranking annotations are reported in our online demo.)</p></list-item></list></p>
        </sec>
      </sec>
    </sec>
    <sec id="Sec25">
      <title>Results</title>
      <p><bold>Dataset</bold> A total of 176 images of larval zebrafish eyes were manually extracted from 20X virtual slides generated by the Zebrafish Functional Imaging Core facility at the Penn State College of Medicine. 100 images were used for training the ten concepts described earlier in this paper, with the remaining 76 to be reserved for testing.</p>
      <p><bold>Online demo</bold> A Web-based demonstration site for SHIRAZ is available through our project Website, <ext-link ext-link-type="uri" xlink:href="http://shiraz.ist.psu.edu">http://shiraz.ist.psu.edu</ext-link>. Figure&#xA0;<xref rid="Fig17" ref-type="fig">17</xref> provides a screenshot of the SHIRAZ online demo homepage. The system has two modes of interactivity. In the first mode (which we will refer to as &#x201C;Pre-extracted Eye Mode&#x201D;), the user chooses to have the system process and annotate one of the pre-extracted eye images. In the second mode (&#x201C;Array Mode&#x201D;), SHIRAZ will directly process an original 20X virtual slide consisting of an array of specimen images. Because the eye images to be annotated have not been previously extracted, the Array Mode requires several additional processing steps. For the purposes of the online demo, many of these steps have been preprocessed offline due to their duration (e.g., using the VIPS image processing suite for rotation and area extraction of the high-resolution images at 20&#xD7; magnification can take several minutes, if not longer).
<fig id="Fig17"><label>Fig.&#xA0;17</label><caption><p>Screenshot of entry point to SHIRAZ Web-based demo site</p></caption><graphic xlink:href="11042_2010_638_Fig17_HTML" id="d28e2486"/></fig></p>
      <p><bold>Performance evaluation of the SHIRAZ &#x201C;Pre-extracted Eye Mode&#x201D;</bold> We report the quality of a given annotation as one of three possibilities: <italic>Correct</italic>, <italic>Acceptable</italic>, or <italic>Incorrect</italic>. A <italic>correct</italic> annotation means that the predicted annotation matches at least one of the ground truth we had previously assigned for that image. An <italic>acceptable</italic> annotation means that the predicted annotation is at least partially correct; for example, if SHIRAZ assigns &#x201C;necrosis - minimal to mild&#x201D; to an image, but the ground truth annotation is actually &#x201C;necrosis - moderate to severe,&#x201D; we consider this an <italic>acceptable</italic> annotation in the sense that it correctly identified the phenotype as &#x201C;necrosis,&#x201D; although it did not correctly identify the <italic>degree</italic> of the phenotype severity. Finally, an <italic>incorrect</italic> annotation means that the predicted annotation had no overlap with any of the ground truth annotations assigned to the given image.Out of the 76 images tested, 73 images produced at least one <italic>correct</italic> annotation (among the three highest-ranking annotations reported), for an accuracy of 96%, with the figure improving to 99% (75 images) if we base the accuracy on whether the system predicted at least one <italic>acceptable</italic> annotation. However, if we only look at the top-ranking annotation for each image, then the accuracy is lower, with 34 out of 76 images (45%) producing a <italic>correct</italic> annotation, although the figure improves to 47 out of 76 images (62%) if the predicted annotation for each image is at least <italic>acceptable.</italic> A selection of several example input test images and their three highest-ranking predicted annotations are shown in Fig.&#xA0;<xref rid="Fig18" ref-type="fig">18</xref>.
<fig id="Fig18"><label>Fig.&#xA0;18</label><caption><p>Selected images and their three highest-ranking annotations as predicted by SHIRAZ, with degree of correctness of the annotation given in parentheses. Correct annotations are shown in <bold>boldface</bold>, with incorrect annotations shown in <italic>italics</italic></p></caption><graphic xlink:href="11042_2010_638_Fig18_HTML" id="d28e2548"/></fig>It should be noted that the ground truth phenotypes were originally assigned based on the original microscope slides at magnifications that may have been much higher than the 20X digital images used in training. Although SHIRAZ was trained to associate its annotation terms with the relatively low detail inherent in these images, a human expert may not necessarily be able to reproduce or otherwise validate the SHIRAZ system&#x2019;s predicted annotation results without viewing finer details visible only in higher-magnification digital images or the original glass slides themselves. A potential representative example is the upper-right image of Fig.&#xA0;<xref rid="Fig18" ref-type="fig">18</xref>, where <italic>necrosis&#x2014;moderate to severe</italic> was the top-ranking annotation. This result was evaluated as correct because it matched the ground truth, but one must keep in mind that the visibility of the necrotic tissue may be compromised by the lower resolution of the 20X digital image as compared to the original glass slide (or perhaps a higher-magnification image, say 40&#xD7;).</p>
      <p><bold>Performance evaluation of the SHIRAZ &#x201C;Array Mode&#x201D;</bold> There are two aspects involved in judging the performance of the &#x201C;Array Mode&#x201D;&#x2014;first, we must examine how well the system can reliably extract the correct positions of the eye images from the original array image (here shown in Fig.&#xA0;<xref rid="Fig19" ref-type="fig">19</xref>a). We can see from the results of the automated lattice detection (Fig.&#xA0;<xref rid="Fig19" ref-type="fig">19</xref>b) that the positions of the whole-larva specimens have been fully identified. The screenshot shown in Fig.&#xA0;<xref rid="Fig19" ref-type="fig">19</xref>c shows the corresponding left- and right-eye images automatically cropped out from the whole-larva specimens shown in the previous figure. Each image is marked with the row-and-column position as determined in the lattice detection step and which enables us to match each specimen to its corresponding entry in our laboratory&#x2019;s embedding record. When possible to distinguish mutants from wild-type by dissecting microscopy, we embed larvae such that each column contains either <italic>all</italic> wild-type (normal) or <italic>all</italic> mutant specimens, and to arrange the columns such that a column of wild-type specimens is directly adjacent to a column of mutant specimens. For the larval array shown here, the first and third columns contain wild-types, and the second and fourth columns contain mutants. Note here that many of the extracted eye images do not contain any intact eye tissue (such as in positions R2C3 or R7C4 in Fig.&#xA0;<xref rid="Fig19" ref-type="fig">19</xref>); this is due to the fact that when zebrafish larvae are embedded in a tissue block, they do not always embed at the same depth or axial rotation angle (or there may be no specimen embedded at all, such as in position R7C2 and below). When this occurs, it is best to phenotype larvae using the best available histological specimen, which may be found in an adjacent section on a different slide. For the current array, we see several instances of apparently missing eyes because SHIRAZ locates eyes based on position only (that is, position relative to an &#x201C;origin point,&#x201D; here chosen to be the leftmost pixel along the midline of the bounding box containing the whole larval specimen). Nonetheless, even when only using position-based training, the system does locate and extract all <italic>available</italic> eyes that are evident in the current array image.
<fig id="Fig19"><label>Fig.&#xA0;19</label><caption><p>An example walkthrough of the SHIRAZ &#x201C;Array mode.&#x201D; Following the eye image extraction step, the user can choose one of the eye images for phenotypic annotation and retrieval of similar images</p></caption><graphic xlink:href="11042_2010_638_Fig19_HTML" id="d28e2592"/></fig>The second aspect of performance evaluation involves how precisely SHIRAZ was able to annotate the available eye images that were extracted from the original array. As shown in Fig.&#xA0;<xref rid="Fig19" ref-type="fig">19</xref>, the demo is presently designed to annotate one user-selected image at a time, accomplished by selecting the eye image&#x2019;s corresponding radio button and then clicking on the button marked &#x201C;Process Selected Specimen.&#x201D; From this point on, the demo functions the same way as in the &#x201C;Pre-extracted Eye Mode.&#x201D;) For selected eye images depicting &#x201C;ideal&#x201D; histological sections (where the tissue is mostly intact and contains either a lens or at least a space from which a lens was obviously detached during sectioning), the system performs well and annotates images with meaningful and accurate tags. In our tests of 17 such images, all yielded at least one <italic>correct</italic> annotation, although only six of these images yielded a <italic>correct</italic> or at least <italic>acceptable</italic> result for the top-ranking prediction. Many phenotypically normal images were mistakenly assigned a top-ranking annotation of &#x201C;hypotrophy&#x2013;present.&#x201D; Upon inspection of the incorrectly annotated images, we attribute such errors in most cases to the lack of appropriate training examples that properly distinguish <italic>mild</italic> hypotrophy from the more severe examples of hypotrophy (for which we had several training examples). It is also possible that, because the eye images here were extracted <italic>automatically</italic> and not chosen on the basis of being <italic>ideal</italic> sections (as in the &#x201C;Pre-extracted Eye Mode&#x201D;), what appears to be hypotrophy here could simply be attributed to the section being from an incorrect angle or tissue block depth. In any case, SHIRAZ is not yet trained to recognize whether a given section is &#x201C;ideal&#x201D; (i.e., sectioned at an appropriate angle and depth with the tissue integrity preserved) but we expect to build this crucial functionality into a future version.</p>
    </sec>
    <sec id="Sec30">
      <title>Discussion and future work</title>
      <p>Because of its potential to save thousands of person-hours of work that would be otherwise spent on manual image processing and subjective annotation and scoring, we believe that SHIRAZ can have a major impact on the long-term success of the Zebrafish Phenome Project. As a proof-of-principle, SHIRAZ performs at a high enough level to be immediately useful as a &#x201C;triage system&#x201D;&#x2014;that is, a tool for automatically screening out obvious phenotypes as well as flagging slides containing artifacts that could potentially obfuscate the underlying phenotypes. However, its use in the phenome project would require much greater speed of scanning and computational power. If SHIRAZ is set to be sensitive enough, a human reviewer could use the annotation results as a &#x201C;first pass&#x201D; before delving into a detailed manual evaluation of all slides, or if too many artifacts are present to permit the scoring of phenotypes, then the slides could be marked as such and thus should be replaced with slides with a higher fraction of specimens bearing informative histological features.</p>
      <p>Because of the pairwise calculations required, the training phase is computationally complex, with the speed of training being proportional to (<italic>n</italic><sup>2</sup>), where <italic>n</italic> is the number of all 3 &#xD7; 3 neighborhoods for which an annotation concept was manually assigned. For training on a relatively small number of concepts and neighborhoods, this is not a significant problem, though the process is slow enough that validation methods requiring repeated training (such as leave-one-out cross validation) were not feasible at this time. Because the slowest step in the training process proceeds by one feature at a time (in our tests, each feature required about 4&#x2013;5 min for training), this step could potentially be parallelized so that each feature is processed simultaneously on its own individual cluster node.</p>
      <p>SHIRAZ is a <italic>region-based</italic> image retrieval method, meaning that if different regions within the same query image correspond to prototype images representing semantic concepts or other forms of scoring and annotation, then the query image may be annotated with more than one such concept. However, it should be noted that the annotation results shown herein are evaluated based on their applicability to the <italic>whole</italic> &#x201C;parent&#x201D; image. To a highly-trained expert, an image that is &#x201C;locally necrotic&#x201D; in one area of the image may also potentially appear &#x201C;locally normal&#x201D; in other areas of the image, but for the purposes of demonstration and for compactness in representation, the annotations are ranked according to their <italic>global</italic> relevance, and for evaluation purposes, if an image bears a ground truth annotation of any abnormal phenotype, then the entire image is assumed to be &#x201C;globally abnormal,&#x201D; and so any predictions of &#x201C;phenotype - normal&#x201D; are assumed incorrect. We may, in a future version of the system, report the annotations on a local (i.e., 3 &#xD7; 3 neighborhood-specific) basis as well, so that histological features that only occupy a small region of the whole image&#x2014;such as single-cell necrosis or the presence of the optic nerve and/or hyaloid artery&#x2014;are not &#x201C;eclipsed&#x201D; by histological features (whether they be phenotypic, or artificial, or both) that may otherwise predominate the image as a&#xA0;whole.</p>
      <p><bold>Advantages of the SHIRAZ system</bold> The feature extraction methods are invariant to the orientation of the eye image, mainly because the features are not extracted from the eye image itself, but on its frieze-expanded counterpart image. In addition, the similarity metric used here is unique in that the extent of agreement is not merely based on the &#x201C;distance&#x201D; between images, but rather on the <italic>infrequency</italic> or <italic>rarity</italic> of the information that the images, based on the probability that the information being shared is either due to random chance or is based on a meaningful and statistically-significant association. For example, because the system here was trained and tested only on images of the 5dpf zebrafish eye, most of the images will share a large number of features (and values) regardless of the degree of phenotype abnormality or the presence of an artifact. For example, the feature signature for the <italic>phenotype&#x2013;normal</italic> concept contains only three features; this is because the values of the features extracted for the images in that class were too dispersed and thus &#x201C;overlapped&#x201D; with the distributions of values belonging to images outside of that class. However, the range of feature values that do belong to the <italic>phenotype&#x2013;normal</italic> class have been determined, using the statistical methods described herein, to be at least reasonably discriminative.</p>
      <sec id="Sec32">
        <title>Future work</title>
        <p>To be useful as a completely automated, closed-loop system for high-throughput phenotyping, however, SHIRAZ will require improvements in accuracy, precision, and speed.</p>
        <p><bold>Improvements in ground truth annotation accuracy and precision</bold> The SHIRAZ system&#x2019;s ability to correctly annotate images depends on a number of factors, such as the accuracy of segmentation of the retina from the surrounding tissues and the selection of certain model parameters (such as the cumulative hypergeometric function&#x2019;s <italic>P</italic>-value threshold, currently set at 0.05). The most important factor, however, is the correctness of the ground truth annotations assigned to each 3 &#xD7; 3 neighborhood of child blocks. Since this is an original, highly specialized, and relatively small data set (as opposed to massive benchmark databases of general photographic images such as the Caltech-101 image set&#xA0;[<xref ref-type="bibr" rid="CR1">1</xref>]), the number of annotation terms is also relatively small and could be improved after review by an independent domain expert. As part of ongoing work on this project, we have enlisted the help of a comparative pathologist who will help us prepare a more comprehensive list of annotation terms that are both more <italic>relevant</italic> and more <italic>precise</italic> than the relatively nonspecific terms we have used in this proof-of-principle demonstration.Imprecise results may also have resulted from choosing prototype images for a given annotation concept that are too dissimilar for SHIRAZ to identify a signature of features that is specific to that class. Certainly, one of the cumulative effects of having a limited set of prototype images for certain annotation concepts in addition to a relatively small set of terms available for phenotypic annotation is that images may yield high-ranking &#x201C;correct&#x201D; annotations, but just because they are &#x201C;correct&#x201D; does not mean they are the most <italic>relevant</italic>. As we see in Fig.&#xA0;<xref rid="Fig18" ref-type="fig">18</xref>e, we have correctly annotated the image in the sense that the hyaloid artery is indeed present, but an annotation such as &#x201C;retinal disorganization&#x201D; would have been more relevant. In addition to making improvements in the quantity and quality of both the annotation terms and their associated prototype images, we are planning a customized relevance feedback system that will allow domain experts (such as comparative/veterinary pathologists as well as organ specialists such as ophthalmologists) to grade the SHIRAZ system&#x2019;s predicted terms according to both correctness and relevance. Such user feedback will be invaluable for periodic re-training of the system while continually adding to the bed of phenotypic knowledge required for the success of the Zebrafish Phenome Project.</p>
        <p><bold>Improvements in computational efficiency</bold> Additional work is also planned for improving the computational performance of SHIRAZ, particularly with respect to increasing overall throughput. For example, rather than process and annotate images in series, one at a time, we can parallelize the code to enable processing of multiple array specimens simultaneously. Parallelization will undoubtedly be necessary when we eventually train SHIRAZ to reliably detect and annotate much finer histological details using higher-magnification (&#x2265;&#x2009;40&#xD7;) virtual slides, which are generally too large for current single-processor architectures to process in a reasonably short period of time&#x2014;something that is essential for a web-based demonstration system. For now, however, even without the benefit of a cluster computing facility (whether shared or owned), the fact that a single image at 20&#xD7; magnification can be automatically annotated in a matter of seconds on a single processor provides a significant step forward in phenotyping efficiency as compared to manual scoring and annotation.</p>
      </sec>
      <sec id="Sec35">
        <title>Concluding remarks</title>
        <p>Our prototype system is presently designed to recognize a mere fraction of the histological phenotypes that are required for complete phenotypic characterization of a given zebrafish genetic mutant. Future versions of SHIRAZ and related automated phenotyping systems must be trained to score a multitude of phenotypes at various levels of detail and in both local and global contexts. While here we have focused on morphological phenotypes (and only in the context of the developing eye, and then only at a &#x201C;global&#x201D; detail level), the Zebrafish Phenome Project will also require the study of physiological and behavioral phenotypes. We will continue to develop and refine SHIRAZ, but in the meantime, we hope that the successful proof-of-concept demonstration provided herein will provide an impetus for others within the biological community to pioneer other high-throughput phenotyping methods.</p>
      </sec>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgements</title>
      <p>The work of B. A. Canada was supported by the Penn State Academic Computing Fellowship as well as the Penn State Clinical and Translational Sciences Institute. This work was also supported by NIH NCRR grant R24 RR01744 for the Zebrafish Atlas, as well as by grants from the Jake Gittlen Cancer Research Foundation and Pennsylvania Tobacco Settlement Funds to K. C. Cheng and a Penn State Graduate fellowship to G. K. Thomas. J. Z. Wang is supported by NSF Grant No. 0347148; in addition, NSF has provided computation infrastructure through Grant Nos. 0219272 and 0821527. The authors wish to thank the following current and former members of the Jake Gittlen Cancer Research Foundation for their invaluable assistance over the course of this project: Steven Peckins, for technical support in the processing of very high resolution images; Kelsey Bauer, for pre-segmentation hand labeling of histological specimens; as well as Jean Copper, Christina Foutz, and Lynn Budgeon for their efforts in preparing and digitizing the source microscope slides. Finally, the authors thank Yanxi Liu (Penn State Department of Computer Science and Engineering) for helpful discussions regarding computational symmetry.</p>
      <p><bold><bold>Open Access</bold></bold> This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.</p>
    </ack>
    <ref-list id="Bib1">
      <title>References</title>
      <ref id="CR1">
        <label>1.</label>
        <mixed-citation publication-type="other">Caltech-101 Image Database. <ext-link ext-link-type="uri" xlink:href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">http://www.vision.caltech.edu/Image_Datasets/Caltech101/</ext-link></mixed-citation>
      </ref>
      <ref id="CR2">
        <label>2.</label>
        <mixed-citation publication-type="other">Canada BA, Thomas GK, Cheng KC, Wang JZ (2007) Automated segmentation and classification of zebrafish histology images for high-throughput phenotyping. In: Proc 3rd IEEE-NIH Life Sci Syst Appl Workshop (LISSA), pp 245&#x2013;248</mixed-citation>
      </ref>
      <ref id="CR3">
        <label>3.</label>
        <mixed-citation publication-type="other">Canada BA, Thomas GK, Cheng KC, Wang JZ, Liu Y (2008) Automatic lattice detection in near-regular histology array images. In: Proc IEEE Int Conf Image Processing (ICIP), pp 1452&#x2013;1455</mixed-citation>
      </ref>
      <ref id="CR4">
        <label>4.</label>
        <mixed-citation publication-type="other">Canada BA, Thomas GK, Cheng KC, Wang JZ, Liu Y (2008) Towards efficient automated characterization of irregular histology images via transformation to frieze-like patterns. In: Proc ACM Int Conf Image Video Retr (CIVR), pp 581&#x2013;590</mixed-citation>
      </ref>
      <ref id="CR5">
        <label>5.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Colquhoun</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Nogueras</surname>
              <given-names>JJ</given-names>
            </name>
            <name>
              <surname>Dipasquale</surname>
              <given-names>B</given-names>
            </name>
            <name>
              <surname>Petras</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Wexner</surname>
              <given-names>SD</given-names>
            </name>
            <name>
              <surname>Woodhouse</surname>
              <given-names>S</given-names>
            </name>
          </person-group>
          <article-title>Interobserver and intraobserver bias exists in the interpretation of anal dysplasia</article-title>
          <source>Dis Colon Rectum</source>
          <year>2003</year>
          <volume>46</volume>
          <fpage>1332</fpage>
          <lpage>1338</lpage>
          <pub-id pub-id-type="doi">10.1007/s10350-004-6744-5</pub-id>
          <pub-id pub-id-type="pmid">14530670</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR6">
        <label>6.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cross</surname>
              <given-names>GC</given-names>
            </name>
            <name>
              <surname>Jain</surname>
              <given-names>AK</given-names>
            </name>
          </person-group>
          <article-title>Markov random field texture models</article-title>
          <source>IEEE Trans Pattern Anal Mach Intell</source>
          <year>1983</year>
          <volume>5</volume>
          <issue>1</issue>
          <fpage>25</fpage>
          <lpage>39</lpage>
          <pub-id pub-id-type="doi">10.1109/TPAMI.1983.4767341</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR7">
        <label>7.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Datta</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Joshi</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>JZ</given-names>
            </name>
          </person-group>
          <article-title>Image retrieval: ideas, influences, and trends of the new age</article-title>
          <source>ACM Comput Surv</source>
          <year>2008</year>
          <volume>40</volume>
          <issue>2</issue>
          <fpage>1</fpage>
          <lpage>60</lpage>
          <pub-id pub-id-type="doi">10.1145/1348246.1348248</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR8">
        <label>8.</label>
        <mixed-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Daubechies</surname>
              <given-names>I</given-names>
            </name>
          </person-group>
          <source>Ten lectures on wavelets</source>
          <year>1992</year>
          <publisher-loc>Philadelphia</publisher-loc>
          <publisher-name>SIAM</publisher-name>
        </mixed-citation>
      </ref>
      <ref id="CR9">
        <label>9.</label>
        <mixed-citation publication-type="other">Demir C, Yener B (2005) Automated cancer diagnosis based on histopathological images: a systematic survey. Department of Computer Science Technical Report TR-05-09, Rensselaer Polytechnic Institute</mixed-citation>
      </ref>
      <ref id="CR10">
        <label>10.</label>
        <mixed-citation publication-type="other">Doyle S, Rodriguez C, Madabhushi A, Tomasezweski J, Feldman M (2006) Detecting prostatic adenocarcinoma from digitized histology using a multi-scale, hierarchical classification approach. In: Proc IEEE Int Conf Eng Med Biol Soc (EMBC), pp 4759&#x2013;4762</mixed-citation>
      </ref>
      <ref id="CR11">
        <label>11.</label>
        <mixed-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Gonzalez</surname>
              <given-names>RC</given-names>
            </name>
            <name>
              <surname>Woods</surname>
              <given-names>RE</given-names>
            </name>
            <name>
              <surname>Eddins</surname>
              <given-names>RE</given-names>
            </name>
          </person-group>
          <source>Digital image processing using MATLAB</source>
          <year>2004</year>
          <publisher-loc>Upper Saddle River</publisher-loc>
          <publisher-name>Pearson Prentice Hall</publisher-name>
        </mixed-citation>
      </ref>
      <ref id="CR12">
        <label>12.</label>
        <mixed-citation publication-type="other">Gou F, Li L, Faloutsos C, Xing EP (2008) C-DEM: a multi-modal query system for drosophila embryo databases. In: Proc 34th Int Conf on Very Large Data Bases (VLDB), pp 1508&#x2013;1511</mixed-citation>
      </ref>
      <ref id="CR13">
        <label>13.</label>
        <mixed-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Gr&#xFC;nbaum</surname>
              <given-names>B</given-names>
            </name>
            <name>
              <surname>Shephard</surname>
              <given-names>GC</given-names>
            </name>
          </person-group>
          <source>Tilings and patterns</source>
          <year>1987</year>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>Freeman</publisher-name>
        </mixed-citation>
      </ref>
      <ref id="CR14">
        <label>14.</label>
        <mixed-citation publication-type="other">Gurcan MN, Kong J, Sertel O, Cambazoglu BB, Saltz J, Catalyurek U (2007) Computerized pathological image analysis for neuroblastoma prognosis. Technical Report OSUBMI_TR_2007_N10, Dept. of Biomedical Informatics, Ohio State University</mixed-citation>
      </ref>
      <ref id="CR15">
        <label>15.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hamilton</surname>
              <given-names>PW</given-names>
            </name>
            <name>
              <surname>Bartels</surname>
              <given-names>PH</given-names>
            </name>
            <name>
              <surname>Thompson</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Anderson</surname>
              <given-names>NH</given-names>
            </name>
            <name>
              <surname>Montironi</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Sloan</surname>
              <given-names>JM</given-names>
            </name>
          </person-group>
          <article-title>Automated location of dysplastic fields in colorectal histology using image texture analysis</article-title>
          <source>J Pathol</source>
          <year>1997</year>
          <volume>182</volume>
          <issue>1</issue>
          <fpage>68</fpage>
          <lpage>75</lpage>
          <pub-id pub-id-type="doi">10.1002/(SICI)1096-9896(199705)182:1&lt;68::AID-PATH811&gt;3.0.CO;2-N</pub-id>
          <pub-id pub-id-type="pmid">9227344</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR16">
        <label>16.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Haralick</surname>
              <given-names>RM</given-names>
            </name>
            <name>
              <surname>Shanmugam</surname>
              <given-names>K</given-names>
            </name>
            <name>
              <surname>Dinstein</surname>
              <given-names>I</given-names>
            </name>
          </person-group>
          <source>IEEE Trans Syst Man Cybern SMC</source>
          <year>1973</year>
          <volume>3</volume>
          <issue>6</issue>
          <fpage>610</fpage>
          <lpage>621</lpage>
          <pub-id pub-id-type="doi">10.1109/TSMC.1973.4309314</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR17">
        <label>17.</label>
        <mixed-citation publication-type="other">Hays JH, Leordeanu M, Efros AA, Liu Y (2006) Discovering texture regularity as a higher-order correspondence problem. In: 9th Eur Conf Comput Vis (ECCV), pp 522&#x2013;535</mixed-citation>
      </ref>
      <ref id="CR18">
        <label>18.</label>
        <mixed-citation publication-type="other">Lee S, Collins RT, Liu Y (2008) Rotation symmetry group detection via frequency analysis of frieze-expansions. In: IEEE Comput Soc Conf Comput Vis Pattern Recognit (CVPR), pp 1&#x2013;8</mixed-citation>
      </ref>
      <ref id="CR19">
        <label>19.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>JZ</given-names>
            </name>
          </person-group>
          <article-title>Automatic linguistic indexing of pictures by a statistical modeling approach</article-title>
          <source>IEEE Trans Pattern Anal Mach Intell</source>
          <year>2003</year>
          <volume>25</volume>
          <fpage>1075</fpage>
          <lpage>1088</lpage>
          <pub-id pub-id-type="doi">10.1109/TPAMI.2003.1227984</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR20">
        <label>20.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>JZ</given-names>
            </name>
          </person-group>
          <article-title>Real-time computerized annotation of pictures</article-title>
          <source>IEEE Trans Pattern Anal Mach Intell</source>
          <year>2008</year>
          <volume>30</volume>
          <issue>6</issue>
          <fpage>985</fpage>
          <lpage>1002</lpage>
          <pub-id pub-id-type="doi">10.1109/TPAMI.2007.70847</pub-id>
          <pub-id pub-id-type="pmid">18421105</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR21">
        <label>21.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lin</surname>
              <given-names>W</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Y</given-names>
            </name>
          </person-group>
          <article-title>A lattice-based MRF model for dynamic near-regular texture tracking</article-title>
          <source>IEEE Trans Pattern Anal Mach Intell</source>
          <year>2007</year>
          <volume>29</volume>
          <issue>5</issue>
          <fpage>777</fpage>
          <lpage>792</lpage>
          <pub-id pub-id-type="doi">10.1109/TPAMI.2007.1053</pub-id>
          <pub-id pub-id-type="pmid">17356199</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR22">
        <label>22.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>Y</given-names>
            </name>
            <name>
              <surname>Collins</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Tsin</surname>
              <given-names>Y</given-names>
            </name>
          </person-group>
          <article-title>A computational model for periodic pattern perception based on frieze and wallpaper groups</article-title>
          <source>IEEE Trans Pattern Anal Mach Intell</source>
          <year>2004</year>
          <volume>26</volume>
          <issue>3</issue>
          <fpage>354</fpage>
          <lpage>371</lpage>
          <pub-id pub-id-type="doi">10.1109/TPAMI.2004.1262332</pub-id>
          <pub-id pub-id-type="pmid">15376882</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR23">
        <label>23.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lussier</surname>
              <given-names>YA</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Y</given-names>
            </name>
          </person-group>
          <article-title>Computational approaches to phenotyping: high-throughput phenomics</article-title>
          <source>Proc Am Thorac Soc</source>
          <year>2007</year>
          <volume>4</volume>
          <issue>1</issue>
          <fpage>18</fpage>
          <lpage>25</lpage>
          <pub-id pub-id-type="doi">10.1513/pats.200607-142JG</pub-id>
          <pub-id pub-id-type="pmid">17202287</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR24">
        <label>24.</label>
        <mixed-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Mitra</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Acharya</surname>
              <given-names>T</given-names>
            </name>
          </person-group>
          <source>Data mining: multimedia, soft computing, and bioinformatics</source>
          <year>2003</year>
          <publisher-loc>Hoboken</publisher-loc>
          <publisher-name>Wiley-Interscience</publisher-name>
        </mixed-citation>
      </ref>
      <ref id="CR25">
        <label>25.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>M&#xFC;ller</surname>
              <given-names>H</given-names>
            </name>
            <name>
              <surname>Michoux</surname>
              <given-names>N</given-names>
            </name>
            <name>
              <surname>Bandon</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Geissbuhler</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>A review of content-based image retrieval systems in medical applications-clinical benefits and future directions</article-title>
          <source>Int J Med Inform</source>
          <year>2004</year>
          <volume>73</volume>
          <issue>1</issue>
          <fpage>1</fpage>
          <lpage>23</lpage>
          <pub-id pub-id-type="doi">10.1016/j.ijmedinf.2003.11.024</pub-id>
          <pub-id pub-id-type="pmid">15036075</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR26">
        <label>26.</label>
        <mixed-citation publication-type="other">Niblack W, Barber R, Equitz W, Flickner M, Glasman E, Petkovic D, Yanker P, Faloutsos C, Taubin G (1993). The QBIC project: querying images by content using color, texture, and shape. In: Proc SPIE&#x2014;Int Soc Opt Eng, in Storage and Retrieval for Image and Video Databases, vol 1908, pp 173&#x2013;187</mixed-citation>
      </ref>
      <ref id="CR27">
        <label>27.</label>
        <mixed-citation publication-type="book">
          <person-group person-group-type="editor">
            <name>
              <surname>Nusslein-Volhard</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Dahm</surname>
              <given-names>R</given-names>
            </name>
          </person-group>
          <source>Zebrafish: a practical approach</source>
          <year>2002</year>
          <publisher-loc>Oxford</publisher-loc>
          <publisher-name>Oxford University Press</publisher-name>
        </mixed-citation>
      </ref>
      <ref id="CR28">
        <label>28.</label>
        <mixed-citation publication-type="other">Penn State Zebrafish Atlas. <ext-link ext-link-type="uri" xlink:href="http://www.zfatlas.psu.edu">http://www.zfatlas.psu.edu</ext-link></mixed-citation>
      </ref>
      <ref id="CR29">
        <label>29.</label>
        <mixed-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Petrou</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Sevilla</surname>
              <given-names>P</given-names>
            </name>
          </person-group>
          <source>Image processing: dealing with texture</source>
          <year>2006</year>
          <publisher-loc>Chichester</publisher-loc>
          <publisher-name>Wiley</publisher-name>
        </mixed-citation>
      </ref>
      <ref id="CR30">
        <label>30.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Plotnick</surname>
              <given-names>RE</given-names>
            </name>
            <name>
              <surname>Gardner</surname>
              <given-names>RH</given-names>
            </name>
            <name>
              <surname>O&#x2019;Neil</surname>
              <given-names>RV</given-names>
            </name>
          </person-group>
          <article-title>Lacunarity indices as measures of landscape texture</article-title>
          <source>Landsc Ecol</source>
          <year>1993</year>
          <volume>8</volume>
          <issue>3</issue>
          <fpage>201</fpage>
          <lpage>211</lpage>
          <pub-id pub-id-type="doi">10.1007/BF00125351</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR31">
        <label>31.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Plummer</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Buiatti</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>Lopez</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Peraza</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Vivas</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Oliver</surname>
              <given-names>W</given-names>
            </name>
            <name>
              <surname>Munoz</surname>
              <given-names>N</given-names>
            </name>
          </person-group>
          <article-title>Histological diagnosis of precancerous lesions of the stomach: a reliability study</article-title>
          <source>Int J Epidemiol</source>
          <year>1997</year>
          <volume>26</volume>
          <issue>4</issue>
          <fpage>716</fpage>
          <lpage>720</lpage>
          <pub-id pub-id-type="doi">10.1093/ije/26.4.716</pub-id>
          <pub-id pub-id-type="pmid">9279602</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR32">
        <label>32.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sabaliauskas</surname>
              <given-names>NA</given-names>
            </name>
            <name>
              <surname>Foutz</surname>
              <given-names>CA</given-names>
            </name>
            <name>
              <surname>Mest</surname>
              <given-names>JR</given-names>
            </name>
            <name>
              <surname>Budgeon</surname>
              <given-names>LR</given-names>
            </name>
            <name>
              <surname>Sidor</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Gershenson</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Joshi</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Cheng</surname>
              <given-names>KC</given-names>
            </name>
          </person-group>
          <article-title>High-throughput zebrafish histology</article-title>
          <source>Methods</source>
          <year>2006</year>
          <volume>39</volume>
          <fpage>246</fpage>
          <lpage>254</lpage>
          <pub-id pub-id-type="doi">10.1016/j.ymeth.2006.03.001</pub-id>
          <pub-id pub-id-type="pmid">16870470</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR33">
        <label>33.</label>
        <mixed-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Salton</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>McGill</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <source>Introduction to modern information retrieval</source>
          <year>1983</year>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>McGraw-Hill</publisher-name>
        </mixed-citation>
      </ref>
      <ref id="CR34">
        <label>34.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Smeulders</surname>
              <given-names>AWM</given-names>
            </name>
            <name>
              <surname>Worring</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Santini</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Gupta</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Jain</surname>
              <given-names>R</given-names>
            </name>
          </person-group>
          <article-title>Content based image retrieval at the end of the early years</article-title>
          <source>IEEE Trans Pattern Anal Mach Intell</source>
          <year>2000</year>
          <volume>22</volume>
          <issue>12</issue>
          <fpage>1349</fpage>
          <lpage>1380</lpage>
          <pub-id pub-id-type="doi">10.1109/34.895972</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR35">
        <label>35.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tang</surname>
              <given-names>HL</given-names>
            </name>
            <name>
              <surname>Hanka</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Ip</surname>
              <given-names>HHS</given-names>
            </name>
          </person-group>
          <article-title>Histological image retrieval based on semantic content analysis</article-title>
          <source>IEEE Trans Inf Technol Biomed</source>
          <year>2003</year>
          <volume>7</volume>
          <issue>1</issue>
          <fpage>26</fpage>
          <lpage>36</lpage>
          <pub-id pub-id-type="doi">10.1109/TITB.2003.808500</pub-id>
          <pub-id pub-id-type="pmid">12670016</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR36">
        <label>36.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tsao-Wu</surname>
              <given-names>GS</given-names>
            </name>
            <name>
              <surname>Weber</surname>
              <given-names>CH</given-names>
            </name>
            <name>
              <surname>Budgeon</surname>
              <given-names>LR</given-names>
            </name>
            <name>
              <surname>Cheng</surname>
              <given-names>KC</given-names>
            </name>
          </person-group>
          <article-title>Agarose embedded tissue arrays for histologic and genetic analysis</article-title>
          <source>Biotechniques</source>
          <year>1998</year>
          <volume>25</volume>
          <fpage>614</fpage>
          <lpage>618</lpage>
          <pub-id pub-id-type="pmid">9793642</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR37">
        <label>37.</label>
        <mixed-citation publication-type="other">VIPS&#x2014;VipsWiki. <ext-link ext-link-type="uri" xlink:href="http://www.vips.ecs.soton.ac.uk/">http://www.vips.ecs.soton.ac.uk/</ext-link></mixed-citation>
      </ref>
      <ref id="CR38">
        <label>38.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>JZ</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Weiderhold</surname>
              <given-names>G</given-names>
            </name>
          </person-group>
          <article-title>SIMPLIcity: Semantics-sensitive integrated matching for picture libraries</article-title>
          <source>IEEE Trans Pattern Anal Mach Intell</source>
          <year>2001</year>
          <volume>23</volume>
          <issue>9</issue>
          <fpage>947</fpage>
          <lpage>963</lpage>
          <pub-id pub-id-type="doi">10.1109/34.955109</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR39">
        <label>39.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Weirauch</surname>
              <given-names>MT</given-names>
            </name>
            <name>
              <surname>Wong</surname>
              <given-names>CK</given-names>
            </name>
            <name>
              <surname>Byrne</surname>
              <given-names>AB</given-names>
            </name>
            <name>
              <surname>Stuart</surname>
              <given-names>JM</given-names>
            </name>
          </person-group>
          <article-title>Information-based methods for predicting gene function from systematic gene knock-downs</article-title>
          <source>BMC Bioinforma</source>
          <year>2008</year>
          <volume>9</volume>
          <fpage>463</fpage>
          <pub-id pub-id-type="doi">10.1186/1471-2105-9-463</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR40">
        <label>40.</label>
        <mixed-citation publication-type="other">Yahoo image search. <ext-link ext-link-type="uri" xlink:href="http://images.search.yahoo.com/">http://images.search.yahoo.com/</ext-link></mixed-citation>
      </ref>
      <ref id="CR41">
        <label>41.</label>
        <mixed-citation publication-type="other">Zebrafish (Danio rerio) Sequencing Project. <ext-link ext-link-type="uri" xlink:href="http://www.sanger.ac.uk/Projects/D_rerio/">http://www.sanger.ac.uk/Projects/D_rerio/</ext-link></mixed-citation>
      </ref>
      <ref id="CR42">
        <label>42.</label>
        <mixed-citation publication-type="other">Zebrafish Phenome Project 2010 Meeting. <ext-link ext-link-type="uri" xlink:href="http://www.blsmeetings.net/zebrafish/">http://www.blsmeetings.net/zebrafish/</ext-link></mixed-citation>
      </ref>
      <ref id="CR43">
        <label>43.</label>
        <mixed-citation publication-type="other">Zhao D, Chen Y, Correa N (2005) Statistical categorization of human histological images. In: Proc IEEE Int Conf Image Process (ICIP), vol 3, pp 628&#x2013;631</mixed-citation>
      </ref>
      <ref id="CR44">
        <label>44.</label>
        <mixed-citation publication-type="other">Zhou XS, Zillner S, Moeller M, Sintek M, Zhan Y, Krishnan A, Gupta A (2008) Semantics and CBIR: a medical imaging perspective. In: Proc ACM Int Conf Image Video Retr (CIVR), pp 571&#x2013;580</mixed-citation>
      </ref>
    </ref-list>
    <fn-group>
      <fn>
        <p>K. C. Cheng and J. Z. Wang have contributed equally to this work.</p>
      </fn>
    </fn-group>
  </back>
</article>
