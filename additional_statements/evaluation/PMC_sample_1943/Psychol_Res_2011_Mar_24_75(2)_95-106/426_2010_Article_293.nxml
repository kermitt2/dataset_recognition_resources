<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v3.0 20080202//EN" "archivearticle3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
  <?properties open_access?>
  <?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
  <?DTDIdentifier.IdentifierType public?>
  <?SourceDTD.DTDName A++V2.4.dtd?>
  <?SourceDTD.Version 2.4?>
  <?ConverterInfo.XSLTName springer2nlmx2.xsl?>
  <?ConverterInfo.Version 2?>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Psychol Res</journal-id>
      <journal-title-group>
        <journal-title>Psychological Research</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">0340-0727</issn>
      <issn pub-type="epub">1430-2772</issn>
      <publisher>
        <publisher-name>Springer-Verlag</publisher-name>
        <publisher-loc>Berlin/Heidelberg</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">3036830</article-id>
      <article-id pub-id-type="pmid">20574661</article-id>
      <article-id pub-id-type="publisher-id">293</article-id>
      <article-id pub-id-type="doi">10.1007/s00426-010-0293-4</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Original Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Decomposing rhythm processing: electroencephalography of perceived and self-imposed rhythmic patterns</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" corresp="yes">
          <name>
            <surname>Schaefer</surname>
            <given-names>Rebecca S.</given-names>
          </name>
          <address>
            <phone>+31-24-3615458</phone>
            <fax>+31-24-3616606</fax>
            <email>r.schaefer@donders.ru.nl</email>
          </address>
          <xref ref-type="aff" rid="Aff1"/>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Vlek</surname>
            <given-names>Rutger J.</given-names>
          </name>
          <xref ref-type="aff" rid="Aff1"/>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Desain</surname>
            <given-names>Peter</given-names>
          </name>
          <xref ref-type="aff" rid="Aff1"/>
        </contrib>
        <aff id="Aff1">Donders Institute for Brain, Cognition and Behaviour, Centre for Cognition, Radboud University, Montessorilaan 3, 6525 HE Nijmegen, The Netherlands </aff>
      </contrib-group>
      <pub-date pub-type="epub">
        <day>24</day>
        <month>6</month>
        <year>2010</year>
      </pub-date>
      <pub-date pub-type="pmc-release">
        <day>24</day>
        <month>6</month>
        <year>2010</year>
      </pub-date>
      <pub-date pub-type="ppub">
        <month>3</month>
        <year>2011</year>
      </pub-date>
      <volume>75</volume>
      <issue>2</issue>
      <fpage>95</fpage>
      <lpage>106</lpage>
      <history>
        <date date-type="received">
          <day>26</day>
          <month>1</month>
          <year>2010</year>
        </date>
        <date date-type="accepted">
          <day>31</day>
          <month>5</month>
          <year>2010</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>&#xA9; The Author(s) 2010</copyright-statement>
      </permissions>
      <abstract id="Abs1">
        <p>Perceiving musical rhythms can be considered a process of attentional chunking over time, driven by accent patterns. A rhythmic structure can also be generated internally, by placing a subjective accent pattern on an isochronous stimulus train. Here, we investigate the event-related potential (ERP) signature of actual and subjective accents, thus disentangling low-level perceptual processes from the cognitive aspects of rhythm processing. The results show differences between accented and unaccented events, but also show that different types of unaccented events can be distinguished, revealing additional structure within the rhythmic pattern. This structure is further investigated by decomposing the ERP into subcomponents, using principal component analysis. In this way, the processes that are common for perceiving a pattern and self-generating it are isolated, and can be visualized for the tasks separately. The results suggest that top-down processes have a substantial role in the cerebral mechanisms of rhythm processing, independent of an externally presented stimulus.</p>
      </abstract>
      <custom-meta-group>
        <custom-meta>
          <meta-name>issue-copyright-statement</meta-name>
          <meta-value>&#xA9; Springer-Verlag 2011</meta-value>
        </custom-meta>
      </custom-meta-group>
    </article-meta>
  </front>
  <body>
    <sec id="Sec1">
      <title>Introduction</title>
      <p>Many of the auditory patterns we perceive around us, such as speech and music, require the structuring of information over time for efficient perception. Perceiving regularities is essential for interpretation of this information, and leads to predictive processing, which, in turn, is needed for goal-directed behavior (for a recent overview, see Winkler, Denham, &amp; Nelken, <xref ref-type="bibr" rid="CR42">2009</xref>). In music listening, it is widely believed that the confirmation and violation of expectations are crucial for a musical piece&#x2019;s own characteristics and what makes it specific and enjoyable (Huron, <xref ref-type="bibr" rid="CR16">2006</xref>).</p>
      <p>Events in auditory patterns such as musical rhythms are believed to be processed more efficiently when their position in time can be predicted, described in dynamic attending theory (Drake, Jones, &amp; Baruch, <xref ref-type="bibr" rid="CR9">2000</xref>; Jones &amp; Boltz, <xref ref-type="bibr" rid="CR18">1989</xref>) as well as other theories of fluctuating expectation levels (Desain, <xref ref-type="bibr" rid="CR6">1992</xref>). The main premise is that expectancy levels can be manipulated by presenting temporal patterns varying in regularity, thus making impending events more or less predictable. This is also reflected in computational models of rhythm processing, such as the coupled oscillator model presented by Large and colleagues (Large &amp; Jones, <xref ref-type="bibr" rid="CR22">1999</xref>; Large &amp; Kolen, <xref ref-type="bibr" rid="CR23">1995</xref>), in which the percept of a rhythm is built up out of multiple oscillators with different period lengths, related to different hierarchical levels of the rhythm. These models include a feedback loop in which highly expected events raise the &#x2018;confidence&#x2019; of the oscillator, contributing more to subsequent expectancy. Consequently, these models can predict how in a very simple train of stimuli (e.g. an isochronous rhythm) chunking of a number of events may occur, so that specific future events incur a higher expectancy. This continues even if the actual accent is no longer present in the stimulus, making the percept of a pulse in the event train quite robust (see Fig.&#xA0;<xref rid="Fig1" ref-type="fig">1</xref> for a graphical representation of this process). In the concept of dynamic attending as proposed by Jones and Boltz (<xref ref-type="bibr" rid="CR18">1989</xref>) and Drake et&#xA0;al. (<xref ref-type="bibr" rid="CR9">2000</xref>), listeners willfully give more weight to the oscillator we choose, thus attending to different hierarchical levels of the rhythmic structure (i.e. the beat, bar or even phrase level). This adds an internally driven factor to the mechanism, which leaves each individual event with its own unique combination of attention levels for each phase of the coupled oscillators. This concept has also been described in terms of music theoretical considerations (London, <xref ref-type="bibr" rid="CR25">2004</xref>), referring to hierarchical levels of metric patterns as cycles. Here, we report the event-related potential (ERP) response to these different events within rhythmic patterns, assuming that varying levels of attention and expectancy will be visible in the ERP of the electro-encephalogram (EEG, for early work demonstrating the effect of attention on the ERP, see N&#xE4;&#xE4;t&#xE4;nen, <xref ref-type="bibr" rid="CR31">1975</xref>; Hillyard, Hink, Schwent, &amp; Picton, <xref ref-type="bibr" rid="CR15">1973</xref>). In order to distinguish between the perceptual responses and cognitive mechanisms that are independent of external stimulation, we do this for both externally presented and internally generated patterns.<fig id="Fig1"><label>Fig.&#xA0;1</label><caption><p>A schematic representation of a metrical percept is shown to continue its pattern on an isochronous stimulus sequence by first introducing the structure. The perceived pulse will persist, resulting in a purely subjective structure. Based on Snyder and Large (<xref ref-type="bibr" rid="CR40">2005</xref>)</p></caption><graphic xlink:href="426_2010_293_Fig1_HTML" id="MO1"/></fig></p>
      <p>We commonly think of rhythmic structure as hierarchical (see, for instance, Lerdahl &amp; Jackendoff, <xref ref-type="bibr" rid="CR24">1983</xref>; Longuet-Higgins &amp; Lee, <xref ref-type="bibr" rid="CR26">1984</xref>), an assumption that has also been supported by showing that brain responses to deviants in different metrical positions resulted in different ERP signatures. This has been shown for the P300 oddball response using intensity decrements on different positions in an isochronous stimulus pattern (Brochard, Abecasis, Potter, Ragot, &amp; Drake, <xref ref-type="bibr" rid="CR4">2003</xref>), and for the mismatch negativity (MMN) response to syncopations in different positions (Ladinig, Honing, H&#xE1;den, &amp; Winkler, <xref ref-type="bibr" rid="CR21">2009</xref>). In these studies, it was shown that deviants in strong metric positions result in larger P300 and MMN components, respectively, suggesting enhanced processing of accented events. However, the question of how this processing hierarchy is built up is not easily answered. Two contrasting hypotheses can be formulated. On the one hand the brain signature for each event in a cycle may be unique, as in a Gestalt, and on the other hand, the response may be predictable and built up of low-level components, for instance, due to its own combination of the positions of multiple coupled oscillators. These hypotheses may be tested by decomposing the response to see if any commonalities are found over the different events. Although there are multiple methods of decomposing EEG data, the method most commonly used for ERPs is principal component analysis (see, for instance, Dien &amp; Frishkoff, <xref ref-type="bibr" rid="CR8">2005</xref> for an overview). This will yield statistically independent components with weight distributions over the different sensors, that combine to form the full signature, and each explain an amount of the variance in the data. If we assume that the EEG traces of the different subprocesses combine linearly in the total signal, we can compare the decomposed EEG response to our own notion of hierarchical processing in rhythm perception.</p>
      <p>In the current study, we use three rhythmic patterns: binary, ternary and quaternary groupings, referred to as 2&#xA0;beat, 3&#xA0;beat and 4&#xA0;beat. These patterns roughly correspond to 2/4, 3/4, and 4/4 meters, and consist of cycles of an accented or louder first event called the downbeat, followed by one, two or three unaccented events that are considered to have a weaker metrical function. These groupings are shown to be easiest to synchronize with in terms of numerosity (Repp, <xref ref-type="bibr" rid="CR37">2007</xref>). Within these patterns, we defined types of events and pooled together the responses to compare them. First, we compare all accented events to all unaccented events, to find the effect of the downbeat, or accented event. However, the literature on rhythm processing generally posits a more complex structure with more than two types of events (i.e. accented/unaccented, see for instance, Lerdahl &amp; Jackendoff, <xref ref-type="bibr" rid="CR24">1983</xref>). Thus, we look for evidence in the brain activity of the processing of a more intricate structure. We postulate that in the different patterns, certain events have something in common, namely the first unaccented event that follows the downbeat (or accented) event, as well as the last unaccented event, also called the upbeat, leading to, perhaps anticipating, the upcoming downbeat. As we are trying to uncover different processes occurring simultaneously, we try to decompose the EEG data to see if we can find a brain signature that is specific to such subprocesses.</p>
      <p>To investigate rhythm processing independent of perceptual input, we make use of subjective accenting: patterns that are self-imposed on ambiguous, unaccented stimuli. A common manifestation of subjective accenting is the so-called clock illusion, when a regularly sounding &#x2018;tick-tick-tick-tick...&#x2019; may spontaneously induce a &#x2018;tick-tock-tick-tock...&#x2019;-percept in which events are chunked into groups of two. The binary grouping arises spontaneously, and the first beat of every group is perceived as distinctively different from the second. Spontaneous subjective rhythmization has been a topic of study for some time, beginning with the influential work of Bolton (<xref ref-type="bibr" rid="CR2">1894</xref>). In an early psychology text, it is described as a mechanism inherent to our sense of time, similar to grouping mechanisms inherent to visual perception (Boring, <xref ref-type="bibr" rid="CR3">1942</xref>). As opposed to this spontaneous process, we here investigate effortful subjective accenting, by inducing a specific pattern as it is represented in Fig.&#xA0;<xref rid="Fig1" ref-type="fig">1</xref>. By investigating rhythm processing based on external input, where the pattern is present in the stimulus, as well as generation of a rhythmic pattern in the absence of any accent in the stimulus, we can find processing mechanisms that take place independently of physical accenting patterns. Though there will be many shared top-down processes active in both tasks, we refer to the instructional phase as the &#x2018;perception&#x2019;-task, and the latter as the &#x2018;imagery&#x2019;-task. By performing the PCA decomposition over the averaged response to both tasks, brain activity patterns that are common to the two tasks may be isolated and interpreted. In this way, the risk of an effect of the instructional effect adding to the perception task is minimized.</p>
      <p>This leads us to a number of hypotheses of rhythm perception, the first and most important one being that the difference between an accented and an unaccented event is detectable in the brain signal. Secondly, we pose that not all unaccented events are equal, more specifically, we hypothesize that events with a similar function in the pattern will show similarities in the EEG response. The unaccented event that follows an accented event (the first unaccented) has a distinctly different function than the upbeat leading up to an accented event (the last unaccented). The former may have some carry-over effect from the downbeat, but is generally considered a weak beat in the pattern, whereas the latter may show some response reflecting the expectation of the downbeat that is approaching. Here, we may see a conflict of rhythmic function, in which the last position in the pattern is never hierarchically important, versus a more cognitive driven view that this event should get most of the anticipatory response leading to the accent. This cognitive view would then result in a variation of the expectation-induced negative deflection in the EEG, the so-called contingent negative variation (CNV, Walter, Cooper, Aldridge, McCallum, &amp; Winter, <xref ref-type="bibr" rid="CR41">1964</xref>). However, this is a slow component generally seen to start at up to 1,000&#xA0;ms before the expected stimulus (see, for instance, Hamano et&#xA0;al., <xref ref-type="bibr" rid="CR13">1997</xref>; but also Chen et&#xA0;al., <xref ref-type="bibr" rid="CR5">2010</xref>, for an example of an earlier manifestation). Even so, later (250&#x2013;500&#xA0;ms) negative responses are often seen in ERPs in musical or rhythmic contexts (Pearce, Herrojo Ruiz, Kapasi, Wiggins, &amp; Bhattacharya, <xref ref-type="bibr" rid="CR34">2010</xref>; Jongsma et&#xA0;al., <xref ref-type="bibr" rid="CR19">2005</xref>). Another indication of the strength of a metric event may be the presence of a processing negativity (N&#xE4;&#xE4;t&#xE4;nen, <xref ref-type="bibr" rid="CR32">1982</xref>), an early negative response thought to reflect the recruitment of extra attentional resources. Finally, to investigate the role of external input in rhythm processing, we look at both the externally cued (&#x2018;perceived&#x2019;), and internally generated (referred to as &#x2018;imagined&#x2019; or subjective) patterns.</p>
      <p>Previous work looking into ERP responses to events in a specific metrical context has focused mainly on intensity decrement deviants in different metric positions added to identical or physically accented stimulus trains (Brochard et&#xA0;al., <xref ref-type="bibr" rid="CR4">2003</xref>; Abecasis, Brochard, Granot, &amp; Drake, <xref ref-type="bibr" rid="CR1">2005</xref>), resulting in different P300-responses for different metric positions, namely larger P300 amplitudes for accented events in parietal regions. This confirms the spontaneous nature of this process, as no instruction to superimpose a structure was given, and suggests enhanced processing for accented events. This is supported by more recent work from this group, showing that an early, small processing negativity may be seen at the left mastoid channel for accented events in both standard and deviant forms (Potter, Fenwick, Abecasis, &amp; Brochard, <xref ref-type="bibr" rid="CR36">2009</xref>). To disentangle the task of deviancy processing from the mechanism of the metric cycle itself, we here look at responses to physically identical sounds (except for the accent in the perception task) in different contexts. As such, no clear predictions can be made for the ERP response to a pattern without deviants. In a recent study, Fujioka, Zendel, and Ross (<xref ref-type="bibr" rid="CR10">2010</xref>) investigated the brain response to different subjective metrical events as measured with magneto-encephalography (MEG), focusing on accented events (downbeats) and the last unaccented events (termed upbeats). Using 2-beat and 3-beat patterns and spatial-filtering source analysis, they found that responses from hippocampus, basal ganglia, and auditory and association cortices showed a significant contrast between the up- and downbeats of the two patterns while listening to identical click stimuli. However, they did not combine events from different patterns to find any commonalities between them. Another study that also focused specifically on voluntary accenting of ambiguous stimuli, also using MEG, found no difference in the event-related field (ERF, presented as low-frequency content from 1&#x2013;10&#xA0;Hz) between subjectively accented and non-accented events (Iversen, Repp, &amp; Patel, <xref ref-type="bibr" rid="CR17">2009</xref>).</p>
      <p>Based on these studies, we expect the actual accents in the stimulus to result in an increased N1 amplitude (N&#xE4;&#xE4;t&#xE4;nen &amp; Picton, <xref ref-type="bibr" rid="CR33">1987</xref>) due to the intensity differences caused by the accent, not present in the unaccented events. Additionally, the different spectral properties of the accent may enhance the P2 response (Meyer, Baumann, &amp; Jancke, <xref ref-type="bibr" rid="CR30">2006</xref>). As for the subjective accents, the literature does not offer a clear-cut prediction. Considering the different types of unaccented event, we expect that if an anticipatory response for the accent is present in the last unaccented events, this will not be present in the other groups of events, thus predicting the first unaccented event not to show either the increased N1/P2 or any sign of anticipation. As no previous work has, to our knowledge, directly compared ERP-responses to different unaccented events in a rhythmic pattern, this part of the work is still exploratory. By comparing events with similar functions we may uncover common processes over different rhythmic patterns, which can be further investigated by decomposing the responses.</p>
    </sec>
    <sec id="Sec2" sec-type="methods">
      <title>Method</title>
      <sec id="Sec3">
        <title>Participants</title>
        <p>Ten volunteers, recruited at the Radboud University of Nijmegen, participated in the experiment. Each gave their informed consent to participate. All participants were right-handed and had normal or corrected-to-normal vision. None of them had a known history of neurological illness. Musical training was not a criterium for inclusion or exclusion in the study, three of the participants had received formal music training but none were professional musicians. Two datasets were rejected due to a disproportionate number of artifacts (see below for procedure). The reported analyses were carried out for the remaining eight participants (5 male, mean age 38.2, SD 11.6).</p>
      </sec>
      <sec id="Sec4">
        <title>Stimuli and equipment</title>
        <p>Three stimulus patterns were used: binary, ternary and quaternary rhythms, consisting of 2-, 3-, and 4-beat cycles. As we expected the imagery response to be much smaller than the perception response, twice as much data were collected for this task. The stimulus sequences were constructed to collect a maximal amount of imagery data, and were made up of four parts: a perception part that also functioned as an instruction, a fade into the imagery part, the imagery part itself, and a probe accent as an attention check at the end, explained further in the procedure. A schematic example of one of the sequences is shown in Fig.&#xA0;<xref rid="Fig2" ref-type="fig">2</xref>. For every sequence, the metronome tick was played throughout and functioned as the time-lock while keeping the tempo stable. The accents were positioned to establish a pattern, every 2, 3 or 4 metronome beats. After three repeats there was one cycle in which the accent is played softly (fading) and after this no accents are sounded anymore. The subjects were instructed to imagine the accent pattern continuing. At the end of the sequence an extra accent (probe) was played. This probe accent could appear at any point in the pattern, and participants had to indicate whether this probe coincided with an imagined accent or not. This task was added to control for attention and to check whether the subject was still on track. While the stimulus played, a fixation cross was shown on a screen. All sequences were constructed this way, only differing in the number of events per cycle. The stimuli can be listened to at <ext-link ext-link-type="uri" xlink:href="http://www.nici.ru.nl/mmm">http://www.nici.ru.nl/mmm</ext-link>.<fig id="Fig2"><label>Fig.&#xA0;2</label><caption><p>A schematic overview of a typical stimulus sequence, in this case a ternary beat pattern, with a probe on an unaccented position. With an IOI of 500&#xA0;ms between events, the sequence consists of 3 perceived patterns or cycles, one transition or fade cycle, and five imagery cycles. As the first cycle in each task is not used, two perception and four imagery cycles per sequence are used for analysis. The sequence was designed to induce an accenting pattern as is represented for a binary pattern in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, but applicable to all the patterns used here</p></caption><graphic xlink:href="426_2010_293_Fig2_HTML" id="MO2"/></fig></p>
        <p>EEG was recorded using a Biosemi Active-Two system with 256 EEG channels mounted into an elastic cap, and six auxiliary channels (double mastoids, horizontal and vertical EOG), and sampled at 512&#xA0;Hz. The fixation cross and instructions were displayed on a 15&#x2032;&#x2032; TFT screen, and stimuli were played through passive speakers (Monacor, type MKS-28/WS) at a comfortable listening level, adjusted to the preference of the participant. The stimuli were programmed in POCO (Desain &amp; Honing, <xref ref-type="bibr" rid="CR7">1992</xref>) and the resulting MIDI file was converted to audio by Quicktime Musical Instruments using general MIDI commands for low bongo (key 61), velocity 0.7&#xA0;&#xD7;&#xA0;127 as the metronome and high wood block (key 76), velocity 0.8&#xA0;&#xD7;&#xA0;127 as the accents. The sounds were presented with an inter-onset interval (IOI) of 500&#xA0;ms and a duration of 200&#xA0;ms. The analyses were performed in MATLAB (Mathworks, Natick, MA, USA), making use of the FieldTrip toolbox for EEG/MEG-analysis (Donders Institute for Brain, Cognition and Behaviour, Radboud University Nijmegen, The Netherlands. See <ext-link ext-link-type="uri" xlink:href="http://www.ru.nl/neuroimaging/fieldtrip">http://www.ru.nl/neuroimaging/fieldtrip</ext-link>).</p>
      </sec>
      <sec id="Sec5">
        <title>Procedure</title>
        <p>Preceding the actual experiment, a practice session was completed, allowing participants to get used to the task and ensure that they understood it. The practice trials were made slightly easier, with a longer perception-phase and longer fading period. To ensure good understanding of the task, the practice procedure was determined as follows: a counter was set, which counted the correct answers to the probe-tone task. Whenever a wrong answer was given, the counter was set back two points, the practice block ended when the counter had a value of five. A fixation cross was presented at a varying interval before the start of every first beat, appearing between 1 and 1.8&#xA0; s before the sound started, with a jittered duration to prevent the occurrence of temporal expectation. This fixation cross remained on the screen for the entire sequence. Participants were instructed to neither move nor use motor imagery or inner speech, for instance, by counting. Their specific instruction was to imagine the <italic>sound</italic> of the accent continuing after it had faded. The experimental task for the probe tone at the end of the sequence was to match it to the internally generated pattern, and respond &#x2018;yes&#x2019; to a congruent probe and &#x2018;no&#x2019; to an incongruent probe accent through a button press. One block in the experiment consisted of 12 sequences of each of these 2-, 3-, or 4-beat patterns, resulting in a total of 36 randomized sequences. Four of these blocks were recorded per subject, yielding roughly 200 instances of every event for the imagery task and 100 for the perceptual task, not taking into account any rejection of data due to artifacts.</p>
      </sec>
      <sec id="Sec6">
        <title>Analyses</title>
        <p>First, some preprocessing steps were taken. The raw EEG signal, which was originally sampled at 512&#xA0;Hz, was temporally down-sampled to a sampling frequency of 128&#xA0;Hz. To segment the data, a time window of &#x2212;50 to +450&#xA0;ms was chosen around each metronome tick where 0 is the sound onset. These data segments of 500&#xA0;s will from here on be referred to as trials. Trials from a sequence with a wrong answer to the probe accent task were rejected (on average 4 sequences per participant, amounting to 2.7% of the data). To avoid possible start-up or state-change effects, the first period of the perception or imagery pattern of a sequence was not used for analyses (marked &#x2018;start&#x2019; in Fig.&#xA0;<xref rid="Fig2" ref-type="fig">2</xref>). Channels with poor signal quality were rejected based on the DC offset, with a cut-off of 35&#xA0;mV, and a variance of 500&#xA0;&#x3BC;V)<sup>2</sup>. From the remaining data, the removed channels were reconstructed by spherical spline interpolation (Perrin et&#xA0;al., <xref ref-type="bibr" rid="CR35">1989</xref>). After this a common average was subtracted. If more than 25% of channels were rejected, the trial was rejected as a whole. If more than 30% of trials were rejected based on these criteria, the whole data set was not used. This resulted in exclusion of two participants and left an average of 89 trials (SD 10.6) for every unique event in the perception task, and 184 (SD 22.3) for imagery events.</p>
        <p>To test our hypotheses, four different comparisons were made between the types of events, shown in Fig.&#xA0;<xref rid="Fig3" ref-type="fig">3</xref>. The ERP was calculated for several types of events by grouping them differently, referring to these groups as conditions. For comparison 1, the accented/unaccented contrast, all the accented (the first beat of the 2-, 3- and 4-beat patterns) and all the unaccented (all other) events were grouped. To investigate the response to different types of unaccented beat, the first unaccented (the second beat of each pattern) and the last unaccented or upbeat (the last beat of each pattern) were grouped and compared to the accented events (comparison 2 and 3). Here, the two-beat pattern was included in the assumption that the second event in the pattern is a combination of both responses. To investigate the actual differences between different unaccented beats, the first and last were compared to each other (comparison 4). This last comparison is only made up out of the 3- and 4-beat patterns (to avoid the overlap of the 2-beat unaccented event). Because of how the trial sequences were constructed, there were about twice as many trials for the &#x2018;imagery&#x2019; task as for the &#x2018;perception&#x2019; task; however, they are not directly compared to each other. Thus, most conditions are built up of three events, yielding an average of 265 (min 228, max 322) trials per condition for perception, and 560 (min 456, max 725) for imagery per participant. Only &#x2018;all unaccented&#x2019; is built up of twice as many events. When directly compared to each other, first and last unaccented are only based on two events. The condition ERPs were compared using a cluster randomization test. This is a non-parametric statistical test, offering a straightforward way to solve the multiple comparison problem present in EEG data by allowing biophysically motivated constraints, namely clustering over channels, increasing the sensitivity of the test (Maris &amp; Oostenveld, <xref ref-type="bibr" rid="CR29">2007</xref>; Maris, <xref ref-type="bibr" rid="CR28">2004</xref>). The significance level of the temporal clusters as well as the spatial clusters was set at <italic>p</italic>&#xA0;&lt;&#xA0;0.05.<fig id="Fig3"><label>Fig.&#xA0;3</label><caption><p>Condition comparisons made in ERP analyses, reflecting the different hypotheses. Each beat pattern is shown as one accented (bigger) event and a number of unaccented (smaller) events, starting with the single events on the left (with repeating events in<italic> gray</italic>), and the groupings shown for each of the four condition comparisons. The conditions are referred to as AA (all accented), AU (all unaccented), FU (first unaccented) and LU (last unaccented). For the last comparison only the 3- and 4-beat patterns were used to avoid overlap</p></caption><graphic xlink:href="426_2010_293_Fig3_HTML" id="MO3"/></fig></p>
        <p>We then tested the assumption of decomposability of the response by running a PCA on all the ERP data. This yields a data-driven way of validating the comparisons that we made in a hypothesis-driven way by grouping the trials according to event type. Assuming that the ERP signatures of different subprocesses combine in a linear way, comparing the amount of variance that each component explains for the different event types offers an unbiased method of supporting the choices made top-down in the event groupings. The results yield a weight distribution over the scalp and a time course for each component. We first decomposed the two tasks separately (perception/imagery), and then also decomposed the whole dataset as one task (rhythm processing). Running the PCA on the average of the perception and imagery data reveals the processes that are common over the two tasks, again with the contribution of each condition to each component to see which component is active when. A cluster randomization test was performed on the contributions of each component to a condition ERP to see if the difference in contribution of a component to a condition was significant.</p>
      </sec>
    </sec>
    <sec id="Sec7">
      <title>Results</title>
      <sec id="Sec8">
        <title>ERPs</title>
        <p>Significant differences were found in every comparison made, for an overview of the effects on Cz and FPz (chosen for comparability to known 10&#x2013;20 positions) and FC1 (for the maximal effect), see Fig.&#xA0;<xref rid="Fig4" ref-type="fig">4</xref>. Although here, clusters with <italic>p</italic>&#xA0;&lt;&#xA0;0.05 are shaded, <italic>p</italic>&#xA0;&lt;&#xA0;0.0001 for the main clusters in each of the comparisons. While keeping in mind that for the perception task, the ERPs are inherently somewhat noisier due to smaller number of trials, we can still see some regularities.<fig id="Fig4"><label>Fig.&#xA0;4</label><caption><p>The ERPs of all the different comparisons, for perceived (<italic>left</italic>) and imagined (<italic>right</italic>) accents, with the<italic> x</italic>-axis running from &#x2212;50 to 450&#xA0;ms after the metronome click and the<italic> y</italic>-axis running from &#x2212;2 to 2&#xA0;&#x3BC;V. The distribution of channels with significant differences between these events is plotted below, the channels that show a significant cluster with <italic>p</italic> &lt;&#xA0;0.05 are highlighted (<italic>bold</italic>) and the<italic> shading</italic> depicts the duration of the significant difference. Below each column of plots, the time-scale is shown in ms</p></caption><graphic xlink:href="426_2010_293_Fig4_HTML" id="MO4"/></fig></p>
        <sec id="Sec9">
          <title>Early effects (100&#x2013;300&#xA0;ms)</title>
          <p>First of all, the accented events (in comparison 1, 2 and 3) consistently show a larger N1/P2 complex in perception than any of the unaccented, mainly visible as a larger positive deflection between about 100 and 250&#xA0;ms. For imagery, where there is no difference in the stimulus, this effect is also significant, albeit smaller. This effect is visible at central locations with the strongest difference for FC1, just left-lateralized from Cz (FC1). Interestingly, the first unaccented events show an early (&#x2248;100&#xA0;ms) positive deflection as well, which distinguishes them from the last unaccented events (comparison 4), and which averages out in the combined condition of all unaccented events. This difference starts earlier in imagery than it does in perception, but is significant in both.</p>
        </sec>
        <sec id="Sec10">
          <title>Late effects (300&#x2013;450&#xA0;ms)</title>
          <p>At higher latencies, an effect at &gt;350&#xA0;ms with a mainly frontal localization also shows differences between the types of events. The accented and last unaccented events each show a negative deflection, which is not there for the first unaccented. During imagery, this effect is slightly larger for the last unaccented events, differing from the accented at more central electrodes (comparison 3). In perception this is hard to distinguish from the central effect described before, as the P2 increase carries over. The late difference between the first and last unaccented events is consistent for both perceived and imagined patterns, as would be expected considering that in this case there is no difference between the stimuli (all unaccented events).</p>
        </sec>
      </sec>
      <sec id="Sec11">
        <title>PCA</title>
        <p>To test the hypothesis of separate effects with distinct distributions and time courses, a PCA analysis was performed on each task separately. The PCA yields a number of components that each has their own weight distribution over the scalp, an amount of variance of the signal explained by this component and a time course of its activity. The distributions of the components on separate tasks are shown in Fig.&#xA0;<xref rid="Fig5" ref-type="fig">5</xref> (top two rows, P1-2-3 and I1-2-3). The first component explains the most variance by far for both tasks, and their distributions correlate highly (<italic>r</italic>&#xA0;&#x2265;&#xA0;0.99). The consecutive two components appear similar in terms of distribution and explained variance for the two tasks, but seem swapped in order, with component P2 correlating best with I3 (<italic>r</italic>&#xA0;=&#xA0;0.5) and P3 correlating best with I2 (<italic>r</italic>&#xA0;=&#xA0;0.7). The correlations of these distributions suggest that these first three components represent related subprocesses, supporting the next step in which the decomposition was carried out on both tasks together (B1-2-3). Of the resulting components, the weight distribution of B2 correlates highly with both P2 (<italic>r</italic>&#xA0;=&#xA0;0.9) and I2 (<italic>r</italic>&#xA0;=&#xA0;0.7) and B3 with P3 (<italic>r</italic>&#xA0;=&#xA0;0.9) and with I3 (<italic>r</italic>&#xA0;=&#xA0;0.8). This supports the notion that the processes (or combination thereof) associated with these components are related. Assuming that the three components that explain most of the variance of the data are indeed shared over the two tasks, we only discuss the components identified over both tasks. In this way, we can use the spatial properties of the activity explaining most variance for the mean of the two tasks investigate how active these processes are for the different events. From component 4 on, the explained variance of individual components is below 5% and will not be discussed further (the scree plot is shown on the right panel of Fig.&#xA0;<xref rid="Fig5" ref-type="fig">5</xref>). Looking further into the activation patterns of these components for both tasks separately, Fig.&#xA0;<xref rid="Fig6" ref-type="fig">6</xref> shows the contribution of the three components to each type of event, with time courses shown below each distribution for each task, and significant differences plotted below for the different comparisons. Although the PCA inherently tends to make orthogonal distributions, the subprocesses shown here are also supported by visual inspection of the ERP data when comparing the time courses and locations of significant differences. The three main components are discussed in turn.<fig id="Fig5"><label>Fig.&#xA0;5</label><caption><p>The<italic> left panel</italic> shows the distributions and explained amount of variance for the PCA performed on separate tasks (components named for the tasks; perception, imagery, and both, numbered in the order of explained variance: P1-2-3, I1-2-3 and B1-2-3). While the separate tasks show weight distributions for the first three components that correlate significantly (shown by the<italic> arrows</italic> between P2-I3 and P3-I2), the distributions for both tasks together appear to capture the same activity (B2) but isolating additional frontal activity in the third, frontal component (B3). On the<italic> right panel</italic>, the explained variance is shown for the first 15 principal components of the decomposition of both tasks together, showing the first three to be the most important (explaining 74.3% of the total variance)</p></caption><graphic xlink:href="426_2010_293_Fig5_HTML" id="MO5"/></fig><fig id="Fig6"><label>Fig.&#xA0;6</label><caption><p>The time courses of the contributions of the first three components from the PCA over both tasks are shown in&#xA0;&#x3BC;V, with the weight distributions plotted above. The weights from the combined PCA are used to show the contribution of these components to the tasks separately. For both perceived and imagined patterns, the part of the ERP that is explained by the component is plotted as a time course below for each event type, and significant differences (<italic>p</italic> &lt;&#xA0;0.05) between these contributions are shown for each comparison (1&#x2013;4) in the<italic> bars</italic> below the time courses</p></caption><graphic xlink:href="426_2010_293_Fig6_HTML" id="MO6"/></fig></p>
        <sec id="Sec12">
          <title>Component 1</title>
          <p>The first component has a central distribution, and shows a positive peak around 100&#xA0;ms, and then shows a large positivity after about 200&#xA0;ms. The shape of the time course is similar for the perception and imagery tasks, but the strength is different between the accented and the unaccented events. There is also a significant effect between the first and last unaccented events in a similar pattern for both tasks. For the perception task, the difference between the accented event and the first and last unaccented events appear to separate in time, where the accented events show a late negativity. This is not the case for imagery, where it mainly distinguishes the different unaccented events. This component likely relates to the N1 in perception and the P2 response in both perception and imagery.</p>
        </sec>
        <sec id="Sec13">
          <title>Component 2</title>
          <p>The second component is a lateralized activity pattern, explaining 7.5% of the data. It appears to contribute mainly to accented events in the perception task with a strong peak at &#x2248;150&#xA0;ms and a negativity at &#x2248;300&#xA0;ms, but does not distinguish between the unaccented events. In the imagery task this response is much smaller, but still significant. Thus, it appears to capture the part of the brain activity associated with the perceptual accent, but, interestingly, the contribution to the ERPs only differs significantly for the second comparison, all accented versus first unaccented, for both tasks. As the latency of the differences between time courses for the first two components appears to coincide with the N1/P2 complex, one interpretation may be that components 1 and 2 represent two subcomponents of this complex.</p>
        </sec>
        <sec id="Sec14">
          <title>Component 3</title>
          <p>Component 3, a central/frontal activity pattern that explains 6.9% of the variance, shows an early peak in explained variance for all events in both tasks, but after about 30&#xA0;ms starts to distinguish different unaccented events during imagery, and later on (at &#x2248;300&#xA0;ms) starts to contribute to the difference between accented and unaccented events. The difference in this contribution is markedly smaller for the perception task, and only reaches significance in relatively small time windows (comparison 1 and 2&#xA0;at &#x2248;400&#xA0;ms, comparison 4&#xA0;at &#x2248;250&#x2013;300&#xA0;ms). The localization and time course suggest that this is an attention-related process, and may include effects of anticipation.</p>
          <p>As an exploratory check on the groupings chosen to form the conditions, the time courses of the components on single events are shown in Fig.&#xA0;<xref rid="Fig7" ref-type="fig">7</xref>. The dash pattern represents the grouping made in Fig.&#xA0;<xref rid="Fig3" ref-type="fig">3</xref>, so comparable activation patterns for similar dashed lines support our grouping. Looking at these time courses clarifies some of the significance results shown in Fig.&#xA0;<xref rid="Fig6" ref-type="fig">6</xref>, namely the absence of significance for Component 3 in perception, the grouping does not appear to reflect structure here. However, for components 1 and 2, and in imagery component 3, the type of event tends to group together, supporting our design. Most obviously, for perception, component 2 indeed isolates the response to all accented events at about 150&#xA0;ms, and in imagery component 3 isolates the response to first unaccented events (3b2 and 4b2) at about 300&#xA0;ms. Component 1 (shown at a much larger scale than the other components) reveals the same grouping, supported by the statistical testing of the group means.<fig id="Fig7"><label>Fig.&#xA0;7</label><caption><p>The time courses of the component contribution of single events are shown in&#xA0;&#x3BC;V with the colors separating the patterns, the<italic> solid lines</italic> representing the accented events and the different<italic> dashed lines</italic> representing different unaccented events as is shown in the legend. Events are denoted by the pattern and the position (i.e. 4b3 is the 3rd beat in a 4-beat pattern). The scales vary to maximally visualize the time course shape; the first component shows a far larger response than the other two</p></caption><graphic xlink:href="426_2010_293_Fig7_HTML" id="MO7"/></fig></p>
        </sec>
      </sec>
    </sec>
    <sec id="Sec15">
      <title>Discussion</title>
      <p>In the current study, the ERP signatures of rhythmic processing were investigated for both actual and subjectively accented rhythmic patterns. Significant differences were shown between responses to metronome ticks on different positions within a rhythmic pattern. Both hypotheses were confirmed; differences were seen between accented and unaccented events in perceived and imagined rhythms, as well as further differentiation of unaccented events. The ERPs showed the predicted increased central N1-P2 response for actual and, to a lesser degree, subjective accents as compared to all unaccented events. This effect is stronger when comparing all accented events to the last unaccented event of a pattern than when compared to the first unaccented. Although this effect is strongest on channel FC1, slightly left-frontal from Cz, we do not interpret this as a lateralized effect, given the distribution of the significant clusters shown in Fig.&#xA0;<xref rid="Fig4" ref-type="fig">4</xref>. Additionally, a late, frontal positive response is seen in &#x2018;first-unaccented&#x2019; events but not in &#x2018;last-unaccented&#x2019; events, as compared to the accented events. The final comparison between different unaccented events supports the notion of independence of these two different responses. This implies that rhythm processing entails more than simply serially processing only accented and unaccented events, but that there are different responses to unaccented events with a different context but an identical sound. Although comparing events with a different immediate history poses some problems, here the results appear to be quite straightforward in that the events with an actual accent show a stronger N1/P2 response ending after &#x2248;350&#xA0;ms, whereas all other events are based on an identical stimulus (the metronome tick). As the time between ticks (500&#xA0;ms) is long enough not to expect purely perceptual responses to leak into the next event, any difference we see is due to cognitive aspects of rhythm processing. This is true especially in the imagery task, where all differences between events are completely subjective. Thus, the later differences between unaccented beats, here interpreted as purely cognitive instead of perceptual, can only be caused by the rhythmic context.</p>
      <p>The most interesting finding here, which has not been shown before, is the difference between different types of unaccented event. Given that, in both perception and imagery tasks, the sound stimuli for the unaccented events are identical, it is not surprising that the significant effects are similar in location and latency. It does, however, imply that the mechanism we see is independent of external input, and thus is active for perceiving and self-generating a rhythmic pattern, and is mediated by metric position. Although the pattern of significant differences is comparable for the two tasks, the largest difference is visible in the comparison between the accented and first unaccented, in which the increased N1/P2 effect we see for perceived patterns is completely absent. It appears that, without the perceptual response to the actual accent, the last unaccented events show a decreased N1/P2 amplitude, and the first unaccented events only show a decreased late frontal negativity when compared to accented events. Although their interpretation is not straightforward, the impression of two distinct processes is given.</p>
      <p>This hypothesis was tested by decomposing the ERP data with PCA. The distributions of the components of the different tasks separately (perception and imagery) correlate highly, which supports the validity of decomposing the two tasks together, namely rhythm processing, with or without external input. This is likely a composite process, including elements of mnemonic processing, tempo tracking, regularity detection, expectancy generation and others. The first three components explain almost 75% of the total variance. The distributions found for the different subcomponents connect well with the existing literature. Kuck, Grossbach, Bangert, and Altenm&#xFC;ller (<xref ref-type="bibr" rid="CR20">2003</xref>) found, when researching the distributions of rhythm and meter processing, that there was sustained cortical activation over bilateral frontal and temporal brain regions, that did not differ much for the two tasks. The different subcomponents of accenting were obviously present in their stimuli as well, and perhaps may also be decomposed. In a study concerning speech rhythm, Geiser, Zaehle, Jancke, and Meyer (<xref ref-type="bibr" rid="CR11">2008</xref>) found that adding an explicit rhythm judgement task, thus directing attention to the rhythmicity of the (spoken) stimulus, increased activity in the supplementary motor areas and the inferior frontal gyrus, both bilaterally. These sources, related to the explicitness (i.e. directed attention) of a rhythmical task, may well be implicated here. However, more work is needed to confirm this. Even so, Geiser, Ziegler, Jancke, and Meyer (<xref ref-type="bibr" rid="CR12">2009</xref>) separated out meter and rhythm deviants and found the ERP response to rhythmic deviants to be maximal in frontal areas, and dependent on directed attention, while meter changes elicited a response more centrally and laterally distributed.</p>
      <p>To asses the activity of the processes explaining most variance over both tasks for each task individually, their distributions were used to visualize the activity for the two tasks separately. The time courses of the components show specific activations for specific aspects of the rhythmic patterns. The first and biggest component contributes to the N1/P2 activity, also showing the first unaccented event to be more like an accented event than the last unaccented event. As this component explains around five times as much variance as the other two, for both tasks, the fact that a difference between the conditions is visible here provides the main support for the grouping into event categories that was decided on. The second component appears to respond mainly to perceived accents, but does not distinguish between the accented events and the last unaccented events, likely also contributing to the increased N1/P2 complex seen in the ERP in perception. Then finally, the third component contributes to the later, more frontal effect, distinguishing well between unaccented events only in the imagery task at a relatively early latency (&#x2248;100&#xA0;ms) and between the accented and all unaccented events a bit later (at &#x2248;350&#xA0;ms). Inspection of the contributions of single events to the different components supports the groupings of events used here, according to metric context, save for component 3 in perception. This difference may be interpreted as a result of increased focus or effort during the self-generation of the rhythmic pattern in imagery which is not necessarily there during perception. Alternatively, it may be due to the smaller number of trials for the perception task. The finding that decomposing both tasks together as one still yields interpretable results was unexpected, and indicates that the cognitive processing of rhythms, be they externally presented or internally generated, shares a common mechanism. The relevance of the decomposition is obviously dependent on the assumption that the EEG traces of the components combine linearly to form the total signal. Other methods (that make the same assumption) can also be used to decompose EEG data into subprocesses, such as ICA (Makeig, Jung, Bell, Ghahremani, &amp; Sejnowski, <xref ref-type="bibr" rid="CR27">1997</xref>) or linear regression (Hauk, Pulverm&#xFC;ller, Ford, Marslen-Wilson, &amp; Davis, <xref ref-type="bibr" rid="CR14">2008</xref>; Schaefer, Desain, &amp; Suppes, <xref ref-type="bibr" rid="CR38">2009</xref>), and a solid comparison of these different methods may be a subject of future work.</p>
      <p>A number of assumptions made in the design may have consequences for the interpretation of the results. First, the assumption that the second event in a 2-beat pattern includes characteristics of both first and last unaccented events in a pattern may have influenced the contrast with the accented events. More detailed analyses are needed to test this, but as this would not exaggerate the difference but instead diminish it, the effect may actually be a bit larger than demonstrated here. The decomposed time courses per event shown in Fig.&#xA0;<xref rid="Fig7" ref-type="fig">7</xref>, however, support our assumption. Then, considering that imagery is never completely controlled we must allow for the possibility that participants were in fact using inner speech or imagery after all, in contrary to explicit instructions. Also, as we were interested in collecting a maximal amount of imagery data, the stimuli were constructed to always have perception preceding imagery. Although the first cycle of each sequence was never used and treated as an instruction cycle, there was a fixed order of tasks in the design. However, as it is not possible to &#x2018;continue a pattern internally&#x2019; that is not presented first, this opportunity was used to extend this presentation to a series of perception trials. Finally, there were subtle differences between the tasks that involve more than the task itself. As previously mentioned, the fact that the &#x2018;perception&#x2019; part of the sequence also includes an element of instruction, and preparation for imagery, has to be kept in mind. However, by decomposing the data based on the mean over both tasks the risk of these processes causing the found effects is minimal. Moreover, if these effects would be present, the increase in variance would again cause an underestimation of the effect instead of an overestimation.</p>
      <p>Considering the literature cited earlier, we can say that the N1/P2 effect that was expected for the accented events is actually affecting the unaccented events as well, although the difference is found mostly in the P2-part of the complex, in a similar time window as where Fujioka et&#xA0;al. (<xref ref-type="bibr" rid="CR10">2010</xref>) found an effect of accenting. Interestingly, this component has been found to be affected by spectral aspects (or timbre) of an auditory stimulus (Shahin, Roberts, Pantev, Trainor, &amp; Ross, <xref ref-type="bibr" rid="CR39">2005</xref>; Meyer et&#xA0;al., <xref ref-type="bibr" rid="CR30">2006</xref>). Given that the stimuli were identical in the imagery task, in this case the percept is completely self-generated. This is even more interesting in the comparison between first and last unaccented events, where even in the Perception task the stimulus is identical. The later, frontal effect is harder to interpret. The reduced negativity seen in the first unaccented events may support the interpretation of a CNV-like response for the last unaccented events, however the fact that it is also present in the accented events contradicts this. To a certain extent, there is of course anticipation for every event, as the stimulus is intentionally rhythmic. Also, as the first unaccented events all lead to events with different functions the level of anticipation, the levels of anticipation would likely differ. Again, the decomposed time courses per type of event shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref> tend to support the grouping we made here in terms of how components contribute to each event, especially for the primary component explaining most of the variance. If however we interpret the absence of the positivity in the accented events as extra anticipation for the first unaccented beat, this would lend new importance to this event in the cycle, not suggested by either music theory or cognitive theory. On the other hand, if we interpret this as a somewhat late processing negativity present for the accented and the last unaccented, this would fit quite well. Although not explicitly discussed as a component related to rhythmic processing, a frontal component with a similar latency is seen in other studies that involve rhythmic musical stimuli (for instance Pearce et&#xA0;al., <xref ref-type="bibr" rid="CR34">2010</xref>; Jongsma et&#xA0;al., <xref ref-type="bibr" rid="CR19">2005</xref>), and further work is called for to elucidate this response. If we consider this negativity a default, then its absence for the first unaccented events may be interpreted as reduced processing, which is supported by the lack of information present in the stimulus at this position (i.e. no accent and no anticipation). The early processing negativity found by Potter et&#xA0;al. (<xref ref-type="bibr" rid="CR36">2009</xref>) was not seen here for the accented events, in either perception or imagery. Looking back at the coupled oscillator models, the decomposition results do not support the view of multiple processes resulting in the responses to the rhythmic events. The difference between the two types of unaccented events is captured mostly in the main PCA component, as is the difference between accented events and the first unaccented events. Thus, the interplay between varying levels of attention, expectation and processing is not separable by statistical decomposition in our study.</p>
      <p>To conclude, the current report shows processing of metronome clicks in a different metric context to result in different ERP responses, and thus to be heavily influenced by attention levels, even without differences in the perceptual input. The decomposition through PCA yields an informative look at the subprocesses involved, offering a decomposition that at least partly relates to the hierarchical levels of rhythm processing. By identifying components that were active over both tasks (perception and imagery), we found support for the notion that similar cerebral sources are active in perceived and self-imposed patterns, although they are clearly not identical. The time courses of these components could be interpreted to separate a more low-level effect on the N1/P2 complex for the perception task, distinguishing accented from unaccented events, from a later, more frontal effect that distinguishes different types of unaccented events in both tasks. As the current data are based only on simple, regular metre, further work is needed to clarify the nature of these responses in the framework of processing models such as coupled oscillators. Also, other IOIs may produce different responses. Even so, a strong case has been made to distinguish between different types of unaccented events within one rhythmic pattern when researching cerebral mechanisms of rhythm processing. Additionally, in the absence of any externally driven process, self-generated or imagined rhythms were shown to be measurable in EEG, differentiating responses based on the rhythmic context.</p>
    </sec>
  </body>
  <back>
    <ack>
      <p>Thanks to Justin London for comments on an earlier version of the manuscript, and Jason Farquhar for helpful discussions. The authors gratefully acknowledge the support of the BrainGain Smart Mix Programme of the Netherlands Ministry of Economic Affairs and the Netherlands Ministry of Education, Culture and Science, and the Dutch Technologiestichting STW.</p>
      <p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.</p>
    </ack>
    <ref-list id="Bib1">
      <title>References</title>
      <ref id="CR1">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Abecasis</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Brochard</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Granot</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Drake</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Differential brain response to metrical accents in isochronous auditory sequences</article-title>
          <source>Music Perception,</source>
          <year>2005</year>
          <volume>22</volume>
          <issue>3</issue>
          <fpage>549</fpage>
          <lpage>562</lpage>
          <pub-id pub-id-type="doi">10.1525/mp.2005.22.3.549</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR2">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bolton</surname>
              <given-names>T.L.</given-names>
            </name>
          </person-group>
          <article-title>Rhythm</article-title>
          <source>American Journal of Psychology,</source>
          <year>1894</year>
          <volume>6</volume>
          <fpage>145</fpage>
          <lpage>238</lpage>
          <pub-id pub-id-type="doi">10.2307/1410948</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR3">
        <mixed-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Boring</surname>
              <given-names>E. G.</given-names>
            </name>
          </person-group>
          <source>Sensation and perception in the history of experimental psychology</source>
          <year>1942</year>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>Appleton-Century</publisher-name>
        </mixed-citation>
      </ref>
      <ref id="CR4">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Brochard</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Abecasis</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Potter</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Ragot</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Drake</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>The &#x201C;ticktock&#x201D; of our internal clock: Direct brain evidence of subjective accents in isochronous sequences</article-title>
          <source>Psychological Science,</source>
          <year>2003</year>
          <volume>14</volume>
          <issue>4</issue>
          <fpage>362</fpage>
          <lpage>366</lpage>
          <pub-id pub-id-type="doi">10.1111/1467-9280.24441</pub-id>
          <pub-id pub-id-type="pmid">12807411</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR5">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Jackson</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Peng</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>An event-related potential study of temporal information encoding and decision making</article-title>
          <source>NeuroReport</source>
          <year>2010</year>
          <volume>21</volume>
          <fpage>152</fpage>
          <lpage>155</lpage>
          <pub-id pub-id-type="doi">10.1097/WNR.0b013e328335b4f7</pub-id>
          <pub-id pub-id-type="pmid">20010443</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR6">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Desain</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>A (de)composable theory of rhythm perception</article-title>
          <source>Music Perception</source>
          <year>1992</year>
          <volume>9</volume>
          <issue>4</issue>
          <fpage>439</fpage>
          <lpage>454</lpage>
        </mixed-citation>
      </ref>
      <ref id="CR7">
        <mixed-citation publication-type="other">Desain P., &amp; Honing, H. (1992) <italic>Music, Mind and Machine: Studies in Computer Music, Music Cognition and Artificial Intelligence</italic>. Amsterdam: Thesis Publishers.</mixed-citation>
      </ref>
      <ref id="CR8">
        <mixed-citation publication-type="other">Dien J., &amp; Frishkoff, G. A. (2005). Principal components analysis of event-related potential datasets. In T. Handy (Eds.), <italic>Event-related potentials: A methods handbook</italic>. Cambridge. MA: MIT Press.</mixed-citation>
      </ref>
      <ref id="CR9">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Drake</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Jones</surname>
              <given-names>M.R.</given-names>
            </name>
            <name>
              <surname>Baruch</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>The development of rhythmic attending in auditory sequences: attunement, referent period, focal attending</article-title>
          <source>Cognition</source>
          <year>2000</year>
          <volume>77</volume>
          <fpage>251</fpage>
          <lpage>288</lpage>
          <pub-id pub-id-type="doi">10.1016/S0010-0277(00)00106-2</pub-id>
          <pub-id pub-id-type="pmid">11018511</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR10">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fujioka</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Zendel</surname>
              <given-names>B.R.</given-names>
            </name>
            <name>
              <surname>Ross</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Endogenous neuromagnetic activity for mental hierarchy of timing</article-title>
          <source>Journal of Neuroscience</source>
          <year>2010</year>
          <volume>30</volume>
          <issue>9</issue>
          <fpage>3458</fpage>
          <lpage>3466</lpage>
          <pub-id pub-id-type="doi">10.1523/JNEUROSCI.3086-09.2010</pub-id>
          <pub-id pub-id-type="pmid">20203205</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR11">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Geiser</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Zaehle</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Jancke</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Meyer</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>The neural correlate of speech rhythm as evidences by metrical speech processing</article-title>
          <source>Journal of Cognitive Neuroscience</source>
          <year>2008</year>
          <volume>20</volume>
          <issue>3</issue>
          <fpage>541</fpage>
          <lpage>552</lpage>
          <pub-id pub-id-type="doi">10.1162/jocn.2008.20029</pub-id>
          <pub-id pub-id-type="pmid">18004944</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR12">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Geiser</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Ziegler</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Jancke</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Meyer</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Early electrophysiological correlates of meter and rhythm processing in music perception</article-title>
          <source>Cerebral Cortex</source>
          <year>2009</year>
          <volume>45</volume>
          <fpage>93</fpage>
          <lpage>102</lpage>
        </mixed-citation>
      </ref>
      <ref id="CR13">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hamano</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>L&#xFC;ders</surname>
              <given-names>H.O.</given-names>
            </name>
            <name>
              <surname>Ikeda</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Collura</surname>
              <given-names>T.F.</given-names>
            </name>
            <name>
              <surname>Comair</surname>
              <given-names>Y.G.</given-names>
            </name>
            <name>
              <surname>Shibasaki</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>The cortical generators of the contingent negative variation in humans: a study with subdural electrodes</article-title>
          <source>Electroencephalography and Clinical Neurophysiology</source>
          <year>1997</year>
          <volume>104</volume>
          <fpage>257</fpage>
          <lpage>268</lpage>
          <pub-id pub-id-type="doi">10.1016/S0168-5597(97)96107-4</pub-id>
          <pub-id pub-id-type="pmid">9186240</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR14">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hauk</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Pulverm&#xFC;ller</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Ford</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Marslen-Wilson</surname>
              <given-names>W.D.</given-names>
            </name>
            <name>
              <surname>Davis</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Can I have a quick word? early electrophysiological manifestations of psycholinguistic processes revealed by event-related regression analysis of the EEG.</article-title>
          <source>Biological Psychology</source>
          <year>2008</year>
          <volume>80</volume>
          <issue>1</issue>
          <fpage>64</fpage>
          <lpage>74</lpage>
          <pub-id pub-id-type="doi">10.1016/j.biopsycho.2008.04.015</pub-id>
          <pub-id pub-id-type="pmid">18565639</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR15">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hillyard</surname>
              <given-names>S.A.</given-names>
            </name>
            <name>
              <surname>Hink</surname>
              <given-names>R.F.</given-names>
            </name>
            <name>
              <surname>Schwent</surname>
              <given-names>V.L.</given-names>
            </name>
            <name>
              <surname>Picton</surname>
              <given-names>T.W.</given-names>
            </name>
          </person-group>
          <article-title>Electrical signs of selective attention in the human brain.</article-title>
          <source>Science</source>
          <year>1973</year>
          <volume>182</volume>
          <fpage>177</fpage>
          <lpage>180</lpage>
          <pub-id pub-id-type="doi">10.1126/science.182.4108.177</pub-id>
          <pub-id pub-id-type="pmid">4730062</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR16">
        <mixed-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Huron</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <source>Sweet anticipation: music and the psychology of expectation</source>
          <year>2006</year>
          <publisher-loc>Cambridge, MA</publisher-loc>
          <publisher-name>MIT Press</publisher-name>
        </mixed-citation>
      </ref>
      <ref id="CR17">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Iversen</surname>
              <given-names>J.R</given-names>
            </name>
            <name>
              <surname>Repp</surname>
              <given-names>B.H.</given-names>
            </name>
            <name>
              <surname>Patel</surname>
              <given-names>A.D.</given-names>
            </name>
          </person-group>
          <article-title>Top-down control of rhythm perception modulates early auditory responses</article-title>
          <source>Annals of the New York Academy of Sciences</source>
          <year>2009</year>
          <volume>1169</volume>
          <fpage>58</fpage>
          <lpage>73</lpage>
          <pub-id pub-id-type="doi">10.1111/j.1749-6632.2009.04579.x</pub-id>
          <pub-id pub-id-type="pmid">19673755</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR18">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jones</surname>
              <given-names>M.R.</given-names>
            </name>
            <name>
              <surname>Boltz</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Dynamic attending and responses to time</article-title>
          <source>Psychological Review</source>
          <year>1989</year>
          <volume>96</volume>
          <issue>3</issue>
          <fpage>459</fpage>
          <lpage>491</lpage>
          <pub-id pub-id-type="doi">10.1037/0033-295X.96.3.459</pub-id>
          <pub-id pub-id-type="pmid">2756068</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR19">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jongsma</surname>
              <given-names>M.L.A.</given-names>
            </name>
            <name>
              <surname>Eichele</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Quian Quiroga</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Jenks</surname>
              <given-names>K.M.</given-names>
            </name>
            <name>
              <surname>Desain</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Honing</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Rijn</surname>
              <given-names>C.M.</given-names>
            </name>
          </person-group>
          <article-title>Expectancy effects on omission evoked potentials in musicians and non-musicians.</article-title>
          <source>Psychophysiology</source>
          <year>2005</year>
          <volume>42</volume>
          <issue>2</issue>
          <fpage>191</fpage>
          <lpage>201</lpage>
          <pub-id pub-id-type="doi">10.1111/j.1469-8986.2005.00269.x</pub-id>
          <pub-id pub-id-type="pmid">15787856</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR20">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kuck</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Grossbach</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Bangert</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Altenm&#xFC;ller</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Brain processing of meter and rhythm in music: electrophysiological evidence of a common network</article-title>
          <source>Annals of the New York Academy of Sciences</source>
          <year>2003</year>
          <volume>999</volume>
          <fpage>244</fpage>
          <lpage>253</lpage>
          <pub-id pub-id-type="doi">10.1196/annals.1284.035</pub-id>
          <pub-id pub-id-type="pmid">14681148</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR21">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ladinig</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Honing</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>H&#xE1;den</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Winkler</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>Probing attentive and preattentive emergent meter in adult listeners without extensive music training</article-title>
          <source>Music Perception</source>
          <year>2009</year>
          <volume>26</volume>
          <issue>4</issue>
          <fpage>377</fpage>
          <lpage>386</lpage>
          <pub-id pub-id-type="doi">10.1525/mp.2009.26.4.377</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR22">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Large</surname>
              <given-names>E.W.</given-names>
            </name>
            <name>
              <surname>Jones</surname>
              <given-names>M.R.</given-names>
            </name>
          </person-group>
          <article-title>The dynamics of attending: How people track time-varying events</article-title>
          <source>Psychological Review</source>
          <year>1999</year>
          <volume>106</volume>
          <issue>1</issue>
          <fpage>119</fpage>
          <lpage>159</lpage>
          <pub-id pub-id-type="doi">10.1037/0033-295X.106.1.119</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR23">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Large</surname>
              <given-names>E.W.</given-names>
            </name>
            <name>
              <surname>Kolen</surname>
              <given-names>J.F.</given-names>
            </name>
          </person-group>
          <article-title>Resonance and the perception of musical meter</article-title>
          <source>Connection Science</source>
          <year>1995</year>
          <volume>6</volume>
          <fpage>177</fpage>
          <lpage>208</lpage>
          <pub-id pub-id-type="doi">10.1080/09540099408915723</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR24">
        <mixed-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Lerdahl</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Jackendoff</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <source>A generative theory of tonal music</source>
          <year>1983</year>
          <publisher-loc>Cambridge, MA</publisher-loc>
          <publisher-name>MIT Press</publisher-name>
        </mixed-citation>
      </ref>
      <ref id="CR25">
        <mixed-citation publication-type="other">London, J. (2004). <italic>Hearing in time: Psychological aspects of musical meter</italic>. Oxford: Oxford University Press.</mixed-citation>
      </ref>
      <ref id="CR26">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Longuet-Higgins</surname>
              <given-names>H.C.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>C.S.</given-names>
            </name>
          </person-group>
          <article-title>The rhythmic interpretation of monophonic music</article-title>
          <source>Music Perception</source>
          <year>1984</year>
          <volume>1</volume>
          <fpage>424</fpage>
          <lpage>441</lpage>
        </mixed-citation>
      </ref>
      <ref id="CR27">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Makeig</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Jung</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Bell</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Ghahremani</surname>
              <given-names>A.J.</given-names>
            </name>
            <name>
              <surname>Sejnowski</surname>
              <given-names>T.J.</given-names>
            </name>
          </person-group>
          <article-title>Blind separation of auditory event-related brain responses into independent components</article-title>
          <source>Proceedings of the National Academy of Sciences of the United States of America</source>
          <year>1997</year>
          <volume>94</volume>
          <fpage>10979</fpage>
          <lpage>10984</lpage>
          <pub-id pub-id-type="doi">10.1073/pnas.94.20.10979</pub-id>
          <pub-id pub-id-type="pmid">9380745</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR28">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Maris</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Randomization tests for ERP topographies and whole spatiotemporal data matrices</article-title>
          <source>Psychophysiology</source>
          <year>2004</year>
          <volume>41</volume>
          <issue>1</issue>
          <fpage>142</fpage>
          <lpage>151</lpage>
          <pub-id pub-id-type="doi">10.1111/j.1469-8986.2003.00139.x</pub-id>
          <pub-id pub-id-type="pmid">14693009</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR29">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Maris</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Oostenveld</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Nonparametric testing of EEG- and MEG-data</article-title>
          <source>Journal of Neuroscience Methods</source>
          <year>2007</year>
          <volume>164</volume>
          <fpage>177</fpage>
          <lpage>190</lpage>
          <pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id>
          <pub-id pub-id-type="pmid">17517438</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR30">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Meyer</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Baumann</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Jancke</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Electrical brain imaging reveals spatio-temporal dynamics of timbre perception in humans</article-title>
          <source>NeuroImage</source>
          <year>2006</year>
          <volume>32</volume>
          <fpage>1510</fpage>
          <lpage>1523</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.04.193</pub-id>
          <pub-id pub-id-type="pmid">16798014</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR31">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>N&#xE4;&#xE4;t&#xE4;nen</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Selective attention and evoked potentials in humans - a critical review</article-title>
          <source>Biological Psychology</source>
          <year>1975</year>
          <volume>2</volume>
          <fpage>237</fpage>
          <lpage>307</lpage>
          <pub-id pub-id-type="doi">10.1016/0301-0511(75)90038-1</pub-id>
          <pub-id pub-id-type="pmid">1156626</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR32">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>N&#xE4;&#xE4;t&#xE4;nen</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Processing negativity: An evoked-potential reflection of selective attention</article-title>
          <source>Psychological Bulletin</source>
          <year>1982</year>
          <volume>92</volume>
          <issue>3</issue>
          <fpage>605</fpage>
          <lpage>640</lpage>
          <pub-id pub-id-type="doi">10.1037/0033-2909.92.3.605</pub-id>
          <pub-id pub-id-type="pmid">7156260</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR33">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>N&#xE4;&#xE4;t&#xE4;nen</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Picton</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>The N1 wave of the human electric and magnetic response to sound: a review and an analysis of the component structure</article-title>
          <source>Psychophysiology</source>
          <year>1987</year>
          <volume>24</volume>
          <issue>4</issue>
          <fpage>375</fpage>
          <lpage>425</lpage>
          <pub-id pub-id-type="doi">10.1111/j.1469-8986.1987.tb00311.x</pub-id>
          <pub-id pub-id-type="pmid">3615753</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR34">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pearce</surname>
              <given-names>M.T.</given-names>
            </name>
            <name>
              <surname>Herrojo Ruiz</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kapasi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wiggins</surname>
              <given-names>G.A.</given-names>
            </name>
            <name>
              <surname>Bhattacharya</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Unsupervised statistical learning underpins computational, behavioral, and neural manifestations of musical expectation</article-title>
          <source>NeuroImage</source>
          <year>2010</year>
          <volume>50</volume>
          <fpage>302</fpage>
          <lpage>313</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.12.019</pub-id>
          <pub-id pub-id-type="pmid">20005297</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR35">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Perrin</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Pernier</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Bertrand</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Echallier</surname>
              <given-names>J.F.</given-names>
            </name>
          </person-group>
          <article-title>Spherical splines for scalp potential and current mapping.</article-title>
          <source>Electroencephalography and Clinical Neurophysiology</source>
          <year>1989</year>
          <volume>72</volume>
          <issue>2</issue>
          <fpage>184</fpage>
          <lpage>187</lpage>
          <pub-id pub-id-type="doi">10.1016/0013-4694(89)90180-6</pub-id>
          <pub-id pub-id-type="pmid">2464490</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR36">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Potter</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Fenwick</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Abecasis</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Brochard</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Perceiving rhythm where none exists: Event-related potential (ERP) correlates of subjective accenting</article-title>
          <source>Cerebral Cortex</source>
          <year>2009</year>
          <volume>45</volume>
          <fpage>103</fpage>
          <lpage>109</lpage>
        </mixed-citation>
      </ref>
      <ref id="CR37">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Repp</surname>
              <given-names>B.H.</given-names>
            </name>
          </person-group>
          <article-title>Perceiving the numerosity of rapidly occurring auditory events in metrical and nonmetrical contexts.</article-title>
          <source>Perception and Psychophysics</source>
          <year>2007</year>
          <volume>69</volume>
          <issue>4</issue>
          <fpage>529</fpage>
          <lpage>543</lpage>
          <pub-id pub-id-type="doi">10.3758/BF03193910</pub-id>
          <pub-id pub-id-type="pmid">17727106</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR38">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schaefer</surname>
              <given-names>R.S.</given-names>
            </name>
            <name>
              <surname>Desain</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Suppes</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Structural decomposition of EEG signatures of melodic processing.</article-title>
          <source>Biological Psychology</source>
          <year>2009</year>
          <volume>82</volume>
          <fpage>253</fpage>
          <lpage>259</lpage>
          <pub-id pub-id-type="doi">10.1016/j.biopsycho.2009.08.004</pub-id>
          <pub-id pub-id-type="pmid">19698758</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR39">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Shahin</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Roberts</surname>
              <given-names>L.E.</given-names>
            </name>
            <name>
              <surname>Pantev</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Trainor</surname>
              <given-names>L.J.</given-names>
            </name>
            <name>
              <surname>Ross</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Modulation of p2 auditory-evoked responses by the spectral complexity of sounds</article-title>
          <source>NeuroReport</source>
          <year>2005</year>
          <volume>16</volume>
          <issue>16</issue>
          <fpage>1781</fpage>
          <lpage>1785</lpage>
          <pub-id pub-id-type="doi">10.1097/01.wnr.0000185017.29316.63</pub-id>
          <pub-id pub-id-type="pmid">16237326</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR40">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Snyder</surname>
              <given-names>J.S.</given-names>
            </name>
            <name>
              <surname>Large</surname>
              <given-names>E.W.</given-names>
            </name>
          </person-group>
          <article-title>Gamma-band activity reflects the metric structure of rhythmic tone sequences</article-title>
          <source>Cognitive Brain Research</source>
          <year>2005</year>
          <volume>24</volume>
          <fpage>117</fpage>
          <lpage>126</lpage>
          <pub-id pub-id-type="doi">10.1016/j.cogbrainres.2004.12.014</pub-id>
          <pub-id pub-id-type="pmid">15922164</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR41">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Walter</surname>
              <given-names>W.G.</given-names>
            </name>
            <name>
              <surname>Cooper</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Aldridge</surname>
              <given-names>V.J.</given-names>
            </name>
            <name>
              <surname>McCallum</surname>
              <given-names>W.C.</given-names>
            </name>
            <name>
              <surname>Winter</surname>
              <given-names>A.L.</given-names>
            </name>
          </person-group>
          <article-title>Contingent negative variation: an electric sign of sensorimotor association and expectancy in the human brain.</article-title>
          <source>Nature</source>
          <year>1964</year>
          <volume>203</volume>
          <fpage>380</fpage>
          <lpage>384</lpage>
          <pub-id pub-id-type="doi">10.1038/203380a0</pub-id>
          <pub-id pub-id-type="pmid">14197376</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR42">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Winkler</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Denham</surname>
              <given-names>S.L.</given-names>
            </name>
            <name>
              <surname>Nelken</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>Modeling the auditory scene: predictive regularity representations and perceptual objects</article-title>
          <source>Trends in Cognitive Sciences</source>
          <year>2009</year>
          <volume>13</volume>
          <issue>12</issue>
          <fpage>532</fpage>
          <lpage>540</lpage>
          <pub-id pub-id-type="doi">10.1016/j.tics.2009.09.003</pub-id>
          <pub-id pub-id-type="pmid">19828357</pub-id>
        </mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>
