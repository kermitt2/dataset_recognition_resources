<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v3.0 20080202//EN" "archivearticle3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
  <?properties open_access?>
  <?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
  <?DTDIdentifier.IdentifierType public?>
  <?SourceDTD.DTDName A++V2.4.dtd?>
  <?SourceDTD.Version 2.4?>
  <?ConverterInfo.XSLTName springer2nlmx2.xsl?>
  <?ConverterInfo.Version 2?>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Neural Comput Appl</journal-id>
      <journal-title-group>
        <journal-title>Neural Computing &amp; Applications</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">0941-0643</issn>
      <issn pub-type="epub">1433-3058</issn>
      <publisher>
        <publisher-name>Springer-Verlag</publisher-name>
        <publisher-loc>London</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">2886091</article-id>
      <article-id pub-id-type="pmid">20651904</article-id>
      <article-id pub-id-type="publisher-id">323</article-id>
      <article-id pub-id-type="doi">10.1007/s00521-009-0323-6</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Original Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Research on an online self-organizing radial basis function neural network</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Han</surname>
            <given-names>Honggui</given-names>
          </name>
          <address>
            <email>Rechard112@emails.bjut.edu.cn</email>
          </address>
          <xref ref-type="aff" rid="Aff1"/>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>Qili</given-names>
          </name>
          <address>
            <email>shuang3045@163.com</email>
          </address>
          <xref ref-type="aff" rid="Aff1"/>
        </contrib>
        <contrib contrib-type="author" corresp="yes">
          <name>
            <surname>Qiao</surname>
            <given-names>Junfei</given-names>
          </name>
          <address>
            <email>adqiao@sina.com</email>
          </address>
          <xref ref-type="aff" rid="Aff1"/>
        </contrib>
        <aff id="Aff1">College of Electronic and Control Engineering, Beijing University of Technology, Beijing, China </aff>
      </contrib-group>
      <pub-date pub-type="epub">
        <day>9</day>
        <month>1</month>
        <year>2010</year>
      </pub-date>
      <pub-date pub-type="pmc-release">
        <day>9</day>
        <month>1</month>
        <year>2010</year>
      </pub-date>
      <pub-date pub-type="ppub">
        <month>7</month>
        <year>2010</year>
      </pub-date>
      <volume>19</volume>
      <issue>5</issue>
      <fpage>667</fpage>
      <lpage>676</lpage>
      <history>
        <date date-type="received">
          <day>20</day>
          <month>10</month>
          <year>2008</year>
        </date>
        <date date-type="accepted">
          <day>9</day>
          <month>12</month>
          <year>2009</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>&#xA9; The Author(s) 2010</copyright-statement>
      </permissions>
      <abstract>
        <p>A new growing and pruning algorithm is proposed for radial basis function (RBF) neural network structure design in this paper, which is named as self-organizing RBF (SORBF). The structure of the RBF neural network is introduced in this paper first, and then the growing and pruning algorithm is used to design the structure of the RBF neural network automatically. The growing and pruning approach is based on the radius of the receptive field of the RBF nodes. Meanwhile, the parameters adjusting algorithms are proposed for the whole RBF neural network. The performance of the proposed method is evaluated through functions approximation and dynamic system identification. Then, the method is used to capture the biochemical oxygen demand (BOD) concentration in a wastewater treatment system. Experimental results show that the proposed method is efficient for network structure optimization, and it achieves better performance than some of the existing algorithms.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Self-organizing RBF neural network (SORBF)</kwd>
        <kwd>Growing and pruning approach</kwd>
        <kwd>BOD soft measurement</kwd>
      </kwd-group>
      <custom-meta-group>
        <custom-meta>
          <meta-name>issue-copyright-statement</meta-name>
          <meta-value>&#xA9; Springer-Verlag London Limited 2010</meta-value>
        </custom-meta>
      </custom-meta-group>
    </article-meta>
  </front>
  <body>
    <sec id="Sec1">
      <title>Introduction</title>
      <p>Radial basis function (RBF) neural networks offer an efficient mechanism for approximating complex nonlinear functions [<xref ref-type="bibr" rid="CR1">1</xref>], pattern recognition [<xref ref-type="bibr" rid="CR2">2</xref>], modeling and controlling dynamic systems [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR4">4</xref>] from the input&#x2013;output data. In fact, the selection of RBF neural network for a special application is dependent on its structure and learning abilities. Recently, with the research objects becoming more and more complex, the conventional RBF neural network with the fixed structures can not satisfy the requirement. The most difficult bottlenecks are the initial number of the hidden nodes, the initial position, and width of the RBF nodes.</p>
      <p>In order to solve the previously mentioned problems, some colleagues have found several kinds of methods. A significant contribution to adding nodes to the neural network is made by Platt [<xref ref-type="bibr" rid="CR5">5</xref>] through the development of a resource allocation network (RAN), in which hidden nodes are added sequentially based on the novelty of the new data. Karayiannis et al. [<xref ref-type="bibr" rid="CR6">6</xref>] propose a growing RBF neural network, in which a merging supervised and unsupervised learning algorithm is presented. The structure of the RBF neural network can be changed by the research objects, but this method does not consider the redundant nodes in the hidden layer. On the other side, Yingwei et al. [<xref ref-type="bibr" rid="CR7">7</xref>] introduce a pruning strategy based on the relative contribution of each hidden neuron to the overall network output to reduce the complex of the RBF neural network. The resulting neural network is minimal in theory by the presented method. Other pruning methods for RBF neural networks have been proposed by Salmer&#xF3;n et al. [<xref ref-type="bibr" rid="CR8">8</xref>] and Rojas et al. [<xref ref-type="bibr" rid="CR9">9</xref>]. Unfortunately, one of the disadvantages of pruning algorithms is that the computational cost is heavy, since the majority of the training time is spent on networks larger than necessary. A promising alternative to growing or pruning alone is to combine them. Wu et al. [<xref ref-type="bibr" rid="CR10">10</xref>] propose a hierarchical online self-organizing learning algorithm for dynamic RBF neural networks. However, all the widths of Gaussian membership functions are the same. This usually does not coincide with the reality, especially when input variables have significantly different operating intervals. Guang-Bin Huang et al. [<xref ref-type="bibr" rid="CR11">11</xref>] propose a simple sequential learning algorithm for RBF neural network referred to growing and pruning algorithm for RBF (GAP-RBF). And in the later, they advance this GAP-RBF to GGAP-RBF [<xref ref-type="bibr" rid="CR12">12</xref>]. Both GAP-RBF and GGAP-RBF neural networks decide to add new nodes or reduce the redundant nodes based on the &#x201C;significance&#x201D; of a neuron and links it to the learning accuracy. However, as these algorithms are online procedures, they do not optimize the network over all past training data. Besides, the network initialization requires a prior knowledge about the data which may not be available.</p>
      <p>For the position and the width of the RBF nodes in the RBF neural network, there are some other papers which adjust the parameters of the position and the width in the learning process. Juan Ignacio et al. [<xref ref-type="bibr" rid="CR13">13</xref>] analyze the bounded-ness of the coefficients involved in Gaussian expansion series. These series arise from the reconstruction of band-limited functions, applying the sampling theorem with Gaussians as the reconstruction filters. The width can be changed by the approximation errors and consequently to the accuracy of the estimation. Sheng Chen et al. [<xref ref-type="bibr" rid="CR14">14</xref>] introduce a powerful symmetric RBF neural network classifier for nonlinear detection. By exploiting the inherent symmetry property of the optimal Bayesian detector, the proposed symmetric RBF neural network is capable of approaching the optimal classification performance using noisy training data. The RBF neural network construction process is robust to the choice of the RBF width and is computationally efficient. And the paper [<xref ref-type="bibr" rid="CR15">15</xref>] shows an adaptive RBF neural network method to estimate two immeasurable physical parameters on-line and to compensate for the model uncertainty and engine time varying dynamics. The adaptive law of the neural network is based on the Lyapunov theory, so that this RBF neural network can adjust the width of the RBFs, and the stability of the network was guaranteed. In recent years, wavelet neural networks [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR17">17</xref>] are considered to be better and simpler alternatives to adjust the position and width of the RBF nodes. In the wavelet RBF neural networks (WRBF), the activation function of hidden layer nodes is substituted with a type of wavelet functions. In these new networks, the position and dilation of the wavelet are fixed, and only the weights are optimized. Although there are other papers [<xref ref-type="bibr" rid="CR18">18</xref>&#x2013;<xref ref-type="bibr" rid="CR22">22</xref>] which are about the position and width of the RBF nodes, few of them consider the optimal number of the hidden layer nodes of the RBF network simultaneity.</p>
      <p>In this paper, a new online SORBF is proposed for the RBF structure design which can add the new RBF nodes and reduce the redundant RBF nodes in the hidden layer. In the structure design phase, the growing and pruning method is used to decide the satisfaction of the architecture of the neural network, which is based on the radius of the receptive field of the RBF nodes and the stable error of the requirement. At the same time, the position and width of the RBF nodes in the hidden layer are adjusted by the gradient-descend algorithm. For highlighting the effect of the SORBF, the performance of the SORBF is compared with other well-known RBF neural networks on some benchmark problems in the nonlinear functions approximation area and nonlinear dynamic system identification. And then, this SORBF is used to capture the key variables in a wastewater treatment system. The results indicate the SORBF can provide comparable generalization performance with less computational complexity.</p>
      <p>This paper is organized as follows. Section&#xA0;<xref rid="Sec2" ref-type="sec">2</xref> describes the details of RBF neural network. Section&#xA0;<xref rid="Sec3" ref-type="sec">3</xref> presents the RBF nodes selection algorithm based on self-organizing method. The algorithms for the position and width of the RBF nodes adjusting are also given in this section. Experimental results of the simulations are presented in Sect. <xref rid="Sec8" ref-type="sec">4</xref>, in order to demonstrate the superior performances of this proposed SORBF algorithm, the results are compared with other self-organizing RBF algorithms. Section&#xA0;<xref rid="Sec12" ref-type="sec">5</xref> summarizes the conclusions from this study.</p>
    </sec>
    <sec id="Sec2">
      <title>Radial basis function (RBF) neural network</title>
      <p>The standard radial basis function (RBF) neural network consists of three layers: an input layer, a hidden layer, and an output layer. Figure&#xA0;<xref rid="Fig1" ref-type="fig">1</xref> shows a schematic representation of the RBF network. The number of the nodes in the input and output layers is decided by the research objects. The nodes in the input layer and output layer represent the vector from an input space and a desired network response, respectively. Through a defined learning algorithm, the error between the actual and desired response is minimized by optimization criterions.<fig id="Fig1"><label>Fig.&#xA0;1</label><caption><p>Schematic representation of RBF neural network</p></caption><graphic xlink:href="521_2009_323_Fig1_HTML" id="MO1"/></fig></p>
      <p>As depicted in Fig.&#xA0;<xref rid="Fig1" ref-type="fig">1</xref>, the <italic>i</italic>th output node of the RBF network can be expressed as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ y_{i} = \sum\limits_{k = 1}^{N} {\varphi_{k} (\left\| {x - v_{k} } \right\|)w_{ik} } ,i = 1,2, \ldots ,m $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>x</italic>&#xA0;=&#xA0;[<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, &#x2026;, <italic>x</italic><sub><italic>n</italic></sub>]<sup><italic>T</italic></sup> is an input value; <italic>n</italic> is the number of input node; <italic>c</italic><sub><italic>k</italic></sub> is the center of the <italic>k</italic>th RBF node in the hidden layer, <italic>k</italic>&#xA0;=&#xA0;1, 2, &#x2026;, <italic>N</italic>, and <italic>N</italic> is the number of hidden nodes; ||<italic>x</italic>&#xA0;&#x2212;&#xA0;<italic>v</italic><sub><italic>k</italic></sub>|| denotes Euclidean distance between <italic>v</italic><sub><italic>k</italic></sub> and <italic>x</italic>; <inline-formula id="IEq1"><alternatives><tex-math id="M2">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \varphi_{k} ( \bullet ) $$\end{document}</tex-math><inline-graphic xlink:href="521_2009_323_Article_IEq1.gif"/></alternatives></inline-formula> is the nonlinear transfer function of the <italic>k</italic>th RBF node; <italic>w</italic><sub><italic>ik</italic></sub> is the weighting value between the <italic>k</italic>th RBF node and the <italic>i</italic>th output node; and <italic>m</italic> is the number of output nodes.</p>
      <p>Equation (<xref rid="Equ1" ref-type="">1</xref>) reveals that the output of the network is computed as a weighted sum of the hidden layer outputs. The nonlinear output of the hidden layer is described as <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \varphi_{k} ( \bullet ) $$\end{document}</tex-math><inline-graphic xlink:href="521_2009_323_Article_IEq2.gif"/></alternatives></inline-formula>, which are radial symmetrical. In this paper, the function chosen for this neural network is Gaussian function, and the description is shown as follows:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M4">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \varphi (x) = e^{{\left( { - {\frac{{(x - v)^{2} }}{{\delta^{2} }}}} \right)}} $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <italic>v</italic> and <italic>&#x3B4;</italic> are the parameter of position and width of the RBF nodes. Figure&#xA0;<xref rid="Fig2" ref-type="fig">2</xref> shows the RBF nodes with different width and position. The activation function most commonly used for classification and regression problems in the Gaussian function, because it is continuous, differentiable; it provides a softer output and improves the interpolation capabilities. The procedure to design an RBF neural network for functional approximation problem is shown below:<fig id="Fig2"><label>Fig.&#xA0;2</label><caption><p>RBF nodes with different widths and positions</p></caption><graphic xlink:href="521_2009_323_Fig2_HTML" id="MO4"/></fig><list list-type="order"><list-item><p>Initialize number <italic>N</italic> of the RBF nodes;</p></list-item><list-item><p>Initialize position <italic>v</italic> of the RBF nodes;</p></list-item><list-item><p>Initialize the width &#x3B4; of the RBF nodes;</p></list-item><list-item><p>Calculate the optimum value for the weights <italic>w</italic>;</p></list-item><list-item><p>Apply local search algorithm to adjust the widths and the positions of the RBF nodes.</p></list-item></list></p>
      <p>Based on 1&#x2013;3, the initializations of the number of the RBF nodes are very important, if an unsuitable initialization number of the RBF nodes is performed, the training time will be heavy, and the approximation error hard to be achieved. The reason is that during the execution of a local search algorithm to make a fine tuning of the positions and widths of the RBF nodes, the search algorithm is hard to offset the flaw of the architecture. So, a self-organizing algorithm is necessary for the RBF structure design. This algorithm can be used to add new RBF nodes and reduce redundancy RBF nodes, which solves the problem of the architecture. Meanwhile, the position and width of the RBF nodes are very important, if the position and width are not appropriate for the RBF nodes, there is a possibility of falling into a bad local minimum. When the RBF neural network selects the correct number of the RBF nodes, the parameters of the weights, the position and width of the RBF nodes will be adjusted at the same time.</p>
    </sec>
    <sec id="Sec3">
      <title>Self-organizing RBF neural network</title>
      <p>The key problems of the RBF neural network are the structure design and the parameters-learning approaches. The structure design mainly relies on the RBF nodes in the hidden layer. This SORBF algorithm can construct the RBF neural network automatically. The procedure to design the SORBF neural network for the application problems is shown below:<list list-type="order"><list-item><p>Initialize the number of RBF nodes <italic>N</italic>;</p></list-item><list-item><p>Initialize the position <italic>v</italic> and width <italic>&#x3B4;</italic> of the RBF nodes;</p></list-item><list-item><p>Initialize the radius <italic>r</italic> of the receptive field for each RBF node;</p></list-item><list-item><p>Calculate the optimum number of the RBF nodes;</p></list-item><list-item><p>Apply local search algorithm to adjust the position <italic>v</italic>, the width <italic>&#x3B4;</italic> and radius <italic>r</italic>;</p></list-item><list-item><p>Calculate the optimum value for the weights <italic>w</italic>.</p></list-item></list>This SORBF neural network can add or reduce the RBF nodes. It is a modification of the neural network structure design method with the aim of catching the suitable RBF neural network architecture.</p>
      <sec id="Sec4">
        <title>RBF nodes selection method</title>
        <p>The main steps of the RBF nodes selection method are shown as follows:<list list-type="order"><list-item><p>Initialization, <italic>k</italic>&#xA0;=&#xA0;1; randomly weight value connecting the output layer with the hidden layer <bold>W</bold>&#xA0;=&#xA0;(<italic>w</italic><sub><italic>ij</italic></sub>)<sub><italic>N&#xD7;m</italic></sub>, 0&#xA0;&#x2264;&#xA0;<italic>w</italic><sub><italic>ij</italic></sub>&#xA0;&#x2264;&#xA0;1; the number of the nodes in hidden layer are pre-given as <italic>N</italic>. The radius <italic>r</italic> of the receptive field for each RBF node is given initially as <italic>r</italic><sub>0</sub>, the expected error is <italic>E</italic><sub><italic>d</italic></sub>. The positions <italic>v</italic> of the RBF nodes are given randomly, and the width of the RBF nodes is given initially as <italic>&#x3B4;</italic><sub>0</sub>; <italic>i</italic>&#xA0;=&#xA0;1, 2, &#x2026;, <italic>N</italic>.</p></list-item><list-item><p>Find the winner node <italic>x</italic> in hidden layer, the winner node at time <italic>k</italic> can be ensured by:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \left\| {v_{x} - P(k)} \right\| = \mathop {\min }\limits_{x = 1,2, \ldots ,N} \left\| {v_{x} - P(k)} \right\|. $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>And ||&#xB7;|| is the Euclidean distance, <italic>P</italic>(<italic>k</italic>) is the input rector at time <italic>k</italic>.</p></list-item><list-item><p>Adding new RBF nodes or reducing redundant RBF nodes.</p><p>If: <italic>E</italic>(<italic>k</italic>)&#xA0;&gt;&#xA0;&#x3B1;<italic>E</italic><sub><italic>d</italic></sub>, and <inline-formula id="IEq3"><alternatives><tex-math id="M6">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \left\| {v_{x} - P(k)} \right\| &gt; \beta r_{x} , $$\end{document}</tex-math><inline-graphic xlink:href="521_2009_323_Article_IEq3.gif"/></alternatives></inline-formula> go to step 4;</p><p>If: <italic>E</italic>(<italic>k</italic>)&#xA0;&#x2264;&#xA0;&#x3B1;<italic>E</italic><sub><italic>d</italic></sub>, and <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \left\| {v_{x} - P(k)} \right\| &gt; \beta r_{x} , $$\end{document}</tex-math><inline-graphic xlink:href="521_2009_323_Article_IEq4.gif"/></alternatives></inline-formula> go to step 6;</p><p>If: <italic>E</italic>(<italic>k</italic>)&#xA0;&#x2264;&#xA0;&#x3B1;<italic>E</italic><sub><italic>d</italic></sub>, and <inline-formula id="IEq5"><alternatives><tex-math id="M8">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \left\| {v_{x} - P(k)} \right\| \le \beta r_{x} , $$\end{document}</tex-math><inline-graphic xlink:href="521_2009_323_Article_IEq5.gif"/></alternatives></inline-formula> go to step 5;</p><p>If: <italic>E</italic>(<italic>k</italic>)&#xA0;&gt;&#xA0;&#x3B1;<italic>E</italic><sub><italic>d</italic></sub>, and <inline-formula id="IEq6"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \left\| {v_{x} - P(k)} \right\| \le \beta r_{x} , $$\end{document}</tex-math><inline-graphic xlink:href="521_2009_323_Article_IEq6.gif"/></alternatives></inline-formula> go to step 6.</p><p>where 1&#xA0;&lt;&#xA0;&#x3B1;&#xA0;&lt;&#xA0;2, and 1&#xA0;&lt;&#xA0;<italic>&#x3B2;</italic>&#xA0;&lt;&#xA0;2.</p></list-item><list-item><p>Adding a new RBF node.<list list-type="order"><list-item><p>Finding out the nearest RBF node <italic>v</italic><sub><italic>x</italic></sub> connected with <italic>P</italic>(<italic>k</italic>) by the formula (<xref rid="Equ3" ref-type="">3</xref>) in the hidden layer.</p></list-item><list-item><p>Inserting a new RBF node <italic>r</italic> between the node <italic>x</italic> and <italic>P</italic>(<italic>k</italic>), the positions and the width values of the new RBF node are as follows:<disp-formula id="Equa"><alternatives><tex-math id="M10">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ v_{r} = aP(k) + (1 - a)v_{x} . $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equa.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \delta_{r} = \delta_{0} $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p></list-item></list></p></list-item></list></p>
        <p>



where, <italic>a</italic> is a plus constant less than 1.</p>
        <p><list list-type="order"><list-item><p>Initializing radius <italic>r</italic><sub><italic>r</italic></sub> of the receptive field for the new RBF node <italic>r</italic>:</p></list-item></list><disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M12">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ r_{r}^{{}} = r_{0} . $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula><list list-type="order"><list-item><p>Deleting a redundant RBF node.<list list-type="order"><list-item><p>Computing the distance <italic>d</italic><sub><italic>x</italic></sub> between the node <italic>v</italic><sub><italic>x</italic></sub> and the nearest node <italic>f</italic>. If <italic>d</italic><sub><italic>x</italic></sub>&#xA0;&lt;&#xA0;<italic>r</italic><sub><italic>x</italic></sub> and <italic>d</italic><sub><italic>x</italic></sub>&#xA0;&lt;&#xA0;<italic>r</italic><sub><italic>f</italic></sub>, go to next; else, break and go to (<xref rid="Equ6" ref-type="">6</xref>).</p></list-item><list-item><p>Deleting the node <italic>x</italic> or <italic>f</italic> (because the widths of these nodes are not the same, the final node left according to the value of the width).</p></list-item></list></p></list-item></list>
If <italic>r</italic><sub><italic>f</italic></sub>&#xA0;&lt;&#xA0;<italic>r</italic><sub><italic>x</italic></sub>, deleting the node <italic>v</italic><sub><italic>f</italic></sub>; else deleting the node <italic>v</italic><sub><italic>x</italic></sub>.</p>
        <p><list list-type="order"><list-item><p>Modifying the radius value <italic>r</italic><sub><italic>x</italic></sub> of the receptive field for the retained node (we assume the left node is <italic>x</italic>).</p></list-item></list><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ r_{x} = br_{x} + (1 - b)r_{f} $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where <italic>b</italic> is a plus constant more than 0.5 and less than 1.<list list-type="order"><list-item><p>Updating the positions and widths of the RBF nodes. Stopping until catching the training time or expected error <italic>E</italic><sub><italic>d</italic></sub>.</p></list-item></list></p>
        <p>The RBF node selection method is essentially implemented to seek the suitable RBF nodes in the hidden layer, where an input acts as the principle to access the respective nodes containing the adjustable weight parameters to compute the activity of RBF nodes. The self-organizing algorithm searches the correct RBF node response to each input vector by choosing the minimum Euclidean distance. For each input, the SORBF can find a suitable RBF node or change the activity nodes based on it. The criterion of the RBF selection is based on the radius of the receptive field.</p>
        <p>According to the former explanation of the RBF nodes selection, the structure of the initial RBF can be modified along with the learning process. The final architecture of the RBF neural network is suitable for the current objects. Following these steps, the SORBF will adjust the other parameters of the RBF neural network.</p>
      </sec>
      <sec id="Sec5">
        <title>Adjusting radius of the receptive field for the RBF nodes</title>
        <p>In this paper, the radius is used to judge whether the structure of the neural network should be reset or not. The radius of the receptive field can affect the capabilities of the final neural network. It is one of the essentials which relate to the results of the RBF neural network.</p>
        <p>The radiuses of the receptive field adjust along with the learning process based on the winner times. The principles are described as follows (we take the node <italic>x</italic> for example):<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M14">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ r_{x} = \left\{ {\begin{array}{ll} {\varepsilon_{x} r_{x} } &amp;{{\text{if}}\;x\;{\text{is}}\;{\text{a}}\;{\text{winner}}} \\ {\tau_{x} r_{x} } &amp; {{\text{if}}\;x\;{\text{is}}\;{\text{not}}\;{\text{a}}\;{\text{winner}}} \\ \end{array} } \right. $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>where <italic>&#x3B5;</italic><sub><italic>x</italic></sub>, <italic>&#x3C4;</italic><sub><italic>x</italic></sub> are the weight value-modifying parameters of node. In fact, <italic>&#x3B5;</italic><sub><italic>x</italic></sub> is more than 1, and <italic>&#x3C4;</italic><sub><italic>x</italic></sub> is less than 1. The real values of these two parameters are computed as:<disp-formula id="Equb"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \varepsilon_{x} = 1 + {\frac{1}{{\sqrt {2M_{x} } }}}. $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equb.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M16">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \tau_{x} = 1 - {\frac{1}{{10\sqrt {M_{x} } }}}. $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where <italic>M</italic><sub><italic>x</italic></sub> is the winner times of the RBF node <italic>v</italic><sub><italic>x</italic></sub>. In order to simplify the computing steps, we give <italic>&#x3B5;</italic><sub><italic>x</italic></sub> as 1.01 and <italic>&#x3C4;</italic><sub><italic>x</italic></sub> as 0.99. But the constant values may bring some problems. For example, the structures of the neural networks change acutely in the complex systems, if these two parameters are the constant values, the structures change slowly, finally, and they can hardly obtain the optimal networks.</p>
      </sec>
      <sec id="Sec6">
        <title>Adjusting weights, positions, and widths</title>
        <p>This RBF type function has three parameters: the position <italic>v</italic>; the width <italic>&#x3B4;</italic> of the RBF nodes; and the weights <italic>w</italic>. In fact, these three parameters relate to the final capabilities of the RBF neural networks directly.</p>
        <p>In order to train a neural network, the parameter-adjusting algorithm is based on the mean squared error (MSE) which is defined as:<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ E(t) = {\frac{1}{{N_{s} }}}\sum\limits_{t = 1}^{{N_{s} }} {(y_{d} (t) - y(t))^{2} } $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>where <italic>N</italic><sub><italic>s</italic></sub> is the total number of the samples, <italic>y</italic><sub><italic>d</italic></sub>(<italic>t</italic>) is the expected output of the <italic>t</italic> step and <italic>y</italic>(<italic>t</italic>) is the neural network output of the <italic>t</italic> step. The goal of this method is to reach <italic>E</italic>(<italic>t</italic>)&#xA0;&lt;&#xA0;<italic>E</italic><sub><italic>d</italic></sub> by learning. <italic>E</italic><sub><italic>d</italic></sub> is the expected stable error. The details of the adjusting process are as follows:<list list-type="order"><list-item><p>The weights <italic>w</italic>;<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M18">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ w_{ij} (t + 1) = w_{ij} (t) - \eta_{1} {\frac{\partial E(t)}{{\partial w_{ij} (t)}}}.\quad i = 1,2, \ldots ,N;j = 1,2, \ldots ,m. $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>
where <italic>&#x3B7;</italic><sub>1</sub> is a plus constant, and it is less than 1.</p></list-item><list-item><p>The width <italic>&#x3B4;</italic> of the RBF nodes;<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \delta_{i} (t + 1) = \delta_{i} (t) - \eta_{2}\; {\frac{\partial E(t)}{{\partial \delta_{i} (t)}}}.\quad i = 1,2, \ldots ,N. $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula>where <italic>&#x3B7;</italic><sub>2</sub> is a plus constant, and it is less than 1.</p></list-item><list-item><p>The position <italic>v</italic> of the RBF nodes;</p></list-item></list></p>
        <p>In Sect.&#xA0;<xref rid="Sec4" ref-type="sec">3.1</xref>, the position <italic>v</italic> of the new inserting nodes has been discussed, and the position <italic>v</italic> of the other RBF nodes will be discussed here.<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M20">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ v_{i} (t + 1) = \left\{ {\begin{array}{ll} {v_{i} (t) - \eta_{3} (P(k) - v_{i} (t))} &amp; {{\text{if}}\;v_{i} \;{\text{is}}\;{\text{the}}\;{\text{winner}}} \\ {v_{i} (t) - \eta_{4} {\frac{\partial E(t)}{{\partial v_{i} (t)}}}} &amp; {\text{others}} \\ \end{array} } \right.\quad i = 1,2, \ldots ,N. $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula>where <italic>&#x3B7;</italic><sub>3</sub>, <italic>&#x3B7;</italic><sub>4</sub> are the plus constants, and they are less than 1.</p>
      </sec>
      <sec id="Sec7">
        <title>Computational complexity and memory requirements</title>
        <p>In this SORBF algorithm, the structure has to be updated after pruning or addition of a neuron. This requires updating the RBF nodes using (<xref rid="Equ5" ref-type="">5</xref>). The time required is, therefore, of the order <italic>O</italic>(<italic>N</italic><sub><italic>s</italic></sub><italic>TM</italic><sup>2</sup>), and the memory required is of the order <italic>O</italic>(<italic>N</italic><sub><italic>s</italic></sub><italic>M</italic>). Where <italic>N</italic><sub><italic>s</italic></sub> is the number of training date, <italic>T</italic> is the number of times the structure needs to be updated by adding or pruning a neuron, and <italic>M</italic>&#xA0;=&#xA0;<italic>N</italic>&#xA0;&#xD7;&#xA0;(<italic>n</italic>&#xA0;+&#xA0;1), in which <italic>n</italic> is the number of input variables, and <italic>N</italic> is the number of RBF nodes. In order to show the good performances of SORBF, the computational complexity and memory requirements of SORBF are compared with other dynamic adaptation methods such as DFNN [<xref ref-type="bibr" rid="CR10">10</xref>] and GAP-RBF [<xref ref-type="bibr" rid="CR11">11</xref>]. The DFNN used the linear least square (LLS) algorithm in batch mode, the time required in the approach based on LLS algorithm was of the order <italic>O</italic>(<italic>N</italic><sub arrange="stack"><italic>s</italic></sub><sup arrange="stack">2</sup><italic>M</italic><sup>2</sup>), and the memory required is of the order <italic>O</italic>(<italic>N</italic><sub arrange="stack"><italic>s</italic></sub><sup arrange="stack">2</sup><italic>M</italic>). Usually, <italic>T</italic>&#xA0;&#x226A;&#xA0;<italic>N</italic><sub><italic>s</italic></sub>. Besides, GAP-RBF needs only one-step (times and division) operation for pruning checking and a simple sparse matrix operation for parameter adjustment at each step by using the EKF (extended Kalman filter), since only the nearest neuron is checked for significance. The time required is, therefore, of the order <italic>O</italic>(<italic>N</italic><sub><italic>s</italic></sub><italic>TM</italic><sup>2</sup>), and the memory required is of the order <italic>O</italic>(<italic>N</italic><sub><italic>s</italic></sub><italic>M</italic>). So, the computational complexity and memory requirements of GAP-RBF are the same as SORBF, and the time required for DFNN is more than that of SORBF.</p>
      </sec>
    </sec>
    <sec id="Sec8">
      <title>Simulations</title>
      <p>To demonstrate the effectiveness of the proposed algorithm, there are three examples analyzed in this paper. They are functions approximation, nonlinear systems identification, and the nonlinear dynamic systems modeling. In order to show the performances of this SORBF algorithm, the results of the approximation and the identification are compared with other algorithms such as DFNN [<xref ref-type="bibr" rid="CR10">10</xref>] and GAP-RBF [<xref ref-type="bibr" rid="CR11">11</xref>].</p>
      <sec id="Sec9">
        <title>Function approximation</title>
        <p>A common function which was widely used to demonstrate the effect of the algorithms [<xref ref-type="bibr" rid="CR11">11</xref>] is used in this paper:<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ y = 0.8 \times e^{ - 0.2x} \times \sin (10x). $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>For each trial, the size of training samples is 200, and the size of testing samples is 200, the values of <italic>x</italic> are randomly distributed in the interval [0, 2].</p>
        <p>The real output at step <italic>k</italic> is <italic>y</italic>(<italic>k</italic>), the required value at time <italic>k</italic> is <italic>y</italic><sub><italic>d</italic></sub>(<italic>k</italic>), the error at step <italic>k</italic> will be <italic>y</italic>(<italic>k</italic>)&#xA0;=&#xA0;<italic>y</italic><sub><italic>d</italic></sub>(<italic>k</italic>)&#xA0;&#x2212;&#xA0;<italic>y</italic>(<italic>k</italic>). The goal training MSE is 0.01, the initial radius of every hidden node is 0.1, the initial weight of every hidden node is randomly given in the interval [0, 1], and the initial RBF nodes of SORBF are 2. The parameters <italic>a</italic> and <italic>b</italic> are given as 0.8 and 0.9 in this paper. The simulation results are shown as follows.</p>
        <p>Figure&#xA0;<xref rid="Fig3" ref-type="fig">3</xref> shows the approximating results by the SORBF, the result demonstrates SORBF can approximate this function exactly. Figure&#xA0;<xref rid="Fig4" ref-type="fig">4</xref> shows the error values in the approximating process within 1,000 steps; Fig.&#xA0;<xref rid="Fig5" ref-type="fig">5</xref> shows the dynamic number of the nodes in the training process; these two figures describe when the nodes add or eliminate in the hidden layer, the error value will shake; and finally, the error can catch the goal quickly. Figure&#xA0;<xref rid="Fig6" ref-type="fig">6</xref> shows RBF nodes with different width and position after training; Fig.&#xA0;<xref rid="Fig7" ref-type="fig">7</xref> shows weight values of the RBF nodes after training. Some averaged values as measures of the performances are selected for this algorithm of this example by the 20 independent runs: the CPU running time of the training process, the test MSE, and the number of preserved RBF nodes in the hidden layer. The detail results compared with other different algorithms are given in Table&#xA0;<xref rid="Tab1" ref-type="table">1</xref>. Based on the results, this proposed SORBF owns better performances than DFNN and GAP-RBF. The results prove that this proposed SORBF demonstrates a good performance for this function approximation. It should be noticed that this SORBF algorithm used for this functions approximation is insensitive to the initial structure of the neural network. The final structure of this SORBF is simpler; the memory space is fewer because of the simple structure.<fig id="Fig3"><label>Fig.&#xA0;3</label><caption><p>The results of function approximation</p></caption><graphic xlink:href="521_2009_323_Fig3_HTML" id="MO18"/></fig><fig id="Fig4"><label>Fig.&#xA0;4</label><caption><p>The error results in the training process</p></caption><graphic xlink:href="521_2009_323_Fig4_HTML" id="MO19"/></fig><fig id="Fig5"><label>Fig.&#xA0;5</label><caption><p>The hidden nodes in the training process</p></caption><graphic xlink:href="521_2009_323_Fig5_HTML" id="MO20"/></fig><fig id="Fig6"><label>Fig.&#xA0;6</label><caption><p>RBF nodes with different widths and positions after training</p></caption><graphic xlink:href="521_2009_323_Fig6_HTML" id="MO21"/></fig><fig id="Fig7"><label>Fig.&#xA0;7</label><caption><p>The weight values of the left nodes after training</p></caption><graphic xlink:href="521_2009_323_Fig7_HTML" id="MO22"/></fig><table-wrap id="Tab1"><label>Table&#xA0;1</label><caption><p>The performances comparison with different algorithms</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Algorithm</th><th align="left">CPU Time (s)</th><th align="left">Training error</th><th align="left">Testing error</th><th align="left">No. of nodes</th></tr></thead><tbody><tr><td align="left">DFNN</td><td char="." align="char">62.32</td><td char="." align="char">0.01</td><td char="." align="char">0.0861</td><td char="." align="char">25</td></tr><tr><td align="left">GAP-RBF</td><td char="." align="char">26.86</td><td char="." align="char">0.01</td><td char="." align="char">0.0415</td><td char="." align="char">19</td></tr><tr><td align="left">SORBF</td><td char="." align="char">22.67</td><td char="." align="char">0.01</td><td char="." align="char">0.0248</td><td char="." align="char">12</td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="Sec10">
        <title>Nonlinear dynamic system identification</title>
        <p>The plant is given as follows. It was used in [<xref ref-type="bibr" rid="CR10">10</xref>] and some other papers to demonstrate the effect of the algorithms:<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M22">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ y(t + 1) = {\frac{y(t)y(t - 1)[y(t) + 2.5]}{{1 + y^{2} (t) + y^{2} (t - 1)}}} + u(t). $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equc"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ t \in [1,100],y(0) = 0,y(1) = 0,u(t) = \sin \left( {{\frac{2\pi t}{25}}} \right). $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equc.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>A set of 100 input-target data is chosen as training data. The different initializations for the two cases are as follows: Case B.1, the initial number of RBF nodes is 2. Case B.2, the initial number of RBF nodes is 30. The training MSE for this example is 0.01. The other initializations of SORBF, DFNN, and GAP-RBF are the same as <italic>example A</italic>. This model is identified in series&#x2013;parallel mode, as given below:<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M24">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \hat{y}({\text{t}} + 1) = f(y(t),y(t - 1),u(t)). $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>There are three inputs (<italic>y</italic>(<italic>t</italic>), <italic>y</italic>(<italic>t</italic>&#xA0;+&#xA0;1), <italic>u</italic>(<italic>t</italic>)) and one output <italic>y</italic>(<italic>t</italic>&#xA0;+&#xA0;1) in the networks. Another 100 input-target data in the interval [301, 400] are chosen as the testing data. The results are shown as Figs.&#xA0;<xref rid="Fig8" ref-type="fig">8</xref>, <xref rid="Fig9" ref-type="fig">9</xref>, <xref rid="Fig10" ref-type="fig">10</xref>, and <xref rid="Fig11" ref-type="fig">11</xref> (Case B.1). Some averaged values as measures of the performances are selected for this algorithm of the two cases by the 20 independent runs: the CPU running time of the training process, the test MSE, and the number of preserved RBF nodes in the hidden layer.<fig id="Fig8"><label>Fig.&#xA0;8</label><caption><p>The training results (Case B.1)</p></caption><graphic xlink:href="521_2009_323_Fig8_HTML" id="MO26"/></fig><fig id="Fig9"><label>Fig.&#xA0;9</label><caption><p>The error value in the training process (Case B.1)</p></caption><graphic xlink:href="521_2009_323_Fig9_HTML" id="MO27"/></fig><fig id="Fig10"><label>Fig.&#xA0;10</label><caption><p>The number of the nodes in the training process (Case B.1)</p></caption><graphic xlink:href="521_2009_323_Fig10_HTML" id="MO28"/></fig><fig id="Fig11"><label>Fig.&#xA0;11</label><caption><p>The testing results (Case B.1)</p></caption><graphic xlink:href="521_2009_323_Fig11_HTML" id="MO29"/></fig></p>
        <p>As shown in Fig.&#xA0;<xref rid="Fig10" ref-type="fig">10</xref>, the SORBF organizes its structure with seven nodes after training (Case B.1). This is an important issue for the practical applications, and the structure is not changed after about 500 steps. The widths and the other parameters are adjusted continuously until achieving the required error. Note that in the learning process, the nodes may be added when the error is large, or the convergence speed is slow. This phenomenon is the same as the self-organizing strategy. Figure&#xA0;<xref rid="Fig9" ref-type="fig">9</xref> shows the error value in the learning process within 1,000 steps (Case B.1). This illuminates the learning performance of the SORBF. The detail results compared with other different algorithms are given in Table&#xA0;<xref rid="Tab2" ref-type="table">2</xref>. The results prove that the SORBF demonstrates a good performance in this example.<table-wrap id="Tab2"><label>Table&#xA0;2</label><caption><p>The performance comparison of different algorithms</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Approaches</th><th align="left" colspan="3">Case B.1: with 2 initial hidden nodes</th><th align="left" colspan="3">Case B.2: with 30 initial hidden nodes</th></tr><tr><th align="left">CPU time (min)</th><th align="left">Testing MSE</th><th align="left">No. of RBF nodes</th><th align="left">CPU time (min)</th><th align="left">Testing MSE</th><th align="left">No. of RBF nodes</th></tr></thead><tbody><tr><td align="left">DFNN</td><td char="." align="char">2.554</td><td char="." align="char">0.0264</td><td char="." align="char">15</td><td char="." align="char">3.864</td><td char="." align="char">0.0164</td><td char="." align="char">16</td></tr><tr><td align="left">GAP-RBF</td><td char="." align="char">0.823</td><td char="." align="char">0.0212</td><td char="." align="char">12</td><td char="." align="char">1.223</td><td char="." align="char">0.0112</td><td char="." align="char">13</td></tr><tr><td align="left">SORBF</td><td char="." align="char">0.521</td><td char="." align="char">0.0112</td><td char="." align="char">7</td><td char="." align="char">0.987</td><td char="." align="char">0.0112</td><td char="." align="char">8</td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="Sec11">
        <title>Soft measurement for BOD</title>
        <p>Rapid, accurate, and reliable measurements of BOD are a very desirable basis for monitoring or controlling wastewater treatment. Unfortunately, producing satisfactory BOD using hardware instrumentation has proved to be difficult. This paper addresses the issue of BOD estimation using a model-based approach which is relied on SORBF. The model estimates the BOD of the settled sewage using suspend solids (SS), chemical oxygen demand (COD), and PH data. The model is straightforward to apply online and offer a method of estimating BOD in real-time that is likely to be cheaper, more reliable and easier to maintain than hardware instrumentation. The structure of soft measurement technique based on SORBF is given as Fig.&#xA0;<xref rid="Fig12" ref-type="fig">12</xref>.<fig id="Fig12"><label>Fig.&#xA0;12</label><caption><p>The structure of soft measurement technique for BOD</p></caption><graphic xlink:href="521_2009_323_Fig12_HTML" id="MO30"/></fig></p>
        <p>The SS, COD, and PH data are used as inputs for the model to estimate the settled sewage BOD. The initial structure of the neural network is 3-2-1; the samples used are from a wastewater treatment plant of Beijing in 2006. Hundred groups are for training, and another 100 groups are for validating. The accuracy of the model estimate was quantitated using the absolute error <italic>Ee</italic> which is defined as follows:<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ Ee(t) = \left| {y_{d} (t) - y(t)} \right| $$\end{document}</tex-math><graphic xlink:href="521_2009_323_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula>where <italic>y</italic><sub><italic>d</italic></sub>(<italic>t</italic>) is the <italic>t</italic>th sample of settled sewage BOD data and <italic>y</italic>(<italic>t</italic>) is model estimate of the <italic>t</italic>th sample of settled sewage BOD data. The results are shown Figs.&#xA0;<xref rid="Fig13" ref-type="fig">13</xref>, <xref rid="Fig14" ref-type="fig">14</xref>, <xref rid="Fig15" ref-type="fig">15</xref>, <xref rid="Fig16" ref-type="fig">16</xref>, <xref rid="Fig17" ref-type="fig">17</xref>, and <xref rid="Fig18" ref-type="fig">18</xref>.<fig id="Fig13"><label>Fig.&#xA0;13</label><caption><p>The training process results of BOD</p></caption><graphic xlink:href="521_2009_323_Fig13_HTML" id="MO32"/></fig><fig id="Fig14"><label>Fig.&#xA0;14</label><caption><p>The absolute error <italic>Ee</italic> value of the trained results</p></caption><graphic xlink:href="521_2009_323_Fig14_HTML" id="MO33"/></fig><fig id="Fig15"><label>Fig.&#xA0;15</label><caption><p>The number of hidden nodes in the training process</p></caption><graphic xlink:href="521_2009_323_Fig15_HTML" id="MO34"/></fig><fig id="Fig16"><label>Fig.&#xA0;16</label><caption><p>The <italic>MSE</italic> error value of the training process</p></caption><graphic xlink:href="521_2009_323_Fig16_HTML" id="MO35"/></fig><fig id="Fig17"><label>Fig.&#xA0;17</label><caption><p>The predictions results of BOD</p></caption><graphic xlink:href="521_2009_323_Fig17_HTML" id="MO36"/></fig><fig id="Fig18"><label>Fig.&#xA0;18</label><caption><p>The absolute error <italic>Ee</italic> value of the predictions results</p></caption><graphic xlink:href="521_2009_323_Fig18_HTML" id="MO37"/></fig></p>
        <p>Figure&#xA0;<xref rid="Fig13" ref-type="fig">13</xref> shows the results of BOD after training; Fig.&#xA0;<xref rid="Fig14" ref-type="fig">14</xref> describes the absolute error value of the modeling results after training; the absolute errors are less than 1&#xA0;mg/L which are highly accurate. Based on the results, SORBF is demonstrated to be suitable for the BOD soft measurement online. Figure&#xA0;<xref rid="Fig15" ref-type="fig">15</xref> shows the dynamic RBF nodes within 1,000 steps; and Fig.&#xA0;<xref rid="Fig16" ref-type="fig">16</xref> describes the <italic>MSE</italic> error value of the training process, these results also illuminate the good performances of the SORBF. Figure&#xA0;<xref rid="Fig17" ref-type="fig">17</xref> shows the predictions results of BOD; Fig.&#xA0;<xref rid="Fig18" ref-type="fig">18</xref> shows the absolute error value of the predicting process. The results have demonstrated that the BOD trends in the settled sewage could be predicted with acceptable accuracy using only SS, COD, and PH data as model inputs. This SORBF-based approach is relatively straightforward to implement online, it could offer real-time predictions of BOD, whereas hardware instruments typically measure short-term BOD. It can be concluded that this is a significant feature, since BOD is the more commonly used and readily understood measure. Finally, this type of SORBF-based approach has the potential to be used in estimating a range of variables which are typically troublesome to measure using hardware. This suggests that such new algorithm can be relatively a cost-effective approach for measuring and other useful applications.</p>
      </sec>
    </sec>
    <sec id="Sec12">
      <title>Conclusions and discussion</title>
      <p>A new self-organizing RBF (SORBF) algorithm is proposed in this paper to design as well as train RBF. Neither the number of nodes in the hidden layer nor the parameters need to be predefined and fixed. They are adjusted automatically in the learning process. And then, two examples are used to demonstrate the effect of this algorithm in contrast to DFNN and GAP-RBF. The results of these two examples prove that this SORBF performs better than the other algorithms. Finally, this SORBF is used to measure and predict the BOD value online. This type of SORBF-based approach offers a promisingly inexpensive approach to real-time measurement of variables that have typically proved difficult to measure reliably using hardware.</p>
    </sec>
  </body>
  <back>
    <ack>
      <p content-type="acknowledgment">The authors would like to thank Prof. Guo-Qiang Bi and Prof. Huan-you Zhang for reading the manuscript and providing valuable comments. And, we would like to express our deepest gratitude to all anonymous reviewers and the editor-in-chief for their valuable comments. This work was supported by the National 863 Scheme Foundation of China under Grant 2007AA04Z160 and 2007A-A04Z160, National Science Foundation of China under Grants 60674066 and 60873043, Ph.D. Program Foundation from Ministry of Chinese Education under Grant 200800050004, and Beijing Municipal Natural Science Foundation under Grant 4092010.</p>
      <p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.</p>
    </ack>
    <ref-list id="Bib1">
      <title>References</title>
      <ref id="CR1">
        <label>1.</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nam</surname><given-names>MD</given-names></name><name><surname>Thanh</surname><given-names>TC</given-names></name></person-group><article-title>Approximation of function and its derivatives using radial basis function networks</article-title><source>Appl Math Model</source><year>2003</year><volume>27</volume><issue>3</issue><fpage>197</fpage><lpage>220</lpage>1024.65012<pub-id pub-id-type="doi">10.1016/S0307-904X(02)00101-4</pub-id></mixed-citation>
      </ref>
      <ref id="CR2">
        <label>2.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sing</surname>
              <given-names>JK</given-names>
            </name>
            <name>
              <surname>Basu</surname>
              <given-names>DK</given-names>
            </name>
            <name>
              <surname>Nasipuri</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Kundu</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <article-title>Face recognition using point symmetry distance-based RBF network</article-title>
          <source>Appl Soft Comput</source>
          <year>2007</year>
          <volume>7</volume>
          <issue>1</issue>
          <fpage>58</fpage>
          <lpage>70</lpage>
          <pub-id pub-id-type="doi">10.1016/j.asoc.2005.02.004</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR3">
        <label>3.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names>T</given-names>
            </name>
          </person-group>
          <article-title>RBFN-based decentralized adaptive control of a class of large-scale non-affine nonlinear systems</article-title>
          <source>Neural Comput Appl</source>
          <year>2008</year>
          <volume>17</volume>
          <issue>4</issue>
          <fpage>357</fpage>
          <lpage>364</lpage>
          <pub-id pub-id-type="doi">10.1007/s00521-007-0125-7</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR4">
        <label>4.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ram</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Srivastava</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Pandit</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Sharma</surname>
              <given-names>J</given-names>
            </name>
          </person-group>
          <article-title>Corrective action planning using RBF neural network</article-title>
          <source>Appl Soft Comput</source>
          <year>2007</year>
          <volume>7</volume>
          <issue>3</issue>
          <fpage>1055</fpage>
          <lpage>1063</lpage>
          <pub-id pub-id-type="doi">10.1016/j.asoc.2006.10.007</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR5">
        <label>5.</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Platt</surname><given-names>J</given-names></name></person-group><article-title>A resource-allocating network for function interpolation</article-title><source>Neural Comput</source><year>1991</year><volume>3</volume><issue>2</issue><fpage>213</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.1162/neco.1991.3.2.213</pub-id>1097332</mixed-citation>
      </ref>
      <ref id="CR6">
        <label>6.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Karayiannis</surname>
              <given-names>NB</given-names>
            </name>
            <name>
              <surname>Mi</surname>
              <given-names>GW</given-names>
            </name>
          </person-group>
          <article-title>Growing radial basis neural networks: merging supervised and unsupervised learning with network growth techniques</article-title>
          <source>IEEE Trans Neural Netw</source>
          <year>1997</year>
          <volume>8</volume>
          <issue>6</issue>
          <fpage>1492</fpage>
          <lpage>1506</lpage>
          <pub-id pub-id-type="doi">10.1109/72.641471</pub-id>
          <pub-id pub-id-type="pmid">18255750</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR7">
        <label>7.</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yingwei</surname><given-names>L</given-names></name><name><surname>Sundararajan</surname><given-names>N</given-names></name><name><surname>Saratchandran</surname><given-names>P</given-names></name></person-group><article-title>A sequential learning scheme for function approximation using minimal radial basis function (RBF) neural networks</article-title><source>Neural Comput</source><year>1997</year><volume>9</volume><issue>2</issue><fpage>461</fpage><lpage>478</lpage>1067.68586<pub-id pub-id-type="doi">10.1162/neco.1997.9.2.461</pub-id><pub-id pub-id-type="pmid">9117909</pub-id></mixed-citation>
      </ref>
      <ref id="CR8">
        <label>8.</label>
        <mixed-citation publication-type="other">Salmer&#xF3;n M, Ortega J, Puntonet CG, Prieto A, Rojas I (2002) SSA, SVD, QR-cp, and RBF model reduction. In Lecture notes in computer science, vol 2415. Springer, Germany, pp 589&#x2013;594</mixed-citation>
      </ref>
      <ref id="CR9">
        <label>9.</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rojas</surname><given-names>I</given-names></name><name><surname>Pomares</surname><given-names>H</given-names></name><name><surname>Bernier</surname><given-names>JL</given-names></name><name><surname>Ortega</surname><given-names>J</given-names></name><name><surname>Pino</surname><given-names>B</given-names></name><name><surname>Pelayo</surname><given-names>FJ</given-names></name><name><surname>Prieto</surname><given-names>A</given-names></name></person-group><article-title>Time series analysis using normalized PG-RBF network with regression weights</article-title><source>Neurocomputing</source><year>2002</year><volume>42</volume><fpage>267</fpage><lpage>285</lpage>1002.68708<pub-id pub-id-type="doi">10.1016/S0925-2312(01)00338-1</pub-id></mixed-citation>
      </ref>
      <ref id="CR10">
        <label>10.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Er</surname>
              <given-names>MJ</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>Y</given-names>
            </name>
          </person-group>
          <article-title>A fast approach for automatic generation of fuzzy rules by generalized dynamic fuzzy neural networks</article-title>
          <source>IEEE Trans Fuzzy Syst</source>
          <year>2002</year>
          <volume>9</volume>
          <issue>4</issue>
          <fpage>578</fpage>
          <lpage>584</lpage>
        </mixed-citation>
      </ref>
      <ref id="CR11">
        <label>11.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Huang</surname>
              <given-names>G-B</given-names>
            </name>
            <name>
              <surname>Saratchandran</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Sundararajan</surname>
              <given-names>N</given-names>
            </name>
          </person-group>
          <article-title>An efficient sequential learning algorithm for growing and pruning RBF (GAP-RBF) networks</article-title>
          <source>IEEE Trans Syst Man Cybern B</source>
          <year>2004</year>
          <volume>34</volume>
          <issue>6</issue>
          <fpage>2284</fpage>
          <lpage>2292</lpage>
          <pub-id pub-id-type="doi">10.1109/TSMCB.2004.834428</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR12">
        <label>12.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Huang</surname>
              <given-names>G-B</given-names>
            </name>
            <name>
              <surname>Saratchandran</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Sundararajan</surname>
              <given-names>N</given-names>
            </name>
          </person-group>
          <article-title>A generalized growing and pruning RBF (GGAP-RBF) neural network for function approximation</article-title>
          <source>IEEE Trans Neural Netw</source>
          <year>2005</year>
          <volume>16</volume>
          <issue>1</issue>
          <fpage>57</fpage>
          <lpage>67</lpage>
          <pub-id pub-id-type="doi">10.1109/TNN.2004.836241</pub-id>
          <pub-id pub-id-type="pmid">15732389</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR13">
        <label>13.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mulero-Martinez</surname>
              <given-names>JI</given-names>
            </name>
          </person-group>
          <article-title>Boundedness of the nominal coefficients in Gaussian RBF neural networks</article-title>
          <source>Neurocomputing</source>
          <year>2007</year>
          <volume>71</volume>
          <issue>1&#x2013;3</issue>
          <fpage>197</fpage>
          <lpage>220</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neucom.2007.01.011</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR14">
        <label>14.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Wolfgang</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Harris</surname>
              <given-names>CJ</given-names>
            </name>
            <name>
              <surname>Hanzo</surname>
              <given-names>L</given-names>
            </name>
          </person-group>
          <article-title>Symmetric RBF classifier for nonlinear detection in multiple-antenna-aided systems</article-title>
          <source>IEEE Trans Neural Netw</source>
          <year>2008</year>
          <volume>19</volume>
          <issue>5</issue>
          <fpage>737</fpage>
          <lpage>745</lpage>
          <pub-id pub-id-type="doi">10.1109/TNN.2007.911745</pub-id>
          <pub-id pub-id-type="pmid">18467204</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR15">
        <label>15.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>DL</given-names>
            </name>
          </person-group>
          <article-title>Adaptive RBF network for parameter estimation and stable air&#x2013;fuel ratio control</article-title>
          <source>Neural Netw</source>
          <year>2008</year>
          <volume>21</volume>
          <issue>1</issue>
          <fpage>102</fpage>
          <lpage>112</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neunet.2007.10.006</pub-id>
          <pub-id pub-id-type="pmid">18166378</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR16">
        <label>16.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jin</surname>
              <given-names>N</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>Wavelet basis function neural networks for sequential learning</article-title>
          <source>IEEE Trans Neural Netw</source>
          <year>2008</year>
          <volume>19</volume>
          <issue>3</issue>
          <fpage>535</fpage>
          <lpage>540</lpage>
          <pub-id pub-id-type="doi">10.1109/TNN.2007.914177</pub-id>
          <pub-id pub-id-type="pmid">18334373</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR17">
        <label>17.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gholizadeh</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Salajegheh</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>Torkzadeh</surname>
              <given-names>P</given-names>
            </name>
          </person-group>
          <article-title>Structural optimization with frequency constraints by genetic algorithm using wavelet radial basis function neural network</article-title>
          <source>J Sound Vib</source>
          <year>2008</year>
          <volume>312</volume>
          <issue>1&#x2013;2</issue>
          <fpage>316</fpage>
          <lpage>331</lpage>
          <pub-id pub-id-type="doi">10.1016/j.jsv.2007.10.050</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR18">
        <label>18.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Suetake</surname>
              <given-names>N</given-names>
            </name>
            <name>
              <surname>Uchino</surname>
              <given-names>E</given-names>
            </name>
          </person-group>
          <article-title>An RBFN&#x2013;Wiener hybrid filters using higher order signal statistics</article-title>
          <source>Appl Soft Comput</source>
          <year>2007</year>
          <volume>7</volume>
          <issue>3</issue>
          <fpage>915</fpage>
          <lpage>922</lpage>
          <pub-id pub-id-type="doi">10.1016/j.asoc.2006.04.005</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR19">
        <label>19.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Falcao</surname>
              <given-names>AO</given-names>
            </name>
            <name>
              <surname>Langlois</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Wichert</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>Flexible kernels for RBF networks</article-title>
          <source>Neurocomputing</source>
          <year>2007</year>
          <volume>69</volume>
          <issue>16&#x2013;18</issue>
          <fpage>2356</fpage>
          <lpage>2359</lpage>
        </mixed-citation>
      </ref>
      <ref id="CR20">
        <label>20.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sun</surname>
              <given-names>YF</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>YC</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>WL</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>HP</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>WZ</given-names>
            </name>
            <name>
              <surname>Cao</surname>
              <given-names>LJ</given-names>
            </name>
          </person-group>
          <article-title>Optimal partition algorithm of the RBF neural network and its application to financial time series forecasting</article-title>
          <source>Neural Comput Appl</source>
          <year>2005</year>
          <volume>14</volume>
          <issue>1</issue>
          <fpage>36</fpage>
          <lpage>44</lpage>
          <pub-id pub-id-type="doi">10.1007/s00521-004-0439-7</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR21">
        <label>21.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Guillen</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Pomares</surname>
              <given-names>H</given-names>
            </name>
            <name>
              <surname>Rojas</surname>
              <given-names>I</given-names>
            </name>
            <name>
              <surname>Gonzalez</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Herrera</surname>
              <given-names>LJ</given-names>
            </name>
            <name>
              <surname>Rojas</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Valenzuela</surname>
              <given-names>O</given-names>
            </name>
          </person-group>
          <article-title>Studying possibility in a clustering algorithm for RBFNN design for function approximation</article-title>
          <source>Neural Comput Appl</source>
          <year>2008</year>
          <volume>17</volume>
          <issue>1</issue>
          <fpage>75</fpage>
          <lpage>89</lpage>
        </mixed-citation>
      </ref>
      <ref id="CR22">
        <label>22.</label>
        <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Er</surname><given-names>MJ</given-names></name><name><surname>Wu</surname><given-names>S</given-names></name></person-group><article-title>A fast learning algorithm for parsimonious fuzzy neural systems</article-title><source>Fuzzy Sets Syst</source><year>2002</year><volume>126</volume><issue>3</issue><fpage>337</fpage><lpage>351</lpage>0996.68633<pub-id pub-id-type="doi">10.1016/S0165-0114(01)00034-3</pub-id>1909727</mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>
