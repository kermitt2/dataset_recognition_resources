<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v3.0 20080202//EN" "archivearticle3.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article">
  <?properties open_access?>
  <?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
  <?DTDIdentifier.IdentifierType public?>
  <?SourceDTD.DTDName journalpublishing.dtd?>
  <?SourceDTD.Version 2.3?>
  <?ConverterInfo.XSLTName jp2nlmx2.xsl?>
  <?ConverterInfo.Version 2?>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Open Neurol J</journal-id>
      <journal-id journal-id-type="publisher-id">TONEUJ</journal-id>
      <journal-title-group>
        <journal-title>The Open Neurology Journal</journal-title>
      </journal-title-group>
      <issn pub-type="epub">1874-205X</issn>
      <publisher>
        <publisher-name>Bentham Open</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">3106335</article-id>
      <article-id pub-id-type="pmid">21643536</article-id>
      <article-id pub-id-type="publisher-id">TONEUJ-5-37</article-id>
      <article-id pub-id-type="doi">10.2174/1874205X01105010037</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Effects of Consonant-Vowel Transitions in Speech Stimuli on Cortical Auditory Evoked Potentials in Adults</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name>
            <surname>Doellinger</surname>
            <given-names>Michael</given-names>
          </name>
          <xref ref-type="corresp" rid="cor1">*</xref>
          <xref ref-type="aff" rid="aff1">a</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Burger</surname>
            <given-names>Martin</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">a</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Hoppe</surname>
            <given-names>Ulrich</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">b</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Bosco</surname>
            <given-names>Enrico</given-names>
          </name>
          <xref ref-type="aff" rid="aff2">b</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Eysholdt</surname>
            <given-names>Ulrich</given-names>
          </name>
          <xref ref-type="aff" rid="aff1">a</xref>
        </contrib>
      </contrib-group>
      <aff id="aff1"><label>a</label>Department of Phoniatrics and Pediatric Audiology, University Hospital Erlangen, Bohlenplatz 21, 91054 Erlangen, Germany</aff>
      <aff id="aff2"><label>b</label>ENT Department, University Hospital Erlangen, Waldstra&#xDF;e 1, 91054 Erlangen, Germany</aff>
      <author-notes>
        <corresp id="cor1"><label>*</label>Address correspondence to this author at the Department of Phoniatrics and Pediatric Audiology, University Hospital Erlangen, Bohlenplatz 21, 91054 Erlangen, Germany; Tel: +49 9131 8533814; Fax: +49 9131 8532687; E-mail: <email xlink:href="michael.doellinger@uk-erlangen.de">michael.doellinger@uk-erlangen.de</email></corresp>
      </author-notes>
      <pub-date pub-type="epub">
        <day>4</day>
        <month>5</month>
        <year>2011</year>
      </pub-date>
      <pub-date pub-type="collection">
        <year>2011</year>
      </pub-date>
      <volume>5</volume>
      <fpage>37</fpage>
      <lpage>45</lpage>
      <history>
        <date date-type="received">
          <day>20</day>
          <month>11</month>
          <year>2010</year>
        </date>
        <date date-type="rev-recd">
          <day>25</day>
          <month>1</month>
          <year>2011</year>
        </date>
        <date date-type="accepted">
          <day>5</day>
          <month>2</month>
          <year>2011</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>&#xA9; Doellinger <italic>et al</italic>.; Licensee <italic>Bentham Open</italic>.</copyright-statement>
        <copyright-year>2011</copyright-year>
        <copyright-holder>Doellinger</copyright-holder>
        <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by-nc/3.0/">
          <license-p>This is an open access article licensed under the terms of the Creative Commons Attribution Non-Commercial License (<uri xlink:type="simple" xlink:href="http://creativecommons.org/licenses/by-nc/3.0/">http://creativecommons.org/licenses/by-nc/3.0/</uri>) which permits unrestricted, non-commercial use, distribution and reproduction in any medium, provided the work is properly cited.</license-p>
        </license>
      </permissions>
      <abstract>
        <p>We examined the neural activation to consonant-vowel transitions by cortical auditory evoked potentials (AEPs). The aim was to show whether cortical response patterns to speech stimuli contain components due to one of the temporal features, the voice-onset time (VOT). In seven normal-hearing adults, the cortical responses to four different monosyllabic words were opposed to the cortical responses to noise stimuli with the same temporal envelope as the speech stimuli. Significant hemispheric asymmetries were found for speech but not in noise evoked potentials. The difference signals between the AEPs to speech and corresponding noise stimuli revealed a significant negative component, which correlated with the VOT. The hemispheric asymmetries can be referred to rapid spectral changes. The correlation with the VOT indicates that the significant component in the difference signal reflects the perception of the acoustic change within the consonant-vowel transition. Thus, at the level of automatic processing, the characteristics of speech evoked potentials appear to be determined primarily by temporal aspects of the eliciting stimuli.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Auditory evoked potentials</kwd>
        <kwd>voice-onset time</kwd>
        <kwd>speech stimuli</kwd>
        <kwd>N1/P2 complex.</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <label>1</label>
      <title>INTRODUCTION</title>
      <p>During the auditory perception of speech, the central hearing has to process complex acoustic structures in real-time. Adequate processing of acoustic information is the requirement for speech and language development [<xref ref-type="bibr" rid="R1">1</xref>]. Particularly, disorders in the temporal processing are supposed to be responsible for deficient auditory discrimination abilities, which can result in an impaired speech perception [<xref ref-type="bibr" rid="R2">2</xref>]. Cortical auditory evoked potentials (AEPs) provide an objective method with high temporal resolution for the investigation of auditory speech processing. </p>
      <p>In the absence of attention, AEPs represent the automatic cortical processing on auditory sensory level. In this case, AEPs are dominated by obligatory components. In adults, these components are particularly the N1 and P2, also called N1/P2 complex [<xref ref-type="bibr" rid="R3">3</xref>]. The N1/P2 complex is thought to reflect the synchronous neural activity of structures in the thalamic-cortical segment of the central auditory system. The N1, a negative peak occurring approximately 100 ms after stimulus onset, is suggested to represent sound detection functions [<xref ref-type="bibr" rid="R4">4</xref>], since it is sensitive to onset sound features, such as intensity and interstimulus interval. For example, it could be demonstrated that the N1 indicates segmentation during the perception of continuous speech [<xref ref-type="bibr" rid="R5">5</xref>]. P2, a positive peak occurring approximately 200 ms after stimulus onset, is assumed to reflect sound content properties like acoustic or phonetic structures [<xref ref-type="bibr" rid="R6">6</xref>]. </p>
      <p>Several investigators have examined the possibility of using AEPs to determine the individual discrimination ability of phonetic structures in speech sounds [<xref ref-type="bibr" rid="R7">7</xref>-<xref ref-type="bibr" rid="R10">10</xref>]. Of particular interest are stimulus features constituting minimal pairs, such as formant gradients of consonant vocal-transitions and the voice onset-time (VOT). VOT is defined as the interval between the release from stop closure and the onset of laryngeal pulsing [<xref ref-type="bibr" rid="R11">11</xref>]. Sharma and colleagues (1999) investigated the morphology of speech evoked potentials in absence of attention depending on the subjective perception of stop consonants. Analyzing the cortical responses to a /da/-/ta/ continuum they found that stimuli with short VOTs (0&#x2013;30 ms) evoked a single N1, whereas stimuli with long VOTs (50&#x2013;80 ms) evoked two distinct negative components (N1&#x2019; and N1). This discontinuity in AEP morphology was in line with the subjective perception of voiceless sounds and thus, it was suggested to represent an electrophysiological correlate of categorical perception [<xref ref-type="bibr" rid="R7">7</xref>]. However, in another study using a /ga/-/ka/ continuum as stimuli, the change in N1 morphology to a double-peaked component did not signal subjective perception of a voiceless sound. This result indicated that the minimum VOT value of 40 ms required for the temporal separation of N1 depends on acoustic properties of the stimulus rather than the perceptual categorization [<xref ref-type="bibr" rid="R8">8</xref>].</p>
      <p>Martin <italic>et al</italic>. investigated cortical responses to syllables with different formant transitions embedded in masking noise [<xref ref-type="bibr" rid="R12">12</xref>]. The stimuli evoked an N1 which systematically changed with the stimulus energy (which is a sound onset feature), but not with the individual subjective discrimination.</p>
      <p>Ostroff <italic>et al</italic>. compared the cortical response to the syllable /sei/ with the cortical responses to the sibilant /s/ and to the vowel /ei/. They ascertained that the response to /sei/ was a combination of the AEPs to the onsets of the two constituent phonemes /s/ and /ei/ [<xref ref-type="bibr" rid="R13">13</xref>]. These overlapping AEPs within one stimulus response have been termed the acoustic change complex (ACC) [<xref ref-type="bibr" rid="R14">14</xref>]. The ACC is supposed to be composed of different N1/P2 complexes reflecting the acoustic changes across the entire stimulus [<xref ref-type="bibr" rid="R15">15</xref>].</p>
      <p>Overall, the aforementioned studies indicate that speech evoked potentials primarily reflect the phonetic composition of the stimuli. The aim of the presented study was to investigate whether cortical responses to speech stimuli contain AEP components referring to the VOT. For this purpose, AEPs to four natural monosyllabic words were compared with AEPs to noise stimuli showing the same temporal envelope as the word stimuli [<xref ref-type="bibr" rid="R16">16</xref>]. For a systematic analysis of the cortical responses, the speech stimuli were chosen with respect to varying VOTs.</p>
    </sec>
    <sec sec-type="materials|methods">
      <label>2</label>
      <title>MATERIALS AND METHODOLOGY</title>
      <sec>
        <label>2.1</label>
        <title>Participants</title>
        <p>We examined 7 normal-hearing, right-handed monolingual native German speakers (3 female, 4 male). The age range was 22 to 27 years. None was on medication at that time. All participants signed an informed consent. </p>
      </sec>
      <sec>
        <label>2.2</label>
        <title>Stimuli and Procedure</title>
        <p>In this study, two types of stimuli were used to elicit electrophysiologic responses: speech stimuli and noise sound stimuli. The speech stimuli consisted of four monosyllabic words naturally produced by a male speaker, taken from Freiburger speech discrimination test [<xref ref-type="bibr" rid="R17">17</xref>]. The stimuli were <italic>Ei</italic> /a:i/, <italic>Bett</italic> /b&#x3B5;t/, <italic>Dieb</italic> /di:b/, and <italic>Pult</italic> /pult/, which mean egg, bed, thief and desk. The durations of the stimuli <italic>Ei, Bett, Dieb</italic>, and <italic>Pult</italic> were 544 ms, 430 ms, 501 ms, and 567 ms, respectively. The speech stimuli exhibited a bandwidth of 8 kHz and were digitalized with 14 bit at a sampling rate of 20,000 s<sup>-1</sup>. Except <italic>Ei</italic>, all speech stimuli started with an initial stop consonant and were chosen with respect to their varying VOT. The VOTs were determined as VOT(<italic>Ei</italic>) = 0 ms, VOT(<italic>Bett</italic>) = 35 ms, VOT(<italic>Dieb</italic>) = 60 ms, and VOT(<italic>Pult</italic>) = 80 ms. The onset times of the closing consonants which succeed the vowels were determined as 215 ms with <italic>Bett,</italic> 390 ms with<italic> Dieb</italic>, and 395 ms with <italic>Pult</italic>.</p>
        <p>The speech stimuli served as raw material for the synthesis of the noise stimuli. Noise stimuli were created by randomly multiplying the discrete samples of the speech stimuli with &#xB1; 1 and subsequent 8 kHz low pass filtering. Fig. (<bold><xref ref-type="fig" rid="F1">1</xref></bold>) shows exemplarily the generation of the noise stimulus corresponding to <italic>Dieb</italic>. Thus, any phonetic information was removed from the speech stimuli, whereas the intensity was held constant. By this means, the synthesized noise stimuli <italic>Ei<sub>N</sub></italic>, <italic>Bett<sub>N</sub></italic>, <italic>Dieb<sub>N</sub></italic>, and <italic>Pult<sub>N</sub></italic> were generated.</p>
        <p>Spectrograms of all presented stimuli are depicted in Fig. (<bold><xref ref-type="fig" rid="F2">2</xref></bold>). The speech stimuli mainly consisted of frequencies lower than 2 kHz, whereas the noise stimuli were distributed over the entire bandwidth of 8 kHz. </p>
        <p>For equated presentation loudness, the applied stimuli were calibrated with respect to their root mean squares to a 1 kHz reference tone with an intensity level of 70 dB SPL (see [<xref ref-type="bibr" rid="R18">18</xref>]). Stimuli were presented in four blocks of 320 sounds each. Within the block, all stimuli were successively presented 40 times in the order of speech and corresponding noise stimulus (speech-noise pairs). Thus, each stimulus was presented 160 times. The interstimulus interval varied randomly between 1400 and 2100 ms. Subjects were tested in a soundproof booth. In order to minimize effects of the subjects&#x2019; state of attention on the applied stimuli, they were asked to watch a silent movie presented on a TV screen. </p>
      </sec>
      <sec>
        <label>2.3</label>
        <title>EEG Recording and Data Processing</title>
        <p>The EEG was derived with Ag/AgCl electrodes, which were integrated in a cap (Braincap, Brain Products GmbH, Gilching, Germany) with 30 fixed electrode positions (Fp1, Fp2, F3, F4, C3, C4, P3, P4, O1, O2, F7, F8, T3, T4, P7, P8, Fz, FCz, Cz, Pz, FC1, FC2, CP1, CP2, FC5, FC6, CP5, CP6, TP9, TP10). The electrode impedances were kept below 5 kOhm. For eye artefact rejection, an electro-oculogram was recorded by an electrode placed under the right eye. The EEG was recorded with Brain Amp-MR (Brain Products GmbH, Gilching, Germany) at a sampling rate of 500 Hz and digitalization of 16 bit.</p>
        <p>During data acquisition, all channels were referenced to FCz. Offline, data were re-referenced to the mean of TP9 and TP10. The EEG was 0.13 Hz &#x2013; 20 Hz band pass filtered with a slope of 12 dB/octave. The recording window included 100 ms prestimulus and 700 ms poststimulus time. Sweeps with artefacts measuring higher than 75 &#xB5;V were rejected. The remaining sweeps were averaged separately for each stimulus and prestimulus baselined.</p>
        <p>For each stimulus, grand mean waveforms were composed and grand mean latencies of the AEP components were visually identified. The individual peak latencies were determined in a time interval of the central latencies &#xB1; 24 ms. Amplitudes were calculated as a mean voltage at the 20 ms period centered at the individual peak latencies.</p>
      </sec>
      <sec>
        <label>2.4</label>
        <title>Data Analysis</title>
        <p>In the first step, statistical comparative tests of the AEP characteristics were carried out for both stimulus types, i.e. speech and noise stimuli, separately. In the second step, cortical responses to speech and corresponding noise sounds were analysed and compared among each other. </p>
        <sec>
          <label>2.4.1</label>
          <title>Responses to Speech and Noise Stimuli</title>
          <p>Preliminary one-way analyses of variance (ANOVA) were conducted to determine the scalp distribution of the amplitudes of N1 and P2. Since the largest responses for all stimuli were obtained at central positions, channel Cz was chosen for following one-way ANOVAs, which examined N1 and P2 latencies and amplitudes of the distinct speech and noise elicited AEPs. Newman-Keuls tests were carried out as post hoc analyses for the determination of equal-mean subsets. For the examination of hemispheric differences, N1-P2 interpeak amplitudes of the lateral scalp sites (T3, FC5, C3 vs. T4, FC6, C4) were compared <italic>via </italic>paired t-tests.</p>
        </sec>
        <sec>
          <label>2.4.2</label>
          <title>Speech-Noise Pairs</title>
          <p>For each subject time intervals were determined, in which speech and corresponding noise responses statistically differed. For this purpose, the sampling values of the single recorded sweeps in response to each stimulus were pooled for every discrete time step independently, resulting in 350 samples of speech and noise responses each (700 ms post-stimulus interval x 500 s<sup>-1 </sup>EEG sampling rate). Speech response samples of every time step were compared to the corresponding noise response sample by Student&#x2019;s two-tailed t-test. As level of significance p&lt;0.01 was selected. In order to avoid Bonferroni problems due to the large number of compared samples, a statistical difference between speech and noise response was accepted, when at least 12 consecutive sampling points (i.e., 24 ms interval) showed significance [<xref ref-type="bibr" rid="R19">19</xref>].</p>
          <p>On average level, difference signals were created by subtracting the noise elicited AEPs from the corresponding speech elicited AEPs. Latencies and amplitudes of occurring components in the difference signals were determined visually. Amplitudes were tested for significant appearance <italic>via </italic>one-tailed t-test. </p>
        </sec>
      </sec>
    </sec>
    <sec sec-type="results">
      <label>3</label>
      <title>RESULTS</title>
      <sec>
        <label>3.1</label>
        <title>Responses to Speech and Noise Stimuli</title>
        <p>Preliminary one-way ANOVAs, which separately analysed N1 and P2 peak amplitudes, revealed significant amplitude differences between the electrode sites for both, speech and noise stimuli (p &lt; 0.001, F<sub>26,162</sub> = 3.3 &#x2013; 8.6). Newman-Keuls post hoc analysis revealed that with all stimuli, the greatest amplitudes were elicited in the central area of the scalp (i.e., channels Cz3, Cz, Cz4) for both, N1 and P2 amplitude. Hence, the following illustrations and statistics are restricted to channel Cz. Fig. (<bold><xref ref-type="fig" rid="F3">3a</xref></bold>) shows the grand mean waveforms of the cortical responses to the four different speech stimuli. Morphologies of the speech-evoked potentials differed clearly across the stimuli. AEPs in response to <italic>Bett</italic> and <italic>Pult</italic> appear to reveal multiple overlapping response patterns. In all subjects, the predominant components were N1 and P2 in all responses. All speech stimuli elicited an N1 component which peaked around 113 &#xB1; 8 ms, and a P2 component that peaked in a range from 180 to 260 ms around 207 &#xB1; 36 ms. </p>
        <p>Fig. (<bold><xref ref-type="fig" rid="F3">3b</xref></bold>) shows the grand mean waveforms of the cortical responses to the four different noise stimuli. The waveforms of the different noise sound elicited AEPs were similar. The predominant components in all responses were N1 and P2. All noise stimuli elicited an N1 component which peaked around 103 &#xB1; 8 ms, and a P2 component that peaked around 175 &#xB1; 11 ms. </p>
        <p>In Tables <bold><xref ref-type="table" rid="T1">1</xref></bold> and <bold><xref ref-type="table" rid="T2">2</xref></bold>, N1 and P2 latencies and amplitudes of both stimulus types are given. Table <bold><xref ref-type="table" rid="T3">3</xref></bold> shows F values of one-way ANOVAs, conducted to reveal significant differences of these characteristics. Newman-Keuls post hoc tests did not show consistent correlations within a subset of both stimulus types. As it is apparent from the higher levels of significance, speech evoked AEP characteristics diverged stronger among each other than those evoked by noise sounds.</p>
        <p>For the analysis of hemispheric asymmetries, a paired t-test was conducted with N1-P2 interpeak amplitudes of opposite channel pairs T3 &#x2013; T4, FC5 &#x2013; FC6, and C3 &#x2013; C4, continuing from lateral to central. The mean N1-P2 interpeak amplitude of T3 was significantly smaller than that of T4 for the speech stimuli but not for the noise stimuli, see <italic>t</italic> values in Table <bold><xref ref-type="table" rid="T4">4</xref></bold>. While N1-P2 interpeak amplitudes at the positions FC5 and FC6 did not differ, an inversion from right to left hemispheric predominance occurred at the central positions C3, C4 for the speech stimuli (C3 &gt; C4, see Table <bold><xref ref-type="table" rid="T4">4</xref></bold>). Exemplarily, cortical responses to <italic>Dieb</italic> and <italic>Dieb<sub>N</sub></italic> across the entire scalp are depicted in Fig. (<bold><xref ref-type="fig" rid="F4">4</xref></bold>), in order to illustrate the hemispheric asymmetries of the speech evoked potentials.</p>
      </sec>
      <sec>
        <label>3.2</label>
        <title>Speech-Noise Pairs </title>
        <p>The AEP waveforms, in response to the speech sounds, were compared to those in response to the corresponding noise sounds. The time intervals, in which speech and corresponding noise responses significantly differed, are depicted in Fig. (<bold><xref ref-type="fig" rid="F5">5</xref></bold>). Blocks of black colour mark temporal areas of significant differences (p&lt;0.01) for each subject and each speech-noise pair. None of the speech-noise pair responses differs strongly during the initial 50 ms, as one can see from Figs. (<bold><xref ref-type="fig" rid="F5">5a</xref>-<xref ref-type="fig" rid="F5">5d</xref></bold>). The highest accumulations of areas of significant differences occur in the time window of 100 &#x2013; 300 ms after stimulus onset.</p>
        <p>Fig. (<bold><xref ref-type="fig" rid="F6">6</xref></bold>) shows the grand mean AEP waveforms of the speech (dashed lines) and noise stimuli (dotted lines) as well as the difference signals (solid lines) speech &#x2013; noise. In the difference signal, a negative component could be observed in a latency range of approximately 140 &#xB1; 15 ms. For each speech-noise pair, one-tailed t-tests (subjects x channels) showed that the occurring negative component was significantly smaller than zero. Latencies and amplitudes of this negative component are given in Table <bold><xref ref-type="table" rid="T5">5</xref></bold>. <italic>Ei</italic> elicits the smallest latency, latencies in response to <italic>Bett</italic> and <italic>Dieb</italic> are similar and <italic>Pult</italic> elicits the longest latency. Fig. (<bold><xref ref-type="fig" rid="F7">7</xref></bold>) shows the VOT of the four speech stimuli plotted against the peak latency of the negative component in the difference signal. A prominent correlation of VOT and peak latency is observable. Spearman rank correlation analysis revealed that the latencies were significant positively correlated with VOT (r = 0.66, p &lt; 0.01).</p>
      </sec>
    </sec>
    <sec sec-type="discussion">
      <label>4</label>
      <title>DISCUSSION</title>
      <p>Generally, different natural speech stimuli evoke distinct neural response patterns [<xref ref-type="bibr" rid="R20">20</xref>]. The aim of this study was to show whether cortical response patterns to speech stimuli contain components due to one of the temporal features, the voice-onset time. For this purpose, AEP characteristics in response to monosyllabic words were compared against each other and with cortical responses to noise stimuli with the same temporal envelope as the speech stimuli.</p>
      <sec>
        <label>4.1</label>
        <title>Responses to Speech and Noise Stimuli</title>
        <p>In accordance with other studies (e.g. [<xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R20"> 20</xref>,<xref ref-type="bibr" rid="R21"> 21</xref>]), for both types of stimuli, the AEPs were maximal at central electrode sites. The different natural speech stimuli evoked distinct neural response patterns, partially showing multiple overlapping responses. These distinctions indicate an extended neural activity with the auditory processing of speech in contrast to noise sound processing. Considering previous studies, a plausible explanation are the variant VOTs of the presented speech stimuli which affect the N1 and P2 [<xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R13"> 13</xref>,<xref ref-type="bibr" rid="R22"> 22</xref>,<xref ref-type="bibr" rid="R23"> 23</xref>].</p>
        <p>Winkler <italic>et al</italic>. demonstrated that N1 latencies evoked by spectral-pitch and missing-fundamental tones do not differ [<xref ref-type="bibr" rid="R24">24</xref>]. Hence, the different pitches of the presented vowels are probably not the reason for the found variability in the speech evoked potentials, at least not regarding the N1. An influence of the cortical response to the closing consonant on the P2 is not expectable due to its relatively late onset time.</p>
      </sec>
      <sec>
        <label>4.2</label>
        <title>Hemispheric Asymmetries</title>
        <p>In contrast to noise elicited AEPs, hemispheric differences occurred with speech evoked AEPs in the present study. Hemispheric left-overbalanced asymmetries with speech perception are well known [<xref ref-type="bibr" rid="R25">25</xref>-<xref ref-type="bibr" rid="R27">27</xref>]. Left-overbalance is primarily reported in studies with attentive designs. However, focused auditory attention can selectively modulate automatic processing in auditory cortex and thus, affect AEPs [<xref ref-type="bibr" rid="R28">28</xref>].</p>
        <p>Dehaene-Lambertz <italic>et al</italic>. demonstrated in a discrimination task that the left-hemispheric predominance with phoneme perception also can be achieved with appropriate non-speech stimuli [<xref ref-type="bibr" rid="R29">29</xref>]. Thus, specialization of left auditory cortex is not speech specific but depends on rapid spectrotemporal changes. However, in the present study, a right-hemispheric predominance was observable at temporal sites which inverted to a left-overbalance at central sites. This could be due to deviant locations in the activated auditory areas of both sides, resulting in different dipole orientations which underlie AEP derivations [<xref ref-type="bibr" rid="R30">30</xref>]. A more plausible explanation is a functional asymmetry of the auditory cortices. PET findings indicated complementary specializations of left and right auditory cortex&#x2019; belt areas [<xref ref-type="bibr" rid="R31">31</xref>]. Responses to temporal features were weighted towards the left, while responses to spectral features were weighted towards the right hemisphere. </p>
        <p>Thus, the spectral changes of the consonant-vowel transitions may be the reason for the observed right hemispheric overbalance. In contrast, noise stimuli did not evoke asymmetric responses.</p>
      </sec>
      <sec>
        <label>4.3</label>
        <title>Speech-Noise Pairs</title>
        <p>Although phonetic changes were present in the speech stimuli during the entire propagation of approximately 500 ms, AEPs of speech-noise pairs differed predominantly in the time window of the N1/P2 complex between 100 and 300 ms (Fig. <bold><xref ref-type="fig" rid="F5">5</xref></bold>), reflecting distinct onset processing.</p>
        <p>Subtracting noise from speech elicited AEPs facilitated a direct comparison of the speech-noise pairs. Thus, the presented paradigm allows observing the neural processing of spectral-acoustic and phonetic information, since differences in stimulus duration and amplitude are eliminated. The comparison revealed a significant negative component in the difference signal that seems to correlate with the VOT of the presented stimulus (see Steinschneider <italic>et al</italic>. who found that synchronized activity in the auditory cortex is time locked with consonant release and voicing onset [<xref ref-type="bibr" rid="R32">32</xref>]). Since the speech evoked N1 and P2 appear somewhat later and heightened in contrast to the noise evoked, the present study suggests that the observed negative component denotes a second N1/P2 complex which is merged with the onset response. Therefore it is not directly visible. This second N1/P2 complex is suggested to reflect the onset of the vowel as an acoustic event.</p>
        <p>This study demonstrated that rapid spectral changes in the perception of speech can be assessed electrophysiologically by means of a time-saving design. In case of modified cortical responses in subjects with central auditory processing disorders, this paradigm might serve as diagnostic tool for phonetic discrimination tasks since it reveals the neural processing of time-critical speech structures.</p>
      </sec>
    </sec>
  </body>
  <back>
    <ack>
      <title>ACKNOWLEDGEMENTS</title>
      <p>This work was supported by the grant from Deutsche Forschungsgemeinschaft DFG no. Ey 15/7-4 and ELAN no. PP-05.09.26.1.</p>
    </ack>
    <ref-list>
      <title>REFERENCES</title>
      <ref id="R1">
        <label>1</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gallagher</surname>
              <given-names>T</given-names>
            </name>
          </person-group>
          <article-title>Treatment research in speech, language and swallowing: lessons from child language disorders</article-title>
          <source>Folia Phoniatr</source>
          <year>1998</year>
          <volume>50</volume>
          <fpage>165</fpage>
          <lpage>82</lpage>
        </element-citation>
      </ref>
      <ref id="R2">
        <label>2</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tallal</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Miller</surname>
              <given-names>SL</given-names>
            </name>
            <name>
              <surname>Bedi</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Byma</surname>
              <given-names>G</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Language comprehension in language-learning, impaired children improved with acoustically modified speech</article-title>
          <source>Science</source>
          <year>1996</year>
          <volume>271</volume>
          <fpage>81</fpage>
          <lpage>3</lpage>
          <pub-id pub-id-type="pmid">8539604</pub-id>
        </element-citation>
      </ref>
      <ref id="R3">
        <label>3</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wolpaw</surname>
              <given-names>JR</given-names>
            </name>
            <name>
              <surname>Penry</surname>
              <given-names>JK</given-names>
            </name>
          </person-group>
          <article-title>A temporal component of the auditory evoked response</article-title>
          <source>Electroencephalogr Clin Neurophysiol</source>
          <year>1975</year>
          <volume>39</volume>
          <fpage>609</fpage>
          <lpage>920</lpage>
          <pub-id pub-id-type="pmid">53139</pub-id>
        </element-citation>
      </ref>
      <ref id="R4">
        <label>4</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>N&#xE4;&#xE4;t&#xE4;nen</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Picton</surname>
              <given-names>T</given-names>
            </name>
          </person-group>
          <article-title>The N1 wave of the human electric and magnetic response to sound: A review and an analysis of the component structure</article-title>
          <source>Psychophysiology</source>
          <year>1987</year>
          <volume>24</volume>
          <fpage>375</fpage>
          <lpage>425</lpage>
          <pub-id pub-id-type="pmid">3615753</pub-id>
        </element-citation>
      </ref>
      <ref id="R5">
        <label>5</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sanders</surname>
              <given-names>LD</given-names>
            </name>
            <name>
              <surname>Newport</surname>
              <given-names>EL</given-names>
            </name>
            <name>
              <surname>Neville</surname>
              <given-names>HJ</given-names>
            </name>
          </person-group>
          <article-title>Segmenting nonsense: An event related potential index of perceived onsets in continuous speech</article-title>
          <source>Nat Neurosci</source>
          <year>2002</year>
          <volume>5</volume>
          <fpage>700</fpage>
          <lpage>3</lpage>
          <pub-id pub-id-type="pmid">12068301</pub-id>
        </element-citation>
      </ref>
      <ref id="R6">
        <label>6</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ceponiene</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Alku</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Westerfield</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Torki</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Townsend</surname>
              <given-names>J</given-names>
            </name>
          </person-group>
          <article-title>ERPs differentiate syllable and nonphonetic sound processing in children and adults</article-title>
          <source>Psychophysiology</source>
          <year>2005</year>
          <volume>42</volume>
          <fpage>391</fpage>
          <lpage>406</lpage>
          <pub-id pub-id-type="pmid">16008768</pub-id>
        </element-citation>
      </ref>
      <ref id="R7">
        <label>7</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sharma</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Dorman</surname>
              <given-names>MF</given-names>
            </name>
          </person-group>
          <article-title>Cortical auditory evoked potential correlates of categorical perception of voice-onset time</article-title>
          <source>J Acoust Soc Am</source>
          <year>1999</year>
          <volume>106</volume>
          <fpage>1078</fpage>
          <lpage>83</lpage>
          <pub-id pub-id-type="pmid">10462812</pub-id>
        </element-citation>
      </ref>
      <ref id="R8">
        <label>8</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sharma</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Marsh</surname>
              <given-names>CM</given-names>
            </name>
            <name>
              <surname>Dorman</surname>
              <given-names>MF</given-names>
            </name>
          </person-group>
          <article-title>Relationship between N1 evoked potential morphology and the perception of voicing</article-title>
          <source>J Acoust Soc Am</source>
          <year>2000</year>
          <volume>108</volume>
          <fpage>3030</fpage>
          <lpage>5</lpage>
          <pub-id pub-id-type="pmid">11144595</pub-id>
        </element-citation>
      </ref>
      <ref id="R9">
        <label>9</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cunningham</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Nicol</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Zecker</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Kraus</surname>
              <given-names>N</given-names>
            </name>
          </person-group>
          <article-title>Speech-evoked neurophysiologic responses in children with learning problems: Development and behavioural correlates of perception</article-title>
          <source>Ear Hear</source>
          <year>2000</year>
          <volume>21</volume>
          <fpage>554</fpage>
          <lpage>68</lpage>
          <pub-id pub-id-type="pmid">11132782</pub-id>
        </element-citation>
      </ref>
      <ref id="R10">
        <label>10</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Khedr</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>El-Nasser</surname>
              <given-names>WA</given-names>
            </name>
            <name>
              <surname>Abdel Haleem</surname>
              <given-names>EK</given-names>
            </name>
            <name>
              <surname>Bakr</surname>
              <given-names>MS</given-names>
            </name>
            <name>
              <surname>Trakham</surname>
              <given-names>MN</given-names>
            </name>
          </person-group>
          <article-title>Evoked potentials and electroencephalography in stuttering</article-title>
          <source>Folia Phoniatr</source>
          <year>2000</year>
          <volume>52</volume>
          <fpage>178</fpage>
          <lpage>86</lpage>
        </element-citation>
      </ref>
      <ref id="R11">
        <label>11</label>
        <element-citation publication-type="confproc">
          <person-group person-group-type="author">
            <name>
              <surname>Lisker</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Abramson</surname>
              <given-names>AS</given-names>
            </name>
          </person-group>
          <source>&#x2018;The voicing dimension: some experiments in comparative phonetics&#x2019;</source>
          <year>1970</year>
          <conf-name>Proceedings of the VI International Congress of Phonetic Sciences</conf-name>
          <conf-loc>Prague</conf-loc>
        </element-citation>
      </ref>
      <ref id="R12">
        <label>12</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Martin</surname>
              <given-names>BA</given-names>
            </name>
            <name>
              <surname>Kurtzberg</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Stapells</surname>
              <given-names>DR</given-names>
            </name>
          </person-group>
          <article-title>The effects of decreased audibility produced by high-pass noise masking on N1 and the mismatch negativity to speech sounds /ba/ and /da/</article-title>
          <source>J Speech Lang Hear Res</source>
          <year>1999</year>
          <volume>42</volume>
          <fpage>271</fpage>
          <lpage>86</lpage>
          <pub-id pub-id-type="pmid">10229446</pub-id>
        </element-citation>
      </ref>
      <ref id="R13">
        <label>13</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ostroff</surname>
              <given-names>JM</given-names>
            </name>
            <name>
              <surname>Martin</surname>
              <given-names>BA</given-names>
            </name>
            <name>
              <surname>Boothroyd</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>Cortical evoked response to acoustic change within a syllable</article-title>
          <source>Ear Hear</source>
          <year>1998</year>
          <volume>19</volume>
          <fpage>290</fpage>
          <lpage>7</lpage>
          <pub-id pub-id-type="pmid">9728724</pub-id>
        </element-citation>
      </ref>
      <ref id="R14">
        <label>14</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Martin</surname>
              <given-names>BA</given-names>
            </name>
            <name>
              <surname>Boothroyd</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>Cortical, auditory, event-related potentials in response to periodic and aperiodic stimuli with the same spectral envelope</article-title>
          <source>Ear Hear</source>
          <year>1999</year>
          <volume>20</volume>
          <fpage>33</fpage>
          <lpage>44</lpage>
          <pub-id pub-id-type="pmid">10037064</pub-id>
        </element-citation>
      </ref>
      <ref id="R15">
        <label>15</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tremblay</surname>
              <given-names>KL</given-names>
            </name>
            <name>
              <surname>Kraus</surname>
              <given-names>N</given-names>
            </name>
            <name>
              <surname>McGee</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Ponton</surname>
              <given-names>CW</given-names>
            </name>
            <name>
              <surname>Otis</surname>
              <given-names>B</given-names>
            </name>
          </person-group>
          <article-title>Central auditory plasticity: Changes in the N1-P2 complex after speech-sound Training</article-title>
          <source>Ear Hear</source>
          <year>2001</year>
          <volume>22</volume>
          <fpage>79</fpage>
          <lpage>90</lpage>
          <pub-id pub-id-type="pmid">11324846</pub-id>
        </element-citation>
      </ref>
      <ref id="R16">
        <label>16</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kummer</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Burger</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Schuster</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Rosanowski</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Eysholdt</surname>
              <given-names>U</given-names>
            </name>
            <name>
              <surname>Hoppe</surname>
              <given-names>U</given-names>
            </name>
          </person-group>
          <article-title>Cortical auditory evoked potentials to acoustic changes in speech stimuli in children</article-title>
          <source>Folia Phoniatr</source>
          <year>2007</year>
          <volume>59</volume>
          <fpage>273</fpage>
          <lpage>80</lpage>
        </element-citation>
      </ref>
      <ref id="R17">
        <label>17</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Keller</surname>
              <given-names>F</given-names>
            </name>
          </person-group>
          <article-title>Verschiedene Aufnahmen des Sprachverst&#xE4;ndlichkeitstest &#x201C;Freiburger Test&#x201D; (Different recordings of the speech intelligibility test &#x201C;Freiburger Test&#x201D;)</article-title>
          <source>Biomed Techn</source>
          <year>1977</year>
          <volume>22</volume>
          <fpage>292</fpage>
          <lpage>8</lpage>
        </element-citation>
      </ref>
      <ref id="R18">
        <label>18</label>
        <element-citation publication-type="book">
          <collab>ANSI S3 - 1996</collab>
          <source>American national standards specification for audiometers</source>
          <year>1996</year>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>American National Standards Institute</publisher-name>
        </element-citation>
      </ref>
      <ref id="R19">
        <label>19</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Guthrie</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Buchwald</surname>
              <given-names>JS</given-names>
            </name>
          </person-group>
          <article-title>Significance testing of difference potentials</article-title>
          <source>Psychophysiology</source>
          <year>1991</year>
          <volume>28</volume>
          <fpage>240</fpage>
          <lpage>4</lpage>
          <pub-id pub-id-type="pmid">1946890</pub-id>
        </element-citation>
      </ref>
      <ref id="R20">
        <label>20</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tremblay</surname>
              <given-names>KL</given-names>
            </name>
            <name>
              <surname>Friesen</surname>
              <given-names>LM</given-names>
            </name>
            <name>
              <surname>Martin</surname>
              <given-names>BA</given-names>
            </name>
            <name>
              <surname>Wright</surname>
              <given-names>R</given-names>
            </name>
          </person-group>
          <article-title>Test-retest reliability of cortical evoked potentials using naturally produced speech sounds</article-title>
          <source>Ear Hear</source>
          <year>2003</year>
          <volume>24</volume>
          <fpage>225</fpage>
          <lpage>32</lpage>
          <pub-id pub-id-type="pmid">12799544</pub-id>
        </element-citation>
      </ref>
      <ref id="R21">
        <label>21</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ponton</surname>
              <given-names>CW</given-names>
            </name>
            <name>
              <surname>Eggermont</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Kwong</surname>
              <given-names>B</given-names>
            </name>
            <name>
              <surname>Dent</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <article-title>Maturation of human central auditory system activity: evidence from multi-channel evoked potentials</article-title>
          <source>Clin Neurophysiol</source>
          <year>2000</year>
          <volume>111</volume>
          <fpage>220</fpage>
          <lpage>36</lpage>
          <pub-id pub-id-type="pmid">10680557</pub-id>
        </element-citation>
      </ref>
      <ref id="R22">
        <label>22</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tremblay</surname>
              <given-names>KL</given-names>
            </name>
            <name>
              <surname>Piskosz</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Souza</surname>
              <given-names>P</given-names>
            </name>
          </person-group>
          <article-title>Effects of age and age-related hearing loss on the neural representation of speech cues</article-title>
          <source>Clin Neurophysiol</source>
          <year>2003</year>
          <volume>114</volume>
          <fpage>1332</fpage>
          <lpage>43</lpage>
          <pub-id pub-id-type="pmid">12842732</pub-id>
        </element-citation>
      </ref>
      <ref id="R23">
        <label>23</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gage</surname>
              <given-names>N</given-names>
            </name>
            <name>
              <surname>Poeppel</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Roberts</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Hickok</surname>
              <given-names>G</given-names>
            </name>
          </person-group>
          <article-title>Auditory evoked M100 reflects onset acoustics of speech sounds</article-title>
          <source>Brain Res</source>
          <year>1998</year>
          <volume>814</volume>
          <fpage>236</fpage>
          <lpage>9</lpage>
          <pub-id pub-id-type="pmid">9838140</pub-id>
        </element-citation>
      </ref>
      <ref id="R24">
        <label>24</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Winkler</surname>
              <given-names>I</given-names>
            </name>
            <name>
              <surname>Tervaniemi</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>N&#xE4;&#xE4;t&#xE4;nen</surname>
              <given-names>R</given-names>
            </name>
          </person-group>
          <article-title>Two separate codes for missing-fundamental pitch in the human auditory cortex</article-title>
          <source>J Acoust Soc Am</source>
          <year>1997</year>
          <volume>102</volume>
          <fpage>1072</fpage>
          <lpage>82</lpage>
          <pub-id pub-id-type="pmid">9265755</pub-id>
        </element-citation>
      </ref>
      <ref id="R25">
        <label>25</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Penhune</surname>
              <given-names>VB</given-names>
            </name>
            <name>
              <surname>Zatorre</surname>
              <given-names>RJ</given-names>
            </name>
            <name>
              <surname>McDonald</surname>
              <given-names>JD</given-names>
            </name>
            <name>
              <surname>Evans</surname>
              <given-names>AC</given-names>
            </name>
          </person-group>
          <article-title>Inter-hemispheric anatomical differences in human primary auditory cortex: probabilistic mapping and volume measurement from magnetic resonance scans</article-title>
          <source>Cereb Cortex</source>
          <year>1996</year>
          <volume>6</volume>
          <fpage>661</fpage>
          <lpage>72</lpage>
          <pub-id pub-id-type="pmid">8921202</pub-id>
        </element-citation>
      </ref>
      <ref id="R26">
        <label>26</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liegeois-Chauvel</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>de Graaf</surname>
              <given-names>JB</given-names>
            </name>
            <name>
              <surname>Laquitton</surname>
              <given-names>V</given-names>
            </name>
            <name>
              <surname>Chauvel</surname>
              <given-names>P</given-names>
            </name>
          </person-group>
          <article-title>Specialization of left auditory cortex for speech perception in man depends on temporal coding</article-title>
          <source>Cereb Cortex</source>
          <year>1999</year>
          <volume>9</volume>
          <fpage>484</fpage>
          <lpage>96</lpage>
          <pub-id pub-id-type="pmid">10450893</pub-id>
        </element-citation>
      </ref>
      <ref id="R27">
        <label>27</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Giraud</surname>
              <given-names>K</given-names>
            </name>
            <name>
              <surname>Demonet</surname>
              <given-names>JF</given-names>
            </name>
            <name>
              <surname>Habib</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Marquis</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Chauvel</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Liegeois-Chauvel</surname>
              <given-names>C</given-names>
            </name>
          </person-group>
          <article-title>Auditory evoked potential patterns to voiced and voiceless speech sounds in adult developmental dyslexics with persistent deficits</article-title>
          <source>Cereb Cortex</source>
          <year>2005</year>
          <volume>15</volume>
          <fpage>1524</fpage>
          <lpage>34</lpage>
          <pub-id pub-id-type="pmid">15689520</pub-id>
        </element-citation>
      </ref>
      <ref id="R28">
        <label>28</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Woldorff</surname>
              <given-names>MG</given-names>
            </name>
            <name>
              <surname>Gallen</surname>
              <given-names>CC</given-names>
            </name>
            <name>
              <surname>Hampson</surname>
              <given-names>SA</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Modulation of early sensory processing in human auditory cortex during auditory selective attention</article-title>
          <source>Proc Natl Acad Sci USA</source>
          <year>1993</year>
          <volume>90</volume>
          <fpage>8722</fpage>
          <lpage>26</lpage>
          <pub-id pub-id-type="pmid">8378354</pub-id>
        </element-citation>
      </ref>
      <ref id="R29">
        <label>29</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Dehaene-Lambertz</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Pallier</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Serniclaes</surname>
              <given-names>W</given-names>
            </name>
            <name>
              <surname>Sprenger-Charolles</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Jobert</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Deheane</surname>
              <given-names>S</given-names>
            </name>
          </person-group>
          <article-title>Neural correlates of switching from auditory to speech perception</article-title>
          <source>Neuroimage</source>
          <year>2005</year>
          <volume>24</volume>
          <fpage>21</fpage>
          <lpage>33</lpage>
          <pub-id pub-id-type="pmid">15588593</pub-id>
        </element-citation>
      </ref>
      <ref id="R30">
        <label>30</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rosburg</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Kreitschmann-Andermahr</surname>
              <given-names>I</given-names>
            </name>
            <name>
              <surname>Emmerich</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>Nowak</surname>
              <given-names>H</given-names>
            </name>
            <name>
              <surname>Sauer</surname>
              <given-names>H</given-names>
            </name>
          </person-group>
          <article-title>Hemispheric differences in frequency dependent dipole orientation of the human auditory evoked field component N100m</article-title>
          <source>Neurosci lett</source>
          <year>1998</year>
          <volume>258</volume>
          <fpage>105</fpage>
          <lpage>8</lpage>
          <pub-id pub-id-type="pmid">9875538</pub-id>
        </element-citation>
      </ref>
      <ref id="R31">
        <label>31</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zatorre</surname>
              <given-names>RJ</given-names>
            </name>
            <name>
              <surname>Belin</surname>
              <given-names>P</given-names>
            </name>
          </person-group>
          <article-title>Spectral and temporal processing in human auditory cortex</article-title>
          <source>Cereb Cortex</source>
          <year>2001</year>
          <volume>11</volume>
          <fpage>946</fpage>
          <lpage>53</lpage>
          <pub-id pub-id-type="pmid">11549617</pub-id>
        </element-citation>
      </ref>
      <ref id="R32">
        <label>32</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Steinschneider</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Volkov</surname>
              <given-names>IO</given-names>
            </name>
            <name>
              <surname>Noh</surname>
              <given-names>MD</given-names>
            </name>
            <name>
              <surname>Garell</surname>
              <given-names>PC</given-names>
            </name>
            <name>
              <surname>Howard</surname>
              <given-names>MA</given-names>
            </name>
          </person-group>
          <article-title>Temporal encoding of the voice onset time phonetic parameter by field potentials recorded directly from human auditory cortex</article-title>
          <source>J Neurophysiol</source>
          <year>1999</year>
          <volume>82</volume>
          <fpage>2346</fpage>
          <lpage>57</lpage>
          <pub-id pub-id-type="pmid">10561410</pub-id>
        </element-citation>
      </ref>
    </ref-list>
  </back>
  <floats-group>
    <fig id="F1" position="float">
      <label>Fig. (1)</label>
      <caption>
        <p>Speech stimulus Dieb and synthesized noise stimulus Dieb<sub>N</sub>. Both stimuli exhibit the same temporal envelope but phonetic information is discarded in the noise stimulus.</p>
      </caption>
      <graphic xlink:href="TONEUJ-5-37_F1"/>
    </fig>
    <fig id="F2" position="float">
      <label>Fig. (2)</label>
      <caption>
        <p>Spectral view of the presented stimuli. Top: Natural spoken monosyllabic words. Bottom: Synthetic noise sounds, derived from the monosyllabic words. Bandwidth of the stimuli is 8 kHz.</p>
      </caption>
      <graphic xlink:href="TONEUJ-5-37_F2"/>
    </fig>
    <fig id="F3" position="float">
      <label>Fig. (3)</label>
      <caption>
        <p>(<bold>a</bold>) Grand mean waveforms of speech stimuli elicited AEPs in response to Ei, Bett, Dieb, and Pult at channel Cz. (<bold>b</bold>) Grand mean waveforms of noise-sound elicited AEPs in response to Ei<sub>N</sub>, Bett<sub>N</sub>, Dieb<sub>N</sub>, and Pult<sub>N</sub> at channel Cz.</p>
      </caption>
      <graphic xlink:href="TONEUJ-5-37_F3"/>
    </fig>
    <fig id="F4" position="float">
      <label>Fig. (4)</label>
      <caption>
        <p>Grand mean AEPs to Dieb (solid) and Dieb<sub>N</sub> (dashed) over the entire scalp.</p>
      </caption>
      <graphic xlink:href="TONEUJ-5-37_F4"/>
    </fig>
    <fig id="F5" position="float">
      <label>Fig. (5)</label>
      <caption>
        <p>Temporal areas of significant differences between the responses to speech and noise stimuli at channel Cz for each subject (black areas). Level of significance is p&lt;0.01.</p>
      </caption>
      <graphic xlink:href="TONEUJ-5-37_F5"/>
    </fig>
    <fig id="F6" position="float">
      <label>Fig. (6)</label>
      <caption>
        <p>Grand mean waveforms at channel Cz of speech evoked responses (dashed), noise evoked responses (dotted) and the difference signal (solid).</p>
      </caption>
      <graphic xlink:href="TONEUJ-5-37_F6"/>
    </fig>
    <fig id="F7" position="float">
      <label>Fig. (7)</label>
      <caption>
        <p>Box and whisker plot of VOTs of the diverse speech stimuli against the latencies of the negative components in the difference signal of the cortical responses. Dashed lines within the boxes display the median, the edges of the boxes display the quartiles. Whiskers indicate maximal and minimal latencies unless outliers &#x2018;+&#x2019; occur (whiskers span 1.5 times the inter-quartile range).</p>
      </caption>
      <graphic xlink:href="TONEUJ-5-37_F7"/>
    </fig>
    <table-wrap id="T1" position="float">
      <label>Table 1</label>
      <caption>
        <p>Mean Latencies of N1 and P2 in ms Elicited by the Speech/Noise Stimuli at Channel Cz</p>
      </caption>
      <table frame="border" rules="cols" width="100%">
        <thead>
          <tr>
            <th rowspan="1" colspan="1">Peak</th>
            <th rowspan="1" colspan="1">Stimulus Type</th>
            <th rowspan="1" colspan="1">
              <italic>Ei</italic>
            </th>
            <th rowspan="1" colspan="1">
              <italic>Bett</italic>
            </th>
            <th rowspan="1" colspan="1">
              <italic>Dieb</italic>
            </th>
            <th rowspan="1" colspan="1">
              <italic>Pult</italic>
            </th>
          </tr>
          <tr>
            <th colspan="6" rowspan="1">
              <hr/>
            </th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="center" rowspan="1" colspan="1">N1</td>
            <td align="center" rowspan="1" colspan="1">Speech</td>
            <td align="center" rowspan="1" colspan="1">106 (4)</td>
            <td align="center" rowspan="1" colspan="1">119 (8)</td>
            <td align="center" rowspan="1" colspan="1">120 (5)</td>
            <td align="center" rowspan="1" colspan="1">107 (7)</td>
          </tr>
          <tr>
            <td align="center" rowspan="1" colspan="1">&#xA0;</td>
            <td align="center" rowspan="1" colspan="1">Noise</td>
            <td align="center" rowspan="1" colspan="1">95 (8)</td>
            <td align="center" rowspan="1" colspan="1">111 (7)</td>
            <td align="center" rowspan="1" colspan="1">108 (7)</td>
            <td align="center" rowspan="1" colspan="1">97 (7)</td>
          </tr>
          <tr>
            <td colspan="6" rowspan="1">
              <hr/>
            </td>
          </tr>
          <tr>
            <td align="center" rowspan="1" colspan="1">P2</td>
            <td align="center" rowspan="1" colspan="1">Speech</td>
            <td align="center" rowspan="1" colspan="1">183 (12)</td>
            <td align="center" rowspan="1" colspan="1">181 (8)</td>
            <td align="center" rowspan="1" colspan="1">206 (12)</td>
            <td align="center" rowspan="1" colspan="1">259 (10)</td>
          </tr>
          <tr>
            <td align="center" rowspan="1" colspan="1">&#xA0;</td>
            <td align="center" rowspan="1" colspan="1">Noise</td>
            <td align="center" rowspan="1" colspan="1">177 (15)</td>
            <td align="center" rowspan="1" colspan="1">181 (16)</td>
            <td align="center" rowspan="1" colspan="1">185 (10)</td>
            <td align="center" rowspan="1" colspan="1">160 (12)</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="TFN1">
          <p>Standard deviations are in parentheses.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <table-wrap id="T2" position="float">
      <label>Table 2</label>
      <caption>
        <p>Mean Amplitudes of N1 and P2 in &#xB5;V Elicited by the Speech/Noise Stimuli at Channel Cz</p>
      </caption>
      <table frame="border" rules="cols" width="100%">
        <thead>
          <tr>
            <th rowspan="1" colspan="1">Peak</th>
            <th rowspan="1" colspan="1">Stimulus Type</th>
            <th rowspan="1" colspan="1">
              <italic>Ei</italic>
            </th>
            <th rowspan="1" colspan="1">
              <italic>Bett</italic>
            </th>
            <th rowspan="1" colspan="1">
              <italic>Dieb</italic>
            </th>
            <th rowspan="1" colspan="1">
              <italic>Pult</italic>
            </th>
          </tr>
          <tr>
            <th colspan="6" rowspan="1">
              <hr/>
            </th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="center" rowspan="1" colspan="1">N1</td>
            <td align="center" rowspan="1" colspan="1">Speech<break/></td>
            <td align="center" rowspan="1" colspan="1">-3.82 (1.29)</td>
            <td align="center" rowspan="1" colspan="1">-2.36 (1.04)</td>
            <td align="center" rowspan="1" colspan="1">-4.45 (1.54)</td>
            <td align="center" rowspan="1" colspan="1">-2.36 (1.36)</td>
          </tr>
          <tr>
            <td align="center" rowspan="1" colspan="1">&#xA0;</td>
            <td align="center" rowspan="1" colspan="1">Noise</td>
            <td align="center" rowspan="1" colspan="1">-3.01 (1.58)</td>
            <td align="center" rowspan="1" colspan="1">-2.28 (1.48)</td>
            <td align="center" rowspan="1" colspan="1">-2.58 (1.33)</td>
            <td align="center" rowspan="1" colspan="1">-2.03 (1.04)</td>
          </tr>
          <tr>
            <td colspan="6" rowspan="1">
              <hr/>
            </td>
          </tr>
          <tr>
            <td align="center" rowspan="1" colspan="1">P2</td>
            <td align="center" rowspan="1" colspan="1">Speech</td>
            <td align="center" rowspan="1" colspan="1">4.00 (1.29)</td>
            <td align="center" rowspan="1" colspan="1">2.75 (1.18)</td>
            <td align="center" rowspan="1" colspan="1">4.40 (1.50)</td>
            <td align="center" rowspan="1" colspan="1">1.99 (0.57)</td>
          </tr>
          <tr>
            <td align="center" rowspan="1" colspan="1">&#xA0;</td>
            <td align="center" rowspan="1" colspan="1">Noise</td>
            <td align="center" rowspan="1" colspan="1">3.22 (1.01)</td>
            <td align="center" rowspan="1" colspan="1">2.35 (0.85)</td>
            <td align="center" rowspan="1" colspan="1">4.11 (1.46)</td>
            <td align="center" rowspan="1" colspan="1">2.49 (0.90)</td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="TFN2">
          <p>Standard deviations are in parentheses.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <table-wrap id="T3" position="float">
      <label>Table 3</label>
      <caption>
        <p>ANOVAs F Values of the Different AEP Characteristics</p>
      </caption>
      <table frame="border" rules="all" width="100%">
        <thead>
          <tr>
            <th rowspan="2" colspan="1"/>
            <th colspan="2" rowspan="1">F value, N1</th>
            <th colspan="2" rowspan="1">F value, P2</th>
          </tr>
          <tr>
            <th rowspan="1" colspan="1">Latency</th>
            <th rowspan="1" colspan="1">Amplitude</th>
            <th rowspan="1" colspan="1">Latency</th>
            <th rowspan="1" colspan="1">Amplitude</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="center" rowspan="1" colspan="1">Speech stimuli</td>
            <td align="center" rowspan="1" colspan="1">9.9<xref ref-type="fn" rid="T3FN4">***</xref></td>
            <td align="center" rowspan="1" colspan="1">4.5<xref ref-type="fn" rid="T3FN2">*</xref></td>
            <td align="center" rowspan="1" colspan="1">84.9<xref ref-type="fn" rid="T3FN4">***</xref></td>
            <td align="center" rowspan="1" colspan="1">5.9<xref ref-type="fn" rid="T3FN3">**</xref></td>
          </tr>
          <tr>
            <td align="center" rowspan="1" colspan="1">Noise stimuli</td>
            <td align="center" rowspan="1" colspan="1">8.4<xref ref-type="fn" rid="T3FN3">**</xref></td>
            <td align="center" rowspan="1" colspan="1">0.7<xref ref-type="fn" rid="T3FN1">n.s.</xref></td>
            <td align="center" rowspan="1" colspan="1">4.7<xref ref-type="fn" rid="T3FN3">**</xref></td>
            <td align="center" rowspan="1" colspan="1">3.9<xref ref-type="fn" rid="T3FN2">*</xref></td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="T3FN1">
          <label>n.s</label>
          <p>not significant,</p>
        </fn>
        <fn id="T3FN2">
          <label>*</label>
          <p>p&lt;0.05,</p>
        </fn>
        <fn id="T3FN3">
          <label>**</label>
          <p>p&lt;0.01,</p>
        </fn>
        <fn id="T3FN4">
          <label>***</label>
          <p>p&lt;0.001,</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <table-wrap id="T4" position="float">
      <label>Table 4</label>
      <caption>
        <p>N1-P2 Interpeak Amplitudes (in &#xB5;V) and t Values for the Comparison of Selected Electrode Pairs (T3-T4, FC5-FC6, C3-C4)</p>
      </caption>
      <table frame="border" rules="all" width="100%">
        <thead>
          <tr>
            <th rowspan="1" colspan="1"/>
            <th rowspan="1" colspan="1">T3</th>
            <th rowspan="1" colspan="1">T4</th>
            <th rowspan="1" colspan="1">
              <italic>t</italic>
            </th>
            <th rowspan="1" colspan="1">FC5</th>
            <th rowspan="1" colspan="1">FC6</th>
            <th rowspan="1" colspan="1">
              <italic>t</italic>
            </th>
            <th rowspan="1" colspan="1">C3</th>
            <th rowspan="1" colspan="1">C4</th>
            <th rowspan="1" colspan="1">
              <italic>t</italic>
            </th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="center" rowspan="1" colspan="1">Speech </td>
            <td align="center" rowspan="1" colspan="1">2.29(1.18)</td>
            <td align="center" rowspan="1" colspan="1">3.23(1.15)</td>
            <td align="center" rowspan="1" colspan="1">-5.7<xref ref-type="fn" rid="T4FN5">***</xref></td>
            <td align="center" rowspan="1" colspan="1">4.80(1.13)</td>
            <td align="center" rowspan="1" colspan="1">4.80(1.11)</td>
            <td align="center" rowspan="1" colspan="1">0.0<xref ref-type="fn" rid="T4FN2">n.s.</xref></td>
            <td align="center" rowspan="1" colspan="1">5.19(1.35)</td>
            <td align="center" rowspan="1" colspan="1">4.98(1.26)</td>
            <td align="center" rowspan="1" colspan="1">2.9<xref ref-type="fn" rid="T4FN4">**</xref></td>
          </tr>
          <tr>
            <td align="center" rowspan="1" colspan="1">Noise</td>
            <td align="center" rowspan="1" colspan="1">2.15(1.14)</td>
            <td align="center" rowspan="1" colspan="1">2.24(1.29)</td>
            <td align="center" rowspan="1" colspan="1">-0.5<xref ref-type="fn" rid="T4FN2">n.s.</xref></td>
            <td align="center" rowspan="1" colspan="1">3.89(1.09)</td>
            <td align="center" rowspan="1" colspan="1">3.83(1.20)</td>
            <td align="center" rowspan="1" colspan="1">0.7<xref ref-type="fn" rid="T4FN2">n.s.</xref></td>
            <td align="center" rowspan="1" colspan="1">3.98(1.21)</td>
            <td align="center" rowspan="1" colspan="1">3.95(1.32)</td>
            <td align="center" rowspan="1" colspan="1">0.5<xref ref-type="fn" rid="T4FN2">n.s.</xref></td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="T4FN1">
          <p>Standard deviations are in parentheses.</p>
        </fn>
        <fn id="T4FN2">
          <label>n.s</label>
          <p>not significant,</p>
        </fn>
        <fn id="T4FN3">
          <label>*</label>
          <p>p&lt;0.05,</p>
        </fn>
        <fn id="T4FN4">
          <label>**</label>
          <p>p&lt;0.01,</p>
        </fn>
        <fn id="T4FN5">
          <label>***</label>
          <p>p&lt;0.001.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <table-wrap id="T5" position="float">
      <label>Table 5</label>
      <caption>
        <p>Mean Latencies and Amplitudes of the Negative Component in the Difference Signal out of the Speech-Noise Pairs Averaged over all Channels</p>
      </caption>
      <table frame="border" rules="cols" width="100%">
        <thead>
          <tr>
            <th rowspan="1" colspan="1">Stimulus Type</th>
            <th rowspan="1" colspan="1">
              <italic>Ei</italic>
            </th>
            <th rowspan="1" colspan="1">
              <italic>Bett</italic>
            </th>
            <th rowspan="1" colspan="1">
              <italic>Dieb</italic>
            </th>
            <th rowspan="1" colspan="1">
              <italic>Pult</italic>
            </th>
          </tr>
          <tr>
            <th colspan="5" rowspan="1">
              <hr/>
            </th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="center" rowspan="1" colspan="1">Latency (ms)</td>
            <td align="center" rowspan="1" colspan="1">123 (12)</td>
            <td align="center" rowspan="1" colspan="1">138 (12)</td>
            <td align="center" rowspan="1" colspan="1">142 (11)</td>
            <td align="center" rowspan="1" colspan="1">160 (17)</td>
          </tr>
          <tr>
            <td align="center" rowspan="1" colspan="1">Amplitude (&#xB5;V)</td>
            <td align="center" rowspan="1" colspan="1">-1.36 (1.08)<xref ref-type="fn" rid="T5FN2">***</xref></td>
            <td align="center" rowspan="1" colspan="1">-0.78 (1.12)<xref ref-type="fn" rid="T5FN2">***</xref></td>
            <td align="center" rowspan="1" colspan="1">-2.64 (1.44)<xref ref-type="fn" rid="T5FN2">***</xref></td>
            <td align="center" rowspan="1" colspan="1">-1.81 (1.11)<xref ref-type="fn" rid="T5FN2">***</xref></td>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="T5FN1">
          <p>Standard deviations are in parentheses. Results of one-tailed t tests</p>
        </fn>
        <fn id="T5FN2">
          <label>***</label>
          <p>p&lt;0.001.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
  </floats-group>
</article>
