<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v3.0 20080202//EN" "archivearticle3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
  <?properties open_access?>
  <?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
  <?DTDIdentifier.IdentifierType public?>
  <?SourceDTD.DTDName A++V2.4.dtd?>
  <?SourceDTD.Version 2.4?>
  <?ConverterInfo.XSLTName springer2nlmx2.xsl?>
  <?ConverterInfo.Version 2?>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Cognit Comput</journal-id>
      <journal-title-group>
        <journal-title>Cognitive Computation</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1866-9956</issn>
      <issn pub-type="epub">1866-9964</issn>
      <publisher>
        <publisher-name>Springer-Verlag</publisher-name>
        <publisher-loc>New York</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">3059816</article-id>
      <article-id pub-id-type="pmid">21475687</article-id>
      <article-id pub-id-type="publisher-id">9076</article-id>
      <article-id pub-id-type="doi">10.1007/s12559-010-9076-x</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Modelling Visual Search with the Selective Attention for Identification Model (VS-SAIM): A Novel Explanation for Visual Search Asymmetries</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" corresp="yes">
          <name>
            <surname>Heinke</surname>
            <given-names>Dietmar</given-names>
          </name>
          <address>
            <phone>+44-121-4144920</phone>
            <fax>+44-121-4144897</fax>
            <email>d.g.heinke@bham.ac.uk</email>
          </address>
          <xref ref-type="aff" rid="Aff1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Backhaus</surname>
            <given-names>Andreas</given-names>
          </name>
          <address>
            <phone>+49-391-4090779</phone>
            <email>Andreas.Backhaus@iff.fraunhofer.de</email>
          </address>
          <xref ref-type="aff" rid="Aff2">2</xref>
        </contrib>
        <aff id="Aff1"><label>1</label>School of Psychology, University of Birmingham, Birmingham, B15 2TT UK </aff>
        <aff id="Aff2"><label>2</label>Fraunhofer IFF, Biosystems Engineering, 39106 Magdeburg, Germany </aff>
      </contrib-group>
      <pub-date pub-type="epub">
        <day>26</day>
        <month>10</month>
        <year>2010</year>
      </pub-date>
      <pub-date pub-type="pmc-release">
        <day>26</day>
        <month>10</month>
        <year>2010</year>
      </pub-date>
      <pub-date pub-type="ppub">
        <month>3</month>
        <year>2011</year>
      </pub-date>
      <volume>3</volume>
      <issue>1</issue>
      <fpage>185</fpage>
      <lpage>205</lpage>
      <history>
        <date date-type="received">
          <day>25</day>
          <month>5</month>
          <year>2010</year>
        </date>
        <date date-type="accepted">
          <day>6</day>
          <month>10</month>
          <year>2010</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>&#xA9; The Author(s) 2010</copyright-statement>
      </permissions>
      <abstract id="Abs1">
        <p>In earlier work, we developed the Selective Attention for Identification Model (SAIM [<xref ref-type="bibr" rid="CR16">16</xref>]). SAIM models the human ability to perform translation-invariant object identification in multiple object scenes. SAIM suggests that central for this ability is an interaction between parallel competitive processes in a selection stage and a object identification stage. In this paper, we applied the model to visual search experiments involving simple lines and letters. We presented successful simulation results for asymmetric and symmetric searches and for the influence of background line orientations. Search asymmetry refers to changes in search performance when the roles of target item and non-target item (distractor) are swapped. In line with other models of visual search, the results suggest that a large part of the empirical evidence can be explained by competitive processes in the brain, which are modulated by the similarity between target and distractor. The simulations also suggest that another important factor is the feature properties of distractors. Finally, the simulations indicate that search asymmetries can be the outcome of interactions between top-down (knowledge about search items) and bottom-up (feature of search items) processing. This interaction in VS-SAIM is dominated by a novel mechanism, the knowledge-based on-centre-off-surround receptive field. This receptive field is reminiscent of the classical receptive fields but the exact shape is modulated by both, top-down and bottom-up processes. The paper discusses supporting evidence for the existence of this novel concept.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Visual attention</kwd>
        <kwd>Visual search</kwd>
        <kwd>Computational modelling</kwd>
        <kwd>Search asymmetry</kwd>
      </kwd-group>
      <custom-meta-group>
        <custom-meta>
          <meta-name>issue-copyright-statement</meta-name>
          <meta-value>&#xA9; Springer Science+Business Media, LLC 2011</meta-value>
        </custom-meta>
      </custom-meta-group>
    </article-meta>
  </front>
  <body>
    <sec id="Sec1">
      <title>Introduction</title>
      <p>The visual search task is a commonly used experimental procedure to study human processing of multiple object scenes. In a standard visual search task, participants are asked to determine whether a pre-defined target item among non-targets (distractors) is present or absent. During the course of the experiments the number of distractors (display size) is varied. Typically, the time it takes participants to make this decision (reaction time) is measured as a function of the display size (search function). The slope of the search function is interpreted as indicator for the search efficiency for particular target-distractor pairings. For instance, search for a diagonal line among vertical lines is highly efficient with a slope close to 0ms/item whereas search for a &#x2019;T&#x2019; among &#x2019;L&#x2019;s is inefficient with a slope of around 25&#xA0;ms/item. Over 40&#xA0;years or so, visual search tasks have produced a plethora of experimental evidence (see [<xref ref-type="bibr" rid="CR31">31</xref>, <xref ref-type="bibr" rid="CR41">41</xref>] for reviews). There have been numerous attempts to develop qualitative theories of visual search, e.g. most prominently the Feature Integration Theory (FIT) by Treisman et&#xA0;al. [<xref ref-type="bibr" rid="CR37">37</xref>] or the Attentional Engagement Theory (AET [<xref ref-type="bibr" rid="CR12">12</xref>]). This article presents a connectionist model of visual search. This model is an extension of the Selective Attention for Identification Model (SAIM; [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR20">20</xref>]) adopted to simulate visual search and therefor is termed VS-SAIM.</p>
      <p>SAIM was developed in a connectionist framework and aims to explain human behaviour in terms of the underlying neurophysiological processes in the brain. However, SAIM avoids the full complexity of neurophysiological processes, e.g. the dynamics of different neurotransmitters and employs rate-coded neuron models. On the other hand, this simplification is balanced with SAIM&#x2019;s objective to unify a broad range of behavioural data in one model (see [<xref ref-type="bibr" rid="CR17">17</xref>]; for extensive discussions on the relationship between models of the neural substrate and modelling behavioural data). SAIM&#x2019;s starting point is the human ability to identify objects in multiple object scenes. SAIM suggests that central for this ability is an interaction between parallel competitive processes in a selection stage and a object identification stage. Based on this assumption, SAIM was able to simulate a broad range of experimental evidence usually associated with normal operation of attention and with dysfunctional attention [<xref ref-type="bibr" rid="CR16">16</xref>]. The simulations of normal attention covered two-object costs on selection, global precedence, spatial cueing both within and between objects, and inhibition of return. The effects of disordered attention included view-centred and object-centred visual neglect. In Heinke et&#xA0;al. [<xref ref-type="bibr" rid="CR19">19</xref>], SAIM was successfully applied to simulate a few visual search experiments. These studies showed that the search functions in visual search can be an emerged property of the competitive processes in the brain. The slopes of the search functions were influenced by the similarity between distractors and target. However, when we attempted to simulate a broader range of visual search experiments, it became clear that this initial version of VS-SAIM was not able to mimic this additional data. Consequently, we modified some operations within VS-SAIM. Especially, we replaced the original similarity measure, the scalar product, with the Euclidian distance. The present article reports on a first set of results of this extension.</p>
      <p>For the first set of results we chose experimental evidence that, on the face of it, is particularly challenging to VS-SAIM&#x2019;s similarity-based approach, the search asymmetry (see [<xref ref-type="bibr" rid="CR43">43</xref>]; for a review). In search asymmetries search slopes differ when the roles of target item and distractor item are swapped. For instance, it is easier to find a tilted line among vertical lines then vice versa [<xref ref-type="bibr" rid="CR37">37</xref>]; a diagonal line among vertical lines than the reverse [<xref ref-type="bibr" rid="CR3">3</xref>]. Other examples are: orange item (easier) versus red item [<xref ref-type="bibr" rid="CR36">36</xref>], moving item (easier) versus static item [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR34">34</xref>]. For a similarity-based approach these data are particular challenging, as the target-distractor similarity simply does not change when target and distractor are swaped around. A theoretical account needs to introduce an additional factor to explain these findings.</p>
      <p>On a wider note, there is no satisfactory theoretical account for the occurrence of search asymmetry at present. Initially, Treisman and Gormican [<xref ref-type="bibr" rid="CR37">37</xref>] suggested that search asymmetries are indicative for the existence of feature maps assuming that detection of the presence of a feature is better than the detection of its absence [<xref ref-type="bibr" rid="CR37">37</xref>]. However, subsequent evidence has not supported their theory. For instance, their assumption does not fit with the findings on diagonal line versus vertical line [<xref ref-type="bibr" rid="CR3">3</xref>], as there are well-known feature maps for diagonal lines in the brain. Moreover, recent evidence showed that search for an &#x201C;inverted elephant&#x201D; among upright elephants is more efficient than the other way around [<xref ref-type="bibr" rid="CR43">43</xref>] pointing towards the involvement of object knowledge in search asymmetries. The current paper aims to develop a first coherent account of search asymmetries. It focuses on the search asymmetries with line orientations.</p>
      <p>The reminder of the paper is organized as follows. After introducing VS-SAIM in detail, we discuss how VS-SAIM relates to other important models and theories of visual search. Then we illustrate how the search process in VS-SAIM plays out in detail (Study 1). Study 2 demonstrates that VS-SAIM mimic the experimental findings of asymmetries of line orientation for both diagonal versus vertical line and titled versus vertical line. We also present detailed explanations for this success. The explanation also suggests that VS-SAIM&#x2019;s search efficiency depends not only on target-distractor similarity but also on the orientations of the distractors. Study 3 confirms this point through simulating findings by Foster and Westland [<xref ref-type="bibr" rid="CR14">14</xref>]. To complete the picture, Study 4 shows that VS-SAIM can also simulate a visual search task with symmetric results [<xref ref-type="bibr" rid="CR13">13</xref>]. The general discussion discusses the theoretical implications and present supporting evidence for VS-SAIM&#x2019;s explanation of search asymmetries.</p>
    </sec>
    <sec id="Sec2">
      <title>VS-SAIM</title>
      <sec id="Sec3">
        <title>Overview</title>
        <p>This short description gives an overview of the interactions between VS-SAIM&#x2019;s modules. Afterwards the model will be explained in more details. The mathematics behind VS-SAIM are documented in the "<xref rid="Sec26" ref-type="sec">Appendix</xref>". Figure&#xA0;<xref rid="Fig1" ref-type="fig">1</xref> illustrates VS-SAIM&#x2019;s architecture. Overall VS-SAIM implements a translation-invariant object identification in multiple object scenes. VS-SAIM&#x2019;s first stage, the early visual processing stage (EVPS), extracts simple features, e.g. orientations, from the visual field. In the bottom-up path, the contents network maps a spatial selection of these feature through to the &#x2019;Focus of Attention&#x2019; (FOA). This mapping is translation-invariant, meaning that the contents of any location in the input image can be mapped through to the FOA. The mapping is controlled by the selection network. The selection network, on one hand, chooses the location from which the contents network takes its input and, on the other hand, ensures that the mapping does not distort the original input. These functions are implemented through competitive and co-operative interactions between units in the selection network. VS-SAIM also contains object knowledge stored in the knowledge network with template units. The knowledge network identifies the content of the FOA by matching it with the templates. The template matching utilizes a Euclidian distance as similarity measure. In addition, the knowledge network biases the selection process in VS-SAIM towards &#x2019;known/relevant&#x2019; over &#x2019;unknown/irrelevant&#x2019; objects. This is done via the top-down pathway from the knowledge network to the selection network mediated via the matching network. The role of the matching network is to compare the template information (matching template) with the output from the EVPS. Consistent with the bottom-up path, this comparison utilizes a Euclidian distance and is translation-invariant.<fig id="Fig1"><label>Fig.&#xA0;1</label><caption><p>Architecture of VS-SAIM (see text for details)</p></caption><graphic xlink:href="12559_2010_9076_Fig1_HTML" id="MO43"/></fig></p>
        <p>It is important to note that, as in the previous versions of SAIM, VS-SAIM was designed with the help of the principle of minimization of energy function. This idea was first introduced into connectionism by Hopfield and Tank [<xref ref-type="bibr" rid="CR21">21</xref>] and implements a soft-constraint satisfaction. The design principle follows the following steps: First the problem is formulated as constraint satisfaction problem which defines the constraints a solution has to fulfil. These solutions are translated into activation patterns in a connectionist network. Then an energy function is designed in which these activation patterns are minimal energy values. Finally, to find these energy minima starting from pre-defined activation pattern, a gradient decent procedure is applied to the energy function. The gradient decent procedure results in nonlinear differential equations which, in turn, define a biologically plausible network topology, including the weights between connections. The advantage of this approach is that the energy minima defines a stable state or attractor state for the nonlinear differential equations. This property makes this approach appealing to the design of connectionist models. However, while designing the model in such a way, we found that some of the terms in the equations did not lead to a successful object selection and identification. Subsequently, we relaxed the minimization approach. The details of this relaxation are discussed in the "<xref rid="Sec26" ref-type="sec">Appendix</xref>". Nevertheless, the topology of the model is still directly motivated by the energy minimization approach.</p>
      </sec>
      <sec id="Sec4">
        <title>Early Visual Processing Stage (EVPS)</title>
        <p>VS-SAIM&#x2019;s early visual processing stage consists of Gabor-filters tuned to four orientations, 0&#xB0;,&#xA0;90&#xB0;,&#xA0;45&#xB0; and 135&#xB0;. Gabor-filters have been widely used to model receptive fields of orientation-selective simple cell in the primary visual cortex V1 (e.g. [<xref ref-type="bibr" rid="CR9">9</xref>]). Details about the implementation of the filters and the parameters can be found in the "<xref rid="Sec26" ref-type="sec">Appendix</xref>".</p>
        <p>The output of the EVPS consists of five feature maps, intensity feature map and four orientation feature maps. Figure&#xA0;<xref rid="Fig2" ref-type="fig">2</xref> shows an example of the feature maps for a search display with a &#x2019;L&#x2019; among &#x2019;T&#x2019;s. In order to take into account random noise in the brain, a quasi-stochastic behaviour is added to each feature map (see "<xref rid="Sec26" ref-type="sec">Appendix</xref>" for details). Finally, before the feature maps are fed into the remainder of the model, the activation of the maps is weighted. Horizontal and vertical orientations are weighted higher than diagonal features, as suggested by physiological evidence (e.g. [<xref ref-type="bibr" rid="CR7">7</xref>]). As we will show later, these different weightings are important for simulating the line search asymmetries.<fig id="Fig2"><label>Fig.&#xA0;2</label><caption><p>Feature maps in the early visual processing stage (EVPS). The display on the <italic>left</italic> shows an example of an input image. The four images on the <italic>right</italic> show the resulting feature maps</p></caption><graphic xlink:href="12559_2010_9076_Fig2_HTML" id="MO44"/></fig></p>
      </sec>
      <sec id="Sec5">
        <title>Contents Network</title>
        <p>The contents network aims to enable a translation-invariant mapping from the output of the EVPS to the smaller focus of attention (FOA). The core mechanism of this mapping is a gating mechanism implemented with sigma-pi units [<xref ref-type="bibr" rid="CR26">26</xref>] which are controlled by layers in the selection network (see Fig.&#xA0;<xref rid="Fig3" ref-type="fig">3</xref> for an illustration and "<xref rid="Sec26" ref-type="sec">Appendix</xref>" for details on the operations of the contents network). Sigma-pi units produce an output activation by combining input activations with two operations, multiplications and additions. In Fig.&#xA0;<xref rid="Fig3" ref-type="fig">3</xref>, these two operations are illustrated separately. The circles illustrate the multiplications and the squares depict the additions. The multiplication operation combines the output activation from a layer in the selection network with the output of the feature maps at spatially corresponding locations. The addition produces the output activation of a sigma-pi unit by summing up the result of the multiplication. The output of each sigma-pi unit represents a pixel in the FOA. Hence, each layer in the selection network controls the activation of one pixel in the FOA. Figure&#xA0;<xref rid="Fig3" ref-type="fig">3</xref> illustrates this gating mechanism for three locations in the visual field and three pixels in the FOA. For instance, in the central layer the unit corresponding to a pixel of the vertical T-stroke is switched on (filled circle). This activation gates this pixel through to the FOA, as indicated by the open circle in the contents network.<fig id="Fig3"><label>Fig.&#xA0;3</label><caption><p>Mapping in the contents network. The mapping is illustrated for three input locations and three FOA-locations. The contents network consists of sigma-pi units which combine two operations: multiplication (<italic>filled circles</italic>) and addition (<italic>squares</italic>). The multiplications combine the output of the selection network with the output of the feature maps at spatially corresponding locations. The addition produces the output activation of the sigma-pi units by summing up the results of the multiplications. The output of each sigma-pi unit represents a pixel in the FOA. The <italic>filled circles</italic> in the selection network indicate activated units. These activated units map the content of the corresponding location of the feature maps to the FOA via the sigma-pi units (see text for more details)</p></caption><graphic xlink:href="12559_2010_9076_Fig3_HTML" id="MO45"/></fig></p>
        <p>It is important to note that the content network can implement an arbitrary mapping which depends on the activation pattern in the selection network. For instance, if the unit in the centre of each layer in selection network had a high activation and all other units in the selection network were set to zero, the content of the centre of the input image would be represented in all FOA pixels. Hence, translation-invariant mapping is a special case that is achieved, if two constraints on the activation pattern in the selection network are fulfilled: First, only one unit in the each layer should be activated. With this restriction only the content of one image location is routed into the FOA, because the multiplication allows only one location to be passed into the FOA. Second, only units across the selection network that map neighbouring locations in input image onto neighbouring locations in FOA are allowed to be active. The constraint ensures that the FOA forms veridical representation of the selected object in the input image and is implemented through a &#x201C;diagonal&#x201D; activation pattern in the selection network. The necessity of &#x201C;diagonality&#x201D; arises from the following rational: If one unit in one layer is activated, the layer that controls the adjacent FOA-pixel has to activated the unit adjacent to the first unit. In this way, two locations adjacent in the input image are mapped into adjacent pixels in the FOA. The connections in the selection network implement the corresponding constraint satisfaction process.</p>
      </sec>
      <sec id="Sec6">
        <title>Selection Network</title>
        <p>The selection network aims to select a stimulus in the input image by producing an appropriate activation pattern. Since the selection network controls the mapping in the contents network this activation pattern has to ensure a veridical representation of this stimulus in the FOA. The selection network is structured in layers whereby every layer controls the routing for one of the FOA pixels in the contents network (see Fig.&#xA0;<xref rid="Fig4" ref-type="fig">4</xref> for an illustration). To ensure a veridical stimulus representation in the FOA, the activation pattern in these layer has to fulfil two constraints (see also section on contents network): (1) Only one unit in each layer is allowed to become active. (2) Units in layers controlling adjacent FOA-units has to become activate only if they are adjacent with respect to image locations. This constraint implements the neighbourhood preserving mapping in the content network. The "<xref rid="Sec26" ref-type="sec">Appendix</xref>" documents the mathematical implementation of these constraints within the framework of the energy minimization approach. The resulting connections are illustrated in Fig.&#xA0;<xref rid="Fig4" ref-type="fig">4</xref>. Each layer has an overall inhibitory connections between units implemented the first constraint (competitive process). Units between layers are connected via excitatory connections along the diagonals implementing the second constraint (co-operative process). In addition, the gradient procedure applied to the overall energy function introduces the input from the matching network. This input results from terms in the energy function which ensure that VS-SAIM&#x2019;s behaviour including the behaviour of the selection network is constraint by the input image and the template-knowledge implemented in the knowledge network.<fig id="Fig4"><label>Fig.&#xA0;4</label><caption><p>The structure of the selection network. The selection network consists of several layers. There are inhibitory connections within each layer and excitatory connections between the layers. As explained in the text, connections ensure that the selection network forms a veridical representation of objects in the FOA. Note that for illustration purposes adjacent pixels are depicted further apart</p></caption><graphic xlink:href="12559_2010_9076_Fig4_HTML" id="MO46"/></fig></p>
      </sec>
      <sec id="Sec7">
        <title>Knowledge Network</title>
        <p>The knowledge network implements the object identification in VS-SAIM. A unit in the knowledge network represents an object by being associated with a template of this object. The template is a copy of the object, as it would appear in the FOA. In order to determine which object is represented in the FOA, the template units compare their template with the FOA activation in a matching process. The similarity measure in this template matching is based on the Euclidian distance commonly used in connectionist networks. In order to determine which of the template units represents the best matching template, the units interact in a competitive process similar to the one implemented in the selection network (see "<xref rid="Sec26" ref-type="sec">Appendix</xref>" for mathematical details). The output activation of the template units represent the output of VS-SAIM. A high output activation indicates that VS-SAIM has successfully identified the content of the FOA.</p>
        <p>In VS-SAIM the knowledge network introduces not only a identification stage as an output stage, but also adds a general knowledge-based constraint on VS-SAIM&#x2019;s behaviour. In order to fully integrate this additional constraint the knowledge network also influences the behaviour of the selection network via the matching network. This top-down pathway is a direct outcome of the energy minimization procedure employed in VS-SAIM (see "<xref rid="Sec26" ref-type="sec">Appendix</xref>" for details). In general, this knowledge biases the VS-SAIM&#x2019;s behaviour towards selecting locations in the input image that matches best the templates. Moreover, if the initial activation in the knowledge network is biased towards one template unit, VS-SAIM&#x2019;s overall behaviour is biased towards selecting the item associated with this template. In this paper, we use this property to implement the fact that the visual search experiment requires the search for a set target. Hence, we will bias VS-SAIM towards the selection of the target item. If the target is not present, VS-SAIM is expected to overcome the initial bias and select a distractor item.</p>
      </sec>
      <sec id="Sec8">
        <title>Matching Network</title>
        <p>The function of the matching network is to mediate the feedback from the knowledge network to the selection network. It implements this function in two stages (see Fig.&#xA0;<xref rid="Fig5" ref-type="fig">5</xref>). First a weighted representation of the templates is formed, termed <italic>matching template</italic>. The weighting is determined by the output activation of the template units. As a consequence of the energy minimization approach (see "<xref rid="Sec26" ref-type="sec">Appendix</xref>" for details), the weighted representation slowly builds up starting from an unbiased overlay of all templates. As a simulation progresses the template reflects more and more a biased overlay between the templates determined by the output activation of the template units. For instance, if the L template units has an output activation of 0.4 and the T template unit an output of 0.6, the resulting matching template is made up of 40% of an L and 60% of an T.<fig id="Fig5"><label>Fig.&#xA0;5</label><caption><p>Structure of the matching network. The matching network implements the top-down modulation of the selection network from the knowledge network. The top-down modulation is implemented in two stages: First the matching template is formed (<italic>top right</italic>). Second the matching template is compared with the feature maps (&#x201D;minus circles&#x201D;). This comparison is translation-invariant. This is illustrated by depicting copies of matching templates in the block of layers on the right. Also note that the output of the matching process, termed matching surface, plays an important role in VS-SAIM&#x2019;s capability to simulate the experimental findings</p></caption><graphic xlink:href="12559_2010_9076_Fig5_HTML" id="MO47"/></fig></p>
        <p>In the second stage, the matching template is compared with the feature maps from the EVPS and the result of this comparison feeds into the selection network. Again, like in the knowledge network the matching is based on the Euclidian distance. The usage of this distance is a direct outcome of the energy function minimization approach. It reflects the necessity that the matching in the bottom-up pathway needs to be consistent with the matching in the top-down pathway to ensure an overall consistent behaviour. Note that the matching network also mirrors the translation-invariant mapping of the bottom-up pathway by implementing the comparison between matching template and feature maps in a location-by-location fashion. Figure&#xA0;<xref rid="Fig5" ref-type="fig">5</xref> illustrates this implementation for &#x2019;L&#x2019; and &#x2019;T&#x2019; as templates and an &#x2019;L&#x2019; and &#x2019;T&#x2019; in the input image. Figure&#xA0;<xref rid="Fig5" ref-type="fig">5</xref> also shows the result of the matching process. Since the outcome plays an important role in this paper we introduced a special term, the <italic>matching surface</italic>. Bright pixels stand for highly matching locations and dark pixels represent no matching. The matching surface forms the input to the selection network, where the competitive processes activate units at locations with high matching values.</p>
      </sec>
      <sec id="Sec9">
        <title>Discussion</title>
        <p>This section presented details on how VS-SAIM achieves translation-invariant object identification in a multiple object display. Crucial for achieving this objective are three mechanisms: competitive interactions for selection and identification of items; similarity-based matching in the bottom-up and top-down pathway to direct the selection process and identify the selected item; and an interaction between top-down and bottom-up pathways to ensure consistency between both levels. To implement the search for a target in visual search, the initial activation in the knowledge network is biased towards one template unit, biasing VS-SAIM&#x2019;s overall behaviour towards selecting the target.</p>
        <p>It should be noted that VS-SAIM is part of an ongoing project. Some of the mechanisms presented here have already been validated against experimental evidence other than data from visual search. For instance, the layered structure in the selection network, turned out to be crucial for simulating attentional disorder, such as extinction and object-based neglect [<xref ref-type="bibr" rid="CR16">16</xref>]. The excitatory connections in the selection network were useful in simulating proximity-based grouping [<xref ref-type="bibr" rid="CR16">16</xref>]. A first step towards the integration of similarity-based grouping was presented in [<xref ref-type="bibr" rid="CR18">18</xref>]. Also, SAIM proved robust enough to process natural images [<xref ref-type="bibr" rid="CR20">20</xref>]. Compared to the version published in 2003, the main extensions here are a different similarity measure (Euclidian distance instead of scalar product) and the introduction of an early visual processing stage.</p>
        <p>VS-SAIM falls into a class of models that conceptualize visual attention as mapping details of an input image into a new representation. The most prominent representative of this class is the Selective Tuning (ST)-model by Tsotsos et&#xA0;al. [<xref ref-type="bibr" rid="CR38">38</xref>]. Similar to VS-SAIM, the ST-model uses competitive processes controlled by bottom-up and top-down pathways to guide the mapping process. Interestingly, in a recent extension of the ST-model Tsotsos et&#xA0;al. [<xref ref-type="bibr" rid="CR39">39</xref>] stressed the importance of considering interactions between recognition and attention when modelling visual attention. This type of integrative approach is also taken by VS-SAIM and its earlier version, SAIM.</p>
        <p>However, for the remainder of this discussion and in keeping with the theme of this paper we will focus on the most prominent theories and models of visual search in experimental psychology. Similar to VS-SAIM, all these models and theories postulate that an interaction between top-down and bottom-up influences plays a role in human performance in visual search. Moreover, all models suggest that at some stage a &#x201C;featureless&#x201D; encoding of the search display. For instance, in the Guided-search model [<xref ref-type="bibr" rid="CR40">40</xref>] this representation is called &#x201C;saliency map&#x201D; or &#x201C;master map&#x201D;. In MORSEL [<xref ref-type="bibr" rid="CR30">30</xref>] the input to the attentional module represents the contents at locations in search display &#x201C;featureless&#x201D;. In Deco and Zihl&#x2019;s biased-competition model of visual attention [<xref ref-type="bibr" rid="CR10">10</xref>] a location map receives inputs from all feature maps in a retinotopic fashion. In VS-SAIM the selection network and its input, the matching surface, are &#x201C;featureless&#x201D; maps. However, the Guided-search model and MORSEL suggest that this &#x201C;featureless&#x201D; map is static and is no longer modified during the search process. In contrast, Deco and Zihl&#x2019;s model [<xref ref-type="bibr" rid="CR10">10</xref>] and VS-SAIM postulate that the &#x201C;featureless&#x201D; map is dynamic and changes during the selection process. Especially, in VS-SAIM the dynamic &#x201C;featureless&#x201D; map, the matching surface, is an integral part of interactions between the selection process and the identification process.
<xref ref-type="fn" rid="Fn1">1</xref> Intuitively, this seems to be a more biologically plausible approach to modelling processes during visual search tasks. Finally, VS-SAIM also shares with the seminal Attentional Engagement Theory (AET [<xref ref-type="bibr" rid="CR12">12</xref>]) the assumption that similarity-based matching plays a crucial role.</p>
        <p>Another point to note is that VS-SAIM implements visual search in completely parallel manner. This contrasts with earlier versions of SAIM [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR19">19</xref>] and also with most other models of visual search. For instance and most prominently the Guide-Search model postulates an entirely serial search process. Even the models with a competitive approach assume that there is some sort of serial rechecking mechanism (see the Search via Recursive Rejection (SERR)-model, [<xref ref-type="bibr" rid="CR22">22</xref>]; for an example). However, our implementation of VS-SAIM does not imply that visual search is performed entirely in parallel. Instead, the work of VS-SAIM focuses on contributions of competitive processes to visual search which we, nevertheless, consider to play a crucial part in visual search. On the same token, the visual search mechanism proposed in this paper are assumed still to play an important role even when a serial mechanism is added to VS-SAIM in future versions.</p>
      </sec>
    </sec>
    <sec id="Sec10">
      <title>Study 1: Basic Behaviour</title>
      <p>This study does not primarily aim at simulating experimental results but to illustrate the interworking of VS-SAIM, such as time courses of activations in the selection network, knowledge network and matching network (see Figs.&#xA0;<xref rid="Fig6" ref-type="fig">6</xref>, <xref rid="Fig7" ref-type="fig">7</xref>). These time courses are representative of the processes in all simulations in this paper. The simulations used &#x2019;L&#x2019; as target and &#x2019;T&#x2019; as distractor (see [<xref ref-type="bibr" rid="CR12">12</xref>]; for corresponding experimental results). The search display contained 5 items. To encode the target, the &#x2019;L&#x2019; template unit was initialized with higher activation (0.506) than the distractor unit (0.494). Also, it is important to note that the reaction time (RT) of the model is the simulation time it takes for one template unit to pass a set threshold. Passing the threshold is interpreted as the model having recognized an item. Moreover, VS-SAIM does not make any mistakes. Compared to human performance in visual search this assumption is not realistic. However, often error rates are not statically significant in visual search tasks and human performance is typically influenced in terms of reaction times. Therefore the simulations focus on reaction times as dependent variable.<fig id="Fig6"><label>Fig.&#xA0;6</label><caption><p>Time course of activations for a target-present display. The image at the <italic>bottom-right</italic> shows the search display with &#x201D;L&#x201D; as target and &#x201D;T&#x201D; as distractor. Note that the numbers in the visual field display are not presented. They correspond to the number in the legend of the time course plot of the selection network. This plot shows the time course of the activation of units in the central layer at the centre of the items. A detailed discussion of this figure can be found in the text</p></caption><graphic xlink:href="12559_2010_9076_Fig6_HTML" id="MO48"/></fig><fig id="Fig7"><label>Fig.&#xA0;7</label><caption><p>Time course of activations for a target-absent display. The image at the bottom-right shows the search display with &#x201D;T&#x201D; as distractor and &#x201D;L&#x201D; as target if it were present. Note that the numbers in the visual field display are not presented. They correspond to the number in the legend of the time course plot of the selection network. This plot shows the time course of the activation of units in the central layer at the centre of the items. A detailed discussion of this simulation result can be found in the text</p></caption><graphic xlink:href="12559_2010_9076_Fig7_HTML" id="MO49"/></fig></p>
      <p>Figure&#xA0;<xref rid="Fig6" ref-type="fig">6</xref> shows the simulation result with target being present. The simulation was terminated after the knowledge network produced a clear-cut winner (see time plot of the knowledge network). At this point of time, activations in VS-SAIM were dominated by the target item. FOA and matching template show a stronger representation of &#x2019;L&#x2019; than of &#x2019;T&#x2019;. The time plot of the selection network (top left) shows only the time course of the activations in the centre layer at the central locations of the items in the search display. The time plot illustrates that the target item (Item 5) in the visual field won the competition. This successful selection of the target item began in knowledge network where the initial activation of the two template units is biased towards the target item. This bias drove the matching network from a unbiased matching template (both templates are equally weighted) towards a matching template that is biased towards the target template. This, in turn, led to better matching values at the target location in the matching surface. Therefore, the selection network began selecting the target item which resulted in a stronger representation of the target item in the FOA. This improvement reinforced the initial bias in the knowledge network, eventually making the target template unit the winner unit.</p>
      <p>When the target is absent (Fig.&#xA0;<xref rid="Fig7" ref-type="fig">7</xref>), the initial bias towards the target unit is overcome and the distractor template eventually wins the competition. Analogous to the present trial, the identity of this winner item was eventually reflected in all parts of the model. However, VS-SAIM reaches this state later than in the present trial. Hence, the initial bias in the knowledge network contributes to the delay of reaction times compared to the present trials. Moreover, in the absent condition the matching surface does not produce a clear winner early on, as in the present condition. Instead, the noise added in the EVPS generates a small difference between distractor items which, eventually, allows the selection network to randomly chose an item.</p>
      <p>It is interesting to note that the delay in VS-SAIM&#x2019;s reaction time in the target presence condition compared to the absent condition mimics typical experimental findings in visual search tasks [<xref ref-type="bibr" rid="CR42">42</xref>]. However, these simulation results go beyond the focus of the present paper. The strategy with which participants treat absent trials represents a entirely different issue (see [<xref ref-type="bibr" rid="CR5">5</xref>]; for a rare example of modelling absent trials). Further simulations will need to explore whether this treatment of absent trials constitutes a valid approach.</p>
    </sec>
    <sec id="Sec11">
      <title>Study 2: Search Asymmetry</title>
      <p>This paper focuses on two asymmetries found in oriented line searches. First, if a tilted line is searched among vertical lines, search is more efficient than a vertical line among tilted lines [<xref ref-type="bibr" rid="CR37">37</xref>]. Second, if a diagonal line is searched among vertical lines, search is more efficient than a vertical line among diagonal lines [<xref ref-type="bibr" rid="CR3">3</xref>].</p>
      <sec id="Sec12">
        <title>Method</title>
        <sec id="Sec13">
          <title>Stimuli</title>
          <p>The input display were grey-value pixel pattern of value range of [0;1]. All items were 9&#xA0;&#xD7;&#xA0;9 pixels of size and were placed in a fixed 3&#xA0;&#xD7;&#xA0;3 grid within the input display evenly spaced. The total pixel size of the input display was 43&#xA0;&#xD7;&#xA0;43. The search items were a vertical line, 30&#xB0;-line and 45&#xB0;-line (see Fig.&#xA0;<xref rid="Fig8" ref-type="fig">8</xref> for examples). To generate the tilted lines, a vertical line of seven pixels was rotated using the Matlab function <italic>imrotate</italic> with a bi-linear interpolation method.<fig id="Fig8"><label>Fig.&#xA0;8</label><caption><p>Search asymmetries of line orientations. The search functions document the simulation results for <bold>a</bold> tilted line and vertical line, and for <bold>b</bold> diagonal line and vertical line. The results show that search is more efficient when the oriented line (diagonal or title) is the search target compared to a search with the vertical line as target. These effects constitute a search asymmetry and mimics experimental findings with the same search times (see text for details)</p></caption><graphic xlink:href="12559_2010_9076_Fig8_HTML" id="MO50"/></fig></p>
        </sec>
        <sec id="Sec14">
          <title>Procedure</title>
          <p>Displays were generated with set-sizes of 2, 3, 4, 5, 6, 7 and 8 items. Each condition was run 5 times amounting to 70 trails in total. Only templates for the items present in a particular experiment were included in the knowledge network. At the beginning of each simulation run the template unit of the target was biased to a higher activation 0.506 than the distractor unit 0.494.</p>
        </sec>
        <sec id="Sec15">
          <title>Reaction Time</title>
          <p>The reaction time (RT) of the model is the simulation time it takes for one template unit to pass a set threshold 0.7. Passing the threshold was interpreted as the model having recognized an item.</p>
        </sec>
        <sec id="Sec16">
          <title>Data Analysis</title>
          <p>VS-SAIM&#x2019;s reaction times were analysed with an ANOVA as well as a linear regression to obtain search slope and intercept. In the search function plots, the search slope is depicted next to the average reaction time for highest set-size.</p>
        </sec>
      </sec>
      <sec id="Sec17">
        <title>Results</title>
        <p>Figure&#xA0;<xref rid="Fig8" ref-type="fig">8</xref>a, b show the RT functions for both orientation differences, 30&#xB0; and 45&#xB0;. For each orientation difference a separate two-way ANOVA with set-size and target-type as independent variables was carried out. The ANOVA for 30&#xB0;-difference revealed significant main effects of set-size (<italic>F</italic>(6,&#xA0;69)&#xA0;=&#xA0;2631.6,&#xA0;<italic>p</italic>&#xA0;&lt;&#xA0;0.001) and target-type (<italic>F</italic>(1,&#xA0;69)&#xA0;=&#xA0;49939.0,&#xA0;<italic>p</italic>&#xA0;&lt;&#xA0;0.001) The interaction between the two factors was also significant (<italic>F</italic>(6,&#xA0;69)&#xA0;&#xA0;=&#xA0;&#xA0;812.45,&#xA0;<italic>p</italic>&#xA0;&lt;&#xA0;0.001). Figure&#xA0;<xref rid="Fig8" ref-type="fig">8</xref>a shows that overall reaction times increased with increasing set-size and that search for a vertical target was slower compared to search for the tilted target. The significant interaction resulted from a higher search efficiency when the tilted line was the target compared to when the vertical line was the target. This finding is also confirmed by the different slopes shown in Fig.&#xA0;<xref rid="Fig8" ref-type="fig">8</xref>a.</p>
        <p>The results for the 45&#xB0;-orientation difference were similar. The main effects of set-size (<italic>F</italic>(6,&#xA0;279)&#xA0;=&#xA0;90355.0,&#xA0;<italic>p</italic>&#xA0;&lt;&#xA0;0.001) and target-type (<italic>F</italic>(1,&#xA0;69)&#xA0;=&#xA0;16700.0,&#xA0;<italic>p</italic>&#xA0;&lt;&#xA0;0.001) were both significant. Also, the interaction between the two factors was significant (<italic>F</italic>(6,&#xA0;69)&#xA0;=&#xA0;4755.4,&#xA0;<italic>p</italic>&#xA0;&lt;&#xA0;0.001). Figure&#xA0;<xref rid="Fig8" ref-type="fig">8</xref>b shows that overall reaction times increased with increasing set-size and that search for a vertical target was slower compared to search for the diagonal target. The significant interaction resulted from a higher search efficiency when the diagonal line was the target compared to when the vertical line was the target. This finding is also confirmed by the different slopes shown in Fig.&#xA0;<xref rid="Fig8" ref-type="fig">8</xref>b.</p>
      </sec>
      <sec id="Sec18">
        <title>Discussion</title>
        <p>The simulation results show that VS-SAIM is able to qualitatively reproduce the central result of asymmetric visual search tasks, that of an altered search efficiency when target and distractor roles are swapped. A vertical line target among tilted lines is searched less efficient compared to a tilted line among vertical lines. There are three interesting aspects of these results. First the results demonstrate that the competition processes can produce set-size effects. Second the set-size effect is modulated by target-distractor similarity. Third target-distractor similarity is not the only factor influencing search efficient as otherwise search asymmetry would not have been possible. The first two results were expected and are briefly discussed here. The third finding needs more explanation and will be discussed in the best part of this discussion.</p>
        <p>As discussed earlier, the fact that competition process can produce set-size effects has been by our earlier work [<xref ref-type="bibr" rid="CR19">19</xref>] by other such as a biased-competition model of visual search [<xref ref-type="bibr" rid="CR10">10</xref>] and MORSEL [<xref ref-type="bibr" rid="CR30">30</xref>]. A good way of conceptualizing the reason for this behaviour is that the speed of convergence of the competitive process in the selection network by and large determines the VS-SAIM&#x2019;s reaction times.
<xref ref-type="fn" rid="Fn2">2</xref> Moreover, the speed of convergence is proportional to the contrast between activations in the matching surface. The contrast is the difference between the highest input activation (target position) and all other input activations (distractor locations and background). For instance, the contrast would be highest, if there was only one item in the display. The contrast diminishes the more items are present in the search display leading to the set-size effect. Furthermore, the search slope depends on the target-distractor similarity, because the more similar target and distractor are the more the contrast diminishes with each additional item.</p>
        <p>However, it is not obvious why the search asymmetry is simulated by VS-SAIM as well. In order to explain this result, it is necessary to examine the matching surface closer. Figure&#xA0;<xref rid="Fig9" ref-type="fig">9</xref> shows two illustrations of a matching surface. The input stimuli were a vertical line and a titled line. The matching template for both illustrations was constructed from a equally weighted vertical and titled line. The resulting matching surface has three important characteristics: First, the highest match is obtained at the item&#x2019;s central location. This is to be expected, as the matching template and the item are aligned at this location. Second, the display background has a lower match than the central locations. Thirdly and interestingly, the area in the immediate vicinity of the items exhibits an even lower match than the background. This <italic>'&#x2019;mismatch&#x201D;</italic> surrounding the item plays an important role in VS-SAIM&#x2019;s behaviour and is a direct consequence of the matching process in the matching network. Figure&#xA0;<xref rid="Fig10" ref-type="fig">10</xref> shows a schematic illustration of the matching process. The graphs at the top show a one-dimensional illustration of the matching surface as found in real simulations (e.g. Fig.&#xA0;<xref rid="Fig9" ref-type="fig">9</xref>). The illustration at the bottom of the figure depicts three positions of the matching template (framed &#x2019;L&#x2019;) and relates them to the resulting matching values in the matching surface. Crucially, the mismatch occurs at the second location where there is only a partial overlap between matching template and item. The left part of the matching template is compared to the display background whereas the right part (background in the matching template) is compared to parts of the L. So the left part produces the same matching value compared to when the matching template is entirely located on the background, but the right part generates a lower matching value than on the background leading to an overall matching value lower than the match against the background. Interestingly the shape of the matching surface is reminiscent of recent findings of behavioral performances surrounding the focus of attention (e.g. [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR29">29</xref>, <xref ref-type="bibr" rid="CR32">32</xref>]) and on-centre-off-surround receptive fields in the early visual system (e.g. [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR35">35</xref>]). However, in contrast to the mismatch occurring in the matching network the on-centre-off-surround effect in these early visual areas is assumed to be unaffected by knowledge-based influences. We will return to this interesting aspect in the general discussion of this paper.<fig id="Fig9"><label>Fig.&#xA0;9</label><caption><p>Examples of two matching surfaces. The matching template consisted of two equally weighted template items (<italic>top row</italic>). This matching template was matched with the single vertical line and tilted line. The centre of the matching surfaces has the highest matching value consistent with the location of the respective items. Importantly the area surrounding the location of the best match shows a strong mismatch. This mismatch area plays an important role VS-SAIM&#x2019;s behaviour (see text for details)</p></caption><graphic xlink:href="12559_2010_9076_Fig9_HTML" id="MO51"/></fig><fig id="Fig10"><label>Fig.&#xA0;10</label><caption><p>Mismatch area in the matching surface. These two graphs explain the origin and the properties of the mismatch surrounding search items. The bottom row illustrate the matching process for three spatial distances from for input items a dim L on the <italic>left side</italic> and a dark L (without frame) on the <italic>right side</italic>. The matching template is depicted as a framed L. The <italic>top row</italic> shows one-dimensional illustrations of the outcome of the matching process (see main text for further explanations)</p></caption><graphic xlink:href="12559_2010_9076_Fig10_HTML" id="MO52"/></fig></p>
        <p>It is also important to note that the amplitude of the mismatch is influenced by the absolute activation in the feature maps, as opposed to the relative activation resulting from the matching between matching template and item. This is illustrated in Fig.&#xA0;<xref rid="Fig10" ref-type="fig">10</xref>. For simplicity this effect is depicted for the intensity feature map. However, it should be noted that each feature map leads to the same effect. In Fig.&#xA0;<xref rid="Fig10" ref-type="fig">10</xref> b) the input item is brighter than in Fig.&#xA0;<xref rid="Fig10" ref-type="fig">10</xref> a). Hence, when matching template and input item partially overlap, the mismatch is larger when the input item is brighter than when the input item is dimmer since the match is mainly performed against the background in the matching template. In VS-SAIM, this matching is implemented with the Euclidian distance. Returning to the simulation results, it is important to note that this Euclidian distance for the vertical line is larger than for the diagonal and tilted line. This results from the fact that the diagonal feature map is less weighted than the feature map for vertical orientations. In turn this difference leads to a smaller mismatch for the diagonal line compared to the vertical line. Moreover this leads to a smaller decrease in contrast in the matching surface for the vertical line as distractor compared to the diagonal line as distractor. Therefore the property of the mismatch surround a search item explains the search asymmetry found in the simulations.</p>
      </sec>
    </sec>
    <sec id="Sec19">
      <title>Study 3: Background Orientation</title>
      <p>The explanation of the previous simulations highlighted that VS-SAIM&#x2019;s search efficiency depends not only on target-distractor similarity but also on the orientations of the distractors. Interestingly there is experimental evidence supporting this assumption. Foster and Westland [<xref ref-type="bibr" rid="CR14">14</xref>] reported that search performance was also modulated by the absolute distractor orientation (&#x2019;background orientation&#x2019;). Search performance peaked at horizontal and vertical distractor orientations and fell towards oblique orientations, while the relative orientation between target and distractor was kept constant. In contrast an increase in the relative orientation only improved the overall performance (see Fig.&#xA0;<xref rid="Fig11" ref-type="fig">11</xref> for an illustration). This study tests whether VS-SAIM is able to simulate this specific modulation of search performance through the background orientation.<fig id="Fig11"><label>Fig.&#xA0;11</label><caption><p>Illustration of visual search results from Foster and Westland<xref ref-type="bibr" rid="CR14">14</xref>].Foster &amp; Westland [<xref ref-type="bibr" rid="CR14">14</xref>] varied line orientations of targets and distractors. They manipulated orientation difference between target and distractors together with the orientation of the distractors (background orientation). They found that both factors affect search performance</p></caption><graphic xlink:href="12559_2010_9076_Fig11_HTML" id="MO53"/></fig></p>
      <sec id="Sec20">
        <title>Method</title>
        <p>In this simulation, the distractor item was one out of 0 to 180&#xB0; rotated counter clockwise from the vertical rotated lines, with a step-size of 30&#xB0;. The target object was either a 30&#xB0; or 45&#xB0; counter clockwise from the vertical rotated line with respect to the background orientation. All distractors in a display had the same orientation. The rotated lines were created with Matlab routine <italic>imrotate</italic> and a bi-linear interpolation. Display size was five.</p>
      </sec>
      <sec id="Sec21">
        <title>Results and Discussion</title>
        <p>Figure&#xA0;<xref rid="Fig12" ref-type="fig">12</xref> shows the mean reaction times across all background orientations for both relative orientation conditions successfully mimicking the findings by Foster and Westland [<xref ref-type="bibr" rid="CR14">14</xref>]. Hence, VS-SAIM can generalize to additional orientations compared to Study 2. They also suggest that the way the mismatch area surrounding the distractors changes (Euclidian distance of the distractor features from the background) represents a good approximation of factors influencing visual search performance.<fig id="Fig12"><label>Fig.&#xA0;12</label><caption><p>General orientation search for 5 items. There were five distractor orientation (&#x2019;background orientation&#x2019;) and two orientation differences between target and distractor (30&#xB0; and 45&#xB0;). The <italic>top row</italic> shows examples of search displays for the 30&#xB0;-differences. These results mimic experimental findings ([<xref ref-type="bibr" rid="CR14">14</xref>]; see also Fig.&#xA0;<xref rid="Fig11" ref-type="fig">11</xref>)</p></caption><graphic xlink:href="12559_2010_9076_Fig12_HTML" id="MO54"/></fig></p>
      </sec>
    </sec>
    <sec id="Sec22">
      <title>Study 4: Symmetric Search</title>
      <p>So far the simulations concentrated on mimicking asymmetric search patterns. Indeed, the simulations seem to imply that the asymmetric search pattern is the standard finding and there should be no symmetric search pattern. However, there is some empirical evidence for symmetric search as well. For instance, Egeth and Dagenach [<xref ref-type="bibr" rid="CR13">13</xref>] showed that in a search with &#x2019;L&#x2019; and &#x2019;T&#x2019; items, the swap of target and distractor has no significant effect on the participants&#x2019; search performance. These simulations tests whether VS-SAIM can also simulate these symmetric experimental results.</p>
      <sec id="Sec23">
        <title>Method</title>
        <p>The same method as in Study 2 was used. The only difference were the search items. In this simulation &#x2019;L&#x2019; and &#x2019;T&#x2019; were used (see Fig.&#xA0;<xref rid="Fig13" ref-type="fig">13</xref>).<fig id="Fig13"><label>Fig.&#xA0;13</label><caption><p>Search symmetry. The search function show the simulation results for &#x201D;T&#x201D; and &#x201D;L&#x201D; as search items. In line with experimental findings VS-SAIM shows that search efficiency does not depend on whether the &#x201D;L&#x201D; or the &#x201D;T&#x201D; is the target item. These results are important as they highlight that not all simulations exhibit asymmetries. The reasons for this result and the theoretical implications are discussed in the text</p></caption><graphic xlink:href="12559_2010_9076_Fig13_HTML" id="MO55"/></fig></p>
      </sec>
      <sec id="Sec24">
        <title>Results and Discussion</title>
        <p>Figure&#xA0;<xref rid="Fig13" ref-type="fig">13</xref> shows the search function produced by the model. A three-way ANOVA revealed a significant main effect of set-size (<italic>F</italic>(6,&#xA0;69)&#xA0;=&#xA0;455.9,&#xA0;<italic>p</italic>&#xA0;&lt;&#xA0;0.001) and no significant main effect in target-type (<italic>F</italic>(1,&#xA0;69)&#xA0;=&#xA0;1.07,&#xA0;<italic>p</italic>&#xA0;=&#xA0;0.31) reflecting the symmetric search behaviour. The interaction between set-size and target-type was not significant (<italic>F</italic>(6,&#xA0;69)&#xA0;=&#xA0;0.84,&#xA0;<italic>p</italic>&#xA0;=&#xA0;0.55). The results show that there is no modulation of search efficiency by swapping the target and distractor roles of the items. The reason for the successful simulation of the symmetric search pattern is that both, L and T, are mainly made up of vertical and horizontal strokes. Only at cross points and end points the diagonal feature map shows some responses (see Fig.&#xA0;<xref rid="Fig2" ref-type="fig">2</xref> for an illustration). Therefore, the mismatch area does not change much when L and T are swaped because the Euclidian distance from the background for both items does not differ. In other words, VS-SAIM suggests that when the item are predominately made of similarly weighted features, e.g. vertical and horizontal strokes, the search results should be symmetrical.</p>
      </sec>
    </sec>
    <sec id="Sec25">
      <title>General Discussion</title>
      <p>The Selective Attention for Identification Model (VS-SAIM) is a model of translation-invariant object recognition in a multiple object scene. In a first step, a early visual processing stage generates feature maps of vertical, horizontal and diagonal orientations. Then translation-invariance is achieved by mapping the content of the feature maps through to an attention window (FOA). Object recognition is implemented by a similarity-based (Euclidian distance) matching between stored templates for objects and activation in the FOA. With the issue of multiple objects, VS-SAIM deals with a mix of competitive and co-operative processes which are controlled by bottom-up and top-down influences. In the present paper, we simulated important findings from visual search experiments. Study 2 utilized search displays consisting of oriented lines (vertical, diagonal and titled lines). Each of these lines were either target or distractor in the simulations. The simulations demonstrated that VS-SAIM was able to mimic the typical increase in reaction times with increasing numbers of items (search slope). This result originates from the competitive processes in the selection network. As discussed in the introduction, this explanation has been put forward by several biologically plausible models, e.g. MORSEL [<xref ref-type="bibr" rid="CR30">30</xref>], a biased-competition model of visual search [<xref ref-type="bibr" rid="CR10">10</xref>] and our own work (e.g. [<xref ref-type="bibr" rid="CR19">19</xref>]). Compared to these earlier works, the main progress is that, despite complex interactions between several competitive layers, VS-SAIM still produces a linear increase in reaction times. Hence, VS-SAIM suggests that, despite the fact that several competitive processes must interact in the brain, it is still possible that linear search function can emerge from these interactions. Furthermore, the slope of the search function is proportional to the similarity between target and distractor, in terms of orientation. For instance, search for the diagonal line among vertical lines is more efficient than search for a titled line among vertical lines. This is not unexpected as similarity-based matching plays a large role in VS-SAIM&#x2019;s behaviour. This outcome also fits to one of the central hypotheses put forward by the Attentional Engagement Theory [<xref ref-type="bibr" rid="CR12">12</xref>].</p>
      <p>Finally and most unexpectedly for a similarity-based approach, the simulation results mimicked the experimental findings of search asymmetries for oriented lines. For instance, search for a diagonal line among vertical lines is more efficient than search for a vertical line among diagonal lines. As explained in detail in the result section, crucial for these results are the contrast in VS-SAIM&#x2019;s matching surface which is modulated by the mismatch surrounding each item. Because the profile of the activation is reminiscent of receptive fields found in early visual processing in the brain and the fact that the profile is generated in the VS-SAIM&#x2019;s top-down path we termed the response profile, <italic>knowledge-based on-centre-off-surround receptive field</italic>. The centre of this new type of receptive field is dominated by the influence of the knowledge and the surround by the featural responses of the input stimuli (see Fig.&#xA0;<xref rid="Fig14" ref-type="fig">14</xref> for an illustration). We will return this concept at the end of this discussion. Finally, the analysis revealed that biologically plausible unbalanced weighting of feature maps (e.g. [<xref ref-type="bibr" rid="CR7">7</xref>]) is crucial for simulating search asymmetries of line orientation, with vertical and horizontal orientation weighted higher than diagonal orientation. By combining behavioural findings with this physiological evidence VS-SAIM&#x2019;s approach is validated further.<fig id="Fig14"><label>Fig.&#xA0;14</label><caption><p>The knowledge-based on-centre-off-surround receptive field. The graph illustrates the activation profile in the matching surface near a search item. The labels above indicate the origin of the activation levels. The profile is term knowledge-based receptive field because on one hand the shape of the profile is reminiscent of classical receptive fields in the early visual system and, on the other hand, the matching surface is the result of the matching between feature maps and top-down modulation</p></caption><graphic xlink:href="12559_2010_9076_Fig14_HTML" id="MO56"/></fig></p>
      <p>The following two studies tested two implications of Study 2. Study 3 showed that VS-SAIM cannot only simulate the influence of distractor orientation on visual search performance in general, but also the specific modulation found by Foster and Westland [<xref ref-type="bibr" rid="CR14">14</xref>]. This success is mainly due to how the Euclidian distance between distractor items and background changes with item orientation. Second, Study 2 seem to imply that search asymmetry is the standard finding. However, there is also evidence for symmetric search pattern [<xref ref-type="bibr" rid="CR13">13</xref>]. With its successful simulation VS-SAIM suggests that symmetry occurs when search items are formed by similarly weighted features, such as &#x2019;L&#x2019; and &#x2019;T&#x2019;. Future research needs to follow up this prediction. For now it is important to notice the simulations presented in this paper underline the validity of VS-SAIM. Moreover, the results go beyond a simple &#x201D;proof of existence&#x201D; and make a specific prediction of what is crucial for these simulation results, the knowledge-based centre-on-surround-off receptive field. Because of its novelty and, to some extend, its counterintuitivity this concept is discussed for the remainder of this paper.</p>
      <p>In general VS-SAIM suggests that search asymmetries can be the outcome of knowledge-based influence. Interestingly, this is consistent with behavioral evidence that can be interpreted as knowledge-based influences, e.g. mirrored letters versus normal letters [<xref ref-type="bibr" rid="CR24">24</xref>] or &#x201D;inverted elephants&#x201D; versus normal elephants [<xref ref-type="bibr" rid="CR43">43</xref>]. On the other hand, search asymmetries are often seen as diagnostic for the existence of feature maps (e.g. [<xref ref-type="bibr" rid="CR37">37</xref>]). However, this apparent contradiction is resolved in VS-SAIM by the fact that the top-down influence is modulated by the featural properties. Moreover, the simulations presented here suggest that this Euclidian-based modulation in VS-SAIM presents a good approximation for searches among lines.</p>
      <p>But in how far are the spatial properties of this top-down modulation, the knowledge-based on-centre-off-surround receptive field, plausible? To begin with, it is intuitive to suggest that top-down influence effects search not only exactly at locations of items but also in the vicinity of items. If this top-down influence consists of some kind of matching processes as assumed in VS-SAIM, this matching should not drop off rapidly, as the system has to be robust against noise, distortion, etc. Now, the matching could either tail off to the level of the background level or go below the background level and then increase again as it is the case in our simulation results. The latter option has the advantages that it improves the contrast against the background and makes it more detectable for following processing stages. Moreover and importantly, apart from these theoretical considerations, there is also empirical support for the on-centre-off-surround shape of the matching surface: the well-known response characteristic of receptive fields in the early visual system (e.g. [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR35">35</xref>]) and recent findings of behavioral performances surrounding the focus of attention (e.g. [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR29">29</xref>, <xref ref-type="bibr" rid="CR32">32</xref>]). The classical findings on on-centre-off-surround receptive fields are usually interpreted as a purely feature-based process located in the retina or the LGN. VS-SAIM generalizes this type of spatial response to a knowledge-based on-centre-off-surround receptive field. The location of such receptive fields in the brain is unclear. It could be that the receptive fields in the early visual field indeed are influenced by knowledge. This has not been tested, but there are indications that responses in early visual processing are influenced by top-down modulation (e.g. see [<xref ref-type="bibr" rid="CR25">25</xref>] for evidence on the effect of spatial attention in V1 in an animal study). An obvious alternative could be regions in which fMRI studies have shown indication of object processing, e.g. lateral occipital cortex (e.g. [<xref ref-type="bibr" rid="CR15">15</xref>]). It is also worth noting that such a generalization from a model of low-level processes to higher-level processes is not uncommon. For instance, models based on the principle component analysis (PCA) have been applied to model the formation of low-level receptive fields (e.g. [<xref ref-type="bibr" rid="CR33">33</xref>]) and human face recognition (see [<xref ref-type="bibr" rid="CR27">27</xref>]; for a recent example). A similar transfer of a mechanism from low-level processes to high-level process is suggested by VS-SAIM for the on-centre-off-surround receptive field.</p>
      <p>The second supporting evidence for VS-SAIM&#x2019;s on-centre-off-surround receptive field comes from behavioural experiments on visual attention. In these experiments, the location of the focus of attention is manipulated by target locations in visual search [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR29">29</xref>], spatial cue [<xref ref-type="bibr" rid="CR8">8</xref>] or identification of letters at a pre-defined location [<xref ref-type="bibr" rid="CR32">32</xref>]. The spatial profile of the focus of attention is determined by measuring the success of detecting a simple probe stimulus [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR4">4</xref>], comparing the identity of the probe letter (same colour) with the target letter [<xref ref-type="bibr" rid="CR8">8</xref>] and identifying the probe stimulus [<xref ref-type="bibr" rid="CR29">29</xref>, <xref ref-type="bibr" rid="CR32">32</xref>]. The experiments show that the probe performance exhibits a similar on-centre-off-surround profile as VS-SAIM. Interestingly, even some details of the response characteristics are consistent with VS-SAIM&#x2019;s profile. The profile is influenced by the saliency of the target whereby the inhibitory zone is deeper when the target is more salient [<xref ref-type="bibr" rid="CR29">29</xref>]. Second, Boehler et&#xA0;al. [<xref ref-type="bibr" rid="CR2">2</xref>] showed that the exact shape of the profile depends on the task performed, i.a. simple target detection vs. detecting a feature on the target. This finding can be interpreted that the profile is affected by top-down processes as in VS-SAIM. However, future research needs to test whether this attentional profile is affected only by the task setting or whether properties of distractors influence the profile, e.g. by applying a probe task to asymmetric and symmetric search tasks. Furthermore, these experimental findings are normally conceptualized as profiling the focus of attention. Hence in VS-SAIM this can be construed as activation profile in the selection network. On the other hand these experiments can also be interpreted as tapping into the control mechanism of attention (see [<xref ref-type="bibr" rid="CR29">29</xref>] for a similar point). This interpretation is consistent with VS-SAIM&#x2019;s prediction that the centre-on-surround-off profile is produced by the matching network. Future experiments need to tease these two hypotheses apart.</p>
      <p>Finally, the simulations with VS-SAIM suggest that search is strongly influenced by bottom-up properties of the distractors, especially highlighted by Study 3. In other words VS-SAIM&#x2019;s simulations suggest that, apart from the target-distractor similarity, the properties of distractors play an important role in the efficiency of visual search. This point is interesting, because it is in contrast to most classical theories on visual search, where the focus is on the properties of the target rather than the distractors. In some sense VS-SAIM&#x2019;s suggestion seems intuitively plausible as there are simply more distractors present in the search display, consequently, exerting stronger influence on human behaviour. Future experiments need to explore this novel suggestion.</p>
    </sec>
  </body>
  <back>
    <ack>
      <p content-type="acknowledgment">This work was supported by grants from the Engineering and Physical Sciences Research Council (EPSRC, UK) to the authors. The authors would like to thank Glyn Humphreys and Gustavo Deco for invaluable discussions.</p>
      <p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.</p>
    </ack>
    <ref-list id="Bib1">
      <title>References</title>
      <ref id="CR1">
        <label>1.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Barlow</surname>
              <given-names>HB</given-names>
            </name>
          </person-group>
          <article-title>Summation and inhibition in the frog&#x2019;s retina</article-title>
          <source>J Physiol</source>
          <year>1953</year>
          <volume>119</volume>
          <fpage>69</fpage>
          <lpage>88</lpage>
          <pub-id pub-id-type="pmid">13035718</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR2">
        <label>2.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Boehler</surname>
              <given-names>CM</given-names>
            </name>
            <name>
              <surname>Tsotsos</surname>
              <given-names>JK</given-names>
            </name>
            <name>
              <surname>Schoenfeld</surname>
              <given-names>MA</given-names>
            </name>
            <name>
              <surname>Heinze</surname>
              <given-names>HJ</given-names>
            </name>
            <name>
              <surname>Hopf</surname>
              <given-names>JM</given-names>
            </name>
          </person-group>
          <article-title>The centre-surround profile of the focus of attention arises from recurrent processing in visual cortex</article-title>
          <source>Cereb Cortex</source>
          <year>2009</year>
          <volume>19</volume>
          <fpage>982</fpage>
          <lpage>991</lpage>
          <pub-id pub-id-type="doi">10.1093/cercor/bhn139</pub-id>
          <pub-id pub-id-type="pmid">18755778</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR3">
        <label>3.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Carrasco</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>McLean</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Katz</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Frieder</surname>
              <given-names>K</given-names>
            </name>
          </person-group>
          <article-title>Feature asymmetries in visual search: effects of display duration, target eccentricity, orientation and spatial frequency</article-title>
          <source>Vision Res</source>
          <year>1998</year>
          <volume>38</volume>
          <issue>3</issue>
          <fpage>347</fpage>
          <lpage>374</lpage>
          <pub-id pub-id-type="doi">10.1016/S0042-6989(97)00152-1</pub-id>
          <pub-id pub-id-type="pmid">9536360</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR4">
        <label>4.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cave</surname>
              <given-names>KR</given-names>
            </name>
            <name>
              <surname>Bichot</surname>
              <given-names>NP</given-names>
            </name>
          </person-group>
          <article-title>Visuo-spatial attention: beyond a spotlight model</article-title>
          <source>Psychon Bull Rev</source>
          <year>1999</year>
          <volume>6</volume>
          <fpage>204</fpage>
          <lpage>223</lpage>
          <pub-id pub-id-type="doi">10.3758/BF03212327</pub-id>
          <pub-id pub-id-type="pmid">12199208</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR5">
        <label>5.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chun</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Wolfe</surname>
              <given-names>J</given-names>
            </name>
          </person-group>
          <article-title>Just say no: how are visual searches terminated when there is no target present?</article-title>
          <source>Cogn Psychol</source>
          <year>1996</year>
          <volume>30</volume>
          <fpage>39</fpage>
          <lpage>78</lpage>
          <pub-id pub-id-type="doi">10.1006/cogp.1996.0002</pub-id>
          <pub-id pub-id-type="pmid">8635311</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR6">
        <label>6.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cook</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>McReynolds</surname>
              <given-names>J</given-names>
            </name>
          </person-group>
          <article-title>Lateral inhibition in the inner retina is important for spatial tuning of ganglion cells</article-title>
          <source>Nat Neurosci</source>
          <year>1998</year>
          <volume>1</volume>
          <fpage>714</fpage>
          <lpage>719</lpage>
          <pub-id pub-id-type="doi">10.1038/3714</pub-id>
          <pub-id pub-id-type="pmid">10196588</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR7">
        <label>7.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Coppola</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>White</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Fitzpatrick</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Purves</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>Unequal representation of cardinal and oblique contours in ferret visual cortex</article-title>
          <source>Neurobiology</source>
          <year>1998</year>
          <volume>95</volume>
          <issue>5</issue>
          <fpage>2621</fpage>
          <lpage>2623</lpage>
        </mixed-citation>
      </ref>
      <ref id="CR8">
        <label>8.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cutzu</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Tsotsos</surname>
              <given-names>JK</given-names>
            </name>
          </person-group>
          <article-title>The selective tuning model of attention: psychophysical evidence for a suppressive annulus around attended item</article-title>
          <source>Vision Res</source>
          <year>2003</year>
          <volume>43</volume>
          <fpage>205</fpage>
          <lpage>219</lpage>
          <pub-id pub-id-type="doi">10.1016/S0042-6989(02)00491-1</pub-id>
          <pub-id pub-id-type="pmid">12536142</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR9">
        <label>9.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Daugman</surname>
              <given-names>J</given-names>
            </name>
          </person-group>
          <article-title>Uncertainty relations for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters</article-title>
          <source>J Opt Soc Am A</source>
          <year>1985</year>
          <volume>2</volume>
          <fpage>1160</fpage>
          <lpage>1169</lpage>
          <pub-id pub-id-type="doi">10.1364/JOSAA.2.001160</pub-id>
          <pub-id pub-id-type="pmid">4020513</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR10">
        <label>10.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Deco</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Zihl</surname>
              <given-names>J</given-names>
            </name>
          </person-group>
          <article-title>Top-down selective visual attention: a neurodynamic approach</article-title>
          <source>Visual Cogn</source>
          <year>2001</year>
          <volume>8</volume>
          <issue>1</issue>
          <fpage>119</fpage>
          <lpage>140</lpage>
        </mixed-citation>
      </ref>
      <ref id="CR11">
        <label>11.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Dick</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Ullman</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Sagi</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>Parallel and serial processes in motion detection</article-title>
          <source>Science</source>
          <year>1987</year>
          <volume>237</volume>
          <fpage>400</fpage>
          <lpage>402</lpage>
          <pub-id pub-id-type="doi">10.1126/science.3603025</pub-id>
          <pub-id pub-id-type="pmid">3603025</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR12">
        <label>12.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Duncan</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Humphreys</surname>
              <given-names>GW</given-names>
            </name>
          </person-group>
          <article-title>Visual search and stimulus similarity</article-title>
          <source>Psychol Rev</source>
          <year>1989</year>
          <volume>96</volume>
          <issue>3</issue>
          <fpage>433</fpage>
          <lpage>458</lpage>
          <pub-id pub-id-type="doi">10.1037/0033-295X.96.3.433</pub-id>
          <pub-id pub-id-type="pmid">2756067</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR13">
        <label>13.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Egeth</surname>
              <given-names>H</given-names>
            </name>
            <name>
              <surname>Dagenbach</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>Parallel versus serial processing in visual search: further evidence from subadditive effects of visual quality</article-title>
          <source>J Exp Psychol Hum Percept Perform</source>
          <year>1991</year>
          <volume>17</volume>
          <issue>2</issue>
          <fpage>551</fpage>
          <lpage>560</lpage>
          <pub-id pub-id-type="doi">10.1037/0096-1523.17.2.551</pub-id>
          <pub-id pub-id-type="pmid">1830092</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR14">
        <label>14.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Foster</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Westland</surname>
              <given-names>S</given-names>
            </name>
          </person-group>
          <article-title>Orientation contrast vs. orientation in line-target detection</article-title>
          <source>Vision Res</source>
          <year>1995</year>
          <volume>35</volume>
          <issue>6</issue>
          <fpage>733</fpage>
          <lpage>738</lpage>
          <pub-id pub-id-type="doi">10.1016/0042-6989(94)00178-O</pub-id>
          <pub-id pub-id-type="pmid">7740765</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR15">
        <label>15.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Grill-Spector</surname>
              <given-names>K</given-names>
            </name>
          </person-group>
          <article-title>The neural basis of object perception</article-title>
          <source>Curr Opin Neurobiol</source>
          <year>2003</year>
          <volume>13</volume>
          <fpage>159</fpage>
          <lpage>166</lpage>
          <pub-id pub-id-type="doi">10.1016/S0959-4388(03)00040-0</pub-id>
          <pub-id pub-id-type="pmid">12744968</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR16">
        <label>16.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Heinke</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Humphreys</surname>
              <given-names>GW</given-names>
            </name>
          </person-group>
          <article-title>Attention, spatial representation and visual neglect: simulating emergent attention and spatial memory in the Selective Attention for Identification Model (SAIM)</article-title>
          <source>Psychol Rev</source>
          <year>2003</year>
          <volume>110</volume>
          <issue>1</issue>
          <fpage>29</fpage>
          <lpage>87</lpage>
          <pub-id pub-id-type="doi">10.1037/0033-295X.110.1.29</pub-id>
          <pub-id pub-id-type="pmid">12529057</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR17">
        <label>17.</label>
        <mixed-citation publication-type="book">
          <person-group person-group-type="editor">
            <name>
              <surname>Heinke</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Mavritsaki</surname>
              <given-names>E</given-names>
            </name>
          </person-group>
          <source>Computational modelling in behavioural neuroscience: closing the gap between neurophysiology and behaviour</source>
          <year>2009</year>
          <publisher-loc>London</publisher-loc>
          <publisher-name>Psychology Press</publisher-name>
        </mixed-citation>
      </ref>
      <ref id="CR18">
        <label>18.</label>
        <mixed-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Heinke</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>YR</given-names>
            </name>
            <name>
              <surname>Humphreys</surname>
              <given-names>GW</given-names>
            </name>
          </person-group>
          <person-group person-group-type="editor">
            <name>
              <surname>Paletta</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Tsotsos</surname>
              <given-names>JK</given-names>
            </name>
            <name>
              <surname>Rome</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>Humphreys</surname>
              <given-names>GW</given-names>
            </name>
          </person-group>
          <article-title>Modeling grouping through interactions between top-down and bottom-up processes: the grouping and selective attention for identification model (G-SAIM)</article-title>
          <source>Attention and performance in computational vision, lecture notes in computer science, vol 3368</source>
          <year>2005</year>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>Springer</publisher-name>
          <fpage>148</fpage>
          <lpage>158</lpage>
        </mixed-citation>
      </ref>
      <ref id="CR19">
        <label>19.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Heinke</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Humphreys</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>Tweed</surname>
              <given-names>C</given-names>
            </name>
          </person-group>
          <article-title>Top-down guidance of visual search: a computational account</article-title>
          <source>Vis Cogn</source>
          <year>2006</year>
          <volume>14</volume>
          <issue>4/5/6/7/8</issue>
          <fpage>985</fpage>
          <lpage>1005</lpage>
          <pub-id pub-id-type="doi">10.1080/13506280500195482</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR20">
        <label>20.</label>
        <mixed-citation publication-type="other">Heinke D, Backhaus A, Sun YR, Humphreys GW. The Selective Attention for Identification model SAIM): simulating visual search in natural colour images. In: Paletta L, Rome E (eds) Attention in cognitive systems, lecture notes in computer science 4840, pp 141&#x2013;54; 2008.</mixed-citation>
      </ref>
      <ref id="CR21">
        <label>21.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hopfield</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Tank</surname>
              <given-names>D</given-names>
            </name>
          </person-group>
          <article-title>&#x2019;Neural&#x2019; computation of decisions in optimization problems</article-title>
          <source>Biol Cybern</source>
          <year>1985</year>
          <volume>52</volume>
          <fpage>141</fpage>
          <lpage>152</lpage>
          <pub-id pub-id-type="pmid">4027280</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR22">
        <label>22.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Humphreys</surname>
              <given-names>G</given-names>
            </name>
            <name>
              <surname>M&#xFC;ller</surname>
              <given-names>HJ</given-names>
            </name>
          </person-group>
          <article-title>SEarch via Recursive Rejection (SERR): a connectionist model of visual search</article-title>
          <source>Cogn Psychol</source>
          <year>1993</year>
          <volume>25</volume>
          <fpage>43</fpage>
          <lpage>110</lpage>
          <pub-id pub-id-type="doi">10.1006/cogp.1993.1002</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR23">
        <label>23.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jones</surname>
              <given-names>HE</given-names>
            </name>
            <name>
              <surname>Andolina</surname>
              <given-names>I</given-names>
            </name>
            <name>
              <surname>Oakely</surname>
              <given-names>N</given-names>
            </name>
            <name>
              <surname>Murphy</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>Sillito</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>Spatial summation in lateral geniculate nucleus and visual cortex</article-title>
          <source>Exp Brain Res</source>
          <year>2000</year>
          <volume>135</volume>
          <fpage>279</fpage>
          <lpage>284</lpage>
          <pub-id pub-id-type="doi">10.1007/s002210000574</pub-id>
          <pub-id pub-id-type="pmid">11131514</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR24">
        <label>24.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Malinowski</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>H&#xE4;ner</surname>
              <given-names>R</given-names>
            </name>
          </person-group>
          <article-title>The effect of familiarity on visual-search performance: evidence for learned basic features</article-title>
          <source>Percept Psychophys</source>
          <year>2001</year>
          <volume>63</volume>
          <fpage>458</fpage>
          <lpage>463</lpage>
          <pub-id pub-id-type="doi">10.3758/BF03194412</pub-id>
          <pub-id pub-id-type="pmid">11414133</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR25">
        <label>25.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>McAdams</surname>
              <given-names>CJ</given-names>
            </name>
            <name>
              <surname>Reid</surname>
              <given-names>RC</given-names>
            </name>
          </person-group>
          <article-title>Attention modulates the responses of simple cells in monkey primary visual cortex</article-title>
          <source>J Neurosci</source>
          <year>2005</year>
          <volume>25</volume>
          <issue>47</issue>
          <fpage>11,023</fpage>
          <lpage>11,033</lpage>
          <pub-id pub-id-type="doi">10.1523/JNEUROSCI.2904-05.2005</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR26">
        <label>26.</label>
        <mixed-citation publication-type="other">McClelland J, Rumelhart D, Hinton G. The appeal of parallel distributed processing. In: Parrallel distributed processing: explorations in the microstructure of cognition, volume I: foundations. MIT Press/Bradford Books, Cambridge; 1986.</mixed-citation>
      </ref>
      <ref id="CR27">
        <label>27.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Meytlis</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Sirovich</surname>
              <given-names>L</given-names>
            </name>
          </person-group>
          <article-title>On the dimensionality of face space</article-title>
          <source>IEEE Trans Pattern Anal Mach Intell</source>
          <year>2007</year>
          <volume>29</volume>
          <issue>7</issue>
          <fpage>1262</fpage>
          <lpage>1267</lpage>
          <pub-id pub-id-type="doi">10.1109/TPAMI.2007.1033</pub-id>
          <pub-id pub-id-type="pmid">17496382</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR28">
        <label>28.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mjolsness</surname>
              <given-names>E</given-names>
            </name>
            <name>
              <surname>Garrett</surname>
              <given-names>C</given-names>
            </name>
          </person-group>
          <article-title>Algebraic transformations of objective functions</article-title>
          <source>Neural Netw</source>
          <year>1990</year>
          <volume>3</volume>
          <fpage>651</fpage>
          <lpage>669</lpage>
          <pub-id pub-id-type="doi">10.1016/0893-6080(90)90055-P</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR29">
        <label>29.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mounts</surname>
              <given-names>JRW</given-names>
            </name>
          </person-group>
          <article-title>Evidence for suppressive mechanisms in attentional selection: Feature singletons produce inhibitory surrounds</article-title>
          <source>Percept Psychophys</source>
          <year>2000</year>
          <volume>62</volume>
          <issue>5</issue>
          <fpage>969</fpage>
          <lpage>983</lpage>
          <pub-id pub-id-type="doi">10.3758/BF03212082</pub-id>
          <pub-id pub-id-type="pmid">10997043</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR30">
        <label>30.</label>
        <mixed-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Mozer</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Sitton</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <person-group person-group-type="editor">
            <name>
              <surname>Pashler</surname>
              <given-names>H</given-names>
            </name>
          </person-group>
          <article-title>Computational modeling of spatial attention</article-title>
          <source>Attention</source>
          <year>1998</year>
          <publisher-loc>London</publisher-loc>
          <publisher-name>London UCL Press</publisher-name>
          <fpage>341</fpage>
          <lpage>393</lpage>
        </mixed-citation>
      </ref>
      <ref id="CR31">
        <label>31.</label>
        <mixed-citation publication-type="book">
          <person-group person-group-type="editor">
            <name>
              <surname>Muller</surname>
              <given-names>H</given-names>
            </name>
            <name>
              <surname>Krummenacher</surname>
              <given-names>J</given-names>
            </name>
          </person-group>
          <source>Visual search and attention, visual cognition</source>
          <year>2006</year>
          <publisher-loc>Essex</publisher-loc>
          <publisher-name>Psychology Press</publisher-name>
        </mixed-citation>
      </ref>
      <ref id="CR32">
        <label>32.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>M&#xFC;ller</surname>
              <given-names>NG</given-names>
            </name>
            <name>
              <surname>Mollenhauer</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>R&#xF6;sler</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Kleinschmidt</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>The attentional field has a Mexican hat distribution</article-title>
          <source>Vision Res</source>
          <year>2005</year>
          <volume>45</volume>
          <fpage>1129</fpage>
          <lpage>1137</lpage>
          <pub-id pub-id-type="doi">10.1016/j.visres.2004.11.003</pub-id>
          <pub-id pub-id-type="pmid">15707921</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR33">
        <label>33.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Olshausen</surname>
              <given-names>BA</given-names>
            </name>
            <name>
              <surname>Field</surname>
              <given-names>DJ</given-names>
            </name>
          </person-group>
          <article-title>Natural image statistics and efficient coding</article-title>
          <source>Netw Comput Neural Syst</source>
          <year>1996</year>
          <volume>7</volume>
          <fpage>333</fpage>
          <lpage>339</lpage>
          <pub-id pub-id-type="doi">10.1088/0954-898X/7/2/014</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR34">
        <label>34.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Royden</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Wolfe</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Klempen</surname>
              <given-names>N</given-names>
            </name>
          </person-group>
          <article-title>Visual search asymmetries in motion and optical flow fields</article-title>
          <source>Percept Psychophys</source>
          <year>2001</year>
          <volume>63</volume>
          <fpage>436</fpage>
          <lpage>444</lpage>
          <pub-id pub-id-type="doi">10.3758/BF03194410</pub-id>
          <pub-id pub-id-type="pmid">11414131</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR35">
        <label>35.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schiller</surname>
              <given-names>P</given-names>
            </name>
          </person-group>
          <article-title>The on and off channels of the visual system</article-title>
          <source>Trends Neurosci</source>
          <year>1992</year>
          <volume>15</volume>
          <fpage>86</fpage>
          <lpage>92</lpage>
          <pub-id pub-id-type="doi">10.1016/0166-2236(92)90017-3</pub-id>
          <pub-id pub-id-type="pmid">1373923</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR36">
        <label>36.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Treisman</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>Features and objects: the 14th bartlett memorial lecture</article-title>
          <source>Q J Exp Psychol</source>
          <year>1988</year>
          <volume>40A</volume>
          <fpage>201</fpage>
          <lpage>237</lpage>
        </mixed-citation>
      </ref>
      <ref id="CR37">
        <label>37.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Treisman</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Gormican</surname>
              <given-names>S</given-names>
            </name>
          </person-group>
          <article-title>Feature analysis in early vision: evidence from search asymmetries</article-title>
          <source>Psychol Rev</source>
          <year>1988</year>
          <volume>95</volume>
          <issue>1</issue>
          <fpage>15</fpage>
          <lpage>48</lpage>
          <pub-id pub-id-type="doi">10.1037/0033-295X.95.1.15</pub-id>
          <pub-id pub-id-type="pmid">3353475</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR38">
        <label>38.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tsotsos</surname>
              <given-names>JK</given-names>
            </name>
            <name>
              <surname>Culhane</surname>
              <given-names>SM</given-names>
            </name>
            <name>
              <surname>Wai</surname>
              <given-names>WYK</given-names>
            </name>
            <name>
              <surname>Lai</surname>
              <given-names>Y</given-names>
            </name>
            <name>
              <surname>Davis</surname>
              <given-names>N</given-names>
            </name>
            <name>
              <surname>Nuflo</surname>
              <given-names>F</given-names>
            </name>
          </person-group>
          <article-title>Modeling visual attention via selective tuning</article-title>
          <source>Artif Intell</source>
          <year>1995</year>
          <volume>78</volume>
          <fpage>507</fpage>
          <lpage>545</lpage>
          <pub-id pub-id-type="doi">10.1016/0004-3702(95)00025-9</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR39">
        <label>39.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tsotsos</surname>
              <given-names>JK</given-names>
            </name>
            <name>
              <surname>Rodriguez-Sanchez</surname>
              <given-names>AJ</given-names>
            </name>
            <name>
              <surname>Rothenstein</surname>
              <given-names>AL</given-names>
            </name>
            <name>
              <surname>Simine</surname>
              <given-names>E</given-names>
            </name>
          </person-group>
          <article-title>The different states of visual recognition need different attentional binding strategies</article-title>
          <source>Brain Res</source>
          <year>2008</year>
          <volume>1225</volume>
          <fpage>119</fpage>
          <lpage>132</lpage>
          <pub-id pub-id-type="doi">10.1016/j.brainres.2008.05.038</pub-id>
          <pub-id pub-id-type="pmid">18585692</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR40">
        <label>40.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wolfe</surname>
              <given-names>JM</given-names>
            </name>
          </person-group>
          <article-title>Guided Search 2.0 A revised model of visual search</article-title>
          <source>Psychomon Bull Rev</source>
          <year>1994</year>
          <volume>1</volume>
          <issue>2</issue>
          <fpage>202</fpage>
          <lpage>238</lpage>
          <pub-id pub-id-type="doi">10.3758/BF03200774</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR41">
        <label>41.</label>
        <mixed-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Wolfe</surname>
              <given-names>JM</given-names>
            </name>
          </person-group>
          <person-group person-group-type="editor">
            <name>
              <surname>Pashler</surname>
              <given-names>H</given-names>
            </name>
          </person-group>
          <article-title>Visual search: a review</article-title>
          <source>Attention</source>
          <year>1998</year>
          <publisher-loc>Essex</publisher-loc>
          <publisher-name>Psychology Press</publisher-name>
          <fpage>13</fpage>
          <lpage>74</lpage>
        </mixed-citation>
      </ref>
      <ref id="CR42">
        <label>42.</label>
        <mixed-citation publication-type="other">Wolfe JM. What can 1 million trials tell us about visual search. Psychol Sci. 1998b;9(1).</mixed-citation>
      </ref>
      <ref id="CR43">
        <label>43.</label>
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wolfe</surname>
              <given-names>JM</given-names>
            </name>
          </person-group>
          <article-title>Asymmetries in visual search: an introduction</article-title>
          <source>Percept Psychophys</source>
          <year>2001</year>
          <volume>63</volume>
          <issue>3</issue>
          <fpage>381</fpage>
          <lpage>389</lpage>
          <pub-id pub-id-type="doi">10.3758/BF03194406</pub-id>
          <pub-id pub-id-type="pmid">11414127</pub-id>
        </mixed-citation>
      </ref>
    </ref-list>
    <app-group>
      <app id="App1">
        <sec id="Sec26">
          <title>Appendix: Mathematical Details of VS-SAIM</title>
          <sec id="Sec27">
            <title>General Modelling Principle</title>
            <p>As explained in the main text, VS-SAIM&#x2019;s design is based on the principle of minimization of an energy function in order to implement a soft-constraint satisfaction. The core idea is that activation patterns fulfilling a set of constraints constitute minima in an energy function. In order to determine the minimum, VS-SAIM performs a gradient descent in the energy function following a suggestion by Hopfield and Tank [<xref ref-type="bibr" rid="CR21">21</xref>]:
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \tau \dot{x}_{k} = -g {\frac{\partial E\left({\bf y}\right)}{\partial y_{k}}}; \quad y_k = f(x_k) $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where &#x3C4; is a time constant, <italic>g</italic> a gain factor, <bold>x</bold> the inner activation of the model and <bold>y</bold> the model output activation. <italic>f</italic>(<italic>x</italic>) is the output function of the individual units. In VS-SAIM each module, knowledge network, contents network, etc. pursuits a different set of constraints depending on its task, e.g. identifying objects in the knowledge network. Hence, for each network an energy function is defined with different minima reflecting a correct completion of its task. In order ensure that VS-SAIM, as a whole, satisfies all constraints at the same time, the individual energy functions are added together to a overall energy function. Apart from the energy function as such, the choice of the output function <italic>f</italic>(<italic>x</italic>) depends on the implemented constraints. Here, we used either the sigmoid function,
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M2">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ f(x) = {\frac{1}{1 + \exp{ \left(-m\left(x - s\right)\right)}}} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>or a linear function
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ f(x) = m \cdot x- s. $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>whereby <italic>m</italic> is the slope and <italic>s</italic> the intercept. The sigmoid function is more suitable if the attractor states have to be either zero or one. In contrast, the linear function is appropriate, if the final state should reflect continuous values, e.g. the activation in the feature maps. Therefore, the sigmoid function was used in the selection network and the knowledge network. The linear function was used in the content network and the matching network. Finally, to achieve some degree of biologically plausible in VS-SAIM, the differential equations can also include a leaky integrator so that the resulting differential equation turns into:
<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M4">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \tau \dot{x}_{k} = -x_{k}- g {\frac{\partial E\left({\bf y}\right)}{\partial y_{k}}} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>Before the equations of the individual networks are introduced, we will present the equations for a simple WTA-network in a separate section. This illustrates the energy function approach in a simple example and also introduces the central building block for VS-SAIM.</p>
          </sec>
          <sec id="Sec28">
            <title>Winner Take All (WTA)</title>
            <p>The WTA energy function was suggested by [<xref ref-type="bibr" rid="CR28">28</xref>]:
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ E^{WTA}({\bf y})=\alpha \left(\sum^{K}_{k=1}y_{k} - 1\right)^2 - \beta \sum^{K}_{k=1} I_{k}y_{k} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>where <italic>y</italic><sub><italic>k</italic></sub> is the output activation of the <italic>kth</italic> neuron and <italic>I</italic><sub><italic>k</italic></sub> its input. Every neuron has an internal activation <italic>x</italic><sub><italic>k</italic></sub> and its output activation <italic>y</italic><sub><italic>k</italic></sub> is calculated via the sigmoid function. The WTA energy function is minimal when all <italic>y</italic><sub><italic>i</italic></sub>s are zero except one (first term), and the corresponding <italic>I</italic><sub><italic>i</italic></sub> has the maximal value of all <italic>I</italic><sub><italic>i</italic></sub>s (second term). The parameter &#x3B1; and &#x3B2; weights the two terms or constraints against each other. The gradient descent together with a leaky integrator for each neuron results in the following equations:
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M6">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \tau \dot{x}_{k} = -x_{k} - {\frac{\partial E^{WTA}\left({\bf y}\right)}{\partial y_{k}}} = -x_{k} - 2\alpha \left(\sum^{K}_{k=1}y_{k} - 1\right) + \beta I_{k} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>Finally, note that the term <inline-formula id="IEq1"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sum^{K}_{k=1}y_{k}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq1.gif"/></alternatives></inline-formula> implements a global inhibition in the WTA- network.</p>
          </sec>
          <sec id="Sec29">
            <title>Early visual processing stage (EVPS)</title>
            <p>The early visual processing stage (EVPS) consists of Gabor-filters with four orientations (0&#xB0;,&#xA0;90&#xB0;,&#xA0;45&#xB0; and 135&#xB0;), modelling V1 operations. The following equation describes the normalization:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M8">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ I^{norm}_{ij} ={\frac{I_{ij}}{\sqrt{\sum^{U}_{u=-U}\sum^{V}_{v=-V}I^{2}_{i+u,j+v}}}} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>with a window size of (2<italic>U</italic>&#xA0;+&#xA0;1)&#xA0;&#xD7;&#xA0;(2<italic>V</italic>&#xA0;+&#xA0;1). The term <italic>I</italic><sub><italic>ij</italic></sub> represents the input image while <italic>I</italic><sup><italic>norm</italic></sup><sub><italic>ij</italic></sub> is the normalized intensity image. The Gabor-filter implementation follows the model of the receptive field of simple cells in the visual cortex proposed by [<xref ref-type="bibr" rid="CR9">9</xref>]:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ x(s,r) = s \cdot\cos{\theta} + r \cdot \sin{\theta} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M10">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ y(s,r) = r \cdot\cos{\theta} - s \cdot \sin{\theta} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ G\left(s,r,\theta,k\right)=\, {\frac{1}{A}} \exp{\left( -0.5 \left( \left({\frac{x(s,r)}{\sigma}}\right)^2 +\left({\frac{y(s,r)}{\sigma}}\right)^2\right)\right)} \cos{ \left(2 \pi k x(s,r) \right)} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M12">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ A = \left\vert\sum_{sr}G\left(s,r,\theta,k\right)\right\vert $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula>with &#x3B8; being the Gabor kernel orientation and <italic>k</italic> its frequency.</p>
            <p>The Gabor-filters are convolved with the normalized intensity image. Consequently, the output of the EVPS consists of a five feature maps: The first feature map, <italic>n</italic>&#xA0;=&#xA0;1, is the normalized intensity image:<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ f^{(1)}_{ij} = p^{(1)}I^{norm}_{ij} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula>while the other four feature maps <italic>n</italic>&#xA0;=&#xA0;2,&#xA0;<italic>n</italic>&#xA0;=&#xA0;3,&#xA0;<italic>n</italic>&#xA0;=&#xA0;4,&#xA0;<italic>n</italic>&#xA0;=&#xA0;5 are the convolution results of the normalized intensity input with a Gabor-filter mask:<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M14">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ f^{(n)}_{ij} =p^{(n)}\left\vert\sum_{s=-S}^{S}\sum_{r=-R}^{R}G\left(s,r,\theta_{n},k_{n}\right)I^{norm}_{i+s,j+r} \right\vert$$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>In the cases <italic>i</italic>&#xA0;+&#xA0;<italic>s</italic> or <italic>j</italic>&#xA0;+&#xA0;<italic>r</italic> exceeds the visual field boundaries, the intensity values are set to zero (boundary handling). The parameter <italic>p</italic><sup>(<italic>n</italic>)</sup> weights the feature maps and were altered in the &#x201C;exploration of the parameter space&#x201D;.</p>
          </sec>
          <sec id="Sec30">
            <title>Noise</title>
            <p>In order to model noise in the visual system, noise was added to the input display and was based on the following equation implemented in an earlier version of VS-SAIM [<xref ref-type="bibr" rid="CR19">19</xref>]:<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \ddot{x} + \gamma \dot{x} + \sin{x} = A \cos\left(\omega t + p\right) $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula></p>
            <p>This equation was inspired by the motion equation of a periodically driven pendulum where &#x3B3; is the damping constant and the right side describes a driving torque with amplitude <italic>A</italic>, angular frequency &#x3C9; and phase <italic>p</italic>.</p>
            <p>This equation was chosen on merely technical grounds and exhibits a chaotic behaviour or quasi-stochastic temporal behaviour. Since this &#x201D;noise&#x201D; is described with a differential equation, it fits seamlessly into the differential equations derived from the energy function approach.</p>
            <p>To ensure that each retinal unit of the input receives a different signal, each retinal unit has its own pendulum equation:<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M16">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \ddot{x}_{ij} + \gamma \dot{x}_{ij} + \sin{ x_{ij} } = A \cos\left(\omega t + p_{ij}\right) $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula>For each retinal unit (<italic>i</italic>,&#xA0;<italic>j</italic>) an initial state <italic>x</italic><sub><italic>ij</italic></sub>(0) and phase <italic>p</italic><sub><italic>ij</italic></sub> is randomly chosen drawn from an equal distribution.</p>
            <p>To limit the amplitude of the noise <italic>x</italic><sub><italic>ij</italic></sub> was fed into the following equation:<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ y^{noise}_{ij}(t)=0.5 \left( \left( max - min\right) \cdot \sin{\left(x_{ij}(t)\right)} + \left(max + min \right) \right) $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula>where <italic>max</italic> and <italic>min</italic> are the limits of the noise amplitude. The term <italic>y</italic><sup><italic>noise</italic></sup><sub><italic>ij</italic></sub> is then added to each feature map <inline-formula id="IEq2"><alternatives><tex-math id="M18">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f^{n}_{ij}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq2.gif"/></alternatives></inline-formula>.</p>
          </sec>
          <sec id="Sec31">
            <title>Contents Network</title>
            <p>The contents network aims to enable a translation-invariant mapping from the EPVS to the smaller focus of attention. The energy function for the contents network is defined as,<disp-formula id="Equ17"><label>17</label><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ E^{CN} \left({\bf y}^{CN},{\bf y}^{SN}\right) = \alpha^{CN} \sum_{lm} \sum_{ij} \sum_{n}\left( y^{CN}_{lmn}-f^{(n)}_{ij} \right)^2 \left( y^{SN}_{lmij}\right)^q $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ17.gif" position="anchor"/></alternatives></disp-formula>where &#x3B1;<sup><italic>CN</italic></sup> is the weight of the energy term in the overall energy function of VS-SAIM. The indices <italic>i</italic> and <italic>j</italic> refer to image locations, and the indices <italic>l</italic> and <italic>m</italic> refer to FOA-locations. <italic>n</italic> indexes the feature map. The variable <inline-formula id="IEq3"><alternatives><tex-math id="M20">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y^{CN}_{lmn}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq3.gif"/></alternatives></inline-formula> is the output activation of the contents network while <inline-formula id="IEq4"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y^{SN}_{lmij}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq4.gif"/></alternatives></inline-formula> is the output activation of the selection network.</p>
            <p>The term <inline-formula id="IEq5"><alternatives><tex-math id="M22">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left( y^{CN}_{lmn}-f^{(n)}_{ij} \right)^2$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq5.gif"/></alternatives></inline-formula> ensures this energy function is minimal when the FOA matches the activations of the features maps. However, because of the multiplication with the output activations from the selection network <inline-formula id="IEq6"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left( y^{SN}_{lmij}\right)^q$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq6.gif"/></alternatives></inline-formula> this match is only required at selected locations (<inline-formula id="IEq7"><alternatives><tex-math id="M24">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y^{SN}_{lmij}=1$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq7.gif"/></alternatives></inline-formula>), whereas deselected locations (<inline-formula id="IEq8"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y^{SN}_{lmij}\approx 0$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq8.gif"/></alternatives></inline-formula>) do not contribute to the minimization of the energy function. The parameter <italic>q</italic>, chosen to a value larger than one, enhances the selection effect of the selection network, since it decreases small activations more than larger activations. This implementation of the contents network was initially design for the Grouping-SAIM (G-SAIM; [<xref ref-type="bibr" rid="CR18">18</xref>]). In G-SAIM we simulated simple effects of with- and between-object grouping. In this version, the contents network was crucial for its success. However, for the aim of this paper it is only relevant the contents network operates similar to a sigma-pi unit, as explained in the main text (see also [<xref ref-type="bibr" rid="CR16">16</xref>]). Here, we will demonstrate this equivalence. The differential equation for the contents network units is described by,<disp-formula id="Equ18"><label>18</label><alternatives><tex-math id="M26">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \tau \dot{x}^{CN}_{lmn} = -x^{CN}_{lmn} - {\frac{\partial E^{CN}\left({\bf y}^{SN},{\bf y}^{CN}\right)}{\partial y^{CN}_{lmn}}} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ18.gif" position="anchor"/></alternatives></disp-formula>with the derivation term defined as,<disp-formula id="Equ19"><label>19</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\frac{\partial E^{CN}\left({\bf y}^{SN},{\bf y}^{CN}\right)} {\partial y^{CN}_{lmn}}} = 2\alpha^{CN}\sum_{ij} \left(y^{CN}_{lmn} - f^{n}_{ij}\right)\left(y^{SN}_{lmij}\right)^q $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ19.gif" position="anchor"/></alternatives></disp-formula></p>
            <p>The contents network uses a linear output function, so that <inline-formula id="IEq9"><alternatives><tex-math id="M28">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y^{CN}_{lmn}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq9.gif"/></alternatives></inline-formula> is equal with <inline-formula id="IEq10"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x^{CN}_{lmn}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq10.gif"/></alternatives></inline-formula>. So once the differential equations are converged, <inline-formula id="IEq11"><alternatives><tex-math id="M30">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\dot{x}^{CN}_{lmn}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq11.gif"/></alternatives></inline-formula> is zero and Eq.&#xA0;<xref rid="Equ18" ref-type="">18</xref> turns into:<disp-formula id="Equ20"><label>20</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ 0 =-y^{CN}_{lmn} - 2\alpha^{CN}\sum_{ij} \left(y^{CN}_{lmn} - f^{n}_{ij}\right)\left(y^{SN}_{lmij}\right)^q $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ20.gif" position="anchor"/></alternatives></disp-formula>The solution of this equation for <inline-formula id="IEq12"><alternatives><tex-math id="M32">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y^{CN}_{lmn}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq12.gif"/></alternatives></inline-formula> is:<disp-formula id="Equ21"><label>21</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ y^{CN}_{lmn} = 2\alpha^{CN}{\frac{\sum_{ij} f^{n}_{ij} \cdot \left(y^{SN}_{lmij}\right)^q}{1 + \sum_{ij} \left(y^{SN}_{lmij}\right)^q}} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ21.gif" position="anchor"/></alternatives></disp-formula>Since <inline-formula id="IEq13"><alternatives><tex-math id="M34">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sum_{ij} y^{SN}_{lmij}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq13.gif"/></alternatives></inline-formula> converges to one (see selection network), the converged contents network operates like a sigma-pi unit:<disp-formula id="Equ22"><label>22</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ y^{CN}_{lmn} = \alpha^{CN}\sum_{ij} f^{n}_{ij} \cdot \left(y^{SN}_{lmij}\right)^q $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ22.gif" position="anchor"/></alternatives></disp-formula></p>
          </sec>
          <sec id="Sec32">
            <title>Selection Network</title>
            <p>The selection network aims to select an item in the input image by generating an appropriate activation pattern which ensures a veridical mapping of this item into the FOA. The selection network is structured into layers whereby each layer controls the routing for a different FOA-pixel. To ensure a veridical representation, the selection network has to fulfil two constraints (see main text): (a) one unit in the FOA should not receive an input from more than one retinal unit and (b) neighbourhood relations should be preserved during the mapping process. Constraint (a) is implemented as a WTA:<disp-formula id="Equ23"><label>23</label><alternatives><tex-math id="M36">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ E^{SN1}({\bf y}^{SN})=\alpha^{SN} \sum_{lm}\left(\sum_{ij}y^{SN}_{lmij} -1\right)^2 $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ23.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq14"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha^{SN}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq14.gif"/></alternatives></inline-formula> weights the constraint. Constraint (b) is realized with excitatory connections between layers:<disp-formula id="Equ24"><label>24</label><alternatives><tex-math id="M38">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ E^{SN2}({\bf y}^{SN}) =-\beta^{SN} \sum_{lm}\sum_{ij}\sum_{\mathop{s=-S}\limits_{s\neq0}}^{S}\sum_{\mathop{r=-R}\limits_{r\neq 0}}^{R}g\left(s,r\right)y_{\mathop{l+s,m+r}\limits_{i+s,j+r}}^{SN}y_{lmij}^{SN}$$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ24.gif" position="anchor"/></alternatives></disp-formula>and is weighted by &#x3B2;<sup><italic>SN</italic></sup>. The strength of the connection <italic>g</italic>(<italic>s</italic>,&#xA0;<italic>r</italic>) decreases with the distance between units and weakens the co-operation between units further apart from each other (see also [<xref ref-type="bibr" rid="CR16">16</xref>]). The neighbourhood function <italic>g</italic>(<italic>s</italic>,&#xA0;<italic>r</italic>) is defined by,<disp-formula id="Equ25"><label>25</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$g\left(s,r\right)={\frac{1}{A}}e^{-{\frac{s^2 + r^2}{\sigma^2}}}$$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ25.gif" position="anchor"/></alternatives></disp-formula>where <italic>A</italic> is a normalization factor with<disp-formula id="Equ26"><label>26</label><alternatives><tex-math id="M40">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ A=\sum^{S}_{s=-S}\sum^{R}_{r=-R} e^{-{\frac{s^2 + r^2}{\sigma^2}}} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ26.gif" position="anchor"/></alternatives></disp-formula>The differential equation for a selection network unit is given with<disp-formula id="Equ27"><label>27</label><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \tau \dot{x}^{SN}_{lmij}= -x^{SN}_{lmij} - {\frac{\partial E^{SN1}\left({\bf y}^{SN}\right)}{\partial y^{SN}_{lmij}}} -{\frac{\partial E^{SN2}\left({\bf y}^{SN}\right)}{\partial y^{SN}_{lmij}}} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ27.gif" position="anchor"/></alternatives></disp-formula>where the individual terms are:<disp-formula id="Equ28"><label>28</label><alternatives><tex-math id="M42">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\frac{\partial E^{SN1}\left({\bf y}^{SN}\right)}{\partial y^{SN}_{lmij}}} =2\alpha^{SN} \left( \sum_{ij} y^{SN}_{lmij} - 1 \right) $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ28.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ29"><label>29</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\frac{\partial E^{SN2}\left({\bf y}^{SN}\right)}{\partial y^{SN}_{lmij}}} =-\beta^{SN} \sum_{\mathop{s=-S}\limits_{s\neq 0}}^{S}\sum_{\mathop{r=-R}\limits_{r\neq 0}}^{R} g(s,r) y^{SN}_{l+s, m+r,i+s, j+r} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ29.gif" position="anchor"/></alternatives></disp-formula></p>
          </sec>
          <sec id="Sec33">
            <title>Knowledge Network</title>
            <p>The knowledge network implements the object identification in VS-SAIM. The energy function for the knowledge network is defined as the following,<disp-formula id="Equ30"><label>30</label><alternatives><tex-math id="M44">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \begin{aligned} E^{KN}\left({\bf y}^{KN},{\bf y}^{CN}\right) =\,&amp; \alpha^{KN}\left(\sum_{k=1}^{K}y^{KN}_k - 1\right)^2 \\ &amp;+ \beta^{KN} \sum_{k=1}^{K} \left( M\left({\bf y}^{CN},{\bf w}^k \right) - {\frac{1}{K}} \sum_{k'=1}^{K} M\left({\bf y}^{CN}, {\bf w}^{k'}\right) \right) y_k^{KN} \end{aligned} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ30.gif" position="anchor"/></alternatives></disp-formula>with <italic>M</italic>(<bold>y</bold><sup><italic>CN</italic></sup>,&#xA0;<bold>w</bold><sup><italic>k</italic></sup> ) being the Euclidean matching function between the FOA and an individual template,<disp-formula id="Equ31"><label>31</label><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ M\left({\bf y}^{CN},{\bf w}^k\right) = \sum_{lm}\sum_{n}\left(y^{CN}_{lmn} - w^{k}_{lmn}\right)^{2} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ31.gif" position="anchor"/></alternatives></disp-formula>where the index <italic>k</italic> refers to template units whose template features are stored in the weights <italic>w</italic><sup><italic>k</italic></sup><sub><italic>lmn</italic></sub>. The parameter <italic>K</italic> is the total number of templates in the model. The term <inline-formula id="IEq15"><alternatives><tex-math id="M46">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left(\sum_k y^{KN}_k - 1\right)^2$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq15.gif"/></alternatives></inline-formula> implements the WTA-constraint. The matching term <inline-formula id="IEq16"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sum_{lmn}\left(y^{CN}_{lmn} - w^k_{lmn}\right)^2 $$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq16.gif"/></alternatives></inline-formula> ensures that only the template unit is activated which gives the best match with the FOA contents. The term <inline-formula id="IEq17"><alternatives><tex-math id="M48">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\frac{1}{K}} \sum_{k'=1}^{K} M\left({\bf y}^{CN}, {\bf w}^{k'}\right)$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq17.gif"/></alternatives></inline-formula> calculates the mean distance of the FOA from all templates and will be explained in the context of the differential equations. The parameter &#x3B1;<sup><italic>KN</italic></sup> and &#x3B2;<sup><italic>KN</italic></sup> weight the constraints against each other.</p>
            <p>The energy function is differentiated with respect to <inline-formula id="IEq18"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y^{KN}_k$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq18.gif"/></alternatives></inline-formula> which creates the following differential equation (without the leaky integrator) for a knowledge network unit,<disp-formula id="Equ32"><label>32</label><alternatives><tex-math id="M50">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \tau \dot{x}^{KN}_{k} = -{\frac{\partial E\left({\bf y}^{KN},{\bf y}^{CN}\right)}{\partial y^{KN}_k}} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ32.gif" position="anchor"/></alternatives></disp-formula>with the partial derivation term defined as,<disp-formula id="Equ33"><label>33</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \begin{aligned} {\frac{\partial E\left({\bf y}^{KN},{\bf y}^{CN}\right)}{\partial y^{KN}_k}} =\,&amp; 2 \alpha^{KN} \left( \sum_{k} y^{KN}_k - 1 \right)\\ &amp;+ \beta^{KN} \left( \sum_{lmn} \left(y^{CN}_{lmn} - w^{k}_{lmn} \right)^2 - {\frac{1}{K}} \sum_{k'} \sum_{lmn} \left(y^{CN}_{lmn} - w^{k'}_{lmn} \right)^2 \right) \end{aligned} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ33.gif" position="anchor"/></alternatives></disp-formula></p>
            <p>With this implementation, the knowledge network &#x201C;waits&#x201D; during the &#x201C;first&#x201D; phase of the selection process until the FOA begins to represent information about the input image. In other words the knowledge network is only influenced by the &#x201C;real&#x201D; selection process and pilot studies showed that this approach made VS-SAM&#x2019;s search performance more robust. This waiting status (<inline-formula id="IEq19"><alternatives><tex-math id="M52">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\dot{x}^{KN}_{k} = 0$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq19.gif"/></alternatives></inline-formula>) results from the following conditions at the beginning of simulations: First, the sum of the initial values of the output activation is one (see parameters), thus, the WTA-term is zero. Second, the FOA output is initialized with average between the two templates (unbiased template) and, consequently, the matching term is zero too, because the mean matching value is subtracted from the matching values.</p>
          </sec>
          <sec id="Sec34">
            <title>Top-Down Feedback and Matching Network</title>
            <p>So far, we only discussed the following partial derivatives of the energy functions: <inline-formula id="IEq20"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\frac{\partial E\left({\bf y}^{KN},{\bf y}^{CN}\right)}{\partial y^{KN}_k}}, {\frac{\partial E^{CN}\left({\bf y}^{SN},{\bf y}^{CN}\right)}{\partial y^{CN}_{lmn}}}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq20.gif"/></alternatives></inline-formula>. In fact, these terms define the bottom-up pathway. However, for the gradient descent approach to be successful, it requires to consider partial derivatives to all dynamical variables: <inline-formula id="IEq21"><alternatives><tex-math id="M54">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\frac{\partial E\left({\bf y}^{KN},{\bf y}^{CN}\right)}{\partial y^{CN}_k}}, {\frac{\partial E^{CN}\left({\bf y}^{SN},{\bf y}^{CN}\right)}{\partial y^{SN}_{lmn} }}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq21.gif"/></alternatives></inline-formula>. These partial derivative construct the top-down pathway.</p>
            <p>In extensive tests, we found that &#x201C;strict&#x201D; application of the gradient descent procedure showed that does not lead to a reliable visual search in VS-SAIM. Subsequently, we modified the network architecture and introduced the <italic>&#x201C;matching network&#x201D;</italic>. Nevertheless, the resulting topology was inspired by a strict application of the gradient descent method. Therefore, we will first present its correct application and, then, introduce and discuss the changes which led to the network architecture, as it is presented in the main text.</p>
            <p>For the knowledge network, the top-down path is derived by a partial derivative with respect to <inline-formula id="IEq22"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x^{CN}_{lmn}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq22.gif"/></alternatives></inline-formula>:<disp-formula id="Equ34"><label>34</label><alternatives><tex-math id="M56">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \tau \dot{x}^{CN}_{lmn} = -x^{CN}_{lmn} - {\frac{\partial E^{KN}\left({\bf y}^{CN},{\bf y}^{KN}\right)}{\partial y^{CN}_{lmn}}} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ34.gif" position="anchor"/></alternatives></disp-formula>with the derivation termed defined as,<disp-formula id="Equ35"><label>35</label><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\frac{\partial E^{KN}\left( {\bf y}^{CN},{\bf y}^{KN}\right)}{\partial y^{CN}_{lmn}}} =\, 2 \beta^{KN} \sum_{k} \left(\left( y^{CN}_{lmn} - w^{k}_{lmn} \right) - {\frac{1}{K}} \sum_{k'} \left( y^{CN}_{lmn} - w^{k'}_{lmn} \right) \right) y^{KN}_{k} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ35.gif" position="anchor"/></alternatives></disp-formula>This term would have been added to Eq.&#xA0;<xref rid="Equ19" ref-type="">19</xref> and would have introduced a direct feedback from the knowledge network into the contents network. For the contents network the energy function is derived with respect to <inline-formula id="IEq23"><alternatives><tex-math id="M58">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y^{SN}_{lmij}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq23.gif"/></alternatives></inline-formula>:<disp-formula id="Equ36"><label>36</label><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\frac{\partial E^{CN}\left({\bf y}^{SN},{\bf y}^{CN}\right)}{\partial y^{CN}_{lmn}}} = \alpha^{CN} \cdot q \sum_{lm} \sum_{ij} \sum_{n}\left( y^{CN}_{lmn}-f^n_{ij} \right)^2 \left( y^{SN}_{lmij}\right)^{(q -1)} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ36.gif" position="anchor"/></alternatives></disp-formula></p>
            <p>This term would have been added to Eq.&#xA0;<xref rid="Equ27" ref-type="">27</xref> and would have introduced a input from the contents network into the selection network.</p>
            <p>The pilot simulations revealed that the direct feedback from the knowledge network into the contents network represented a major problem for a successful visual search, because the directness of the feedback loop made it difficult to balance bottom-up and top-down influence. Therefore, we replaced the contents network in the top-down path with the matching network <inline-formula id="IEq24"><alternatives><tex-math id="M60">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x^{MN}_{lmn}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq24.gif"/></alternatives></inline-formula>, so that the top-down influence became less immediate and the feedback loop is closed via the selection network. Expressed in mathematical terms, Eq.&#xA0;<xref rid="Equ34" ref-type="">34</xref> turned into<disp-formula id="Equ37"><label>37</label><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \tau \dot{x}^{MN}_{lmn} = -x^{MN}_{lmn} - {\frac{\partial E^{KN}\left( {\bf y}^{MN},{\bf y}^{KN}\right)}{\partial y^{NM}_{lmn}}} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ37.gif" position="anchor"/></alternatives></disp-formula>with the derivation termed defined as,<disp-formula id="Equ38"><label>38</label><alternatives><tex-math id="M62">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\frac{\partial E^{KN}\left( {\bf y}^{MN},{\bf y}^{KN}\right)} {\partial y^{MN}_{lmn}}} =\, 2 \beta^{KN} \sum_{k} \left( \left( y^{MN}_{lmn} - w^{k}_{lmn} \right) - {\frac{1}{K}} \sum_{k'} \left( y^{MN}_{lmn} - w^{k'}_{lmn} \right) \right) y^{KN}_{k} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ38.gif" position="anchor"/></alternatives></disp-formula>Like the contents network, the initial values of the matching network were the averaged templates. The way the matching network projects into the selection network following Eq.&#xA0;<xref rid="Equ37" ref-type="">37</xref>. However, our pilot studies showed that the factor <inline-formula id="IEq25"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left(y^{SN}_{lmij}\right)^{(q -1)}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq25.gif"/></alternatives></inline-formula> often prevented a successful selection, as this term is often close to zero and therefore blocks any top-down modulation from the matching network. There we set <italic>q</italic>&#xA0;=&#xA0;1 in the top-down pathway and eliminated this factor.</p>
            <p>The pilot studies revealed a further problem with the matching network. The matching term (<inline-formula id="IEq26"><alternatives><tex-math id="M64">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y^{CN}_{lmn} - w^{k'}_{lmn}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq26.gif"/></alternatives></inline-formula>) does not take into account any the spatial neighbourhood relations between pixels in items. Even though the neighbourhood constraint in the selection network should be able to solve this problem, the pilot simulations indicated that this is not sufficient to ensure a successful operation of VS-SAIM for all items used here. Therefore, we introduced a spatial matching window for each feature map location with the same size as the matching template. This heuristics was successfully employed in natural images [<xref ref-type="bibr" rid="CR20">20</xref>]. The resulting equation is the following:<disp-formula id="Equ39"><label>39</label><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ E^{MN} \left({\bf y}^{SN},{\bf y}^{MN}\right)=\, \alpha^{MN} \sum_{lm} \sum_{ij}\left(\sum_{s=-S}^{S}\sum_{r=-R}^{R}\sum_{n}\left( y^{MN}_{l+s,m+r,n} - f^n_{i+s,j+r} \right)^2 \right) y^{SN}_{lmij} $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ39.gif" position="anchor"/></alternatives></disp-formula>The term <inline-formula id="IEq27"><alternatives><tex-math id="M66">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sum_{s=-S}^{S}\sum_{r=-R}^{R}\left(y^{MN}_{l+s,m+r,n}-f^n_{i+s,j+r}\right)^2$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq27.gif"/></alternatives></inline-formula> constitutes the additional matching window. The partial derivative with respect to <inline-formula id="IEq28"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y^{SN}_{lmij}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq28.gif"/></alternatives></inline-formula> is<disp-formula id="Equ40"><label>40</label><alternatives><tex-math id="M68">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\frac{\partial E^{MN}\left({\bf y}^{SN},{\bf y}^{MN}\right)}{\partial y^{SN}_{lmij}}} = \alpha^{MN} \sum_{s,r} \sum_{n} \left( y^{MN}_{l+s, m+r,n} - f^n_{i+s, j+r}\right)^2 $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ40.gif" position="anchor"/></alternatives></disp-formula></p>
            <p>The pilot studies highlighted a final problem. The activation amplitudes of the input into the selection <inline-formula id="IEq29"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left({\frac{\partial E^{MN}\left({\bf y}^{SN},{\bf y}^{MN}\right)}{\partial y^{SN}_{lmij}}}\right)$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq29.gif"/></alternatives></inline-formula> can greatly vary depending on the search items. Occasionally, these activations were too high for the WTA-constraint in the selection network to work successfully, especially for large displays. In other words, the global inhibition was not able to restrict the number of winners to one. This situation particularly occurred at the beginning of the selection process, termed Phase 1 in the main text, when the unbiased matching templates matches equally well all items. A possible consequence of this failure is that several distractors are being selected before the knowledge network induces the target template in the matching template. Therefore, we added a normalization to input of the selection network:<disp-formula id="Equ41"><label>41</label><alternatives><tex-math id="M70">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \tau \dot{x}^{SN}_{lmij}= -x^{SN}_{lmij} - {\frac{\partial E^{SN1}\left({\bf y}^{SN}\right)}{\partial y^{SN}_{lmij}}} -{\frac{\partial E^{SN2}\left({\bf y}^{SN}\right)}{\partial y^{SN}_{lmij}}} -norm({\frac{\partial E^{MN}\left({\bf y}^{SN},{\bf y}^{MN}\right)}{\partial y^{SN}_{lmij} }}) $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ41.gif" position="anchor"/></alternatives></disp-formula>It is important to note the result of the normalization is termed <italic>matching surface</italic> in the main text. The function <italic>norm</italic>(&#x2022;) is defined as,<disp-formula id="Equ42"><label>42</label><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ norm\left(z\right) = \nu*{\frac{ z - \left\Vert {\bf y}^{MN}(t=0)\right\Vert^{2}}{\left\Vert{\bf y}^{MN}(t=0) - {\bf w}^{1}\right\Vert^{2}}} + \xi $$\end{document}</tex-math><graphic xlink:href="12559_2010_9076_Article_Equ42.gif" position="anchor"/></alternatives></disp-formula>where the term <bold>y</bold><sup><italic>MN</italic></sup>(<italic>t</italic>&#xA0;=&#xA0;0) is the activation of the matching template at initialization (unbiased matching template), while <bold>w</bold><sup>1</sup> is the first template in the knowledge network template. The normalization ensures that the matching level is &#x3BE; in the background and &#x3BD;&#xA0;+&#xA0;&#x3BE; at the centre of an item. &#x3BE; and &#x3BD; were set to 0.5 and 1.0 respectively. The normalization subtracts the matching value of the unbiased matching templates with the background <inline-formula id="IEq30"><alternatives><tex-math id="M72">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\Vert{\bf y}^{MN}(t=0) \right\Vert^{2}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq30.gif"/></alternatives></inline-formula> from the matching levels (<italic>z</italic>). Hence, the normalization transforms the matching level in the background to &#x3BE;. The best matching level at the beginning of the selection process is <inline-formula id="IEq32"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\Vert{\bf y}^{MN}(t=0)-{\bf w}^{1}\right\Vert^{2}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq32.gif"/></alternatives></inline-formula> which, in fact, is the same value as <inline-formula id="IEq33"><alternatives><tex-math id="M74">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\Vert{\bf y}^{MN}(t=0)-{\bf w}^{2}\right\Vert^{2}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq33.gif"/></alternatives></inline-formula>, as the unbiased matching template has the same Euclidian distance from the two templates. By dividing <inline-formula id="IEq31"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z - \left\Vert{\bf y}^{MN}(t=0) \right\Vert^{2}$$\end{document}</tex-math><inline-graphic xlink:href="12559_2010_9076_Article_IEq31.gif"/></alternatives></inline-formula> through this highest value the normalization restricts the input activation to selection network to &#x3BD;&#xA0;+&#xA0;&#x3BE;.</p>
          </sec>
        </sec>
        <sec id="Sec35">
          <title>Simulation Parameter</title>
          <p>See Tables <xref rid="Tab1" ref-type="table">1</xref>, <xref rid="Tab2" ref-type="table">2</xref>, <xref rid="Tab3" ref-type="table">3</xref>, <xref rid="Tab4" ref-type="table">4</xref>, <xref rid="Tab5" ref-type="table">5</xref>, and <xref rid="Tab6" ref-type="table">6</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Contents network</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Parameter</th><th align="left">Value</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left"><italic>f</italic>(<italic>x</italic>)</td><td align="left">Linear</td><td align="left">Type of output function</td></tr><tr><td align="left">m</td><td align="left">1.0</td><td align="left"/></tr><tr><td align="left">s</td><td align="left">0.0</td><td align="left"/></tr><tr><td align="left">&#x3C4;</td><td align="left">1.0</td><td align="left">Time constant</td></tr><tr><td align="left">&#x3B1;<sup><italic>CN</italic></sup></td><td align="left">1.0</td><td align="left">Mapping factor</td></tr><tr><td align="left">q</td><td align="left">2.0</td><td align="left">Mapping power factor</td></tr></tbody></table></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>Feature extraction</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Parameter</th><th align="left">Value</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">I&#xA0;&#xD7;&#xA0;J</td><td align="left">43&#xA0;&#xD7;&#xA0;43</td><td align="left">Size of input</td></tr><tr><td align="left">L&#xA0;&#xD7;&#xA0;M</td><td align="left">9&#xA0;&#xD7;&#xA0;9</td><td align="left">Size of FoA</td></tr><tr><td align="left">N</td><td align="left">5</td><td align="left">Feature dimensions</td></tr><tr><td align="left">k</td><td align="left">0.3</td><td align="left">Gabor frequency</td></tr><tr><td align="left">&#x3C3;</td><td align="left">3.0</td><td align="left">Gabor sigma</td></tr><tr><td align="left">&#x3B8;</td><td align="left">[0, 90, 45, 135]</td><td align="left">Tuned orientations in [&#xB0;]</td></tr><tr><td align="left"><italic>p</italic><sup><italic>n</italic></sup></td><td align="left">[0.3,1.0,1.0,0.1,0.1]</td><td align="left">Feature weighting</td></tr><tr><td align="left">S &amp; R</td><td align="left">2</td><td align="left">Half window size Gabor filter</td></tr></tbody></table></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption><p>Selection Network</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Parameter</th><th align="left">Value</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left"><italic>f</italic>(<italic>x</italic>)</td><td align="left">Sigmoid</td><td align="left">Type of output function</td></tr><tr><td align="left">m</td><td align="left">15.0</td><td align="left"/></tr><tr><td align="left">s</td><td align="left">1.0</td><td align="left"/></tr><tr><td align="left">&#x3C4;</td><td align="left">0.5</td><td align="left">Time constant</td></tr><tr><td align="left">&#x3B1;<sup><italic>SN</italic></sup></td><td align="left">120.0</td><td align="left">Weighting WTA</td></tr><tr><td align="left">&#x3B2;<sup><italic>SN</italic></sup></td><td align="left">20.0</td><td align="left">Weighting neighbourhood function</td></tr><tr><td align="left">&#x3C3;</td><td align="left">7.0</td><td align="left">Sigma in neighbourhood function</td></tr><tr><td align="left">S &amp; R</td><td align="left">4</td><td align="left">Half size of neighbourhood function</td></tr></tbody></table></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>Knowledge network</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Parameter</th><th align="left">Value</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left"><italic>f</italic>(<italic>x</italic>)</td><td align="left">Sigmoid</td><td align="left">Type of output function</td></tr><tr><td align="left">m</td><td align="left">15.0</td><td align="left"/></tr><tr><td align="left">s</td><td align="left">0.5</td><td align="left"/></tr><tr><td align="left">&#x3C4;</td><td align="left">0.1</td><td align="left">Time constant</td></tr><tr><td align="left">&#x3B1;<sup><italic>KN</italic></sup></td><td align="left">0.1</td><td align="left">Weighting WTA</td></tr><tr><td align="left">&#x3B2;<sup><italic>KN</italic></sup></td><td align="left">1.0</td><td align="left">Weighting matching</td></tr><tr><td align="left"><italic>Decision</italic>&#xA0;&#x2212;&#xA0;<italic>thresh</italic></td><td align="left">0.7</td><td align="left">Search termination threshold</td></tr><tr><td align="left"><italic>Target</italic>&#xA0;&#x2212;&#xA0;<italic>init</italic></td><td align="left">0.506</td><td align="left">Biased initialization of target unit</td></tr><tr><td align="left"><italic>Distractor</italic>&#xA0;&#x2212;&#xA0;<italic>init</italic></td><td align="left">0.494</td><td align="left">Biased initialization of distractor unit</td></tr></tbody></table></table-wrap><table-wrap id="Tab5"><label>Table 5</label><caption><p>Noise</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Parameter</th><th align="left">Value</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">Tau</td><td char="." align="char">1.0</td><td align="left">Time constant</td></tr><tr><td align="left">Min</td><td char="." align="char">0.0</td><td align="left">Minimum noise amplitude</td></tr><tr><td align="left">Max</td><td char="." align="char">0.0001</td><td align="left">Maximum noise amplitude</td></tr><tr><td align="left">&#x3B3;</td><td char="." align="char">0.00001</td><td align="left">Damping constant</td></tr><tr><td align="left">&#x3C9;</td><td char="." align="char">10.0</td><td align="left">Frequency</td></tr><tr><td align="left"><italic>A</italic></td><td char="." align="char">7.0</td><td align="left">Amplitude</td></tr></tbody></table></table-wrap><table-wrap id="Tab6"><label>Table 6</label><caption><p>Matching network</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Parameter</th><th align="left">Value</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left"><italic>f</italic>(<italic>x</italic>)</td><td align="left">Linear</td><td align="left">Type of output function</td></tr><tr><td align="left">m</td><td align="left">1.0</td><td align="left"/></tr><tr><td align="left">s</td><td align="left">0.0</td><td align="left"/></tr><tr><td align="left">&#x3C4;</td><td align="left">1.0</td><td align="left">Time constant</td></tr><tr><td align="left">&#x3B1;<sup><italic>MN</italic></sup></td><td align="left">1.0</td><td align="left">Feedback factor</td></tr><tr><td align="left">S &amp; R</td><td align="left">4</td><td align="left">Half window size</td></tr><tr><td align="left">&#x3BE;</td><td align="left">1.0</td><td align="left">Background matching surface</td></tr><tr><td align="left">&#x3BD;</td><td align="left">0.5</td><td align="left">Item location to blank background contrast</td></tr></tbody></table></table-wrap></p>
        </sec>
      </app>
    </app-group>
    <fn-group>
      <fn id="Fn1">
        <label>1</label>
        <p>Indeed simulations not included in this paper suggest that the concept of a &#x201D;dynamic saliency map&#x201D; can improve our understanding of visual search tasks.</p>
      </fn>
      <fn id="Fn2">
        <label>2</label>
        <p>In fact, the dynamics of the activations in the matching surface also play a role, but are not crucial for the simulation results in this paper.</p>
      </fn>
    </fn-group>
  </back>
</article>
