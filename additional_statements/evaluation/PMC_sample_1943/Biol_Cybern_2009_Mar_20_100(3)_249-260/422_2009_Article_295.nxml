<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN" "archivearticle.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xml:lang="EN" article-type="research-article">
  <?origin publisher?>
  <?properties no_embargo?>
  <?properties open_access?>
  <?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
  <?DTDIdentifier.IdentifierType public?>
  <?SourceDTD.DTDName A++V2.4.dtd?>
  <?SourceDTD.Version 2.4?>
  <?ConverterInfo.XSLTName springer2nlm.xsl?>
  <?ConverterInfo.Version 1?>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Biol Cybern</journal-id>
      <journal-title>Biological Cybernetics</journal-title>
      <issn pub-type="ppub">0340-1200</issn>
      <issn pub-type="epub">1432-0770</issn>
      <publisher>
        <publisher-name>Springer-Verlag</publisher-name>
        <publisher-loc>Berlin/Heidelberg</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">2798030</article-id>
      <article-id pub-id-type="pmid">19229556</article-id>
      <article-id pub-id-type="publisher-id">295</article-id>
      <article-id pub-id-type="doi">10.1007/s00422-009-0295-8</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Original Paper</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Learning to reach by reinforcement learning using a receptive field based function approximation approach with continuous actions</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <name name-style="western">
            <surname>Tamosiunaite</surname>
            <given-names>Minija</given-names>
          </name>
          <address>
            <email>m.tamosiunaite@if.vdu.lt</email>
          </address>
          <xref ref-type="aff" rid="Aff1">1</xref>
          <xref ref-type="aff" rid="Aff2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <name name-style="western">
            <surname>Asfour</surname>
            <given-names>Tamim</given-names>
          </name>
          <address>
            <email>asfour@ira.uka.de</email>
          </address>
          <xref ref-type="aff" rid="Aff3">3</xref>
        </contrib>
        <contrib contrib-type="author" corresp="yes">
          <name name-style="western">
            <surname>W&#xF6;rg&#xF6;tter</surname>
            <given-names>Florentin</given-names>
          </name>
          <address>
            <email>worgott@bccn-goettingen.de</email>
          </address>
          <xref ref-type="aff" rid="Aff1">1</xref>
        </contrib>
        <aff id="Aff1"><label>1</label>Bernstein Centre for Computational Neuroscience, University of G&#xF6;ttingen, Bunsenstr. 10, 37073 G&#xF6;ttingen, Germany </aff>
        <aff id="Aff2"><label>2</label>Department of Informatics, Vytautas Magnus University, Vileikos 8, 44404 Kaunas, Lithuania </aff>
        <aff id="Aff3"><label>3</label>Institute for Computer Science and Engineering, Universit&#xE4;t Karlsruhe (TH), Karlsruhe, Germany </aff>
      </contrib-group>
      <pub-date pub-type="epub">
        <day>20</day>
        <month>2</month>
        <year>2009</year>
      </pub-date>
      <pub-date pub-type="ppub">
        <month>3</month>
        <year>2009</year>
      </pub-date>
      <volume>100</volume>
      <issue>3</issue>
      <fpage>249</fpage>
      <lpage>260</lpage>
      <history>
        <date date-type="received">
          <day>1</day>
          <month>2</month>
          <year>2008</year>
        </date>
        <date date-type="accepted">
          <day>3</day>
          <month>2</month>
          <year>2009</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>&#xA9; The Author(s) 2009</copyright-statement>
      </permissions>
      <abstract xml:lang="EN">
        <p>Reinforcement learning methods can be used in robotics applications especially for specific target-oriented problems, for example the reward-based recalibration of goal directed actions. To this end still relatively large and continuous state-action spaces need to be efficiently handled. The goal of this paper is, thus, to develop a novel, rather simple method which uses reinforcement learning with function approximation in conjunction with different reward-strategies for solving such problems. For the testing of our method, we use a four degree-of-freedom reaching problem in 3D-space simulated by a two-joint robot arm system with two DOF each. Function approximation is based on 4D, overlapping kernels (receptive fields) and the state-action space contains about 10,000 of these. Different types of reward structures are being compared, for example, reward-on- touching-only against reward-on-approach. Furthermore, forbidden joint configurations are punished. A continuous action space is used. In spite of a rather large number of states and the continuous action space these reward/punishment strategies allow the system to find a good solution usually within about 20 trials. The efficiency of our method demonstrated in this test scenario suggests that it might be possible to use it on a real robot for problems where mixed rewards can be defined in situations where other types of learning might be difficult.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Reinforcement learning</kwd>
        <kwd>Function approximation</kwd>
        <kwd>Robot control</kwd>
      </kwd-group>
      <custom-meta-wrap>
        <custom-meta>
          <meta-name>issue-copyright-statement</meta-name>
          <meta-value>&#xA9; Springer-Verlag 2009</meta-value>
        </custom-meta>
      </custom-meta-wrap>
    </article-meta>
  </front>
  <body>
    <sec id="Sec1" sec-type="introduction">
      <title>1 Introduction</title>
      <p>In robotics as well as other applications it is required to match the employed learning method to the task. A somewhat extreme example makes this very clear: it is useless to employ methods for the learning of induction logic to the learning of motor skills, like the tying of a shoe lace, whereas such methods are useful for the learning of all kinds of declarative knowledge.</p>
      <p>Even within one domain, for example the field of motor learning, one has still many choices of learning methods depending on the different target applications and the choice of the method can much influence success and efficiency of learning. Many times supervised learning methods are chosen, because they are well controlled and fast. For example learning from demonstration can be used to teach a robot certain skills like simple manipulations of objects (<xref ref-type="bibr" rid="CR5">Breazeal and Scassellati 2008</xref>; <xref ref-type="bibr" rid="CR39">Schaal et al. 2003</xref>; <xref ref-type="bibr" rid="CR8">Dillmann 2004</xref>). Supervised learning requires an explicit error function between outcome and the taught examples for controlling the learning progress. Reinforcement learning (RL) is more general, because such an error function is not anymore required and the system learns from receiving general feedback about the goodness of its actions. Commonly, the less specific feedback of RL will, however, lead to slower convergence times especially in systems with many degrees of freedom, which have large state-action spaces. To improve on this an appropriate representation of the state-action space has to be found. An appropriate match between problem and method can lead to major learning successes like in the works of Schaal and colleagues, who concentrate on the task of predefined trajectory following, and use RL approaches tuned to this task (<xref ref-type="bibr" rid="CR30">Peters and Schaal 2006a</xref>, <xref ref-type="bibr" rid="CR32">2007</xref>, <xref ref-type="bibr" rid="CR33">2008</xref>).</p>
      <p>In the present study, we will also focus on a reinforcement learning system in a relatively large space asking, how to bring together a useful, generalizable representation of the state-action space with a combination of reward and punishment strategies. Such a problem commonly exists when robots have to learn to calibrate their own actions relative to a desired outcome (which defines the reward). For example, Learning to fill a glass could be learned from demonstration by a human and &#x201C;copied&#x201D; by the robot, but it is known (A. Ude, personal communication) that the results of such a plain learning attempt are not satisfactory (spill-over, low filling level). With an RL method we can, however, recalibrate the learned parameter set as it will suffice to correct the robot&#x2019;s action close to the glass, leading to a small state-action space. Here we have a natural situation where a combination of rewards and punishments can be used: the filling level (recorded from the human&#x2019;s action) would be the primary reward, spill over would be a punishment, and the distance to the target would be a secondary reward.</p>
      <p>To generalize beyond this example problem we focus here on the learning of visual servoing, where the approach to an object is guided by the visual difference measure between the end-effector of the robot arm and the target object. This more complex learning problem (as compared to the recalibration mentioned above) is chosen to demonstrate how far we can go with the newly introduced method. For an excellent introduction to visual servoing the reader is referred to (<xref ref-type="bibr" rid="CR19">Hutchinson et al. 1996</xref>; <xref ref-type="bibr" rid="CR6">Chaumette and Hutchinson 2007a</xref>,<xref ref-type="bibr" rid="CR7">b</xref>). A visual servoing control schema uses the Jacobian matrix to gradually reduce the difference between the end-effector and the target object. However, the estimation of the Jacobian matrix becomes quickly hard as the number of joints and consequently the redundancy of the robot system increases. The high number of degrees of freedom, especially in humanoid robot arms and anthropomorphic hands, together with the necessity to handle constraints in joint space (singular configurations, self- collision and mechanical joint limits avoidance) as well as in object space (obstacles) makes visual servoing in highly redundant robot systems a difficult problem.</p>
      <p>Several methods exist for solving the visual servoing problem, some are control-based (<xref ref-type="bibr" rid="CR10">Espiau et al. 1992</xref>; <xref ref-type="bibr" rid="CR18">Hosoda and Asada 1994</xref>; <xref ref-type="bibr" rid="CR19">Hutchinson et al. 1996</xref>; <xref ref-type="bibr" rid="CR16">Horaud et al. 1998</xref>), while others focus on learning (<xref ref-type="bibr" rid="CR40">Shibata and Ito 1999</xref>; <xref ref-type="bibr" rid="CR25">Martinez-Marin and Duckett 2004</xref>; <xref ref-type="bibr" rid="CR29">Perez and Cook 2004</xref>; <xref ref-type="bibr" rid="CR23">Leonard and Jagersand 2004</xref>).</p>
      <p>Methods range from conventional neural networks, where some classical work had been performed early by the group of Schulten (<xref ref-type="bibr" rid="CR26">Martinetz et al. 1990</xref>) and Torras [mainly in the domain of inverse kinematics learning, <xref ref-type="bibr" rid="CR35">Ruis de Angulo and Torras (2005a)</xref>; <xref ref-type="bibr" rid="CR36">Ruis de Angulo and Torras (2005b)</xref>], to the now dominating methods of reinforcement learning (RL). Indeed a very large number of articles has appeared in the last years on RL for learning arm control as well as learning grasping. A large diversity of methods has been suggested exemplary represented by the papers cited here, Arm: <xref ref-type="bibr" rid="CR53">Tham and Prager (1993)</xref>; <xref ref-type="bibr" rid="CR21">Kobayashi et al. (2005)</xref>; <xref ref-type="bibr" rid="CR24">Li et al. (2006)</xref>;<xref ref-type="bibr" rid="CR55">Wang et al. (2006)</xref>; <xref ref-type="bibr" rid="CR31">Peters and Schaal (2006b)</xref>; Grasp: <xref ref-type="bibr" rid="CR28">Moussa and Kamel (1998)</xref>; <xref ref-type="bibr" rid="CR27">Moussa (2004)</xref>; <xref ref-type="bibr" rid="CR38">Rezzoug et al. (2006)</xref>. This diversity arises due to the fact that conventional RL methods (<xref ref-type="bibr" rid="CR45">Sutton 1988</xref>;<xref ref-type="bibr" rid="CR56">Watkins 1989</xref>;<xref ref-type="bibr" rid="CR57">Watkins and Dayan 1992</xref>), for which rigorous convergence proofs exist, cannot be directly used as the state-action spaces in robot control are too large and convergence will take far too long, especially when using continuous actions, like here. As a consequence, the state-action value function of the used RL-method needs to be approximated by so-called function approximation methods. This is where the diversity arises as there is a terrifically high number of possible such methods existing (e.g. <xref ref-type="bibr" rid="CR52">Tesauro 1995</xref>; <xref ref-type="bibr" rid="CR17">Horiuchi et al. 1997</xref>; <xref ref-type="bibr" rid="CR12">Fukao et al. 1998</xref>; <xref ref-type="bibr" rid="CR15">Gross et al. 1998</xref>; <xref ref-type="bibr" rid="CR9">Enokida et al. 1999</xref>; <xref ref-type="bibr" rid="CR34">Qiang et al. 2000</xref>; <xref ref-type="bibr" rid="CR49">Takahashi et al. 1999</xref>; <xref ref-type="bibr" rid="CR50">Takeda et al. 2001</xref>; <xref ref-type="bibr" rid="CR20">Kabudian et al. 2004</xref>; <xref ref-type="bibr" rid="CR58">Wiering 2004</xref>; <xref ref-type="bibr" rid="CR54">van Hasselt and Wiering 2007</xref>; <xref ref-type="bibr" rid="CR44">Sugiyama et al. 2007</xref> for a textbook discussion see <xref ref-type="bibr" rid="CR46">Sutton and Barto 1998</xref>) and it lies at the discretion of the researcher to invent more. Convergence to the optimal solution can in general not be rigorously assured anymore. However, this aspect is of minor importance for a robot application as any trial will have to be interrupted if it is too long. Thus, as soon as these systems reach a &#x201C;good solution&#x201D; within &#x201C;reasonable&#x201D; time, the used method appears acceptable from an applied perspective. Especially when considering the task of learning to calibrate an action using a desired outcome as a reward one is interested to perform this calibration quickly and will be satisfied with any action which is good enough to obtain this outcome.</p>
      <p>To achieve this we will adopt a strategy inspired by the place field system of rats, which is used for navigation learning as suggested by several models (<xref ref-type="bibr" rid="CR11">Foster et al. 2000</xref>; <xref ref-type="bibr" rid="CR1">Arleo and Gerstner 2000</xref>; <xref ref-type="bibr" rid="CR43">Str&#xF6;sslin et al. 2005</xref>), and use overlapping place fields to structure our state-action space (<xref ref-type="bibr" rid="CR51">Tamosiunaite et al. 2008</xref>). Rats run on the ground and are, thus, faced with a 2D target problem with a 2DoF motion needing about 500 place fields in a 1&#xD7;1 m arena for good convergence (<xref ref-type="bibr" rid="CR51">Tamosiunaite et al. 2008</xref>). On the other hand, the simulated arm, we use, operates in a 3D target domain with a 4DoF angle space for which we require 10,000 4D place fields. As a consequence, efficient strategies for structuring the reward space are required, too, without which convergence to a good solution would take too long. Thus, this paper will compare different types of visual (distance dependent) and non-visual (touch, angle configuration) rewards showing that, with this combination of methods, it is generically possible to find a good solution within about 20 trials.</p>
    </sec>
    <sec id="Sec2">
      <title>2 Methods</title>
      <sec id="Sec3">
        <title>2.1 General setup</title>
        <p>Figure <xref rid="Fig1" ref-type="fig">1</xref> shows the setup of our system in 3D as well as two planar projections. The arm has two joints with two angles each (azimuth <italic>&#x3B1;</italic><sub>1,2</sub> and elevation <italic>&#x3D5;</italic><sub>1,2</sub>) covering a 3D reaching space of 4.0<sup>3</sup> units<sup>3</sup> (each arm segment is 1.0 unit long). The target for reaching is a sphere with diameter 0.84 units. Joint angles are constraint between 0 and 360&#xB0;. Hence, when at a constraint the arm needs to go back. This simulates similar constraints in human (e.g. elbow constraint) or robot arms.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Two-joint arm and an object for reaching in 3D space as well as two projections (<italic>x, y</italic>) and (<italic>x, z</italic>) of the arm and the target object for reaching</p></caption><graphic position="anchor" xlink:href="422_2009_Article_295_Fig1" id="MO1"/></fig></p>
        <p>The control and learning circuit used for arm acting and learning is provided in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. The four joint angles are considered to form a 4D-state space. In the state space spherical, binary 4D-kernels <italic>&#x3D5;</italic><sup><italic>k</italic></sup> with a diameter between 1, 382 and 3, 358 (uniform distribution) are formed in a 4D-space of 10, 000<sup>4</sup> units<sup>4</sup>. Those kernels, of which we use 10, 000 in total, have eight trainable connections to themotor units (total of 80, 000 connections). Kernels centers are distributed with a uniform distribution and a coverage of about 10&#x2013;12 kernels overlapping for any given 4D-location is obtained. A kernel is active, with an output value of 1, as soon as the 4D-joint angle vector falls into the topographical region covered by the kernel, otherwise kernel output is zero. As kernels are overlapping, total activity is then given by the average over active kernels (see definition of <italic>Q</italic>-values below).
<fig id="Fig2"><label>Fig. 2</label><caption><p>Schematic diagram of learning and action generation in our system. The angle vector (<italic>bottom</italic>) represents a location in 4D space and, thus, stimulates the gray kernels which fire while the white kernels are not stimulated and do not fire. The layers of units above calculate the finally required values <italic>A</italic> and <italic>D</italic> (<italic>top box</italic> see Eqs. <xref rid="Equ3" ref-type="">3</xref>, <xref rid="Equ4" ref-type="">4</xref>) which control the motors and move the joints to a new position</p></caption><graphic position="anchor" xlink:href="422_2009_Article_295_Fig2" id="MO2"/></fig></p>
        <p>According to the trajectory forming strategy (see below) the actual movement is then generated from the motor unit activity. The trajectory forming strategy includes, for example, the exploration-exploitation trade-off and trajectory smoothing (if applicable).</p>
        <p>The employed learning scheme uses a certain type of <italic>Q</italic> learning with state-action value function approximation, where function approximation is introduced through the fact, that we are not using discrete states, but state information is, instead, provided by the activity of the kernels (commonly called &#x2018;feature vectors&#x2019; in the reinforcement learning literature) as described above.</p>
        <p>Let us use the proximal joint (subscript &#x201C;1&#x201D;), and its azimuth component <italic>&#x3B1;</italic><sub>1</sub> to explain our methods. Explanation is the same for other angular components. We will be operating with <italic>Q</italic> values, and for the component <italic>&#x3B1;</italic><sub>1</sub> two <italic>Q</italic>-values will be defined: <italic>Q</italic><sub>&#x3B1;1</sub> (<italic>s, i </italic>) and <italic>Q</italic><sub>&#x3B1;1</sub> (<italic>s, d</italic>), where <italic>a</italic> = <italic>i</italic> or <italic>a</italic> = <italic>d</italic> denotes the possible actions (increase, decrease of angle) and <italic>s</italic> a state. <italic>Q</italic> values are obtained through function approximation: <disp-formula id="Equ1"><tex-math id="M1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\eqalign{ &amp; {Q_{{\alpha _1}}}(s,a) = {{\sum\limits_{k = 1}^N {\theta _{{\alpha _1}}^k(a){\Phi ^k}(s)} } \mathord{\left/ {\vphantom {{\sum\limits_{k = 1}^N {\theta _{{\alpha _1}}^k(a){\Phi ^k}(s)} } {\sum\limits_{k = 1}^N {{\Phi ^k}(s)} }}} \right. \kern-\nulldelimiterspace} {\sum\limits_{k = 1}^N {{\Phi ^k}(s)} }} \cr &amp; \Delta {\alpha _1}(s) = {Q_{{\alpha _1}}}(s,i) - {Q_{{\alpha _1}}}(s,d) \cr} $$
\end{document}</tex-math></disp-formula> where <italic>&#x3A6;</italic><sup><italic>k</italic></sup> (<italic>s</italic>) is the activation function of the <italic>k</italic>th kernel (either 0 or 1 in our case) in state <italic>s</italic>, <italic>&#x3D1;</italic><sub arrange="stack"><italic>&#x3B1;</italic>1</sub><sup arrange="stack"><italic>k</italic></sup> (<italic>a</italic>) are the weights from the <italic>k</italic>-th kernel to the motor units for the two possible actions, and <italic>N</italic> is the overall number or kernels in the system. Weights <italic>&#x3B8;</italic> are adapted through learning as described in the subsection on <italic>Q</italic> learning below.</p>
        <p>Next we calculate the difference <italic>&#x394;</italic><sub><italic>&#x3B1;</italic>1</sub> (<italic>s</italic>) used for continuous action formation: <disp-formula id="Equ2"><tex-math id="M2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\Delta {\alpha _1}(s) = {Q_{{\alpha _1}}}(s,i) - {Q_{{\alpha _1}}}(s,d)$$
\end{document}</tex-math></disp-formula> Movement direction <italic>D</italic> is given by the sign function <disp-formula id="Equ3"><tex-math id="M3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${D_\alpha }_{_1} = {\rm{sign}}({\Delta _\alpha }_{_1})$$
\end{document}</tex-math></disp-formula> and movement amplitude <italic>A</italic> by the absolute value normalized against the other components, which are calculated in the same way (now omitting <italic>s</italic> for simplicity): <disp-formula id="Equ4"><tex-math id="M4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${A_{{\alpha _1}}} = {{\left| {{\Delta _\alpha }_{_1}} \right|} \over {\sqrt {{\Delta _{\alpha _1^2}} + {\Delta _{\alpha _2^2}} + {\Delta _{\phi _1^2}} + {\Delta _{\phi _2^2}}} }}$$
\end{document}</tex-math></disp-formula> Normalization leads to a situation where the movement vector is formed proportional to the increase/decrease values, but the total length of the vector is kept fixed. This is done to preserve the topology of the space in which learning takes place.</p>
        <p>To cover a full sphere, normally the polar representation for azimuth is defined over 360&#xB0; and for elevation over 180&#xB0;. With the methods used here both, azimuth and elevation, are defined over 360&#xB0; leading to duplicate angular space and, hence, an over-representation. Yet it is unnecessary to introduce any compensatory space transformation as the learning overcomes the over-representation anyways. For the same reason, the action space is duplicate, too, but this has the nice aspect of allowing us to treat actions at the angular constraints in a continuous way avoiding having to deal with wrap-around problems. Noise of around 7% of the movement amplitude was added to each step. This imitates imperfections in the control of the angle in a real robot <italic>and</italic> introduces the required exploration component into our model. This way exploration stays &#x201C;near the path&#x201D; which leads to faster convergence than random exploration, which we had also tried but abandoned for this reason.</p>
        <p>Movement strategies with smoothing were employed. All new steps in angle space were made as a combination of the currently calculated steps combined with the previous steps: <disp-formula id="Equ5"><tex-math id="M5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\Delta _{{\rm{current}}}} = {(\Delta {\alpha _1},{\Delta _\alpha }_2,{\Delta _\phi }_1,{\Delta _\phi }_2)_{{\rm{current}}}}$$
\end{document}</tex-math></disp-formula><disp-formula id="Equ6"><tex-math id="M6">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\Delta _{{\rm{previous}}}} = {({\Delta _{\alpha 1}},{\Delta _{\alpha 2}},{\Delta _{\phi 1}},{\Delta _{\phi 2}})_{{\rm{previous}}}}$$
\end{document}</tex-math></disp-formula> using: <disp-formula id="Equ7"><tex-math id="M7">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${\Delta _{{\rm{final}}}} = c{\Delta _{{\rm{current}}}} + (1 - c){\Delta _{{\rm{previous}}}}$$
\end{document}</tex-math></disp-formula> where <italic>c</italic> = 0.6 was used. Smoothing helps to avoid jerky movements in the beginning of learning and also influences the process of learning, as in <italic>Q</italic> learning with function approximation, which we are using here, the convergence pattern is dependent on the paths performed during learning.</p>
        <p>Finally, if a newly calculated angle falls outwith the constraint boundaries, then random steps with uniform distribution in angle space are tried out until a point satisfying the constraints is obtained.</p>
      </sec>
      <sec id="Sec4">
        <title>2.2 Learning method</title>
        <p>From eight possible actions only four learn during a single step. These are the directions along which the actual movement has been taken. For example, if a particular angle has been increased, the unit, which has driven the increase, learns while the antagonistic &#x2018;decrease unit&#x2019; will not be changed. Furthermore, learning will affect connections of all currently active kernels. Again we will demonstrate the learning rule for components representing angle <italic>&#x3B1;</italic><sub>1</sub>.</p>
        <p>Weight update follows an algorithm similar to conventional <italic>Q</italic>-learning (<xref ref-type="bibr" rid="CR56">Watkins 1989</xref>; <xref ref-type="bibr" rid="CR57">Watkins and Dayan 1992</xref>). For each angle, two actions <italic>a</italic><sub><italic>i,d</italic></sub> are possible: increase or decrease. Let us say the current state is described by angles <italic>s</italic> = (<italic>&#x3B1;</italic><sub>1</sub>, <italic>&#x3D5;</italic><sub>1</sub>, <italic>&#x3B1;</italic><sub>2</sub>, <italic>&#x3D5;</italic><sub>2</sub>)<sub>current</sub> and some action <italic>a</italic> (increase or decrease) is chosen that leads to a state <italic>s</italic>&#x2032; = (<italic>&#x3B1;</italic><sub>1</sub>, <italic>&#x3D5;</italic><sub>1</sub>, <italic>&#x3B1;</italic><sub>2</sub>, <italic>&#x3D5;</italic><sub>2</sub>)<sub>next</sub>. In our learning framework, the change in the value <italic>&#x3B8;</italic><sub><italic>&#x3B1;</italic>1</sub> (<italic>a</italic>) of that state-action pair follows the mean across all activated kernels of <italic>s</italic>&#x2032;: <disp-formula id="Equ8"><tex-math id="M8">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\eqalign{ &amp; \theta _{{\alpha _1}}^k(a) = \theta _{{\alpha _1}}^k(a) + \mu [r + \gamma {Q_{{\alpha _1}}}(s',a) \cr &amp; - \theta _{{\alpha _1}}^k(a)]{\Phi ^k}(s) \cr &amp; {Q_{{\alpha _1}}}(s',a) = {{\sum\limits_{k = 1}^N {\theta _{{\alpha _1}}^k(a){\phi ^k}(s')} } \mathord{\left/ {\vphantom {{\sum\limits_{k = 1}^N {\theta _{{\alpha _1}}^k(a){\phi ^k}(s')} } {\sum\limits_{k = 1}^N {{\phi ^k}(s')} }}} \right. \kern-\nulldelimiterspace} {\sum\limits_{k = 1}^N {{\phi ^k}(s')} }} \cr} $$
\end{document}</tex-math></disp-formula> where <italic>k</italic> is the number of the kernel to which the weight is associated, <italic>r</italic> the reward, <italic>&#x3BC;</italic> &lt; 1 the learning rate, <italic>&#x3B3;</italic> &lt; 1 the discount factor, <italic>&#x3A6;</italic><sup><italic>k</italic></sup> (<italic>s</italic>) the activity function for kernel <italic>k</italic> in state s, <italic>Q</italic><sub><italic>&#x3B1;</italic>1</sub> (<italic>s&#x2032;, a</italic>) the <italic>Q</italic> value of the system in the next state, for action <italic>a</italic> defined as: <disp-formula id="Equ9"><tex-math id="M9">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$${Q_{{\alpha _1}}}(s',a) = {{\sum\limits_{k = 1}^N {\theta _{{\alpha _1}}^k(a){\Phi ^k}(s')} } \mathord{\left/ {\vphantom {{\sum\limits_{k = 1}^N {\theta _{{\alpha _1}}^k(a){\Phi ^k}(s')} } {\sum\limits_{k = 1}^N {{\Phi ^k}(s')} }}} \right. \kern-\nulldelimiterspace} {\sum\limits_{k = 1}^N {{\Phi ^k}(s')} }}$$
\end{document}</tex-math></disp-formula> Equivalent expressions are used for updating <italic>Q</italic> values for the angles <italic>&#x3D5;</italic><sub>1</sub>, <italic>&#x3B1;</italic><sub>2</sub> and <italic>&#x3D5;</italic><sub>2</sub>. Throughout this study we use <italic>&#x3BC;</italic> = 0.7 and <italic>&#x3B3;</italic> = 0.7. This rule is called averaging function approximation rule and is considered to perform more stably (<xref ref-type="bibr" rid="CR37">Reynolds 2002</xref>) in function approximation schemes as compared to standard methods (e.g. see <xref ref-type="bibr" rid="CR46">Sutton and Barto 1998</xref>).</p>
        <p>Note, the algorithm is not so easy to attribute to standard <italic>Q</italic>- or to SARSA-learning. It is neither SARSA, because it does not take into account where the system has gone in the next step; nor is it <italic>Q</italic>-learning, as it does not consider which action is best in the next step. The algorithm rather considers if continuing along the current direction is valuable according to the <italic>Q</italic>-values of the next state <italic>Q</italic><sub><italic>&#x3B1;</italic>1</sub> (<italic>s&#x2032;, a</italic>) and attempts to straighten the path.</p>
        <p>Also <italic>Q</italic>-learning and SARSA-learning can be implemented onto the same function approximation scheme, for which we will give the equations in the following. For <italic>Q</italic>-learning we use: <disp-formula id="Equ10"><tex-math id="M10">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\eqalign{ &amp; \theta _{{\alpha _1}}^k(a) = \theta _{{\alpha _1}}^k(a) + \mu [r + \gamma \mathop {\max }\limits_{a,\beta } {Q_\beta }(s',a) \cr &amp; - \theta _{{\alpha _1}}^k(a)]{\Phi ^k}(s) \cr} $$
\end{document}</tex-math></disp-formula> where maximum operation runs over all actions <italic>a</italic> and all angles (<italic>&#x3B1;</italic><sub>1</sub>, <italic>&#x3B1;</italic><sub>2</sub>, <italic>&#x3D5;</italic><sub>1</sub>, <italic>&#x3D5;</italic><sub>2</sub>) here subsumed under the index <italic>&#x3B2;</italic>, finding the biggest value from all possible action descriptors associated with the state <italic>s</italic>. For SARSA learning we use <disp-formula id="Equ11"><tex-math id="M11">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$\eqalign{ &amp; \theta _{{\alpha _1}}^k(a) = \theta _{{\alpha _1}}^k(a) + \mu [r + \gamma M(s',a') \cr &amp; - \theta _{{\alpha _1}}^k(a)]{\Phi ^k}(s) \cr &amp; M(s',a') = \sqrt {\sum\limits_\beta {{Q_\beta }{{(s',a')}^2}} } \cr} $$
\end{document}</tex-math></disp-formula> where <italic>M</italic>(<italic>s&#x2032;, a&#x2032;</italic>) is defined by the magnitudes of the action components of the next performed action: <disp-formula id="Equ12"><tex-math id="M12">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$M(s',a') = \sqrt {\sum\limits_\beta {{Q_\beta }{{(s',a')}^2}} } $$
\end{document}</tex-math></disp-formula> where <italic>&#x3B2;</italic> runs over all four angles.</p>
        <p>Finally, to reduce computing time, we have introduced a variable path-length limit. If the arm has not reached the target within this limit, it is reset to the start and all changes in <italic>Q</italic>-values learned during this trial are undone. For a new experiment, path length limit starts with <italic>l</italic>(1) = 1,000, and further develops according to the following recurrent equation: <disp-formula id="Equ13"><tex-math id="M13">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$l(n + 1) = \left\{ {\matrix{ {2.5*k(n){\rm{ if goal obtained in }}{n^{{\rm{th}}}}{\rm{ trial}}} \hfill \cr {l(n) + 20{\rm{ if goal not obtained in }}{n^{{\rm{th}}}}{\rm{ trial}}} \hfill \cr } } \right.$$
\end{document}</tex-math></disp-formula>. where <italic>k</italic>(<italic>n</italic>) is the number of steps to goal in trial <italic>n</italic>. That is, the limit comparable to the previous path to the goal is imposed once the goal has been obtained, but if the goal is not obtained in some trial, then the limit is relaxed by 20 units.</p>
        <sec id="Sec5">
          <title>2.2.1 Reward structure</title>
          <p>Different rewards have been used, alone and in combination, in this study (Fig. <xref rid="Fig3" ref-type="fig">3</xref>) normalized to a maximal reward of 1. Highest reward is (usually) obtained on touching the target (Fig. <xref rid="Fig3" ref-type="fig">3a</xref>), but we also define a vision-based reward structure (Fig. <xref rid="Fig3" ref-type="fig">3b</xref>). For this, we assume that the distance between tip of the arm and target can be evaluated. Distance dependent reward is then defined linearly along this distance. Different combinations of distant dependent and touch rewards are shown in panels c1 and c2 of Fig. <xref rid="Fig3" ref-type="fig">3</xref>. Panel d shows an approach distance (differential) dependent reward structure were reward is only given if the arm has approached the target, while reward is generally zero if the arm has moved away. These reward structures are using absolute distance <italic>x</italic> to define the reward. By contrast, we have also used a differential, relative reward structure (Fig. <xref rid="Fig3" ref-type="fig">3e</xref>). For this we first normalize the distance to target at the start of an experiment to one. Then reward is given according to the <italic>relative</italic> approach <italic>d</italic>. Hence a larger step towards target was rewarded more than a smaller one, where absolute distance does not play any role. In the following, we will always refer to the different rewards used in individual experiments using the panel labels from Fig. <xref rid="Fig3" ref-type="fig">3</xref>.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Schematic representation of different types of rewards used in this study. The <italic>bar</italic> in the middle of the <italic>x</italic>-axis represents the target location. Reward can be given only there <bold>a</bold> or on approach <bold>b</bold> or combining approach with touch (<bold>c</bold>1, <bold>c</bold>2). <bold>d</bold> depicts a situation where reward is given on approach (<italic>top</italic>) but not on leaving (<italic>bottom</italic>, see direction of the <italic>arrows</italic>). <bold>e</bold> Shows a purely differential reward calculated by how far the arm has moved forward to the target</p></caption><graphic position="anchor" xlink:href="422_2009_Article_295_Fig3" id="MO3"/></fig></p>
          <p>Furthermore, we also introduced punishments (negative rewards) for approaching the constraint boundaries. For this, we were increasing punishment linearly, starting with zero at a distance of 1/20th of the field width to the border up to a certain maximal value reached when touching the border. Maximal values were changed in different experiments and details will be given below.</p>
        </sec>
      </sec>
    </sec>
    <sec id="Sec6" sec-type="results">
      <title>3 Results</title>
      <sec id="Sec7">
        <title>3.1 Individual convergence patterns</title>
        <p>The main aim of the paper is to demonstrate condensed statistical results that help comparing convergence of learning processes using various reward structures and other learning parameters. However, before presenting those statistics, we will show examples of individual training sessions to provide a better intuition about the behavior of this system. In Fig. <xref rid="Fig4" ref-type="fig">4</xref> five qualitatively different cases of system development are shown: (a) quick convergence, (b) delayed convergence, (c) intermittent episode of divergence, (d) bad convergence; and (e) the persistently falling back onto a wrong trajectory, when the optimal trajectory has already been learned. The bars shown in black demonstrate successful runs; the bars shown in gray are for unsuccessful runs, where the limit for the path length was exhausted before the goal was reached. For these experiments, a D type reward (Fig. <xref rid="Fig3" ref-type="fig">3</xref>) has been used. It is of interest to make a practical remark here: cases shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref> panels c and e are largely irrelevant from a practical point of view as any robot learning system of this kind would either have to be controlled by an operator or&#x2014;at least&#x2014;would require a stopping criterion, which tells the machine to stop as soon as it has found a reasonably good solution. In both cases the longer trajectories, which occur later in (c) or in an interspersed way in (e), would just be ignored (or not even be recorded). Note, Fig. <xref rid="Fig4" ref-type="fig">4</xref> showed five archetypical examples.Many times transitions between them exist, which cannot easily be distinguished. As a consequence, from now on we will not separate these cases anymore and perform statistical analyses always across whole data sets. In the following for every experiment we train the system for 20 trials and then for another 100 trials we will determine the average trajectory length and the percentage how often the system has reached the goal. As this is indicative of the average convergence property of a certain setup, it will allow us to compare strategies. In general we repeat every experiment 50 times.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Different example runs. <bold>a</bold> Quick convergence to a good trajectory within about 22 trials. <bold>b</bold> Delayed convergence (&#x2248;50 trials). <bold>c</bold> Quick convergence in less than 20 trials and thereafter some divergence and re-convergence to a different trajectory. Note from a practical perspective the robot would just stop learning after about trial 20 and the second phase is not of relevance. <bold>d</bold> Badly convergent case. <bold>e</bold> Good convergence but with interspersed other, longer trajectories (<italic>light shading</italic>). Also here the robot would normally stop learning after having found the better trajectory and ignore the others</p></caption><graphic position="anchor" xlink:href="422_2009_Article_295_Fig4" id="MO4"/></fig></p>
        <p>Observations show that for best learning regimes divergent cases never occur, but various intermittent phenomena, or delayed convergence were present. For bad regimes about three to five divergence cases happen during 50 experiments, and quick and clearly convergent cases are relatively rare (e.g. 15 out of 50), the biggest proportion being taken by delayed convergence or by cases with intermittent behaviors.</p>
      </sec>
      <sec id="Sec8">
        <title>3.2 Comparison of learning modes</title>
        <p>In the simplest case reinforcement is only given when the target is actually hit (reward type A). Note, this would correspond to a task of finding an unknown object in the dark. As touching is a singular, rare event, this leads to a pronounced temporal credit assignment problem for large state-action spaces such as here. Thus, this case will only be considered for comparison reasons and not analyzed any further.</p>
        <p>Of much more relevance for robotic applications is the case where visual information is used to position the arm (visual servoing) and this can be encoded by a distance dependent reward structure (reward type B).On touching additional reward may be given (reward type C). Furthermore, it makes sense to avoid giving a distance dependent reward when the arm has actuallymoved away from the target (reward type D). First, experiments to compare influence of these three different basic reward structures were performed. In Fig. <xref rid="Fig5" ref-type="fig">5a</xref> the average number of steps to target is shown and in (b) the percentage of cases reaching the goal is presented. The bars show the standard deviation of the distributions. We observe that in the case where vision is excluded the trajectories of the arm are around ten times longer as compared to cases with vision. Standard deviation is 218 steps, and is clipped to preserve the scale of the figure. Similarly, the percentage of attaining the goal without vision is only around 50%. This happens because in 3D space the target only takes a small part of the total space and it is not easy to hit it by chance. Furthermore, due to the singular reward structure, kernels with non-zero <italic>Q</italic>-values after around 100 runs are still too sparse to give clear indications about preferred trajectories in the 4D angle space. Runs for distance rewards (middle column) are substantially longer and the percentage of successful trials is smaller, as compared to the case that includes the approach distant dependent component (right column). Thus, in the following, we will focus our attention on cases with a approach distant dependent reward component, as these have noticeable better convergence properties compared to the others.
<fig id="Fig5"><label>Fig. 5</label><caption><p>Statistics for different reward types. <bold>a, b</bold> Two distance dependent reward types (<italic>right</italic>) compared to touch-only reward (<italic>left</italic>). Annotations at the bottom refer to the annotations introduced in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. <bold>c, d</bold> Comparison of different slopes for distance dependent rewards. For <bold>c, d</bold> slope values from left to right are: 1/3 (no touch reward case), 1/3, 1/4, 1/5, 1/10, 1/20, 1/50, reaching values at the goal boundary correspondingly: 0.86, 0.65, 0.52, 0.26, 0.13, 0.05. Touch reward is 1</p></caption><graphic position="anchor" xlink:href="422_2009_Article_295_Fig5" id="MO5"/></fig></p>
        <p>Next we analyze the steepness of the distance dependent reward curve in Fig. <xref rid="Fig5" ref-type="fig">5c, d</xref>. Reward for reaching the target was kept at one and the slope of the distant dependent reward was varied from 0.86 (almost reaching one, the value of the touch reward) at the border of the target for the steepest slope to down to 0.05 for the shallowest slope. One can observe that for steep slopes (c, d left) values are quite similar and better than for shallow slopes. The first column shows the limiting case when no special reward for touching was applied. Here the entire process of approaching the target was regulated using exclusively distance-dependent rewards. This case in terms of convergence appears among the best cases, but step length was longer than for some of the mixed cases. Thus, for further experiments we were choosing slope steepness 0.25, which seems a good compromise. Using this steepness parameter, more reward structures were explored in Fig. <xref rid="Fig6" ref-type="fig">6a</xref>, b. In the columns from left to right, we compare (1) differential reward (type E), (2) approach distance (type D), (3) a variant of type D, where we always give the same amount of reward <italic>r</italic> = 0.3, and (4) type D but applying punishment of the same size as reward on approach would be, if the action leads away from the goal by more than a certain distance. In general results are very similar where the rightmost column is slightly better than the others.
<fig id="Fig6"><label>Fig. 6</label><caption><p>Statistics for other types of rewards. <bold>a, b</bold> As per annotation at the bottom (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>). <bold>c, d</bold> Reward Type D is combined with punishment on approaching the maximally allowed joint angles. Note <italic>light gray shaded</italic> cases are identical across panels</p></caption><graphic position="anchor" xlink:href="422_2009_Article_295_Fig6" id="MO6"/></fig></p>
        <p>When applying distant dependent rewards, the problem arises that the joint might be drawn to go towards some direction, though this movement is not allowed due to physical limitations of the joint construction. For example here the joints were not allowed to cross the border of 0&#x2013;360&#xB0;. The position and size of the reward was chosen such that again and again learning tried to violate these constraints. This problem can be mitigated best by applying punishment also at the constraints as described in the methods section. This additional mechanism leads to a noticeable improvement of convergence times (Fig. <xref rid="Fig6" ref-type="fig">6c</xref>) and a much higher percentage of convergent cases (Fig. <xref rid="Fig6" ref-type="fig">6d</xref>).</p>
        <p>In the columns from left to right in Fig. <xref rid="Fig6" ref-type="fig">6c</xref>, d maximal punishment is 1, 5, 20, 35, 50, 75 and 100. The rightmost column shows a control experiment without punishment. Reward structure was the same as in panels A,B column 2. Hence gray shaded columns in panels a, b are the same as those in c, d.</p>
        <p>One can observe that punishment at constraints in general improves rate and speed of convergence. Performance of learning first increases with increasing punishment values and approaches an optimum around a punishment of 35&#x2013;75 and then slightly drops again. Of importance, however, is that punishment at constraint works rather reliably over a large parameter range <italic>and</italic> that the variances have been drastically reduced. Hence quick convergence became rather reliable with this mechanism.</p>
        <p>Combining constraint-based punishment, specifically employing the case with the maximal punishment value 35, with punishment for going away from the reward, as described before (column 4 in panels A,B), did not provide better results, remaining in the range of an average of 13&#x2013;14 steps to the goal with around 90% of successful trials.</p>
        <p>The system was also tested with changing the position of the goal, and reverting positions of start and goal, to exclude possibilities of some special circumstances which were only providing good performance for one specific task. Trials with changed position were providing similar average trajectory length, and success percentage results.</p>
      </sec>
      <sec id="Sec9">
        <title>3.3 Comparison of methods</title>
        <sec id="Sec10">
          <title>3.3.1 Comparing different update methods using the same function approximation</title>
          <p>In the following section, we will use always the same function approximation method and compare different update algorithms like <italic>Q</italic>-learning and SARSA-learning with our path straightening PS as well as with a fully discounted version of the algorithms (Fig. <xref rid="Fig7" ref-type="fig">7</xref>). In general, all methods produce similar results where the path straightening method is only non-significantly better than the others. Hence, while reward structure and function approximation methods are clearly influencing the performance in a critical way, the choice of <italic>Q</italic>-value update method does less so. &#x201C;Fully-discounted&#x201D; refers to a simulation where <italic>&#x3B3;</italic> = 0 has been used. This case also produces similar results arguing that it would be possible for this problem to reduce the learning problem and even operate with fully-discounted rewards. Note, all update equations (<italic>Q</italic>, SARSA, PS) become the same when using <italic>&#x3B3;</italic> = 0.Care, however, has to be taken in this case that there are no states with zero or indistinguishable reward, where a fully discounted method would fail. As this can in general not be assured, a fully discounted procedure should not be adopted. In general, the conclusion of this subsection is that the actual choice of the update method is not critical as long as an appropriate function approximation as well as reward structure is being used.
<fig id="Fig7"><label>Fig. 7</label><caption><p>Statistics for other types of learning methods. PS = path straightening (our method). We used reward type D, no punishment, discount factor <italic>&#x3B3;</italic> = 0.7 for all cases except case &#x201C;fully discounted&#x201D;, where we have <italic>&#x3B3;</italic> = 0</p></caption><graphic position="anchor" xlink:href="422_2009_Article_295_Fig7" id="MO7"/></fig></p>
        </sec>
        <sec id="Sec11">
          <title>3.3.2 Comparing to natural actor critic reinforcement learning</title>
          <p><xref ref-type="bibr" rid="CR31">Peters and Schaal (2006b</xref>, <xref ref-type="bibr" rid="CR32">2007</xref>, <xref ref-type="bibr" rid="CR33">2008</xref>) use policy gradient methods for robotic applications. These form a different class of RL methods as compared to the one analyzed in this study, as the policy is updated not in every step, but only after several trajectories have been produced. Also no representation of state is being considered, but actions are described directly. As currently <italic>natural actor-critic RL</italic> is often considered to be the method of choice in robotics we made a comparison of our method to the version of the episodic natural-actor critic with time-variant baseline for learning the same reaching task, following <xref ref-type="bibr" rid="CR33">Peters and Schaal (2008)</xref>. We have made a simplifying assumption that the best path to goal is straight. We were describing the trajectory by a difference equation: <disp-formula id="Equ14"><tex-math id="M14">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{mathrsfs}
\usepackage{upgreek}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$$s(t + 1) = s(t) + u(t)$$
\end{document}</tex-math></disp-formula> where state vector <italic>s</italic>(<italic>t</italic>)=[<italic>&#x3B1;</italic><sub>1</sub>(<italic>t</italic>), <italic>&#x3D5;</italic><sub>1</sub>(<italic>t</italic>), <italic>&#x3B1;</italic><sub>2</sub>(<italic>t</italic>), <italic>&#x3D5;</italic><sub>2</sub>(<italic>t</italic>)}]<sup><italic>T</italic></sup> includes the analyzed angles, and <italic>u</italic>(<italic>t</italic>) is a 4D control signal. We used the following normal distribution <italic>N</italic>(&#xB7;) for the control signal: <italic>u</italic>(<italic>t</italic>) &#x223C; <italic>N</italic>(&#x3B8;, &#x3C3;}<sup>2</sup><italic>I</italic>), where <italic>&#x3B8;</italic> is a 4D vector of control averages and <italic>I</italic> is the 4 &#xD7; 4 unit matrix. The value of <italic>&#x3C3;</italic> determines the variability (&#x201C;exploration&#x201D;) in the different trajectories.</p>
          <p>Reward was provided under reward structure C (Fig. <xref rid="Fig3" ref-type="fig">3</xref>), where in each step the distance to the target was evaluated and a bigger reward was given on touching the target. We did not use the (D)-reward structure (Fig. <xref rid="Fig3" ref-type="fig">3</xref>) preferred in our on-line method, as the preliminary assumption for the implemented natural actor-critic was a straight trajectory and the (D)-structure is only meaningful when curved trajectories are allowed. We used a discount factor of <italic>&#x3B3;</italic> = 0.95, to emphasize rewards that are obtained earlier. If the trajectory was leaving the allowed angle range (0, 360&#x3B3;), negative rewards of the size of &#x2212;1 were given. Each trajectory was composed of 15 steps of the same average length as for our on-line learning method. We always used the same total trajectory length (that is, we did not stop the trajectory after a touching event had happened) and were giving maximum reward several times, if the trajectory stayed in the volume of the object for several steps.</p>
          <p>We used the natural actor-critic algorithm with timevariant baseline to optimize <italic>&#x3B8;</italic> and <italic>&#x3C3;</italic>. An experiment was made using the same initial position of the arm, as well as the same position of the ball as in our on-line learning experiments. We were using hand-tuned parameters, as proposed by <xref ref-type="bibr" rid="CR33">Peters and Schaal (2008)</xref>. Learning for this type of algorithm is divided into &#x201C;epochs&#x201D;. Every epoch consists of 20 trajectories after which the gradient needed for the RL-update could be calculated. One learning &#x201C;trial&#x201D; consists of <italic>n</italic> epochs, where we usually chose <italic>n</italic> = 50 leading to 1,000 executed trajectories. Step size for the gradient calculation was tuned to obtain quick convergence and at the same time relatively stable optimization performance. Using less than 20 trajectories in a single epoch to evaluate the gradient made the learning process unstable. Initial values for <italic>&#x3B8;</italic> were chosen randomly from the interval [&#x2212;1, 1] with uniform distribution.</p>
          <p>At first we observed that usually one learning trial (50&#xD7;20 traj.) was enough for the algorithm to lead to a good final trajectory. In Fig. <xref rid="Fig8" ref-type="fig">8a</xref> an example of a well convergent trial is shown, while in Fig. <xref rid="Fig8" ref-type="fig">8b</xref> we show an example with delayed convergence. This shows that (similar to our algorithm) convergence varies and, thus, we performed 100 learning trials for a statistical analysis (100 &#xD7; 50 &#xD7; 20 traj.) were we used 100 different initial values for the <italic>&#x3B8;</italic>-vector. From this statistics, in Fig. <xref rid="Fig8" ref-type="fig">8c</xref> the average number of successful reaches obtained at a certain epoch is shown. The marked crossing point (<italic>x</italic> = 16.48, <italic>y</italic> = 15) is of interest as this represents 15 successful reaches after on average 16.48 epochs of here amounting to about 329.54 performed trajectories. By contrast, in our algorithm we get 15 successful reaches after about 20 trajectories (as we do not have to use epochs, hence, for us every &#x201C;epoch&#x201D; is just one single trajectory). However, the intermediate trajectories may be longer (curved) in our case.
<fig id="Fig8"><label>Fig. 8</label><caption><p>Learning curves for natural actor-critic: <bold>a</bold> good convergence example, <bold>b</bold> delayed convergence example. <bold>c</bold> average number of successful trials to reach an object</p></caption><graphic position="anchor" xlink:href="422_2009_Article_295_Fig8" id="MO8"/></fig></p>
        </sec>
      </sec>
    </sec>
    <sec id="Sec12" sec-type="discussion">
      <title>4 Discussion</title>
      <p>In this paper, we have developed a novel action approximation scheme for reinforcement learning, and implemented three methods: path straightening, <italic>Q</italic>-learning and SARSA -learning based on this scheme. This kernel based approach for value function approximation was inspired by the place-field system in the hippocampus of rodents (<xref ref-type="bibr" rid="CR51">Tamosiunaite et al. 2008</xref>). Our focus lay on a comparison of different reward structures and we could show that a combination of distance dependent rewards with constraint punishment and extra reward on touching will lead to a fast convergence to &#x201C;a good&#x201D; trajectory.</p>
      <p>A central problem for the use of RL in large state-action spaces is that convergence of conventional methods will take far too long. Thus, value function approximation is required for which a wide variety of methods exists. Alas, only few of them have proven convergence properties (<xref ref-type="bibr" rid="CR46">Sutton and Barto 1998</xref>; <xref ref-type="bibr" rid="CR14">Gordon 2001</xref>; <xref ref-type="bibr" rid="CR37">Reynolds 2002</xref>; <xref ref-type="bibr" rid="CR48">Szepesvari and Smart 2004</xref>) and those are in praxis many times not the fastest ones. Hence, the field of applied robotics is largely uninterested in the mathematical rigor of a given method as long as the trade of is reasonable between goodness of solution and time to find it. From a practical perspective, on any humanoid robot learning will be stopped by force as soon as a predefined variable, which handles this trade-off, approaches a desired threshold.</p>
      <p>The control of a multi-joint arm poses a terrific problem for RL methods as the state-action spaces are in general very high, needing to cover a space-time continuous system within which redundant solutions exist. While a very large number of papers exist on space-time continuous low-dimensional problems (e.g. <xref ref-type="bibr" rid="CR34">Qiang et al. 2000</xref>; <xref ref-type="bibr" rid="CR15">Gross et al. 1998</xref>; <xref ref-type="bibr" rid="CR13">Gaskett et al. 2000</xref>), only few articles try, to tackle this problem with a reasonable degree of complexity in a high dimensional system (<xref ref-type="bibr" rid="CR29">Perez and Cook 2004</xref>; <xref ref-type="bibr" rid="CR32">Peters and Schaal 2007</xref>, <xref ref-type="bibr" rid="CR33">2008</xref>). Also many times action spaces of the problems solved are discrete or quite limited (<xref ref-type="bibr" rid="CR9">Enokida et al. 1999</xref>; <xref ref-type="bibr" rid="CR54">van Hasselt and Wiering 2007</xref>; <xref ref-type="bibr" rid="CR25">Martinez-Marin and Duckett 2004</xref>) and, even if continuous state representations are used, actions are kept discrete in continuous task approximations (<xref ref-type="bibr" rid="CR9">Enokida et al. 1999</xref>; <xref ref-type="bibr" rid="CR24">Li et al. 2006</xref>). This is possible when action spaces are small, but with bigger action spaces (4D in our case) the usage of discrete actions becomes impractical. If continuous actions are to be used, often methods for the interpolation of discrete samples are employed relying on weighted sums or wire fitting (<xref ref-type="bibr" rid="CR13">Gaskett et al. 2000</xref>; <xref ref-type="bibr" rid="CR50">Takeda et al. 2001</xref>; <xref ref-type="bibr" rid="CR54">van Hasselt and Wiering 2007</xref>). Instead, here we are offering a generic approach to continuous action formation, where reinforcement learning is implemented on the features defining continuous actions.</p>
      <p>A recent and quite successful contribution to RL in robotics concerns the so-called natural-actor critic method (<xref ref-type="bibr" rid="CR31">Peters and Schaal 2006b</xref>, <xref ref-type="bibr" rid="CR32">2007</xref>, <xref ref-type="bibr" rid="CR33">2008</xref>). This algorithm belongs to the class of policy gradient methods (<xref ref-type="bibr" rid="CR59">Williams 1992</xref>; <xref ref-type="bibr" rid="CR4">Baxter and Bartlett 2000</xref>; <xref ref-type="bibr" rid="CR47">Sutton et al. 2000</xref>), which are structurally quite different from the one analyzed here (see Sect. <xref rid="Sec11" ref-type="sec">3.3.2</xref> above for details). Policy gradient methods work directly with action representations, and thus need much less parameters than on-line RL methods. Also one can choose appropriate representation of the initial trajectory. The advantageous feature of the implemented natural actor critic method, to our observation, is that the convergence performance does not decrease too dramatically with rising dimensionality of the problem, what is very important in robotic applications. Yet the natural actor-critic in our implementation was quite sensitive to parameter choices, and the number of trajectories required to obtain a good policy was one order of magnitude larger than for our on-line method. The inventors of the natural actor critic method originally implemented it in a different context. They start with a very close to target trajectory and attempt to improve it, while here the method was applied for learning from scratch, <italic>without</italic> any &#x201C;close to target&#x201D; initial trajectory. Although formulation of robot learning tasks as in (<xref ref-type="bibr" rid="CR30">Peters and Schaal 2006a</xref>, <xref ref-type="bibr" rid="CR33">2008</xref>), with a very close to optimal initial trajectory obtained from demonstration is very appealing, tasks exist (especially for when there is a large mismatch in embodiment between human and robot) where there is no possibility to obtain a (human-) demonstration, and our current experiments indicate that on-line learning methods, like the method proposed here, may be more efficient in such &#x201C;learning from scratch&#x201D; tasks.</p>
      <p>Another theoretical development of the current study is the analysis of different reward structures for RL in visual servoing tasks. Robotic applications tend to be multidimensional, and if a reward is provided only at the goal, then additional means to guide a robot to the target are required, like &#x2018;learning from easy missions&#x2019; (hence, bootstrapping the system with simple tasks) or using simpler controllers (like teachers, <xref ref-type="bibr" rid="CR25">Martinez-Marin and Duckett 2004</xref>), otherwise learning times would be impractically long. In image based visual servoing (IBVS) systems, richer reward structures were many times used, attempting to achieve convergence in bigger state spaces (<xref ref-type="bibr" rid="CR23">Leonard and Jagersand 2004</xref>; <xref ref-type="bibr" rid="CR29">Perez and Cook 2004</xref>). We analyzed and compared about half a dozen of vision-based reward structures and studies addressing applications of visual servoing could gain from that comparison using the structures that performed best according to our modeling. Although we are investigating these structures in the framework of position based visual servoing (PBVS), similar ones could be applied to IBVS, using visual error instead of distance. Possibly some adaptations of the method we used here would be required, as mapping from distance to visual features is rather complex.</p>
      <p>Of specific interest is that our formalism is very simple and can still handle large state-action spaces in a moderately high-dimensional problem. Currently, this method is being implemented on ARMAR III, a humanoid robot built by the University of Karlsruhe (<xref ref-type="bibr" rid="CR3">Asfour et al. 2006</xref>). Each arm has seven degrees of freedom: three DOF in the shoulder, two DOF in the elbow and two DOF in the wrist. For this purpose, a control schema of the arm is realized which makes use of hypothesis and results from neurophysiology on human movement control (<xref ref-type="bibr" rid="CR41">Soechting and Flanders 1989a</xref>,<xref ref-type="bibr" rid="CR42">b</xref>). Soechting and Flanders have shown that arm movements are planned in shoulder-centered spherical coordinates and suggest a sensorimotor transformation model that maps the wrist position on a natural arm posture using a set of four parameters, which are the elevation and azimuth (yaw) of both the upperarm and forearm similar to the problem addressed in the current paper. Once these parameters are obtained for a given position of the wrist, they are mapped on the joint angles of the shoulder and elbow of the robot arm, which completely define the wrist position. For more details the reader is referred to (<xref ref-type="bibr" rid="CR2">Asfour and Dillmann 2003</xref>). In the Introduction we had mentioned the task of recalibrating an agents movement, which is the task to be performed by ARMAR III. The agent holds a container full of liquid and needs to learn filling another glass standing in front. Figure <xref rid="Fig9" ref-type="fig">9</xref> shows a simulation of this task and the agent&#x2019;s performance. A full description of this simulation and its modeled physics goes beyond the scope of this paper. This figure, however, is meant to show the efficiency of our new method in a task which is difficult to achieve by methods other than reinforcement learning and we note that already after three trials (a) the agent has learned the task to more than 50%. In panel (b) one sees that the agent moves around over the glass and keeps on pouring liquid until its container is empty. Panel (c) shows the vector field of <italic>Q</italic>-values, which point towards the center of the glass. This experiment carries a natural reward structure (liquid filled) and is, thus, well-adapted to the assumptions of RL and the methods developed here allow solving this task with very few trials only. In the central part of this paper we deal, however, with position based visual servoing (<xref ref-type="bibr" rid="CR6">Chaumette and Hutchinson 2007a</xref>), assuming that the inverse kinematics is not known and that the arm has to learn to target an object without this knowledge. This task had been chosen as it is much harder than the glass-filling problem and we wanted to show that our methods still reach quite a high performance in order to raise confidence in this approach.
<fig id="Fig9"><label>Fig. 9</label><caption><p>Example of a glass-filling RL task as mentioned in the Introduction, where the agent learns to recalibrate a prior learned target position. The simulated agent is supposed to learn approaching a glass (<italic>circles</italic>) to optimally pour liquid into it. The reward is defined as the amount of liquid filled into the glass. The agent starts exploring from a location close to the glass, reached for example by plain visual servoing or by learning from demonstration. <bold>a</bold> Return versus number of trials. <bold>b</bold> Example trajectory. <bold>c</bold><italic>Q</italic>-vector field after 20 trials</p></caption><graphic position="anchor" xlink:href="422_2009_Article_295_Fig9" id="MO9"/></fig></p>
      <p>In summary, our method is meant to provide a solution with some practical relevance for robotics, but, given the kernel structure, it is also possible to combine it with a newly developed biophysical framework for implementing <italic>Q</italic>- or SARSA learning by an LTP/LDT based differential Hebbian framework (<xref ref-type="bibr" rid="CR60">W&#xF6;rg&#xF6;tter and Porr 2005</xref>;<xref ref-type="bibr" rid="CR22">Kolodziejski et al. 2008</xref>). Hence, from a biophysical point of view receptive field based (i.e., kernel based) function approximation could indeed operate in a Hebbian network emulating not just correlation based unsupervised, but also reward based reinforcement learning.</p>
    </sec>
  </body>
  <back>
    <ack>
      <p content-type="acknowledgment">The authors acknowledge the support of the European Commission, IP-Project &#x201C;PACO-PLUS&#x201D; (IST-FP6-IP-027657). We are grateful to T. Kulvicius for help with the RL methods and to Ales Ude for very fruitful discussions.</p>
      <p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.</p>
    </ack>
    <ref-list id="Bib1">
      <title>References</title>
      <ref id="CR1">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Arleo</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Gerstner</surname>
              <given-names>W</given-names>
            </name>
          </person-group>
          <article-title>Spatial cognition and neuro-mimetic navigation: a model of hippocampal place cell activity</article-title>
          <source>Biol Cyber</source>
          <year>2000</year>
          <volume>83</volume>
          <issue>3</issue>
          <fpage>287</fpage>
          <lpage>299</lpage>
          <pub-id pub-id-type="doi">10.1007/s004220000171</pub-id>
        </citation>
        <citation citation-type="display-unstructured">Arleo A, Gerstner W (2000) Spatial cognition and neuro-mimetic navigation: a model of hippocampal place cell activity. Biol Cyber 83(3): 287&#x2013;299 </citation>
      </ref>
      <ref id="CR2">
        <citation citation-type="other">Asfour T, Dillmann R (2003) Human-like motion of a humanoid robot arm based on a closed-form solution of the inverse kinematics problem. In: IEEE/RSJ international conference on intelligent robots and systems</citation>
      </ref>
      <ref id="CR3">
        <citation citation-type="other">Asfour T, Regenstein K, Azad P, Schr&#xF6;der J, Vahrenkamp N, Dillmann R (2006) ARMAR-III: an integrated humanoid platform for sensory-motor control. In: IEEE/RAS International conference on humanoid robots</citation>
      </ref>
      <ref id="CR4">
        <citation citation-type="other">Baxter J, Bartlett PL (2000) Direct gradient-based reinforcement learning. In: Proceedings of the ISCAS, Geneva, vol 3, pp 271&#x2013;74</citation>
      </ref>
      <ref id="CR5">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Breazeal</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Scassellati</surname>
              <given-names>B</given-names>
            </name>
          </person-group>
          <article-title>Robots that imitate humans</article-title>
          <source>TICS</source>
          <year>2008</year>
          <volume>6</volume>
          <issue>11</issue>
          <fpage>481</fpage>
          <lpage>487</lpage>
        </citation>
        <citation citation-type="display-unstructured">Breazeal C, Scassellati B (2008) Robots that imitate humans. TICS 6(11): 481&#x2013;487 </citation>
      </ref>
      <ref id="CR6">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chaumette</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Hutchinson</surname>
              <given-names>S</given-names>
            </name>
          </person-group>
          <article-title>Visual servo control, part i: basic approaches</article-title>
          <source>IEEE Robot Autom Mag</source>
          <year>2007</year>
          <volume>13</volume>
          <issue>4</issue>
          <fpage>82</fpage>
          <lpage>90</lpage>
        </citation>
        <citation citation-type="display-unstructured">Chaumette F, Hutchinson S (2007a) Visual servo control, part i: basic approaches. IEEE Robot Autom Mag 13(4): 82&#x2013;90 </citation>
      </ref>
      <ref id="CR7">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chaumette</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Hutchinson</surname>
              <given-names>S</given-names>
            </name>
          </person-group>
          <article-title>Visual servo control, part ii: advanced approaches</article-title>
          <source>IEEE Robot Autom Mag</source>
          <year>2007</year>
          <volume>14</volume>
          <issue>1</issue>
          <fpage>109</fpage>
          <lpage>118</lpage>
          <pub-id pub-id-type="doi">10.1109/MRA.2007.339609</pub-id>
        </citation>
        <citation citation-type="display-unstructured">Chaumette F, Hutchinson S (2007b) Visual servo control, part ii: advanced approaches. IEEE Robot Autom Mag 14(1): 109&#x2013;118 </citation>
      </ref>
      <ref id="CR8">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Dillmann</surname>
              <given-names>R</given-names>
            </name>
          </person-group>
          <article-title>Teaching and learning of robot tasks via observation of human performance</article-title>
          <source>Robot Autonom Sys</source>
          <year>2004</year>
          <volume>47</volume>
          <fpage>109</fpage>
          <lpage>116</lpage>
          <pub-id pub-id-type="doi">10.1016/j.robot.2004.03.005</pub-id>
        </citation>
        <citation citation-type="display-unstructured">Dillmann R (2004) Teaching and learning of robot tasks via observation of human performance. Robot Autonom Sys 47: 109&#x2013;116 </citation>
      </ref>
      <ref id="CR9">
        <citation citation-type="other">Enokida S, Ohashi T, Yoshida T, Ejima T (1999) Stochastic field model for autonomous robot learning. In: IEEE international conference on systems, Man, and Cybernetics, vol 2, pp 752&#x2013;757. doi:10.1109/ICSMC.1999.825356</citation>
      </ref>
      <ref id="CR10">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Espiau</surname>
              <given-names>B</given-names>
            </name>
            <name>
              <surname>Cahumette</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Rives</surname>
              <given-names>P</given-names>
            </name>
          </person-group>
          <article-title>A new approach to visual servoing in robotics</article-title>
          <source>IEEE Trans Robot Autom</source>
          <year>1992</year>
          <volume>8</volume>
          <issue>3</issue>
          <fpage>313</fpage>
          <lpage>326</lpage>
          <pub-id pub-id-type="doi">10.1109/70.143350</pub-id>
        </citation>
        <citation citation-type="display-unstructured">Espiau B, Cahumette F, Rives P (1992) A new approach to visual servoing in robotics. IEEE Trans Robot Autom 8(3): 313&#x2013;326 </citation>
      </ref>
      <ref id="CR11">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Foster</surname>
              <given-names>DJ</given-names>
            </name>
            <name>
              <surname>Morris</surname>
              <given-names>RG</given-names>
            </name>
            <name>
              <surname>Dayan</surname>
              <given-names>P</given-names>
            </name>
          </person-group>
          <article-title>A model of hippocampally dependent navigation, using the temporal difference learning rule</article-title>
          <source>Hippocampus</source>
          <year>2000</year>
          <volume>10</volume>
          <issue>1</issue>
          <fpage>1</fpage>
          <lpage>16</lpage>
          <pub-id pub-id-type="doi">10.1002/(SICI)1098-1063(2000)10:1&lt;1::AID-HIPO1&gt;3.0.CO;2-1</pub-id>
        </citation>
        <citation citation-type="display-unstructured">Foster DJ, Morris RG, Dayan P (2000) A model of hippocampally dependent navigation, using the temporal difference learning rule. Hippocampus 10(1): 1&#x2013;16 <pub-id pub-id-type="pmid">10706212</pub-id></citation>
      </ref>
      <ref id="CR12">
        <citation citation-type="other">Fukao T, Sumitomo T, Ineyama N, Adachi N (1998) Q-learning based on regularization theory to treat the continuous states and actions. In: IEEE international joint conference on neural networks, pp 1057&#x2013;062</citation>
      </ref>
      <ref id="CR13">
        <citation citation-type="other">Gaskett C, Fletcher L, Zelinsky A (2000) Reinforcement learning for a vision based mobile robot. In: IEEE/RSJ international conference on intelligent robots and systems, pp 403&#x2013;09</citation>
      </ref>
      <ref id="CR14">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gordon</surname>
              <given-names>GJ</given-names>
            </name>
          </person-group>
          <article-title>Reinforcement learning with function approximation converges to a region</article-title>
          <source>Adv Neural Inform Process Syst</source>
          <year>2001</year>
          <volume>13</volume>
          <issue>6</issue>
          <fpage>1040</fpage>
          <lpage>1046</lpage>
        </citation>
        <citation citation-type="display-unstructured">Gordon GJ (2001) Reinforcement learning with function approximation converges to a region. Adv Neural Inform Process Syst 13(6): 1040&#x2013;1046 </citation>
      </ref>
      <ref id="CR15">
        <citation citation-type="other">Gross H, Stephan V, Krabbes M (1998) A neural field approach to topological reinforcement learning in continuous action spaces. In: IEEE world congress on computational intelligence and international joint conference on neural networks, Anchorage, Alaska, pp 1992&#x2013;997. <ext-link ext-link-type="uri" xlink:href="http://www.citeseer.ist.psu.edu/article/gross98neural.html">http://www.citeseer.ist.psu.edu/article/gross98neural.html</ext-link></citation>
      </ref>
      <ref id="CR16">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Horaud</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Dornaika</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Espiau</surname>
              <given-names>B</given-names>
            </name>
          </person-group>
          <article-title>Visually guided object grasping</article-title>
          <source>IEEE Trans Robot Autom</source>
          <year>1998</year>
          <volume>14</volume>
          <issue>4</issue>
          <fpage>525</fpage>
          <lpage>532</lpage>
          <pub-id pub-id-type="doi">10.1109/70.704214</pub-id>
        </citation>
        <citation citation-type="display-unstructured">Horaud R, Dornaika F, Espiau B (1998) Visually guided object grasping. IEEE Trans Robot Autom 14(4): 525&#x2013;532 </citation>
      </ref>
      <ref id="CR17">
        <citation citation-type="other">Horiuchi T, Fujino A, Katai O, Sawaragi T (1997) Fuzzy interpolation-based Q-learning with profit sharing planscheme. In: Proceedings of the sixth IEEE international conference on fuzzy systems, vol 3, pp 1707&#x2013;712</citation>
      </ref>
      <ref id="CR18">
        <citation citation-type="other">Hosoda K, Asada M (1994) Versatile visual servoing without knowledge of true jacobian. In: IEEE/RSJ international conference on intelligent robots and systems</citation>
      </ref>
      <ref id="CR19">
        <citation citation-type="other">Hutchinson SA, Hager GD, Corke PI (1996) A tutorial on visual servo control. IEEE Trans Robot Autom 12(5): 651&#x2013;70. <ext-link ext-link-type="uri" xlink:href="http://www.citeseer.ist.psu.edu/hutchinson96tutorial.html">http://www.citeseer.ist.psu.edu/hutchinson96tutorial.html</ext-link></citation>
      </ref>
      <ref id="CR20">
        <citation citation-type="other">Kabudian J, Meybodi MR, Homayounpour MM (2004) Applying continuous action reinforcement learning automata(carla) to global training of hidden markov models. In: Proceedings of the international conference on information technology: coding and computing, IEEE Computer Society, Washington, DC, vol 4, pp 638&#x2013;42</citation>
      </ref>
      <ref id="CR21">
        <citation citation-type="other">Kobayashi Y, Fujii H, Hosoe S (2005) Reinforcement learning for manipulation using constraint between object and robot. In: IEEE international conference on systems, man and cybernetics, vol 1, pp 871&#x2013;76</citation>
      </ref>
      <ref id="CR22">
        <citation citation-type="other">Kolodziejski C, Porr B, W&#xF6;rg&#xF6;tter F (2008) On the equivalence between differential hebbian and temporal difference learning. Neural Comp</citation>
      </ref>
      <ref id="CR23">
        <citation citation-type="other">Leonard S, Jagersand M (2004) Learning based visual servoing. In: International conference on intelligent robots and systems, Sendai, Japan, pp 680&#x2013;85</citation>
      </ref>
      <ref id="CR24">
        <citation citation-type="other">Li J, Lilienthal AJ, Martinez-Marin T, Duckett T (2006) Q-ran: a constructive reinforcement learning approach for robot behavior learning. In: Proceedings of the IEEE/RSJ international conference on intelligent robots and systems, pp 2656&#x2013;662</citation>
      </ref>
      <ref id="CR25">
        <citation citation-type="other">Martinez-Marin T, Duckett T (2004) Robot docking by reinforcement learning in a visual servoing framework. In: IEEE conference on robotics, automation and mechatronics, vol 1, pp 159&#x2013;64</citation>
      </ref>
      <ref id="CR26">
        <citation citation-type="other">Martinetz TM, Ritter HJ, Schulten KJ (1990) Three-dimensional neural net for learning visuomotor coordination of a robot arm. IEEE Trans Neural Netw 1(1): 131&#x2013;36. <ext-link ext-link-type="uri" xlink:href="http://www.citeseer.ist.psu.edu/martinetz90threedimensional.html">http://www.citeseer.ist.psu.edu/martinetz90threedimensional.html</ext-link></citation>
      </ref>
      <ref id="CR27">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Moussa</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <article-title>Combining expert neural networks using reinforcement feedback for learning primitive grasping behavior</article-title>
          <source>IEEE Trans Neural Netw</source>
          <year>2004</year>
          <volume>15</volume>
          <issue>3</issue>
          <fpage>629</fpage>
          <lpage>638</lpage>
          <pub-id pub-id-type="doi">10.1109/TNN.2004.824412</pub-id>
        </citation>
        <citation citation-type="display-unstructured">Moussa M (2004) Combining expert neural networks using reinforcement feedback for learning primitive grasping behavior. IEEE Trans Neural Netw 15(3): 629&#x2013;638 <pub-id pub-id-type="pmid">15384551</pub-id></citation>
      </ref>
      <ref id="CR28">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Moussa</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Kamel</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <article-title>An experimental approach to robotic grasping using a connectionist architecture and generic grasping functions</article-title>
          <source>IEEE Trans Systems Man Cybernetics Part C: Appl Rev</source>
          <year>1998</year>
          <volume>28</volume>
          <fpage>239</fpage>
          <lpage>253</lpage>
          <pub-id pub-id-type="doi">10.1109/5326.669561</pub-id>
        </citation>
        <citation citation-type="display-unstructured">Moussa M, Kamel M (1998) An experimental approach to robotic grasping using a connectionist architecture and generic grasping functions. IEEE Trans Systems Man Cybernetics Part C: Appl Rev 28: 239&#x2013;253 </citation>
      </ref>
      <ref id="CR29">
        <citation citation-type="other">Perez MA, Cook PA (2004) Actor-critic architecture to increase the performance of a 6-dof visual servoing task. In: IEEE 4th international conference on intelligent systems design and application, Budapest, pp 669&#x2013;74</citation>
      </ref>
      <ref id="CR30">
        <citation citation-type="other">Peters J, Schaal S (2006a) Reinforcement learning for parameterized motor primitives. In: International joint conference on neural networks, pp 73&#x2013;0. <ext-link ext-link-type="uri" xlink:href="http://www-clmc.usc.edu/publications/P/peters-IJCNN2006.pdf">http://www-clmc.usc.edu/publications/P/peters-IJCNN2006.pdf</ext-link></citation>
      </ref>
      <ref id="CR31">
        <citation citation-type="other">Peters J, Schaal S (2006b) Policy gradient methods in robotics. In: IEEE/RSJ international conference on intelligent robots and systems, IROS2006, pp 2219&#x2013;225</citation>
      </ref>
      <ref id="CR32">
        <citation citation-type="other">Peters J, Schaal S (2007) Reinforcement learning for operation space control. In: IEEE international conference robotics and automatation, pp 2111&#x2013;116</citation>
      </ref>
      <ref id="CR33">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Peters</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Schaal</surname>
              <given-names>S</given-names>
            </name>
          </person-group>
          <article-title>Reinforcement learning of motor skills with policy gradients</article-title>
          <source>Neural Netw</source>
          <year>2008</year>
          <volume>21</volume>
          <fpage>682</fpage>
          <lpage>697</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neunet.2008.02.003</pub-id>
        </citation>
        <citation citation-type="display-unstructured">Peters J, Schaal S (2008) Reinforcement learning of motor skills with policy gradients. Neural Netw 21: 682&#x2013;697 <pub-id pub-id-type="pmid">18482830</pub-id></citation>
      </ref>
      <ref id="CR34">
        <citation citation-type="other">Qiang L, Hai ZH, Ming LL, Zheng YG (2000) Reinforcement learning with continuous vector output. In: IEEE international conference on systems, man, and cybernetics, vol 1, pp 188&#x2013;193. doi:10.1109/ICSMC.2000.884987</citation>
      </ref>
      <ref id="CR35">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ruis de Angulo</surname>
              <given-names>V</given-names>
            </name>
            <name>
              <surname>Torras</surname>
              <given-names>C</given-names>
            </name>
          </person-group>
          <article-title>Speeding up the learning of robot kinematics through function decomposition</article-title>
          <source>IEEE Trans Neural Netw</source>
          <year>2005</year>
          <volume>16</volume>
          <issue>6</issue>
          <fpage>1504</fpage>
          <lpage>1512</lpage>
          <pub-id pub-id-type="doi">10.1109/TNN.2005.852970</pub-id>
        </citation>
        <citation citation-type="display-unstructured">Ruis de Angulo V, Torras C (2005a) Speeding up the learning of robot kinematics through function decomposition. IEEE Trans Neural Netw 16(6): 1504&#x2013;1512 <pub-id pub-id-type="pmid">16342491</pub-id></citation>
      </ref>
      <ref id="CR36">
        <citation citation-type="other">Ruis de Angulo V, Torras C (2005b) Using psoms to learn inverse kinematics through virtual decomposition of the robot. In: International work-conference on artificial and natural neural networks (IWANN), pp 701&#x2013;08</citation>
      </ref>
      <ref id="CR37">
        <citation citation-type="other">Reynolds SI (2002) The stability of general discounted reinforcement learning with linear function approximation. In: UK workshop on computational intelligence (UKCI-02), pp 139&#x2013;46</citation>
      </ref>
      <ref id="CR38">
        <citation citation-type="other">Rezzoug N, Gorce P, Abellard A, Khelifa MB, Abellard P (2006) Learning to grasp in unknown environment by reinforcement learning and shaping. In: IEEE international conference on systems, man and cybernetics, vol 6, pp 4487&#x2013;4492. doi:10.1109/ICSMC.2006.384851</citation>
      </ref>
      <ref id="CR39">
        <citation citation-type="other">Schaal S, Ijspeert A, Billard A (2003) Decoding, imitating and influencing the actions of others: the mechanisms of social interaction. In: Computational approaches to motor learning by imitation, vol 358, pp 537&#x2013;47</citation>
      </ref>
      <ref id="CR40">
        <citation citation-type="other">Shibata K, Ito K (1999) Hand&#x2013;eye coordination in robot arm reaching task by reinforcement learning using a neural network. In: IEEE international conference on systems, man, and cybernetics, vol 5, pp 458&#x2013;63</citation>
      </ref>
      <ref id="CR41">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Soechting</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Flanders</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <article-title>Errors in pointing are due to approximations in targets in sensorimotor transformations</article-title>
          <source>J Neurophysiol</source>
          <year>1989</year>
          <volume>62</volume>
          <issue>2</issue>
          <fpage>595</fpage>
          <lpage>608</lpage>
        </citation>
        <citation citation-type="display-unstructured">Soechting J, Flanders M (1989a) Errors in pointing are due to approximations in targets in sensorimotor transformations. J Neurophysiol 62(2): 595&#x2013;608 <pub-id pub-id-type="pmid">2769350</pub-id></citation>
      </ref>
      <ref id="CR42">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Soechting</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Flanders</surname>
              <given-names>M</given-names>
            </name>
          </person-group>
          <article-title>Sensorimotor representations for pointing to targets in three-dimensional space</article-title>
          <source>J Neurophysiol</source>
          <year>1989</year>
          <volume>62</volume>
          <issue>2</issue>
          <fpage>582</fpage>
          <lpage>594</lpage>
        </citation>
        <citation citation-type="display-unstructured">Soechting J, Flanders M (1989b) Sensorimotor representations for pointing to targets in three-dimensional space. J Neurophysiol 62(2): 582&#x2013;594 <pub-id pub-id-type="pmid">2769349</pub-id></citation>
      </ref>
      <ref id="CR43">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Str&#xF6;sslin</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Sheynikhovich</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Chavarriaga</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Gerstner</surname>
              <given-names>W</given-names>
            </name>
          </person-group>
          <article-title>Robust self-localisation and navigation based on hippocampal place cells</article-title>
          <source>Neural Netw</source>
          <year>2005</year>
          <volume>18</volume>
          <issue>9</issue>
          <fpage>1125</fpage>
          <lpage>1140</lpage>
          <pub-id pub-id-type="doi">10.1016/j.neunet.2005.08.012</pub-id>
        </citation>
        <citation citation-type="display-unstructured">Str&#xF6;sslin T, Sheynikhovich D, Chavarriaga R, Gerstner W (2005) Robust self-localisation and navigation based on hippocampal place cells. Neural Netw 18(9): 1125&#x2013;1140 <pub-id pub-id-type="pmid">16263241</pub-id></citation>
      </ref>
      <ref id="CR44">
        <citation citation-type="other">Sugiyama M, Hachiya H, Towell, Vijayakumar S (2007) Value function approximation on non-linear manifolds for robot motor control. In: IEEE international conference on robotics and automation, pp 1733&#x2013;1740. doi:10.1109/ROBOT.2007.363573</citation>
      </ref>
      <ref id="CR45">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sutton</surname>
              <given-names>RS</given-names>
            </name>
          </person-group>
          <article-title>Learning to predict by the methods of temporal differences</article-title>
          <source>Mach Learn</source>
          <year>1988</year>
          <volume>3</volume>
          <fpage>9</fpage>
          <lpage>44</lpage>
        </citation>
        <citation citation-type="display-unstructured">Sutton RS (1988) Learning to predict by the methods of temporal differences. Mach Learn 3: 9&#x2013;44 </citation>
      </ref>
      <ref id="CR46">
        <citation citation-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Sutton</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Barto</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <source>Reinforcement learning: an introduction</source>
          <year>1998</year>
          <publisher-loc>Cambridge</publisher-loc>
          <publisher-name>MIT Press</publisher-name>
        </citation>
        <citation citation-type="display-unstructured">Sutton R, Barto A (1998) Reinforcement learning: an introduction. MIT Press, Cambridge </citation>
      </ref>
      <ref id="CR47">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sutton</surname>
              <given-names>RS</given-names>
            </name>
            <name>
              <surname>McAllester</surname>
              <given-names>D</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Mansour</surname>
              <given-names>Y</given-names>
            </name>
          </person-group>
          <article-title>Policy gradient methods for reinforcement learning with function approximation</article-title>
          <source>Adv Neural Inform Process Syst</source>
          <year>2000</year>
          <volume>12</volume>
          <fpage>1057</fpage>
          <lpage>1063</lpage>
        </citation>
        <citation citation-type="display-unstructured">Sutton RS, McAllester D, Singh S, Mansour Y (2000) Policy gradient methods for reinforcement learning with function approximation. Adv Neural Inform Process Syst 12: 1057&#x2013;1063 </citation>
      </ref>
      <ref id="CR48">
        <citation citation-type="other">Szepesvari C, Smart WD (2004) Interpolation-based Q-learning. In: Twenty-first international conference on machine learning (ICML04), vol 21, pp 791&#x2013;98</citation>
      </ref>
      <ref id="CR49">
        <citation citation-type="other">Takahashi Y, Takeda M, Asada M (1999) Continuous valued Q-learning for vision-guided behavior. In: Proceedings of the IEEE/SICE/RSJ international conference on multisensor fusion and integration for intelligent systems, pp 255&#x2013;60</citation>
      </ref>
      <ref id="CR50">
        <citation citation-type="other">Takeda M, Nakamura T, Ogasawara T (2001) Continuous valued Q-learning method able to incrementally refine state space. In: IEEE/RSJ International conference on intelligent robots and systems, vol 1, pp 265&#x2013;271. doi:10.1109/IROS.2001.973369</citation>
      </ref>
      <ref id="CR51">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tamosiunaite</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Ainge</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Kulvicius</surname>
              <given-names>T</given-names>
            </name>
            <name>
              <surname>Porr</surname>
              <given-names>B</given-names>
            </name>
            <name>
              <surname>Dudchenko</surname>
              <given-names>P</given-names>
            </name>
            <name>
              <surname>W&#xF6;rg&#xF6;tter</surname>
              <given-names>F</given-names>
            </name>
          </person-group>
          <article-title>Path-finding in real and simulated rats: assessing the influence of path characteristics on navigation learning</article-title>
          <source>J Comput Neurosci</source>
          <year>2008</year>
          <volume>25</volume>
          <fpage>562</fpage>
          <lpage>582</lpage>
          <pub-id pub-id-type="doi">10.1007/s10827-008-0094-6</pub-id>
        </citation>
        <citation citation-type="display-unstructured">Tamosiunaite M, Ainge J, Kulvicius T, Porr B, Dudchenko P, W&#xF6;rg&#xF6;tter F (2008) Path-finding in real and simulated rats: assessing the influence of path characteristics on navigation learning. J Comput Neurosci 25: 562&#x2013;582 <pub-id pub-id-type="pmid">18446432</pub-id></citation>
      </ref>
      <ref id="CR52">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tesauro</surname>
              <given-names>G</given-names>
            </name>
          </person-group>
          <article-title>Temporal difference learning and TD-gammon</article-title>
          <source>Comm ACM</source>
          <year>1995</year>
          <volume>38</volume>
          <issue>3</issue>
          <fpage>58</fpage>
          <lpage>67</lpage>
          <pub-id pub-id-type="doi">10.1145/203330.203343</pub-id>
        </citation>
        <citation citation-type="display-unstructured">Tesauro G (1995) Temporal difference learning and TD-gammon. Comm ACM 38(3): 58&#x2013;67 </citation>
      </ref>
      <ref id="CR53">
        <citation citation-type="other">Tham C, Prager R (1993) Reinforcement learning methods for multi-linked manipulator obstacle avoidance and control. In: Proceedings of the IEEE Asia-Pacific workshop on advances in motion control, Singapore, pp 140&#x2013;45</citation>
      </ref>
      <ref id="CR54">
        <citation citation-type="other">van Hasselt H, Wiering M (2007) Reinforcement learning in continuous action spaces. In: IEEE international symposium on approximate dynamic programming and reinforcement learning, pp 272&#x2013;279. doi:10.1109/ADPRL.2007.368199</citation>
      </ref>
      <ref id="CR55">
        <citation citation-type="other">Wang B, Li J, Liu H (2006) A heuristic reinforcement learning for robot approaching objects. In: IEEE conference on robotics, automation and mechatronics, pp 1&#x2013;5. doi:10.1109/RAMECH.2006.252749</citation>
      </ref>
      <ref id="CR56">
        <citation citation-type="other">Watkins CJ (1989) Learning from delayed rewards. Ph.D. thesis, Cambridge University, Cambridge</citation>
      </ref>
      <ref id="CR57">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Watkins</surname>
              <given-names>CJ</given-names>
            </name>
            <name>
              <surname>Dayan</surname>
              <given-names>P</given-names>
            </name>
          </person-group>
          <article-title>Q-learning</article-title>
          <source>Mach Learn</source>
          <year>1992</year>
          <volume>8</volume>
          <fpage>279</fpage>
          <lpage>292</lpage>
        </citation>
        <citation citation-type="display-unstructured">Watkins CJ, Dayan P (1992) Q-learning. Mach Learn 8: 279&#x2013;292 </citation>
      </ref>
      <ref id="CR58">
        <citation citation-type="other">Wiering M (2004) Convergence and divergence in standard and averaging reinforcement learning. In: Boulicaut J, Esposito F, Giannotti F, Pedreschi D (eds) Proceedings of the 15th European conference on machine learning ECML&#x2019;04, pp 477&#x2013;88</citation>
      </ref>
      <ref id="CR59">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Williams</surname>
              <given-names>RJ</given-names>
            </name>
          </person-group>
          <article-title>Simple statistical gradient-following algorithms for connectionists reinforcement learning</article-title>
          <source>Mach Learn</source>
          <year>1992</year>
          <volume>8</volume>
          <fpage>229</fpage>
          <lpage>256</lpage>
        </citation>
        <citation citation-type="display-unstructured">Williams RJ (1992) Simple statistical gradient-following algorithms for connectionists reinforcement learning. Mach Learn 8: 229&#x2013;256 </citation>
      </ref>
      <ref id="CR60">
        <citation citation-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>W&#xF6;rg&#xF6;tter</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Porr</surname>
              <given-names>B</given-names>
            </name>
          </person-group>
          <article-title>Temporal sequence learning, prediction, and control: a review of different models and their relation to biological mechanisms</article-title>
          <source>Neural Comput</source>
          <year>2005</year>
          <volume>17</volume>
          <issue>2</issue>
          <fpage>245</fpage>
          <lpage>319</lpage>
          <pub-id pub-id-type="doi">10.1162/0899766053011555</pub-id>
        </citation>
        <citation citation-type="display-unstructured">W&#xF6;rg&#xF6;tter F, Porr B (2005) Temporal sequence learning, prediction, and control: a review of different models and their relation to biological mechanisms. Neural Comput 17(2): 245&#x2013;319 <pub-id pub-id-type="pmid">15720770</pub-id></citation>
      </ref>
    </ref-list>
    <fn-group>
      <fn>
        <p content-type="funding">This work was supported by EU-Grant PACO-PLUS.</p>
      </fn>
    </fn-group>
  </back>
</article>
