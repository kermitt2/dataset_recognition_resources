<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD Journal Archiving and Interchange DTD v3.0 20080202//EN" "archivearticle3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="case-report">
  <?properties open_access?>
  <?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
  <?DTDIdentifier.IdentifierType public?>
  <?SourceDTD.DTDName A++V2.4.dtd?>
  <?SourceDTD.Version 2.4?>
  <?ConverterInfo.XSLTName springer2nlmx2.xsl?>
  <?ConverterInfo.Version 2?>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Psychon Bull Rev</journal-id>
      <journal-title-group>
        <journal-title>Psychonomic Bulletin &amp; Review</journal-title>
      </journal-title-group>
      <issn pub-type="ppub">1069-9384</issn>
      <issn pub-type="epub">1531-5320</issn>
      <publisher>
        <publisher-name>Springer-Verlag</publisher-name>
        <publisher-loc>New York</publisher-loc>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmc">3098361</article-id>
      <article-id pub-id-type="pmid">21468774</article-id>
      <article-id pub-id-type="publisher-id">76</article-id>
      <article-id pub-id-type="doi">10.3758/s13423-011-0076-y</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Article</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Power laws from individual differences in learning and forgetting: mathematical analyses</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" corresp="yes">
          <name>
            <surname>Murre</surname>
            <given-names>Jaap M. J.</given-names>
          </name>
          <address>
            <email>jaap@murre.com</email>
          </address>
          <xref ref-type="aff" rid="Aff1"/>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Chessa</surname>
            <given-names>Antonio G.</given-names>
          </name>
          <xref ref-type="aff" rid="Aff1"/>
        </contrib>
        <aff id="Aff1">University of Amsterdam, Amsterdam, The Netherlands </aff>
      </contrib-group>
      <pub-date pub-type="epub">
        <day>6</day>
        <month>4</month>
        <year>2011</year>
      </pub-date>
      <pub-date pub-type="pmc-release">
        <day>6</day>
        <month>4</month>
        <year>2011</year>
      </pub-date>
      <pub-date pub-type="ppub">
        <month>6</month>
        <year>2011</year>
      </pub-date>
      <volume>18</volume>
      <issue>3</issue>
      <fpage>592</fpage>
      <lpage>597</lpage>
      <permissions>
        <copyright-statement>&#xA9; The Author(s) 2011</copyright-statement>
      </permissions>
      <abstract id="Abs1">
        <p>It has frequently been claimed that learning performance improves with practice according to the so-called &#x201C;Power Law of Learning.&#x201D; Similarly, forgetting may follow a power law. It has been shown on the basis of extensive simulations that such power laws may emerge through averaging functions with other, nonpower function shapes. In the present article, we supplement these simulations with a mathematical proof that power functions will indeed emerge as a result of averaging over exponential functions, if the distribution of learning rates follows a gamma distribution, a uniform distribution, or a half-normal function. Through a number of simulations, we further investigate to what extent these findings may affect empirical results in practice.</p>
      </abstract>
      <kwd-group>
        <title>Keywords</title>
        <kwd>Power law of learning</kwd>
        <kwd>Learning</kwd>
        <kwd>Forgetting</kwd>
        <kwd>Effects of averaging</kwd>
      </kwd-group>
      <custom-meta-group>
        <custom-meta>
          <meta-name>issue-copyright-statement</meta-name>
          <meta-value>&#xA9; Psychonomic Society, Inc. 2011</meta-value>
        </custom-meta>
      </custom-meta-group>
    </article-meta>
  </front>
  <body>
    <sec id="Sec1">
      <title>Power laws of learning and forgetting</title>
      <p>At what rate can we expect to learn and forget? We become faster and more accurate as we practice new activities, such as piano playing or speaking a foreign language. It has frequently been claimed that learning performance <italic>P</italic> improves with practice time <italic>t</italic>, according to the so-called &#x201C;Power Law of Learning,&#x201D; or that the forgetting of learned material follows a power function (J. R. Anderson &amp; Schooler, <xref ref-type="bibr" rid="CR2">1991</xref>; Newell &amp; Rosenbloom, <xref ref-type="bibr" rid="CR11">1981</xref>; Wixted &amp; Ebbesen, <xref ref-type="bibr" rid="CR13">1991</xref>). In its simplest form, a power function is a function of the shape <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ P = {t^{\mu }} $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq1.gif"/></alternatives></inline-formula>, where <italic>&#x3BC;</italic> is the learning (or forgetting) rate parameter, and <italic>t</italic> is number of learning episodes or time. <italic>P</italic> may refer to how accurate or how fast we carry out a learned activity.</p>
      <p>Despite the compact form of <italic>P</italic>, it describes different types of behavior. For instance, the relative learning rate slows down with prolonged practice. There are situations, however, in which <italic>P</italic> needs some adjustment. The previous equation is not correct if <italic>P</italic> denotes a probability, <italic>&#x3BC;</italic> is negative, and <italic>t</italic> is small. For example, for <italic>t</italic> = 0.5 and <italic>&#x3BC;</italic> = &#x2212;0.1, we have <italic>P</italic> = 0.5<sup>-0.1</sup> = 1.072. This would give a probability greater than 1, which is impossible. We can easily remedy this by adding 1 to <italic>t</italic>, thus obtaining <inline-formula id="IEq2"><alternatives><tex-math id="M2">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ P = {\left( {t + {1}} \right)^{\mu }} $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq2.gif"/></alternatives></inline-formula>. This form ensures that its value remains properly scaled as a probability (i.e., remains between 0 and 1) if <italic>&#x3BC;</italic> is negative.</p>
      <p>Several authors dispute that learning follows a power function (e.g., Heathcote, Brown, &amp; Mewhort, <xref ref-type="bibr" rid="CR7">2000</xref>), reporting exponential curves for individuals. Exponential curves have basic shape <italic>P</italic> = <italic>&#x3BC;</italic><sup><italic>t</italic></sup>. If learning shows an exponential improvement, the learning process itself does not slow down but continues at the same relative pace. These opposing viewpoints can be reconciled, if averaging over individual exponential curves would yield an averaged power function. This has indeed been found in an extensive simulation study by R.B. Anderson (<xref ref-type="bibr" rid="CR1">2001</xref>). This study showed for a variety of component-curve shapes, not just exponentials, that averaging tends to give power-like functions. There are also theoretical arguments based on a geometrical analysis that explain why there is a general tendency for averaged curves to give a superior fit for power functions as compared with exponential functions (Myung, Kim, &amp; Pitt, <xref ref-type="bibr" rid="CR10">2000</xref>).</p>
      <p>The motivation by R. B. Anderson (<xref ref-type="bibr" rid="CR1">2001</xref>) for carrying out a simulation study rather than a mathematical analysis was that a mathematical proof had not been established and may, in fact, be impossible. As we will demonstrate in the present article, however, this is not the case. For certain relevant cases, a mathematical proof can in fact be derived, which we will outline below. We will limit our analysis to variations in learning rate, noting that there are several other possible sources of variation that we will ignore here, such as differences in asymptotes and intercept and individual levels of variability in learning performance. We will also ignore noise from sampling error, which tends to increase spurious power law fits (Brown &amp; Heathcote, <xref ref-type="bibr" rid="CR4">2003b</xref>; Myung et al., <xref ref-type="bibr" rid="CR10">2000</xref>). Finally, note that although we use the Power Law of Learning as a starting point of our analysis, our proof is general and applies to any situation in which the assumptions are met. In particular, it also applies to the shape of forgetting functions.</p>
    </sec>
    <sec id="Sec2">
      <title>Exponential learning curves</title>
      <p>Our measure of learning performance is the probability <italic>p</italic>(<italic>t</italic>) that a student will make an error on a certain test item (e.g., knowing foreign language vocabulary) after study time <italic>t</italic>. In the analysis, we will first assume an exponential learning curve for each individual student <italic>i</italic>: <inline-formula id="IEq3"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ P = p(t) = {e^{{ - {\mu_i}t}}} $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq3.gif"/></alternatives></inline-formula> with <italic>&#x3BC;</italic><sub><italic>i</italic></sub> &#x2265; 0. Such a curve starts at 100% error at <italic>t</italic> = 0 and will reach an asymptote with 0 errors (100% correct), given enough study time. Our second assumption is that students&#x2019; learning rates are not all equal. Instead, we make the more reasonable assumption that some will be fast learners (high <italic>&#x3BC;</italic><sub><italic>i</italic></sub>) and others slow learners (low <italic>&#x3BC;</italic><sub><italic>i</italic></sub>). The aim of the present article is limited to showing that for certain common probability distributions, the shape of the averaged curve can be derived mathematically and that it conforms to a power function. We will also explore numerically the extent of the distortion introduced through averaging over exponential curves, which at times may give misleading averaged curves.</p>
    </sec>
    <sec id="Sec3">
      <title>Analyses with different learning rate distributions</title>
      <sec id="Sec4">
        <title>Gamma distribution</title>
        <p>We will first consider the case in which learning rates follow a gamma distribution. This is a well-known probability distribution that can take different shapes, depending on its parameters <italic>a</italic> (the &#x201C;shape&#x201D; parameter) and <italic>b</italic> (the &#x201C;scale&#x201D; parameter). If the shape parameter <italic>a</italic> is 1, the gamma distribution becomes the exponential distribution as a special case. The mean of the gamma distribution is given by the product <italic>ab</italic>, and the variance by <italic>ab</italic><sup>2</sup>. As can be seen from Fig.&#xA0;<xref rid="Fig1" ref-type="fig">1</xref>, its shape is flexible and may vary from a peaked distribution in which most learners tend to have low or average learning rates, to a broader distribution in which learning rates are more variable.
<fig id="Fig1"><label>Fig.&#xA0;1</label><caption><p>Illustration of the flexibility of the gamma distribution. Shown are plots for different values of <italic>a</italic> and <italic>b</italic></p></caption><graphic xlink:href="13423_2011_76_Fig1_HTML" id="MO1"/></fig></p>
        <p>In the <xref rid="Sec11" ref-type="sec">Appendix</xref>, we show that, if we assume that the learning rates <italic>&#x3BC;</italic> of individual participants follow a gamma distribution, the average <italic>p</italic><sub><italic>A</italic></sub>(<italic>t</italic>) of a (large) number of exponential learning curves will approach <inline-formula id="IEq4"><alternatives><tex-math id="M4">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {p_A}(t) = {(1 + b\,t)^{{ - a}}} $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq4.gif"/></alternatives></inline-formula> (e.g., Feller, <xref ref-type="bibr" rid="CR5">1966</xref>, p. 48). This is a power function, which is properly scaled as a probability (i.e., remaining between 0 and 1) and starts at zero performance (100% error) at <italic>t</italic> = 0. Simulation studies with a range of parameter values of <italic>a</italic> and <italic>b</italic>, not reported here, confirm that averaged simulated learning curves approach the theoretical power function very closely.</p>
      </sec>
      <sec id="Sec5">
        <title>Uniqueness of the result</title>
        <p>We might wonder whether there are any other distributions for which we would find exactly this power function? This is not the case, because the method by which we calculate the expected value over exponential curves is identical to the Laplace transform (and is also very similar to the moment generating function in statistics). It is a well-known result from mathematics that the Laplace transform is a so-called &#x201C;one-to-one transformation,&#x201D; meaning that the transformed function is uniquely related to the resulting function. Hence, there is no other statistical distribution other than the gamma distribution that will give exactly the power function in this case. By a similar argument, we can immediately conclude that there is only one distribution that will retain exactly the shape of the exponential function when averaging, namely the Dirac delta function&#x2014;a distribution that has all probability mass at a single point. In the present article, this means that only if the learning rates of all students are exactly identical will the averaged curve still be of the same exponential form; any deviations will compromise the exponential shape in some way or another.</p>
        <p>There are, however, several distributions that will converge exactly to a power function in the limit, for higher <italic>t</italic> (the gamma distribution result is valid for all <italic>t</italic> &#x2265; 0) with only the initial portions of the averaged curve deviating (somewhat) from a power curve. We will discuss the result for two learning rate distributions: the uniform and the (truncated) normal distribution. Surprisingly, averaging exponential curves of which the learning rates follow either of these very different distributions still gives rise to a rapid convergence to a power function.</p>
      </sec>
    </sec>
    <sec id="Sec6">
      <title>Uniform distribution</title>
      <p>The form of the uniform distribution is not very flexible, but it is nonetheless informative for our analysis, since it is a closed distribution: Learning rates remain between set bounds, as compared with the gamma distribution, which allows a fraction of very high learning rates. We might encounter closed distributions if participants have passed a preselection test such that the slow and fast learners are eliminated (e.g., they are in a different group or class). This leaves us with participants who have learning rates higher than <italic>a</italic> and lower than <italic>b</italic>. Suppose that the number of participants is distributed evenly between <italic>a</italic> and <italic>b</italic>; we then have a uniform distribution of learning rates. If <italic>a</italic> = 0 and <italic>b</italic> = 1, we speak of a <italic>standard uniform distribution</italic>. In that case, only the fast learners have been eliminated.</p>
      <p>In the <xref rid="Sec11" ref-type="sec">Appendix</xref>, we show that with both exponential individual learning curves and uniformly distributed learning rates, the averaged curve for the uniform distribution with <italic>a</italic> = 0 and <italic>b</italic> &gt; <italic>a</italic>, rapidly converges to (<italic>bt</italic>)<sup>-1</sup> with increasing <italic>t</italic>. This is also a power function, with exponent &#x2212;1 (a so-called &#x201C;hyperbolic function&#x201D;). In Fig.&#xA0;<xref rid="Fig2" ref-type="fig">2b</xref>, one can see that this convergence is typically very rapid and that the averaged curve approaches a power function even for low <italic>t</italic> (e.g., <italic>t</italic> &gt; 10). However, if <italic>a</italic> is higher than 0, and if <italic>b</italic> approaches <italic>a</italic>, the averaged curve approaches the exponential <italic>e</italic><sup>-<italic>a t</italic></sup> (see the <xref rid="Sec11" ref-type="sec">Appendix</xref>), which is to be expected, because in that case, we then have nearly all learning rates similar to <italic>a</italic> (see previous remark about the Dirac function). For the in-between case, where <italic>b</italic> &gt; <italic>a</italic> &gt; 0, we have the mixture of a power function and exponentials, for which we were not able to find a useful closed-form expression for the limit. These results corroborate the simulations studies by Brown and Heathcote (<xref ref-type="bibr" rid="CR3">2003a</xref>), who found that when averaging over exponential curves with high and low learning rates, a large variation in rates increased chances of finding spurious power function fits.
<fig id="Fig2"><label>Fig.&#xA0;2</label><caption><p><bold>a</bold> Pdfs of three distributions: a gamma distribution (<italic>a</italic> = 1, <italic>b</italic> =1, <italic>dotted line</italic>; this is an exponential distribution, which is a special case of the gamma distribution), standard uniform distribution (<italic>dashed line</italic>), and half-normal distribution (<inline-formula id="IEq5"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \theta = \pi /2 $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq5.gif"/></alternatives></inline-formula>, solid line). <bold>b</bold> Theoretical averaged exponential learning curves with learning rates distributed as in (<bold>a</bold>). The line styles in (<bold>b</bold>) refer to the same distributions as in (<bold>a</bold>). All curves converge to the curve <italic>t</italic><sup>-1</sup></p></caption><graphic xlink:href="13423_2011_76_Fig2_HTML" id="MO2"/></fig></p>
    </sec>
    <sec id="Sec7">
      <title>Half-normal distribution</title>
      <p>The normal distribution is ubiquitous, and that alone merits its inclusion in the present article. Of course, negative learning rates are meaningless, so we must use the so-called &#x201C;half-normal distribution,&#x201D; which is a conditioned normal distribution with mean 0 but with the left half &#x201C;chopped off.&#x201D; In the <xref rid="Sec11" ref-type="sec">Appendix</xref>, we prove that with increasing <italic>t</italic>, the averaged curve also converges to a power function of the form <inline-formula id="IEq6"><alternatives><tex-math id="M6">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\left( {\frac{\pi }{{2\theta }}t} \right)^{{ - 1}}} $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq6.gif"/></alternatives></inline-formula>, where <italic>&#x3B8;</italic> is a parameter that determines the shape of the half-normal distribution.</p>
    </sec>
    <sec id="Sec8">
      <title>Different distributions, yet similar averaged curves</title>
      <p>Of particular interest is that there are many cases in which an observed averaged learning curve may be produced by different underlying learning rate distributions. This point is illustrated in Fig.&#xA0;<xref rid="Fig2" ref-type="fig">2</xref>, which shows the three predicted functions derived in this article. We have chosen parameter values such that the averaged curves converge to the same form <italic>t</italic><sup>-1</sup>, with increasing learning time <italic>t</italic>. In Fig.&#xA0;<xref rid="Fig2" ref-type="fig">2a</xref>, it can be observed that the selected learning rate distributions are very different, whereas in Fig.&#xA0;<xref rid="Fig2" ref-type="fig">2b</xref>, we see a rapid convergence to the shape <italic>t</italic><sup>-1</sup>. Only the initial portions of the learning curves differ visibly.</p>
    </sec>
    <sec id="Sec9">
      <title>Application to small numbers of participants</title>
      <p>One might wonder how the mathematical results apply to cases in which we average over small numbers of participants. In particular, what would happen if we fitted both a power function and an exponential function to averaged exponential learning curves? Would the power function always fit better, even with small numbers of participants, or would spurious power functions appear only with very large numbers of participants? To investigate, we simulated experiments with increasing numbers of participants. Each artificial participant contributed an exponential (noise-free) learning curve with a certain learning rate drawn from the gamma distribution. A learning curve averaged over all artificial participants was fitted to both an exponential function and a power function using a least-squares criterion. The goodness of fit of the two function types was compared using the <italic>r</italic><sup>2</sup> value (variance explained).</p>
      <p>In Fig.&#xA0;<xref rid="Fig3" ref-type="fig">3</xref>, we show the <italic>r</italic><sup>2</sup> for one particular choice of learning rate distribution (<italic>a</italic> = 1, <italic>b</italic> = 1) and number of learning episodes (20), which was one of the distributions shown in Fig.&#xA0;<xref rid="Fig2" ref-type="fig">2</xref>. As can be observed, the power function fits better than the exponential function, as long as there are more than a few participants, even though the individual curves are exponential. When we repeated these simulations for the uniform and half-normal distribution, with parameters as in Fig.&#xA0;<xref rid="Fig2" ref-type="fig">2</xref>, the graphs were highly similar. We also explored this type of simulation for other parameter values of the gamma distribution with similar results. Only if the variance of the gamma distribution is very small (i.e., a strongly peaked distribution) does the exponential function fit better for nontrivial numbers (four or fewer) of participants.
<fig id="Fig3"><label>Fig.&#xA0;3</label><caption><p>For simulated experiments with increasing numbers of artificial participants, the <italic>r</italic><sup>2</sup> value (fraction of variance explained) is shown for the fits of exponential and power function. Each point is the average of 1,000 simulated experiments with gamma distribution parameters (<italic>a</italic> = 1, <italic>b</italic> = 1), and the number of learning episodes is 20</p></caption><graphic xlink:href="13423_2011_76_Fig3_HTML" id="MO3"/></fig></p>
    </sec>
    <sec id="Sec10">
      <title>Discussion</title>
      <p>In the present article, we discussed mathematical analyses of the effects of averaging exponential learning curves, where it is assumed that individuals have learning rates that follow a known probability distribution. We demonstrated mathematically that if the individual learning curves are exponential in shape, averaging over these curves gives rise to spurious power laws if the learning rates follow a gamma distribution, a uniform distribution, or a half-normal distribution.</p>
      <p>The theoretical result can be generalized to forgetting functions in which we consider <italic>t</italic> to be time since the completion of learning. Using <inline-formula id="IEq7"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ p(t) = {e^{{ - \mu t}}} $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq7.gif"/></alternatives></inline-formula> for individual curves, and assuming a gamma distribution of the individual forgetting rates <italic>&#x3BC;</italic>, we obtain for the averaged forgetting curve: <inline-formula id="IEq8"><alternatives><tex-math id="M8">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {p_A}(t) = {\left( {1 + a\,t} \right)^{{ - b}}} $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq8.gif"/></alternatives></inline-formula>. Our result is corroborated by a recent analysis (Lee, <xref ref-type="bibr" rid="CR9">2004</xref>) of over 200 forgetting studies taken from the literature, most of which average across participants. These forgetting curves are best modeled by the power function (1+<italic>t</italic>)<sup>-1</sup>. If, as assumed here, this function is a result of averaging over exponential forgetting functions, we expect the distribution of the forgetting rates of individual participants to be<inline-formula id="IEq9"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ f(\mu ) = {a^{{ - 1}}}{e^{{ - {a^{{ - 1}}}\mu }}} $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq9.gif"/></alternatives></inline-formula>, with <italic>a</italic> = 1 This is an exponential distribution, implying that in these experiments, we should observe rather many students who show little or no forgetting. This is not implausible, with the short retention intervals often encountered in the psychology laboratory, in which there may not be enough time to allow sufficient forgetting for many participants. The resulting ceiling effects would foster spurious power laws in the averaged forgetting curves.</p>
      <p>Analyses such as these can also be applied to averaging over items rather than over students (R. B. Anderson, <xref ref-type="bibr" rid="CR1">2001</xref>; Newell &amp; Rosenbloom, <xref ref-type="bibr" rid="CR11">1981</xref>). We then assume that single items to be learned (e.g., foreign language words) have different learning rates, according to a gamma distribution. The learning curve averaged over items will then appear as a power function. Thus, even a single student may show a power learning curve based on averaged performance of heterogeneous items to be learned. The analysis can be carried even further, to the level below that of a single item, namely to the features that make up its representation.</p>
      <p>Our analysis complements earlier simulation studies (R. B. Anderson, <xref ref-type="bibr" rid="CR1">2001</xref>) and theoretical work (Myung, et al., <xref ref-type="bibr" rid="CR10">2000</xref>) by providing further mathematical analyses. Myung et al. show why, in general, averaging over exponentials will tend to produce good power function fits, with very general assumptions about the distribution of learning rates. R. B. Anderson&#x2019;s results suggest that averaging over certain nonexponential types of curves may also give power-like functions. We will address this issue elsewhere. Clearly, our results do not rule out that processes other than averaging may give rise to power laws (Wixted, <xref ref-type="bibr" rid="CR12">2004</xref>). Nonetheless, we have presently adduced rigorous mathematical proof that power laws may arise as a result of mere data aggregation without reflecting directly the properties of fundamental cognitive processes, which may well be exponential in nature.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgements</title>
      <p>This research was funded by the Netherlands Organisation for Scientific Research. We would like to thank Cathleen Moore, Robert Nosofsky, Michael Lee, Scott Brown, and Richard Anderson for helpful remarks on earlier versions of this paper.</p>
      <p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution Noncommercial License which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited.</p>
    </ack>
    <ref-list id="Bib1">
      <title>References</title>
      <ref id="CR1">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Anderson</surname>
              <given-names>RB</given-names>
            </name>
          </person-group>
          <article-title>The power law as an emergent property</article-title>
          <source>Memory &amp; Cognition</source>
          <year>2001</year>
          <volume>7</volume>
          <fpage>1061</fpage>
          <lpage>1068</lpage>
          <pub-id pub-id-type="doi">10.3758/BF03195767</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR2">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Anderson</surname>
              <given-names>JR</given-names>
            </name>
            <name>
              <surname>Schooler</surname>
              <given-names>LJ</given-names>
            </name>
          </person-group>
          <article-title>Reflections of the environment in memory</article-title>
          <source>Psychological Science</source>
          <year>1991</year>
          <volume>2</volume>
          <fpage>396</fpage>
          <lpage>408</lpage>
          <pub-id pub-id-type="doi">10.1111/j.1467-9280.1991.tb00174.x</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR3">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Brown</surname>
              <given-names>SD</given-names>
            </name>
            <name>
              <surname>Heathcote</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>Averaging learning curves across and within participants</article-title>
          <source>Behavior Research Methods, Instruments, &amp; Computers</source>
          <year>2003</year>
          <volume>35</volume>
          <fpage>11</fpage>
          <lpage>21</lpage>
          <pub-id pub-id-type="doi">10.3758/BF03195493</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR4">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Brown</surname>
              <given-names>SD</given-names>
            </name>
            <name>
              <surname>Heathcote</surname>
              <given-names>A</given-names>
            </name>
          </person-group>
          <article-title>Bias in exponential and power function fits due to noise: Comment on Myung, Kim and Pitt</article-title>
          <source>Memory &amp; Cognition</source>
          <year>2003</year>
          <volume>31</volume>
          <fpage>656</fpage>
          <lpage>661</lpage>
          <pub-id pub-id-type="doi">10.3758/BF03196105</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR5">
        <mixed-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Feller</surname>
              <given-names>W</given-names>
            </name>
          </person-group>
          <source>An introduction to probability theory and its applications (Vol. 2)</source>
          <year>1966</year>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>Wiley</publisher-name>
        </mixed-citation>
      </ref>
      <ref id="CR6">
        <mixed-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Gautschi</surname>
              <given-names>W</given-names>
            </name>
          </person-group>
          <person-group person-group-type="editor">
            <name>
              <surname>Abramowitz</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Stegun</surname>
              <given-names>IA</given-names>
            </name>
          </person-group>
          <article-title>Error function and fresnel integrals</article-title>
          <source>Handbook of mathematical functions with formulas, graphs, and mathematical tables</source>
          <year>1965</year>
          <publisher-loc>New York</publisher-loc>
          <publisher-name>Dover</publisher-name>
        </mixed-citation>
      </ref>
      <ref id="CR7">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Heathcote</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Brown</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Mewhort</surname>
              <given-names>DJK</given-names>
            </name>
          </person-group>
          <article-title>The power law repealed: The case for an exponential law of practice</article-title>
          <source>Psychonomic Bulletin &amp; Review</source>
          <year>2000</year>
          <volume>7</volume>
          <fpage>185</fpage>
          <lpage>207</lpage>
          <pub-id pub-id-type="doi">10.3758/BF03212979</pub-id>
          <pub-id pub-id-type="pmid">10909131</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR8">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Killeen</surname>
              <given-names>PR</given-names>
            </name>
          </person-group>
          <article-title>Writing and overwriting short-term memory [Review]</article-title>
          <source>Psychonomic Bulletin &amp; Review</source>
          <year>2001</year>
          <volume>8</volume>
          <fpage>18</fpage>
          <lpage>43</lpage>
          <pub-id pub-id-type="doi">10.3758/BF03196137</pub-id>
          <pub-id pub-id-type="pmid">11340865</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR9">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lee</surname>
              <given-names>MD</given-names>
            </name>
          </person-group>
          <article-title>A Bayesian analysis of retention functions</article-title>
          <source>Journal of Mathematical Psychology</source>
          <year>2004</year>
          <volume>48</volume>
          <fpage>310</fpage>
          <lpage>321</lpage>
          <pub-id pub-id-type="doi">10.1016/j.jmp.2004.06.002</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR10">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Myung</surname>
              <given-names>IJ</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>C</given-names>
            </name>
            <name>
              <surname>Pitt</surname>
              <given-names>MA</given-names>
            </name>
          </person-group>
          <article-title>Toward an explanation of the power law artifact: Insights from response surface analysis</article-title>
          <source>Memory &amp; Cognition</source>
          <year>2000</year>
          <volume>28</volume>
          <fpage>832</fpage>
          <lpage>840</lpage>
          <pub-id pub-id-type="doi">10.3758/BF03198418</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR11">
        <mixed-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Newell</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Rosenbloom</surname>
              <given-names>PS</given-names>
            </name>
          </person-group>
          <person-group person-group-type="editor">
            <name>
              <surname>Anderson</surname>
              <given-names>JR</given-names>
            </name>
          </person-group>
          <article-title>Mechanisms of skill acquisition and the law of practice</article-title>
          <source>Cognitive skills and their acquisition</source>
          <year>1981</year>
          <publisher-loc>Hillsdale</publisher-loc>
          <publisher-name>Erlbaum</publisher-name>
        </mixed-citation>
      </ref>
      <ref id="CR12">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wixted</surname>
              <given-names>JT</given-names>
            </name>
          </person-group>
          <article-title>On common ground: Jost's (1897) Law of forgetting and Ribot's (1881) Law of retrograde amnesia</article-title>
          <source>Psychological Review</source>
          <year>2004</year>
          <volume>111</volume>
          <fpage>864</fpage>
          <lpage>879</lpage>
          <pub-id pub-id-type="doi">10.1037/0033-295X.111.4.864</pub-id>
          <pub-id pub-id-type="pmid">15482065</pub-id>
        </mixed-citation>
      </ref>
      <ref id="CR13">
        <mixed-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wixted</surname>
              <given-names>JT</given-names>
            </name>
            <name>
              <surname>Ebbesen</surname>
              <given-names>EB</given-names>
            </name>
          </person-group>
          <article-title>On the form of forgetting</article-title>
          <source>Psychological Science</source>
          <year>1991</year>
          <volume>2</volume>
          <fpage>409</fpage>
          <lpage>415</lpage>
          <pub-id pub-id-type="doi">10.1111/j.1467-9280.1991.tb00175.x</pub-id>
        </mixed-citation>
      </ref>
    </ref-list>
    <app-group>
      <app id="App1">
        <sec id="Sec11">
          <title>Appendix</title>
          <p>The starting point of our analysis is the form of the function for the probability of correctly recalling a learned item for an individual participant. Let us denote the initial amount of learned information stored per unit of practice time <italic>t</italic> by <italic>&#x3BC;</italic>. We take the following form for the recall function <italic>p</italic>(<italic>t</italic>) of an individual participant after learning time <italic>t</italic>:
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M10">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ p(t) = 1 - {e^{{ - \mu \,t\;}}}. $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
        </sec>
        <sec id="Sec12">
          <title>Gamma distribution</title>
          <p>We assume that individual learning rates <italic>&#x3BC;</italic> follow a gamma distribution with density function
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ f(\mu ) = \tfrac{1}{{\Gamma (a){b^a}}}{\mu^{{a - 1}}}{e^{{ - \mu /b}}}, $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>with parameters <italic>a</italic>, <italic>b</italic> &gt; 0, where &#x393;(<italic>a</italic>) is the gamma function.</p>
        </sec>
        <sec id="Sec13">
          <title>Averaged exponential functions</title>
          <p>The recall function, which we denote as <italic>p</italic><sub><italic>A</italic></sub>(<italic>t</italic>), averages over participants that learn exponentially but with different learning rates. It is equal to the mathematical expectation of function (1) with respect to <italic>&#x3BC;</italic>:
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M12">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \eqalign{ {p_A}(t) = \int\limits_0^{\infty } {p(t)f(\mu )\,d\mu } \cr = \int\limits_0^{\infty } {\left( {1 - {e^{{ - \mu \;t}}}} \right)\tfrac{1}{{\Gamma (a){b^a}}}{\mu^{{a - 1}}}{e^{{ - \mu /b}}}d\mu } \cr = 1 - {\left( {1 + b\;t} \right)^{{ - a}}}. \cr }&lt;!endgathered&gt; $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
          <p>This proof is based on elementary probability calculus (e.g., Feller, <xref ref-type="bibr" rid="CR5">1966</xref>, p. 48) and also applies when averaging over exponential forgetting curves with shape<sub>s</sub>. A special case of this result (for <italic>a</italic> = 1 and <italic>b</italic> = 1, i.e., an exponential distribution) was derived by Killeen (<xref ref-type="bibr" rid="CR8">2001</xref>, p. 34).</p>
        </sec>
        <sec id="Sec14">
          <title>Uniform distribution</title>
          <p>This distribution has pdf
<disp-formula id="Equa"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ f{\left( \mu \right)} = \left\{ {\begin{array}{*{20}l} {{\frac{1} {{b - a}}} \hfill} &amp; {{for\;a \leqslant \mu \leqslant b,} \hfill} \\ {0 \hfill} &amp; {{for\mu &lt; a\;or\mu &gt; b.} \hfill} \\ \end{array} } \right. $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p>
          <p>If we integrate this function with the exponential individual learning curves, we obtain
<disp-formula id="Equb"><alternatives><tex-math id="M14">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {P_A}(t) = \int_a^b {\frac{{{e^{{ - \mu t}}}}}{{b - a}}d\mu = \frac{{{e^{{ - at}}} - {e^{{ - bt}}}}}{{t(b - a)}}} . $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Equb.gif" position="anchor"/></alternatives></disp-formula></p>
          <p>For <italic>a</italic> = 0, this becomes
<disp-formula id="Equc"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \frac{{1 - {e^{{ - bt}}}}}{{bt}}, $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Equc.gif" position="anchor"/></alternatives></disp-formula>which converges to 1/<italic>bt</italic> for large <italic>t</italic>. If <italic>b</italic> = 1, for increasing <italic>t</italic>, this rapidly converges to 1/<italic>t</italic>.</p>
          <p>If <italic>b</italic> &gt; <italic>a</italic> &gt; 0, and if <italic>b</italic> approaches <italic>a</italic> very closely, we obtain an exponential function. This is to be expected because in that case (nearly) all participants will have the same learning rate <italic>a</italic>:
<disp-formula id="Equd"><alternatives><tex-math id="M16">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathop{{\lim }}\limits_{{b \to a}} \frac{{{e^{{ - at}}} - {e^{{ - bt}}}}}{{t(b - a)}} = {e^{{ - at}}}. $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Equd.gif" position="anchor"/></alternatives></disp-formula></p>
        </sec>
        <sec id="Sec15">
          <title>Half-normal distribution</title>
          <p>The half-normal distribution is a normal distribution with mean 0, of which the left half (i.e., below 0) is removed and the remaining part is multiplied by 2 to retain a total probability mass of 1. The pdf uses a different parameterization from the normal distribution where the familiar parameter <italic>&#x3C3;</italic> is replaced by <inline-formula id="IEq10"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\left( {\theta \sqrt {{2/\pi }} } \right)^{{ - 1}}} $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq10.gif"/></alternatives></inline-formula> , which gives the pdf:
<disp-formula id="Eque"><alternatives><tex-math id="M18">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \matrix{ {f(\mu ) = \frac{{2\theta {e^{{\frac{{{\mu^2}{\theta^2}}}{\pi }}}}}}{\pi },} \hfill &amp; {{\hbox{for}}\,\mu \geqslant 0.} \hfill \cr }&lt;!end array&gt; $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Eque.gif" position="anchor"/></alternatives></disp-formula></p>
          <p>The half-normal distribution has mean <inline-formula id="IEq11"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \frac{1}{\theta } $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq11.gif"/></alternatives></inline-formula> and variance <inline-formula id="IEq12"><alternatives><tex-math id="M20">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \frac{{\pi - 2}}{{2{\theta^2}}} $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq12.gif"/></alternatives></inline-formula>.</p>
          <p>As above, we derive the expected value for the exponential base function:
<disp-formula id="Equf"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {P_A}(t) = \int_0^{\infty } {\frac{{2\theta {e^{{ - \frac{{{\mu^2}{\theta^2}}}{\pi }}}}}}{\pi }{e^{{ - \mu t}}}{\hbox{d}}\mu = {e^{{\frac{{\pi {t^2}}}{{4{\theta^2}}}}}}{\hbox{erfc}}\left( {\frac{{\sqrt {\pi } t}}{{2\theta }}} \right)} $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Equf.gif" position="anchor"/></alternatives></disp-formula>Here, <inline-formula id="IEq13"><alternatives><tex-math id="M22">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\hbox{erfc}}\left( {\frac{{\sqrt {\pi } t}}{{2\theta }}} \right) = \left( {1 - {\hbox{erf}}\left( {\frac{{\sqrt {\pi } t}}{{2\theta }}} \right)} \right) $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq13.gif"/></alternatives></inline-formula>, where erf(<italic>x</italic>) is the error function, which is an integral of form: <inline-formula id="IEq14"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\hbox{erf}}(x) = \frac{2}{{\sqrt {\pi } }}\int_0^x {{e^{{ - {\tau^2}}}}d\tau } $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq14.gif"/></alternatives></inline-formula>.</p>
          <p>For large <italic>t</italic>, we can derive the limit for <italic>P</italic><sub><italic>A</italic></sub>(<italic>t</italic>) as follows: We start with the following inequality (see Gautschi, <xref ref-type="bibr" rid="CR6">1965</xref>, p. 298):
<disp-formula id="Equg"><alternatives><tex-math id="M24">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \frac{1} {{{\sqrt {x^{2} + 2} } + x}} &lt; e^{{x^{2} }} {\int_x^\infty {e^{{ - \tau ^{2} }} d\tau \leqslant \frac{1} {{{\sqrt {x^{2} + \frac{4} {\pi }} } + x}}} } $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Equg.gif" position="anchor"/></alternatives></disp-formula></p>
          <p>Using
<disp-formula id="Equh"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\hbox{erfc}}(x) = \frac{2}{{\sqrt {\pi } }}\int_x^{\infty } {{e^{{ - {\tau^2}}}}d\tau, } $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Equh.gif" position="anchor"/></alternatives></disp-formula>we multiply all parts by <inline-formula id="IEq15"><alternatives><tex-math id="M26">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ 2/\sqrt {\pi } $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq15.gif"/></alternatives></inline-formula>:
<disp-formula id="Equi"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \frac{2} {{{\sqrt \pi }{\left( {{\sqrt {x^{2} + 2} } + x} \right)}}} &lt; \frac{{2e^{{x^{2} }} {\int_x^\infty {e^{{ - \tau ^{2} }} d\tau } }}} {{{\sqrt \pi }}} \leqslant \frac{2} {{{\sqrt \pi }{\left( {{\sqrt {x^{2} + \frac{4} {\pi }} } + x} \right)}}} $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Equi.gif" position="anchor"/></alternatives></disp-formula>and, thus, obtain
<disp-formula id="Equj"><alternatives><tex-math id="M28">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \frac{2} {{{\sqrt \pi }{\left( {{\sqrt {x^{2} + 2} } + x} \right)}}} &lt; e^{{x^{2} }} {\text{erfc}}{\left( x \right)} \leqslant \frac{2} {{{\sqrt \pi }{\left( {{\sqrt {x^{2} + \frac{4} {\pi }} } + x} \right)}}} $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Equj.gif" position="anchor"/></alternatives></disp-formula></p>
          <p>The derived result for the averaged curve was
<disp-formula id="Equk"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {e^{{\frac{{\pi {t^2}}}{{4{\theta^2}}}}}}{\hbox{erfc}}\left( {\frac{{\sqrt {\pi } t}}{{2\theta }}} \right), $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Equk.gif" position="anchor"/></alternatives></disp-formula>so that, if we substitute <inline-formula id="IEq16"><alternatives><tex-math id="M30">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \frac{{\sqrt {\pi } t}}{{2\theta }} $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq16.gif"/></alternatives></inline-formula> for <italic>x</italic>, we obtain
<disp-formula id="Equl"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \frac{2} {{{\sqrt \pi }{\left( {{\sqrt {\frac{{\pi t^{2} }}{{4\theta ^{2} }} + 2} } + \frac{{{\sqrt {\pi t} }}}{{2\theta }}} \right)}}} &lt; e^{{\frac{{\pi t^{2} }}{{4\theta ^{2} }}}} {\text{erfc}}{\left( {\frac{{{\sqrt \pi }t}}{{2\theta }}} \right)} \leqslant \frac{2}{{{\sqrt \pi }{\left( {{\sqrt {\frac{{\pi t^{2} }}{{4\theta ^{2} }} + \frac{4}{\pi }} } + \frac{{{\sqrt \pi }t}}{{2\theta }}} \right)}}}. $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Equl.gif" position="anchor"/></alternatives></disp-formula></p>
          <p>This can be rewritten as
<disp-formula id="Equm"><alternatives><tex-math id="M32">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \frac{{4\theta }} {{{\sqrt \pi }{\sqrt {\pi t^{2} + 8\theta ^{2} } } + \pi t}} &lt; e^{{\frac{{\pi t^{2} }} {{4\theta ^{2} }}}} {\text{erfc}}{\left( {\frac{{{\sqrt \pi }t}} {{2\theta }}} \right)} \leqslant \frac{{4\theta }} {{{\sqrt {\pi ^{2} t^{2} + 16\theta ^{2} } } + \pi t}}. $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Equm.gif" position="anchor"/></alternatives></disp-formula></p>
          <p>We can now verify the limits for large <italic>t</italic> for the left- and right-hand sides of the inequality:
<disp-formula id="Equn"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathop{{\lim }}\limits_{{t \to \infty }} \frac{{4\theta }}{{\sqrt {\pi } \sqrt {{\pi {t^2} + 8{\theta^2}}} + \pi t}} = \frac{{2\theta }}{{\pi t}} $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Equn.gif" position="anchor"/></alternatives></disp-formula>and
<disp-formula id="Equo"><alternatives><tex-math id="M34">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathop{{\lim }}\limits_{{t \to \infty }} \frac{{4\theta }}{{\sqrt {{{\pi^2}{t^2} + 16{\theta^2}}} + \pi t}} = \frac{{2\theta }}{{\pi t}}. $$\end{document}</tex-math><graphic xlink:href="13423_2011_76_Article_Equo.gif" position="anchor"/></alternatives></disp-formula></p>
          <p>We observe that both sides converge to <inline-formula id="IEq17"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \frac{{2\theta }}{{\pi t}} $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq17.gif"/></alternatives></inline-formula> and conclude that the expression itself converges to <inline-formula id="IEq18"><alternatives><tex-math id="M36">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \frac{{2\theta }}{{\pi t}} $$\end{document}</tex-math><inline-graphic xlink:href="13423_2011_76_Article_IEq18.gif"/></alternatives></inline-formula> for large <italic>t</italic>.</p>
        </sec>
      </app>
    </app-group>
  </back>
</article>
