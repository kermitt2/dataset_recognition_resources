<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/094151</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Bioinformatics</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The value of prior knowledge in machine learning of complex network systems</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Craft</surname><given-names>David</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Ferranti</surname><given-names>Dana</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Krane</surname><given-names>David</given-names></name>
</contrib>
<aff><institution>Department of Radiation Oncology, Massachusetts General Hospital, Harvard Medical School</institution></aff>
</contrib-group>
<pub-date pub-type="epub"><year>2016</year></pub-date>
<elocation-id>094151</elocation-id>
<history>
<date date-type="received">
<day>14</day>
<month>12</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>14</day>
<month>12</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2016, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2016</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="094151.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract><title>Abstract</title>
<p>Our overall goal is to develop machine learning approaches based on genomics and other relevant accessible information for use in predicting how a patient will respond to a given proposed drug or treatment. Given the complexity of this problem, we begin by developing, testing, and analyzing learning methods using data from simulated systems, which allows us access to a known ground truth. We examine the benefits of using prior system knowledge and investigate how learning accuracy depends on various system parameters as well as the amount of training data available. The simulations are based on Boolean networks &#x2013; directed graphs with 0/1 node states and logical node update rules &#x2013; which are the simplest computational systems that can mimic the dynamic behavior of cellular systems. Boolean networks can be generated and simulated at scale, have complex yet cyclical dynamics, and as such provide a useful framework for developing machine learning algorithms for modular and hierarchical networks such as biological systems in general and cancer in particular. We demonstrate that utilizing prior knowledge (in the form of network connectivity information), without detailed state equations, greatly increases the power of machine learning algorithms to predict network steady state node values (&#x201C;phenotypes&#x201D;) and perturbation responses (&#x201C;drug effects&#x201D;).</p>
</abstract>
<counts>
<page-count count="15"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1"><label>1</label><title>Introduction and motivation</title>
<p>The ability to better predict the response of a patient, regarding both intended therapeutic effect and potential toxicities, to a candidate drug, radiation, or other treatment modality, would have immediate positive consequences for human health. Currently, cancer patients are usually prescribed drugs based on their tumor type (location, histology, stage). With the advent of targeted therapies, which are designed to interact with biological pathways that are specifically altered in the patient&#x2019;s cancerous cells, tumor-specific genetic mutations are increasingly being used for drug selection [<xref ref-type="bibr" rid="c1">1</xref>]. Examples of gene/disease site pairs for which target therapies exist include HER2/breast cancer, BRAF/melanoma, and EGFR in colorectal and lung. However, even in these target cases, there is a wide spectrum of response to the drugs, which is due to the heterogeneity across patients and within tumors [<xref ref-type="bibr" rid="c2">2</xref>, <xref ref-type="bibr" rid="c3">3</xref>].</p>
<p>The problem of predicting how a patient will respond to a particular treatment can be framed as statistical machine learning problem. One can view this as a regression problem if there are quantitative measures of response or a classification problem if the response is binary, e.g. &#x201C;responders&#x201D; or &#x201C;non-responders.&#x201D; For cancer, which is linked to gene mutations, copy number variations, genomic rearrangements, and epigenetic modifications [<xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c5">5</xref>], to perform such a classification, measurements of the genomic state of the patient &#x2013; from both their healthy tissues and their malignant cells &#x2013; will serve as key inputs. Supervised machine learning attempts to find statistically valid relationships between inputs and outputs. One of the main barriers to using machine learning for genomics in healthcare is the &#x201C;large <italic>p</italic> small <italic>n</italic>&#x201D; problem [<xref ref-type="bibr" rid="c6">6</xref>]. The number of genes (<italic>p</italic>) in the human genome is on the order of 20,000, and the amount of additional information above and beyond the gene expression levels of these 20,000 genes is always growing with new assays. Meanwhile, the number of patient samples (<italic>n</italic>) for a cancer trial is typically on the order of hundreds, rarely reaching into the thousands. In situations where <italic>p</italic> &#x003E;&#x003E; <italic>n</italic>, patterns will appear in the data by chance (for example, all patients with high expressions of gene <italic>x</italic> do well on the drug) [<xref ref-type="bibr" rid="c7">7</xref>]. More generally, it is impossible to find statistically valid relationships without somehow regularizing or compressing the data.</p>
<p>Compressing genomic data into a compact signal could be done without reference to any of the underlying biology of the system that generates the data. However, domain knowledge (biology) could extract more relevant information from the signals and may turn out to be effective, and even crucial, for making high quality clinical predictions. In order to study the question of how prior knowledge can be incorporated into machine learning approaches, we use a computational simulation approach, which allows us access to a known ground truth. More specifically, we generate and simulate networks, analogous to biological systems, and use the data from these simulations to assess machine learning algorithms that predict network behavior with and without the use of knowledge about the underlying system.</p>
</sec>
<sec id="s2"><label>2</label><title>Methods</title>
<p>We use randomly generated Boolean networks [<xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c9">9</xref>] as means of producing large datasets to apply machine learning algorithms to. Boolean networks are graphs (nodes and directed arcs) with Boolean logical rules attached to each node. The logical rules update the nodes at each time step. We choose Boolean networks because they have many analogies to biochemical circuits and because they are inexpensive to simulate at large scales (one does not need to solve differential equations, for instance). The entire workflow, from the creation of random Boolean networks, to their simulation, and to the prediction problem, is depicted in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Detailed steps are explained in the following sections.</p>
<fig id="fig1" position="float"><label>Figure 1:</label>
<caption><title>Network generation, simulation, and machine learning problem workflow shown for the phenotype prediction problem (see <xref ref-type="sec" rid="s2c">section 2.3</xref>) using random Boolean networks. Further information for Step 1 is detailed in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</title>
</caption>
<graphic xlink:href="094151_fig1.tif"/></fig>
<sec id="s2a"><label>2.1</label><title>Random Boolean networks: generation and simulation</title>
<p>The steps for generating a random Boolean network on <italic>N</italic> nodes are 1) Add random directed edges between nodes (we require all networks to be fully connected) and 2) For each node, create a random Boolean rule on the incoming arcs.</p>
<p>For step 1, the graph generation step, we impose a modular and hierarchical structure to the networks in order to mimic the configuration of biological systems [<xref ref-type="bibr" rid="c10">10</xref>]. We define a four-leveled hierarchy (roughly: genes, pathways, subfunctions, and functions). We define <italic>n<sub>g</sub></italic> as the number of genes in a pathway, <italic>n<sub>p</sub></italic> as the number of pathways per subfunction, <italic>n<sub>s</sub></italic> as the number of subfunctions per function, and <italic>n<sub>f</sub></italic> as the number of functions defining the entire system. The total number of nodes (genes) in a network is <italic>N</italic> &#x003D; <italic>n<sub>f</sub>n<sub>s</sub>n<sub>p</sub>n<sub>g</sub></italic>. An example of this modular and hierarchical structured network setup is shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Once the nodes and the module groupings are defined by setting those values, we randomly add edges to the network. For the pathways, we loop through all possible pairs of nodes (directed pairs, meaning pair <italic>a</italic>, <italic>b</italic> is distinct from pair <italic>b</italic>, <italic>a</italic>) and add a directed arc with probability <italic>p<sub>g</sub></italic> (we do not allow self loops). After making all of the connections within the pathways, we loop through and select all pairs of nodes within the same next level up of the hierarchy, the subfunctions, not including the pairs of nodes that belong to the same pathway (since that pair has already been &#x201C;visited&#x201D;). We add a directed arc between two nodes at this level with a smaller probability, <italic>p<sub>p</sub></italic>. We continue this outward expansion, adding edges between nodes belonging to distinct functions, and then finally across the entire network, with probabilities <italic>p<sub>s</sub></italic> and <italic>p<sub>f</sub></italic>, with <italic>p<sub>f</sub></italic> &#x003C; <italic>p<sub>s</sub></italic> &#x003C; <italic>p<sub>p</sub></italic> &#x003C; <italic>p<sub>g</sub></italic>. Connecting genes in this way mimics the concept of pleiotropy, where a gene (protein) may have multiple functions in an organism.</p>
<fig id="fig2" position="float"><label>Figure 2:</label>
<caption><title>Modular and hierarchical network construction. For the probabilities of an arc between two nodes, we used <italic>p<sub>f</sub></italic> &#x003D; .001, <italic>p<sub>s</sub></italic> &#x003D; .005, <italic>p<sub>p</sub></italic> &#x003D; .018, and <italic>p<sub>g</sub></italic> &#x003D; .28.</title>
</caption>
<graphic xlink:href="094151_fig2.tif"/></fig>
<p>We add a special node to the baseline network which we call a super-node, which is meant to be an aggregate assessment of the state of the system. Thus the super&#x2013;node is a newly created node that has inputs from each different component of the network at a certain chosen level of the hierarchy. For example, if we choose to use the subfunction level of the hierarchy, we select one node from each of the subfunctions to pipe into the super&#x2013;node, see <xref ref-type="fig" rid="fig2">Figure 2</xref>. We choose these pipe&#x2013;in nodes by selecting a node from each component that only has incoming nodes, with the idea that a node of this type reprents some downstream indication of the state of that component. Since network simulation to steady state (described below) is the most time&#x2013;consuming part of the data generation step, but including more super&#x2013;nodes adds on only a negligible amount of simulation time, we add 100 super&#x2013;nodes to each network. Prior to the learning step, we choose 20 of these 100 supernodes to form 20 distinct learning problems from each full population simulation (see <xref ref-type="sec" rid="s2d">2.4</xref> for details on how the 20 supernodes are chosen).</p>
<p>Next we add random Boolean logic rules to each node, including the super&#x2013;nodes. Nodes may have an arbitrary number of incoming arcs. If a node has zero incoming arcs, then that node&#x2019;s initial condition is the value that persists at this node. If a node has a single incoming arc, then there are only two possibilites for a Boolean function: identity or negation. Thus for these nodes we flip a coin to choose which type we put on that node. For nodes with two inputs, we have many more options. Either input can be negated, and for combining the (possibly negated) inputs we can choose AND, OR, or XOR, and we allow the option to negate the final result as well. For nodes with more than two incoming arcs, we first choose a pair of them and apply the random Boolean logic creation rule for two nodes, as just described. This then effectively reduces the number inputs by one. If we started with <italic>K</italic> input arcs, we now have <italic>K</italic> &#x2012; 2 primitive input arcs and one input that is the pair connection Boolean statement, for a total of <italic>K</italic> &#x2012; 1 inputs. We next choose a pair from these <italic>K</italic> &#x2012; 1 inputs (note that we might get a primitive arc or a prior created compound input) and perform another pairwise connection on it, and continue until there is a single compound Boolean statement including all of the incoming arcs. An example for node with a four incoming arcs and a compound Boolean expression is shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p>
<fig id="fig3" position="float"><label>Figure 3:</label>
<caption><title>Iterative pairwise construction of a compound Boolean logical statement. In this example, the first two input nodes randomly chosen were <bold>b</bold> and <bold>d</bold>, which got connected with an <bold>AND</bold> statement. Then this compound input was further combined with another <bold>AND</bold> statement with <bold>c</bold>, which randomly got negated. This compound statement was then connected to the final primitive input <bold>a</bold> by negation on <bold>a</bold> and then an <bold>OR</bold> statement.</title>
</caption>
<graphic xlink:href="094151_fig3.tif"/></fig>
<p>With the Boolean logic in place, a network can be simulated. To begin a simulation, nodes are randomly intialized to either 0 or 1. At each time step and at each node, the node&#x2019;s next value is computed based on the current incoming node values and the Boolean statement. For each node, the result of the Boolean logic gets &#x201C;broadcast&#x201D; to all of the nodes connected to that node via its outgoing arcs. Thus, nodal values are associated with the nodes and the outgoing arcs of each node. We use synchronous updating, meaning each node is updated simultaneously at each time step. For the update, each node uses the incoming state values and its logical rule to update its own state value.</p>
<p>Since the networks have a finite number of nodes <italic>N</italic> and each node is either in state 0 or 1, there are a finite (2<sup><italic>N</italic></sup>) number of possible network states and therefore at some time the state will repeat. When such a cycle is detected, the simulation is terminated and the node state values across the cycle (the sequence of 0s and 1s that the node goes through) are averaged to produce a steady state nodal vector. If a cycle is not found after some large number of state transitions, we terminate the search and replace that member of the population with a new member (a new set of mutations).</p>
</sec>
<sec id="s2b"><label>2.2</label><title>Random Boolean networks: population creation</title>
<p>To create a population of members (which could represent cell lines, patient tumor samples, etc.) we start by creating a baseline Boolean network as described above and then, for each member of the population, we take this baseline network and put random mutations on it, thus producing a diversity of related networks. Mutations come in the following forms:</p>
<list list-type="order">
<list-item><p>If a node has no predecessors, it is mutated by changing (i.e. negating) its initial condition.</p></list-item>
<list-item><p>If a node has only a single predecessor, it is mutated by negating the rule on the incoming arc.</p></list-item>
<list-item><p>If a node has two or more predecessors then the following mutations are possible:</p>
<list list-type="bullet">
<list-item><p>The node is activated, meaning independent of the inputs the output is always 1.</p></list-item>
<list-item><p>The node is deactivated (output always 0).</p></list-item>
<list-item><p>Logic change: one of the symbols in the Boolean algebra is changed. AND gets changed to OR or XOR, etc., or an entity is negated.</p></list-item>
</list></list-item></list>
<p>To create a population, we first create a library of <italic>L</italic> mutations. For each <italic>i</italic> &#x220A; 1, 2,&#x2026;,<italic>L</italic> we select a node from the network, with replacement, and create a mutation from the above mutation type list. This serves as our mutation library. We do not allow the pipe&#x2013;in nodes nor the supernodes to be mutated. For each member of the population, we randomly select a smaller number of mutations <italic>R</italic> from the mutation library and apply those mutations (i.e. we overwrite the Boolean logic or initial conditions) to the baseline network. Once a population is created, each member is simulated until a steady state cycle is found. The steady state values are used as data for the machine learning problem.</p>
</sec>
<sec id="s2c"><label>2.3</label><title>Two learning problems: phenotype prediction and drug effect (perturbation) prediction</title>
<p>We define two learning scenarios for our network datasets. In the <italic>phenotype prediction problem</italic> we attempt to predict the steady state value of the super&#x2013;node based on the steady state value of the other nodes in the network. In the <italic>drug prediction problem</italic> we simulate a more clinically relevant scenario. We first simulate the patients in a population and record their steady state values on all nodes except the super&#x2013;nodes. This data represents the pre&#x2013;treatment patient assay, for example a gene expression profiling of their tumor. We then simulate a drug added to each of the patients by applying the same randomly chosen mutation to each of patient networks (which mimics the basic understanding of a targeted agent, where a certain gene is affected in a consistent way across the patients). We then simulate all of the patients again with this mutation and record the steady state value of the super&#x2013;node. For patients in the test set, we then try to predict the post&#x2013;drug super&#x2013;node value from the pre&#x2013;drug network steady state values. This <italic>drug prediction problem</italic> is thus once removed and leads to a harder learning problem.</p>
<p>For all problems in this work, we use classification algorithms, which requires rounding steady state values of the super&#x2013;nodes to 0 or 1. We verified that random forest and support vector machines, the best performing algorithms, are also effective in regression mode for this problem, but we focus on classification, for simplicity, and do not show any regression results.</p>
<p>For both the phenotype and the drug effect prediction problem we also investigate an idealized learning problem where the node value to predict is a Boolean function of the steady state values of the other nodes (in our normal case, super&#x2013;node values are based on the full network dynamics). This yields an easier &#x201C;pure Boolean logic&#x201D; learning problem, where we take the steady state node values, round them to 0 or 1, then apply the randomly generated Boolean logic statement to those values to form the super&#x2013;node value. This problem assess the capability of the learning algorithms to learn pure Boolean functions.</p>
</sec>
<sec id="s2d"><label>2.4</label><title>Description of baseline network and datasets</title>
<p>The parameters for the baseline network are as follows: <italic>n</italic> &#x003D; [<italic>n<sub>g</sub></italic>, <italic>n<sub>p</sub></italic>, <italic>n<sub>s</sub></italic>, <italic>n<sub>f</sub></italic>] &#x003D; [6, 5, 3, 2] and <italic>p</italic> &#x003D; [<italic>p<sub>g</sub></italic>, <italic>p<sub>p</sub></italic>, <italic>p<sub>s</sub></italic>, <italic>p<sub>f</sub></italic>] &#x003D; [.28, .018, .005, .001], see <xref ref-type="fig" rid="fig2">Figure 2</xref>. Pipe-in nodes are selected at the subfunction level, thus there are typically 6 (3 &#x00D7; 2) pipe&#x2013;in nodes out of the total of <italic>N</italic> &#x003D; 180 nodes. There will be fewer than 6 pipe&#x2013;in nodes if one or more of the subfunctions had no nodes with only incoming arcs. For the baseline case, the mutation library size is <italic>L</italic> &#x003D; 180 and the number of mutations imposed on each network sample is <italic>R</italic> &#x003D; 36 (20&#x0025; of the number of nodes).</p>
<p>For each set of network parameters (<italic>n</italic>, <italic>p</italic>, <italic>L</italic>, <italic>R</italic>) studied, including the baseline set, we generate five network topologies (directed arc connections) and for each of these network topologies we generate five different Boolean rule sets, for a total of 25 distinct networks per parameter set studied. For each of these 25 base networks, we generate 4<italic>N</italic> samples by drawing random mutations from the mutation library and applying them to the base network. Thus for the baseline case, where <italic>N</italic> &#x003D; 180, we generate and simulate to steady state 720 networks. For each network we create 100 super nodes to test prediction algorithms on. After the 4<italic>N</italic> simulations we choose the 20 supernodes that have the greatest variation among the samples (that is, we choose the supernodes that are closest to having a 50&#x2013;50 split between samples of phenotype 0 or 1).</p>
</sec>
<sec id="s2e"><label>2.5</label><title>Machine learning with and without prior knowledge for various network parameters</title>
<p>We test the following machine learning algorithms: logistic regression, lasso and elastic net regularized logistic regression, support vector machine (SVM), random forest (RF), principle component analysis (PCA) compression with RF, and nearest cluster. The regularization parameter &#x03BB; for lasso and elastic net is estimated as the largest value in a given sequence that gives a non&#x2013;null model (a null model has all the terms equal to 0) and the deviance (the model fit) is estimated using 10&#x2013;fold cross validation on the training set. For SVM, a quadratic kernel consistently outperformed the linear and radial basis function kernels and this is subsequently used in our results. For RF, we use an ensemble of 75 decision trees and each tree used a random <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="094151_inline1.gif"/></alternatives></inline-formula> predictors to make splits on, where <italic>p</italic> is the number of nodes seen by the algorithm.</p>
<p>For no prior knowledge, we apply the algorithms to the complete steady state data from all the nodes in the networks (except for the super&#x2013;nodes, which are the ones we try to predict). To demonstrate the use of prior knowledge we apply machine learning algorithms using the nodes that are suspected, based on network connectivity, to be better predictors of the super&#x2013;node values. The simplest possibility, and the one we use, is to use the nodes that are directly connected to the super&#x2013;nodes, the pipe&#x2013;in nodes. Additionally, we examine the use of patient&#x2013;specific mutation information, either alone or along with the steady state node values, in the prediction algorithms.</p>
<p>We generate and run many sets of experiments to understand how classification accuracy depends on various network parameters, including network size, number of pipe&#x2013;in nodes, mutation rate and mutation library size, all with and without the use of prior knowledge.</p>
</sec>
<sec id="s2f"><label>2.6</label><title>Univariate selection of nodes to include in the machine learning</title>
<p>In order to improve on &#x201C;no prior knowledge&#x201D; learning method, but still without utilizing prior knowledge, we study a few versions of pre&#x2013;selecting nodes from the entire set based on how well they separate the data in a univariate mode. Samples are divided into two classes based on their thresholded supernode value (either a 0 or 1). Then for these two groups, a t&#x2013;test was conducted for each node to assess if the node had any predictive power, i.e. if the steady state node value was significantly different for the two groups. P&#x2013;values for each node were ranked by importance (lowest p&#x2013;value to highest p&#x2013;value). In order to find the optimal set of nodes, we create a classification model (e.g., random forest) using the most important node, then the two most important nodes, etc. until we use all of the nodes. These models are then used on the test data and the optimal classifier from all of these <italic>N</italic> models is used as our result, which represents the best we can do for this style of univariate node selection.</p>
<p>We also run the univariate node selection strategy replacing the t&#x2013;test with a mutual information score as well as a chi&#x2013;squared score, which performed the same as the t&#x2013;test so we suppress these results.</p>
</sec>
<sec id="s2g" sec-type="availability"><label>2.7</label><title>Data and code availability</title>
<p>We make available a large set of data from this paper as well as the code base (in Matlab, version R2015b) to generate and analyze new datasets. We used native Matlab functions for all machine learning algorithms.</p>
</sec>
</sec>
<sec id="s3"><label>3</label><title>Results</title>
<p>We begin by assessing a variety of commonly used machine learning algorithms on our baseline phenotype prediction case. Random forest, support vector machine, logistic regression and the related methods lasso and elastic net, and nearest cluster, are compared in <xref ref-type="fig" rid="fig4">Figure 4a</xref>, which clearly shows the advantage of RF and SVM. We select RF as our baseline learning algorithms going forward. <xref ref-type="fig" rid="fig4">Figure 4a</xref> also displays the key result that classification accuracy improves when the algorithms use information from only the pipe&#x2013;in nodes instead of all of the nodes. This is a direct demonstration of the value of prior knowledge, a consistent theme throughout our results. The p&#x2013;value for this comparison, from a paired t&#x2013;test, is &#x003C; .001. We get similarly small p&#x2013;values for every comparison of prior knowledge vs. no prior knowledge, and also for the comparisons between any two algorithms, thus we do not continually report p&#x2013;values. In fact, for all comparisons between two groups, whether paired (when the results come from the same set of networks) or unpaired, we achieve significant differences due to our large sample sizes, 500, even when the differences are not practically significant. For this reason, we opt to report <italic>effect size</italic>, which incorporates the magnitude of the difference in means (mean classification accuracy in our case) of the populations [<xref ref-type="bibr" rid="c11">11</xref>]. We opt for the specific version of effect size called Common Language Effect Size (CLES) [<xref ref-type="bibr" rid="c12">12</xref>]. CLES gives the probability that a random draw from one group will exceed a random draw from the other group, and is defined for non&#x2013;paired group testing. For paired tests, we do not report the vanishingly small p&#x2013;values throughout; for unpaired testing, we report the CLES probabilities.</p>
<fig id="fig4" position="float"><label>Figure 4:</label>
<caption><title>Algorithm comparison on the set of 25 baseline 180&#x2013;node networks, each with 20 distinct supernodes (20 prediction problems per network) for a total of 500 learning problems. Each dataset contains 720 samples. (a) Box&#x2013;whisker plots for classification accuracy for various algorithms for all nodes (solid) and just pipe&#x2013;in nodes (dashed). (b) Using RF, we compare no prior knowledge to various types of prior knowledge: no prior knowledge baseline, no prior knowledge but using t&#x2013;test on the training set data to identify predictive nodes and then using just those nodes in the random forest algorithm, prior knowledge in the form of PCA compression of the data from each subfunction module (which requires network connectivity knowledge) into two principle components, mutation knowledge only, mutation knowledge in addition to baseline steady&#x2013;state data from the 180 nodes, mutation knowledge plus steady&#x2013;state data from just pipein nodes, and the pipe&#x2013;in node result. For visual comparison, the first and last box&#x2013;whisker plots are repeats from the first two box&#x2013;whisker plots in (a).</title>
</caption>
<graphic xlink:href="094151_fig4.tif"/></fig>
<p><xref ref-type="fig" rid="fig4">Figure 4b</xref> displays that node selection by t&#x2013;test (similar results for chi&#x2013;square and mutual information univariate node selection, results not shown) improves the &#x201C;no&#x2013;prior knowledge&#x201D; learning technique, but not as much as prior knowledge node selection (more results of the t&#x2013;test selection method are discussed below). PCA compression of the subfunction node sets steady state values apparently results in a loss of useful information and performs poorly. We also see that using binary information of which mutations a patient has, while offering some predictive value by itself, adds noise and thus weakens the performance of the learning algorithms when node steady state information is available. Similarly, using additional network connectivity knowledge of which nodes are directly connected to the pipe&#x2013;in nodes weakens the strong signal of the pipe&#x2013;in nodes and thus makes classification accuracy drop (results not shown).</p>
<p>We explore the t&#x2013;test node selection strategy in more detail in <xref ref-type="fig" rid="fig5">Figure 5</xref>. In 5(a), the solid curve shows that there is an optimal number of nodes to use when using a t&#x2013;test selection strategy, in this case around 18. The dashed curve shows that the t&#x2013;test only gradually picks out the pipe&#x2013;in nodes. If the t&#x2013;test idea worked perfectly, the pipe&#x2013;in node would always be the top ranked by the t&#x2013;test, but we see that there is a long tail where, if the pipe&#x2013;in node is not ranked in the top few nodes, than it could appear anywhere in the ranked order, <xref ref-type="fig" rid="fig5">Figure 5b</xref>.</p>
<fig id="fig5" position="float"><label>Figure 5:</label>
<caption><title>Selecting nodes to use based on a univariate t-test. Nodes are sorted based on their p&#x2013;value from the t&#x2013;test which assesses how useful they are in the classification problem. The solid line in a) shows the overall accuracy of the random forest algorithm as nodes are successively added in from the sorted order, accuracy peaking around 18 nodes. The dashed line shows the average number of pipe&#x2013;in nodes that have been selected by the t&#x2013;test method for the given number of total nodes added. The linear increase of this curve until it reaches 1 demonstrates that the t&#x2013;test method does not reliably uncover all of the pipe&#x2013;in nodes, which is further emphasized in b), a histogram which shows at the node set sub&#x2013;function level (the place in the hierarchy for our baseline network where pipe&#x2013;in nodes are selected from) what ranking the pipe&#x2013;in nodes are in the t&#x2013;test: less than 25&#x0025; of the time a pipe&#x2013;in node is the top ranking node from the relevant node set.</title>
</caption>
<graphic xlink:href="094151_fig5.tif"/></fig>
<p><xref ref-type="fig" rid="fig6">Figure 6</xref> demonstrates the correlation between area under the receiver operating characteristic curve (AUC), a standard measure of performance of classification algorithms, and classification accuracy (CA, which is fraction of super&#x2013;nodes classified correctly). Since they correlate well and since CA is cheaper to compute, we use CA throughout to compare algorithms.</p>
<fig id="fig6" position="float"><label>Figure 6:</label>
<caption><title>Correlation of AUC and classification accuracy for 500 learning problems of the baseline 180 node networks for (a) all nodes and (b) pipe&#x2013;in nodes (prior knowledge).</title>
</caption>
<graphic xlink:href="094151_fig6.tif"/></fig>
<p>With RF and CA established as our learning algorithm and performance measure we look at the phenotype and the drug effect prediction problems, for full dynamic network generated data and for datasets generated from the steady state nodal values. These results, shown in <xref ref-type="table" rid="tbl1">Table 1</xref>, verify that the drug effect prediction problem is slightly harder, and if the underlying Boolean function is based on steady state data rather than the network dynamics data, the problem is slightly easier. Noteably, all problems are of overall similar difficulty. We choose the phenotype prediction problem with actual network dynamics as our baseline problem type to investigate further.</p>
<table-wrap id="tbl1" position="float"><label>Table 1:</label>
<caption><p>Four learning problems compared using classification accuracy [mean (standard deviation)]. Phenotype prediction attempts to predict the steady state value of a single network node based on access to the steady state values of the other nodes. The drug effect problem is the two step network simulation and prediction problem as described in the text. The right column repeats these experiments for an idealized setting where the node values to predict are functions of the steady state results rather than functions of the actual system dynamics. The steady state idealization is meant to test the power of the the random forest algorithm to uncover pure but complex Boolean logical rules.</p></caption>
<graphic xlink:href="094151_tbl1.tif"/>
</table-wrap>
<p><xref ref-type="table" rid="tbl2">Table 2</xref> summarizes how the learning problem depends on various network parameters. For all results, we compare no prior knowledge (&#x201C;all nodes&#x201D;) to prior knowledge (&#x201C;pipe&#x2013;in nodes&#x201D;) and see that using prior network knowledge consistently wins over no knowledge. Network size in our setting does not greatly affect the difficulty of the learning problem. CLES for network sizes of 90 nodes vs 180 nodes is 64&#x0025;. Thus, 64&#x0025; of the time a randomly selected 90 node network dataset has a higher CA than a 180 network dataset (this value is the same for all nodes and for pipe&#x2013;in nodes). The 180 node networks compared to the 800 node networks are even less distinguishable, with CLES &#x003D; 60&#x0025;.</p>
<table-wrap id="tbl2" position="float"><label>Table 2:</label>
<caption><p>Classification accuracy [mean (standard deviation)] of random forest algorithm for various network populations. For ease of comparison, the baseline case (&#x00B7;) is repeated for each bundle. For network size runs, the number in brackets is the number of pipe&#x2013;in nodes.</p></caption>
<graphic xlink:href="094151_tbl2.tif"/>
</table-wrap>
<p>As the number of pipe&#x2013;in nodes increases from 2 to 6, the learning problem gets more difficult (CLES &#x003D; 70&#x0025; for all nodes, 85&#x0025; for pipe&#x2013;in nodes learning), as expected. From 6 to 30 pipe&#x2013;in nodes has much less an effect, with CLES 55&#x0025; and 59&#x0025; for all and pipe&#x2013;in, respectively.</p>
<p>In the limit as the mutation rate approaches 0, all the members of the population would be the same thus leading to a trivial learning problem. However, regarding mutation rate and mutation library size, we do not see large effects. The only CLES scores for mutation rate that exceed 70&#x0025; are for all nodes, mutation rate of 10&#x0025; vs 20&#x0025; (20&#x0025; mutation rate is harder to classify). For mutation library size <italic>L</italic>, only the jump from <italic>L</italic> &#x003D; 90 to <italic>L</italic> &#x003D; 180 gives a CLES over 70&#x0025;: both all and pipe&#x2013;in nodes learning give CLES &#x003D; 76&#x0025;. Thus in all cases where we expect the learning problem to get more difficult with increasing (network size, number of pipe&#x2013;in nodes, mutation rate, and mutation library size) we see a stronger effect initially but a saturation of difficulty to somewhere between 75&#x0025; and 80&#x0025;.</p>
<p><xref ref-type="fig" rid="fig7">Figure 7</xref> shows how classification accuracy increases with training set size (this is all paired data, all results p&#x2013;values &#x003C; .001), for both uninformed learning and prior knowledge learning. In particular, for this dataset we see that the uninformed learning requires a dataset size of 50&#x0025; of our full patient sample (360 samples out of 720) to reach the same accuracy as prior knowledge&#x2013;based learning reaches with only 5&#x0025; (36 samples) training data size.</p>
<fig id="fig7" position="float"><label>Figure 7:</label>
<caption><title>Classification accuracy for the baseline networks for various sizes of the training set. Sizes are percentages of the number of samples we generate in total (720 for the baseline networks).</title>
</caption>
<graphic xlink:href="094151_fig7.tif"/></fig>
<p>In order to compare further the value of additional training data versus the value of prior knowledge, we run RF on the 500 baseline datasets for training set sizes from <italic>i</italic> &#x003D; 10&#x0025; (72 samples) up to 90&#x0025; in increments of 10&#x0025; assuming no prior knowledge (i.e. using all the network nodes). To mimic partial prior knowledge, we assume we know only <italic>j</italic> of the pipe&#x2013;in nodes, and vary <italic>j</italic> from 1 to all of them, which is 6 for the baseline networks. For each <italic>j</italic> &#x2264; 6 and for each learning run, we randomly select <italic>j</italic> of the 6 actual pipe&#x2013;in nodes to perform the learning problem with. For this prior knowledge investigation, we assume we are training on 10&#x0025; of the data (the default training set size). For each <italic>i</italic>, <italic>j</italic> pair, we can compute the average classification accuracy for the no prior knowledge <italic>i</italic>th run and the prior knowledge <italic>j</italic>th run, and form the difference CA(<italic>j</italic>) &#x2013; CA(<italic>i</italic>). Plotting these differences as a 2D matrix gives us a visual comparison of the value of each type of information, see <xref ref-type="fig" rid="fig8">Figure 8</xref>. The top left of this figure demonstrates that prior knowledge with a 10&#x0025; training set size dominates the no prior knowledge machine learning of up to 50&#x0025; training size. We see a sharp drop in classification accuracy as prior knowledge, in this case the number of pipe&#x2013;in nodes that are known, decreases. This explains why the t&#x2013;test method, <xref ref-type="fig" rid="fig5">Figure 5</xref>, while improving on the na&#x00EF;ve no prior knowledge method, is still inferior to full knowledge of the pipe&#x2013;in nodes: the t&#x2013;test does not reliably select out the pipe&#x2013;in nodes, and not having all the pipe&#x2013;in nodes when learning with only 10&#x0025; of the data, as seen in <xref ref-type="fig" rid="fig8">Figure 8</xref>, is greatly detrimental.</p>
<fig id="fig8" position="float"><label>Figure 8:</label>
<caption><title>The value of prior knowledge versus the value of additional training data for the baseline networks. The color values indicate (classification accuracy of prior knowledge approach) &#x2013; (classification accuracy of no&#x2013;prior knowledge approach). Note that for the prior knowledge runs, 10&#x0025; training size was used throughout. Full prior knowledge (knowing the six pipe&#x2013;in nodes in this case) and having a training dataset size of 10&#x0025; (of the total population size simulated, which is 720 for the base case, thus 72 samples) gives a classification accuracy on par with having five times as much data (50&#x0025;) and no prior knowledge.</title>
</caption>
<graphic xlink:href="094151_fig8.tif"/></fig>
</sec>
<sec id="s4"><label>4</label><title>Discussion and conclusions</title>
<p>Since genomic data has become widely &#x2013; and increasingly cheaply &#x2013; available there has been much effort to analyze it to discover (or, as is often the case, rediscover) biological mechanisms and to advance medical practice. More often than not, genomic data is analyzed as an independent data problem, and no inputs from known biology are used to regularize the data. A common result of such papers is &#x201C;our methods uncover the genes <italic>x</italic> and <italic>y</italic> that are known to be important in this context, but also reveal gene <italic>z</italic>, which has not previously been implicated in this setting.&#x201D; This type of analysis begs the question: if we already knew that genes <italic>x</italic> and <italic>y</italic> were important, and they were somehow put into the method up front, perhaps more information could be extracted from the data. Stated another way, if we use known biology, rather than ignoring it, perhaps we can increase the power of our machine learning methods. Statistical machine learning should complement findings from wet&#x2013;lab biology, not compete with it.</p>
<p>We demonstrate for idealized systems loosely analogous to biological systems that incorporating prior knowledge greatly increases the ability of machine learning algorithms to make predictions of the system behavior. This indicates that it will be prudent to investigate how to incorporate biological knowledge into machine learning algorithms. Our work suggests that identifying signals (e.g. proteins) that are downstream in a pathway, and offer an indication of whether or not the pathway is functioning, could be a more useful signal for the machine learning than the complete set of signals. However, we stress that our work is suggestive rather than immediately practically applicable, and which are the best signals to focus on for particular biological contexts remains to be investigated.</p>
<p>The notion of prior knowledge is a vague one. In our case, the knowledge of which nodes pipe into the nodes we are trying to predict worked well whereas PCA compression of module level steady state data did not. We speculate that pathway&#x2013;level data compression has been shown to be effective for real genomic datasets [<xref ref-type="bibr" rid="c13">13</xref>, <xref ref-type="bibr" rid="c14">14</xref>, <xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c16">16</xref>, <xref ref-type="bibr" rid="c17">17</xref>] because the pathways chosen are intrinsically selecting the genes known to show errant behavior in cancer, for example. In theoretical studies like ours, and in moving this type of thinking to practice, it will be interesting to further investigate various forms of prior knowledge, both more complete (for example, knowledge of state equations) and less complete (less certainty about the pipe&#x2013;in nodes, or networks where the super&#x2013;node wiring is more complex). We make the datasets and code to generate them freely available for researchers to test their methods on, which we hypothesize will only strengthen our claim of the value of prior knowledge.</p>
<p>While we considered constructing our networks to mimic known signaling and metabolic pathways in cancer [<xref ref-type="bibr" rid="c18">18</xref>], we opted for random networks to ensure hard learning problems and to avoid only representing canonical cancer pathways. Nevertheless, we think it would be interesting to try learning algorithms on curated Boolean and non&#x2013;Boolean cancer network reconstructions, which there are many of, e.g. cellcollective.org. Fumi&#x00E3; <italic>et al</italic> [<xref ref-type="bibr" rid="c18">18</xref>] show that different initial state vectors lead to qualitively different phenotypic states (attractors) of their networks (apoptotic &#x2013; active caspases, immortalized &#x2013; active hTERT, migratory &#x2013; inactive E cadherin, etc.). We ran the same networks from different initial conditions and did not observe such initial state dependent behavior, likely due to the random rules and wiring of our networks as opposed to networks tuned for distinct functions and tuned over millions of years of evolution.</p>
<p>Random forest and non&#x2013;linear support vector machines are known to be able to learn complex rules, including Boolean logic [<xref ref-type="bibr" rid="c19">19</xref>]. Since biological systems have inherent Boolean logic embedded in them (consider the need for the use of IF, AND, OR, NOT, etc. in the description of any biochemical process) [<xref ref-type="bibr" rid="c20">20</xref>], improved prediction of their behavior will require methods that can represent Boolean logic. RF and SVM methods outperform the more standard statistical learning techniques, including lasso and elastic net, that do not consider complex interactions amongst the input variables, <xref ref-type="fig" rid="fig4">Figure 4</xref>. Clustering methods are also often used to analyze genomic data, but our results indicate that Boolean logic is better learned with RF and SVM methods. Of course, if enough data were available such that all sample types were represented, clustering methods could become a workable choice.</p>
<p>We show that algorithms that are able to find combinatorial patterns are aided by pre&#x2013;selection of important variable using non&#x2013;combinatorial techniques. Choosing network nodes which independently have a high predictive value for the learning problem (assessed by univariate t&#x2013;test, mutual information, or chi&#x2013;squared methods, which all produced similar results) and using only these nodes in the RF algorithm increases the overall predictive capability, <xref ref-type="fig" rid="fig4">Figure 4</xref>. However, univariate selection, since it fails to select all the pipe-in nodes, cannot compete with prior knowledge. We see that t&#x2013;test selection often selects nodes that, while apparently useful for the classification problem, are likely spurious correlations, whereas the nodes that are actually useful for classification, the pipe&#x2013;in nodes, must often be only useful in combination with the rest of the pipe&#x2013;in nodes, and thus not ranked highly by the t&#x2013;test approach.</p>
<p>While our main result is on the usefulness of prior knowledge, we also study how the difficulty of the learning problem depends on network parameters, notably network size, mutation rate, and number of pipe&#x2013;in nodes. The trend that we observe is that as these parameters increase, the problem gets harder, but for all three of these there is a point after which the problem seems to get easier. This result, while statistically significant regarding a p&#x2013;test, is less so if interpreted with the CLES score. Nonetheless, examining the underlying data shows that as mutation rate and number of pipe&#x2013;in nodes increased past the baseline values, the variance in the super&#x2013;node values to predict decreases (there is less of a 50-50 split), which may explain the easier learning problem. We observe the same dip when we analyze the data using AUC instead of CA.</p>
<p>Many custom algorithms have been written to interpret and classify genetic data, including approaches using traditional techniques such as logistic regression and attempts using more modern methods, for example matrix factorization as reviewed in [<xref ref-type="bibr" rid="c21">21</xref>]. The most relevant to our work are methods which utilize the underlying network structure of the data, such as [<xref ref-type="bibr" rid="c22">22</xref>, <xref ref-type="bibr" rid="c23">23</xref>], however these methods make Gaussian assumptions which we wanted to avoid, and these methods are also unsupervised, which is not the problem we study. More importantly, none of the methods to our knowledge use or assess the value of prior knowledge (with the exception of methods that use gene sets and pathways, which is an implicit form of prior knowledge, as already discussed), the main contribution of our work. Assuming that predicting the behavior of complex Boolean networks provides an analogy for predicting biological systems, this paper gives a strong indication that incorporating information about the system being learned in the machine learning process will yield substantial accuracy improvements.</p>
</sec>
</body>
<back>
<ack><title>Acknowledgement</title>
<p>The authors thank Josh Reed for helpful discussions and explorations of the various methods we used.</p>
</ack>
<ref-list><title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><given-names>Matthew</given-names> <surname>Holderfield</surname></string-name>, <string-name><given-names>Marian M</given-names> <surname>Deuker</surname></string-name>, <string-name><given-names>Frank</given-names> <surname>McCormick</surname></string-name>, and <string-name><given-names>Martin</given-names> <surname>McMahon</surname></string-name>. <article-title>Targeting RAF kinases for cancer therapy: BRAF mutated melanoma and beyond. Nature reviews</article-title>. <source>Cancer</source>, <volume>14</volume>(<issue>7</issue>):<fpage>455</fpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><given-names>Nicholas</given-names> <surname>McGranahan</surname></string-name> and <string-name><given-names>Charles</given-names> <surname>Swanton</surname></string-name>. <article-title>Biological and therapeutic impact of intratumor heterogeneity in cancer evolution</article-title>. <source>Cancer cell</source>, <volume>27</volume>(<issue>1</issue>):<fpage>15</fpage>&#x2013;<lpage>26</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><given-names>Min</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>Aijun</given-names> <surname>Shen</surname></string-name>, <string-name><given-names>Jian</given-names> <surname>Ding</surname></string-name>, and <string-name><given-names>Meiyu</given-names> <surname>Geng</surname></string-name>. <article-title>Molecularly targeted cancer therapy: some lessons from the past decade</article-title>. <source>Trends in pharmacological sciences</source>, <volume>35</volume>(<issue>1</issue>):<fpage>41</fpage>&#x2013;<lpage>50</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><given-names>Bert</given-names> <surname>Vogelstein</surname></string-name>, <string-name><given-names>Nickolas</given-names> <surname>Papadopoulos</surname></string-name>, <string-name><given-names>Victor E</given-names> <surname>Velculescu</surname></string-name>, <string-name><given-names>Shibin</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>Luis A</given-names> <surname>Diaz</surname></string-name>, and <string-name><given-names>Kenneth W</given-names> <surname>Kinzler</surname></string-name>. <article-title>Cancer genome landscapes</article-title>. <source>science</source>, <volume>339</volume>(<issue>6127</issue>):<fpage>1546</fpage>&#x2013;<lpage>1558</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><given-names>Sweta</given-names> <surname>Mishra</surname></string-name> and <string-name><given-names>Johnathan R</given-names> <surname>Whetstine</surname></string-name>. <article-title>Different facets of copy number changes: permanent, transient, and adaptive</article-title>. <source>Molecular and cellular biology</source>, <volume>36</volume>(<issue>7</issue>):<fpage>1050</fpage>&#x2013;<lpage>1063</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><given-names>JG</given-names> <surname>Liao</surname></string-name> and <string-name><given-names>Khew-Voon</given-names> <surname>Chin</surname></string-name>. <article-title>Logistic regression for disease classification using microarray data: model selection in a large p and small n case</article-title>. <source>Bioinformatics</source>, <volume>23</volume>(<issue>15</issue>):<fpage>1945</fpage>&#x2013;<lpage>1951</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><given-names>Anastasia</given-names> <surname>Chalkidou</surname></string-name>, <string-name><given-names>Michael J</given-names> <surname>ODoherty</surname></string-name>, and <string-name><given-names>Paul K</given-names> <surname>Marsden</surname></string-name>. <article-title>False discovery rates in PET and CT studies with texture features: a systematic review</article-title>. <source>PloS one</source>, <volume>10</volume>(<issue>5</issue>):<fpage>e0124165</fpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><given-names>L.</given-names> <surname>Raeymaekers</surname></string-name>. <article-title>Dynamics of boolean networks controlled by biologically meaningful functions</article-title>. <source>Journal of Theoretical Biology</source>, <volume>218</volume>(<issue>3</issue>):<fpage>331</fpage>&#x2013;<lpage>341</lpage>, <year>2002</year>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><given-names>I.</given-names> <surname>Shmulevich</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Dougherty</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Kim</surname></string-name>, and <string-name><given-names>W.</given-names> <surname>Zhang</surname></string-name>. <article-title>Probabilistic boolean networks: a rule-based uncertainty model for gene regulatory networks</article-title>. <source>Bioinformatics</source>, <volume>18</volume>(<issue>2</issue>):<fpage>261</fpage>&#x2013;<lpage>274</lpage>, <year>2002</year>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><given-names>Albert-Laszlo</given-names> <surname>Barabasi</surname></string-name> and <string-name><given-names>Zoltan N</given-names> <surname>Oltvai</surname></string-name>. <article-title>Network biology: understanding the cell&#x2019;s functional organization</article-title>. <source>Nature reviews genetics</source>, <volume>5</volume>(<issue>2</issue>):<fpage>101</fpage>&#x2013;<lpage>113</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><given-names>Gail M</given-names> <surname>Sullivan</surname></string-name> and <string-name><given-names>Richard</given-names> <surname>Feinn</surname></string-name>. <article-title>Using effect size-or why the P value is not enough</article-title>. <source>Journal of graduate medical education</source>, <volume>4</volume>(<issue>3</issue>):<fpage>279</fpage>&#x2013;<lpage>282</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><given-names>Kenneth O</given-names> <surname>McGraw</surname></string-name> and <string-name><given-names>SP</given-names> <surname>Wong</surname></string-name>. <article-title>A common language effect size statistic</article-title>. <source>Psychological bulletin</source>, <volume>111</volume>(<issue>2</issue>):<fpage>361</fpage>, <year>1992</year>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><given-names>Michael R</given-names> <surname>Young</surname></string-name> and <string-name><given-names>David L</given-names> <surname>Craft</surname></string-name>. <article-title>Pathway-informed classification system (pics) for cancer analysis using gene expression data</article-title>. <source>Cancer Informatics</source>, <volume>15</volume>:<fpage>151</fpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Drier</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Sheffer</surname></string-name>, and <string-name><given-names>E.</given-names> <surname>Domany</surname></string-name>. <article-title>Pathway-based personalized analysis of cancer</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>110</volume>(<issue>16</issue>):<fpage>6388</fpage>&#x2013;<lpage>6393</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Vaske</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Benz</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Sanborn</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Earl</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Szeto</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Zhu</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Haussler</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Stuart</surname></string-name>. <article-title>Inference of patient-specific pathway activities from multi-dimensional cancer genomics data using PARADIGM</article-title>. <source>Bioinformatics</source>, <volume>26</volume>(<issue>12</issue>):<fpage>i237</fpage>&#x2013;<lpage>i245</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Tarca</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Draghici</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Khatri</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Hassan</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Mittal</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Kusanovic</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Romero</surname></string-name>. <article-title>A novel signaling pathway impact analysis</article-title>. <source>Bioinformatics</source>, <volume>25</volume>(<issue>1</issue>):<fpage>75</fpage>&#x2013;<lpage>82</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><given-names>Shinuk</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>Mark</given-names> <surname>Kon</surname></string-name>, and <string-name><given-names>Charles</given-names> <surname>DeLisi</surname></string-name>. <article-title>Pathway-based classification of cancer subtypes</article-title>. <source>Biology Direct</source>, <volume>7</volume>(<issue>1</issue>):<fpage>21</fpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><given-names>Herman F</given-names> <surname>Fumi&#x00E3;</surname></string-name> and <string-name><given-names>Marcelo L</given-names> <surname>Martins</surname></string-name>. <article-title>Boolean network model for cancer pathways: predicting carcinogenesis and targeted therapy outcomes</article-title>. <source>PloS one</source>, <volume>8</volume>(<issue>7</issue>):<fpage>e69008</fpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="book"><string-name><given-names>Ken</given-names> <surname>Sadohara</surname></string-name>. <chapter-title>Learning of boolean functions using support vector machines</chapter-title>. In <source>International Conference on Algorithmic Learning Theory</source>, pages <fpage>106</fpage>&#x2013;<lpage>118</lpage>. <publisher-name>Springer</publisher-name>, <year>2001</year>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><given-names>Rui-Sheng</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Assieh</given-names> <surname>Saadatpour</surname></string-name>, and <string-name><given-names>Reka</given-names> <surname>Albert</surname></string-name>. <article-title>Boolean modeling in systems biology: an overview of methodology and applications</article-title>. <source>Physical biology</source>, <volume>9</volume>(<issue>5</issue>):<fpage>055001</fpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><given-names>Andrew V</given-names> <surname>Kossenkov</surname></string-name> and <string-name><given-names>Michael F</given-names> <surname>Ochs</surname></string-name>. <article-title>Matrix factorisation methods applied in microarray data analysis</article-title>. <source>International journal of data mining and bioinformatics</source>, <volume>4</volume>(<issue>1</issue>):<fpage>72</fpage>&#x2013;<lpage>90</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><given-names>Safiye</given-names> <surname>Celik</surname></string-name>, <string-name><given-names>Benjamin A</given-names> <surname>Logsdon</surname></string-name>, and <string-name><given-names>Su-In</given-names> <surname>Lee</surname></string-name>. <article-title>Efficient dimensionality reduction for high-dimensional network estimation</article-title>. In <source>ICML</source>, pages <fpage>1953</fpage>&#x2013;<lpage>1961</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><given-names>Safiye</given-names> <surname>Celik</surname></string-name>, <string-name><given-names>Benjamin A</given-names> <surname>Logsdon</surname></string-name>, <string-name><given-names>Stephanie</given-names> <surname>Battle</surname></string-name>, <string-name><given-names>Charles W</given-names> <surname>Drescher</surname></string-name>, <string-name><given-names>Mara</given-names> <surname>Rendi</surname></string-name>, <string-name><given-names>R David</given-names> <surname>Hawkins</surname></string-name>, and <string-name><given-names>Su-In</given-names> <surname>Lee</surname></string-name>. <article-title>Extracting a low-dimensional description of multiple gene expression datasets reveals a potential driver for tumor-associated stroma in ovarian cancer</article-title>. <source>Genome Medicine</source>, <volume>8</volume>(<issue>1</issue>):<fpage>1</fpage>, <year>2016</year>.</mixed-citation></ref>
</ref-list>
</back>
</article>