<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/274894</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>We need to talk about reliability: Making better use of test-retest studies for study design and interpretation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5646-4547</contrib-id>
<name>
<surname>Matheson</surname>
<given-names>Granville J.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Center for Psychiatry Research, Department of Clinical Neuroscience, Karolinska Institutet &#x0026; Stockholm Health Care Services</institution>, Stockholm County Council, Stockholm, <country>Sweden</country>.</aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>1</label>Corresponding author: Granville J. Matheson, Email address: <email>granville.matheson@ki.se</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2018</year>
</pub-date>
<elocation-id>274894</elocation-id>
<history>
<date date-type="received">
<day>02</day>
<month>3</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>02</day>
<month>3</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>06</day>
<month>3</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="274894.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>ABSTRACT</title>
<p>Positron emission tomography (PET), along with many other fields of clinical research, is both time-consuming and expensive, and recruitable patients can be scarce. These constraints limit the possibility of large-sample experimental designs, and often lead to statistically underpowered studies. This problem is exacerbated by the use of outcome measures whose accuracy is sometimes insufficient to answer the scientific questions posed. Reliability is usually assessed in validation studies using healthy participants, however these results are often not easily applicable to clinical studies examining different populations. I present a new method and tools for using summary statistics from previously published test-retest studies to approximate the reliability of outcomes in new samples. In this way, the feasibility of a new study can be assessed during planning stages, and before collecting any new data. An R package called relfeas also accompanies this article for performing these calculations. In summary, these methods and tools will allow researchers to avoid performing costly studies which are, by virtue of their design, unlikely to yield informative conclusions.</p>
</abstract>
<counts>
<page-count count="15"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>INTRODUCTION</title>
<p>In the assessment of individual differences, reliability is typically assessed using test-retest reliability, inter-rater reliability or internal consistency. Reliability is a measure of the accuracy or consistency of an outcome, the distinguishability of individual measurements, and a measure of the signal-to-noise ratio in a set of data. It is an approximation of the fraction of the total variance which is not accounted for by measurement error (i.e. random noise in the measurement). Accordingly, a reliability of 1 means that all variability is attributable to true differences and there is no measurement error, while a reliability of 0 means that all variability is accounted for by measurement error. A reliability of 0.5 means that there is equal measurement- and error-related variance: this means that an individual obtaining a score equal to the mean of the group <italic>could</italic> have the highest, or the lowest, underlying true value of the outcome in the group. Reliability can therefore be increased either by reducing the measurement error, or by increasing the amount of true interindividual variability in the sample such that measurement error is proportionally smaller.</p>
<p>In clinical research, most statistical inference is performed using the null hypothesis significance testing (NHST) paradigm. A result is considered significant when the p value is less than the prespecified alpha threshold, and the null hypothesis is rejected. In this paradigm, study design is performed by considering the risk of type I and type II errors. In practice, there is a great deal of attention given to the minimisation of type I errors, i.e. false positives. This usually takes the form of correction for multiple comparisons. Another important consideration is the minimisation of type II errors, i.e. false negatives. This takes the form of power analysis: reasoning about the number of participants to include in the study based on the effect size of interest. In contrast, comparatively little consideration is usually given to the reliability of the measures to be used in the study. The reliability of outcome measures limits the range of effect sizes which can be expected, which is vital information for study design. Lower reliability of outcome measures leads to diminished power, or alternatively an increase in the risk of type M (magnitude) and type S (sign) errors (<xref ref-type="bibr" rid="c13">Gelman and Carlin 2014</xref>). In extreme cases, it can lead to what has been called type III error, defined as obtaining the right answer, but to the wrong question (<xref ref-type="bibr" rid="c18">Kimball 1957</xref>): if measured values are too dissimilar to the underlying &#x2018;true&#x2019; values, then a statistical test will not be inferring on meaningful outcomes but simply random noise.</p>
<p>Assessment of reliability is critical both for study design and interpretation. However it is also dependent on characteristics of the sample: we can easily use a bathroom scale to contrast the weight of a feather and a brick. In all cases, the scale will correctly indicate that the brick weighs more than the feather. However, we cannot conclude from these results that this bathroom scale can reliably measure the weight of bricks, and proceed to use it to examine the subtle drift in weight between individual bricks produced by a brick factory. In this way, the reliability of a measure is calibrated to the inter-individual differences in that sample. In psychometrics, reliability is often assessed using internal consistency: this involves examining the similarity of the responses between individual items of the scale, compared to the total variability in scores within the sample. This means that the reliability of the scale can be estimated using only the data from a single completion of the scale by each participant. However, for most clinical/physiological measures, estimation of reliability by internal consistency is not possible, as the measurement itself cannot be broken down into smaller representative parts. For these measures, reliability can only be assessed using test-retest studies. This means that measurements are made twice on a set of individuals, and the inter- and intra-individual variability are compared to determine the reliability. If the variability of the outcome is similar in the test-retest and applied studies, it can reasonably be assumed that the measure will be equally reliable in both studies.</p>
<p>When examining clinical patient samples however, it often cannot be assumed that these samples are similar to that of the test-retest study. One solution is to perform a new test-retest study in a representative sample. However, for outcome measures which are invasive or costly, it is usually not feasible to perform test-retest studies using every clinical sample which might later be examined. Positron emission tomography (PET), which allows imaging of in-vivo protein concentrations or metabolism, is both invasive and costly: participants are injected with harmful radioactivity, and a single measurement can cost upwards of USD 10 000. In PET imaging, it is usually only young, healthy men who are recruited for test-retest studies. These samples can be expected to exhibit low measurement error, but may also be expected to show limited inter-individual variability. Despite reporting of reliability in test-retest studies being common practice, when the reported reliability is low, little consideration can given to these results on the basis of insufficient inter-individual variability, i.e. it is assumed that there will be more variation in clinical comparison studies and that the reliability does not accurately reflect this. This is certainly true in some circumstances. However, when it is not true, it can lead to the design of problematic studies whose ability to yield biologically meaningful conclusions is greatly limited. This is costly both in time and resources for researchers, and leads to the needless exposure of participants to radiation in PET research. It is therefore important to approximate the reliability of a measure for the sample of interest before data collection begins for studies investigating individual differences.</p>
<p>In this paper, I present how the reliability can be used for study design, and introduce a new method for roughly approximating the reliability of an outcome measure for new samples based on the results of previous test-retest studies. This method uses only the reliability and summary statistics from previous test-retest studies, and does not require access to the raw data. Further, this method allows for calculation of the characteristics of a sample which would be required for the measure to reach sufficient levels of reliability. This will aid in study planning and in assessment of the feasibility of new study designs, and importantly can be performed <italic>before</italic> the collection of any new data. I will demonstrate how these methods can be utilised by using five examples based on published PET test-retest studies. This paper is also accompanied by an R package called <italic>relfeas</italic> (<ext-link ext-link-type="uri" xlink:href="http://www.github.com/mathesong/relfeas">www.github.com/mathesong/relfeas</ext-link>), with which all the calculations presented can easily be applied.</p>
</sec>
<sec id="s2">
<title>METHODS</title>
<sec id="s2a">
<title>Reliability</title>
<p>Reliability itself is defined as the proportion of the total variance which is due to true differences.</p>
<p>
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="274894_eqn1.gif"/></alternatives>
</disp-formula>
<italic>where &#x03C3;</italic> <sup>2</sup> represents the variance due to different sources (t: true, e: error, and tot: total). Measures of internal consistency (such as Cronbach&#x2019;s <italic>&#x03B1;</italic>), and of test-retest reliability (such as the ICC), are both estimates of this same definition of reliability.</p>
<p>Reliability can therefore be considered a measure of the distinguishability of measurements (<xref ref-type="bibr" rid="c4">Carrasco et al. 2014</xref>). For example, if the uncertainty around each measurement is large, but inter-individual variance is much larger, scores can still be meaningfully compared between different individuals. Similarly, even if a measure is extremely accurate, it is still incapable of meaningfully distinguishing between individuals who all obtain the same score.</p>
<sec id="s2a1">
<title>Test-Retest Reliability</title>
<p>Test-retest reliability is typically defined using the intraclass correlation coefficient (ICC). In measures for which there are no systematic effects of the measurement number, such as PET test-retest studies in which individuals are each measured twice by the same PET system, we use the one-way ANOVA fixed effects model (<xref ref-type="bibr" rid="c34">Shrout and Fleiss 1979</xref>). This is the most conservative formulation of the ICC. It is described by the following equation:
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="274894_eqn2.gif"/></alternatives>
</disp-formula>
where MS<sub>B</sub> represents the between subjects mean sum of squares, MS<sub>W</sub> represents the within subject mean sum of squares, and k represents the number of observations (usually 2). The ICC is an approximation of the true population reliability: while true reliability can never be negative (<xref ref-type="disp-formula" rid="eqn1">equation 1</xref>) one can obtain negative ICC values if the MS<sub>W</sub> is larger than MS<sub>B</sub> (<xref ref-type="disp-formula" rid="eqn2">equation 2</xref>) in which case the reliability can be treated as zero.</p>
</sec>
<sec id="s2a2">
<title>Measurement Error</title>
<p>Each measurement is made with an associated error, which can be described by its standard error (<italic>&#x03C3;</italic> <sub><italic>e</italic></sub>). It can be estimated as the square root of the within subject mean sum of squares (MS<sub>W</sub>), which is used in the calculation of the ICC above (<xref ref-type="bibr" rid="c1">Baumgartner et al. 2018</xref>).</p>
<p>
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="274894_eqn3.gif"/></alternatives>
</disp-formula>
where n represents the number of participants, i represents the subject number, j represents the measurement number, k represents the number of measurements per subject, y represents the outcome and <inline-formula><alternatives><inline-graphic xlink:href="274894_inline1.gif"/></alternatives></inline-formula> represents the mean outcome for that subject.</p>
<p>The standard error can also be estimated indirectly by rearranging <xref ref-type="disp-formula" rid="eqn1">equation 1,</xref> using the ICC as an estimate of reliability. This is often referred to as the standard error of measurement (SEM) (<xref ref-type="bibr" rid="c38">Weir 2005</xref>; <xref ref-type="bibr" rid="c14">Harvill 1991</xref>).</p>
<p>
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="274894_eqn4.gif"/></alternatives>
</disp-formula>
in which the <italic>&#x03C3;</italic> <sub><italic>t</italic></sub> is the standard deviation of all measurements (both test and retest measurements for test-retest studies).</p>
<p>The standard error can be expressed either in the units of measurement, or relative to some property of the sample: it can be i) scaled to the variance of the sample as an estimate the reliability (ICC, Cronbach&#x2019;s <italic>&#x03B1;</italic>), ii) scaled to the mean of the sample as an estimate the relative uncertainty (the within-subject coefficient of variation, WSCV) (<xref ref-type="bibr" rid="c1">Baumgartner et al. 2018</xref>), or iii) unscaled as an estimate the absolute uncertainty (<italic>&#x03C3;</italic> <sub><italic>e</italic></sub> or SEM).</p>
<p>The relative or absolute uncertainty can be used to calculate the smallest detectable difference (SDD) between two measurements in a given subject which could be considered sufficiently large that it is unlikely to have been due to chance alone (say, according to a 95&#x0025; confidence interval, i.e. using <italic>z</italic><sub>(1-<italic>&#x03B1;/</italic>2)</sub>=1.96 below) (<xref ref-type="bibr" rid="c38">Weir 2005</xref>; <xref ref-type="bibr" rid="c1">Baumgartner et al. 2018</xref>). This is calculated using the following equation.</p>
<p>
<disp-formula id="eqn5">
<alternatives><graphic xlink:href="274894_eqn5.gif"/></alternatives>
</disp-formula>
</p>
<p>It can also be extended to a group level examining differences in means.</p>
<p>
<disp-formula id="eqn6">
<alternatives><graphic xlink:href="274894_eqn6.gif"/></alternatives>
</disp-formula>
</p>
<p>It is important to mention that this measure assumes that all measurements belonging to the original test-retest study and later application study exhibit exactly the same standard error. Further, this measure is not a power analysis: the SDD simply infers that a change in outcome or mean outcome of a group of measurements before and after an experimental manipulation is larger than would be expected by chance. For performing a power analysis for the within-subject change for application to a new study, one must consider the expected effect size relative to the standard deviation of the within-individual changes in the outcome measure.</p>
</sec>
<sec id="s2a3">
<title>Standard thresholds</title>
<p>In psychometrics, reliability can be calculated with one scale by examining its internal consistency. Despite being calculated from the consistency of responding between items, reliability assessed by internal consistency amounts to the same fundamental definition of reliability, namely the ratio of the true to the total variance in the data. It is considered good practice in psychometric studies to calculate the reliability of a scale using measures of internal consistency in the study sample prior to performing statistical inference. The reliability can be used to confirm that the data is sufficiently capable of distinguishing between individuals, for which Nunnally (1978), recommended 0.7 as a default lowest acceptable standard of reliability for scales used in basic research, and 0.8 as adequate. For applied settings in which important decisions are made based on measured outcomes, he suggests a reliability of 0.9 as a minimum and 0.95 as adequate, since even with a reliability of 0.9, the standard error is almost a third the size of the standard deviation.</p>
<p>Test-retest reliability has traditionally been defined by more lenient standards: Fleis (1986) defined ICC values between 0.4 and 0.75 as good, and above 0.75 as excellent. <xref ref-type="bibr" rid="c6">Cicchetti (1994)</xref> defined 0.4 to 0.59 as fair, 0.60 to 0.74 as good, and above 0.75 as excellent. These standards, however, were defined considering the test-retest (as opposed to internal consistency) reliability of psychometric questionnaires (<xref ref-type="bibr" rid="c34">Shrout and Fleiss 1979</xref>; <xref ref-type="bibr" rid="c6">Cicchetti 1994</xref>) for which changes from test to retest could be caused by measurement error, but could also be caused by actual changes in the underlying &#x2018;true&#x2019; value. In PET measurement where protein concentrations are estimated, or indeed in many other physiological measures, these within-subject fluctuations can usually be assumed to be negligibly small over short time periods. For this reason, these standards cannot be considered to be directly applicable. More conservative standards have been proposed by <xref ref-type="bibr" rid="c31">Portney &#x0026; Watkins (2015)</xref>, who define values between 0.5 and 0.75 as &#x201C;poor to moderate&#x201D;, 0.75 to 0.9 as &#x201C;good&#x201D;, and above 0.9 as acceptable for &#x201C;clinical measures&#x201D;. These standards correspond more closely with those defined for the internal consistency of psychometric instruments, and can be considered to be more applicable for measures for which &#x2018;true&#x2019; changes are thought to be negligible.</p>
</sec>
<sec id="s2a4">
<title>Relation to effect size estimates</title>
<p>It is important to consider that reliability is related to effect sizes, and more specifically effect size attenuation. This applies both to studies examining correlations with a continuous variable, and those making group comparisons. This relation is described by the following equation (<xref ref-type="bibr" rid="c36">Spearman 1904</xref>; <xref ref-type="bibr" rid="c27">Nunnally 1970</xref>).</p>
<p>
<disp-formula id="eqn7">
<alternatives><graphic xlink:href="274894_eqn7.gif"/></alternatives>
</disp-formula>
where r<sub>A,B</sub> is the true association between variables A and B, and r<sub>ObsA,ObsB</sub> is the observed correlation between the measured values for A and B. When comparing two or more measures, each of which are recorded with some degree of imprecision, the combined reliability is equal to the square root of their product. This defines the degree to which an association is attenuated by the imprecision of the recorded outcomes, and sets an upper bound on the strength of association between variables that can be expected (<xref ref-type="bibr" rid="c37">Vul et al. 2009</xref>). This attenuation is important when performing power analysis as it decreases the range of potential effect size estimates which can be expected.</p>
<p>When planning a study, one determines the sample size required using power analysis. Power analysis is often performed by estimating the sample size for an estimated effect size. <xref ref-type="bibr" rid="c22">Morey &#x0026; Lakens (2016)</xref> point out that this makes studies vulnerable to the accuracy by which the effect size is approximated. It has been suggested that a better approach is to power studies for the smallest effect size of interest, such that all interesting effects will be reliably detected. More conservative still is the suggestion to power studies according to the level at which meaningful differences between individual studies can be reliably detected (<xref ref-type="bibr" rid="c22">Morey and Lakens 2016</xref>). These strategies would ensure that the scientific literature would produce robust findings, and that the effects in individual studies could be statistically compared respectively. However, for costly and/or invasive methods, such an ideal is not usually attainable. Rather, for studies with these constraints, we require a lower standard of evidence, with robust conclusions perhaps left for meta-analyses, and specifically meta-analyses of pre-registered studies to avoid effect size inflation due to low power (<xref ref-type="bibr" rid="c9">Cremers, Wager, and Yarkoni 2017</xref>) or publication bias (<xref ref-type="bibr" rid="c11">Ferguson and Heene 2012</xref>). To determine the feasibility of an individual study using these methods, it may be more useful not to consider the point at which an individual study is <italic>ideal</italic> for the scientific literature as described by <xref ref-type="bibr" rid="c22">Morey &#x0026; Lakens (2016)</xref>, but rather the point at which the study will simply <italic>not be bad</italic>, i.e. will at least be capable of answering the research question at hand given realistic assumptions.</p>
<p>For this latter aim, we can perform power analysis for study feasibility assessment in terms of a maximum realistic strength of association in biological terms. In this way, the corresponding effect size can be calculated after taking the effect size attenuation into account. If a study will be insufficiently powered even for such a strength of association, then the study is certainly miscalibrated for the detection of the effect of interest. In this way, this method is useful for deciding which research questions can be rejected out of hand given the relevant constraints on the sample size.</p>
</sec>
</sec>
<sec id="s2b">
<title>Extending Test-Retest Reliability</title>
<p>In order for the results of previous test-retest studies to be applicable for new samples with different characteristics, we need to be able to extrapolate the reliability for new samples. To this end, I propose the use of the extrapolated ICC (below). This calculation is based upon two assumptions.</p>
<p><bold>Assumption 1</bold> The absolute measurement error (<italic>&#x03C3;</italic> <sub><italic>e</italic></sub>) will be either similar between groups and/or studies, or the extent to which it will be larger or smaller in a new sample can be approximated.</p>
<p><italic>Comment:</italic> Since test-retest studies are typically conducted using young, healthy volunteers, and not patient samples, these groups will usually provide a good estimate at least of the maximum bound of the precision of the measurement. For comparison groups for which measurement error is likely to be larger, it can either be assumed to be similar (a liberal assumption), or the extent to which it is larger can be approximated as a small multiple (a conservative assumption, e.g. 20&#x0025; larger in the patient group).</p>
<p><bold>Assumption 2</bold> The absolute measurement error (SEM) is likely to be relatively stable across different ranges of the same outcome measure (i.e. an individual with low score will have the same SEM as an individual with a high score on the same outcome measure).</p>
<p><italic>Comment:</italic> There is often some variation expected in the measurement error for different values of the outcome (e.g. less error for individuals with a higher value, and more error for individuals with a lower value of the outcome), however this variation is usually expected to be fairly small. If, on the other hand, this variation across the range of the outcome measure <italic>is</italic> expected to be substantial, then results from such a study should be interpreted with caution in any case.</p>
<p><bold>Assumption Summary</bold> These assumptions are reasonable given that the goal is a rough approximation of the reliability in a new sample in order to assess feasibility in study design and interpretation. The assumptions are primarily based on the fact that most previous and current test-retest studies are performed on young, healthy participants. For planning a new study for which large deviations are expected from either of these two assumptions, whose effects cannot be approximated, a new test-retest study should probably be conducted before proceeding in any case.</p>
<sec id="s2b1">
<title>The extrapolated ICC</title>
<p>The ICC can be expressed using the SEM and the SD (<italic>&#x03C3;</italic>) of the original study from <xref ref-type="disp-formula" rid="eqn4">equation 4</xref>.</p>
<p>
<disp-formula id="eqn8">
<alternatives><graphic xlink:href="274894_eqn8.gif"/></alternatives>
</disp-formula>
</p>
<p>Given these two assumptions above, we can approximate the ICC for a new sample given only the standard deviation of the new sample (and an approximation of the inflation extent of the SEM):
<disp-formula id="eqn9">
<alternatives><graphic xlink:href="274894_eqn9.gif"/></alternatives>
</disp-formula>
where <italic>&#x03C1;</italic> represents the error inflation factor, representing the multiplicative increase in expected measurement error in the new study, for whichever reason. Using <italic>&#x03C1;</italic> &#x003D; 1 assumes that the SEM is the same between studies, while <italic>&#x03C1;</italic> &#x003D; 1.2 would assume a 20&#x0025; increase in the measurement error. If <italic>&#x03C1;</italic> is thought to be substantially greater than 1, then a new test-retest study should probably be performed. With <italic>&#x03C1;</italic> &#x003D; 1 or <italic>&#x03C1;&#x2248;</italic> 1, the difference between the ICC of the original study and the extrapolated ICC of a new study will be determined primarily by the SD of the new study.</p>
<p><bold>Studies making use of continuous independent variables (correlations)</bold> For a new study which is going to examine a correlation, reliability is simply determined by the variance of the study group. With more variation in the study sample, the reliability is therefore higher, all else being equal. Increasing sample variability can be attained by implementing wider recruiting strategies, and not simply relying on convenience sampling (see <xref ref-type="bibr" rid="c16">Henrich et al., 2010</xref>). This also has the advantage of increasing the external reliability of findings.</p>
<p>It should be noted, however, that increasing sample variability by including individuals who differ on another variable which is known to be associated with the dependent variable (for example, age) would <italic>not</italic> necessarily increase the reliability despite increasing the variance of the study group. In these cases, this variable should be included in the statistical test of an applied study as a covariate, and hence the variance of the unstandardised residuals <italic>after</italic> accounting for the covariate would therefore be a better estimation of the total variance for reliability analysis in this test-retest sample.</p>
<p><bold>Studies making use of binary independent variables (t-tests)</bold> For a new study which is comparing two independent groups, one is fitting a binary regressor to the dependent outcome variable. The SD for the ICC is calculated based on the sample variability before fitting the regressor, and thus all individuals are included in the calculation of this value (<xref rid="fig1" ref-type="fig">Figure 1</xref>). The total SD is therefore dependent on both the within-group standard deviation, as well as the degree of difference between the two groups, which is measured by the effect size. For independent sample t tests, one can therefore calculate the effect size (Cohen&#x2019;s D) for which the reliability of a measure would reach a certain desired threshold.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Left: Measured values and their 95&#x0025; confidence intervals are represented by the small points and error bars. Underlying true values are represented by the larger points. Low reliability within groups is increased for the total sample in a comparison, given a sufficiently large effect size, due to the larger variance of the combined sample. Right: True underlying effect sizes (Cohen&#x2019;s D, left) are attenuated by measurement error (right). The population (Popn.) effect size (ES) and sample ES are of the underlying distributions from which the data are sampled and of the obtained sample respectively.</p></caption>
<graphic xlink:href="274894_fig1.tif"/>
</fig>
<p>Thus, by estimating the within-group standard deviation of each of the two groups, one can calculate the required effect size to obtain a sufficient level of reliability. The total SD of the entire sample (including both groups) is described by the following equation:
<disp-formula id="eqn10">
<alternatives><graphic xlink:href="274894_eqn10.gif"/></alternatives>
</disp-formula>
where n is the number of participants in each group, <italic>&#x00B5;</italic> is the mean of each group, <italic>&#x03C3;</italic> is the SD of each group, and the subscripts 1, 2 and total refer to group 1, group 2, and the combined sample. The total mean (<italic>&#x00B5;</italic><sub><italic>total</italic></sub>) is calculated as follows.</p>
<p>
<disp-formula id="eqn11">
<alternatives><graphic xlink:href="274894_eqn11.gif"/></alternatives>
</disp-formula>
</p>
<p>We therefore only need to solve this equation for <italic>&#x00B5;</italic><sub>2</sub>, the mean of the second (e.g. patient) group. This can be calculated as follows (separating the equation into three parts):
<disp-formula id="eqn12">
<alternatives><graphic xlink:href="274894_eqn12.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn13">
<alternatives><graphic xlink:href="274894_eqn13.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn14">
<alternatives><graphic xlink:href="274894_eqn14.gif"/></alternatives>
</disp-formula>
</p>
<p>In this way, given the group sizes and estimates of the standard deviation within both groups, we can calculate the mean difference and effect size for which the degree of variability in the total sample is sufficient to reach a specified level of reliability.</p>
</sec>
</sec>
<sec id="s2c">
<title>Software</title>
<p>All analyses in this paper were performed using R (version 3.4.3, <italic>Kite-Eating Tree</italic>) with the <italic>relfeas</italic> package (<ext-link ext-link-type="uri" xlink:href="https://github.com/mathesong/relfeas">https://github.com/mathesong/relfeas</ext-link>).</p>
</sec>
</sec>
<sec id="s3">
<title>RESULTS</title>
<p>The use of these methods will be demonstrated using examples based on two PET tracers for which test-retest studies have been published. PET imaging involves injection of a radioligand (or tracer) into the blood which binds to a target of interest, and the concentration of radioactivity in the tissue, and sometimes also the blood, are measured over time. The two tracers used as examples include [<sup>11</sup>C]AZ10419369 for imaging of the serotonin 1B receptor (5-HT<sub>1B</sub>) and [<sup>11</sup>C]PBR28 for imaging of translocator protein (TSPO). Both of these tracers have been well-validated and have been used to make clinical comparisons. The reason for the selection of these two radioligands is that they both demonstrate properties which make the interpretation of reliability outcomes especially difficult. [<sup>11</sup>C]AZ10419369 exhibits low measurement error, but also low inter-individual heterogeneity, resulting in low reliability. [<sup>11</sup>C]PBR28 shows high error variance, but also high inter-individual heterogeneity, resulting in high reliability.</p>
<p>Outcome measures reported for these PET studies include binding potential (BP<sub>ND</sub>), volume of distribution (V<sub>T</sub>), which are defined based on compartmental models, as well as the distribution volume ratio (DVR) and standardised uptake value (SUV). For more details, see <xref rid="app1" ref-type="app">Appendix 1</xref>.</p>
<sec id="s3a">
<title>Example 1: Low reliability and low variance</title>
<p>[<sup>11</sup>C]AZ10419369 is a radiotracer for the serotonin 1B (5-HT<sub>1B</sub>) receptor, which is widely expressed in the brain and an important target in depression (<xref ref-type="bibr" rid="c32">Ruf and Bhagwagar 2009</xref>). In the test-retest study published with this ligand (<xref ref-type="bibr" rid="c25">Nord, Finnema, et al. 2014</xref>), using the frontal cortex as an example, it showed what is considered a high mean BP<sub>ND</sub> (1.6), a favourable absolute variability (6.8&#x0025;) and a good coefficient of variation (6-7&#x0025;). However in the sample measured, the ICC was very low: 0.32. It was concluded that &#x201C;[the low ICC value] can be explained by&#x2026;a low between-subject variance. Thus, despite the low ICC values, it cannot be excluded that the test-retest reliability is also high in these regions&#x201D; (p.304).</p>
<p>A followup study was published examining age-related changes in 5-HT<sub>1B</sub> binding (<xref ref-type="bibr" rid="c23">Nord, Csel&#x00E9;nyi, et al. 2014</xref>), concluding that 5-HT<sub>1B</sub> receptor availability decreases with age in cortical regions. If one were to have considered the ICC of the original study as a fixed property of this measure, then such a conclusion should be treated with caution due to the low reliability of the outcome. However, due to the differences in [<sup>11</sup>C]AZ10419369 BP<sub>ND</sub> across the age range examined, the standard deviation was 3.2-fold greater than that of the test-retest study. To assess the reliability of frontal cortex [<sup>11</sup>C]AZ10419369 BP<sub>ND</sub> for this particular application, it is necessary to take the greater variability into account.</p>
<p>We can calculate the reliability of the new study using the results of the test-retest study by assuming the same measurement error between the studies. In this way we obtain a reliability of this tracer and for this particular correlation equal to 0.93. From this analysis, the outcome of the study can be considered reliable in terms of measurement error, and that individuals can be easily distinguished from one another by their outcome measures. This conclusion also holds after taking into account the use of partial volume effect correction in this study (see <xref rid="app2" ref-type="app">Appendix 2</xref>).</p>
</sec>
<sec id="s3b">
<title>Example 2: Reliability and power analysis</title>
<p>Let us assume that a new study was being planned to examine the relationship between [<sup>11</sup>C]AZ10419369 BP<sub>ND</sub> and a particular self-report measure. This study would be performed in young, healthy control subjects. Having observed from the test-retest study that the reliability in healthy controls was not high for most regions due to limited inter-individual variability, the investigators choose to examine the occipital cortex where reliability is highest (ICC=0.8).</p>
<p>The self-report scale is also associated with its own measurement error: it usually exhibits a reliability of 0.7 in healthy samples (and it is assumed that this will apply to this sample too). Using the equation7, we obtain an attenuation of the correlation coefficient of 0.75. In other words, if the scale were to explain 100&#x0025; of the variance in [<sup>11</sup>C]AZ10419369 BP<sub>ND</sub> in the absence of any measurement error, we would expect to see a correlation of r=0.75 after taking this error into account.</p>
<p>For deciding whether to go ahead and conduct this study, we can consider the <italic>maximum realistic biological</italic> effect size for power analysis (see Methods). The investigators approximate that 30&#x0025; explained variance is probably the maximum extent to which the self-report measure could realistically be explained by occipital cortex 5-HT<sub>1B</sub> availability. For this effect, a power analysis (for 80&#x0025; power) will reveal that such a correlation would require a sample size of 23 in the absence of measurement error. Accounting for the attenuation due to measurement error however, 44 participants would be required to observe this effect.</p>
<p>It can therefore be concluded that unless more than 44 participants can be examined in the planned study, then it will not be possible to draw strong conclusions from the results of this study. If significant, it would likely overestimate the true magnitude of the association as the study would only be sufficiently powered for effect sizes larger than what what was considered realistic a priori, i.e. a type M error (<xref ref-type="bibr" rid="c13">Gelman and Carlin 2014</xref>). If insignificant, such a result would be uninformative as the type II error rate would be too large. In this way, a costly study which had a low probability of yielding meaningful conclusions can be avoided before even its inception.</p>
</sec>
<sec id="s3c">
<title>Example 3: Reliability for differences</title>
<p>[<sup>11</sup>C]AZ10419369 is also known to be displaced by serotonin release. This can be measured by performing a baseline measurement, and then performing another measurement following administration of a serotonin-releasing agent and assessing the decrease in binding (see <xref rid="app3" ref-type="app">Appendix 3</xref>). We will assume that high doses of the drug similar to those previously administered to non-human primates (NHPs) (<xref ref-type="bibr" rid="c24">Nord et al. 2013</xref>) will be administered to humans, and it is estimated that this will produce a similar displacement of the radioligand in humans. The study will consider the occipital cortex, where the reliability of [<sup>11</sup>C]AZ10419369 BP<sub>ND</sub> is highest, and where escitalopram-induced displacement was significant in NHPs (mean=-12&#x0025;, SD=10&#x0025;) (<xref ref-type="bibr" rid="c24">Nord et al. 2013</xref>).</p>
<p>For this study, there are two questions to be answered with regard to its feasibility, namely the within- and between-individual effects respectively. The first is whether we can reliably measure serotonin release within individuals at all. The second question is whether the reliability of this radioligand is sufficiently high to compare the degree of radioligand displacement (i.e. &#x0394; BP<sub>ND</sub>), as opposed to binding (i.e. BP<sub>ND</sub>), between individuals and hence groups (patients vs controls).</p>
<p>The first question can be answered using the smallest detectable difference (SDD) metric as well as power analysis. The SDD for individual scores is 14.3&#x0025; (<xref ref-type="disp-formula" rid="eqn5">equation 5</xref>). To detect a difference in group means, the SDD is 10.1&#x0025; for 2 individuals (<xref ref-type="disp-formula" rid="eqn6">equation 6</xref>). This means that with two or more individuals, a change of 12&#x0025; is greater than that which would be expected by chance based on the reliability of [<sup>11</sup>C]AZ10419369 BP<sub>ND</sub>. We also perform a power analysis by considering the size of the expected effect (&#x2212;12&#x0025; change) relative to the standard deviation of test-retest variability (7.3&#x0025;), i.e. a Cohen&#x2019;s D of 1.64. This means that 4 individuals would be required for 80&#x0025; power for a one-sided test of this effect size, assuming that it is accurate.</p>
<p>The second question requires that we reassess the reliability of the radioligand for the change in [<sup>11</sup>C]AZ10419369 BP<sub>ND</sub>. We must consider that there are two measurements, both of which have error terms: the error (<italic>&#x03C3;</italic> <sub><italic>e</italic></sub>) term is therefore doubled for &#x0394; BP<sub>ND</sub> as the measurement error is additive. The total variance term is also no longer determined by the range of BP<sub>ND</sub> values, but by the range of &#x0394; BP<sub>ND</sub> values. This can be approximated from the displacement statistics as 10&#x0025; of the mean BP<sub>ND</sub> value before treatment. The ICC obtained for &#x0394; BP<sub>ND</sub> would therefore be &#x2212;0.06. This is so low due to the low variability in the outcome: there is large error variance, and a small true variance in the &#x0394; BP<sub>ND</sub> outcome measure. However this does not take the expected differences between the controls and the patients into consideration.</p>
<p>In planning this study, the investigators estimate that there should be 2.5 times as much serotonin release in the patient group. This will result in a larger degree of interindividual differences, and thus higher reliability. With two groups of equal size (20) and standard deviation, with this effect size, the ICC grows to 0.41. This means that the true effect (Cohen&#x2019;s D=1.8) will be attenuated to 0.95 when measured due to the measurement error, and a much larger sample size (308&#x0025; for unpaired; 179&#x0025; for paired) will be required to detect a difference of this size as significant with 80&#x0025; power (with a one-sided test). Further, since differences between individuals in &#x0394; BP<sub>ND</sub> values will be mostly determined by random noise, even if the study were to produce a significant outcome, it would still be unclear to what extent this was due to noise, and we would remain uncertain about its true magnitude. It can therefore be concluded that this study is probably not worth investing in, as its results will not be greatly informative.</p>
</sec>
<sec id="s3d">
<title>Example 4: High reliability and high variance</title>
<p>[<sup>11</sup>C]PBR28 is a second generation tracer for translocator protein (TSPO), which is expressed in glial cells including immune cells, and represents an important target for studies of immune activation in various clinical disorders. Due to genetic effects (leading to high- and low-affinity subtypes of the protein) and high intra- and inter-individual variability, several different strategies have been proposed for quantification of this tracer.</p>
<p>While the absolute variance of [<sup>11</sup>C]PBR28 V<sub>T</sub> appears very high (<italic>&#x2248;</italic>20&#x0025;), its test-retest reliability is also very high (<italic>&#x2248;</italic> 0.9) (<xref ref-type="bibr" rid="c8">Collste et al. 2016</xref>; <xref ref-type="bibr" rid="c21">Matheson et al. 2017</xref>) in both high-affinity binders (HABs) and medium affinity binders (MABs) (for more information, see <xref ref-type="bibr" rid="c30">Owen et al., 2012</xref>) due to the several-fold variability (COV <italic>&#x2248;</italic> 40&#x0025;) between individuals (<xref ref-type="bibr" rid="c8">Collste et al. 2016</xref>; <xref ref-type="bibr" rid="c21">Matheson et al. 2017</xref>). As such, clinical studies comparing patients and controls using V<sub>T</sub> are unlikely to suffer from poor reliability. However, a large effect size (Cohen&#x2019;s D &#x003D; 0.8) corresponds to a mean difference of over 30&#x0025; in [<sup>11</sup>C]PBR28 V<sub>T</sub>, while an equivalent large effect size for [<sup>11</sup>C]AZ10419369 frontal cortex BP<sub>ND</sub> is less than 6&#x0025;. Put differently, to detect a 10&#x0025; difference in binding between groups, 278 (HAB) participants would be required per group for [<sup>11</sup>C]PBR28, compared to 7 per group for [<sup>11</sup>C]AZ10419369. Large numbers of participants will be required to observe even relatively large proportional differences in [<sup>11</sup>C]PBR28 binding. However, large proportional differences are also more likely for [<sup>11</sup>C]PBR28 due to the large differences between individuals.</p>
</sec>
<sec id="s3e">
<title>Example 5: Variance reduction strategies</title>
<p>Attempts have been made to account for this variability in [<sup>11</sup>C]PBR28 V<sub>T</sub>, both within and between genotype groups, with suggestions of simplified ratio-based approaches such as the distribution volume ratio (DVR) despite the absence of a reference region (<xref ref-type="bibr" rid="c20">Lyoo et al. 2015</xref>). We previously reported high interregional associations (R &#x2265; 0.98), including denominator regions, resulting in poor reliability for this outcome measure (mean ICC=0.5) (<xref ref-type="bibr" rid="c21">Matheson et al. 2017</xref>). While we advised caution in the interpretation of results making use of ratio methods, it is still theoretically possible that ratio methods might show good reliability for the detection of very large effects in specific neurological conditions which would not be otherwise detectable with small samples due to the large inter-individual variability (<xref ref-type="bibr" rid="c20">Lyoo et al. 2015</xref>). Using this method of extrapolating reliability, we can calculate what size of an effect between groups would be required for the ratio methods to begin to show reasonable reliability for informative conclusions.</p>
<p>For this, we first calculate the required total SD for each of our specified levels of reliability. We then calculate the effect size which would produce this SD. The methods are calculated for a planned comparison of 20 healthy controls and 20 patients with equal standard deviation between groups, assuming that the SD of the test-retest sample is representative of that for the healthy group. For reliabilities of 0.7, 0.8 and 0.9, one would require Cohen&#x2019;s D effect sizes of 1.6, 2.4 and 4 respectively. Considering that a Cohen&#x2019;s D of 0.8 is considered a <italic>large</italic> effect size (<xref ref-type="bibr" rid="c7">Cohen 1988</xref>), these effects are unrealistically large. These effect sizes can be plotted as overlapping distributions to provide a visual reference for understanding these effect sizes (<xref rid="fig2" ref-type="fig">Figure 2</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Overlap and effect size required for DVR differences between groups to reach different levels of reliability. The overlap is the distributional overlap. Cohen&#x2019;s U3 is the fraction of one group which does not overlap with the mean of the other. The Common Language Effect Size (CLES) is the chance that a person picked at random from the red group will have a higher value than a person picked at random from the blue group.</p></caption>
<graphic xlink:href="274894_fig2.tif"/>
</fig>
<p>From this information, it is clear that unless one is expecting <italic>very</italic> large effects, the use of DVR is unlikely to be useful due to its poor reliability. Due to the strong associations reported between numerator and denominator regions, DVR effect sizes will even further underestimate actual differences in V<sub>T</sub> in addition to the attenuation of effect sizes due to its poor reliability. V<sub>T</sub> therefore appears to be statistically superior to DVR for detection of any effects, without the need to be concerned about issues of reliability.</p>
</sec>
</sec>
<sec id="s4">
<title>DISCUSSION</title>
<p>This paper has demonstrated how the results of previously published test-retest studies can be utilised for the rough approximation of the reliability in new samples based only on commonly reported summary statistics. Test-retest studies are usually, and should usually, be performed to validate new PET tracers and quantification strategies. However, while reliability is an important consideration for the design and interpretation of studies, measures of reliability reported in test-retest studies are not currently used to their full potential. This paper outlines a method for how reliability can be extrapolated to new samples, provides examples of how these methods can be applied in practice for both study design and interpretation, and is accompanied by an open-source R package <italic>relfeas</italic> with functions for all calculations.</p>
<p>In this way, researchers can avoid performing costly studies whose results are unlikely to be replicable or sufficiently informative with regard the relevant biological question being asked, and can interpret previously published studies whose reliability might be questionable with an appropriate degree of caution. This method is analogous to the philosophy of &#x201C;fail fast&#x201D; from software development (<xref ref-type="bibr" rid="c33">Shore 2004</xref>): systems should be designed such that bugs which may lead to system failure will cause the system to fail at an early stage of operation, rather than unexpectedly at a late stage. In the same way, this method allows for studies to fail early, before even reaching the the data collection stage, leading to savings in cost and time, and with PET, avoiding exposure of participants to radioactivity.</p>
<p>The importance and neglect, of reliability has come up before, perhaps most notably in the study of <xref ref-type="bibr" rid="c37">Vul et al. (2009)</xref> examining fMRI studies of emotion, personality and social cognition. This study found &#x2018;puzzlingly high correlations&#x2019;, in which, given the reliability of the measures, over 100&#x0025; of the biological variance was being explained. It was concluded that &#x201C;a disturbingly large, and quite prominent, segment of fMRI research&#x2026; is using seriously defective research methods&#x2026; and producing a profusion of numbers that should not be believed.&#x201D; (p. 285). The failing of researchers to sufficiently acknowledge considerations of reliability has been related to the historical separation of correlational (i.e. individual differences research) and experimental (i.e. within-subject) approaches to scientific inquiry: outcome consistency with one approach does not necessarily apply to the other (<xref ref-type="bibr" rid="c15">Hedge, Powell, and Sumner 2017</xref>). It was shown that several well-validated cognitive tasks (e.g. Stroop, Go/No-go), despite demonstrating robust within-individual effects, exhibited very low reliability for inter-individual comparisons. This was due primarily to low inter-individual variation. It is noted that it is likely this same characteristic that both makes these measures robust in experimental research, but unreliable for correlational research (<xref ref-type="bibr" rid="c15">Hedge, Powell, and Sumner 2017</xref>).</p>
<p>The results from the examples above can be interpreted in this context: [<sup>11</sup>C]AZ10419369 BP<sub>ND</sub> appears to be an excellent outcome for the assessment of within-individual changes, such as in PET blocking studies where the receptor is measured before and after pharmacological blockade. However, its utility is limited for the assessment of between-individual differences. The opposite is true of [<sup>11</sup>C]PBR28 V<sub>T</sub>: this measure differentiates between individuals very well, but may be less useful for assessing within-individual changes. [<sup>11</sup>C]PBR28 DVR may be more useful for within-individual effects, provided that equivalence is shown in the denominator region (<xref ref-type="bibr" rid="c19">Lakens 2017</xref>). For between-subjects designs, it is important to emphasise that measures whose precision is low can still be useful when differences between individuals are very large (e.g. [<sup>11</sup>C]PBR28 V<sub>T</sub>), while measures whose precision is very high can be practically uninformative when the differences between individuals are extremely small (e.g. frontal cortex [<sup>11</sup>C]AZ10419369 BP<sub>ND</sub>). Since reliability is indirectly proportional to the amount of inter-individual variability in a sample, reliability can be increased by recruiting more diverse participant samples. In contrast, relying on convenience sampling results in study samples which are overwhelmingly comprised of university students from privileged backgrounds (WEIRD) (<xref ref-type="bibr" rid="c16">Henrich, Heine, and Norenzayan 2010</xref>), who are unlikely to be representative of the broader population (i.e. showing low external validity).</p>
<p>Failure to replicate is a substantial problem in science (<xref ref-type="bibr" rid="c2">Begley and Ellis 2012</xref>; Open Science Collaboration et al. 2015). Questionable research practices (QRPs) (<xref ref-type="bibr" rid="c17">John, Loewenstein, and Prelec 2012</xref>; <xref ref-type="bibr" rid="c35">Simmons, Nelson, and Simonsohn 2011</xref>) and underpowered studies (<xref ref-type="bibr" rid="c3">Button et al. 2013</xref>; <xref ref-type="bibr" rid="c22">Morey and Lakens 2016</xref>) have been proposed as some of the primary reasons for this failure. It has even been suggested that this may threaten the very notion of cumulative science (<xref ref-type="bibr" rid="c22">Morey and Lakens 2016</xref>; <xref ref-type="bibr" rid="c10">Elk et al. 2015</xref>). Consideration of the reliability is another important, and related, factor. Studies based on unreliable outcomes are unlikely to even produce results representative of any underlying reality. Studies can be determined as not worth being performed if outcomes are unreliable, or if maximal feasible &#x2018;true&#x2019; effects after effect size attenuation is deemed to be too small to measure. This can provide enormous savings in both time and resources. A utopian science would be one in which i) analysis plans were fully pre-registered or, better yet, peer-reviewed prior to study initiation such as in Registered Reports (<xref ref-type="bibr" rid="c26">Nosek, Spies, and Motyl 2012</xref>; <xref ref-type="bibr" rid="c5">Chambers et al. 2015</xref>), ii) studies were sufficiently powered to provide meaningful effect size estimates and comparison between studies (<xref ref-type="bibr" rid="c22">Morey and Lakens 2016</xref>); and iii) all studies utilised outcome measures whose reliability was sufficient for the specific research question in the study sample such that they would yield informative estimates of magnitude. By providing methods such that the reliability in study samples can at least be roughly approximated and planned for, and tools by which to make this easily implementable, this paper hopes to aid researchers in this last goal.</p>
</sec>
</body>
<back>
<ack>
<title>ACKNOWLEDGEMENTS</title>
<p>I would like to say an enormous thank you, in alphabetical order, to Simon Cervenka, Lieke de Boer, Vincent Millischer, Pontus Plav&#x00E9;n-Sigray, Bj&#x00F6;rn Schiffler, Jonas Svensson and William Hedley Thompson for their helpful comments and discussions about the manuscript.</p>
</ack>
<ref-list>
<title>REFERENCES</title>
<ref id="c1"><mixed-citation publication-type="book"><string-name><surname>Baumgartner</surname>, <given-names>Richard</given-names></string-name>, <string-name><given-names>Aniket</given-names> <surname>Joshi</surname></string-name>, <string-name><given-names>Dai</given-names> <surname>Feng</surname></string-name>, <string-name><given-names>Francesca</given-names> <surname>Zanderigo</surname></string-name>, and R <string-name><given-names>Todd</given-names> <surname>Ogden</surname></string-name>. <year>2018</year>. &#x201C;<chapter-title>Sta488 tistical evaluation of test-retest studies in PET brain imaging</chapter-title>.&#x201D; <source>EJNMMI Research</source> <volume>8</volume> (<issue>1</issue>). <publisher-name>EJNMMI Research</publisher-name>: <fpage>13</fpage>. doi:<pub-id pub-id-type="doi">10.1186/s13550-018-0366-8</pub-id>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Begley</surname>, <given-names>C. Glenn</given-names></string-name>, and <string-name><given-names>Lee M.</given-names> <surname>Ellis</surname></string-name>. <year>2012</year>. &#x201C;<article-title>Drug development: Raise standards for preclinical cancer research.&#x201D;</article-title> <source>Nature</source> <volume>483</volume> (<issue>7391</issue>): <fpage>531</fpage>&#x2013;<lpage>33</lpage>. doi:<pub-id pub-id-type="doi">10.1038/483531a</pub-id>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="book"><string-name><surname>Button</surname>, <given-names>Katherine S</given-names></string-name>, <string-name><given-names>John P a</given-names> <surname>Ioannidis</surname></string-name>, <string-name><given-names>Claire</given-names> <surname>Mokrysz</surname></string-name>, <string-name><given-names>Brian a</given-names> <surname>Nosek</surname></string-name>, <string-name><given-names>Jonathan</given-names> <surname>Flint</surname></string-name>, <string-name><given-names>Emma S J</given-names> <surname>Robinson</surname></string-name>, and <string-name><surname>Marcus</surname> <given-names>R Munaf&#x00F2;</given-names></string-name>. <year>2013</year>. &#x201C;<chapter-title>Power failure: why small sample size undermines the reliability of neuroscience</chapter-title>.&#x201D; <source>Nature Reviews. Neuroscience</source> <volume>14</volume> (<issue>5</issue>). <publisher-name>Nature Publishing Group</publisher-name>: <fpage>365</fpage>&#x2013;<lpage>76</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nrn3475</pub-id>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Carrasco</surname>, <given-names>Josep L.</given-names></string-name>, <string-name><given-names>Alejandro</given-names> <surname>Caceres</surname></string-name>, <string-name><given-names>Georgia</given-names> <surname>Escaramis</surname></string-name>, and <string-name><given-names>Lluis</given-names> <surname>Jover</surname></string-name>. <year>2014</year>. &#x201C;<article-title>Distinguishability and agreement with continuous data.&#x201D;</article-title> <source>Statistics in Medicine</source> <volume>33</volume> (<issue>1</issue>): <fpage>117</fpage>&#x2013;<lpage>28</lpage>. doi:<pub-id pub-id-type="doi">10.1002/sim.5896</pub-id>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Chambers</surname>, <given-names>Christopher D.</given-names></string-name>, <string-name><given-names>Zoltan</given-names> <surname>Dienes</surname></string-name>, <string-name><given-names>Robert D.</given-names> <surname>McIntosh</surname></string-name>, <string-name><given-names>Pia</given-names> <surname>Rotshtein</surname></string-name>, and <string-name><given-names>Klaus</given-names> <surname>Willmes</surname></string-name>. <year>2015</year>. &#x201C;<article-title>Registered Reports: Realigning incentives in scientific publishing.&#x201D;</article-title> <source>Cortex</source> <volume>66</volume>: <fpage>1</fpage>&#x2013;<lpage>2</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cortex.2015.03.022</pub-id>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Cicchetti</surname>, <given-names>Domenic V.</given-names></string-name> <year>1994</year>. &#x201C;<article-title>Guidelines, criteria, and rules of thumb for evaluating normed and standardized assessment instruments in psychology</article-title>.&#x201D; <source>Psychological Assessment</source> <volume>6</volume> (<issue>4</issue>): <fpage>284</fpage>&#x2013;<lpage>90</lpage>. doi:<pub-id pub-id-type="doi">10.1037/1040-3590.6.4.284</pub-id>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Cohen</surname>, <given-names>J.</given-names></string-name> <year>1988</year>. &#x201C;<source>Statistical power analysis for the behavioral sciences</source>.&#x201D; doi:<pub-id pub-id-type="doi">10.1234/12345678</pub-id>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Collste</surname>, <given-names>K.</given-names></string-name>, <string-name><given-names>A.</given-names> <surname>Forsberg</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Varrone</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Amini</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Aeinehband</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Yakushev</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Halldin</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Farde</surname></string-name>, and <string-name><given-names>S.</given-names> <surname>Cervenka</surname></string-name>. <year>2016</year>. &#x201C;<article-title>Test&#x2013;retest reproducibility of [11C]PBR28 binding to TSPO in healthy control subjects.&#x201D;</article-title> <source>European Journal of Nuclear Medicine and Molecular Imaging</source> <volume>43</volume> (<issue>1</issue>): <fpage>173</fpage>&#x2013;<lpage>83</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s00259-015-3149-8</pub-id>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Cremers</surname>, <given-names>Henk R.</given-names></string-name>, <string-name><given-names>Tor D.</given-names> <surname>Wager</surname></string-name>, and <string-name><given-names>Tal</given-names> <surname>Yarkoni</surname></string-name>. <year>2017</year>. &#x201C;<article-title>The relation between statistical power and inference in fMRI.&#x201D;</article-title> <source>PLoS ONE</source> <volume>12</volume> (<issue>11</issue>): <fpage>1</fpage>&#x2013;<lpage>20</lpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0184923</pub-id>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Elk</surname>, <given-names>Michiel van</given-names></string-name>, <string-name><given-names>Dora</given-names> <surname>Matzke</surname></string-name>, <string-name><given-names>Quentin F.</given-names> <surname>Gronau</surname></string-name>, <string-name><given-names>Maime</given-names> <surname>Guan</surname></string-name>, <string-name><given-names>Joachim</given-names> <surname>Vandekerckhove</surname></string-name>, and <string-name><given-names>Eric-Jan</given-names> <surname>Wagenmakers</surname></string-name>. <year>2015</year>. &#x201C;<article-title>Meta-analyses are no substitute for registered replications: a skeptical perspective on religious priming.&#x201D;</article-title> <source>Frontiers in Psychology</source> <volume>6</volume>. doi:<pub-id pub-id-type="doi">10.3389/fpsyg.2015.01365</pub-id>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Ferguson</surname>, <given-names>Christopher J.</given-names></string-name>, and <string-name><given-names>Moritz</given-names> <surname>Heene</surname></string-name>. <year>2012</year>. &#x201C;<article-title>A Vast Graveyard of Undead Theories: Publication Bias and Psychological Science&#x2019;s Aversion to the Null</article-title>.&#x201D; <source>Perspectives on Psychological Science</source> <volume>7</volume> (<issue>6</issue>): <fpage>555</fpage>&#x2013;<lpage>61</lpage>. doi:<pub-id pub-id-type="doi">10.1177/1745691612459059</pub-id>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Fleiss</surname>, <given-names>Joseph L.</given-names></string-name> <year>1986</year>. <source>Reliability of Measurement.</source></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Gelman</surname>, <given-names>Andrew</given-names></string-name>, and <string-name><given-names>John</given-names> <surname>Carlin</surname></string-name>. <year>2014</year>. &#x201C;<article-title>Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors</article-title>.&#x201D; <source>Perspectives on Psychological Science</source> <volume>9</volume> (<issue>6</issue>): <fpage>641</fpage>&#x2013;<lpage>51</lpage>. 12/15 doi:<pub-id pub-id-type="doi">10.1177/1745691614551642</pub-id>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Harvill</surname>, <given-names>LM</given-names></string-name>. <year>1991</year>. <source>&#x201C;Standard error of Measurement.&#x201D; Instructional Topics in Educational Measurement</source> <volume>1991</volume> (<issue>1</issue>): <fpage>33</fpage>&#x2013;<lpage>41</lpage>. doi:<pub-id pub-id-type="doi">10.1177/0272989X10380925</pub-id>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Hedge</surname>, <given-names>Craig</given-names></string-name>, <string-name><given-names>Georgina</given-names> <surname>Powell</surname></string-name>, and <string-name><given-names>Petroc</given-names> <surname>Sumner</surname></string-name>. <year>2017</year>. &#x201C;<article-title>The reliability paradox: Why robust cognitive tasks do not produce reliable individual differences.&#x201D;</article-title> <source>Behavior Research Methods</source>, <month>July</month>. doi:<pub-id pub-id-type="doi">10.3758/s13428-017-0935-1</pub-id>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Henrich</surname>, <given-names>Joseph</given-names></string-name>, <string-name><given-names>Steven J.</given-names> <surname>Heine</surname></string-name>, and <string-name><given-names>Ara</given-names> <surname>Norenzayan</surname></string-name>. <year>2010</year>. &#x201C;<article-title>The weirdest people in the world?&#x201D;</article-title> <source>Behavioral and Brain Sciences</source> <volume>33</volume> (<issue>2-3</issue>): <fpage>61</fpage>&#x2013;<lpage>83</lpage>. doi:<pub-id pub-id-type="doi">10.1017/S0140525X0999152X</pub-id>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>John</surname>, <given-names>Leslie K.</given-names></string-name>, <string-name><given-names>George</given-names> <surname>Loewenstein</surname></string-name>, and <string-name><given-names>Drazen</given-names> <surname>Prelec</surname></string-name>. <year>2012</year>. &#x201C;<article-title>Measuring the Prevalence of Question529 able Research Practices With Incentives for Truth Telling</article-title>.&#x201D; <source>Psychological Science</source> <volume>23</volume> (<issue>5</issue>): <fpage>524</fpage>&#x2013;<lpage>32</lpage>. doi:<pub-id pub-id-type="doi">10.1177/0956797611430953</pub-id>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Kimball</surname>, <given-names>A. W.</given-names></string-name> <year>1957</year>. &#x201C;<article-title>Errors of the Third Kind in Statistical Consulting</article-title>.&#x201D; <source>Journal of the American Statistical Association</source> <volume>52</volume> (<issue>278</issue>): <fpage>133</fpage>&#x2013;<lpage>42</lpage>. doi:<pub-id pub-id-type="doi">10.1080/01621459.1957.10501374</pub-id>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Lakens</surname>, <given-names>Danie&#x00A8;l</given-names></string-name>. <year>2017</year>. &#x201C;<article-title>Equivalence Tests: A Practical Primer for t Tests, Correlations, and Meta-Analyses</article-title>.&#x201D; <source>Social Psychological and Personality Science</source> <volume>8</volume> (<issue>4</issue>): <fpage>355</fpage>&#x2013;<lpage>62</lpage>. doi:<pub-id pub-id-type="doi">10.1177/1948550617697177</pub-id>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Lyoo</surname>, <given-names>C H</given-names></string-name>, <string-name><given-names>M</given-names> <surname>Ikawa</surname></string-name>, <string-name><given-names>J S</given-names> <surname>Liow</surname></string-name>, <string-name><given-names>S S</given-names> <surname>Zoghbi</surname></string-name>, <string-name><given-names>C L</given-names> <surname>Morse</surname></string-name>, <string-name><given-names>V W</given-names> <surname>Pike</surname></string-name>, <string-name><given-names>M</given-names> <surname>Fujita</surname></string-name>, <string-name><given-names>R B</given-names> <surname>Innis</surname></string-name>, and <string-name><given-names>W C</given-names> <surname>Kreisl</surname></string-name>. <year>2015</year>. &#x201C;<article-title>Cerebellum Can Serve As a Pseudo-Reference Region in Alzheimer Disease to Detect Neuroinflammation Measured with PET Radioligand Binding to Translocator Protein.&#x201D;</article-title> <source>Journal of Nuclear Medicine</source> <volume>56</volume> (<issue>5</issue>): <fpage>701</fpage>&#x2013;<lpage>6</lpage>. doi:<pub-id pub-id-type="doi">10.2967/jnumed.114.146027</pub-id>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Matheson</surname>, <given-names>Granville J.</given-names></string-name>, <string-name><given-names>Pontus</given-names> <surname>Plave&#x00B4;n-Sigray</surname></string-name>, <string-name><given-names>Anton</given-names> <surname>Forsberg</surname></string-name>, <string-name><given-names>Andrea</given-names> <surname>Varrone</surname></string-name>, <string-name><given-names>Lars</given-names> <surname>Farde</surname></string-name>, and <string-name><given-names>Simon</given-names> <surname>Cervenka</surname></string-name>. <year>2017</year>. &#x201C;<article-title>Assessment of simplified ratio-based approaches for quantification of PET [11C]PBR28 data.&#x201D;</article-title> <source>EJNMMI Research</source> <volume>7</volume> (<issue>1</issue>): <fpage>58</fpage>. doi:<pub-id pub-id-type="doi">10.1186/s13550-017-0304-1</pub-id>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Morey</surname>, <given-names>Richard</given-names></string-name>, and <string-name><surname>Danie&#x00A8;l</surname> <given-names>Lakens.</given-names></string-name> <year>2016</year>. <source>&#x201C;Why most of psychology is statistically unfalsifiable.&#x201D;</source> doi:<pub-id pub-id-type="doi">10.5281/zenodo.838685</pub-id>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Nord</surname>, <given-names>Magdalena</given-names></string-name>, <string-name><given-names>Zsolt</given-names> <surname>Csele&#x00B4;nyi</surname></string-name>, <string-name><given-names>Anton</given-names> <surname>Forsberg</surname></string-name>, <string-name><given-names>G??ran</given-names> <surname>Rosenqvist</surname></string-name>, <string-name><given-names>Mikael</given-names> <surname>Tiger</surname></string-name>, <string-name><given-names>Johan</given-names> <surname>Lundberg</surname></string-name>, <string-name><given-names>Andrea</given-names> <surname>Varrone</surname></string-name>, and <string-name><given-names>Lars</given-names> <surname>Farde</surname></string-name>. <year>2014</year>. &#x201C;<article-title>Distinct regional age effects on [11C]AZ10419369 binding to 5-HT1B receptors in the human brain.&#x201D;</article-title> <source>NeuroImage</source> <volume>103</volume>: <fpage>303</fpage>&#x2013;<lpage>8</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.09.040</pub-id>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Nord</surname>, <given-names>Magdalena</given-names></string-name>, <string-name><given-names>Sjoerd J</given-names> <surname>Finnema</surname></string-name>, <string-name><given-names>Christer</given-names> <surname>Halldin</surname></string-name>, and <string-name><given-names>Lars</given-names> <surname>Farde</surname></string-name>. <year>2013</year>. &#x201C;<article-title>Effect of a single dose of es549 citalopram on serotonin concentration in the non-human and human primate brain.&#x201D;</article-title> <source>The International Journal of Neuropsychopharmacology / Official Scientific Journal of the Collegium Internationale Neuropsychopharmacologicum (CINP)</source> <volume>16</volume> (<issue>7</issue>): <fpage>1577</fpage>&#x2013;<lpage>86</lpage>. doi:<pub-id pub-id-type="doi">10.1017/S1461145712001617</pub-id>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Nord</surname>, <given-names>Magdalena</given-names></string-name>, <string-name><given-names>Sjoerd J.</given-names> <surname>Finnema</surname></string-name>, <string-name><given-names>Martin</given-names> <surname>Schain</surname></string-name>, <string-name><given-names>Christer</given-names> <surname>Halldin</surname></string-name>, and <string-name><given-names>Lars</given-names> <surname>Farde</surname></string-name>. <year>2014</year>. &#x201C;<article-title>Test-retest reliability of [11C]AZ10419369 binding to 5-HT 1B receptors in human brain.&#x201D;</article-title> <source>European Journal of Nuclear Medicine and Molecular Imaging</source> <volume>41</volume> (<issue>2</issue>): <fpage>301</fpage>&#x2013;<lpage>7</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s00259-013-2529-1</pub-id>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Nosek</surname>, <given-names>Brian A.</given-names></string-name>, <string-name><given-names>J. R.</given-names> <surname>Spies</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Motyl</surname></string-name>. <year>2012</year>. &#x201C;<article-title>Scientific Utopia: II. Restructuring Incentives and Practices to Promote Truth Over Publishability.&#x201D;</article-title> <source>Perspectives on Psychological Science</source> <volume>7</volume> (<issue>6</issue>): <fpage>615</fpage>&#x2013;<lpage>31</lpage>. doi:<pub-id pub-id-type="doi">10.1177/1745691612459058</pub-id>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="book"><string-name><surname>Nunnally</surname>, <given-names>Jum C.</given-names></string-name> <year>1970</year>. &#x201C;<source>Introduction to psychological measurement</source>.&#x201D; <publisher-name>McGraw-Hill</publisher-name>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Nosek</surname>, <given-names>Brian A.</given-names></string-name>, <string-name><given-names>J. R.</given-names> <surname>Spies</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Motyl</surname></string-name>. <year>1978</year>. <article-title>Psychometric Theory. Open Science Collaboration</article-title>, <source>Open Science</source>,</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="book"><string-name><given-names>C.</given-names> <surname>Hempel</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Hempel</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Oppenheim</surname></string-name>, <string-name><given-names>P. E.</given-names> <surname>Meehl</surname></string-name>, <string-name><given-names>J. R.</given-names> <surname>Platt</surname></string-name>, <string-name><given-names>B. A.</given-names> <surname>Nosek</surname></string-name>, <etal>et al.</etal> <year>2015</year>. &#x201C;<chapter-title>Estimating the reproducibility of psychological science</chapter-title>.&#x201D; <source>Science</source> (<publisher-loc>New York, N.Y</publisher-loc>.) <volume>349</volume> (<issue>6251</issue>): <fpage>aac4716</fpage>. doi:<pub-id pub-id-type="doi">10.1126/science.aac4716</pub-id>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Owen</surname>, <given-names>David R.</given-names></string-name>, <string-name><given-names>Astrid J.</given-names> <surname>Yeo</surname></string-name>, <string-name><given-names>Roger N.</given-names> <surname>Gunn</surname></string-name>, <string-name><given-names>Kijoung</given-names> <surname>Song</surname></string-name>, <string-name><given-names>Graham</given-names> <surname>Wadsworth</surname></string-name>, <string-name><given-names>Andrew</given-names> <surname>Lewis</surname></string-name>, <string-name><given-names>Chris</given-names> <surname>Rhodes</surname></string-name>, <etal>et al.</etal> <year>2012</year>. &#x201C;<article-title>An 18-kDa Translocator Protein (TSPO) polymorphism explains differences in binding affinity of the PET radioligand PBR28.&#x201D;</article-title> <source>Journal of Cerebral Blood Flow and Metabolism 13/15</source> <volume>32</volume> (<issue>1</issue>): <fpage>1</fpage>&#x2013;<lpage>5</lpage>. doi:<pub-id pub-id-type="doi">10.1038/jcbfm.2011.147</pub-id>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Portney</surname>, <given-names>Leslie G</given-names></string-name>, and <string-name><given-names>Mary P</given-names> <surname>Watkins</surname></string-name>. <year>2015</year>. <article-title>Foundations of clinical research: applications to practice</article-title>. <source>FA Davis.</source></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Ruf</surname>, <given-names>B M</given-names></string-name>, and <string-name><given-names>Z</given-names> <surname>Bhagwagar</surname></string-name>. <year>2009</year>. &#x201C;<article-title>The 5-HT1B receptor: a novel target for the pathophysiology of depression.&#x201D;</article-title> <source>Current Drug Targets</source> <volume>10</volume> (<issue>11</issue>): <fpage>1118</fpage>&#x2013;<lpage>38</lpage>. doi:<pub-id pub-id-type="doi">10.2174/138945009789735192</pub-id>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Shore</surname>, <given-names>J.</given-names></string-name> <year>2004</year>. &#x201C;<article-title>Fail fast.&#x201D;</article-title> <source>IEEE Software</source> <volume>21</volume> (<issue>5</issue>): <fpage>21</fpage>&#x2013;<lpage>25</lpage>. doi:<pub-id pub-id-type="doi">10.1109/MS.2004.1331296</pub-id>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Shrout</surname>, <given-names>Patrick E.</given-names></string-name>, and <string-name><given-names>Joseph L.</given-names> <surname>Fleiss</surname></string-name>. <year>1979</year>. &#x201C;<source>Intraclass correlations: Uses in assessing rater reliability</source>.&#x201D; doi:<pub-id pub-id-type="doi">10.1037/0033-2909.86.2.420</pub-id>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Simmons</surname>, <given-names>Joseph P.</given-names></string-name>, <string-name><given-names>Leif D.</given-names> <surname>Nelson</surname></string-name>, and <string-name><given-names>Uri</given-names> <surname>Simonsohn</surname></string-name>. <year>2011</year>. &#x201C;<article-title>False-Positive Psychology: Undis575 closed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant.&#x201D;</article-title> <source>Psychological Science</source> <volume>22</volume> (<issue>11</issue>): <fpage>1359</fpage>&#x2013;<lpage>66</lpage>. doi:<pub-id pub-id-type="doi">10.1177/0956797611417632</pub-id>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Spearman</surname>, <given-names>Charles</given-names></string-name>. <year>1904</year>. &#x201C;<article-title>The proof and measurement of association between two things.&#x201D;</article-title> <source>The American Journal of Psychology</source> <volume>15</volume> (<issue>1</issue>). JSTOR: <fpage>72</fpage>&#x2013;<lpage>101</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Vul</surname>, <given-names>Edward</given-names></string-name>, <string-name><given-names>Christine</given-names> <surname>Harris</surname></string-name>, <string-name><given-names>Piotr</given-names> <surname>Winkielman</surname></string-name>, and <string-name><given-names>Harold</given-names> <surname>Pashler</surname></string-name>. <year>2009</year>. &#x201C;<article-title>Puzzlingly High Correlations in fMRI Studies of Emotion, Personality, and Social Cognition1.&#x201D;</article-title> <source>Perspectives on Psychological Science</source> <volume>4</volume> (<issue>3</issue>): <fpage>274</fpage>&#x2013;<lpage>90</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1745-6924.2009.01125.x</pub-id>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Weir</surname>, <given-names>Joseph P Jp</given-names></string-name>. <year>2005</year>. &#x201C;<article-title>Quantifying test-retest reliability using the intraclass correlation coefficient and the SEM.&#x201D;</article-title> <source>Journal of Strength and Conditioning Research / National Strength &#x0026; Conditioning Association</source> <volume>19</volume> (<issue>1</issue>): <fpage>231</fpage>&#x2013;<lpage>40</lpage>. doi:<pub-id pub-id-type="doi">10.1519/15184.1</pub-id>.</mixed-citation></ref>
</ref-list>
<app-group>
<app id="app1">
<label>APPENDICES</label>
<title>Appendix 1: Outcome Measures</title>
<p>BP<sub>ND</sub> is a measure of the binding potential: a measure of concentration of the radioligand in the specific (S) compartment of the target tissue relative to sum of free (F) and non-specific (NS) compartments (together defined as the nondisplaceable (ND) compartment). When BP<sub>ND</sub> is zero, there is no S binding; and when it is 1, there is an equal equal radioligand concentration in the S and ND compartments. V<sub>T</sub> is the total distribution volume: a measure of the total uptake in the target tissue (S &#x002B; NS &#x002B; F) relative to the concentration of radioligand in blood plasma. It is therefore a measure of the uptake in the target tissue relative to the exposure of that tissue to the radioligand. DVR is the distribution volume ratio, and is the ratio of V<sub>T</sub> values from a target (numerator) and reference (denominator) region of the tissue. SUV is the standardised uptake value: a measure of the total uptake in the target tissue relative to the amount of tracer injected and the body mass. This measure is sensitive to differences in tissue exposure, due to peripheral binding for example, which is assumed to scale with body weight. These outcomes range from more to less specific in this order, and different outcomes are used for different circumstances depending on the fulfilment of certain assumptions underlying them, as well as whether arterial plasma radioactivity concentrations were recorded. More information can be found in Innis et al (2007).</p>
<p>Innis, R. B., Cunningham, V. J., Delforge, J., Fujita, M., Gjedde, A., Gunn, R. N.,&#x2026; &#x0026; Iida, H. (2007). Consensus nomenclature for in vivo imaging of reversibly binding radioligands. <italic>Journal of Cerebral Blood Flow &#x0026; Metabolism</italic>, 27(9), 1533-1539.</p>
</app>
<app id="app2">
<title>Appendix 2: Example 1 Caveat</title>
<p>There is one caveat: the correlational study of age was performed using partial volume effect (PVE) correction. This procedure is used to minimise the bias due to differences in brain volumes and usually produces higher BP<sub>ND</sub> estimates. However it can also introduce more noise to the data (i.e. a higher SEM). Without access to the original test-retest data, this increase in SEM (<italic>&#x03C1;</italic>) would have to be approximated by assuming that the true between-subject variance is similar between the test-retest sample and the subset of participants from the correlational study of similar age to the test-retest sample (i.e. <italic>&#x03C1;&#x2248; COV</italic><sub><italic>NewStudySubset</italic></sub> <italic>/COV</italic><sub><italic>T</italic></sub> <sub><italic>RT</italic></sub>). Another possibility would be to consider the reliability for possible values of <italic>&#x03C1;</italic>. In this way, it can be shown that even in the extreme case that the SEM were expected to be doubled following PVE correction, the extrapolated reliability would remain sufficiently high at 0.73.</p>
</app>
<app id="app3">
<title>Appendix 3: Example 3 Caveat</title>
<p>Radioligand displacement by endogenous or exogenous substances is common in PET studies, resulting in a decrease in binding. However, it is worth noting that clinical doses of SSRIs have been found to produce a paradoxical increase in [<sup>11</sup><italic>C</italic>]AZ10419369 BP<sub>ND</sub> in humans (<xref ref-type="bibr" rid="c24">Nord et al. 2013</xref>) for reasons beyond the scope of the current paper. Large doses of SSRIs have been found to produce decreases in NHPs though (<xref ref-type="bibr" rid="c24">Nord et al. 2013</xref>). For convenience, we will assume that high doses similar to those administered to NHPs will be administered, and assume that it will produce the same change in humans.</p>
</app>
</app-group>
</back>
</article>