<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/141010</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Inferring Functional Neural Connectivity with Deep Residual Convolutional Networks</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Dunn</surname>
<given-names>Timothy W.</given-names>
</name>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Koo</surname>
<given-names>Peter K.</given-names>
</name>
</contrib>
<aff><institution><italic>Harvard University</italic></institution></aff>
</contrib-group>
<pub-date pub-type="epub"><year>2017</year></pub-date>
<elocation-id>141010</elocation-id>
<history>
<date date-type="received">
<day>22</day>
<month>5</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>22</day>
<month>5</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="141010.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<p>Measuring synaptic connectivity in large neuronal populations remains a major goal of modern neuroscience. While anatomical methods are advancing, computational techniques for inferring functional connectivity from recordings of neural activity represent a promising avenue of analysis. Here, we report significant improvements to a deep learning method for functional connectomics, as assayed on synthetic ChaLearn Connectomics data. The method, which integrates recent advances in convolutional neural network architecture and model-free partial correlation coefficients, outperforms published methods on competition data and can achieve over 90&#x0025; precision at 1&#x0025; recall on validation datasets. This suggests that future application of the model to <italic>in vivo</italic> whole-brain imaging data in larval zebrafish could reliably recover on the order of 10<sup>6</sup> synaptic connections with a 10&#x0025; false discovery rate. The model also generalizes to networks with different underlying connection probabilities and should scale well when parallelized across multiple GPUs. The method offers real potential as a statistical complement to existing experiments and circuit hypotheses in neuroscience.</p>
</abstract>
<counts>
<page-count count="11"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<label>1.</label>
<title>Introduction</title>
<p>Precise brain wiring diagrams can be used to validate and generate hypotheses of neural circuit function. In recent years, considerable resources have been devoted to ultrastructural reconstructions of synaptic connectivity from serial section electron microscopy (EM) data (<xref ref-type="bibr" rid="c1">Briggman et al., 2011</xref>; <xref ref-type="bibr" rid="c2">Chalfie et al., 1985</xref>; <xref ref-type="bibr" rid="c3">Hildebrand et al., 2017</xref>). But these anatomical methods, while improving, remain slow, costly, and laborious. An emerging alternative to EM reconstruction is statistical inference of the synaptic connectivity latent in observed neural activity patterns. Various statistical methods for inferring connectivity have been published, ranging from model-free approaches, such as Granger causality (<xref ref-type="bibr" rid="c4">Cadotte et al., 2008</xref>; <xref ref-type="bibr" rid="c5">Garofalo et al., 2009</xref>), transfer entropy (<xref ref-type="bibr" rid="c6">Stetter et al., 2012</xref>), and partial correlation coefficients (<xref ref-type="bibr" rid="c7">Sutera et al., 2014</xref>), to model-based Bayesian approaches that seek to invert generative models of neural activity (<xref ref-type="bibr" rid="c8">Mishchenko et al., 2011</xref>; <xref ref-type="bibr" rid="c9">Soudry et al., 2015</xref>).</p>
<p>One critical issue with connectivity inference is that two unconnected neurons with common inputs will have correlated activity that is easily interpreted as causal if the common input is unobserved. A recent method proposes a generalized linear model-based Bayesian method to combat the common input problem when the analysis is coupled with a &#x201C;shotgun&#x201D; imaging experiment that hypothetically measures activity in all neurons serially (<xref ref-type="bibr" rid="c9">Soudry et al., 2015</xref>). The method is accurate and efficient when prior information about connection sparsity is known and when activity in the neural population of interest behaves according to the dynamics assumed by the generative model. In practice, these are strong assumptions when the sparsity of the network is unknown.</p>
<p>The common input problem ceases to be an issue in neural systems with complete observability. Such universal access has been granted by recent technological advances enabling cellular resolution fluorescence imaging of neural activity on large scales (<xref ref-type="bibr" rid="c10">Chen et al., 2013</xref>). In particular, the activity of nearly all neurons in the larval zebrafish brain can be imaged at speeds up to 12 Hz (<xref ref-type="bibr" rid="c11">Ahrens et al., 2013</xref>; <xref ref-type="bibr" rid="c12">Dunn et al., 2016</xref>; <xref ref-type="bibr" rid="c13">Tomer et al., 2015</xref>). While these experiments skirt the common input problem, the acquired data presents a unique set of challenges for connectivity inference, as action potentials are obscured by the noise and slow dynamics of fluorescent indicators. Nevertheless, there have been attempts to extract meaningful connectivities from these recordings. Recently, several methods were published together as part of the Kaggle ChaLearn Connectomics competition (<xref ref-type="bibr" rid="c14">Orlandi et al., 2014</xref>). Here, we report an improved deep learning method that outperforms the competition leader and, importantly, provides a viable false discovery rate (FDR) at a true positive rate (TPR) poised to reveal millions of synaptic connections in real brain volumes.</p>
<p>For clarity, throughout this report we will refer to neurons in the synthetic networks that emit measurable fluorescence signals as &#x201C;cells&#x201D; and refer to those synthetic networks as &#x201C;graphs.&#x201D; The neural networks used for prediction and classification will be referred to as &#x201C;networks&#x201D; or &#x201C;models.&#x201D;</p>
<p>Code for the following models and analyses is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/spoonsso/TFconnect">https://github.com/spoonsso/TFconnect</ext-link>.</p>
</sec>
<sec id="s2">
<label>2.</label>
<title>Related Work</title>
<p>Convolutional neural networks (CNNs) are considerably powerful for computer vision, automatically extracting spatially invariant, hierarchical features and performing classification within an end-to-end learning framework (<xref ref-type="bibr" rid="c15">Krizhevsky et al., 2012</xref>). Outside of the computer vision domain, CNNs were recently adapted to analyze fluorescence time series from cells in connected graphs, with spatiotemporal filters operating across cells over time (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>) (<xref ref-type="bibr" rid="c16">Romaszko, 2015</xref>). These networks learned to classify pairwise binary connectivity when trained on activity generated from synthetic graphs of 1000 cells, using binary labels from the associated <italic>in silico</italic> connectivity matrix.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig 1:</label>
<caption><p>Overview of the problem and method. (a) We train a deep convolutional neural network to predict the underlying connectivity. (b) The graphs consists of 1000 cells separated into 10 sub-graphs (different colors) with higher clustering coefficients. For clarity, only 400 cells are shown. (c) The goal of the method is to predict the presence or absence of a connection between each cell pair in the network based only on the observed activity-generated fluorescence in each cell. The method operates on signals subsampled at time points where the overall network activity is high (black dots).</p></caption>
<graphic xlink:href="141010_fig1.tif"/>
</fig>
<p>Over the past three years, a host of improvements to CNN architectures have been published. These improvements include dropout (<xref ref-type="bibr" rid="c17">Srivastava et al., 2014</xref>), where a random subset of connection weights are temporarily withheld over a mini-batch during training in order to combat overfitting, batch normalization (<xref ref-type="bibr" rid="c18">Ioffe and Szegedy, 2015</xref>), where the pre-activated output of each layer is re-normalized before being passed to the next layer in order to improve training efficiency, and residual blocks (<xref ref-type="bibr" rid="c19">He et al., 2015</xref>), which drastically increase the effective depth and therefore the expressivity of the CNN. Furthermore, parametric rectified linear unit (PReLU) activations allow the network to learn a suitable leakiness in its non-linear rectification activation, and intelligent weight initialization has been shown to increase training efficiency (<xref ref-type="bibr" rid="c20">He et al., 2016</xref>).</p>
<p>In parallel, several alternative methods for inferring functional connectivity have been published. One method, using a partial correlation coefficient metric estimated from the inverse covariance matrix (<xref ref-type="bibr" rid="c7">Sutera et al., 2014</xref>), set the state-of-the-art benchmark at the inaugural Kaggle ChaLearn Connectomics competition, edging out baseline summary statistics like correlation coefficients and entropy-based causality estimations (<xref ref-type="bibr" rid="c14">Orlandi et al., 2014</xref>).</p>
<p>We find that by incorporating these advances in deep network design, and by combining partial correlation measurements with fluorescence traces, we can dramatically increase the performance of a residual CNN (RCNN) model for connectomics analysis. While we focus primarily on the Kaggle ChaLearn competition datasets, as they enable direct comparisons across several methods, we will discuss our work in the context of formal Bayesian approaches that attempt to infer underlying connectivity matrices via inversion of generative models. Compared to these approaches, we suggest that our RCNN method is more robust to variation in graph connection probability and will be much more scalable.</p>
</sec>
<sec id="s3">
<label>3.</label>
<title>Synthetic Graph Architecture</title>
<p>The Kaggle ChaLearn Connectomics data are generated from a realistic model of neural dynamics, calcium buffering, and fluorescence (<xref ref-type="bibr" rid="c6">Stetter et al., 2012</xref>; <xref ref-type="bibr" rid="c14">Orlandi et al., 2014</xref>). Graph model parameters were tuned so that cells were spontaneously active, including periods of pan-graph bursting. These parameters and associated dynamics were chosen by the competition organizers to closely resemble real networks of primary cultured neurons. In the future, we aim to train on synthetic data generated from graphs whose architectures and dynamics better resemble that of brains in <italic>vivo</italic>. That being said, the existing competition dataset does include realistic features that are likely shared by the brains of real animals.</p>
<p>In our study, we used signals from cells in 8 different graphs for the purposes of training, validation, testing, and analysis. Six of the graphs had the same average architecture, with 3 used for training sets, 1 for validation, and 2 for testing. Each of these graphs contained 1000 cells with an overall average connection probability <italic>&#x03C1;</italic> &#x003D; 0.014 (<xref rid="fig1" ref-type="fig">Fig. 1b</xref>). The graph comprised 10 subgraphs whose cells had higher within group connection probabilities (<italic>&#x03C1;<sub>int</sub></italic> &#x003D; 0.012) than between group connection probabilities (<italic>&#x03C1;<sub>ext</sub></italic> &#x003D; 0.002). The weights of connections, which were all positive/excitatory, were the same within a given subgraph, and the weights in each subgraph were ultimately optimized to produce a desired universal bursting frequency of 0.1 Hz.</p>
</sec>
<sec id="s4">
<label>4.</label>
<title>Residual Convolutional Neural Network Architecture</title>
<sec id="s4a">
<title>Preprocessing of fluorescence signals</title>
<p>Before training, signals were subjected to three rounds of preprocessing in order to accentuate information content and reduce overall data size. First, signal noise arising from simulated light scattering effects was removed via spatial deconvolution (physical locations of each cell were provided as part of the dataset). Second, each signal was downsampled by thresholding a high-pass filtered representation of the total graph activity (<xref rid="fig1" ref-type="fig">Fig. 1c</xref>). Third, each downsampled signal was z-scored by subtracting the downsampled signal mean and dividing by the downsampled signal standard deviation.</p>
</sec>
<sec id="s4b">
<title>Structuring training input</title>
<p>Following the general scheme of (<xref ref-type="bibr" rid="c16">Romaszko, 2015</xref>), these processed signals were then assembled into chunks of 3x330 continuous samples, beginning at random starting points in the processed signals. The first two rows of this data structure contained processed signals from a pair of cells, with the associated label referencing the binary directed connection between the cell in row 1 and the cell in row 2. The third row contained the corresponding average fluorescence across the entire graph for the time interval included in rows 1 and 2 (<xref rid="fig1" ref-type="fig">Fig. 1a</xref>).</p>
<p>The final training set included approximately 1.1 million paired examples in this format, with equal representation of positive and negative examples (i.e. the presence or absence of a directed connection, respectively). Note that 550,000 far exceeds the total number of positive examples contained in 3 networks (&#x007E; 42,000), but a rich, unique training set can be assembled because the starting position for each 330-sample training chunk is random for each example. Of the &#x007E; 1.1 million examples in the training set, 75&#x0025; were used for training epochs and the other 25&#x0025; were used as cross-validation.</p>
</sec>
<sec id="s4c">
<title>Residual convolutional neural network architecture</title>
<p>The RCNN begins with input [3 &#x00D7; 330 &#x00D7; 1], contains 1 block of (1 &#x002B; 2) convolutional &#x002B; residual convolutional layers (each [2 &#x00D7; 326 &#x00D7; 32] with filters [2x5x1]), another 1 block of (1 &#x002B; 2) convolutional &#x002B; residual convolutional layers (each [1 &#x00D7; 322 &#x00D7; 64] with filters [2 &#x00D7; 5 &#x00D7; 32]) followed by [1 &#x00D7; 10] max pooling, another full convolutional layer (size [1 &#x00D7; 32 &#x00D7; 128] with filters [1 &#x00D7; 1 &#x00D7; 64]), 1 block of (1 &#x002B; 2) dense &#x002B; residual dense layers (each with 256 units), and a dense readout layer (2 units) indicating the final softmax classification (<xref rid="fig2" ref-type="fig">Fig. 2a</xref>). Each layer implemented PReLU activations.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig 2:</label>
<caption><p>Details of the RCNN and training. (a) Summary of layer architecture. (b) Training time-course for a single model. Where indicated, the batch size was annealed to promote learning. (c) Training time-course but for area under the receiver operator characteristic and precision recall curve metrics (ROC-AUC and PR-AUC, respectively). Note that the typical number of training epochs was 200 for each batch period (this example shows only 100/50/50).</p></caption>
<graphic xlink:href="141010_fig2.tif"/>
</fig>
</sec>
<sec id="s4d">
<title>Training</title>
<p>The model was trained using a cross-entropy loss function based on proper binary classification of each cell pair&#x2019;s connectivity (<xref rid="fig1" ref-type="fig">Fig. 1a, 1c</xref>). For each individual model, we started with a batch size of 100 and performed a maximum of 200 epochs with an automatic early stopping criterion: if the best cross-validation loss did not improve over a span of 20 epochs, training halted. We then annealed the batch size, progressing to 1000 examples and then 2000 examples per batch. These extra steps consistently resulted in large, sudden reductions in training loss (<xref rid="fig2" ref-type="fig">Fig. 2b</xref>) and gains in performance metrics (<xref rid="fig2" ref-type="fig">Fig. 2c</xref>). Gradient descent was performed using Adam optimization using recommended default parameters (<xref ref-type="bibr" rid="c21">Kingma and Ba, 2014</xref>). We employ <italic>l</italic>2 weight decay with a regularization parameter set to 10<sup>&#x2212;6</sup>.</p>
<p>The RCNN was implemented in Python using tfomics (<ext-link ext-link-type="uri" xlink:href="https://github.com/p-koo/tfomics">https://github.com/p-koo/tfomics</ext-link>), a high-level API for TensorFlow (v0.12.1). On a single NVIDIA Titan X Pascal graphics card, 1 epoch during the 100 batch phase took 434 s (50 ms / batch) including a forward pass for crossvalidation, during the 1000 batch phase took 241 s (280 ms / batch), and during the 2000 batch phase took 245 s (569 ms / batch).</p>
<p>During training, we also introduced dropout at each of the layers in the RCNN. For the convolutional layers, the dropout probability was set to 0.2, and for the dense layers, the dropout probability was set to 0.5. At each layer, we included batch normalization of parameters. At the beginning of training, weights were initialized following the distribution outlined in (<xref ref-type="bibr" rid="c20">He et al., 2016</xref>). Taken together, these measures help to reduce overfitting and increase training efficiency.</p>
</sec>
<sec id="s4e">
<title>Testing</title>
<p>Similar to (<xref ref-type="bibr" rid="c16">Romaszko, 2015</xref>), we completed 14 forward passes through the trained network for each cell pair, tiling the entire length of the signals in 330 sample patches. The predicted scores were then averaged across each of these 14 passes to arrive at a final prediction for connectivity between the two cells. Forward passes took 1.3 ms per cell pair, for a total of &#x007E;21.7 minutes to evaluate the entire connectivity matrix. We then ensemble averaged the predictions for each cell pair from 5 independent models trained separately, with differences in final model parameters resulting from the random initialization of weights and the random assembly of network input from training data.</p>
</sec>
</sec>
<sec id="s5">
<label>5.</label>
<title>Results</title>
<sec id="s5a">
<title>The RCNN sets competition benchmark</title>
<p>When we evaluated the RCNN performance on the Kaggle ChaLearn Connectomics competition test datasets, it outperformed all other entries in the area under the receiver operator characteristic (ROC) curve (<xref rid="tbl1" ref-type="table">Table 1</xref>). Examination of full ROC curves on the &#x201C;normal&#x201D; validation network revealed that while the model ensemble outperformed any individual model in total area under the curve (AUC), some individual models performed better than the ensemble locally within small ranges along the curve (<xref rid="fig3" ref-type="fig">Fig. 3a</xref>). The RCNN model also outperformed the winning algorithm, which was based on partial correlation coefficients (PCs) between cells across the dataset, on a denser validation network with a higher average connection probability <italic>&#x03C1;</italic> &#x003D; 0.022. However, the RCNN model struggled on a sparser validation network with a lower average connection probability <italic>&#x03C1;</italic> &#x003D; 0.007 (<xref rid="tbl1" ref-type="table">Table 1</xref>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig 3:</label>
<caption><p>Performance of the RCNN. (a) ROC curves on a validation network for 5 individual models (gray lines) and a final ensemble-averaged model (black line). (b) PR curves for the same models. (c) Schematics of three problematic connectivity motifs with which the model struggles. Black arrows represent true connections and red arrows denote inferred false positives.</p></caption>
<graphic xlink:href="141010_fig3.tif"/>
</fig>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1</label>
<caption><p>Comparison of algorithms on predictions for five networks from the Chalearn connectomics challenge. Area under the ROC and PR curves is compared. PR curves for Test Networks 1 and 2 are unavailable as the data are managed by Kaggle. RCNN &#x002B; PC sets benchmarks in all networks and metrics.</p></caption>
<graphic xlink:href="141010_tbl1.tif"/>
</table-wrap>
</sec>
<sec id="s5b">
<title>Evaluating the model&#x2019;s realistic potential</title>
<p>The ultimate goal of our method is to infer, with an acceptable degree of precision, real synaptic connectivity from real neural imaging data. Examination of the model&#x2019;s precision recall (PR) curve on the validation dataset suggests that RCNNs could achieve a stable level of &#x007E;80&#x0025; precision (&#x007E;20&#x0025; FDR) at up to 5&#x0025; recall (<xref rid="fig3" ref-type="fig">Fig. 3b</xref>). For a real zebrafish brain imaging dataset with 10<sup>5</sup> cells and 10<sup>10</sup> possible connections with &#x007E; 2&#x0025; connectivity (<xref ref-type="bibr" rid="c22">Stobb et al., 2012</xref>), even 5&#x0025; recall would report 10<sup>7</sup> connections, with 1 in 5 being false. At an FDR of 10&#x0025;, the RCNN recalls 0.07&#x0025; of the connections, or &#x007E; 10<sup>5</sup> connections in a typical zebrafish brain.</p>
<p>To gain a better understanding of where the RCNN model fails, we quantified the frequency of three potentially problematic connectivity motifs. Because the RCNN operates only on pairwise input, the model is particularly susceptible to misclassifying correlated activity from common inputs as a directed causal relationship (<xref rid="fig3" ref-type="fig">Fig. 3c</xref>, top). Similarly, and enhanced by the slow indicator dynamics and acquisition rate, the RCNN might also mistake a chain of connectivity for a directed connection between the first and last cell in the chain (<xref rid="fig3" ref-type="fig">Fig. 3c</xref>, bottom left). Finally, without access to millsecond spike times, it should be difficult for a model operating on slow and noisy fluorescence signals to infer the direction of causality in correlated activity (<xref rid="fig3" ref-type="fig">Fig. 3c</xref>, bottom right). Indeed, 47&#x0025; of all possible common input relationships, 49&#x0025; of all possible &#x201C;short circuit&#x201D; motifs, and 71&#x0025; of all possible &#x201C;reciprocated&#x201D; connectivities were classified as false positives (<xref rid="tbl2" ref-type="table">Table 2</xref>). The model especially struggled when two or more of these motifs were combined, with 98&#x0025; of all possible false positives shared by all 3 motifs being classified erroneously.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2</label>
<caption><p>Analysis of predicted false positives (FPs) for the original RCNN model and the improved RCNN model incorporating partial correlation coefficients. For each of the FP types (icons left to right: common input, short circuit, reciprocated), and combinations thereof (overlapping double and triple motifs), the total number of false positives and, fraction of the total possible base frequency of each type are indicated. FPs are evaluated using either forced choice or at a 99<sup>th</sup> percentile prediction score threshold.</p></caption>
<graphic xlink:href="141010_tbl2.tif"/>
</table-wrap>
</sec>
<sec id="s5c">
<title>Improving the model</title>
<p>We reasoned that many of these errors could be eliminated if the RCNN could make multivariate evaluations that considered the activity across all cells in the graph rather than just pairwise correlations. Partial correlation coefficients (PCs) are multivariate summaries of causality and led the overall competition leaderboard (<xref ref-type="bibr" rid="c7">Sutera et al., 2014</xref>). Reasoning that the strengths of PC-based classification and our RCNN model might be complementary, we incorporated the PC for each cell pair into our input structure as a fourth row of data during evaluation and training. Other than adjusting the size of input layer and the first residual block to accommodate the PC addition, this RCNN &#x002B; PC model used the same gross network topology.</p>
<p>The final RCNN &#x002B; PC model, which was an ensemble average of 8 models, including 1 model trained on data with an unequal representation of positive and negative examples (10&#x0025; and 90&#x0025;, respectively), significantly improved the ROC curves on the validation dataset (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>) and the ROC-AUC and PR-AUC metrics on all validation and test datasets (<xref rid="tbl1" ref-type="table">Table 1</xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Fig 4:</label>
<caption><p>Improvement of predictions in precise regimes. (a) Full PR curves on a validation dataset for the original RCNN, a new RCNN that incorporates multivariate partial correlation (RCNN &#x002B; PC), and PC alone (b). Zoomed-in PR curves for the same models, plus a model with pre-ensemble weight optimization (opt) and an ensemble average excluding a model with low representation of positive examples (standard). (c) Full ensemble (solid) and &#x201C;standard&#x201D; ensemble (dashed) PR curves for additional dense (black) and sparse (magenta) connectivity validation networks.</p></caption>
<graphic xlink:href="141010_fig4.tif"/>
</fig>
<p>Comparing the precise regions of the PR curves across algorithms reveals that the RCNN &#x002B; PC model provides an increase in the precision plateau at the start of the curve, maintaining over 85&#x0025; precision past 3&#x0025; recall, a significant improvement over the original RCNN model and the PC algorithm alone (<xref rid="fig4" ref-type="fig">Fig. 4b</xref>). Note that at up to 0.5&#x0025; recall, the PC algorithm shows enhanced precision over the RCNN &#x002B; PC model, but this may be due to the incorporation of predictions from the model trained on 10&#x0025;/90&#x0025; positive/negative examples; without this model, the RCNN &#x002B; PC ensemble shows enhanced precision at low recall but does not sustain this precision for long. We were also able to enhance the early precision (from 0 to 2&#x0025; recall) of the full RCNN &#x002B; PC ensemble using a Gaussian process search for each model&#x2019;s weight.</p>
<p>These improvements in PR reflect, at least in part, reductions in the misclassification of some problematic connectivity motifs (<xref rid="tbl2" ref-type="table">Table 2</xref>). Overall common input false positives (FPs) were reduced by 20&#x0025;, &#x201C;short circuit&#x201D; FPs by 22&#x0025;, &#x201C;reciprocated&#x201D; FPs by 3&#x0025;, doublet FPs by 14&#x0025;, and triplet FPs by 2&#x0025;. For the top 1&#x0025; of the predictions, however, these gains were somewhat muted. In this regime, &#x201C;reciprocated&#x201D; FPs actually increased in the RCNN &#x002B; PC, perhaps indicating the dominance of the PC representation, which is agnostic to edge direction, in this regime.</p>
</sec>
</sec>
<sec id="s6">
<label>6.</label>
<title>Model Generalization</title>
<p>Effective generalization is a major consideration for future applications of these models to real data. Any model trained on data generated from a limited set of synthetic graphs will likely need to operate on signals arising from a graph with a different underlying parameter distribution, even if the models can eventually be trained on ground truth labeled data. It is thus important that the predictive power of the model generalize to validation graphs with different architectures that were not included in the training set. We thus tested the trained RCNN &#x002B; PC ensemble on the dense and sparse validation datasets. For the sparse graph, the model was reduced in effectiveness, achieving on 75&#x0025; precision at 1&#x0025; recall, although there continued to be regimes where the model achieved high levels of precision. For the dense graph, the model performed better, achieving almost 95&#x0025; precision at 1&#x0025; recall (<xref rid="fig4" ref-type="fig">Fig. 4c</xref>). Note that the connection probability of the dense graph is close to the anticipated average connection density of the zebrafish brain (<xref ref-type="bibr" rid="c22">Stobb et al., 2012</xref>).</p>
<p>Successful generalization also requires that a desirable prediction threshold deduced from analysis on validation data be effective when applied to real data, where PR curves cannot be plotted in search of a viable threshold. While the distribution tails of prediction scores depend on connection sparsity, (<xref rid="fig5" ref-type="fig">Fig. 5a</xref>), we find that recall and precision on the dense and sparse validation data can be recovered by choosing thresholds matched to prediction score percentiles in the &#x201C;normal&#x201D; validation dataset (<xref rid="fig5" ref-type="fig">Figs. 5b, 5c</xref>).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Fig 5:</label>
<caption><p>Generalization of the model to networks with different structure. (a) Histograms of score distribution tails for each network. (b) Model recall for densely and sparsely connected validation networks as a function of recall on a validation network whose density matched the training data. (c) Precision for each network as a function of validation recall.</p></caption>
<graphic xlink:href="141010_fig5.tif"/>
</fig>
</sec>
<sec id="s7">
<label>7.</label>
<title>Scalability</title>
<p>The evaluation times for the trained RCNN &#x002B; PC model have time complexity <italic>O</italic>(<italic>n</italic><sup>2</sup>), as the network independently operates on all cell pairs in the dataset. With a current evaluation time for 1.3 seconds for a single cell pair, we anticipate that applying the method on a full fish dataset of 10<sup>5</sup> cells will require 3611 GPU hours. This evaluation process is easily parallelized, however, as independent GPUs can load the final model parameters and evaluate batches of input independently. On a 128 GPU cluster, this process would take 28.2 hours for a single model running perfectly parallel. While this evaluation would need to be run multiple times, once for each model in the ensemble, we note that we found a single model that outperformed the ensemble on the competition test ROC-AUC metric. Future work should determine the utility of single-model solutions to reduce compute time. We also note that these times reflect evaluation on an entire brain. Analyses of local connectivity in smaller brain nuclei would be considerably faster.</p>
</sec>
<sec id="s8">
<label>8.</label>
<title>Discussion</title>
<sec id="s8a">
<title>Improvements to the RCNN architecture</title>
<p>Future work should explore a larger space of layer topology and hyperparameters in order to test the limits of classification efficacy. Deeper models may improve the overall performance and efficiency of the training process. The input data structure may also benefit from an expansion in the total number of samples passed to the network during evaluation and training. Longer traces, which contain more information about the temporal relationships between cells, may also improve the overall performance of the model.</p>
<p>The model should, in principle, be at least as good as the PC metrics passed to the network alongside the fluorescence signals, but in early regimes of the PR curve, the PC algorithm alone appears to be more accurate. In order to provide more salience to the PC score, it may help to pass PC coefficients through the model separated from the learned convolutional filters, as PC coefficients have no spatiotemporal correspondence to cell activity. Future model designs should also incorporate better multivariate representations of cell activity throughout the graph. If the model were able to operate on more than just one cell pair at a time, it could in principle learn representations of higher order correlational structure.</p>
<p>In order to combat problematic connectivity motifs, it may help to over-represent them in the training dataset. In the final RCNN &#x002B; PC ensemble, we included a model that was trained on 10&#x0025;/90&#x0025; positive/negative examples rather than 50&#x0025;/50&#x0025; with the hope that including more FP examples might help the network distinguish FPs from true positives more easily. While we have not explored this configuration exhaustively, that model appeared to contribute greater precision in the 1-5&#x0025; recall range at the expense of precision in the 0-1&#x0025; recall range. Future work should explore the extent to which models trained on different proportions of positive and negative examples can complement each other and enhance overall classification accuracy.</p>
</sec>
<sec id="s8b">
<title>Generalization to non-synthetic neural data</title>
<p>While ground truth labeled data may become available as EM and all-optical circuit mapping techniques progress, the success of the current method will rely on synthetic training data reflecting realistic models of neural dynamics. Studies of neural network generalization from synthetic to real data (<xref ref-type="bibr" rid="c23">Jaderberg et al., 2014</xref>; <xref ref-type="bibr" rid="c24">Le et al., 2017</xref>) suggest that training on synthetic data can result in powerful generalization, given that the networks are trained on a large enough space of synthetic examples. Going forward, it will be critical to simulate neural activity and concomitant fluorescence signals that better reflect the activity observed <italic>in vivo</italic>. We also note that generalization of the RCNN model will depend on what it has learned to detect in the fluorescence traces. It is unclear how much the learned convolution filters depend on the specifics of the observed neural dynamics and thus the underlying parameters of the generative model of simulated activity. It&#x2019;s possible that the RCNN model is learning a model-free causality statistic that will generalize well even to neural dynamics that differ substantially from the training data. Future saliency analyses and probes of RCNN network representation will thus speak to the method&#x2019;s overall potential for generalization.</p>
<p>Ultimately, the vision is to train the RCNN on synthetic data closely matching the dynamics observed spontaneously in larval zebrafish (<xref ref-type="bibr" rid="c12">Dunn et al., 2016</xref>). Conservative predictions, thresholded at a percentile reflecting a target confidence in precision or recall, can be cross-validated against known larval zebrafish brain anatomy and eventually validated via optical or ultrastructural circuit mapping methods, such as channelrhodopsin/GCaMP (<xref ref-type="bibr" rid="c25">Packer et al., 2015</xref>), PA-GFP (<xref ref-type="bibr" rid="c12">Dunn et al., 2016</xref>), and EM (<xref ref-type="bibr" rid="c3">Hildebrand et al., 2017</xref>). The idea is not that the current method will be able to uncover the complete connectome of an animal but rather that it will be able to lend statistical support to circuit-level hypotheses of neural structure and function.</p>
<p>One advantage of this method over Bayesian techniques for inferring functional connectivity is that it works directly on calcium imaging data, avoiding slow and potentially error-prone inference of spike timing from fluorescence signals. Furthermore, full Bayesian inference of adjacency matrices is slow, and faster approximations typically make strong assumptions about underlying connection sparsity (<xref ref-type="bibr" rid="c9">Soudry et al., 2015</xref>), a property that is not straightforward to estimate in real systems. In contrast, our method is relatively robust to changes in connection probability.</p>
<p>Finally, we note that the precision of the method is agnostic to the types of underlying FPs. While it will be important in the future to better combat problems related to common inputs in order to improve the overall accuracy of the method, the RCNN &#x002B; PC model is applicable in its current form as long as one accepts the uncertainty of a target FDR. As in genomics analyses, a target FDR of 5 to 10&#x0025; should be reasonable assuming the researcher is aware of its implications.</p>
</sec>
<sec id="s8c">
<title>Summary</title>
<p>We present here a new deep learning method for inferring functional connectivity from recordings of fluorescence from calcium-dependent activity indicators. The method sets an important state-of-the-art benchmark, as evaluated on the Kaggle ChaLearn Connectomics competition, and shows immediate promise for applications to real data. Going forward, a comprehensive understanding of the constraints and pitfalls of the method will be critical for establishing reliable, conservative predictions of functional connectivity.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>This work was supported by an NVIDIA hardware grant (Titan X Pascal).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><given-names>Kevin L</given-names> <surname>Briggman</surname></string-name>, <string-name><given-names>Moritz</given-names> <surname>Helmstaedter</surname></string-name>, and <string-name><given-names>Winfried</given-names> <surname>Denk</surname></string-name>. <article-title>Wiring specificity in the direction-selectivity circuit of the retina</article-title>. <source>Nature</source>, <volume>471</volume>(<issue>7337</issue>):<fpage>183</fpage>&#x2013;<lpage>8</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>Chalfie</surname></string-name>, <string-name><given-names>J E</given-names> <surname>Sulston</surname></string-name>, <string-name><given-names>J G</given-names> <surname>White</surname></string-name>, <string-name><given-names>E</given-names> <surname>Southgate</surname></string-name>, <string-name><given-names>J N</given-names> <surname>Thomson</surname></string-name>, and <string-name><given-names>S</given-names> <surname>Brenner</surname></string-name>. <article-title>The neural circuit for touch sensitivity in Caenorhabditis elegans</article-title>. <source>The Journal of neuroscience: the official journal of the Society for Neuroscience</source>, <volume>5</volume>(<issue>4</issue>): <fpage>956</fpage>&#x2013;<lpage>64</lpage>, <year>1985</year>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><given-names>David</given-names> <surname>Grant</surname></string-name> <string-name><given-names>Colburn</given-names> <surname>Hildebrand</surname></string-name>, <string-name><given-names>Marcelo</given-names> <surname>Cicconet</surname></string-name>, <string-name><given-names>Russel Miguel</given-names> <surname>Torres</surname></string-name>, <string-name><given-names>Woohyuk</given-names> <surname>Choi</surname></string-name>, <string-name><given-names>Tran Minh</given-names> <surname>Quan</surname></string-name>, <string-name><given-names>Jung-min</given-names> <surname>Moon</surname></string-name>, <string-name><given-names>Arthur Willis</given-names> <surname>Wetzel</surname></string-name>, <string-name><given-names>Andrew Scott</given-names> <surname>Champion</surname></string-name>, <string-name><given-names>Brett Jesse</given-names> <surname>Graham</surname></string-name>, <string-name><given-names>Owen Randlett</given-names>, <surname>George</surname></string-name> <string-name><given-names>Scott</given-names> <surname>Plummer</surname></string-name>, <string-name><given-names>Ruben</given-names> <surname>Portugues</surname></string-name>, <string-name><given-names>Isaac Henry</given-names> <surname>Bianco</surname></string-name>, <string-name><given-names>Stephan</given-names> <surname>Saalfeld</surname></string-name>, <string-name><given-names>Alexander David</given-names> <surname>Baden</surname></string-name>, <string-name><given-names>Kunal</given-names> <surname>Lillaney</surname></string-name>, <string-name><given-names>Randal</given-names> <surname>Burns</surname></string-name>, <string-name><given-names>Joshua Tzvi</given-names> <surname>Vogelstein</surname></string-name>, <string-name><given-names>Alexander Franz</given-names> <surname>Schier</surname></string-name>, <string-name><given-names>Wei-Chung Allen</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>Won-Ki</given-names> <surname>Jeong</surname></string-name>, <string-name><given-names>Jeff William</given-names> <surname>Lichtman</surname></string-name>, and <string-name><given-names>Florian</given-names> <surname>Engert</surname></string-name>. <article-title>Whole-brain serial-section electron microscopy in larval zebrafish</article-title>. <source>Nature</source>, <volume>545</volume> (<issue>7654</issue>):<fpage>345</fpage>&#x2013;<lpage>349</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><given-names>Alex J.</given-names> <surname>Cadotte</surname></string-name>, <string-name><given-names>Thomas B.</given-names> <surname>DeMarse</surname></string-name>, <string-name><given-names>Ping</given-names> <surname>He</surname></string-name>, and <string-name><given-names>Minzhou</given-names> <surname>Ding</surname></string-name>. <article-title>Causal measures of structure and plasticity in simulated and living neural networks</article-title>. <source>PLoS ONE</source>, <volume>3</volume>(<issue>10</issue>), <year>2008</year>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><given-names>Matteo</given-names> <surname>Garofalo</surname></string-name>, <string-name><given-names>Thierry</given-names> <surname>Nieus</surname></string-name>, <string-name><given-names>Paolo</given-names> <surname>Massobrio</surname></string-name>, and <string-name><given-names>Sergio</given-names> <surname>Martinoia</surname></string-name>. <article-title>Evaluation of the performance of information theory-based methods and cross-correlation to estimate the functional connectivity in cortical networks</article-title>. <source>PLoS ONE</source>, <volume>4</volume>(<issue>8</issue>), <year>2009</year>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><given-names>Olav</given-names> <surname>Stetter</surname></string-name>, <string-name><given-names>Demian</given-names> <surname>Battaglia</surname></string-name>, <string-name><given-names>Jordi</given-names> <surname>Soriano</surname></string-name>, and <string-name><given-names>Theo</given-names> <surname>Geisel</surname></string-name>. <article-title>Model-Free Reconstruction of Excitatory Neuronal Connectivity from Calcium Imaging Signals</article-title>. <source>PLoS Computational Biology</source>, <volume>8</volume>(<issue>8</issue>), <year>2012</year>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="other"><string-name><given-names>Antonio</given-names> <surname>Sutera</surname></string-name>, <string-name><given-names>Arnaud</given-names> <surname>Joly</surname></string-name>, <string-name><given-names>Vincent</given-names> <surname>Francois-Lavet</surname></string-name>, <string-name><given-names>Zixiao Aaron</given-names> <surname>Qiu</surname></string-name>, <string-name><given-names>Gilles</given-names> <surname>Louppe</surname></string-name>, <string-name><given-names>Damien</given-names> <surname>Ernst</surname></string-name>, and <string-name><given-names>Pierre</given-names> <surname>Geurts</surname></string-name>. <article-title>Simple connectome inference from partial correlation statistics in calcium imaging</article-title>. <source>arXiv preprint arXiv</source>, <year>2014</year>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><given-names>Yuriy</given-names> <surname>Mishchenko</surname></string-name>, <string-name><given-names>Joshua T.</given-names> <surname>Vogelstein</surname></string-name>, and <string-name><given-names>Liam</given-names> <surname>Paninski</surname></string-name>. <article-title>A bayesian approach for inferring neuronal connectivity from calcium fluorescent imaging data</article-title>. <source>Annals of Applied Statistics</source>, <volume>5</volume>(<issue>2 B</issue>):<fpage>1229</fpage>&#x2013;<lpage>1261</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><given-names>Daniel</given-names> <surname>Soudry</surname></string-name>, <string-name><given-names>Suraj</given-names> <surname>Keshri</surname></string-name>, <string-name><given-names>Patrick</given-names> <surname>Stinson</surname></string-name>, <string-name><given-names>Min Hwan</given-names> <surname>Oh</surname></string-name>, <string-name><given-names>Garud</given-names> <surname>Iyengar</surname></string-name>, and <string-name><given-names>Liam</given-names> <surname>Paninski</surname></string-name>. <article-title>Efficient &#x201C;Shotgun&#x201D; Inference of Neural Connectivity from Highly Sub-sampled Activity Data</article-title>. <source>PLoS Computational Biology</source>, <volume>11</volume>(<issue>10</issue>): <fpage>1</fpage>&#x2013;<lpage>28</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><given-names>Tsai-Wen</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Trevor J</given-names> <surname>Wardill</surname></string-name>, <string-name><given-names>Yi</given-names> <surname>Sun</surname></string-name>, <string-name><given-names>Stefan R</given-names> <surname>Pulver</surname></string-name>, <string-name><given-names>Sabine L</given-names> <surname>Renninger</surname></string-name>, <string-name><given-names>Amy</given-names> <surname>Baohan</surname></string-name>, <string-name><given-names>Eric R</given-names> <surname>Schreiter</surname></string-name>, <string-name><given-names>Rex a</given-names> <surname>Kerr</surname></string-name>, <string-name><given-names>Michael B</given-names> <surname>Orger</surname></string-name>, <string-name><given-names>Vivek</given-names> <surname>Jayaraman</surname></string-name>, <string-name><given-names>Loren L</given-names> <surname>Looger</surname></string-name>, <string-name><given-names>Karel</given-names> <surname>Svoboda</surname></string-name>, and <string-name><given-names>Douglas S</given-names> <surname>Kim</surname></string-name>. <article-title>Ultrasensitive fluorescent proteins for imaging neuronal activity</article-title>. <source>Nature</source>, <volume>499</volume>(<issue>7458</issue>):<fpage>295</fpage>&#x2013;<lpage>300</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><given-names>Misha B</given-names> <surname>Ahrens</surname></string-name>, <string-name><given-names>Michael B</given-names> <surname>Orger</surname></string-name>, <string-name><given-names>Drew N</given-names> <surname>Robson</surname></string-name>, <string-name><given-names>Jennifer M</given-names> <surname>Li</surname></string-name>, and <string-name><given-names>Philipp J</given-names> <surname>Keller</surname></string-name>. <article-title>Whole-brain functional imaging at cellular resolution using light-sheet microscopy</article-title>. <source>Nature Methods</source>, <volume>10</volume>(<issue>5</issue>), <year>2013</year>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="other"><string-name><given-names>Timothy W.</given-names> <surname>Dunn</surname></string-name>, <string-name><given-names>Yu</given-names> <surname>Mu</surname></string-name>, <string-name><given-names>Sujatha</given-names> <surname>Narayan</surname></string-name>, <string-name><given-names>Owen</given-names> <surname>Randlett</surname></string-name>, <string-name><given-names>Eva A.</given-names> <surname>Naumann</surname></string-name>, <string-name><given-names>Chao-Tsung</given-names> <surname>Yang</surname></string-name>, <string-name><given-names>Alexander F.</given-names> <surname>Schier</surname></string-name>, <string-name><given-names>Jeremy</given-names> <surname>Freeman</surname></string-name>, <string-name><given-names>Florian</given-names> <surname>Engert</surname></string-name>, and <string-name><given-names>Misha B.</given-names> <surname>Ahrens</surname></string-name>. <article-title>Brain-wide mapping of neural activity controlling zebrafish exploratory locomotion</article-title>. <source>eLife</source>, <year>2016</year>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><given-names>Raju</given-names> <surname>Tomer</surname></string-name>, <string-name><given-names>Matthew</given-names> <surname>Lovett-Barron</surname></string-name>, <string-name><given-names>Isaac</given-names> <surname>Kauvar</surname></string-name>, <string-name><given-names>Aaron</given-names> <surname>Andalman</surname></string-name>, <string-name><given-names>Vanessa M.</given-names> <surname>Burns</surname></string-name>, <string-name><given-names>Sethuraman</given-names> <surname>Sankaran</surname></string-name>, <string-name><given-names>Logan</given-names> <surname>Grosenick</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Broxton</surname></string-name>, <string-name><given-names>Samuel</given-names> <surname>Yang</surname></string-name>, and <string-name><given-names>Karl</given-names> <surname>Deisseroth</surname></string-name>. <article-title>SPED Light Sheet Microscopy: Fast Mapping of Biological System Structure and Function</article-title>. <source>Cell</source>, <volume>163</volume>(<issue>7</issue>):<fpage>1796</fpage>&#x2013;<lpage>1806</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="confproc"><string-name><given-names>JG</given-names> <surname>Orlandi</surname></string-name>, <string-name><given-names>Bisakha</given-names> <surname>Ray</surname></string-name>, <string-name><given-names>D</given-names> <surname>Battaglia</surname></string-name>, and <string-name><given-names>Isabelle</given-names> <surname>Guyon</surname></string-name>. <article-title>First Connectomics Challenge: From Imaging to Connectivity</article-title>. <conf-name>JMLR: Workshop and Conference Proceedings</conf-name>, <volume>1</volume>:<fpage>1</fpage>&#x2013;<lpage>17</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><given-names>Alex</given-names> <surname>Krizhevsky</surname></string-name>, <string-name><given-names>Ilya</given-names> <surname>Sutskever</surname></string-name>, and <string-name><given-names>Geoffrey E</given-names> <surname>Hinton</surname></string-name>. <article-title>Image Net Classification with Deep Convolutional Neural Networks</article-title>. <source>Advances In Neural Information Processing Systems</source>, pages <fpage>1</fpage>&#x2013;<lpage>9</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="confproc"><string-name><given-names>Lukasz</given-names> <surname>Romaszko</surname></string-name>. <article-title>Signal Correlation Prediction Using Convolutional Neural Networks</article-title>. <conf-name>JMLR: Workshop and, Conference Proceedings</conf-name>, <volume>46</volume>:<fpage>45</fpage>&#x2013;<lpage>56</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><given-names>Nitish</given-names> <surname>Srivastava</surname></string-name>, <string-name><given-names>Geoffrey</given-names> <surname>Hinton</surname></string-name>, <string-name><given-names>Alex</given-names> <surname>Krizhevsky</surname></string-name>, <string-name><given-names>Ilya</given-names> <surname>Sutskever</surname></string-name>, and <string-name><given-names>Ruslan</given-names> <surname>Salakhutdinov</surname></string-name>. <article-title>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</article-title>. <source>Journal of Machine Learning Research</source>, <volume>15</volume>:<fpage>1929</fpage>&#x2013;<lpage>1958</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="other"><string-name><given-names>Sergey</given-names> <surname>Ioffe</surname></string-name> and <string-name><given-names>Christian</given-names> <surname>Szegedy</surname></string-name>. <article-title>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</article-title>. <source>arXiv preprint arXiv</source>, pages <fpage>1</fpage>&#x2013;<lpage>11</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><given-names>Kaiming</given-names> <surname>He</surname></string-name>, <string-name><given-names>Xiangyu</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Shaoqing</given-names> <surname>Ren</surname></string-name>, and <string-name><given-names>Jian</given-names> <surname>Sun</surname></string-name>. <article-title>Deep Residual Learning for Image Recognition</article-title>. <source>arXiv preprint arXiv</source>, <volume>7</volume>(<issue>3</issue>):<fpage>171</fpage>&#x2013;<lpage>180</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="confproc"><string-name><given-names>Kaiming</given-names> <surname>He</surname></string-name>, <string-name><given-names>Xiangyu</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Shaoqing</given-names> <surname>Ren</surname></string-name>, and <string-name><given-names>Jian</given-names> <surname>Sun</surname></string-name>. <article-title>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</article-title>. <conf-name>Proceedings of the IEEE International Conference on Computer Vision</conf-name>, pages <fpage>1026</fpage>&#x2013;<lpage>1034</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="other"><string-name><given-names>Diederik P.</given-names> <surname>Kingma</surname></string-name> and <string-name><given-names>Jimmy Ba.</given-names> <surname>Adam</surname></string-name>: <article-title>A Method for Stochastic Optimization</article-title>. <source>arXiv preprint arXiv</source>, pages <fpage>1</fpage>&#x2013;<lpage>15</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><given-names>Michael</given-names> <surname>Stobb</surname></string-name>, <string-name><given-names>Joshua M.</given-names> <surname>Peterson</surname></string-name>, <string-name><given-names>Borbala</given-names> <surname>Mazzag</surname></string-name>, and <string-name><given-names>Ethan</given-names> <surname>Gahtan</surname></string-name>. <article-title>Graph theoretical model of a sensorimotor connectome in zebrafish</article-title>. <source>PLoS ONE</source>, <volume>7</volume>(<issue>5</issue>), <year>2012</year>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="other"><string-name><given-names>Max</given-names> <surname>Jaderberg</surname></string-name>, <string-name><given-names>Karen</given-names> <surname>Simonyan</surname></string-name>, <string-name><given-names>Andrea</given-names> <surname>Vedaldi</surname></string-name>, and <string-name><given-names>Andrew</given-names> <surname>Zisserman</surname></string-name>. <article-title>Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition</article-title>. <source>arXiv preprint arXiv</source>, pages <fpage>1</fpage>&#x2013;<lpage>10</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="other"><string-name><given-names>Tuan Anh</given-names> <surname>Le</surname></string-name>, <string-name><given-names>Atilim Gunes</given-names> <surname>Baydin</surname></string-name>, <string-name><given-names>Robert</given-names> <surname>Zinkov</surname></string-name>, and <string-name><given-names>Frank</given-names> <surname>Wood</surname></string-name>. <article-title>Using Synthetic Data to Train Neural Networks is Model-Based Reasoning</article-title>. <source>arXiv preprint arXiv</source>, <year>2017</year>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><given-names>Adam M</given-names> <surname>Packer</surname></string-name>, <string-name><given-names>Lloyd E</given-names> <surname>Russell</surname></string-name>, <string-name><given-names>Henry W P</given-names> <surname>Dalgleish</surname></string-name>, and <string-name><given-names>Michael</given-names> <surname>H&#x00E4;usser</surname></string-name>. <article-title>Simultaneous all-optical manipulation and recording of neural circuit activity with cellular resolution in vivo</article-title>. <source>Nature methods</source>, <volume>12</volume>(<issue>2</issue>):<fpage>140</fpage>&#x2013;<lpage>6</lpage>, <year>2015</year>.</mixed-citation></ref>
</ref-list>
</back>
</article>