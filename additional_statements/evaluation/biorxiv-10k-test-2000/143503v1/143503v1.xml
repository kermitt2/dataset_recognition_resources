<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/143503</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Confirmatory Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Bioinformatics</subject>
</subj-group>
</article-categories>
<title-group><article-title>Experimenting with reproducibility in bioinformatics</article-title></title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1583-3297</contrib-id>
<name><surname>Kim</surname> <given-names>Yang-Min</given-names></name><xref ref-type="aff" rid="a1">1</xref><xref ref-type="aff" rid="a2">2</xref><xref ref-type="aff" rid="a3">3</xref><xref ref-type="aff" rid="a4">4</xref><xref ref-type="corresp" rid="cor1">&#x002A;</xref></contrib>
<contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9794-749X</contrib-id>
<name><surname>Poline</surname> <given-names>Jean-Baptiste</given-names></name><xref ref-type="aff" rid="a5">5</xref></contrib>
<contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2253-1844</contrib-id>
<name><surname>Dumas</surname> <given-names>Guillaume</given-names></name><xref ref-type="aff" rid="a1">1</xref><xref ref-type="aff" rid="a2">2</xref><xref ref-type="aff" rid="a3">3</xref><xref ref-type="aff" rid="a4">4</xref></contrib>
<aff id="a1"><label>1</label><institution>Institut Pasteur, Human Genetics and Cognitive Functions Unit</institution>, Paris, <country>France</country>,</aff>
<aff id="a2"><label>2</label><institution>CNRS UMR 3571 Genes, Synapses and Cognition, Institut Pasteur</institution>, Paris, <country>France</country>,</aff>
<aff id="a3"><label>3</label><institution>University Paris Diderot, Sorbonne Paris Cit&#x00E9;</institution>, Paris, <country>France</country>,</aff>
<aff id="a4"><label>4</label><institution>Centre de Bioinformatique, Biostatistique et Biologie Int&#x00E9;grative (C3BI, USR 3756 Institut Pasteur and CNRS)</institution>, Paris, <country>France</country>,</aff>
<aff id="a5"><label>5</label><institution>Henry H. Wheeler Jr. Brain Imaging Center, Helen Wills Neuroscience Institute, University of California</institution>, Berkeley, California, <country>USA</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label>To whom correspondence should be addressed.</corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2017</year>
</pub-date>
<elocation-id>143503</elocation-id>
<history>
<date date-type="received">
<day>19</day>
<month>6</month>
<year>2017</year>
</date>
<date date-type="rev-recd">
<day>19</day>
<month>6</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>20</day>
<month>6</month>
<year>2017</year>
</date>
</history><permissions><copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license></permissions>
<self-uri xlink:href="143503.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract><title>Abstract</title>
<p>Reproducibility or replication has been shown to be limited in many scientific fields. This question is a fundamental tenet of the scientific activity, but the related issues of reusability of scientific data are poorly documented. Here, we present a case study of our attempt to reproduce a bioinformatics method and illustrate the challenges to use a published method for which code and data were available. From this example, we address the difficulties that pave the way towards reproducibility and propose some recommendations to the research community to improve the reusability of the data.</p>
<p content-type="availability"><bold>Availability and implementation:</bold> last version of <italic>StratiPy</italic> (Python) is available at GitHub: <ext-link ext-link-type="uri" xlink:href="http://https://github.com/GHFC/stratipy">https://github.com/GHFC/stratipy</ext-link></p>
<p><bold>Contact:</bold> <ext-link ext-link-type="uri" xlink:href="http://yang-min.kim@pasteur.fr">yang-min.kim@pasteur.fr</ext-link></p>
</abstract>
<counts>
<page-count count="6"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1"><label>1</label><title>Introduction</title>
<p>The collective endeavour of science depends on researchers being able to replicate the work of others. In a recent interview with 1,576 researchers, 70&#x0025; of them admitted having difficulty in reproducing experiments proposed by other scientists (<xref ref-type="bibr" rid="c1">Baker, 2016</xref>). For 50&#x0025;, this reproducibility issue even concerns with their own experiments. Despite the growing attention on the replication crisis in science, this controversial subject is far from being new: already in the 17th century, scientists criticized the air pump invented by physicist Robert Boyle because it was too complicated and expensive to build (<xref ref-type="bibr" rid="c9">Shapin and Schaffer, 2011</xref>).</p>
<p>While <bold>reproducibility</bold> &#x2013; here defined as being able to obtain the results using the same software and same data &#x2013; or <bold>replicability</bold> &#x2013; here defined as obtaining the same results with different data and software (<xref ref-type="bibr" rid="c8">Peng, 2011</xref>) &#x2013; are key to the scientific progress, it seems that researchers are in general not considering these as priorities. Indeed, it takes great efforts and competence to overcome all the obstacles on the path to replication. The process is costly in resources, both in time and funding. In computational science, there are also many technical barriers ranging from unavailable data to hardware infrastructure. Even when authors provide data and code, the outcome can vary either marginally or fundamentally (<xref ref-type="bibr" rid="c5">Herndon <italic>et al.</italic>, 2014</xref>). Tackling irreproducibility in bioinformatics thus requires considerable effort beyond code and data availability (<xref ref-type="fig" rid="fig1">Fig 1</xref>). Such effort is nevertheless necessary to increase the robustness of the literature and efficiency of the scientific research process. Indeed, behind reproducibility hides re-usability.
<fig id="fig1" position="float" orientation="portrait" fig-type="figure"><label>Fig. 1.</label><caption><title>Hidden reproducibility issues like underwater iceberg.</title><p>Scientific journals readers have the impression that they can almost see the full work of method. But in reality, articles do not take into account adjustment and configuration for significant replication in most cases. Therefore, there is a significant gap between apparent executable work (i.e. above water portion of iceberg) and necessary effort in practice (i.e. full iceberg).</p></caption><graphic xlink:href="143503_fig1.tif"/></fig></p>
<p>In this case study, we focus on reproducing a promising bioinformatics method (<xref ref-type="bibr" rid="c6">Hofree <italic>et al.</italic>, 2013</xref>) and identify and document different issues related to the reproducibility process. First, we tried to re-run the analysis with the code and data provided by the authors. Second, we reimplemented the method in Python to avoid dependency on a MATLAB licence and ease the execution of the code on HPCC (High-Performance Computing Cluster). Third, we assessed reusability of our reimplementation and the quality of our documentation. Then, we experimented with our own software and tested how easy it would be to start from our implementation to reproduce the results, hence attempting to estimate the reproducibility of reproducibility. Finally, in a second part, we propose solutions from this case study and other observations to improve reproducibility and research efficiency at the individual and collective level.</p>
</sec>
<sec id="s2"><label>2</label><title>Reproducibility in bioinformatics: a case study</title>
<sec id="s2a"><label>2.1</label><title>From MATLAB to MATLAB:OS and Environment</title>
<p>Our team studies Autism Spectrum Disorders (ASD), a group of neurodevelopmental disorders well known for its heterogeneity. One of the current challenges of our research is to uncover homogeneous subgroups of patients (i.e. stratification) with more precise clinical outcomes, improving their prognosis and treatment (<xref ref-type="bibr" rid="c2">Bourgeron, 2015</xref>; <xref ref-type="bibr" rid="c7">Loth <italic>et al.</italic>, 2016</xref>). An interesting stratification method was recently proposed in the field of cancer research (<xref ref-type="bibr" rid="c6">Hofree <italic>et al.</italic>, 2013</xref>), where the authors proposed to combine genetic profiles of patients tumors with protein-protein interaction networks to uncover meaningful homogeneous subgroups, a method called Network Based Stratification (NBS).</p>
<p>Before using this NBS method on our data, we studied the method by reproducing results from the original study. We are very grateful to the main authors who kindly provided online all the related data and code, and gave us invaluable input upon request. The authors of this study thus should not be blamed for the difficulty that we experienced in attempting to reproduce and replicate their study, as they did more to help replicate their results than is generally done. Despite their help we experienced a number of difficulties that we document here, hoping that this report will help future researchers to improve the reproducibility of results and reusability of research products.</p>
<p>The first step of our project was to execute the original method code with the given data. The programming code was written in MATLAB, an interpreted language originally developed for linear algebra computations which is easier and faster to write as well as more readable than compiled language such as C, making our reproducibility attempt easier. To improve execution speed, the original authors used a library for MATLAB using executable compiled code MEX file) callable from MATLAB: MTIMESX (<xref ref-type="bibr" rid="c11">Tursa, 2009</xref>), a library with compiled code allowing acceleration of large matrix multiplication. MEX files however are specific to the architecture and have to be recompiled for each Operating System (OS). The original MEX file was initially developed for Linux. Since our lab was using Mac OS X Sierra, the compilation of this MEX file into a mac64 binary required a new version of MTIMESX. It was also necessary to install and to configure properly OpenMP (<ext-link ext-link-type="uri" xlink:href="http://www.openmp.org/specifications/">http://www.openmp.org/specifications/</ext-link>), a development library for parallel computing. After this, the original MATLAB code was successfully run in our environment.</p>
<p>These issues are classic, but may not be overcome by researchers with little experience in compilation or installation issues. For these reasons alone, many individuals may turn down the opportunity of reusing code.</p>
<p>The next part will focus on code re-implementation, a procedure, which can help understanding the method, but can be even more costly.</p>
</sec>
<sec id="s2b"><label>2.2</label><title>From MATLAB to Python: Language and Organization</title>
<p>To fully master the method, adapt it to our data, and ease its re-use, we developed a complete open source toolkit of genomic stratification in Python. Python is also an interpreted programming language, but contrary to MATLAB is free of use and has a GPL-compatible license (<underline><ext-link ext-link-type="uri" xlink:href="http://https://docs.python.org/3/license.html">https://docs.python.org/3/license.html</ext-link></underline>). This is particularly interesting for both replicability and scalability. Re-coding in another language in a different environment will lead to be some unavoidable problems such as random initialization, and variation in low level libraries (e.g. glibc): it is likely that the outcomes will vary even if the same algorithm is implemented. In addition, we rely on Python packages to perform visualization or linear algebra computations (e.g. Matplotlib, SciPy, NumPy), and results may depend on these packages versions. Python is currently in a transitional period between two major versions 2 and 3. We chose to write the code in Python 3, which is the current recommendation.</p>
<sec id="s2b1"><label>2.2.1</label><title>Metadata and File formats</title>
<p>Even if the original code could be run, we had to handle several file formats to check and understand the structure of the original data. For instance the data on patients with cancer data was provided by The Cancer Genome (TCGA, <underline><ext-link ext-link-type="uri" xlink:href="http://https://cancergenome.nih.gov">https://cancergenome.nih.gov</ext-link>)</underline> and made available in a MATLAB <italic>.mat</italic> file format. Thanks to SciPy, Python can load all versions through v7.2 MATLAB files. To read v7.3 <italic>.mat</italic> files, we however needed an HDF5 Python library. Moreover, the original authors had denoted download dates of patients&#x2019; data of TCGA, thereby clarifying source of data. But in the absence of structural metadata, it was not always obvious how to interpret patients&#x2019; dataset variables (e.g. patient ID, gene ID, phenotype).</p>
</sec>
<sec id="s2b2"><label>2.2.2</label><title>Codes and parameters</title>
<p>Once the environment and file format issues were resolved, the code was finally executable with genetic data. Unfortunately, several attempts produced error messages. Alternatively, &#x201C;unexpected&#x201D; results were obtained (<xref ref-type="fig" rid="fig2">Fig 2</xref>): e.g. during the application of hierarchical clustering, we used the clustering tools of SciPy (<xref ref-type="bibr" rid="c3">Eads, 2007</xref>) (<underline><ext-link ext-link-type="uri" xlink:href="http://https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html">https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html</ext-link></underline>). Both SciPy and MATLAB (MathWorks, <underline><ext-link ext-link-type="uri" xlink:href="http://www.mathworks.com">http://www.mathworks.com</ext-link></underline>) functions offer seven linkage methods, however, SciPy&#x2019;s default option (single method) differs from MATLAB&#x2019;s default option (UPGMA method), which was used in the original study. Another key example is the value of one of the most important parameters of the method, the graph regulator factor, which was not clarified in the original paper. We believed that this factor had a constant value of 1.0 until we found in the code that during iterations, its value was changing and converged to a high optimal value (&#x007E;1800). Therefore, we obtained different results from the original NBS at the beginning (<xref ref-type="fig" rid="fig3">Fig 3</xref>). We observed heterogeneous subgroups instead of obtaining homogeneous clusters. No or little explanation on the parameter choices can explain variability in the results as we explored the possible parameters. Moreover, while attempting the original code to understand the causes of the errors, we realized that some part of the code were not run anymore (e.g. discarded work, remaining traces of debugging) which made the attempt to understand the implementation harder.
<fig id="fig2" position="float" orientation="portrait" fig-type="figure"><label>Fig. 2.</label><caption><title>Analogy between irreproducibility and road transport.</title> <p>The aim is to achieve same output (i.e. to reach the same location) using published methods (i.e. engine). Despite the same input data (i.e. gasoline), we obtained different results due to different programming languages &#x2014;e.g. MATLAB and Python&#x2014; (i.e. different roadways) and environments (i.e. different vehicles).</p></caption><graphic xlink:href="143503_fig2.tif"/></fig>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure"><label>Fig. 3.</label><caption><title>Normalized confusion matrices between original and replicated results.</title> <p>Before (a) and after (b) applying appropriate value of graph regularization factor on NBS method. Each row or column corresponds to a subgroup of patients (here three subgroups). The diagonal elements show the frequency of correct classifications for each subgroup: a high value indicates a correct prediction.</p></caption><graphic xlink:href="143503_fig3.tif"/></fig></p>
</sec>
<sec id="s2b3"><label>2.2.3</label><title>iPython/Jupyter</title>
<p>During the re-cording process, we used an enhanced Python interpreter to debug: IPython, an interactive shell supporting both Python 2 and 3. Since the dataset is large and the execution takes a significant amount of time, we used IPython to re-run interactively some sub-sections of the script, which is one of the most helpful features. IPython can be integrated in the web interface Jupyter Notebook, offering an advanced structure for mixing code and documentation. For instance, verifying intermediate results by plotting helped us to better understand the original code. While the IPython notebook was therefore initially convenient, it does not scale well and is not well adapted to versioning. However, ability of mixing code with document text is very useful for tutorials: a user of the code can read documentation (docstring), text explanations, and see how to run the code, explore parameters and visualize results in the browser.</p>
</sec>
</sec>
<sec id="s2c"><label>2.3</label><title>From Python to Python: Replication of Replication</title>
<p>Besides IPython, we used versioning tools like the git code version control system (VCS) to document the development of our Python code. Git is arguably one of the most powerful VCS, allowing easy development of branches and helping us to work together as a distributed team (Paris, Berkeley) on the same project. This project, <italic>StratiPy</italic>, is hosted on GitHub, a web-based Git repository hosting service <underline>(<ext-link ext-link-type="uri" xlink:href="http://https://github.com/GHFC/stratipy">https://github.com/GHFC/stratipy</ext-link>).</underline> While the original code was not available on GitHub, the main authors shared their code on a website. This should be sufficient for our purpose, but makes it less easy to collaborate on code. While working on our GitHub repository, several researchers from all over the world contacted us about our reproducibility experiment. Not only GitHub supports a better organization of projects, it also facilitates the collaboration of open-source software projects, thanks to several social network functions. We tried to comply with open source coding standards and to learn how to efficiently use Git and GitHub. Both required considerable efforts on the short-term but brought clear benefits on the long-term, especially regarding collaboration and debugging.</p>
<p>We then attempted to re-run and reproduce the results we obtained on another platform. While the Python code was developed under Mac OS X Sierra (10.12) we used an Ubuntu 16.04.1 (Xenial) computer to test the Python implementation. Some additional issues emerged. First, our initial documentation was not complete enough to know which packages were required and how to launch the code. Second, the code was very slow to the extent that it was impractical to run it on a laptop because the Numpy package had not been compiled with BLAS (Basic Linear Algebra Subprograms), low-level routines performing basic vector and matrix operations. Last, there was (initially) no easy way to check whether the results obtained on a different architecture were the expected ones. We added documentation and tests on the results files md5sum to solve this. To summarize, although the re-use and reproducibility of the results of the developed package were improved, these were far from being optimal.</p>
</sec>
</sec>
<sec id="s3"><label>3</label><title>Potential solutions: from local to global</title>
<sec id="s3a"><label>3.1</label><title>Act locally: simple practices and available tools</title>
<p>Given the observed difficulties, we draw some conclusions on this reproducibility case study experiment and suggest some practices and tools.</p>
<sec id="s3a1"><label>3.1.1</label><title>Environment</title>
<p>Container technologies such as Docker and Vagrant are becoming a standard solution to installation issues. These rely however on competencies that we think few biologists possess today. Also, while the container will encapsulate everything needed for the software execution, it is hard to develop in a container, limiting the reusability of the code.</p>
</sec>
<sec id="s3a2"><label>3.1.2</label><title>Metadata</title>
<p>Standard metadata are vital for an efficient documentation of both data and software. In our example, we still lack the standard lexicon to document the data as well as documenting the software: e.g. using HDF5 file instead of <italic>.mat</italic> file is more suitable to store patient&#x2019;s&#x2019; data. We however aim to follow the recommendations by <xref ref-type="bibr" rid="c10">Stodden <italic>et al.</italic> (2016)</xref>: <italic>&#x201C;Software metadata should include, at a minimum, the title, authors, version, language, license, Uniform Resource Identifier/DOI, software description (including purpose, inputs, outputs, dependencies), and execution requirements.&#x201D;</italic> The more comprehensive is the metadata description, the more likely the re-use will be both efficient and appropriate.</p>
</sec>
<sec id="s3a3"><label>3.1.3</label><title>Write readable code</title>
<p>Anyone who has spent time to understand someone else&#x2019;s code would advise some simple basic rules to help make the code readable and understandable.</p>
<p>First, the structure of the program should be clear and easily accessible. Second, good concise code documentation and naming convention will help readability. Third, the code should not contain left-overs of previously tested solutions. When a solution takes a long time to compute, an option to store it locally can be proposed. Nevertheless, the code to compute, this variable should be given in any case (e.g. inverse of large matrix). Using standard coding and documentation conventions (e.g. PEP 8 and PEP 257 in Python, <underline><ext-link ext-link-type="uri" xlink:href="http://https://www.python.org/dev/peps">https://www.python.org/dev/peps</ext-link>)</underline> with detailed comments and references of papers makes the code more accessible. When an algorithm from another paper is used, any modification should be explained and discussed in the paper as well as in the code. All these remarks are not necessarily obvious especially if the developer is working on her/his own, and to some extent &#x201C;writes for her/himself&#x201D;. We advocate for researchers to write code &#x201C;for their colleagues&#x201D;, hence, the opinion and notice of co-working or partner laboratories should be very helpful. Furthermore, the collaboration between researchers working on different environments can more easily isolate reproducibility problems. In the future, journals may consider review of code as part of the standard review process.</p>
</sec>
<sec id="s3a4"><label>3.1.4</label><title>Test the code</title>
<p>To check if the code is yielding a correct answer, software developers associate test suites (unit tests or integration tests) with their software. While we developed only a few tests in this project, we realize that this has a number of advantages, such as checking if the software installation seems correct, check if updates in the operating system impact the results, etc. This does not in general validate the method, but at least provides a basic check. In our case, we propose to check for the integrity of the data and for the results of some key processing.</p>
</sec>
</sec>
<sec id="s3b"><label>3.2</label><title>Think globally: from education to community standards</title>
<sec id="s3b1"><label>3.2.1</label><title>Training the new generation of scientists to digital tools and practices</title>
<p>Unlike theoretical and academic courses and projects, software testing systems are well developed in industry. For a student, discovering and learning this core system of reproducibility, possibly during an internship in cooperation with industry, is a great opportunity for her/his future. Furthermore, as Internet applications in science are growing, networks of scientists and developers are forming and provide learning opportunities on the development practices. For instance, software developers have recently adopted &#x201C;agile&#x201D; practices and fast prototyping, test based development, etc. Some of these ideas and practices can &#x2014;and should&#x2014; be adapted to scientific software development.</p>
<p>The training in coding is still too limited for biologists. Often, it is self-training, from searching answers on Stack Overflow or equivalent. Despite efforts by organizations such as software <underline>(<ext-link ext-link-type="uri" xlink:href="http://https://softwarecarpentry.org">https://softwarecarpentry.org</ext-link>)</underline> or data carpentry (<ext-link ext-link-type="uri" xlink:href="http://www.datacarpentry.org">http://www.datacarpentry.org</ext-link>) and the growing demand for &#x2018;data scientists&#x2019; in life science, university training on coding practices is not enough generalized. The difficulty to access and understand code may lead to applying code blindly without checking the validity of the results: often, scientists may prefer to believe that the results are correct because of the time that would be needed to check the validity of the results. Mastering a package such that results are truly understood can take a long time, as it was the case in our experiment.</p>
</sec>
<sec id="s3b2"><label>3.2.2</label><title>Standard consensus dataset and workflow system</title>
<p>We propose here that bioinformatics methods publications are systematically accompanied with a test dataset, code source and some basic tests. As the method is tested on new datasets, the number of tests of the method would increase in number and cover a wider range of applications. We give a first example with our NBS re-implementation. We develop below how this could generalize and what would be the benefit for the scientific community. In a sense, we propose to use the software development test framework idea but apply it to the scientific context.</p>
<p>A schematic overview of workflow system is shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>. The core of this system would be a standard consensus dataset used to validate methods. Data could be classified in general categories such as binary, text, image (A, B, C in <xref ref-type="fig" rid="fig4">Fig 4b</xref>), and with sub-categories to introduce criteria such as size, quantitative/qualitative, discrete/continuous using a tagging system (e.g. A-2, B-1, C-5 in <xref ref-type="fig" rid="fig4">Fig 4b</xref>). Dataset could be issued from simulations or from acquisition, and would validate a method on a particular component. This workflow system will help scientists that cannot release their data because of privacy issues (although these can often be overcome) but also give access to data and tests to a wide community.
<fig id="fig4" position="float" orientation="portrait" fig-type="figure"><label>Figure 4:</label><caption><title>Working principles of Workflow system with private data.</title> <p>Figure 4a shows a classical workflow: (a.1) Methodologists (M) take private data; (a.2) M publish their method and corresponding outputs/results; (a.3) Scientists (requesters = R) (e.g. biologists, doctors) having their own data ask developers (D) (e.g. bioinformaticians) for handling them; (a.4) D find a relevant paper and will be lost in the labyrinth of reproducibility. Figu re 4b shows workflow with standard consensus dataset: (b.1) If Methodol o- gists (M) work with their own data, they must identify corresponding standard data tag(s) (e.g. A-2). They can use only standard data at the beginning of the project. If there is no appropriate data, they have to suggest a new data standard; (b.2) M choose a method category (e.g. &#x201C;Classification&#x201D;); (b.3) Reproducibility profile with several standard data is progressively built with method upgrade. Color corresponds to method category depending score and bar length corresponds to progression of replication test. Programming code with guide should be accessible although M do not publish; (b.4) Publication of the first version method by M; (b.5) Developers (D) can test proposed method with other data standards and thus participate to enhancement of the reproducibility profile. D then obtain automatically credit for this activity of validation; (b.6) Thanks to the collective work on testing, the method could be optimized and M can upgrade their initial paper (versioning); (b.7) Requesters (R) can now input corresponding tags (e.g. C-3 and E-6) of their data and search available methods in the category of their problem (e.g. &#x201C;Regression&#x201D;); (b.8) Following the request, a list of suggested methods and people involved (M, D) is obtained.</p></caption><graphic xlink:href="143503_fig4.tif"/></fig></p>
<p>Roughly, we divide those who interact with scientific software or analysis code in three categories. First, the methodologists who propose a method and need to verify its validity and usefulness with public and/or their own - often private &#x2013; data (&#x201C;M&#x201D; in <xref ref-type="fig" rid="fig4">Fig 4</xref>). Second, the developers or engineers who need to test and evaluate the proposed methods with other data (&#x201C;D&#x201D; in <xref ref-type="fig" rid="fig4">Fig 4</xref>). Third, expert and non-expert users who need to have a good understanding of the results obtained on various types of data (&#x201C;R&#x201D; as requester in <xref ref-type="fig" rid="fig4">Fig 4</xref>).</p>
<p>Each method belongs to a general category of methods (e.g. classification, regression) and could have a reproducibility profile, which will progressively be built by methodologists and developers (<xref ref-type="fig" rid="fig4">Fig 4b.3</xref>, <xref ref-type="fig" rid="fig4">4b.5</xref>). The information of which method does or does not work with a standard/public data is a crucial information for the scientist. Developers who test and approve reproducibility on original or new data could be credited and recognized by the scientific and developer communities (i.e. Stack Overflow, GitHub). When scientists search about the appropriate methods according to their type of data (<xref ref-type="fig" rid="fig4">Fig 4b.7</xref>), they would also be interested in obtaining information about the people who invested heavily in test and optimization of suggested methods (<xref ref-type="fig" rid="fig4">Fig 4b.8</xref>). This workflow system could facilitate the gathering of diverse users of the science community.</p>
</sec>
</sec>
</sec>
<sec id="s4"><label>4</label><title>Conclusion and perspective</title>
<p>Across the scientific fields, the reproducibility issue is seen as a growing concern. Before reusing a published method, we attempted to reproduce the initial results and recoded the method to have a deep understanding of it. The investment in time to verify a previously published method can be more important than the work needed to publish a new paper. Despite the willingness of the authors to share their tool and help us in our work, we have faced reproducibility problems due to compatibility between environments, programming languages and software versions, choice of parameters, etc. In addition to individual effort to write well documented and readable code, we recommend to use online repositories and tools to help other scientists in their exploration of the method: Docker for environment standardization, GitHub for code version management, and Jupyter notebooks for demonstration and tutorial. At the community level, we should enhance the cooperation between academic education and industry to foster a new generation of well-trained scientists in software development. We propose a workflow system where the community uses standard datasets to validate tools. The proposed method success on data profile will be evaluated continuously with new datasets. Eventually, data and software can be versioned and cited to give credit to the individuals who have contributed to these building blocks of Science. This workflow is not merely a reproducibility validation tool, it is an attempt to make research product more reusable by the community using an online platform, beyond the publication process. Some top-down initiatives already provide some incentives for such a process i.e. Horizon 2020 (H2020, <underline><ext-link ext-link-type="uri" xlink:href="http://https://ec.europa.eu/research/press/2016/pdf/opendata-infographic072016.pdf">https://ec.europa.eu/research/press/2016/pdf/opendata-infographic072016.pdf</ext-link>)</underline> project of the European Commission (EC) mandates open access of research data, while respecting security and liability. H2020 supports OpenAIRE <underline>(<ext-link ext-link-type="uri" xlink:href="http://https://www.openaire.eu/edocman?id=749&#x0026;task=document.viewdoc">https://www.openaire.eu/edocman?id=749&#x0026;task=document.viewdoc</ext-link>),</underline> a technical infrastructure of the open access, which allows the interconnection between projects, publications, datasets, and author information across Europe. Thanks to common guidelines, OpenAIRE interoperates with other web-based <italic>generalist</italic> scientific data repositories such as Zenodo, hosted by CERN, which allows combining data and GitHub repository using digital object identifiers (DOI). The Open Science Framework also hosts data and software for a given project (<xref ref-type="bibr" rid="c4">Foster and Deardorff, 2017</xref>). Respecting standard guidelines to transparently communicate the scientific work is a key step towards tackling irreproducibility and insures a robust scientific endeavor.</p></sec>
</body>
<back>
<ref-list><title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Baker</surname>,<given-names>M.</given-names></string-name> (<year>2016</year>) <article-title>1,500 scientists lift the lid on reproducibility</article-title>. <source>Nat. News</source>, <volume>533</volume>, <fpage>452</fpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Bourgeron</surname>,<given-names>T.</given-names></string-name> (<year>2015</year>) <article-title>From the genetic architecture to synaptic plasticity in autism spectrum disorder</article-title>. <source>Nat. Rev. Neurosci.</source>, <volume>16</volume>, <fpage>551</fpage>&#x2013;<lpage>563</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="other"><collab>Eads</collab>(<year>2007</year>) <source>Hierarchical clustering (scipy.cluster.hierarchy) &#x2014; SciPy v0.19.0 Reference Guide</source>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Foster</surname>,<given-names>E.D.</given-names></string-name> and <string-name><surname>Deardorff</surname>,<given-names>A.</given-names></string-name> (<year>2017</year>) <article-title>Open Science Framework (OSF)</article-title>. <source>J. Med. Libr. Assoc. JMLA</source>, <volume>105</volume>, <fpage>203</fpage>&#x2013;<lpage>206</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Herndon</surname>,<given-names>T.</given-names></string-name> <etal>et al.</etal> (<year>2014</year>) <article-title>Does high public debt consistently stifle economic growth? A critique of Reinhart and Rogoff</article-title>. <source>Camb. J. Econ.</source>, <volume>38</volume>, <fpage>257</fpage>&#x2013;<lpage>279</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Hofree</surname>,<given-names>M.</given-names></string-name> <etal>et al.</etal> (<year>2013</year>) <article-title>Network-based stratification of tumor mutations</article-title>. <source>Nat. Methods</source>, <volume>10</volume>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Loth</surname>,<given-names>E.</given-names></string-name> <etal>et al.</etal> (<year>2016</year>) <article-title>Identification and validation of biomarkers for autism spectrum disorders</article-title>. <source>Nat. Rev. Drug Discov.</source>, <volume>15</volume>, <fpage>70</fpage>&#x2013;<lpage>73</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Peng</surname>,<given-names>R.D.</given-names></string-name> (<year>2011</year>) <article-title>Reproducible research in computational science</article-title>. <source>Science</source>, <volume>334</volume>, <fpage>1226</fpage>&#x2013;<lpage>1227</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="book"><string-name><surname>Shapin</surname>,<given-names>S.</given-names></string-name> and <string-name><surname>Schaffer</surname>,<given-names>S.</given-names></string-name> (<year>2011</year>) <source>Leviathan and the Air-Pump: Hobbes, Boyle, and the Experimental Life (New in Paper)</source> <publisher-name>Princeton University Press</publisher-name>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Stodden</surname>,<given-names>V.</given-names></string-name> <etal>et al.</etal> (<year>2016</year>) <article-title>Enhancing reproducibility for computational methods</article-title>. <source>Science</source>, <volume>354</volume>, <fpage>1240</fpage>&#x2013;<lpage>1241</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="other"><string-name><surname>Tursa</surname></string-name> (<year>2009</year>) <source>MTIMESX - Fast Matrix Multiply with Multi-Dimensional Support - File Exchange - MATLAB Central</source>.</mixed-citation></ref>
</ref-list></back>
</article>