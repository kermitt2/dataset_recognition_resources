<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/220137</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Behavioral and neural representations of spatial directions across words, schemas, and images</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Weisberg</surname>
<given-names>Steven M.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Marchette</surname>
<given-names>Steven A.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chatterjee</surname>
<given-names>Anjan</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Neurology, University of Pennsylvania</institution>, Philadelphia, PA 19104.</aff>
<aff id="a2"><label>2</label><institution>Department of Psychology, University of Pennsylvania</institution>, Philadelphia, PA 19104.</aff>
</contrib-group>
<author-notes>
<corresp id="cor1">Correspondence concerning this article should be addressed to Steven M. Weisberg, at Center for Cognitive Neuroscience, <institution>University of Pennsylvania</institution>, Philadelphia, PA 19104 <country>USA</country> Email: <ext-link ext-link-type="uri" xlink:href="http://stweis@pennmedicine.upenn.edu">stweis@pennmedicine.upenn.edu</ext-link>
<fn><p>Conflict of Interest Declaration: The authors have no conflicts of interest to declare.</p></fn>
</corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2017</year>
</pub-date>
<elocation-id>220137</elocation-id>
<history>
<date date-type="received">
<day>15</day>
<month>11</month>
<year>2017</year>
</date>
<date date-type="rev-recd">
<day>15</day>
<month>11</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>16</day>
<month>11</month>
<year>2017</year>
</date>
</history><permissions><copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="220137.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Modern spatial navigation requires fluency with multiple representational formats, including visual scenes, signs, and words. These formats convey different information. Visual scenes are rich and specific, but contain extraneous details. Arrows, as an example of signs, are schematic representations in which the extraneous details are eliminated, but analog spatial properties are preserved. Words eliminate all spatial information and convey spatial directions in a purely abstract form. How does the human brain compute spatial directions within and across these formats&#x003F; To investigate this question, we conducted two experiments on humans: a behavioral study that was preregistered, and a neuroimaging study using multivoxel pattern analysis of fMRI data to uncover similarities and differences among representational formats. Participants in the behavioral study viewed spatial directions presented as images, schemas, or words (e.g., &#x201C;left&#x201D;), and responded to each trial, indicating whether the spatial direction was the same or different as the one viewed previously. They responded more quickly to schemas and words than images, despite the visual complexity of stimuli being matched. Participants in the fMRI study performed the same task, but responded only to occasional catch trials. Spatial directions in images were decodable in the intraparietal sulcus (IPS) bilaterally, but schemas and words were not. Spatial directions were also decodable between all three formats. These results suggest that IPS plays a role in calculating spatial directions in visual scenes, but this neural circuitry may be bypassed when the spatial directions are presented as schemas or words.</p>
<sec>
<title>Significance Statement</title>
<p>Human navigators encounter spatial directions in various formats: words (&#x201C;turn left&#x201D;), schematic signs (an arrow showing a left turn), and visual scenes (a road turning left). The brain must transform the spatial direction in these formats into a motor plan for action. Here, we investigate similarities and differences between neural representations of these formats. We find that regions of the brain that computes egocentric directions, bilateral intraparietal sulcus, also represents spatial directions in visual scenes and across the three formats. We also found that words and schemas are responded to more quickly. Combined, these results support a model of spatial direction interpretation that sometimes computes spatial directions, but other times does not in favor of efficient visual processing.</p>
</sec>
</abstract>
<counts>
<page-count count="42"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s2">
<title>Introduction</title>
<p>Humans fluently interpret spatial directions when navigating. But spatial directions are often presented in distinct representational formats &#x2013; i.e., words, signs, and scenes &#x2013; which have to be converted into a correct series of turns relative to one&#x2019;s facing direction. While spatial directions conveyed through visual scenes are well-studied, little is known about how spatial directions are processed within and across these other frequently encountered formats. How does the evolutionarily ancient neural architecture for spatial cognition convert spatial maps and scenes into schematic maps and verbal directions&#x003F;</p>
<p>Representational formats &#x2013; visual scenes, schematic signs, and words &#x2013; have distinct properties, which allow them to convey information differently. Visual scenes convey navigational information directly &#x2013; paths are visible &#x2013; but they contain irrelevant information (e.g., color, objects, context). Words, by contrast, categorize continuously varying turn angles and, by virtue of being symbolic, are related arbitrarily to the spatial directions conveyed. Schematic signs, or schemas, are exemplified by arrows in this investigation. Schemas are simplified visual representations of concepts (<xref ref-type="bibr" rid="c22">Talmy, 2000</xref>). Unlike visual scenes, schemas abstract over properties of spatial directions and omit those that are irrelevant. Unlike words, schemas maintain an iconic mapping between the spatial direction depicted and its representational format (i.e., a left-pointing arrow points to the left). Schemas seem to occupy a middle ground between images and words representing concepts (for actions, <xref ref-type="bibr" rid="c26">Watson, Cardillo, Bromberger, &#x0026; Chatterjee, 2014</xref>; for prepositions, <xref ref-type="bibr" rid="c2">Amorapanth et al., 2012</xref>; c.f. <xref ref-type="bibr" rid="c5">Gilboa &#x0026; Marlatte, 2017</xref>).</p>
<p>An intricate neural network interprets the spatial content of visual scenes. The occipital place area (OPA), retrosplenial complex (RSC), and intraparietal sulcus (IPS) are implicated in different aspects of calculating spatial directions. The OPA codes egocentric spatial directions (anchored to one&#x2019;s own body position) visible in visual scenes (<xref ref-type="bibr" rid="c3">Bonner and Epstein, 2017</xref>), whereas RSC codes allocentric spatial directions (anchored to properties of the environment) with respect to a known reference direction (i.e., a major axis of inside a building, (<xref ref-type="bibr" rid="c15">Marchette, Vass, Ryan, &#x0026; Epstein, 2015</xref>; or north, <xref ref-type="bibr" rid="c24">Vass &#x0026; Epstein, 2017</xref>). Unlike OPA and RSC, the IPS codes egocentric spatial directions, which can be either present in a scene or which were learned and then imagined. For example, <xref ref-type="bibr" rid="c21">Schindler &#x0026; Bartels (2013)</xref> had participants memorize a circular array of objects and imagine movements with the same egocentric angle, but anchored to different objects (i.e., &#x201C;face the lamp, point to the chair&#x201D; and &#x201C;face the chair, point to the vase&#x201D; would both require a 60&#x00B0; clockwise rotation). Using multivoxel pattern analysis (MVPA) of functional magnetic resonance imaging (fMRI) data, they showed that the IPS exhibited similar patterns of activation for the same spatial direction. This work used visual scenes to encode spatial directions. Does the IPS represent egocentric spatial directions from arrows (schematic depictions) and words similarly to visual scenes of egocentric spatial directions&#x003F;</p>
<p>In the current work, we investigate how representational formats affect the behavioral and neural responses to spatial directions. Our broad hypothesis is that schemas and words elide the spatial processing required by visual scenes. If true, we predict evidence supporting two hypotheses: 1) schemas and words are processed more efficiently than scenes, and 2) visual scenes, but not schemas or words are processed in brain regions known to process spatial information. To test our first hypothesis, we predict that people will most quickly identify spatial directions depicted in words (&#x201C;left&#x201D; or &#x201C;sharp right&#x201D;) and schemas (arrows), compared to scenes (Google Maps images of roads). To test the second hypothesis, in an fMRI study using multivoxel pattern analysis (MVPA), we query the neural representations of spatial directions as a function of representational format. We expected visual scenes to be processed spatially and thus spatial directions would be decoded in IPS. We were agnostic if IPS would decode schemas and words, since these formats need not be processed egocentrically. We also looked for cross-decoding in IPS between all three representational formats.</p>
</sec>
<sec id="s3">
<title>Materials and Methods</title>
<sec id="s3a">
<title>Participants</title>
<sec id="s3a1">
<title>Norming Study</title>
<p>We recruited 42 participants (23 identifying as female) from Amazon Mechanical Turk. Two participants were removed for responding below chance. Of the remaining 40 participants (21 identifying as female, 1 did not report gender), 5 participants self-reported as Asian, 1 as African-American or Black, 2 as Hispanic, 1 as Other, and 29 as White. Two participants did not report ethnicity. Participants&#x2019; average age was 34.6 years (<italic>SD</italic> &#x003D; 12.6). All but one participant reported speaking English as a first language.</p>
</sec>
<sec id="s3a2">
<title>Behavioral Study</title>
<p>We recruited 48 right-handed participants (27 identifying as female, 1 did not report gender) from a large urban university using an online recruitment tool specific to that university. 18 participants self-reported as Asian, 13 as African-American or Black, 1 as American Indian, 5 as Hispanic, 1 as Other, and 9 as Caucasian or White. One participant did not report ethnicity.Participants&#x2019; average age was 22.5 years (<italic>SD</italic> &#x003D; 3.3). Participants reported speaking English as a first language.</p>
</sec>
<sec id="s3a3">
<title>fMRI Study</title>
<p>We recruited 22 right-handed participants from a large urban university using an online recruitment tool specific to that university. We excluded data from two participants because of motion. The resulting sample consisted of 20 participants (11 identifying as female). 4 participants self-reported as Asian, 4 as African-American or Black, and 12 as Caucasian or White. Participants&#x2019; average age was 21.4 years (<italic>SD</italic> &#x003D; 2.7). Participants reported speaking English as a first language. Laterality quotient indicated participants were right-handed (<italic>Min.</italic> &#x003D; 54.17, <italic>M</italic> &#x003D; 80.63, <italic>SD</italic> &#x003D; 16.41).</p></sec></sec>
<sec id="s3b">
<title>Experimental materials</title>
<sec id="s3b1">
<title>Stimuli</title>
<p>When given an open number of categories, people freely sort spatial directions into eight categories (<xref ref-type="bibr" rid="c13">Klippel and Montello, 2007</xref>). For the present study, we used seven of those eight categories: <italic>ahead, left, right, sharp left, sharp right, slight left,</italic> and <italic>slight right</italic>. We excluded <italic>behind</italic> because this direction would require the participant to imagine starting at the top of the image, rather than the bottom.</p>
<p>Spatial directions were depicted in three formats &#x2013; words, schemas, and images. All stimuli were cropped to be 400x400 pixel squares. For each spatial direction, 24 words were created in Photoshop by modifying the size (small or large), font (Arial or Times New Roman), and color (blue, orange, pink, or purple). For each spatial direction, 24 schemas were created in Photoshop by modifying size (small or large), style (chevron or arrow), and color (blue, orange, pink, or purple). For the fMRI study, all 24 stimuli were used for each spatial direction. For the behavioral study, we psuedo-randomly chose three words and schemas to remove (retaining as close to the same number of colors, sizes, and fonts as possible across the directions), resulting in 21 exemplars per direction.</p>
<p>For each spatial direction, 28 images<sup><xref ref-type="fn" rid="fn1">1</xref></sup> were created from Google Earth. Overhead satellite views were used to identify roads that turned in that direction, limiting the presence of cars, arrows on the road, and obscured view (like shadows or trees blocking the view). These 28 images were presented to 40 independent raters on Amazon Mechanical Turk, who answered a multiple choice question, selecting the spatial direction that best corresponded to the spatial direction in the image. All raters rated all images. Across each image we compared the percentage of raters who selected the same direction we chose for each image as a measure of direction judgment reliability. We selected the top 30 raters on Mechanical Turk (eliminating the bottom nine participants who rated less than 75&#x0025; of images correctly). For the fMRI study, we selected 24 images from each direction to match, as closely as possible, the agreement across spatial directions (overall agreement &#x003D; 86.8&#x0025;, <italic>SD</italic> &#x003D; 11.6&#x0025;). The images differed on overall agreement across spatial directions, <italic>F</italic>(6, 161) &#x003D; 3.18, <italic>p</italic> &#x003D; .006, &#x03C9;<sup>2</sup> &#x003D; 0.07. Ahead images were the most agreed upon (<italic>M</italic> &#x003D; 92.6&#x0025;) and slight right images (<italic>M</italic> &#x003D; 80.3&#x0025;) were the least agreed upon. Across spatial directions, the difference in ratings was significant between the slight right images and the ahead images, <italic>t</italic>(46) &#x003D; 2.62, <italic>p</italic> &#x003D; .012, <italic>d</italic> &#x003D; 0.77, and between the slight right images and the left images <italic>t</italic>(46) &#x003D; 3.173, <italic>p</italic> &#x003D; .003, <italic>d</italic> &#x003D; 0.94. None of the other differences between pairs of spatial directions reached statistical significance. For the behavioral study, we used the most agreed upon 21 of these 24 images per spatial direction.</p>
<p>We created two versions of words and schemas. For the White background stimuli, words and schemas were displayed on a white square. For the Scrambled background stimuli, words and schemas were overlaid on phase-scrambled images. Two copies of each phase-scrambled image were used, to provide backgrounds for schemas and words. For Scrambled backgrounds, spatial directions and backgrounds were randomly paired for each participant. Sample stimuli (with phase-scrambled backgrounds) can be seen in <xref ref-type="fig" rid="fig1">Figure 1A</xref>.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><title>Stimuli and fMRI study paradigm.</title><p>Sample stimuli from the behavioral and fMRI studies (A). Stimuli with phase shifted backgrounds are shown. A segment of the experimental paradigm shown to participants in the fMRI study (B). In the fMRI study, participants saw a spatial direction in one of the formats for 1 second, followed by a fixation cross for 2 seconds. During catch trials, the fixation cross instead disappeared after .5 seconds, and a new spatial direction appeared, in any of the formats, with the word &#x201C;Same&#x201D; underneath in red letters. Participants pressed one button to indicate that the spatial direction on screen matched the one previously seen, and the other button to indicate that it did not match (buttons were counter-balanced across participants).</p></caption>
<graphic xlink:href="220137_fig1.tif"/>
</fig>
</sec>
<sec id="s3b2">
<title>Self-report and debriefing</title>
<p>All participants in the fMRI and behavioral studies completed the Verbalizer-Visualizer Questionnaire (VVQ; <xref ref-type="bibr" rid="c12">Kirby, Moore, &#x0026; Schofield, 1988</xref>) before the experimental task. Participants in the fMRI study also completed the Edinburgh handedness inventory (<xref ref-type="bibr" rid="c18">Oldfield, 1971</xref>) before the experimental task. After the experimental task, participants in the fMRI study completed a debriefing questionnaire, which asked how participants tried to remember the spatial directions and whether they felt it was difficult to switch between formats.</p>
</sec>
</sec>
<sec id="s3c">
<title>Experimental Procedure (Behavioral Study)</title>
<p>To see how efficiently words, schemas, and images are encoded and translated across formats, participants performed a rolling one-back task. First, participants viewed instructions, which described the task and included samples of the seven spatial directions in all three formats. After a brief practice session, participants viewed spatial directions one at a time, responding by pressing one key to indicate that the current spatial direction was the same as the previously seen spatial direction, and another key to indicate that the current spatial direction was different from the previously seen direction. The spatial direction stayed on screen until the participant responded. Keys (&#x2018;F&#x2019; and &#x2018;J&#x2019; on a standard keyboard) were counter-balanced across participants. Reaction time and accuracy were recorded. We generated a unique continuous carryover sequence for each participant (<xref ref-type="bibr" rid="c1">Aguirre, 2007</xref>) such that each spatial direction and format appeared before every other format and direction, including itself. This resulted in 441-trial sequences, of which approximately 1/7 were matches. Half of the participants performed the task with the White background spatial directions; the other half on the Scrambled background spatial directions. Because the first trial does not have a previous direction to compare, this trial was excluded from analysis.</p>
</sec>
<sec id="s3d">
<title>Experimental Procedure (fMRI Study)</title>
<p>To investigate the neural representations of spatial directions across and within formats, we presented spatial directions one at a time while the participant detected matches and non-matches in catch trials. Unlike the behavioral study, for the fMRI study we wanted to distinguish neural activation associated with individual spatial directions, rather than the comparison between one spatial direction and another. For that reason, participants only responded to occasional catch trials. The Scrambled spatial directions were used for the fMRI study because a computational model of early visual cortex processing (the Gist model: <xref ref-type="bibr" rid="c19">Oliva &#x0026; Torralba, 2006</xref>) could not cross-decode spatial direction across formats when scrambled backgrounds were used, but could when white backgrounds were used. Using the scrambled backgrounds in the fMRI study reduced the likelihood of decoding spatial directions across formats because early visual cortex might be sensitive to low level visual properties that could, for example, distinguish schemas and images.</p>
<p>Continuous carryover sequences (<xref ref-type="bibr" rid="c1">Aguirre, 2007</xref>) were generated with 24 conditions &#x002D;seven spatial directions in each of the three formats made up 21 conditions; two catch trials (one match and one non-match); and one null condition. The resulting sequence consisted of 601 trials (504 spatial directions trials, 48 catch trials, 48 null trials, and the first trial repeated at the end). Except for the catch trials, which could consist of any stimulus, exemplars were only presented once per participant. A schematic of the trial structure can be seen in <xref ref-type="fig" rid="fig1">Figure 1B</xref>.</p>
<p>For spatial direction trials, participants viewed spatial directions one at a time, presented on screen for 1000ms with a 2000ms inter-stimulus interval consisting of a fixation cross. Participants were instructed to attend to the spatial direction for each trial. On catch trials, a subsequent spatial direction appeared with the word &#x201C;Same&#x003F;&#x201D; underneath in large red letters. For these trials, participants pressed one key to indicate that the spatial direction was the same as the one they just saw, and another key to indicate that the spatial direction was different. Keys (the leftmost and rightmost buttons of a four-button MRI-compatible button box) were counter-balanced across participants.</p>
<p>Catch trials consisted of 1000 ms stimulus presentation, followed by 500 ms fixation, then 4500 ms of the catch stimulus. The catch stimulus was randomly chosen each time, with the constraint that it could not be the exact same stimulus. If the catch trial was a match trial, the spatial direction had to be the same. If the catch trial was a non-match trial, the spatial direction was randomly chosen from all the other spatial directions. Catch stimuli could be any format. Null trials consisted of a fixation cross, presented for double the normal trial length, 6000ms (<xref ref-type="bibr" rid="c1">Aguirre, 2007</xref>).</p>
<p>The experimental session was divided into 6 runs. The runs were 100 trials each, except for the last, which was 101 trials. The second through sixth run began with the last five trials of the previous run to re-instate the continuous carryover sequence. These overlap trials, as well as the catch trials, and null trials, were not analyzed. Because runs contained between 6-9 catch trials, which were 6000 ms, the runs varied slightly in length, but were approximately 5 min, 50 s. Because of this variation, the scanner collected functional data for 6 min, 12 s. Additional volumes collected after the stimuli for those trials were finished were discarded. Reaction time and accuracy were recorded. After each run, the participant received feedback on his performance (e..g, &#x201C;You got 6 out of 8 correct.&#x201D;).</p>
</sec>
<sec id="s3e">
<title>MRI Acquisition</title>
<p>Scanning was performed at the Hospital of the University of Pennsylvania using a 3T Siemens Trio scanner equipped with a 64-channel head coil. High&#x00AD;resolution T1-weighted images for anatomical localization were acquired using a three-dimensional magnetization-prepared rapid acquisition gradient echo pulse sequence &#x005B;repetition time (TR), 1850 ms; echo time (TE), 3.91 ms; inversion time, 1100 ms; voxel size, 0.9 &#x00D7; 0.9 &#x00D7; 1 mm; matrix size, 240 &#x00D7; 256 &#x00D7; 160&#x005D;. T2&#x002A;-weighted images sensitive to blood oxygenation level-dependent (BOLD) contrasts were acquired using a multiband gradient echo echoplanar pulse sequence (TR, 3000 ms; TE, 30 ms; flip angle, 90&#x00B0;; voxel size, 2 &#x00D7; 2 &#x00D7; 2 mm; field of view, 192; matrix size, 96 &#x00D7; 96 &#x00D7; 80; acceleration factor, 2.). Visual stimuli were displayed by rear-projecting them onto a Mylar screen at 1024 &#x00D7; 768 pixel resolution with an Epson 8100 3-LCD projector equipped with a Buhl long-throw lens. Participants viewed the stimuli through a mirror attached to the head coil.</p>
<p>Functional images were corrected for differences in slice timing using FSL slice-time correction and providing the interleaved slice time order. Images were then realigned to the first volume of the scan run, and subsequent analyses were performed within the participants&#x2019; own space. Motion correction was performed using MCFLIRT (<xref ref-type="bibr" rid="c8">Jenkinson et al., 2002</xref>), but motion outliers were also removed using the Artifact Detection Toolbox (<ext-link ext-link-type="uri" xlink:href="http://www.nitrc.org/projects/artifact_detect">http://www.nitrc.org/projects/artifact_detect</ext-link>).</p>
<p>For two participants, data from two runs were discarded. For one participant, data were excluded because the scanning computer crashed during the final run. For a second participant, data were excluded because performance on the behavioral task was below 50&#x0025; for the final run. All other runs for all other participants exceeded 63&#x0025; correct.</p>
</sec>
<sec id="s3f">
<title>Multivoxel pattern analysis</title>
<p>To test the information about spatial direction in each participant, we calculated the similarities across scan runs between the multivoxel activity patterns elicited by each spatial direction in each format. If a region contains information about spatial direction, then patterns corresponding to the same direction in different scan runs should be more similar than patterns corresponding to different directions (<xref ref-type="bibr" rid="c7">Haxby et al., 2001</xref>). Moreover, if this effect is observed for patterns elicited by stimuli of different formats (i.e., word-schema), then the spatial direction code generalizes across formats.</p>
<p>To define activity patterns, we used general linear models (GLMs), implemented in FSL (<xref ref-type="bibr" rid="c9">Jenkinson et al., 2012</xref>), to estimate the response of each voxel to each stimulus condition (three formats for each of seven spatial directions) in each scan run. Each runwise GLM included one regressor for each spatial direction in each format (21 total), regressors for motion parameters, and nuisance regressors to exclude outlier volumes discovered using the Artifact Detection Toolbox (<ext-link ext-link-type="uri" xlink:href="http://www.nitrc.org/projects/artifact_detect/">http://www.nitrc.org/projects/artifact_detect/</ext-link>). Additional nuisance regressors removed catch trials and the reinstatement trials which began runs 2-5. High-pass filters were used to remove low temporal frequencies before fitting the GLM, and the first three volumes of each run were discarded to ensure data quality. Multivoxel patterns were created by concatenating the estimated responses across all voxels within the searchlight sphere. These patterns were then averaged across the first three runs, and then across the second three runs. For the two participants for whom the final run was discarded, the last two runs were averaged together.</p>
<p>To determine similarities between activity patterns, we calculated Kendall&#x2019;s &#x03C4;<sub>A</sub> correlations (<xref ref-type="bibr" rid="c17">Nili et al., 2014</xref>) between patterns in the first half and second half scan runs. Individual patterns were normalized before this computation by subtracting the grand mean pattern (i.e., the cocktail mean) for each run (<xref ref-type="bibr" rid="c23">Vass and Epstein, 2013</xref>). We then performed representational similarity analyses by comparing the Pearson correlations between the neural similarity and theoretical models of the spatial direction similarity. This either consisted of correlating the neural representational dissimilarity matrix (RDM) with the theoretical RDM, or by subtracting the average of a subset of correlations within the neural RDM (e.g., different direction correlations) from the average of another subset (e.g., same direction correlations) to obtain a discrimination index.</p>
</sec>
<sec id="s3g">
<title>Searchlight analysis</title>
<p>To test for format decoding across the brain, we implemented a whole-brain searchlight analysis (<xref ref-type="bibr" rid="c14">Kriegeskorte et al., 2006</xref>) in which we centered a spherical ROI (radius, 5 mm) around every voxel of the brain, calculated the spatial direction correlation within this spherical neighborhood using the method described above, and assigned the resulting value to the central voxel. Searchlight maps from individual participants were then aligned to the Montreal Neurological Institute (MNI) template with a linear transformation and submitted to a second-level random-effects analysis to test the reliability of discrimination across participants. To find the true type I error rate, we performed Monte Carlo simulations that permuted the sign of the whole-brain maps from individual participants (<xref ref-type="bibr" rid="c16">Nichols and Holmes, 2002</xref>; <xref ref-type="bibr" rid="c28">Winkler et al., 2014</xref>). We performed this procedure 1,000 times across the whole brain. Voxels were considered significant if they survived correction for multiple comparisons across the entire brain. The mean chance correlation was 0.</p>
</sec>
</sec>
<sec id="s4">
<title>Regions of interest</title>
<sec id="s4a">
<title>Scene-selective regions</title>
<p>We identified scene-selective regions of interest (ROIs). These ROIs were defined for each participant individually using a contrast of images&#x003E;words&#x002B;schemas, and a group-based anatomical constraint of scene-selective activation derived from a large number (42) of localizer participants from a previous study (<xref ref-type="bibr" rid="c10">Julian et al., 2012</xref>). Specifically, each ROI was defined as the top 100 voxels in each hemisphere that responded more to images than to words&#x002B;schemas and fell within the group-parcel mask for the ROI. To avoid double-dipping, we defined the ROI using the image&#x003E;word&#x002B;schema contrast for one run, then performed the MVPA analysis as described above on the remaining runs. This method ensures that all scene-selective ROIs could be defined in both hemispheres in every participant and that all ROIs contain the same number of voxels, thus facilitating comparisons between regions.</p>
</sec>
<sec id="s4b">
<title>Visual and Parietal regions</title>
<p>We defined early visual cortex (EVC) and intraparietal sulcus (IPS) using the probabilistic atlas from Wang and colleagues (<xref ref-type="bibr" rid="c25">Wang et al., 2015</xref>). These parcels were registered to participants&#x2019;-own-space and voxels were extracted. The MVPA analysis was then performed as described above using all data. We analyzed all IPS regions in one combined ROI (the union of all voxels from the Wang et al IPS parcels).</p>
</sec>
</sec>
<sec id="s5">
<title>Experimental Design and Statistical Analysis</title>
<p>We conducted 2 experimental studies. The sample size for the behavioral study was selected based on a power analysis from a smaller pilot study with 15 participants. The behavioral study and reported analyses were preregistered on Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/5dk37">https://osf.io/5dk37</ext-link>), but the code used to analyze the data was altered, because of software bugs, which were unknown at the time of the original registration. Within&#x002D; and between-subject factors and materials for that study can be found on Open Science Framework, and in the method section above.</p>
<p>The sample size for the imaging study was based on previous similar studies (<xref ref-type="bibr" rid="c21">Schindler and Bartels, 2013</xref>; <xref ref-type="bibr" rid="c15">Marchette et al., 2015</xref>) that examined within-subject differences in MVPA of the BOLD fMRI signal. Although we looked at individual differences in an exploratory fashion, we interpret these results with caution. Details and important parameters for the imaging study can be found in the Method section.</p>
<p>Across both studies, where appropriate, we corrected for multiple comparisons and report in the text how these determinations were made.</p>
</sec>
<sec id="s6">
<title>Results</title>
<sec id="s6a">
<title>Behavioral Study</title>
<p>Accuracy on the rolling one-back task was high (<italic>M</italic> &#x003D; 93.7&#x0025;, <italic>SD</italic> &#x003D; 21.57, Range &#x003D; &#x005B;75.0&#x0025; - 99.8&#x0025;&#x005D;), and reaction time was as expected (<italic>M</italic> &#x003D; 1.31s, <italic>SD</italic> &#x003D; 0.34s, Range &#x003D; &#x005B;0.70s &#x2013; 2.14s&#x005D;). We excluded data from one participant because of low accuracy (53.9&#x0025;) and fast reaction time (0.24s) compared to data from the rest of the sample.</p>
<p>Participants were equally accurate when responding to schemas (<italic>M</italic> &#x003D; 94.7&#x0025;, <italic>SD</italic> &#x003D; 4.2&#x0025;), and words (<italic>M</italic> &#x003D; 94.4&#x0025;, <italic>SD</italic> &#x003D; 4.2&#x0025;), <italic>t</italic>(46) &#x003D; 1.21, <italic>p</italic> &#x003D; .23, d &#x003D; 0.14, but more accurate responding to schemas compared to images (<italic>M</italic> &#x003D; 91.3&#x0025;, <italic>SD</italic> &#x003D; 7.2&#x0025;), <italic>t</italic>(46) &#x003D; 5.26, <italic>p</italic> &#x003D; .0000004, <italic>d</italic> &#x003D;0.97. Participants were also more accurate for words compared to images, <italic>t</italic>(46) &#x003D; 4.68, <italic>p</italic> &#x003D; .000003, <italic>d</italic> &#x003D; 0.88. This result shows that, compared to schemas and words, the spatial directions in the images were relatively ambiguous (i.e., it was possible to interpret a slight right turn as a right or an ahead). To avoid this confound and a speed-accuracy tradeoff, we excluded incorrect trials and only analyzed reaction times for correct trials across formats.</p>
</sec>
<sec id="s6b">
<title>Schemas are processed more quickly than images or words</title>
<p>In addition to excluding correct trials, we excluded trials for which the participant responded especially slowly &#x2013; greater than two standard deviations above his/her mean reaction time. We also excluded trials for which the answer was &#x201C;same,&#x201D; because these trials occurred relatively infrequently and could be considered oddball trials. They also required a different response than the other trials. All further analyses exclude trials as described above.</p>
<p><xref ref-type="fig" rid="fig2">Figure 2</xref> displays the main reaction time results for the behavioral study. Reaction time for schemas was quicker (<italic>M</italic> &#x003D; 1.13s, <italic>SD</italic> &#x003D; 0.28s) than for images (<italic>M</italic> &#x003D; 1.27s, <italic>SD</italic> &#x003D; 0.35s), <italic>t</italic>(46) &#x003D; 7.40, <italic>p</italic> &#x003D; .000000002, <italic>d</italic> &#x003D; 1.19, and words (<italic>M</italic> &#x003D; 1.18s, <italic>SD</italic> &#x003D; 0.26s), <italic>t</italic>(46) &#x003D; 3.09, <italic>p</italic> &#x003D; .003, <italic>d</italic> &#x003D; 0.49. Reaction time for words was also quicker than for images, <italic>t</italic>(46) &#x003D; 4.38, <italic>p</italic> &#x003D; .00007, <italic>d</italic> &#x003D; 0.97.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><title>Results from the behavioral study.</title><p>Response times were fastest overall for schemas and words. Schemas also showed the largest within-format effect. That is, participants were faster to respond when a schema came after a schema compared to word-word or image&#x00AD;image.</p></caption>
<graphic xlink:href="220137_fig2.tif"/>
</fig>
</sec>
<sec id="s6c">
<title>Same-format advantage</title>
<p>Comparing a spatial direction was faster when the preceding stimulus was in the same format. We calculated the average reaction time for each current format (the trial for which a response is generated) separately based on whether the previous trial was the same or a different format. The same-format advantage is operationalized as the difference in reaction time between same-format-preceding trials and different-format-preceding trials. Images (<italic>M</italic> &#x003D; 0.06s, <italic>SD</italic> &#x003D; 0.16s), one-sample <italic>t</italic>(46) &#x003D; 2.47, <italic>p</italic> &#x003D; .017, <italic>d</italic> &#x003D; 0.38, schemas (<italic>M</italic> &#x003D; 0.15s, <italic>SD</italic> &#x003D; 0.15s), one-sample <italic>t</italic>(46) &#x003D; 6.94, <italic>p</italic> &#x003D; .00000001, <italic>d</italic> &#x003D; 1.00, and words (<italic>M</italic> &#x003D; 0.12s, <italic>SD</italic> &#x003D; 0.13s), one-sample <italic>t</italic>(46) &#x003D; 6.38, <italic>p</italic> &#x003D; .00000008, <italic>d</italic> &#x003D; 0.92, all showed significant same-format advantages. Comparing the same-format advantage between images, schemas, and words revealed that schemas showed a larger same-format advantage than images, <italic>t</italic>(46) &#x003D; 3.43, <italic>p</italic> &#x003D; .001, <italic>d</italic> &#x003D; 0.50, and a marginally larger advantage than words, <italic>t</italic>(46) &#x003D; 1.86, <italic>p</italic> &#x003D; .07, <italic>d</italic> &#x003D; 0.34. Words showed a significantly larger same-format advantage than images, <italic>t</italic>(46) &#x003D; 2.35, <italic>p</italic> &#x003D; .02, <italic>d</italic> &#x003D; 0.28. Comparing spatial directions was always faster when these comparisons were within format, but schemas showed this effect most strongly.</p>
</sec>
<sec id="s6d">
<title>Scrambled backgrounds did not have behavioral effects</title>
<p>As reported in the Method section, the Gist model could not decode spatial directions across formats when scrambled backgrounds were used for schemas and words, but could do so with white backgrounds. Despite this finding, we did not find behavioral effects based on background. Reaction time (on all trials) was similar for White (<italic>M</italic> &#x003D; 1.28s, <italic>SD</italic> &#x003D; 0.32s) and Scrambled backgrounds (<italic>M</italic> &#x003D; 1.33s, <italic>SD</italic> &#x003D; 0.36s), <italic>t</italic>(45) &#x003D; 0.43, <italic>p</italic> &#x003D; .67, <italic>d</italic> &#x003D; 0.15. Accuracy was similar for White (<italic>M</italic> &#x003D; 93.1&#x0025;, <italic>SD</italic> &#x003D; 5.50&#x0025;) and Scrambled backgrounds (<italic>M</italic> &#x003D; 94.2&#x0025;, <italic>SD</italic> &#x003D; 4.30&#x0025;), <italic>t</italic>(45) &#x003D; 0.76, <italic>p</italic> &#x003D; .45, <italic>d</italic> &#x003D; 0.22. In addition, none of the above analyses interacted with the background condition.</p>
</sec>
<sec id="s6e">
<title>Reaction time correlates with egocentric not visual angular distance between trials</title>
<p>We instructed participants to imagine the directions as egocentric, with respect to their own body position, but wondered whether reaction time data were consistent with participants following this instruction. Thus, we calculated the angular distance between each pair of trials in two ways. The visual angle was calculated as the absolute value of the difference between the current and previous trials. The egocentric angle was calculated similarly, except that all angular distances were calculated to include ahead. That is, the angular distance between sharp right and sharp left was 90&#x00B0; for visual angle, but 270&#x00B0; for egocentric angle. To determine whether there was a significant correlation within participants, we calculated the Pearson&#x2019;s correlation between each participant&#x2019;s reaction time on that trial with the visual and egocentric angular distance between that trial and the previous trial. We then conducted one-sample <italic>t</italic>-tests to determine if there was a significant correlation in our sample, and within-subject <italic>t</italic>-tests to compare correlations. We found that egocentric angles correlated with reaction time positively (<italic>M</italic> &#x003D; .040, <italic>SD</italic> &#x003D; .065), <italic>t</italic>(46) &#x003D; 4.21, <italic>p</italic> &#x003D; .0001, <italic>d</italic> &#x003D; 0.61, but visual angle did not (<italic>M</italic> &#x003D; .0074, <italic>SD</italic> &#x003D; .064), <italic>t</italic>(46) &#x003D; 0.79, <italic>p</italic> &#x003D; .43, <italic>d</italic> &#x003D; 0.12. These patterns significantly differed from each other, <italic>t</italic>(46) &#x003D; 2.75, <italic>p</italic> &#x003D; .009, <italic>d</italic> &#x003D; 0.82. This pattern of results was obtained within each format separately, and angular distance calculation did not interact with format. This pattern of results reveals that participants interpreted spatial directions egocentrically because longer reaction times were associated with larger egocentric but not visual angle distances.</p>
</sec>
<sec id="s6f">
<title>Individual differences in cognitive style did not correlate with reaction time</title>
<p>No measures of reaction time correlated with either measure of the Verbalizer-Visualizer Questionnaire, all <italic>p</italic>&#x2019;s &#x003E; .08.</p>
</sec>
</sec>
<sec id="s7">
<title>fMRI Study</title>
<sec id="s7a">
<title>Behavioral performance during the fMRI task</title>
<p>Responses to the catch trials during the fMRI task were accurate (<italic>M</italic> &#x003D; 89.9&#x0025;, <italic>SD</italic> &#x003D; 6.74&#x0025;). Behavioral responses during one run for one participant fell below chance (43&#x0025;, 3/7 correct), so fMRI data for that run for that participant were excluded.</p>
</sec>
<sec id="s7b">
<title>Spatial direction decoding</title>
<p>Due to the large effects of format (see below), the following results are reported with the cocktail mean (the average neural activity pattern across all conditions) removed within each format and within each run separately. The pattern of results was unchanged when the cocktail mean was removed within each run and also across formats.</p>
</sec>
<sec id="s7c">
<title>Within-format decoding of spatial direction</title>
<p><xref ref-type="fig" rid="fig3">Figure 3A</xref> displays the within-format contrasts used to calculate whether spatial directions were decoded in each ROI. The whole grid represents a theoretical RDM of three separate contrasts: Same minus different spatial direction within each format. These contrasts were performed on each participant&#x2019;s neural RDM, calculated as described in the Method section by correlating the averaged parameter estimates for each trial type (e.g., a slight right word, or a sharp left schema) separately for the first and second half of each participant&#x2019;s runs. Separately for each format, grey squares were subtracted from colored squares. White squares were omitted.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><title>Within-format decoding of spatial direction.</title><p>The theoretical RDM (A) was compared to the neural RDM from five ROIs: the intraparietal sulcus (IPS), early visual cortex (EVC), and visual scene regions: occipital place area (OPA), parahippocampal place area (PPA), and retrosplenial complex (RSC). The IPS decoded spatial directions within images significantly greater than EVC. Within-format decoding was not significant in IPS or EVC for either schemas or words. Visual scene regions did not decode spatial direction within any of the three formats.</p></caption>
<graphic xlink:href="220137_fig3.tif"/>
</fig>
<p>Results for each of the ROIs and within each format are displayed in <xref ref-type="fig" rid="fig3">Figure 3B</xref>. Spatial direction was decoded within images in IPS (<italic>M</italic> &#x003D; 0.08, <italic>SD</italic> &#x003D; 0.10), one-sample <italic>t</italic>(19) &#x003D; 3.56, <italic>p</italic> &#x003D; .002, <italic>d</italic> &#x003D; 0.80. EVC did not decode spatial direction within images, (<italic>M</italic> &#x003D; 0.01, <italic>SD</italic> &#x003D; 0.09), one-sample <italic>t</italic>(19) &#x003D;0.73, <italic>p</italic> &#x003D; .47, <italic>d</italic> &#x003D; 0.10. IPS decoded spatial direction within images significantly more than EVC, <italic>t</italic>(19) &#x003D; 2.97, <italic>p</italic> &#x003D; .0078, <italic>d</italic> &#x003D;0.69. Scene regions did not decode spatial directions within images (<italic>M</italic><sub>OPA</sub> &#x003D; -0.003, <italic>SD</italic><sub>OPA</sub> &#x003D; 0.04, <italic>t</italic><sub>OPA</sub>(19) &#x003D; 0.36, <italic>p</italic> &#x003D; .72, <italic>d</italic> &#x003D; -0.08; <italic>M</italic><sub>PPA</sub> &#x003D; -0.006, <italic>SD</italic><sub>PPA</sub> &#x003D; 0.03, <italic>t</italic><sub>PPA</sub>(19) &#x003D; 0.90, <italic>p</italic> &#x003D; .38, <italic>d</italic> &#x003D; -0.20;<italic>M</italic><sub>RSC</sub> &#x003D; -0.003, <italic>SD</italic><sub>RSC</sub> &#x003D; 0.04, <italic>t</italic><sub>RSC</sub>(19) &#x003D; 0.34, <italic>p</italic> &#x003D; .74, <italic>d</italic> &#x003D; -0.08).</p>
<p>Spatial directions were not decoded for schemas or for words in any of the ROIs (all <italic>p</italic>&#x2019;s &#x003E; .26).</p>
</sec>
<sec id="s7d">
<title>Cross-format decoding of spatial direction</title>
<p>We also wished to learn if spatial directions could be decoded independently of the visual properties of individual formats. A brain region would show evidence of cross-format decoding of spatial direction if the correlation between the same spatial direction, presented in different formats, exceeded the correlation between different spatial directions, presented in different formats. <xref ref-type="fig" rid="fig4">Figure 4A</xref> displays the cross-format decoding theoretical RDM. Grey squares (different direction, different format) are subtracted from black squares (same direction, different format) to yield the degree of generalization. White squares are omitted.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><title>Cross-format decoding of spatial direction.</title><p>The theoretical RDM (A) was compared to the neural RDM from five ROIs: the intraparietal sulcus (IPS), early visual cortex (EVC), and visual scene regions: occipital place area (OPA), parahippocampal place area (PPA), and retrosplenial complex (RSC). The IPS decoded spatial directions across all three formats, but only marginally greater than EVC. Cross-format decoding was not significant in IPS or EVC for either schemas or words. Visual scene regions did not decode spatial direction across formats.</p></caption>
<graphic xlink:href="220137_fig4.tif"/>
</fig>
<p>Results for each of the ROIs are displayed in <xref ref-type="fig" rid="fig4">Figure 4B</xref>. Cross-format spatial directions were decoded in IPS (<italic>M</italic> &#x003D; 0.04, <italic>SD</italic> &#x003D; 0.06), one-sample <italic>t</italic>(19)&#x003D; 2.64, <italic>p</italic> &#x003D; .0128, <italic>d</italic> &#x003D; 0.67. There was marginally significant cross-format decoding in EVC (<italic>M</italic> &#x003D; 0.01, <italic>SD</italic> &#x003D; 0.02), one-sample <italic>t</italic>(19)&#x003D; 1.80, <italic>p</italic> &#x003D; .087, <italic>d</italic> &#x003D; 0.50. IPS decoded spatial directions across formats marginally more than EVC, <italic>t</italic>(19) &#x003D; 2.09, <italic>p</italic> &#x003D; .051, <italic>d</italic> &#x003D; 0.66. The scene regions did not decode spatial directions across formats (<italic>M</italic><sub>OPA</sub> &#x003D; -0.003, <italic>SD</italic><sub>OPA</sub> &#x003D; 0.02, <italic>t</italic><sub>OPA</sub>(19) &#x003D; 0.79, <italic>p</italic> &#x003D; .44, <italic>d</italic> &#x003D; -0.15; <italic>M</italic><sub>PPA</sub> &#x003D; 0.004, <italic>SD</italic><sub>PPA</sub> &#x003D; 0.015, <italic>t</italic><sub>PPA</sub>(19) &#x003D; 1.21, <italic>p</italic> &#x003D; .24, <italic>d</italic> &#x003D; 0.27;<italic>M</italic><sub>RSC</sub> &#x003D; 0.0009, <italic>SD</italic><sub>RSC</sub> &#x003D; 0.01, <italic>t</italic><sub>RSC</sub>(19) &#x003D; 0.34, <italic>p</italic> &#x003D; .74, <italic>d</italic> &#x003D; 0.09). These data reveal that IPS contain cross-format representations of spatial direction, but EVC and scene regions do not.</p>
<p>Within IPS we wanted to know whether cross-format decoding of spatial direction was driven by particular pairs of formats. For example, it is possible that spatial direction decoding was high between images and schemas, but comparatively lower between images and words. To investigate this, we conducted follow-up contrasts between each pair of formats similar to the omnibus test above (e.g., same direction, different format minus different direction, different format just for images versus schemas). These follow-up contrasts revealed significant schema&#x00AD;word decoding (<italic>M</italic> &#x003D; 0.036, <italic>SD</italic> &#x003D; 0.07), one-sample <italic>t</italic>(19) &#x003D; 2.23, <italic>p</italic> &#x003D; .04, <italic>d</italic> &#x003D; 0.51 and marginally significant image-schema decoding (<italic>M</italic> &#x003D; 0.024, <italic>SD</italic> &#x003D; 0.05), one-sample <italic>t</italic>(19) &#x003D;1.99, <italic>p</italic> &#x003D; .06, <italic>d</italic> &#x003D; 0.48. Image-word decoding was not significant (<italic>M</italic> &#x003D; 0.008, <italic>SD</italic> &#x003D; 0.07), one-sample <italic>t</italic>(19) &#x003D; 0.52, <italic>p</italic> &#x003D; .61, <italic>d</italic> &#x003D; 0.11. Although cross-format decoding of spatial direction in IPS could broadly be driven by amodal properties of spatial directions, these follow-up contrasts were not significantly different from each other (all pairwise <italic>p</italic>&#x2019;s &#x003E; .25). This pattern of results suggest that schemas may occupy an intermediary role, sharing neural responses in IPS with images and words respectively in a way not seen with images and words.</p>
</sec>
<sec id="s7e">
<title>Spatial direction similarity analysis</title>
<p>The preceding analyses reveal that IPS can distinguish between the seven spatial directions within images, and across formats. But there are two possible ways IPS could do this. IPS could be creating seven arbitrary and ad hoc categories for each spatial direction, which could allow any type of information to be decoded. If this interpretation is correct, the IPS&#x2019; role in spatial direction coding would be that it is creating a problem space onto which any possible stimulus categories could be mapped. For example, if the task were to sort stimuli based on seven colors, IPS would create seven color categories, which would be most similar to themselves (e.g., red is most similar to red), and different from all others. On the other hand, IPS could be involved because it helps distinguish spatial directions, specifically. If this interpretation is correct, the IPS&#x2019; role in spatial direction coding would be that it constructs a spatial representation of the possible directions. A counter-example for color would be that IPS contains a color-wheel representation. To distinguish which of these possibilities is correct, we can analyze off-diagonal spatial direction similarity. We would expect categories of turns (e.g., left to slight left) to be more similar to each other than to more distant turns (e.g., left to sharp right). We created a new theoretical RDM in which all left turns (sharp left, left, and slight left) were similar to each other, and dissimilar to all right turns (and vice versa for right to left turns). Ahead directions were coded as dissimilar from everything else. We excluded the diagonal to ensure that these results are not recapitulations of the spatial direction decoding analyses above. That is, this analysis captures similarity among non-identical spatial directions to show that IPS neural patterns contain spatial information (not arbitrary category information).</p>
<p>We found that the neural pattern of activity in IPS in response to images correlated more strongly between left turns than across left and right turns (<italic>M</italic> &#x003D; 0.036, <italic>SD</italic> &#x003D; 0.063), <italic>t</italic>(19) &#x003D; 2.59, <italic>p</italic> &#x003D; .018, <italic>d</italic> &#x003D; 0.57. This was not the case for schemas, (<italic>M</italic> &#x003D; -0.018, <italic>SD</italic> &#x003D; 0.086), <italic>t</italic>(19) &#x003D; 0.09, <italic>p</italic> &#x003D; .93, <italic>d</italic> &#x003D; -0.21, nor for words, (<italic>M</italic> &#x003D; -0.017, <italic>SD</italic> &#x003D; 0.075), <italic>t</italic>(19) &#x003D; 1.02, <italic>p</italic> &#x003D; .32, <italic>d</italic> &#x003D; -0.23, nor across formats, (<italic>M</italic> &#x003D; 0.009, <italic>SD</italic> &#x003D; 0.037), <italic>t</italic>(19) &#x003D; 1.10, <italic>p</italic> &#x003D; .29, <italic>d</italic> &#x003D; 0.24. This result provides evidence that images were represented spatially, by distinguishing left from right turns, in IPS, and not as seven arbitrary and ad hoc categories. Although this analysis shows that IPS codes spatial content, the theoretical RDM we chose was not the only possible one. We also conducted a representational similarity analysis wherein we correlated the neural RDM with a spatial direction model where similarity linearly decreased as a function of spatial angle, but this analysis did not achieve statistical significance. We thus interpret this result as some evidence of spatial content in IPS, but do not feel strongly that the representation is categorical (i.e., all lefts are more similar to each other than to rights).</p>
</sec>
<sec id="s7f">
<title>Format decoding</title>
<p>In the following analyses, we removed the cocktail mean within run, across all formats.</p>
</sec>
<sec id="s7g">
<title>Format decoding in ROIs</title>
<p>In addition to direction coding, we wanted to determine whether the format of images was represented in these ROIs. The theoretical RDM for this contrast is presented in <xref ref-type="fig" rid="fig5">Figure 5A</xref>. For this analysis, we excluded correlations between stimuli that were the same direction and the same format (white squares in <xref ref-type="fig" rid="fig5">Figure 5A</xref>). To decode format, a region would show higher correlations between stimuli that were the same format compared to stimuli that were different formats (black squares minus grey squares in <xref ref-type="fig" rid="fig5">Figure 5A</xref>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><title>Decoding of format in ROIs.</title><p>The theoretical RDM (A) was compared to the neural RDM from five ROIs: : the intraparietal sulcus (IPS), early visual cortex (EVC), and visual scene regions: occipital place area (OPA), parahippocampal place area (PPA), and retrosplenial complex (RSC). All regions significantly decoded the format of the representation, except for RSC. Multidimensional scaling plots (C) reveal that IPS separates all three formats whereas OPA distinguishes images from the other two. Arrows depict that categorical spatial directions (right, slight right, and sharp right collapsed as right arrows; left, slight left, and sharp left collapsed as left arrows; ahead as an up arrow).</p></caption>
<graphic xlink:href="220137_fig5.tif"/>
</fig>
<p>Results from the omnibus format decoding contrast can be seen in <xref ref-type="fig" rid="fig5">Figure 5B</xref>. Format could be decoded in IPS (<italic>M</italic> &#x003D; 0.14, <italic>SD</italic> &#x003D; 0.09), one-sample <italic>t</italic>(19) &#x003D; 7.25, <italic>p</italic> &#x003D; .0000007, <italic>d</italic> &#x003D; 1.56, and EVC (<italic>M</italic> &#x003D; 0.02, <italic>SD</italic> &#x003D; 0.03), one-sample <italic>t</italic>(19) &#x003D; 3.58, <italic>p</italic> &#x003D; .002, <italic>d</italic> &#x003D; 0.67, although format decoding was significantly higher in IPS than EVC t(19) &#x003D; 6.39, <italic>p</italic> &#x003D; .000004, <italic>d</italic> &#x003D; 1.63. OPA (<italic>M</italic> &#x003D; 0.04, <italic>SD</italic> &#x003D; 0.03), one-sample <italic>t</italic>(19) &#x003D; 7.07, <italic>p</italic> &#x003D; .000001, <italic>d</italic> &#x003D; 1.33, and PPA (<italic>M</italic> &#x003D; 0.008, <italic>SD</italic> &#x003D; 0.01), one-sample <italic>t</italic>(19) &#x003D; 2.54, <italic>p</italic> &#x003D; .02, <italic>d</italic> &#x003D; 0.80, also decoded format, although RSC did not (<italic>M</italic> &#x003D; 0.003, <italic>SD</italic> &#x003D; 0.008), one-sample <italic>t</italic>(19) &#x003D; 1.39, <italic>p</italic> &#x003D; .18, <italic>d</italic> &#x003D; 0.38.</p>
<p>We wanted to know whether the regions that significantly decoded format generally (IPS, EVC, OPA, and PPA) could decode pairwise formats. We thus looked at schema-word, schema&#x00AD;image, and image-word decoding separately for each ROI. See <xref ref-type="table" rid="tbl1">Table 1</xref> for the complete results. In sum, pairwise formats could be decoded to some extent in each ROI except RSC. In IPS and OPA, all three pairs of formats could be distinguished, whereas PPA predominantly dissociated images from the schemas and words.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><p>IPS (Intraparietal Sulcus). EVC (Early Visual Cortex). OPA (Occipital Place Area). PPA (Parahippocampal Place Area).</p></caption>
<graphic xlink:href="220137_tbl1.tif"/>
</table-wrap>
<p>To visualize whether format decoding was similar across IPS and OPA, each ROI&#x2019;s neural RDM was submitted to multidimensional scaling (MDS), resulting in two-dimensional maps of each spatial direction and format. In these maps (<xref ref-type="fig" rid="fig5">Figure 5C</xref>), each arrow depicts one trial type, and the distance between arrows are interpreted as the pair&#x2019;s representational dissimilarity. For ease of interpretation, we collapsed across left, right, and ahead. The MDS plots emphasize that while both regions distinguish between all three formats, schemas and words are more clearly disambiguated in IPS. Notably, format accounts for a large proportion of the variance captured by both regions, in spite of the fact that participants were asked to respond only to the spatial direction in the stimulus independent of the format.</p>
</sec>
<sec id="s7h">
<title>Format decoding in searchlight analyses</title>
<p>Format decoding was robust within our regions of interest. We also queried the whole brain. We ran two searchlight analyses to see where formats were decoded across the whole brain. First, we analyzed which regions represented images as more similar to images than images to schemas or words. These regions are visualized in hot colors in <xref ref-type="fig" rid="fig6">Figure 6</xref>. In addition to parietal lobes, canonical scene regions (OPA, RSC, PPA) have higher correlations between images than with images to other formats. Second, we analyzed which regions represented schemas as more similar to schemas compared to words, and words more similar to words than schemas. This analysis uses the same baseline, word-schema correlations, and thus cannot distinguish whether these regions represent words as more similar to words, schemas more similar to schemas, or both. These regions are visualized in cool colors in <xref ref-type="fig" rid="fig6">Figure 6</xref>. Here, we saw bilateral fusiform gyrus, and inferior lateral occipital cortex, regions which have been implicated in word and object processing.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><title>Decoding of format, whole brain searchlight.</title><p>The theoretical RDM from <xref ref-type="fig" rid="fig5">Figure 5A</xref> generated the contrast between same format minus different format correlations for images-images minus images-words/schemas (in hot colors) and for schemas-schemas and words-words minus schemas-words (in cool colors). Image correlations were strongest in scene regions(OPA, PPA, RSC) and IPS, whereas schema and word correlations were strongest in word and object visual areas. Lower bound for searchlights are permutation corrected thresholds; upper bounds are <italic>p</italic> &#x003C; .00001 uncorrected.</p></caption>
<graphic xlink:href="220137_fig6.tif"/>
</fig>
</sec>
<sec id="s7i">
<title>Individual differences in cognitive style correlate with cross-format decoding</title>
<p>Given the small sample size, these results should be interpreted cautiously. In an exploratory analysis, we correlated both dimensions of the Verbalizer-Visualizer Questionnaire (VVQ) with spatial direction decoding, within and across formats, in IPS. Cross-format decoding in IPS significantly negatively correlated with Visualizer score, <italic>r</italic>(20) &#x003D; -.51, <italic>p</italic> &#x003D; .02. No other decoding correlated significantly with either the Visualizer or Verbalizer subscales, <italic>p</italic> &#x003E; .19. This finding hints at individual differences at how different representational formats are cross-decoded. Responses to the de-briefing questionnaire also indicated that some participants preferred to say words to themselves, whereas others preferred to picture directions, or even imagine part of their body (e.g., their left shoulder for slight left).</p>
</sec>
</sec>
<sec id="s8">
<title>Discussion</title>
<p>The goal of these experiments was to investigate how spatial directions conveyed by distinct representational formats &#x2013; visual scenes, schemas, and words &#x2013; are behaviorally processed and neurally organized. Our broad hypothesis is that schemas and words elide the spatial processing required by visual scenes, and are thus processed more efficiently. This work bridges non-human animal models of navigation, which show cognitive mapping of environments from visual scenes, with human neuroimaging and behavioral research, which allow investigation of schematic and verbal communication of spatial directions.</p>
<p>Overall, these findings support a model of spatial direction processing which, for the current experiments, taps the egocentric network to compute planned paths in visual scenes (<xref ref-type="bibr" rid="c21">Schindler &#x0026; Bartels, 2013</xref>), but eschews in-depth spatial computations for more efficient format-specific visual processing. In other words, computing the egocentric spatial direction in visual scenes requires imagining travel on the path shown. Computing the spatial direction in words and schemas requires only visual identification. This difference is consistent with the distinct information conveyed by each representational format. Visual scenes contain concrete detail, which is irrelevant to the spatial direction, but allows the navigator to imagine traveling through the scene. On the other hand, schemas and words contain abstract direction information, which can be visually distinguished, but do not invite imagining travel in the same way as scenes.</p>
<p>In support of this model, we report three main findings. First, in a behavioral experiment, people responded to schemas and words more quickly than to scenes. Second, in an fMRI experiment, the intraparietal sulcus bilaterally (IPS) decoded spatial directions in scenes, and across the three formats, but not within schemas or words. Taken together, the first and second findings suggest that, compared to words and schemas, scenes require relatively costly egocentric spatial computation, resulting in decoding spatial directions in IPS for scenes but faster processing for schemas and words. Third, format decoding independent of spatial directions was robust in ROI analyses and whole-brain searchlights. This finding suggests that, despite being task irrelevant (i.e., once the spatial direction is encoded, participants would be better off discarding format, since they could be asked to recall the spatial direction in any format), formats tap distinct neural pathways to convey conceptual information.</p>
<p>Why might schemas and words be processed more quickly than scenes&#x003F; First, unlike schemas, which discard irrelevant visual information and distill conceptually-important content, visual scenes contain irrelevant detail, to be ignored when computing the spatial direction being depicted. Second, the spatial directions conveyed by schemas and words are always the same exact direction (or, in the case of words, can be imagined to be). Visual scenes can deviate from, for example, an exact 90&#x00B0; left turn. Thus, the spatial direction in a visual scene must be computed for each presentation, then compared to the previous spatial direction, whereas schemas and words need not be processed with this level of discrimination.</p>
<p>If spatial directions are computed from visual scenes, brain regions which support direction processing should contain representations of spatial directions for visual scenes, but not for schemas or words. This pattern was observed in the IPS bilaterally, regions of the brain which are implicated in egocentric spatial direction processing when people imagine visual scenes (<xref ref-type="bibr" rid="c11">Karnath, 1997</xref>; <xref ref-type="bibr" rid="c27">Whitlock et al., 2008</xref>; <xref ref-type="bibr" rid="c4">Galati et al., 2010</xref>; <xref ref-type="bibr" rid="c21">Schindler and Bartels, 2013</xref>). We also found cross-decoding between schemas, words, and visual scenes in the IPS bilaterally. One possible explanation of our results is that when an individual views a scene, the IPS compute egocentric spatial directions from visual scenes by imagining the path of travel, resulting in a strong signal for each direction. However, when an individual views a schema or word, the discrimination of spatial direction does not require IPS to compute egocentric spatial directions, yet it does so transiently, resulting in a weak signal. Within schema and word formats, then, this weak signal might not be decodable by themselves. Comparing the weak signal from schemas to the strong signal from scenes yields cross-format decoding. A second possibility is that all representational formats cued participants to orient attention toward a region of space, which was strongest for scenes, weaker for schemas, and weakest for words. The orientation of visual attention would then have resulted in similar signals from IPS for the same direction, and resulted in the same outcome.</p>
<p>We did not observe spatial direction decoding in OPA or RSC. The current results are reconcilable with previous researching showing spatial direction decoding in OPA (<xref ref-type="bibr" rid="c3">Bonner and Epstein, 2017</xref>) because our participants did not view walkable pathways. Similarly, we can reconcile the current results with previous research showing allocentric spatial direction decoding in RSC (<xref ref-type="bibr" rid="c23">Vass and Epstein, 2013</xref>; <xref ref-type="bibr" rid="c15">Marchette et al., 2015</xref>), because our instructions requested that participants imagine traveling through each scene. Moreover, the correlation between reaction time and egocentric angle between the current and preceding spatial directions (but not visual angle) provide evidence from the behavioral experiment that participants were encoding spatial direction egocentrically. Changing task instructions may result in allocentric direction coding, yielding spatial direction decoding in RSC, but this possibility awaits further study.</p>
<p>Do schemas occupy a middle ground between words (abstract and arbitrarily related to the concept they denote), and visual scenes (concrete, rich in relevant and irrelevant detail)&#x003F; Other conceptual domains &#x2013; spatial relations and actions &#x2013; support this notion of neural overlap between schemas, words, and visual depictions of concepts. Previous work on spatial prepositions has shown neural overlap in regions which process schemas and words, and separate areas which process schemas and visual images (<xref ref-type="bibr" rid="c2">Amorapanth et al., 2012</xref>). Viewing action words (like running) and schemas also resulted in cross-format decoding in action simulation and semantics areas (<xref ref-type="bibr" rid="c20">Quandt et al., 2017</xref>). In the current work, cross-format decoding of spatial directions was present in regions of the brain which process egocentric spatial directions. Although this result seemed to be driven by schema-word and schema-visual scene similarity, these post-hoc tests did not meet statistical significance.</p>
<p>Despite format being task irrelevant, format decoding was robust. Whereas images were processed distinctly from schemas and words in, predominantly, visual scene regions, schemas and words were disambiguated in IPS, as well as in object and word visual areas. This pattern of results supports a model of concept coding in which abstract features are extracted from stimuli in format-dependent regions, then passed along to brain regions which perform computations on the abstract concept. This finding is consistent with our behavioral data, suggesting implicit neural differences in the way scenes, schemas, and words are processed.</p>
<p>One limitation of the current results is that we cannot account for all possible task-based effects (<xref ref-type="bibr" rid="c6">Harel et al., 2014</xref>) such as requiring participants to group spatial directions into seven categories, or to prepare to respond to any representational format. We opted to use a naturalistic task because of its applied relevance. We also tried to maximize the opportunity to find cross-decoding. When reading directions, for example, one might need to match a &#x2018;slight left&#x2019; from memory to an egocentric road direction, a task which is comparable to our rolling one-back design, and requires a navigator to translate words to scenes. Still, spatial directions are not always categorized into seven groups. During walking, for example, a human navigator can easily and automatically turn precisely 145&#x00B0; clockwise, while not necessarily categorizing this turn as a sharp right. Nevertheless, we observed spatially-specific categorization in bilateral IPS for the visual scenes: lefts were more similar to each other than rights, excluding the exact same direction. Note that this finding is counter-productive for the one-back task. Representing a slight left as more similar to a left than it is to a slight right means it is harder to disambiguate a slight left from a left. This could be due to noise in our selection of images (i.e., a participant mistakenly thinking a left is a slight left), but still shows the spatial information contained in IPS coding. Future work should disentangle the task-specific effects of active categorization of spatial directions from the automatic processing of spatial direction during an orthogonal task.</p>
<p>In sum, the current experiments reveal the similarities and differences in formats of spatial direction depictions. Behaviorally, schemas and words were responded to more quickly than visual scenes. Neural decoding of spatial directions was strongest for visual scenes in IPS bilaterally. This region also revealed evidence of cross-format, abstract representation of spatial directions. These data challenge the specificity of IPS in encoding egocentric spatial directions, and support a model of spatial processing wherein images require spatial direction computation, whereas schemas and words do not.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>The authors wish to acknowledge NIH grants F32DC015203 to S.M.W., and R01DC012511 and NSF Science of Learning Centers award 1041707 (subcontract 330161-18110-7341) to A.C. They also acknowledge Sam Trinh for stimuli development and pilot testing, Antonio Nicosia for behavioral study data collection, and Russell Epstein for helpful comments and feedback.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Aguirre</surname> <given-names>GK</given-names></string-name> (<year>2007</year>) <article-title>Continuous carry-over designs for fMRI</article-title>. <source>Neuroimage</source> <volume>35</volume>:<fpage>1480</fpage>&#x2013;<lpage>1494</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Amorapanth</surname> <given-names>P</given-names></string-name>, <string-name><surname>Kranjec</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bromberger</surname> <given-names>B</given-names></string-name>, <string-name><surname>Lehet</surname> <given-names>M</given-names></string-name>, <string-name><surname>Widick</surname> <given-names>P</given-names></string-name>, <string-name><surname>Woods</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Kimberg</surname> <given-names>DY</given-names></string-name>, <string-name><surname>Chatterjee</surname> <given-names>A</given-names></string-name> (<year>2012</year>) <article-title>Language, perception, and the schematic representation of spatial relations</article-title>. <source>Brain Lang</source> <volume>120</volume>:<fpage>226</fpage>&#x2013;<lpage>236</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Bonner</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Epstein</surname> <given-names>RA</given-names></string-name> (<year>2017</year>) <article-title>Coding of navigational affordances in the human visual system</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>114</volume>:<fpage>4793</fpage>&#x2013;<lpage>4798</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Galati</surname> <given-names>G</given-names></string-name>, <string-name><surname>Pelle</surname> <given-names>G</given-names></string-name>, <string-name><surname>Berthoz</surname> <given-names>A</given-names></string-name>, <string-name><surname>Committeri</surname> <given-names>G</given-names></string-name> (<year>2010</year>) <article-title>Multiple reference frames used by the human brain for spatial perception and memory</article-title>. <source>Exp Brain Res</source> <volume>206</volume>:<fpage>109</fpage>&#x2013;<lpage>120</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Gilboa</surname> <given-names>A</given-names></string-name>, <string-name><surname>Marlatte</surname> <given-names>H</given-names></string-name> (<year>2017</year>) <article-title>Neurobiology of Schemas and Schema-Mediated Memory</article-title>. <source>Trends Cogn Sci</source> <volume>21</volume>:<fpage>618</fpage>&#x2013;<lpage>631</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Harel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Kravitz</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Baker</surname> <given-names>CI</given-names></string-name> (<year>2014</year>) <article-title>Task context impacts visual object processing differentially across the cortex</article-title>. <source>Proc Natl Acad Sci</source> <volume>111</volume>:<fpage>E962</fpage>&#x2013;<lpage>E971</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Haxby</surname> <given-names>J V.</given-names></string-name>, <string-name><surname>Gobbini</surname> <given-names>MI</given-names></string-name>, <string-name><surname>Furey</surname> <given-names>ML</given-names></string-name>, <string-name><surname>Ishai</surname> <given-names>A</given-names></string-name>, <string-name><surname>Schouten</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Pietrini</surname> <given-names>P</given-names></string-name> (<year>2001</year>) <article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title>. <source>Science</source> <volume>293</volume>:<fpage>2425</fpage>&#x2013;<lpage>2430</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Jenkinson</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bannister</surname> <given-names>P</given-names></string-name>, <string-name><surname>Brady</surname> <given-names>M</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>S</given-names></string-name> (<year>2002</year>) <article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title>. <source>Neuroimage</source> <volume>17</volume>:<fpage>825</fpage>&#x2013;<lpage>841</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Jenkinson</surname> <given-names>M</given-names></string-name>, <string-name><surname>Beckmann</surname> <given-names>CF</given-names></string-name>, <string-name><surname>Behrens</surname> <given-names>TEJ</given-names></string-name>, <string-name><surname>Woolrich</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>SM</given-names></string-name> (<year>2012</year>) <article-title>FSL</article-title>. <source>Neuroimage</source> <volume>62</volume>:<fpage>782</fpage>&#x2013;<lpage>790</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Julian</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Fedorenko</surname> <given-names>E</given-names></string-name>, <string-name><surname>Webster</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name> (<year>2012</year>) <article-title>An algorithmic method for functionally defining regions of interest in the ventral visual pathway</article-title>. <source>Neuroimage</source> <volume>60</volume>:<fpage>2357</fpage>&#x2013;<lpage>2364</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Karnath</surname> <given-names>HO</given-names></string-name> (<year>1997</year>) <article-title>Spatial orientation and the representation of space with parietal lobe lesions</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source> <volume>352</volume>:<fpage>1411</fpage>&#x2013;<lpage>1419</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Kirby</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Moore</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Schofield</surname> <given-names>NJ</given-names></string-name> (<year>1988</year>) <article-title>Verbal and visual learning styles</article-title>. <source>Contemp Educ Psychol</source> <volume>13</volume>:<fpage>169</fpage>&#x2013;<lpage>184</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="book"><string-name><surname>Klippel</surname> <given-names>A</given-names></string-name>, <string-name><surname>Montello</surname> <given-names>DR</given-names></string-name> (<year>2007</year>) <chapter-title>Linguistic and Nonlinguistic Turn Direction Concepts</chapter-title>. <source>In: Spatial Information Theory</source>, pp <fpage>354</fpage>&#x2013;<lpage>372</lpage>. <publisher-name>Berlin, Heidelberg</publisher-name>: <publisher-loc>Springer Berlin Heidelberg</publisher-loc>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Kriegeskorte</surname> <given-names>N</given-names></string-name>, <string-name><surname>Goebel</surname> <given-names>R</given-names></string-name>, <string-name><surname>Bandettini</surname> <given-names>P</given-names></string-name> (<year>2006</year>) <article-title>Information-based functional brain mapping</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>103</volume>:<fpage>3863</fpage>&#x2013;<lpage>3868</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Marchette</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Vass</surname> <given-names>LK</given-names></string-name>, <string-name><surname>Ryan</surname> <given-names>J</given-names></string-name>, <string-name><surname>Epstein</surname> <given-names>RA</given-names></string-name> (<year>2015</year>) <article-title>Outside Looking In: Landmark Generalization in the Human Navigational System</article-title>. <source>J Neurosci</source> <volume>35</volume>:<fpage>14896</fpage>&#x2013;<lpage>14908</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Nichols</surname> <given-names>TE</given-names></string-name>, <string-name><surname>Holmes</surname> <given-names>AP</given-names></string-name> (<year>2002</year>) <article-title>Nonparametric permutation tests for functional neuroimaging: a primer with examples</article-title>. <source>Hum Brain Mapp</source> <volume>15</volume>:<fpage>1</fpage>&#x2013;<lpage>25</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Nili</surname> <given-names>H</given-names></string-name>, <string-name><surname>Wingfield</surname> <given-names>C</given-names></string-name>, <string-name><surname>Walther</surname> <given-names>A</given-names></string-name>, <string-name><surname>Su</surname> <given-names>L</given-names></string-name>, <string-name><surname>Marslen-Wilson</surname> <given-names>W</given-names></string-name>, <string-name><surname>Kriegeskorte</surname> <given-names>N</given-names></string-name> (<year>2014</year>) <article-title>A Toolbox for Representational Similarity Analysis</article-title>. <source>PLoS Comput Biol 10</source>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Oldfield</surname> <given-names>RC</given-names></string-name> (<year>1971</year>) <article-title>The assessment and analysis of handedness: the Edinburgh inventory</article-title>. <source>Neuropsychologia</source> <volume>9</volume>:<fpage>97</fpage>&#x2013;<lpage>113</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Oliva</surname> <given-names>A</given-names></string-name>, <string-name><surname>Torralba</surname> <given-names>A</given-names></string-name> (<year>2006</year>) <article-title>Building the gist of a scene: the role of global image features in recognition</article-title>. <source>Prog Brain Res</source> <volume>155</volume>:<fpage>23</fpage>&#x2013;<lpage>36</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Quandt</surname> <given-names>LC</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>Y&#x2010;S</given-names></string-name>, <string-name><surname>Chatterjee</surname> <given-names>A</given-names></string-name> (<year>2017</year>) <article-title>Neural bases of action abstraction</article-title>. <source>Biol Psychol</source> <volume>129</volume>:<fpage>314</fpage>&#x2013;<lpage>323</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Schindler</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bartels</surname> <given-names>A</given-names></string-name> (<year>2013</year>) <article-title>Parietal Cortex Codes for Egocentric Space beyond the Field of View</article-title>. <source>Curr Biol</source> <volume>23</volume>:<fpage>177</fpage>&#x2013;<lpage>182</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="book"><string-name><surname>Talmy</surname> <given-names>L</given-names></string-name> (<year>2000</year>) <source>Toward a cognitive semantics</source>. <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Vass</surname> <given-names>LK</given-names></string-name>, <string-name><surname>Epstein</surname> <given-names>RA</given-names></string-name> (<year>2013</year>) <article-title>Abstract Representations of Location and Facing Direction in the Human Brain</article-title>. <source>J Neurosci</source> <volume>33</volume>:<fpage>6133</fpage>&#x2013;<lpage>6142</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Vass</surname> <given-names>LK</given-names></string-name>, <string-name><surname>Epstein</surname> <given-names>RA</given-names></string-name> (<year>2017</year>) <article-title>Common Neural Representations for Visually Guided Reorientation and Spatial Imagery</article-title>. <source>Cereb Cortex</source> <volume>27</volume>:<fpage>1457</fpage>&#x2013;<lpage>1471</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Wang</surname> <given-names>L</given-names></string-name>, <string-name><surname>Mruczek</surname> <given-names>REB</given-names></string-name>, <string-name><surname>Arcaro</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Kastner</surname> <given-names>S</given-names></string-name> (<year>2015</year>) <article-title>Probabilistic Maps of Visual Topography in Human Cortex</article-title>. <source>Cereb Cortex</source> <volume>25</volume>:<fpage>3911</fpage>&#x2013;<lpage>3931</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Watson</surname> <given-names>CE</given-names></string-name>, <string-name><surname>Cardillo</surname> <given-names>ER</given-names></string-name>, <string-name><surname>Bromberger</surname> <given-names>B</given-names></string-name>, <string-name><surname>Chatterjee</surname> <given-names>A</given-names></string-name> (<year>2014</year>) <article-title>The specificity of action knowledge in sensory and motor systems</article-title>. <source>Front Psychol</source> <volume>5</volume>:<fpage>494</fpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Whitlock</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Sutherland</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Witter</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>M&#x2010;B</given-names></string-name>, <string-name><surname>Moser</surname> <given-names>EI</given-names></string-name> (<year>2008</year>) <article-title>Navigating from hippocampus to parietal cortex</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>105</volume>:<fpage>14755</fpage>&#x2013;<lpage>14762</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Winkler</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Ridgway</surname> <given-names>GR</given-names></string-name>, <string-name><surname>Webster</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Nichols</surname> <given-names>TE</given-names></string-name> (<year>2014</year>) <article-title>Permutation inference for the general linear model</article-title>. <source>Neuroimage</source> <volume>92</volume>:<fpage>381</fpage>&#x2013;<lpage>397</lpage>.</mixed-citation></ref>
</ref-list>
<fn-group>
<fn id="fn1"><label><sup>1</sup></label><p>We are agnostic about whether the images used in this study can actually be considered visual scenes. We use the term &#x201C;image&#x201D; below to be consistent with the general terms &#x2018;word&#x2019; and &#x2018;schema.&#x2019; In the introduction and discussion, on the other hand, we discuss &#x2018;visual scenes&#x2019; to connect the domain specific work here with other research on visual scenes. The robust activation of the scene network while participants viewed the images also lead us to speculate that participants treated these stimuli as scenes.</p>
</fn>
</fn-group>
</back>
</article>