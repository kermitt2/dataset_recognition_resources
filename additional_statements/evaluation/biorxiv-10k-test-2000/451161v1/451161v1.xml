<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/451161</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Animal Behavior and Cognition</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Communicating compositional patterns</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Schulz</surname><given-names>Eric</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Quiroga</surname><given-names>Francisco</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Gershman</surname><given-names>Samuel J.</given-names></name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Psychology Harvard University</institution></aff>
<aff id="a2"><label>2</label><institution>Department of Experimental Psychology University College</institution> London</aff>
<aff id="a3"><label>3</label><institution>Department of Psychology Harvard University</institution></aff>
</contrib-group>
<author-notes>
<corresp>Correspondence concerning this article should be addressed to Eric Schulz, Harvard University, 52 Oxford Street, Room 295.08, Cambridge, MA 02138, E-mail: <email>ericschulz@fas.harvard.edu</email>.</corresp>
</author-notes>
<pub-date pub-type="epub"><year>2018</year></pub-date>
<elocation-id>451161</elocation-id>
<history>
<date date-type="received">
<day>23</day>
<month>10</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>23</day>
<month>10</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>23</day>
<month>10</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="451161.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>How do people perceive and communicate structure? We investigate this question by letting participants play a communication game, where one player describes a pattern, and another player redraws it based on the description alone. We use this paradigm to compare two models of pattern description, one compositional (complex structures built out of simpler ones) and one non-compositional. We find that compositional patterns are communicated more effectively than non-compositional patterns, that a compositional model of pattern description predicts which patterns are harder to describe, and that this model can be used to evaluate participants&#x2019; drawings, producing human-like quality ratings. Our results suggest that natural language can tap into a compositionally structured pattern description language.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Communication games;</kwd>
<kwd>Cultural transmission;</kwd>
<kwd>Compositionality;</kwd>
<kwd>Function learning</kwd>
</kwd-group>
<counts>
<page-count count="21"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Humans see patterns everywhere, and eagerly communicate them to one another. However, little is known formally about how we communicate patterns, what kinds of patterns are easier or harder to communicate, and how we reconstruct patterns from natural language. This paper seeks to bridge this gap by combining a pattern communication game with a mathematical model of pattern description (<xref ref-type="bibr" rid="c19">Quiroga, Schulz, Speekenbrink, &#x0026; Harvey, 2018</xref>; <xref ref-type="bibr" rid="c24">Schulz, Tenenbaum, Duvenaud, Speekenbrink, &#x0026; Gershman, 2017</xref>).</p>
<p>Consider the graphs shown in <xref rid="fig1" ref-type="fig">Figure 1</xref>, which plot time series of CO2 emission, airline passenger volume, search frequency for the term &#x201C;gym membership.&#x201D; Experiments suggest that humans perceive these graphs as compositions of simpler patterns, such as lines, oscillations, and smoothly changing curves (<xref ref-type="bibr" rid="c19">Quiroga et al., 2018</xref>; <xref ref-type="bibr" rid="c24">Schulz, Tenenbaum, et al., 2017</xref>). For example, there is seasonal variation in passenger volume (a periodic component with time-dependent amplitude), superimposed on a linear increase over time.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>(Colour online) Examples of compositional patterns.</title>
<p>Left: Monthly average atmospheric CO2 concentrations collected at the Mauna Loa Observatory in Hawaii from 1960-2010. Center: Number of airline passengers from 1960-2010, originally collected by <xref ref-type="bibr" rid="c1">Box, Jenkins, Reinsel, and Ljung (2015)</xref>. Right: Google queries for &#x201C;Gym membership&#x201D; from 2002-2012 in the city of London.</p></caption>
<graphic xlink:href="451161_fig1.tif"/>
</fig>
<p>As described in more detail in the next section, we can formalize this idea using a pattern description language consisting of functional primitives and algebraic operations that compose them together. By defining a probability distribution over this description language, we can express an inductive bias for certain kinds of functions&#x2014;in particular, functions that can be described with a small number of compositions (<xref ref-type="bibr" rid="c4">Duvenaud, Lloyd, Grosse, Tenenbaum, &#x0026; Ghahramani, 2013</xref>; <xref ref-type="bibr" rid="c14">Lloyd, Duvenaud, Grosse, Tenenbaum, &#x0026; Ghahramani, 2014</xref>; <xref ref-type="bibr" rid="c24">Schulz, Tenenbaum, et al., 2017</xref>). In other words, the &#x201C;mental&#x201D; description length of a function relates to the complexity of its encoding in the compositional pattern description language.</p>
<p>Here we extend this idea one step further, asking whether there is a correspondence between the pattern description language and natural language descriptions of functions. We proceed in three steps. First, we ask participants to describe functions sampled from compositional or non-compositional distributions. Second, we asked a separate group of participants to redraw the original function using only the description. Third, we ask another group of participants to rate how well each drawing corresponds to the original. We hypothesized that compositional functions would be easier to reconstruct compared to non-compositional functions, under the assumption that the former allow for a mental description that can be more easily encoded into natural language and decoded back into the function space.</p>
<sec id="s1a">
<title>A compositional pattern description language</title>
<p>Our model of pattern description is based on a Gaussian Process (GP) regression approach to function learning (<xref ref-type="bibr" rid="c20">C. Rasmussen &#x0026; Williams, 2006</xref>; <xref ref-type="bibr" rid="c23">Schulz, Speekenbrink, &#x0026; Krause, 2017</xref>). A GP is a collection of random variables, any finite subset of which is jointly Gaussian. A GP defines a distribution over functions. Let <italic>f</italic>: <italic>&#x03C7;</italic> &#x2192; &#x211D; denote a function over an input space <italic>&#x03C7;</italic> that maps to real-valued scalar outputs. This function can be modeled as a random draw from a GP:</p>
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="451161_eqn1.gif"/></alternatives></disp-formula>
<p>The mean function <italic>m</italic> specifies the expected output of the function given input <bold>x</bold>, and the kernel function<italic>k</italic> specifies the covariance between outputs.</p>
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="451161_eqn2.gif"/></alternatives></disp-formula>
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="451161_eqn3.gif"/></alternatives></disp-formula>
<p>We follow standard convention in assuming a prior mean of <bold>0</bold> (<xref ref-type="bibr" rid="c20">C. Rasmussen &#x0026; Williams, 2006</xref>).</p>
<p>All positive semi-definite kernels are closed under addition and multiplication, allowing us to create richly structured and interpretable kernels from well-understood base components. We use this property to construct a class of compositional kernels (<xref ref-type="bibr" rid="c4">Duvenaud et al., 2013</xref>; <xref ref-type="bibr" rid="c14">Lloyd et al., 2014</xref>; <xref ref-type="bibr" rid="c24">Schulz, Tenenbaum, et al., 2017</xref>). To give some intuition for this approach, consider again the C02 data in Figure <bold>??</bold>. This function is naturally decomposed into a sum of linearly increasing component and a seasonally periodic component. The compositional kernel captures this structure by summing a linear and periodic kernel.</p>
<p>Compositional GPs have been used to model complex time series data (<xref ref-type="bibr" rid="c4">Duvenaud et al., 2013</xref>), as well as to generate automated natural language descriptions from data (<xref ref-type="bibr" rid="c14">Lloyd et al., 2014</xref>), an approach coined the &#x201C;automated statistician&#x201D; (<xref ref-type="bibr" rid="c7">Ghahramani, 2015</xref>). Although it is frequently assumed that people will easily understand the generated description of the &#x201C;automated statistician&#x201D;, it is not known whether compositional patterns are indeed more communicable.</p>
<p>We follow the approach developed in <xref ref-type="bibr" rid="c24">Schulz, Tenenbaum, et al. (2017)</xref>, using three base kernels that define basic structural patterns: a linear kernel that can encode trends, a radial basis function kernel that can encode smooth functions, and a periodic kernel that can encode repeated patterns (see <xref rid="tbl1" ref-type="table">Tab. 1</xref>). These kernels can be combined by either multiplying or adding them together. In previous research, we found that this compositional grammar can account for participants&#x2019; behavior across a variety of experimental paradigms, including pattern completions, change detection, and working memory tasks (<xref ref-type="bibr" rid="c24">Schulz, Tenenbaum, et al., 2017</xref>). We fix the maximum number of combined kernels to be three and do not allow for repetition of kernels in order to restrict the complexity of inference (see next section).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1</label>
<caption><p>Base kernels in the compositional grammar.</p></caption>
<graphic xlink:href="451161_tbl1.tif"/>
</table-wrap>
<p>We compare the compositional model to a non-compositional GP model based on spectral mixture kernels. This model is derived from the fact that any stationary kernel can be expressed as an integral using Bochner&#x2019;s theorem. Letting <bold><italic>&#x03C4;</italic></bold> &#x003D; <bold>x</bold> &#x2212; <bold>x&#x2032;</bold> &#x2208; &#x211D;<sup><italic>P</italic></sup>, then</p>
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="451161_eqn4.gif"/></alternatives></disp-formula>
<p>If <italic>&#x03C8;</italic> has a density <italic>S</italic>(<bold>s</bold>), then <italic>S</italic> is the spectral density of <italic>k</italic>; <italic>S</italic> and <italic>k</italic> are Fourier duals (<xref ref-type="bibr" rid="c20">C. Rasmussen &#x0026; Williams, 2006</xref>). Thus, a spectral density over the kernel space fully defines the kernel. Furthermore, every stationary kernel can be expressed as a spectral density. <xref ref-type="bibr" rid="c26">Wilson and Adams (2013)</xref> showed that the spectral density can be approximated by a mixture of <italic>Q</italic> Gaussians, such that
<disp-formula id="eqn5">
<alternatives><graphic xlink:href="451161_eqn5.gif"/></alternatives></disp-formula>
where the <italic>q</italic>th component has mean vector <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="451161_inline1.gif"/></alternatives></inline-formula> and a covariance matrix <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="451161_inline2.gif"/></alternatives></inline-formula>. This model has comparable expressivity compared to the compositional model, but does not encode structure explicitly. <xref ref-type="bibr" rid="c27">Wilson, Dann, Lucas, and Xing (2015)</xref> have used this model to reverse-engineer &#x201C;human kernels&#x201D; in standard function learning tasks.</p>
</sec>
<sec id="s1b">
<title>Modeling function learning</title>
<p>We model human pattern description using a Bayesian inference over functions with a GP prior, an approach that has been successfully applied to a range of experimental data (<xref ref-type="bibr" rid="c10">Griffths, Lucas, Williams, &#x0026; Kalish, 2009</xref>; <xref ref-type="bibr" rid="c15">Lucas, Griffths, Williams, &#x0026; Kalish, 2015</xref>; <xref ref-type="bibr" rid="c28">Wu, Schulz, Speekenbrink, Nelson, &#x0026; Meder, 2017</xref>). Given an observed pattern <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="451161_inline3.gif"/></alternatives></inline-formula>, where <italic>y<sub>n</sub></italic> &#x223C; &#x1D4DD;(<italic>f</italic>(<bold>x</bold><sub><italic>n</italic></sub>), &#x03C3;<sup>2</sup>) is a draw from the latent function, the posterior predictive distribution for a new input <bold>x</bold><sub>&#x002A;</sub> is also normally distributed, where
<disp-formula id="eqn6">
<alternatives><graphic xlink:href="451161_eqn6.gif"/></alternatives></disp-formula>
<disp-formula id="eqn7">
<alternatives><graphic xlink:href="451161_eqn7.gif"/></alternatives></disp-formula>
are the mean and variance respectively. The term <bold>y</bold> &#x003D; [<italic>y</italic><sub>1</sub>, &#x2026;, <italic>y<sub>N</sub></italic>]<sup>&#x22A4;</sup>, <bold>K</bold> is the <italic>N</italic> &#x00D7; <italic>N</italic> matrix of covariances evaluated at each pair of observed inputs, and <bold>k<sub>&#x002A;</sub></bold> &#x003D; [<italic>k</italic>(<bold>x</bold><sub>1</sub>, <bold>x<sub>&#x002A;</sub></bold>), &#x2026;, <italic>k</italic>(<bold>x</bold><sub><italic>N</italic></sub>, <bold>x<sub>&#x002A;</sub></bold>)] is the covariance between each observed input and the new input <bold>x<sub>&#x002A;</sub></bold>.</p>
<p>We use a Bayesian model comparison approach to evaluate how well a particular kernel captures the data, while accounting for model complexity. Assuming a uniform prior over kernels, the posterior probability favoring a particular kernel is proportional to the marginal likelihood of the data under that model. The log marginal likelihood for a GP with hyper-parameters &#x03B8; is given by:
<disp-formula id="eqn8">
<alternatives><graphic xlink:href="451161_eqn8.gif"/></alternatives></disp-formula>
where the dependence of <italic>K</italic> on <italic>&#x03B8;</italic> is left implicit. The hyper-parameters are chosen to maximize the log-marginal likelihood, using gradient-based optimization (<xref ref-type="bibr" rid="c21">C. E. Rasmussen &#x0026; Nickisch, 2010</xref>).</p>
</sec>
<sec id="s1c">
<title>Generating patterns</title>
<p>We use the same patterns as in <xref ref-type="bibr" rid="c24">Schulz, Tenenbaum, et al. (2017)</xref>. These patterns were generated from both compositional and non-compositional (spectral mixture) kernels. The compositional patterns were sampled randomly from a compositional grammar by first randomly sampling a kernel composition and then sampling a function from that kernel, whereas the non-compositional patterns were sampled from the spectral mixture kernel, where the number of components was varied between 2 and 6 uniformly. A subset of these sampled patterns were then chosen so that compositional and non-compositional functions were matched based on their spectral entropy and wavelet distance (<xref ref-type="bibr" rid="c8">Goerg, 2013</xref>), leading to a final set of 40 patterns.</p>
</sec>
<sec id="s1d">
<title>Pattern communication game</title>
<p>Our study assessed how well different patterns can be communicated in a free form communication game (i.e., without any restrictions on participants&#x2019; description lengths or word usage). The study consisted of three parts: description, drawing, and quality rating. Participants were recruited from Amazon Mechanical Turk, and no participant was allowed to participate in more than one part. The study was approved by Harvard ethic&#x2019;s review board.</p>
<sec id="s1d1">
<title>Part 1: Eliciting descriptions</title>
<p>31 participants (6 female, mean age&#x003D;34.91, SD&#x003D;10.25) took part in the description study. Participants sequentially saw 6 different patterns, represented as graphs which they had to describe afterwards. Three of the patterns were randomly sampled from the 20 compositional patterns without replacement, and three were sampled from the non-compositional pool of patterns. The order of the presented patterns was determined at random. On every trial, participants first saw a pattern for 10 seconds, after which the pattern disappeared. The pattern was shown to them as 100 equidistant points indicating a function on a canvas (see <xref rid="fig3" ref-type="fig">Fig. 3</xref>). After the pattern disappeared, participants had to describe it using as many words as they liked. Participants were told that we would pass on their descriptions to someone else who would then have to redraw the patterns without ever having seen them.</p>
<p>Two judges independently rated the descriptions<xref ref-type="fn" rid="fn01"><sup>1</sup></xref> on a scale from 1 (bad descriptions) to 5 (great descriptions). The agreement between the two judges was suffciently high, with a inter-rater correlation of <italic>r</italic> (29) &#x003D; 0.46, <italic>t</italic> &#x003D; 2.45, <italic>p</italic> &#x003D; .02, <italic>BF</italic> &#x003D; 3.8. We then retained the descriptions with average rating higher than 3, giving 14 &#x201C;describers&#x201D; and a total pool of 31 different patterns. Sixteen of these patterns were compositional, and fifteen were non-compositional. All participants were paid &#x0024;2 for their participation.</p>
</sec>
<sec id="s1d2">
<title>Part 2: Drawing the patterns</title>
<p>We recruited 49 participants (21 females, mean age&#x003D;33.6, SD&#x003D;9.6) for the drawing part of the experiment. In this part, participants only saw the descriptions of the patterns and had to redraw them by placing dots on an empty canvas. Below the canvas, participants saw the descriptions of the patterns, which they knew had been written by a past participant. Participants were told that they could place any number of dots onto the canvas, but had to place at least 5 dots to draw a pattern before they could submit their drawings. Each participant received the 6 descriptions written by a randomly-matched participant from the description part, i.e. they were paired with one of the top 14 &#x201C;describers&#x201D; from the first part of the study. Participants were paid &#x0024;2 for their participation.</p>
</sec>
<sec id="s1d3">
<title>Part 3: Rating the quality of the drawings</title>
<p>104 participants (35 females, mean age&#x003D; 37.7, SD&#x003D;8.6) were recruited to rate the quality of participants&#x2019; performance in the previous parts. Participants were told the rules of the game the previous participants had played. They then had to rate 30 randomly sampled drawings, where the drawings were always presented right next to the original pattern. Participants did not see the descriptions that lead to the eventual drawings, but rather only had to evaluate how much the drawing resembled the original, i.e. how well they thought two participants performed in one round of the game. They did this by entering values on a slider from 0 (bad performance) to 100 (great performance). We paid participants &#x0024;1 for their participation.</p>
</sec>
</sec>
</sec>
<sec id="s2">
<title>Results</title>
<p><xref rid="fig3" ref-type="fig">Figure 3</xref> shows three examples of participants&#x2019; descriptions and drawings for both compositional and non-compositional patterns. We first assessed whether participants in the description part of the study entered longer descriptions for the compositional than the non-compositional patterns. This analysis revealed no significant difference between the two kinds of patterns (<italic>t</italic>(30) &#x003D; 0.15, <italic>p</italic> &#x003D; .88, <italic>d</italic> &#x003D; 0.03. <italic>BF</italic> &#x003D; 0.2). Next, we assessed whether participants in the drawing part of the study used more dots to redraw compositional than non-compositional patterns. This also showed no difference between the two kinds of patterns (<italic>t</italic>(49) &#x003D; 1.00, <italic>p</italic> &#x003D; .32, <italic>d</italic> &#x003D; 0.14, <italic>BF</italic> &#x003D; 0.2).</p>
<p>Although one might conclude from these analyses that the descriptions and redrawings were relatively similar across the two pattern classes, inspection of which words frequently appeared in the compositional descriptions but not the non-compositional descriptions (and vice versa; see <xref rid="fig2" ref-type="fig">Fig. 2</xref>) revealed that compositional descriptions often included more abstract words such as &#x201C;mountain&#x201D;, &#x201C;repeat&#x201D; or &#x201C;valley&#x201D;, whereas non-compositional descriptions used words such as &#x201C;start&#x201D;, &#x201C;bottom&#x201D; or &#x201C;top&#x201D;, likely describing exactly how to draw a particular shape. These qualitative differences are accompanied by quantitative effects, as we describe next.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p>(Color online.) <bold>Left</bold>: Words that were used more than twice in the compositional but not the non-compositional descriptions. <bold>Right:</bold> Words that were used more than twice in the non-compositional but not the compositional descriptions. Size represents the frequency for each word over all participants and descriptions.</p></caption>
<graphic xlink:href="451161_fig2.tif"/>
</fig>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><title>(Color online.) Examples of descriptions and drawings.</title>
<p>Figures show the 3 best (based on the quality ratings) unique drawings for both compositional (upper panel in orange) and non-compositional (lower panel in blue) patterns. The upper rows always show the original pattern, the middle rows show the descriptions, and the bottom rows show the redrawn patterns.</p></caption>
<graphic xlink:href="451161_fig3.tif"/>
</fig>
<p>We next analyzed the quality of participants&#x2019; drawings. In order to compare the two, we used polynomial smoothing splines to connect the dots. The splines were forced to go through every point on the canvas such that the original and redrawn patterns have the same length. Our results also hold even if we just use the raw points or other methods of extracting the patterns such as generalized additive models (see Supporting Information). We then calculated the absolute difference (absolute error) between the original and the redrawn patterns. This difference was larger for non-compositional than for compositional patterns (<xref rid="fig4" ref-type="fig">Fig. 4a</xref>; <italic>t</italic>(49) &#x003D; 2.43, <italic>p</italic> &#x003D; .01, <italic>d</italic> &#x003D; 0.34, <italic>BF</italic> &#x003D; 4.1), indicating that participants were more accurate at redrawing compositional patterns.</p>
<p>The absolute distance between two patterns might not be the best indicator of performance, because two patterns can look alike but still show a large absolute difference (e.g., if the redrawn pattern is smaller than the original, or if one pattern is just slightly shifted to either side). We therefore also applied a distance measure that takes into account these possible deviations by assessing the similarity of two patterns based on their differences after performing a Haar wavelet transform. The idea behind this similarity measure is to replace the original pattern by its wavelet approximation coeffcients, and then to measure similarity between these coeffcients (see Supporting Information, Montero, <xref ref-type="bibr" rid="c17">Vilar, et al., 2014</xref>). Technicalities aside, this measure is robust to scaling and shifting of the patterns. We have previously verified that it corresponds well with participants&#x2019; similarity judgments when comparing two patterns (<xref ref-type="bibr" rid="c24">Schulz, Tenenbaum, et al., 2017</xref>). Analyzing participants&#x2019; performance using this measurement (Wavelet distance) showed an even stronger advantage for compositional patterns (<xref rid="fig4" ref-type="fig">Fig. 4b</xref>; <italic>t</italic>(49) &#x003D; 3.02, <italic>p</italic> &#x003D; .004, <italic>d</italic> &#x003D; 0.43, <italic>BF</italic> &#x003D; 11.7).</p>
<p>Next, we looked at the quality ratings collected in the third part of our study. We estimated a linear-mixed effects model with random intercepts for each describer-drawer pair and each rater. Compositional patterns were rated more highly than non-compositional patterns (<xref rid="fig4" ref-type="fig">Fig. 4c</xref>; &#x03B2; &#x003D; 4.5, <italic>SE</italic> &#x003D; .75 <italic>t</italic>(2989) &#x003D; 5.9, <italic>p</italic> &#x003C;.001, <italic>BF</italic> &#x003E; 100).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><title>(Colour online) Difference between compositional and non-compositional functions.</title>
<p>Colors indicate the type of pattern. Red dots show the mean, along with the 95&#x0025; confidence interval. a: Absolute error between original and redrawn patterns. <bold>b:</bold> Wavelet distance between original and redrawn patterns. c: Rated quality shown as 100-Rating to transform it to a distance measure (i.e. lower values are better).</p></caption>
<graphic xlink:href="451161_fig4.tif"/>
</fig>
<p>We also assessed how well both models captured the diffculty of communicating the different patterns, as well as participants&#x2019; quality ratings. First, we assessed whether the likelihood of each model, when fitted to the original patterns, was predictive of how communicable that pattern was. The idea behind this analysis was that, if participants were really using one of the two models to extract and compress patterns, then how well this model can compress the patterns (as measured by the likelihood given the data) should be related to how well people can communicate it. We therefore fitted a set of multi-level regression models with the previously used error measures as the dependent variables, and the log-likelihood for each pattern as estimated by both compositional and non-compositional models as the independent variables. We also included a random intercept, as participants might vary systematically in their ability to redraw the described pattern. The resulting fixed effects regression coeffcients (<xref rid="tbl2" ref-type="table">Table 2</xref>) showed the same pattern for both error measurements: there was a significant effect for the compositional but not the non-compositional log-likelihoods. This means that patterns that were easier to compress by the compositional model were also easier to communicate for participants. This was not true for the non-compositional model.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2</label>
<caption><p>Results of multi-level regression. Columns show the standardized regression estimates for modeling the absolute error, the wavelet distance error, or participants&#x2019; quality ratings as the dependent variable. Significant effects (p &#x003C; 0.05) are flagged by asterisks. Standard errors of the coeffcients are displayed below each coeffcient in brackets.</p></caption>
<graphic xlink:href="451161_tbl2.tif"/>
</table-wrap>
<p>Finally, we applied the same regression approach, using the log-likelihood as the independent variable, to predict the quality ratings collected in the third part of the study. The idea behind this analysis is that if participants were indeed using one of the two models to evaluate the quality of the drawings, then they should evaluate the likelihood of the drawing to have been produced by the same generative process as the original drawing. Only the compositional model significantly predicted participant&#x2019;s ratings in part 3 (<xref rid="tbl2" ref-type="table">Table 2</xref> and <xref rid="fig4" ref-type="fig">Fig. 4c</xref>). This suggests that participants assessed the quality of the drawings based on how well they could be described by similar compositions as the original patterns.</p>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We investigated how people perceive and communicate patterns in a pattern communication game where one participant described a pattern and another participant used this description to redraw the pattern. Our results provide evidence that compositional patterns are more communicable, that a compositional model better captures participants&#x2019; diffculty in communicating patterns, and that participants&#x2019; quality ratings when evaluating the performance of other participants are also best captured by a compositional model. Taken together, these results suggest that there is an interface between natural language and the compositional pattern description language uncovered by our earlier work (<xref ref-type="bibr" rid="c24">Schulz, Tenenbaum, et al., 2017</xref>).</p>
<p>We are not the first to study how patterns are transmitted from one person to another. <xref ref-type="bibr" rid="c12">Kalish, Griffths, and Lewandowsky (2007)</xref> let participants learn and reproduce functional patterns in an &#x201C;iterated learning&#x201D; paradigm. In this paradigm, participants drew functions which were then passed onto the next person, who then had to redraw them, and so forth. The results of this study showed that participants converged to linear functions with a positive slope, even if they started out from linear function with a negative slope or just random dots. A key difference from our study is that <xref ref-type="bibr" rid="c12">Kalish et al. (2007)</xref> did not ask participants to generate natural language descriptions. Another difference is that in iterated learning studies, the object of interest is typically the stationary distribution, which reveals the learner&#x2019;s inductive biases (<xref ref-type="bibr" rid="c9">Griffths &#x0026; Kalish, 2007</xref>; Kirby &#x0026; <xref ref-type="bibr" rid="c13">Hurford, 2002</xref>). We have not attempted to simulate a Markov chain to convergence, so our study does not say anything about the stationary distribution. Here we ask whether particular pattern classes are more or less communicable. <xref ref-type="bibr" rid="c24">Schulz, Tenenbaum, et al. (2017)</xref> provides a systematic investigation into the nature of inductive biases in function learning, supporting the claim that these inductive biases are compositional in nature.</p>
<p>There are two important limitations of the current work, which point the way towards future research. First, we do not have a computational account of how patterns are encoded into natural language. Based on work in machine learning (<xref ref-type="bibr" rid="c14">Lloyd et al., 2014</xref>), one starting point is to assume that people first infer a structural description of the pattern, and then &#x201C;translate&#x201D; this structural description into natural language. Although the work of <xref ref-type="bibr" rid="c14">Lloyd et al. (2014)</xref> shows how to do this for the compositional GP model, the natural language descriptions are highly technical, and therefore a rather poor match for lay descriptions of patterns. As the word clouds in <xref rid="fig2" ref-type="fig">Fig. 2</xref> illustrate, people seem to make use of more metaphorical language when describing compositional functions&#x2014;a property not captured by the austere statistical descriptions of Lloyd and colleagues. What we need is a kind of pattern &#x201C;vernacular&#x201D; that maps coherently (though perhaps approximately) to the structural description.</p>
<p>The second limitation of our work is that we do not have a computational account of how descriptions are decoded into patterns for redrawing. One natural hypothesis is that this is essentially a reverse of the process described above: natural language descriptions are first translated into structural descriptions, which can then be plugged into the GP model to a generate the mean function or sample from the posterior.</p>
<p>Both of these limitations might be addressed in a data-driven way by using machine learning tools to find invertible mappings from structural descriptions to natural language. In particular, we could treat this as a form of<italic>structured output prediction</italic>, a supervised learning problem in which the inputs and outputs are both multi-dimensional. Modern structured output prediction algorithms have developed a variety of ways to exploit the structured nature of linguistic data (e.g., <xref ref-type="bibr" rid="c3">Daum&#x00E9;, Langford, &#x0026; Marcu, 2009</xref>; <xref ref-type="bibr" rid="c25">Tsochantaridis, Joachims, Hofmann, &#x0026; Altun, 2005</xref>). These algorithms have not yet been applied to human pattern description.</p>
</sec>
<sec id="s4">
<title>Conclusion</title>
<p>The idea that concepts are represented in a &#x201C;language of thought&#x201D; is pervasive in cognitive science (<xref ref-type="bibr" rid="c5">Fodor, 1975</xref>; <xref ref-type="bibr" rid="c18">Piantadosi, Tenenbaum, &#x0026; Goodman, 2016</xref>), and we have previously shown that human function learning also appears to be governed by a structured &#x201C;language&#x201D; of functions (<xref ref-type="bibr" rid="c6">Gershman, Malmaud, &#x0026; Tenenbaum, 2017</xref>; <xref ref-type="bibr" rid="c24">Schulz, Tenenbaum, et al., 2017</xref>). Specifically, people decompose complex patterns into compositions of simpler ones, ultimately producing a structural description of patterns that allows them to effectively perform a variety of tasks, such as extrapolation, interpolation, compression, and decision making. The results in this paper suggest that the availability of a structural description can also be used to communicate patterns in natural language. Because non-compositional functions are less effectively encoded into a structural description, they are disadvantaged in terms of accurate pattern communication. This finding provides new insight into how a language of thought might mediate translation between vision, language, and action.</p>
<sec id="s4a">
<title>Supporting Information</title>
<sec id="s4a1">
<title>Data, descriptions and analysis code</title>
<p>All data, analysis script and experimental code can be found online at: <ext-link ext-link-type="uri" xlink:href="https://github.com/panchoqv/function_communication">https://github.com/panchoqv/function_communication</ext-link></p>
<p>All descriptions, originals and redrawn patterns can be found online at: <ext-link ext-link-type="uri" xlink:href="https://ericschulz.github.io/comcomppats.pdf">https://ericschulz.github.io/comcomppats.pdf</ext-link></p>
</sec>
<sec id="s4a2">
<title>Statistical tests</title>
<p>We report all statistics using both frequentist and Bayesian tests. Frequentist tests are presented alongside their effect sizes, i.e. Cohen&#x2019;s d (<xref ref-type="bibr" rid="c2">Cohen&#x2019;s d; Cohen, 1988</xref>). Bayesian statistics are expressed as Bayes factors (BFs). A Bayes factor quantifies the likelihood of the data under the alternative hypothesis <italic>H<sub>A</sub></italic> compared to the likelihood of the data under the null hypothesis <italic>H</italic><sub>0</sub>. For example, a <italic>BF</italic> of 10 indicates that the data are 10 times more likely under <italic>H<sub>A</sub></italic> than under <italic>H</italic><sub>0</sub>; a <italic>BF</italic> of 0.1 indicates that the data are 10 times more likely under <italic>H</italic><sub>0</sub> than under <italic>H<sub>A</sub></italic>. We use the &#x201C;default&#x201D; Bayesian <italic>t</italic>-test as proposed by <xref ref-type="bibr" rid="c22">Rouder and Morey (2012)</xref> for comparing independent groups, using a Jeffreys-Zellner-Siow prior with its scale set to <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="451161_inline4.gif"/></alternatives></inline-formula>. The Bayes factor for the correlation between the judges&#x2019; ratings is based on Jeffrey&#x2019;s test for linear correlation as put forward by <xref ref-type="bibr" rid="c16">Ly, Verhagen, and Wagenmakers (2016)</xref>.</p>
</sec>
<sec id="s4a3">
<title>Wavelet transform similarity measure</title>
<p>The discrete wavelet Haar transform performs a scale-wise decomposition of a pattern in such a way that most of the energy of the data can be represented by a few coeffcients. The main idea behind this measure is to replace the original series by its wavelet approximation coeffcients <bold>a</bold>, and then to measure the dissimilarity between the wavelet approximations. We use the R-package TSclust (<xref ref-type="bibr" rid="c17">Montero et al., 2014</xref>) to find the appropriate scale of the transform. We then measured the dissimilarity between two patterns <bold>x</bold><sub>1</sub> and <bold>x</bold><sub>2</sub> by the Euclidean distance at the selected scale: <italic>d</italic>(<bold>x</bold><sub>1</sub>, <bold>x</bold><sub>2</sub>) &#x003D; ||<bold>a</bold><sub>1</sub>&#x2212;<bold>a</bold><sub>2</sub>||.</p>
</sec>
<sec id="s4a4">
<title>Assessing other distance measures</title>
<p>We also compared compositional and non-compositional patterns using two other distance measure. The first one is the absolute distance of the actual points participants put onto the canvas and the closest points (on the x-axis) of the true patterns. This measure led to a smaller error for compositional than for non-compositional patterns (<italic>t</italic>(49) &#x003D; 3.38, <italic>p</italic> &#x003D; .001, <italic>d</italic> &#x003D; 0.48, <italic>BF</italic> &#x003D; 20.9). The second one is the absolute distance between two generalized additive models (<xref ref-type="bibr" rid="c11">Hastie, 2017</xref>), one fitted to participants&#x2019; drawings and one to the true underlying pattern. In contrast to the smoothing lines used in the main text, this regression was not forced to go through every point, but rather to be a more compact representation of the drawn patterns. Using this distance measure, we found the same result as before, with a smaller error for compositional than non-compositional patterns (<italic>t</italic>(49) &#x003D; 2.72, <italic>p</italic> &#x003D; .009, <italic>d</italic> &#x003D; 0.38, <italic>BF</italic> &#x003D; 4.1). We therefore conclude that compositional patterns are more communicable than non-compositional patterns, independent of the distance measure.</p>
</sec>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>The authors thank Matthias Hofer for fruitful discussions. ES received funding from the Harvard Data Science Initiative.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="book"><string-name><surname>Box</surname>, <given-names>G. E.</given-names></string-name>, <string-name><surname>Jenkins</surname>, <given-names>G. M.</given-names></string-name>, <string-name><surname>Reinsel</surname>, <given-names>G. C.</given-names></string-name>, &#x0026; <string-name><surname>Ljung</surname>, <given-names>G. M.</given-names></string-name> (<year>2015</year>). <chapter-title>Time series analysis</chapter-title>: <source>forecasting and control</source>. <publisher-name>John Wiley &#x0026; Sons</publisher-name>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="other"><string-name><surname>Cohen</surname>, <given-names>J.</given-names></string-name> (<year>1988</year>). <article-title>Statistical power analysis for the behavioral sciences</article-title>. <source>2nd. Hillsdale, NJ: erlbaum</source>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Daum&#x00E9;</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Langford</surname>, <given-names>J.</given-names></string-name>, &#x0026; <string-name><surname>Marcu</surname>, <given-names>D.</given-names></string-name> (<year>2009</year>). <article-title>Search-based structured prediction</article-title>. <source>Machine Learning</source>, <volume>75</volume>, <fpage>297</fpage>&#x2013;<lpage>325</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="other"><string-name><surname>Duvenaud</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Lloyd</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Grosse</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name>, &#x0026; <string-name><surname>Ghahramani</surname>, <given-names>Z.</given-names></string-name> (<year>2013</year>). <article-title>Structure discovery in nonparametric regression through compositional kernel search</article-title>. <source>Proceedings of the 30th International Conference on Machine Learning</source>, <fpage>1166</fpage>&#x2013;<lpage>1174</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="book"><string-name><surname>Fodor</surname>, <given-names>J. A.</given-names></string-name> (<year>1975</year>). <source>The language of thought</source> (Vol. <volume>5</volume>). <publisher-name>Harvard University Press</publisher-name>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name>, <string-name><surname>Malmaud</surname>, <given-names>J.</given-names></string-name>, &#x0026; <string-name><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name> (<year>2017</year>). <article-title>Structured representations of utility in combinatorial domains</article-title>. <source>Decision</source>, <volume>4</volume>, <fpage>67</fpage>&#x2013;<lpage>86</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Ghahramani</surname>, <given-names>Z.</given-names></string-name> (<year>2015</year>). <article-title>Probabilistic machine learning and artificial intelligence</article-title>. <source>Nature</source>, <volume>521</volume> (<issue>7553</issue>), <fpage>452</fpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="other"><string-name><surname>Goerg</surname>, <given-names>G.</given-names></string-name> (<year>2013</year>). <article-title>Forecastable component analysis</article-title>. <source>In Proceedings of the 30th International Conference on Machine Learning (ICML-13)</source> (pp. <fpage>64</fpage>&#x2013;<lpage>72</lpage>).</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Griffths</surname>, <given-names>T. L.</given-names></string-name>, &#x0026; <string-name><surname>Kalish</surname>, <given-names>M. L.</given-names></string-name> (<year>2007</year>). <article-title>Language evolution by iterated learning with Bayesian agents</article-title>. <source>Cognitive Science</source>, <volume>31</volume>, <fpage>441</fpage>&#x2013;<lpage>480</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="other"><string-name><surname>Griffths</surname>, <given-names>T. L.</given-names></string-name>, <string-name><surname>Lucas</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>J.</given-names></string-name>, &#x0026; <string-name><surname>Kalish</surname>, <given-names>M. L.</given-names></string-name> (<year>2009</year>). <article-title>Modeling human function learning with gaussian processes</article-title>. <source>In Advances in Neural Information Processing Systems</source> (pp. <fpage>553</fpage>&#x2013;<lpage>560</lpage>).</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="book"><string-name><surname>Hastie</surname>, <given-names>T. J.</given-names></string-name> (<year>2017</year>). <chapter-title>Generalized additive models</chapter-title>. <source>In Statistical models in s</source> (pp. <fpage>249</fpage>&#x2013;<lpage>307</lpage>). <publisher-name>Routledge</publisher-name>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Kalish</surname>, <given-names>M. L.</given-names></string-name>, <string-name><surname>Griffths</surname>, <given-names>T. L.</given-names></string-name>, &#x0026; <string-name><surname>Lewandowsky</surname>, <given-names>S.</given-names></string-name> (<year>2007</year>). <article-title>Iterated learning: Intergenerational knowledge transmission reveals inductive biases</article-title>. <source>Psychonomic Bulletin &#x0026; Review</source>, <volume>14</volume>, <fpage>288</fpage>&#x2013;<lpage>294</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="book"><string-name><surname>Kirby</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Hurford</surname>, <given-names>J. R.</given-names></string-name> (<year>2002</year>). <chapter-title>The emergence of linguistic structure: An overview of the iterated learning model</chapter-title>. <source>In Simulating the evolution of language</source> (pp. <fpage>121</fpage>&#x2013;<lpage>147</lpage>). <publisher-name>Springer</publisher-name>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="other"><string-name><surname>Lloyd</surname>, <given-names>J. R.</given-names></string-name>, <string-name><surname>Duvenaud</surname>, <given-names>D. K.</given-names></string-name>, <string-name><surname>Grosse</surname>, <given-names>R. B.</given-names></string-name>, <string-name><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name>, &#x0026; <string-name><surname>Ghahramani</surname>, <given-names>Z.</given-names></string-name> (<year>2014</year>). <article-title>Automatic construction and natural-language description of nonparametric regression models</article-title>. <source>In Aaai</source> (pp. <fpage>1242</fpage>&#x2013;<lpage>1250</lpage>).</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Lucas</surname>, <given-names>C. G.</given-names></string-name>, <string-name><surname>Griffths</surname>, <given-names>T. L.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>J. J.</given-names></string-name>, &#x0026; <string-name><surname>Kalish</surname>, <given-names>M. L.</given-names></string-name> (<year>2015</year>). <article-title>A rational model of function learning</article-title>. <source>Psychonomic Bulletin &#x0026; Review</source>, <volume>22</volume> (<issue>5</issue>), <fpage>1193</fpage>&#x2013;<lpage>1215</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Ly</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Verhagen</surname>, <given-names>J.</given-names></string-name>, &#x0026; <string-name><surname>Wagenmakers</surname>, <given-names>E.-J.</given-names></string-name> (<year>2016</year>). <article-title>Harold Jeffreys&#x2019;s default Bayes factor hypothesis tests: Explanation, extension, and application in psychology</article-title>. <source>Journal of Mathematical Psychology</source>, <volume>72</volume>, <fpage>19</fpage>&#x2013;<lpage>32</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="other"><string-name><surname>Montero</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Vilar</surname>, <given-names>J. A.</given-names></string-name>, <etal>et al.</etal> (<year>2014</year>). <article-title>Tsclust: An r package for time series clustering</article-title>. <source>Journal of Statistical Software</source>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="other"><string-name><surname>Piantadosi</surname>, <given-names>S. T.</given-names></string-name>, <string-name><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name>, &#x0026; <string-name><surname>Goodman</surname>, <given-names>N. D.</given-names></string-name> (<year>2016</year>). <source>The logical primitives of thought: Empirical foundations for compositional cognitive models</source>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="other"><string-name><surname>Quiroga</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Schulz</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Speekenbrink</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Harvey</surname>, <given-names>N.</given-names></string-name> (<year>2018</year>). <article-title>Structured priors in human forecasting</article-title>. <source>bioRxiv</source>. doi: <pub-id pub-id-type="doi">10.1101/285668</pub-id></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="book"><string-name><surname>Rasmussen</surname>, <given-names>C.</given-names></string-name>, &#x0026; <string-name><surname>Williams</surname>, <given-names>C.</given-names></string-name> (<year>2006</year>). <source>Gaussian processes for machine learning</source>. <publisher-name>MIT Press</publisher-name>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Rasmussen</surname>, <given-names>C. E.</given-names></string-name>, &#x0026; <string-name><surname>Nickisch</surname>, <given-names>H.</given-names></string-name> (<year>2010</year>). <article-title>Gaussian processes for machine learning (gpml) toolbox</article-title>. <source>Journal of machine learning research</source>, <volume>11</volume> (Nov), <fpage>3011</fpage>&#x2013;<lpage>3015</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Rouder</surname>, <given-names>J. N.</given-names></string-name>, &#x0026; <string-name><surname>Morey</surname>, <given-names>R. D.</given-names></string-name> (<year>2012</year>). <source>Default Bayes Factors for Model Selection in Regression</source>., <volume>47</volume>, <fpage>877</fpage>&#x2013;<lpage>903</lpage>. doi: <pub-id pub-id-type="doi">10.1080/00273171.2012.734737</pub-id></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="other"><string-name><surname>Schulz</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Speekenbrink</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Krause</surname>, <given-names>A.</given-names></string-name> (<year>2017</year>). <article-title>A tutorial on gaussian process regression: Modelling, exploring, and exploiting functions</article-title>. <source>bioRxiv. Retrieved from</source> <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2017/10/10/095190">https://www.biorxiv.org/content/early/2017/10/10/095190</ext-link> doi: <pub-id pub-id-type="doi">10.1101/095190</pub-id></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Schulz</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Tenenbaum</surname>, <given-names>J. B.</given-names></string-name>, <string-name><surname>Duvenaud</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Speekenbrink</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Gershman</surname>, <given-names>S. J.</given-names></string-name> (<year>2017</year>). <article-title>Compositional inductive biases in function learning</article-title>. <source>Cognitive psychology</source>, <volume>99</volume>, <fpage>44</fpage>&#x2013;<lpage>79</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Tsochantaridis</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Joachims</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Hofmann</surname>, <given-names>T.</given-names></string-name>, &#x0026; <string-name><surname>Altun</surname>, <given-names>Y.</given-names></string-name> (<year>2005</year>). <article-title>Large margin methods for structured and interdependent output variables</article-title>. <source>Journal of Machine Learning Research</source>, <volume>6</volume> (Sep), <fpage>1453</fpage>&#x2013;<lpage>1484</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="other"><string-name><surname>Wilson</surname>, <given-names>A.</given-names></string-name> G., &#x0026; <string-name><surname>Adams</surname>, <given-names>R. P.</given-names></string-name> (<year>2013</year>). <article-title>Gaussian process kernels for pattern discovery and extrapolation</article-title>. <source>arXiv preprint arXiv:1302.4245</source>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="other"><string-name><surname>Wilson</surname>, <given-names>A. G.</given-names></string-name>, <string-name><surname>Dann</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Lucas</surname>, <given-names>C.</given-names></string-name>, &#x0026; <string-name><surname>Xing</surname>, <given-names>E. P.</given-names></string-name> (<year>2015</year>). <source>The human kernel. In Advances in Neural Information Processing Systems</source> (pp. <fpage>2836</fpage>&#x2013;<lpage>2844</lpage>).</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="other"><string-name><surname>Wu</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Schulz</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Speekenbrink</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nelson</surname>, <given-names>J. D.</given-names></string-name>, &#x0026; <string-name><surname>Meder</surname>, <given-names>B.</given-names></string-name> (<year>2017</year>). <article-title>Exploration and generalization in vast spaces</article-title>. <source>bioRxiv</source>, <fpage>171371</fpage>.</mixed-citation></ref>
</ref-list>
<fn-group content-type="footnotes">
<fn id="fn01"><label>1</label><p>All descriptions can be found online: <ext-link ext-link-type="uri" xlink:href="https://ericschulz.github.io/comcompresps.pdf">https://ericschulz.github.io/comcompresps.pdf</ext-link></p></fn>
</fn-group>
</back>
</article>
