<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/024406</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Signal Variability Reduction and Prior Expectation Generation through Wiring Plasticity</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Hiratani</surname>
<given-names>Naoki</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Fukai</surname>
<given-names>Tomoki</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Complexity Science and Engineering</institution>, The University of Tokyo, Kashiwa, Chiba, <country>Japan</country></aff>
<aff id="a2"><label>2</label><institution>Laboratory for Neural Circuit Theory</institution>, RIKEN Brain Science Institute, Wako, Saitama, <country>Japan</country></aff>
</contrib-group>
<author-notes>
<fn id="n1"><label>&#x002A;</label><p>E-mail: <email>N.Hiratani@gmail.com</email></p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2015</year>
</pub-date>
<elocation-id>024406</elocation-id>
<history>
<date date-type="received">
<day>11</day>
<month>8</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>11</day>
<month>8</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2015, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2015</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="024406.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>In the adult mammalian cortex, a small fraction of spines are created and eliminated every day, and the resultant synaptic connection structure is highly non-random, even in local circuits. However, it remains unknown whether a particular synaptic connection structure is functionally advantageous in local circuits, and why creation and elimination of synaptic connections is necessary in addition to rich synaptic weight plasticity. To answer these questions, we studied an inference task model through theoretical and numerical analyses. We show that a connection structure helps synaptic weight learning when it provides prior expectations. We further demonstrate that an adequate network structure naturally emerges from dual Hebbian learning for both synaptic weight plasticity and wiring plasticity. Especially in a sparsely connected network, wiring plasticity achieves reliable computation by enabling efficient information transmission. Correlations between spine dynamics and task performance generated by the proposed rule are consistent with experimental observations.</p>
</abstract>
<counts>
<page-count count="32"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Author Summary</title>
<p>A virtue of the brain that is missing from artificial machines is its ability to reorganize and improve the circuit structure. Neural circuits should be adequately tuned to perform information processing such as decoding of sensory signal from noisy sensory inputs, or motor command generation from stochastic premotor inputs. Activity-dependent modifications of synaptic efficiency through long-term potentiation and depression are considered to play a major role in this tuning, but rewiring through creation and elimination of synaptic connections is also active even in the cortex of adult mammalian. It is still unknown what neural circuits learn to represent through the changes in synaptic efficiency and connections, and how such learning is performed by local spine dynamics. In this study, we reveal the functional advantage of representation by synaptic connection structure over that by synaptic efficiency. Furthermore we derive a dual-Hebbian learning rule that governs the two forms of plasticity. The rule improves network communication and enables robust computation by capturing slow components of the environment with connection structure. Our work provides an important step towards understanding of synaptic wiring plasticity and resultant connection structure.</p>
</sec>
<sec id="s2">
<title>Introduction</title>
<p>The amplitude of excitatory and inhibitory postsynaptic potentials (EPSPs and IPSPs), often referred to as synaptic weight, is considered a fundamental variable in neural computation [<xref rid="c1" ref-type="bibr">1</xref>] [<xref rid="c2" ref-type="bibr">2</xref>]. In the mammalian cortex, excitatory synapses often show large variations in EPSP amplitudes [<xref rid="c3" ref-type="bibr">3</xref>] [<xref rid="c4" ref-type="bibr">4</xref>] [<xref rid="c5" ref-type="bibr">5</xref>], and the amplitude of a synapse can be stable over trials [<xref rid="c6" ref-type="bibr">6</xref>] and time [<xref rid="c7" ref-type="bibr">7</xref>], enabling rich information capacity compared with that at binary synapses [<xref rid="c8" ref-type="bibr">8</xref>] [<xref rid="c9" ref-type="bibr">9</xref>]. In addition, synaptic weight shows a wide variety of plasticity which depend primarily on the activity of presynaptic and postsynaptic neurons [<xref rid="c10" ref-type="bibr">10</xref>] [<xref rid="c11" ref-type="bibr">11</xref>]. Correspondingly, previous theoretical results suggest that under appropriate synaptic plasticity, a randomly connected network is computationally sufficient for various tasks [<xref rid="c12" ref-type="bibr">12</xref>] [<xref rid="c13" ref-type="bibr">13</xref>].</p>
<p>On the other hand, it is also known that synaptic wiring plasticity and the resultant synaptic connection structure are crucial for computation in the brain [<xref rid="c14" ref-type="bibr">14</xref>] [<xref rid="c15" ref-type="bibr">15</xref>]. Elimination and creation of dendritic spines are active even in the brain of adult mammalians. In rodents, the spine turnover rate is up to 15&#x0025; per day in sensory cortex [<xref rid="c16" ref-type="bibr">16</xref>] and 5&#x0025; per day in motor cortex [<xref rid="c17" ref-type="bibr">17</xref>]. Recent studies further revealed that spine dynamics are tightly correlated with the performance of motor-related tasks [<xref rid="c18" ref-type="bibr">18</xref>] [<xref rid="c19" ref-type="bibr">19</xref>]. Previous modeling studies suggested that wiring plasticity helps memory storage [<xref rid="c20" ref-type="bibr">20</xref>] [<xref rid="c21" ref-type="bibr">21</xref>] [<xref rid="c22" ref-type="bibr">22</xref>]. However, in those studies, EPSP amplitude was assumed to be a binary variable, and wiring plasticity was performed in a heuristic manner. Thus it remains unknown what should be encoded by synaptic connection structure when synaptic weights have a rich capacity for representation, and how such a connection structure can be achieved through a local spine elimination and creation mechanism.</p>
<p>To answer these questions, we constructed a theoretical model of an inference task. We found that the computational benefit of a connection structure depends on the sparseness of connectivity. In particular, when a connection is sparse, the connection structure improves performance compared with that of a randomly connected network by reducing signal variability. Based on these insights, we proposed a local unsupervised rule for wiring and synaptic weight plasticity. In the rule, connection structure and synaptic weight learn different components under a dynamic environment, enabling robust computation. The model also replicates various experimental results on spine dynamics.</p>
</sec>
<sec id="s3">
<title>Results</title>
<sec id="s3a">
<title>Connection structure helps computation in sparsely connected networks</title>
<p>What should be represented by synaptic connections and their weights, and how are those representations acquired? To explore the answers to these questions, we studied a hidden variable estimation task (<bold><xref rid="fig1" ref-type="fig">Fig 1A</xref></bold>), which appears in various stages of neural information processing [<xref rid="c23" ref-type="bibr">23</xref>] [<xref rid="c24" ref-type="bibr">24</xref>] [<xref rid="c25" ref-type="bibr">25</xref>]. In the task, at every time <italic>t</italic>, one hidden state is sampled with equal probability from <italic>p</italic> number of external states <italic>s</italic><sup><italic>t</italic></sup> = &#x007B;0, 1<italic>, &#x2026;, p -</italic> 1&#x007D;. Neurons in the input layer show independent stochastic responses <inline-formula><alternatives><inline-graphic xlink:href="024406_inline1.gif"/></alternatives></inline-formula> due to various noises (<bold><xref rid="fig1" ref-type="fig">Fig 1B</xref></bold> middle), where <italic>&#x003B8;</italic><sub><italic>j&#x000B5;</italic></sub> is the average firing rate of neuron <italic>j</italic> to the stimulus <italic>&#x000B5;</italic>, and <italic>&#x003C3;</italic><sub><italic>x</italic></sub> is the constant noise amplitude. Although, we used Gaussian noise for analytical purposes, the following argument is applicable for any stochastic response that follows a general exponential family, including Poisson firing (<bold><xref rid="figS1" ref-type="fig">S1 Fig</xref></bold>). Neurons in the output layer estimate the hidden variable from input neuron activity and represent the variable with population firing. This task is computationally difficult because most input neurons have mixed selectivity for several hidden inputs, and the responses of the input neurons are highly stochastic (<bold><xref rid="fig1" ref-type="fig">Fig 1C</xref></bold>). Let us assume that the dynamics of output neurons are written as follows:
<disp-formula id="eqn1">
<alternatives>
<graphic xlink:href="024406_eqn1.gif"/>
</alternatives>
</disp-formula>
where <italic>c</italic><sub><italic>ij</italic></sub> (= 0 or 1) represents connectivity from input neuron <italic>j</italic> to output neuron <italic>i</italic>, <italic>w</italic><sub><italic>ij</italic></sub> is its synaptic weight (EPSP size), and <italic>h</italic><sub><italic>w</italic></sub> is the threshold. <italic>M</italic> and <italic>N</italic> are population sizes of the input and output layers, respectively. In the model, all feedforward connections are excitatory, and the inhibitory input is provided as the global inhibition <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline2.gif"/></alternatives></inline-formula></italic>.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Description of the model. (<bold>A</bold>) Schematic diagram of the model. (<bold>B</bold>) An example of model behavior calculated at <italic>&#x003C1;</italic> = 0.16, when the synaptic connection is organized using the weight-coding scheme. The top panel represents the external variable, which takes an integer 0 to 9 in the simulation. The middle panel is the response of input neurons, and the bottom panel shows the activity of output neurons. In the simulation, each external state was randomly presented, but here the trials are sorted in ascending order. (<bold>C</bold>) Examples of neural activity in a simulation. Graphs on the top row represent the average firing rates of five randomly sampled input neurons for given external states (black lines) and their standard deviation (gray shadows). The bottom graphs are subthreshold responses of output neurons that represent the external state <italic>s</italic> = 1. Because the boundary condition for the membrane parameter <inline-formula><alternatives><inline-graphic xlink:href="024406_inline4.gif"/></alternatives></inline-formula> is introduced as <italic>v</italic><sub><italic>i</italic></sub> <italic>&#x0003E;</italic> max<sub><italic>l</italic></sub> &#x007B;<italic>v</italic><sub><italic>l</italic></sub> <italic>- v</italic><sub><italic>d</italic></sub>&#x007D;, <italic>v</italic><sub><italic>i</italic></sub> is typically bounded at <italic>-v</italic><sub><italic>d</italic></sub>. Note that <italic>v</italic><sub><italic>i</italic></sub> is the unnormalized log-likelihood, and the units on the y-axis are arbitrary.</p></caption>
<graphic xlink:href="024406_fig1.tif"/>
</fig>
<p>If the feedforward connection is all-to-all (i.e., <italic>c</italic><sub><italic>ij</italic></sub> = 1 for all <italic>i, j</italic> pairs), by setting the weights as <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline3.gif"/></alternatives></inline-formula></italic> for output neuron <italic>i</italic> that represents external state <italic>&#x000B5;</italic>, the network gives an optimal inference from the given firing rate vector <inline-formula><alternatives><inline-graphic xlink:href="024406_inline5.gif"/></alternatives></inline-formula>, where the value <italic>q</italic><sub><italic>j&#x000B5;</italic></sub> represents how much evidence the firing rate of neuron <italic>j</italic> provides for a particular external state <italic>&#x000B5;</italic> (for details, see <bold>Materials and methods</bold>). However, if the connectivity between the two layers is sparse, as it is in most regions of the brain, optimal inference is generally unattainable because each output neuron can obtain a limited set of information from the input layer. How should one choose connection structure and synaptic weights in such a case? We first considered two extreme examples for illustration purposes. One strategy is to use synaptic weight for approximating the optimal representation while keeping the connection random with a fixed connection probability (weight coding). In this case, <italic>c</italic> and <italic>w</italic> are given with Pr[<italic>c</italic><sub><italic>ij</italic></sub> = 1] = <italic>&#x003C1;</italic> and <italic>w</italic><sub><italic>ij</italic></sub> = <italic>w</italic><sub><italic>&#x000B5;j</italic></sub> = <italic>q</italic><sub><italic>j&#x000B5;</italic></sub><italic>/&#x003C1;</italic>, where the mean connectivity is given as <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline6.gif"/></alternatives></inline-formula></italic>, and <inline-formula><alternatives><inline-graphic xlink:href="024406_inline7.gif"/></alternatives></inline-formula> is the average of the normalized mean response <italic>q</italic><sub><italic>j&#x03BC;</italic></sub>/&#x03C1; <italic>(i.e., <inline-formula><alternatives><inline-graphic xlink:href="024406_inline8.gif"/></alternatives></inline-formula></italic>). Parameter <italic>&#x003B3;</italic> is introduced to control the sparseness of connections, and here we assume that neuron <italic>i</italic> represents the external state <inline-formula><alternatives><inline-graphic xlink:href="024406_inline9.gif"/></alternatives></inline-formula>, output neuron <italic>i</italic> represents the state <italic>&#x000B5;</italic>). The other strategy is to use synaptic connectivity for the representation while fixing the synaptic weight (connectivity coding). In this case, the model is given by Pr[<italic>c</italic><sub><italic>ij</italic></sub> = 1] = <italic>&#x003C1;</italic><sub><italic>&#x000B5;j</italic></sub> and <italic>w</italic><sub><italic>ij</italic></sub> = <italic>w</italic><sub><italic>&#x000B5;j</italic></sub> = 1<italic>/&#x003B3;</italic>, where <italic>&#x003C1;</italic><sub><italic>&#x000B5;j</italic></sub> = min(<italic>&#x003B3;q</italic><sub><italic>j&#x000B5;</italic></sub>, 1). If we sort input neurons with their preferred external states, the diagonal components of the connection matrix show high synaptic weights in the weight-coding scheme, whereas the diagonal components show dense connection in the connection-coding scheme (<bold><xref rid="fig2" ref-type="fig">Fig 2A</xref></bold>). Note that neither of the realizations is strictly the optimal solution under each constraint. However, as we discuss later, both of them are obtainable through biologically plausible local Hebbian learning rules.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Connection structure helps computation in sparsely connected networks. (<bold>A</bold>) Examples of synaptic weight matrices in weight-coding (W-coding) and connectivity-coding (C-coding) schemes. X-neurons were sorted by their selectivity for external states. (<bold>B</bold>) Comparison of the performance between connectivity-coding and weight-coding schemes at various sparseness of connectivity. Orange and cyan lines are simulation results. The error bars represent standard deviation over 10 independent simulations. In the following panels, error bars are trial variability over 10 simulations. Red and blue lines are analytical results. (<bold>C</bold>) Analytically evaluated coefficient of variation (CV) of output firing rate and corresponding simulation results. For simulation results, the variance was evaluated over whole output neurons from their firing rates for their selective external states. (<bold>D</bold>) Estimated maximum transfer entropy for two coding strategies. Black horizontal line is the maximal information <italic>log</italic><sub><italic>e</italic></sub><italic>p</italic>. (<bold>E</bold>) Relationships between the performance and the degree of weight coding (<italic>&#x03BA;</italic><sub><italic>w</italic></sub>) and connection coding (<italic>&#x03BA;</italic><sub><italic>c</italic></sub>). The upper left corner represents the performance of the connection-coding scheme (<italic>&#x03BA;</italic><sub><italic>c</italic></sub> = 1<italic>, &#x03BA;</italic><sub><italic>w</italic></sub> = 0), and the lower right corner corresponds to that of the weight-coding scheme (<italic>&#x03BA;</italic><sub><italic>c</italic></sub> = 0<italic>, &#x03BA;</italic><sub><italic>w</italic></sub> = 1). (<bold>F</bold>) Estimated log-likelihood ratio between the likelihood calculated in redundant representation and the likelihood derived from optimal inference. Log-likelihood was estimated by <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline10.gif"/></alternatives></inline-formula></italic>. The graph was calculated for combined representations of weight coding and connection coding (i.e., <italic>&#x03BA;</italic> = <italic>&#x03BA;</italic><sub><italic>w</italic></sub> = <italic>&#x03BA;</italic><sub><italic>c</italic></sub>).</p></caption>
<graphic xlink:href="024406_fig2.tif"/>
</fig>
<p>So which strategy gives a better representation? We evaluated the accuracy of the external state estimation using a bootstrap method (see <bold>Materials and methods</bold>). Under intermediate connectivity, both strategies showed reasonably good performance (as in <bold><xref rid="fig1" ref-type="fig">Fig 1B</xref></bold> bottom). Intriguingly, in sparse cases, connectivity coding outperformed weight coding, despite its binary representation (<bold><xref rid="fig2" ref-type="fig">Fig 2B</xref></bold> cyan/orange lines). The analytical results confirmed this tendency (<bold><xref rid="fig2" ref-type="fig">Fig 2B</xref></bold> red/blue lines) and indicated that the firing rates of output neurons selective for the given external state show less variability in connectivity coding than in weight coding, enabling more reliable information transmission (<bold><xref rid="fig2" ref-type="fig">Fig 2C</xref></bold>). To further understand this phenomenon, we evaluated the maximum transfer entropy of the feed forward connections: <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline11.gif"/></alternatives></inline-formula></italic>. Because of limited connectivity, each output neuron obtained information only from the connected input neurons. Thus, the transfer entropy was typically lower under sparse than under dense connections in both strategies (<bold><xref rid="fig2" ref-type="fig">Fig 2D</xref></bold>). However, for the connectivity-coding scheme, each output neuron obtained information from relevant input neurons, suppressing the reduction in transfer entropy (orange line in <bold><xref rid="fig2" ref-type="fig">Fig 2D</xref></bold>). Therefore, in the given inference model, the connection structure is helpful for improving performance when the structure increases the transfer entropy of the connections.</p>
<p>In the brain, synaptic connectivity and weights often have some redundancy. For example, the EPSP size of a connection in a clustered network is typically larger than the average EPSP size [<xref rid="c6" ref-type="bibr">6</xref>] [<xref rid="c26" ref-type="bibr">26</xref>]. This positive correlation between connectivity and weight indicates redundancy in the neural representation, and a similar property is expected to hold for interlayer connections [<xref rid="c27" ref-type="bibr">27</xref>]. Thus, we next considered the function of this redundancy. To this end, we mixed weight coding and connectivity coding as Pr[<italic>c</italic><sub><italic>i</italic>,<italic>j</italic></sub> = 1] = <italic>&#x003C1;</italic> and <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline12.gif"/></alternatives></inline-formula></italic>, where <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline13.gif"/></alternatives></inline-formula></italic>, and <italic>&#x03BA;</italic><sub><italic>w</italic></sub> and <italic>&#x03BA;</italic><sub><italic>c</italic></sub> are the degrees of weight and connectivity coding, respectively (0 <italic>&#x02264; &#x03BA;</italic><sub><italic>w</italic></sub><italic>, &#x03BA;</italic><sub><italic>c</italic></sub> <italic>&#x02264;</italic> 1). Note that (<italic>&#x03BA;</italic><sub><italic>w</italic></sub><italic>, &#x03BA;</italic><sub><italic>c</italic></sub>) = (1, 0) corresponds to the weight coding, whereas (<italic>&#x03BA;</italic><sub><italic>w</italic></sub><italic>, &#x03BA;</italic><sub><italic>c</italic></sub>) = (0, 1) corresponds to connectivity coding. In these representations, the performance improved by combining the two schemes (<bold><xref rid="fig2" ref-type="fig">Fig 2E</xref></bold>), even if the representation was redundant (i.e., <italic>&#x03BA;</italic><sub><italic>w</italic></sub> &#x002B; <italic>&#x03BA;</italic><sub><italic>c</italic></sub> <italic>&#x0003E;</italic> 1.0). The log-likelihood ratio with an optimal estimation became higher under a redundant representation (i.e., <italic>&#x03BA;</italic><sub><italic>w</italic></sub> = <italic>&#x03BA;</italic><sub><italic>c</italic></sub> <italic>&#x0003E;</italic> 0.5) for both correct (<italic>s</italic><sup><italic>t</italic></sup> = <italic>&#x000B5;</italic>) and incorrect (<italic>s</italic><sup><italic>t</italic></sup> <italic>&#x2260; &#x000B5;</italic>) responses (<bold><xref rid="fig2" ref-type="fig">Fig 2F</xref></bold>; calculated for <italic>&#x03BA;</italic><sub><italic>w</italic></sub> = <italic>&#x03BA;</italic><sub><italic>c</italic></sub> = <italic>&#x03BA;</italic>) because output neurons became overconfident on its decision. Nevertheless, as the amplitude of lateral inhibition became stronger, overall redundant representation was not harmful.</p>
</sec>
<sec id="s3b">
<title>Connection structure enables rapid learning</title>
<p>In the last section, we showed that in a sparsely connected network, non-random connection structure could be beneficial for computation. But is there any benefit to having a connection structure in a dense network? The results in the previous section indicated that when connectivity was sufficiently dense (<italic>&#x003C1; &#x0003E;</italic> 0.4 in the simulation), both performance and the estimated transfer entropy saturated under an appropriate synaptic weight configuration, even if the connectivity was random. Thus, to consider the potential benefits of non-random connection structures, we next implemented synaptic weight learning in our model while fixing the connectivity. Synaptic weights should minimize KL-divergence between the true input distribution and the estimated input distribution to represent the internal model [<xref rid="c28" ref-type="bibr">28</xref>] [<xref rid="c29" ref-type="bibr">29</xref>]. Thus, by considering stochastic gradient descending, synaptic weight change <inline-formula><alternatives><inline-graphic xlink:href="024406_inline14.gif"/></alternatives></inline-formula> is given as:
<disp-formula id="eqn2">
<alternatives>
<graphic xlink:href="024406_eqn2.gif"/>
</alternatives>
</disp-formula></p>
<p>The first Hebbian term is derived from the gradient descending, and the second term is the homeostatic term heuristically added to constrain the average firing rates of output neurons [<xref rid="c30" ref-type="bibr">30</xref>] (see <bold>Materials and methods</bold>). We first performed this unsupervised synaptic weight learning on a randomly connected network. When the connectivity was moderately dense, the network successfully acquired a suitable representation (<bold><xref rid="fig3" ref-type="fig">Fig 3A</xref></bold>), and the model error (<bold>Materials and methods</bold>) eventually converged (<bold><xref rid="fig3" ref-type="fig">Fig 3B</xref></bold>). Especially under a sufficient level of homeostatic plasticity (<bold><xref rid="fig3" ref-type="fig">Fig 3C</xref></bold>), the average firing rate showed a narrow unimodal distribution (<bold><xref rid="fig3" ref-type="fig">Fig 3D</xref></bold> top), and most of the output neurons acquired selectivity for one of external states (<bold><xref rid="fig3" ref-type="fig">Fig 3D</xref></bold> bottom). However, when a part of the true model was given as the connection structure with <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline15.gif"/></alternatives></inline-formula></italic>, at larger <italic>&#x003BB;</italic>, the initial performance became higher and the convergence was faster (<bold><xref rid="fig3" ref-type="fig">Fig 3E</xref></bold>, <bold><xref rid="fig3" ref-type="fig">Fig 3F</xref></bold>; <italic>&#x003BB;</italic> = 0 corresponds to the model with random connectivity). Note that the low correlation between the external model and the connection structure (<italic>&#x003BB; &#x0223C;</italic> 0.4) was sufficient to observe this effect. This result suggests that an adequate connection structure can induce fast learning if the structure is correlated with the external model.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Synaptic weight learning on random or non-random connection structures. (<bold>A</bold>) An example of output neuron activity before (top) and after (bottom) synaptic weight learning at connectivity <italic>&#x003C1;</italic> = 0.4. (<bold>B</bold>) Model error decreases with synaptic weight learning regardless of connectivity. (<bold>C</bold>) Selectivity and accuracy of estimation at various strengths of homeostatic plasticity at <italic>&#x003C1;</italic> = 0.4. (<bold>D</bold>) Histogram of average firing rates of output neurons (top), and selectivity of each neuron. Selectivity was defined as for in the simulation depicted in <bold>A</bold>. (<bold>E</bold>) Relationship between learning curve and connection structure at connectivity <italic>&#x003C1;</italic> = 0.4 and the strength of homeostatic plasticity <italic>b</italic><sub><italic>h</italic></sub> = 1.0. The parameter <italic>&#x003BB;</italic> represents the similarity between the connection structure and the external model. (<bold>F</bold>) Model error calculated from synaptic weights for the simulation depicted in <bold>E</bold>.</p></caption>
<graphic xlink:href="024406_fig3.tif"/>
</fig>
</sec>
<sec id="s3c">
<title>Dual Hebbian learning rule enables efficient information transmission</title>
<p>So far, we have revealed that in both sparse and dense networks, non-random connection structures can be beneficial for computation or at least for learning. However, in the previous sections, a specific connection structure was given a priori, although structures in local neural circuits are expected to be obtained with wiring plasticity through the elimination and creation of spines. Thus, we next investigated the underlying rewiring rules that can induce beneficial connection structures. To this end, for each combination (<italic>i, j</italic>) of presynaptic neuron <italic>j</italic> and postsynaptic neuron <italic>i</italic>, we introduced a variable <italic>&#x003C1;</italic><sub><italic>ij</italic></sub>, which represents the connection probability. The biological correspondence of this variable is discussed below. If we randomly create a synaptic connection between neuron (<italic>i, j</italic>) with probability <italic>&#x003C1;</italic><sub><italic>ij</italic></sub><italic>/&#x003C4;</italic><sub><italic>c</italic></sub> and eliminate it with probability (1 &#x2212; <italic>&#x003C1;</italic><sub><italic>ij</italic></sub>)<italic>/&#x003C4;</italic><sub><italic>c</italic></sub>, on average there is a connection between neuron (<italic>i, j</italic>) with probability <italic>&#x003C1;</italic><sub><italic>ij</italic></sub>, when the maximum number of synaptic connections is bounded by 1. This provides a wiring plasticity rule for a given <italic>&#x003C1;</italic><sub><italic>ij</italic></sub>, but how should we choose <italic>&#x003C1;</italic><sub><italic>ij</italic></sub>? Because synaptic connection structure should be correlated with the external model, by considering stochastic gradient descendent by <italic>&#x003C1;</italic><sub><italic>ij</italic></sub> on KL-divergence between the true input firing rate distributions and the estimated distribution, the learning rule of <italic>&#x003C1;</italic> is given as
<disp-formula id="eqn3">
<alternatives>
<graphic xlink:href="024406_eqn3.gif"/>
</alternatives>
</disp-formula></p>
<p>Remarkably, although this rule does not maximize the transfer entropy of the connections, the directions of stochastic gradients of two objective functions are on average close to one another; therefore, the above stated rule does not reduce the transfer entropy of the connection on average (see <bold>Materials and methods</bold>). <bold><xref rid="fig4" ref-type="fig">Fig 4A</xref></bold> shows the typical behavior of <italic>&#x003C1;</italic><sub><italic>ij</italic></sub> and <italic>w</italic><sub><italic>ij</italic></sub> under this dual Hebbian rule defined by equations (2) and (3). When the connection probability is low, a connection between two neurons is rare, and, even when a spine is created due to probabilistic creation, the spine is rapidly eliminated. In the moderate connection probability, spine creation is more frequent, and the created spine survives longer. When the connection probability is high enough, a connection is nearly always formed, and the synaptic weight of the connection is large because synaptic weight dynamics also follow a similar Hebbian rule.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Dual Hebbian learning for synaptic weights and connections. (<bold>A</bold>) Examples of spine creation and elimination. In all three panels, green lines show synaptic weights, and blue lines are connection probability. When there is not a synaptic connection between two neurons, the synaptic weight becomes zero, but the connection probability can take a non-zero value. Simulation was calculated at <italic>&#x003C1;</italic> = 0.48, <italic>&#x003B7;</italic><sub><italic>p</italic></sub> = 0.001, and <italic>&#x003C4;</italic><sub><italic>c</italic></sub> = 105. (<bold>B</bold>) Change in connectivity due to synaptic elimination and creation. Number of spines eliminated (red) and created (green) per unit time was balanced (top). As a result, connectivity did not appreciably change due to rewiring (bottom). Black lines in the bottom graph are the mean connectivity at <italic>&#x003B3;</italic> = 0.1 and <italic>&#x003B3;</italic> = 0.101 in the model without rewiring. (<bold>C,D</bold>) Accuracy of estimation (<bold>C</bold>) and the estimated maximum transfer entropy (<bold>D</bold>) for the model with/without wiring plasticity. For the dual Hebbian model, the sparseness parameter was set as <italic>&#x003B3;</italic> = 0.1, whereas &#x003B3; = 0.101 was used for the weight plasticity model to perform comparisons at the same connectivity (see B). (<bold>E</bold>) Synaptic weight matrices before (left) and after (right) learning. Both X-neurons (input neuron) and Y-neurons (output neurons) were sorted based on their preferred external states. (<bold>F</bold>) Accuracy of estimation with various timescales for rewiring <italic>&#x003C4;</italic><sub><italic>c</italic></sub>. Note that the simulation was performed only for 5 &#x000D7; 106 time steps, and the performance did not converge for the model with a longer timescale. (<bold>G,H</bold>) Comparison of the performance (<bold>G</bold>) and the maximum estimated transfer entropy (H) between the dual Hebbian model and the model implemented with synaptic plasticity only at various degrees of connectivity. Horizontal line in H represents the total information log<sub><italic>e</italic></sub> <italic>p</italic>.</p></caption>
<graphic xlink:href="024406_fig4.tif"/>
</fig>
<p>We implemented the dual Hebbian rule in our model and compared the performance of the model with that of synaptic weight plasticity on a fixed random synaptic connection. Because spine creation and elimination are naturally balanced in the proposed rule (<bold><xref rid="fig4" ref-type="fig">Fig 4B</xref></bold> top), the total number of synaptic connections was nearly unchanged throughout the learning process (<bold><xref rid="fig4" ref-type="fig">Fig 4B</xref></bold> bottom). As expected, the dual Hebbian rule yielded better performance (<bold><xref rid="fig4" ref-type="fig">Fig 4C</xref></bold>) and higher estimated transfer entropy than the corresponding weight plasticity only model (<bold><xref rid="fig4" ref-type="fig">Fig 4D</xref></bold>). This improvement was only observed when the frequency of rewiring was in an intermediate range (<bold><xref rid="fig4" ref-type="fig">Fig 4F</xref></bold>). When rewiring was too slow, the model showed essentially the same behavior as that in the weight plasticity only model, whereas excessively frequent probabilistic rewiring disturbed the connection structure. Although a direct comparison with experimental results is difficult, the optimal rewiring timescale occurred within hours to days, under the assumption that firing rate dynamics (<xref ref-type="disp-formula" rid="eqn1">equation (1)</xref>) are updated every 10-100 ms. Initially, both connectivity and weights were random (<bold><xref rid="fig4" ref-type="fig">Fig 4E</xref></bold> left), but after the learning process, the diagonal components of the weight matrix developed relatively larger synaptic weights, and, at the same time, connectivity was denser than that for the off-diagonal components (<bold><xref rid="fig4" ref-type="fig">Fig 4E</xref></bold> right). Thus, through dual Hebbian learning, a network can indeed acquire a connection structure that enables efficient information transmission between two layers; as a result, the performance increases when the connectivity is moderately sparse (<bold><xref rid="fig4" ref-type="fig">Fig 4G</xref>,<xref rid="fig4" ref-type="fig">H</xref></bold>). Although the performance was slightly worse than a fully-connected network, synaptic transmission consumes a large amount of energy [<xref rid="c31" ref-type="bibr">31</xref>], and synaptic connection is a major source of noise [<xref rid="c32" ref-type="bibr">32</xref>]; therefore, it is beneficial to achieve a similar level of performance using a network with fewer connections.</p>
</sec>
<sec id="s3d">
<title>Connection structure can acquire constant components of stimuli and enable rapid learning</title>
<p>We have shown that the dual Hebbian learning rule helps computation in a sparsely connected network. But what happens in densely connected networks? To consider this issue, we extended the previous static external model to a dynamic one, in which at every interval <italic>T</italic><sub>2</sub>, response probabilities of input neurons partly change. If we define the constant component as <italic>&#x003B8;</italic><sub>const</sub> and the variable component as <italic>&#x003B8;</italic><sub>var</sub>, then the total model becomes <inline-formula><alternatives><inline-graphic xlink:href="024406_inline16.gif"/></alternatives></inline-formula>, where the normalization term is given as <inline-formula><alternatives><inline-graphic xlink:href="024406_inline17.gif"/></alternatives></inline-formula> (<bold><xref rid="fig5" ref-type="fig">Fig 5A</xref></bold>). In this case, when the learning was performed only with synaptic weights based on fixed random connections, although the performance rapidly improved, every time a part of the model changed, the performance dropped dramatically and only gradually returned to a higher level (cyan line in <bold><xref rid="fig5" ref-type="fig">Fig 5B</xref></bold>). By contrast, under the dual Hebbian learning rule, the performance immediately after the model shift (i.e., the performance at the trough of the oscillation) gradually increased, and convergence became faster (<bold><xref rid="fig5" ref-type="fig">Fig 5B</xref>,C</bold>), although the total connectivity stayed nearly the same (<bold><xref rid="fig5" ref-type="fig">Fig 5D</xref></bold>). After learning, the synaptic connection structure showed a higher correlation with the constant component than with the variable component (<bold><xref rid="fig5" ref-type="fig">Fig 5E</xref></bold>; see <bold>Materials and methods</bold>). By contrast, at every session, synaptic weight structure learned the variable component better than it learned the constant component (<bold><xref rid="fig5" ref-type="fig">Fig 5F</xref></bold>). The timescale for synaptic rewiring needed to be long enough to be comparable with the timescale of the external variability <italic>T</italic><sub>2</sub> to capture the constant component. Otherwise, connectivity was also strongly modulated by the variable component of the external model (<bold><xref rid="fig5" ref-type="fig">Fig 5G</xref></bold>), and unable to provide the expectation. After sufficient learning, the synaptic weight <italic>w</italic> and the corresponding connection probability <italic>&#x003C1;</italic> roughly followed a linear relationship (<bold><xref rid="fig5" ref-type="fig">Fig 5H</xref></bold>). Remarkably, some synapses developed connection probability <italic>&#x003C1;</italic> = 1, meaning that these synapses were almost permanently stable because the elimination probability (1 &#x2212; <italic>&#x003C1;</italic>)<italic>/&#x003C4;</italic><sub><italic>c</italic></sub> became nearly zero.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Dual learning under a dynamic environment. (<bold>A</bold>) Examples of input neuron responses. Blue lines represent the constant components &#x003B8;const, green lines show the variable components <italic>&#x003B8;</italic><sub><italic>var</italic></sub>, and magenta lines are the total external models <italic>&#x003B8;</italic> calculated from the normalized sum. (<bold>B</bold>) Learning curves for the model with or without wiring plasticity, when the variable components change every 105 time steps. (<bold>C</bold>) Accuracy of estimation for various ratios of constant components. Early phase performance was calculated from the activity within 10,000 steps after the variable component shift, and the late phase performance was calculated from the activity within 10,000 steps before the shift. As in <bold>B</bold>, orange lines represent the dual Hebbian model, and cyan lines are for the model with weight plasticity only. (<bold>D</bold>) Trajectories of connectivity change. Connectivity tends to increase slightly during learning. Dotted lines are mean connectivity at (<italic>K</italic><sub>m</sub>, <italic>&#x003B3;</italic>) = (0.0, 0.595), (0.2, 0.625), (0.4, 0.64), (0.5, 0.64), (0.6, 0.635), and (0.8, 0.620). In <bold>C</bold>, these parameters were used for the synaptic plasticity only model, whereas <italic>&#x003B3;</italic> is fixed at = 0.6 for the dual Hebbian model. (<bold>E,F</bold>) Model error calculated from connectivity (<bold>E</bold>) and synaptic weights (<bold>F</bold>). Note that the timescale of <bold>E</bold> is the duration in which the variable component is constant, not the entire simulation. (<bold>G</bold>) Model error calculated from connectivity for various rewiring timescales <italic>&#x003C4;</italic><sub><italic>c</italic></sub>. For a large <italic>&#x003C4;</italic><sub><italic>c</italic></sub>, the learning process does not converge during the simulation. (<bold>H</bold>) Relationship between synaptic weight <italic>w</italic> and connection probability <italic>&#x003C1;</italic> at the end of learning. When the external model is stable, <italic>w</italic> and <italic>&#x003C1;</italic> have a more linear relationship than that for the variable case.</p></caption>
<graphic xlink:href="024406_fig5.tif"/>
</fig>
</sec>
<sec id="s3e">
<title>Semi-dual Hebbian learning rule explains experimentally observed spine dynamics</title>
<p>The results to this point have revealed the functional advantages of dual Hebbian learning. However, we do not yet know whether the brain really uses such a dual learning rule. Although the dual Hebbian rule appears theoretically preferable, the effects of presynaptic and postsynaptic activity on spine creation and elimination remain unclear [<xref rid="c15" ref-type="bibr">15</xref>] [<xref rid="c33" ref-type="bibr">33</xref>]. Thus we modified the rule such that spine dynamics do not directly depend on neural activities, and demonstrated that the model well replicates experimentally observed spine dynamics and the resultant animal behavior. Under the dual Hebbian rule, both synaptic weight and connection probability follow similar Hebbian-type plasticity rules (<xref ref-type="disp-formula" rid="eqn2">Equations (2)</xref> and <xref ref-type="disp-formula" rid="eqn3">(3)</xref>). Therefore, even if we assume that the change in the connection probability is given as a function of synaptic weight, the rule should still give a good approximation. Thus we defined the semi-dual Hebbian learning rule as
<disp-formula id="eqn4">
<alternatives>
<graphic xlink:href="024406_eqn4.gif"/>
</alternatives>
</disp-formula></p>
<p>The upper equation means that if there is a connection between two neurons, the change in connection probability solely depends on its synaptic weight. Previous experimental results suggest that a small spine is more likely to be eliminated [<xref rid="c7" ref-type="bibr">7</xref>] [<xref rid="c33" ref-type="bibr">33</xref>], and spine size often increases or decreases in response to LTP or LTD, respectively, with a certain delay [<xref rid="c34" ref-type="bibr">34</xref>] [<xref rid="c35" ref-type="bibr">35</xref>]. Thus we can naturally assume that the connection probability <italic>&#x003C1;</italic> is proportional to spine size. In the absence of a synaptic connection (i.e., <italic>c</italic><sub><italic>ij</italic></sub> = 0), we assume that the connection probability is fixed at a constant value <italic>&#x003B3;</italic><sup>2</sup><italic>w</italic><sub><italic>o</italic></sub>, regardless of the firing rates of presynaptic and postsynaptic neurons; thus spine creation is totally random. We first applied this rule for the task in the previous section. Although the rule performed poorly compared with the original dual Hebbian rule due to the lack of activity dependence in spine creation, the rule still outperformed the synaptic weight only model in the early phase of the model shift (<bold><xref rid="fig6" ref-type="fig">Fig 6A</xref></bold>). For a static external model, the dynamics of connection probability well mimicked the experimentally observed spine dynamics [<xref rid="c7" ref-type="bibr">7</xref>] [<xref rid="c33" ref-type="bibr">33</xref>] (<bold><xref rid="fig6" ref-type="fig">Fig 6B</xref>-E</bold>).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6.</label>
<caption><p>Spine dynamics of the semi-dual Hebbian model. (<bold>A</bold>) Comparison of performances among the model without wiring plasticity (cyan), the approximated model (purple), and the dual Hebbian model (orange). (<bold>B</bold>) Relative change of connection probability within 1 <sup>5</sup> time steps. If the original connection probability is low, the relative change after 10<sup>5</sup> time steps has a tendency to be positive, whereas spines with a high connection probability are more likely to show negative change. The black line at the bottom represents eliminated spines (i.e., relative change = -1). (<bold>C</bold>) Synaptic weight distribution (top), connection probability distribution (middle), and non-bounded connection probability distributio (bottom). Histograms were scaled by 1/(7 &#x00D7; 10<sup>5</sup>) for normalization. In the bottom panel, for connections with <italic>&#x003C1; &#x0003E;</italic> 1, non-bounded values were defined by <italic>&#x003C1;</italic><sub><italic>est</italic></sub> = <italic>w&#x003B3;</italic><sup>2</sup>. See <bold>Materials and Methods</bold> for details of the analytical evaluation. (<bold>D, E</bold>) Relationships between spine age and the mean connection probability (<bold>D</bold>) and the 5-day survival rate (<bold>E</bold>). As expected from the experimental results, survival rate is positively correlated with spine age.</p></caption>
<graphic xlink:href="024406_fig6.tif"/>
</fig>
<p>We next examined the performance of the model in motor learning tasks. Appropriate motor commands are expected to be inferred in the motor cortex based on inputs from pre-motor regions [<xref rid="c36" ref-type="bibr">36</xref>] [<xref rid="c37" ref-type="bibr">37</xref>]. In addition, the connection from layer 2/3 to layer 5 is considered a major pathway in motor learning [<xref rid="c38" ref-type="bibr">38</xref>]. Thus we hypothesized that the input and output layers of our model roughly correspond to layers 2/3 and 5 of the motor cortex. We first studied the influence of training on spine survival [<xref rid="c19" ref-type="bibr">19</xref>] (<bold><xref rid="fig7" ref-type="fig">Fig 7A</xref></bold>). Below, to compare with experimental results, we defined 10<sup>5</sup> time steps as one day, and the training and control were defined as two independent external models <italic>&#x003B8;</italic><sub>ctrl</sub> and <italic>&#x003B8;</italic><sub>train</sub>. In both training and control cases, newly created spines were less stable than pre-existing spines (solid lines vs. dotted lines in <bold><xref rid="fig7" ref-type="fig">Fig 7B</xref></bold>), because older spines tended to have larger connection probability (<bold><xref rid="fig6" ref-type="fig">Fig 6D</xref></bold>). By continuous training, pre-existing spines became less stable than those in the control case, while new spines became more stable compared with those in the control case (red lines vs. lime lines in <bold><xref rid="fig7" ref-type="fig">Fig 7B</xref></bold>). The 5-day survival rate of a spine was higher for spines created within a couple of days from the beginning of training compared with that of the control, whereas the survival rate converged to the control level after continuous training (<bold><xref rid="fig7" ref-type="fig">Fig 7C</xref></bold>). We next considered the relationship between spine dynamics and task performance [<xref rid="c18" ref-type="bibr">18</xref>]. For this purpose, we compared task performance at the beginning of the test period among simulations with various training lengths (<bold><xref rid="fig7" ref-type="fig">Fig 7D</xref></bold>). Here, we assumed that spine elimination was enhanced during continuous training, as is observed in experiments [<xref rid="c18" ref-type="bibr">18</xref>] [<xref rid="c19" ref-type="bibr">19</xref>]. The performance was positively correlated with both the survival rate at day 7 for new spines formed during the first 2 days and the elimination rate of existing spines (left and right panels of <bold><xref rid="fig7" ref-type="fig">Fig 7E</xref></bold>). By contrast, the performance was independent from the total ratio of newly formed spines from day 0 to 6 (middle panel of <bold><xref rid="fig7" ref-type="fig">Fig 7E</xref></bold>). Without the assumption of enhanced elimination, total new spines were also positively correlated with the performance (<bold><xref rid="figS2" ref-type="fig">S2 Fig</xref> B</bold>). These results demonstrate that complex spine dynamics are well described by the semi-dual Hebbian rule, suggesting that the brain uses a dual learning mechanism.</p>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Figure 7.</label>
<caption><p>Influence of training on spine dynamics. (<bold>A</bold>) Schematic diagrams of the simulation protocols for <bold>B,C,</bold> and examples of spine dynamics for pre-existing spines and new spines. (<bold>B</bold>) Spine survival rates for control and training simulations. Dotted lines represent survival rates of pre-existing spines (spines created before day 0 and existing on day 2), and solid lines are new spines created between day 0 and day 2. (<bold>C</bold>) The 5-day survival rate of spines created at different stages of learning. (<bold>D,E</bold>) Relationships between creation and elimination of spines and task performance. Performance was calculated from the activity within 2,000-7,000 time steps after the beginning of the test phase. In the simulation, the synaptic elimination was increased fivefold from day 1 to the end of training.</p></caption>
<graphic xlink:href="024406_fig7.tif"/>
</fig>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>The results of our study propose the following answers to the questions presented in the introduction. When connections are sparsely organized, the synaptic connection structure should be organized such that the estimated transfer entropy becomes larger than that of a randomly connected network to reduce signal variability (<bold><xref rid="fig2" ref-type="fig">Fig 2C</xref></bold>) and improve performance, even in the presence of synaptic weight plasticity (<bold><xref rid="fig4" ref-type="fig">Fig 4C</xref></bold>). In a densely connected network in which synaptic weight plasticity is sufficient in terms of performance, the synaptic connection structure should encode the time-invariant components of the external model to achieve rapid learning and robust performance (<bold><xref rid="fig5" ref-type="fig">Fig 5B</xref></bold>). In both cases, synaptic connection structures can be achieved by a Hebbian-type learning rule in which the elimination and creation of dendritic spines are probabilistically performed based on the activity of presynaptic and postsynaptic neurons. Similar results are obtained even if spine creation is random, when spine elimination is probabilistically performed based on the synaptic weight, and this approximated model is indeed sufficient to reproduce various experimental results (<bold><xref rid="fig6" ref-type="fig">Figs 6</xref>-<xref rid="fig7" ref-type="fig">7</xref></bold>).</p>
<sec id="s4a">
<title>Model evaluation</title>
<p>Spine dynamics depend on the age of the animal [<xref rid="c16" ref-type="bibr">16</xref>], the brain region [<xref rid="c17" ref-type="bibr">17</xref>], spine shape [<xref rid="c39" ref-type="bibr">39</xref>], and many molecules play crucial roles [<xref rid="c33" ref-type="bibr">33</xref>] [<xref rid="c40" ref-type="bibr">40</xref>], making it difficult for any theoretical model to fully capture the complexity. Nevertheless, our simple mathematical model replicated many key features [<xref rid="c7" ref-type="bibr">7</xref>] [<xref rid="c18" ref-type="bibr">18</xref>] [<xref rid="c19" ref-type="bibr">19</xref>] [<xref rid="c33" ref-type="bibr">33</xref>]. For instance, small spines often show enlargement, while large spines are more likely to show shrinkage (<bold><xref rid="fig6" ref-type="fig">Fig 6B</xref></bold>). Older spines tend to have a large connection probability, which is proportional to spine size (<bold><xref rid="fig6" ref-type="fig">Fig 6D</xref></bold>), and they are more stable (<bold><xref rid="fig6" ref-type="fig">Fig 6E</xref></bold>). In addition, training enhances the stability of newly created spines, whereas it degrades the stability of older spines (<bold><xref rid="fig7" ref-type="fig">Fig 7B</xref></bold>).</p>
</sec>
<sec id="s4b">
<title>Experimental prediction</title>
<p>In the developmental stage, both axon guidance [<xref rid="c41" ref-type="bibr">41</xref>] and dendritic extension [<xref rid="c42" ref-type="bibr">42</xref>] show Hebbian-type activity dependence, but in the adult cortex, both axons and dendrites seldom change their structures [<xref rid="c15" ref-type="bibr">15</xref>]. Thus, although recent experimental results suggest some activity dependence for spine creation [<xref rid="c43" ref-type="bibr">43</xref>] [<xref rid="c44" ref-type="bibr">44</xref>], it is still unclear to what extent spine creation depends on the activity of presynaptic and postsynaptic neurons. Our model indicates that in terms of performance, spine creation should fully depend on both presynaptic and postsynaptic activity (<bold><xref rid="fig6" ref-type="fig">Fig 6A</xref></bold>). However, it is possible to replicate a wide range of experimental results on spine dynamics without assuming activity dependence of spine creation (<bold><xref rid="fig6" ref-type="fig">Fig 6</xref>, <xref rid="fig7" ref-type="fig">7</xref></bold>).</p>
<p>In addition, whether or not spine survival rate increases through training is controversial [<xref rid="c18" ref-type="bibr">18</xref>] [<xref rid="c19" ref-type="bibr">19</xref>]. Our model implies that the stability of new spines highly depends on the similarity between new task and control behavior (<bold><xref rid="figS2" ref-type="fig">S2 Fig</xref> A</bold>). When the similarity is low, new spines would be expected to be more stable than those in the control case, because the synaptic connection structure also would need to be reorganized. By contrast, when the similarity is high, the stability of the new spines would be comparable to that of the control. Our model additionally replicates the effect of varying training duration for spine stability [<xref rid="c18" ref-type="bibr">18</xref>]. When training was rapidly terminated, newly formed spines became less stable than those undergoing continuous training (<bold><xref rid="figS2" ref-type="fig">S2 Fig</xref> C</bold>).</p>
</sec>
<sec id="s4c">
<title>Related studies</title>
<p>Several theoretical investigations have been conducted on phenomenological characteristics of synaptogenesis [<xref rid="c45" ref-type="bibr">45</xref>] [<xref rid="c46" ref-type="bibr">46</xref>] [<xref rid="c47" ref-type="bibr">47</xref>]. Some studies further considered the functional implications [<xref rid="c20" ref-type="bibr">20</xref>] [<xref rid="c22" ref-type="bibr">22</xref>] or optimality in regard to wiring cost [<xref rid="c48" ref-type="bibr">48</xref>], but the functional significance of synaptic plasticity and the variability of EPSP size were not considered in those studies.</p>
<p>It was previously determined that learning with two variables on different timescales is beneficial under a dynamic environment [<xref rid="c49" ref-type="bibr">49</xref>]. In our model, both fast and slow variables played important roles, whereas in previous studies, only one variable was usually effective, depending on the context. In addition, our model provides a biologically plausible interpretation of the learning process with two variables.</p>
</sec>
</sec>
<sec id="s5">
<title>Materials and methods</title>
<sec id="s5a">
<title>Model</title>
<sec id="s5a1">
<title>Model dynamics</title>
<p>We first define the model and the learning rule for general exponential family, and derive equations for two examples (Gaussian and Poisson). In the task, at every time <italic>t</italic>, one hidden state <italic>s</italic><sup><italic>t</italic></sup> is sampled from prior distribution <italic>p</italic>(<italic>s</italic>). Neurons in the input layer show stochastic response <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline18.gif"/></alternatives></inline-formula></italic> that follows probabilistic distribution <italic>f</italic> (<italic>r</italic><sub><italic>X</italic>,<italic>j</italic></sub><italic>&#x007C;s</italic><sup><italic>t</italic></sup>):
<disp-formula id="eqn5">
<alternatives>
<graphic xlink:href="024406_eqn5.gif"/>
</alternatives>
</disp-formula>
Neurons in output layer estimate the hidden variables from input neuron activity. Here we assume maximum likelihood estimation for decision making unit, as the external state is a discrete variable. In this framework, in order to detect the hidden signal, firing rate of neuron <italic>i</italic> should be proportional to posterior
<disp-formula id="eqn6">
<alternatives>
<graphic xlink:href="024406_eqn6.gif"/>
</alternatives>
</disp-formula>
where <italic>&#x003C3;</italic><sub><italic>i</italic></sub> represents the index of the hidden variable preferred by output neuron <italic>i</italic> [<xref rid="c23" ref-type="bibr">23</xref>] [<xref rid="c24" ref-type="bibr">24</xref>]. Due to Bayes rule, estimation of <italic>s</italic><sup><italic>t</italic></sup> is given by,
<disp-formula id="eqn7">
<alternatives>
<graphic xlink:href="024406_eqn7.gif"/>
</alternatives>
</disp-formula>
where <italic>q</italic><sub><italic>j&#x000B5;</italic></sub> <italic>&#x2261; h</italic>(<italic>&#x003B8;</italic><sub><italic>&#x000B5;j</italic></sub>), <italic>a</italic>(<italic>q</italic><sub><italic>&#x000B5;j</italic></sub>) &#x2261; <italic>A</italic> (<italic>h</italic><sup>&#x2212;1</sup>(<italic>q</italic><sub><italic>j&#x000B5;</italic></sub>)). If we assume the uniformity of hidden states as log <italic>p</italic>(<italic>s</italic><sup><italic>t</italic></sup> = <italic>&#x000B5;</italic>) : const, and <inline-formula><alternatives><inline-graphic xlink:href="024406_inline19.gif"/></alternatives></inline-formula>, the equation above becomes
<disp-formula id="ueqn1">
<alternatives>
<graphic xlink:href="024406_ueqn1.gif"/>
</alternatives>
</disp-formula></p>
<p>Let us assume that, at every time <italic>t</italic>, firing rate of output neurons follow,
<disp-formula id="eqn8">
<alternatives>
<graphic xlink:href="024406_eqn8.gif"/>
</alternatives>
</disp-formula>
where,
<disp-formula id="ueqn2">
<alternatives>
<graphic xlink:href="024406_ueqn2.gif"/>
</alternatives>
</disp-formula></p>
<p>If connection is all-to-all, <italic>w</italic><sub><italic>ij</italic></sub> = <italic>q</italic><sub><italic>j&#x000B5;</italic></sub> gives optimal inference, because
<disp-formula id="eqn9">
<alternatives>
<graphic xlink:href="024406_eqn9.gif"/>
</alternatives>
</disp-formula></p>
<p>Note that <italic>h</italic><sub><italic>w</italic></sub> is not necessary to achieve optimal inference, however, under a sparse connection, <italic>h</italic><sub><italic>w</italic></sub> is important for reducing the effect of connection variability. In this formalization, even in non-all-to-all network, if the sparseness of connectivity stays in reasonable range, near-optimal inference can be erformed for arbitrary feedforward connectivity by adjusting synaptic weight to <italic>w</italic><sub><italic>ij</italic></sub> = <italic>w</italic><sub><italic>&#x000B5;j</italic></sub> <italic>= q</italic><sub><italic>j&#x000B5;</italic></sub><italic>/&#x003C1;</italic><sub><italic>&#x000B5;j</italic></sub> where <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline20.gif"/></alternatives></inline-formula>.</italic></p>
</sec>
<sec id="s5a2">
<title>Synaptic weight learning</title>
<p>To perform maximum likelihood estimation from output neuron activity, synaptic weight matrix between input neurons and output neurons should provide a reverse model of input neuron activity. If the reverse model is faithful, KL-divergence between the true input and the estimated distributions <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline21.gif"/></alternatives></inline-formula></italic>be minimized [<xref rid="c28" ref-type="bibr">28</xref>] [<xref rid="c29" ref-type="bibr">29</xref>]. Therefore, synaptic weights learning can be performed by argmin<sub><italic>W</italic></sub> <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline22.gif"/></alternatives></inline-formula></italic> is approximated as
<disp-formula id="eqn10">
<alternatives>
<graphic xlink:href="024406_eqn10.gif"/>
</alternatives>
</disp-formula>
<inline-formula><alternatives><inline-graphic xlink:href="024406_inline23.gif"/></alternatives></inline-formula> second line is the average response estimated from connectivity matrix <italic>C</italic>, and weight matrix <italic>W</italic>. In the last equation, <inline-formula><alternatives><inline-graphic xlink:href="024406_inline24.gif"/></alternatives></inline-formula> is substituted for <inline-formula><alternatives><inline-graphic xlink:href="024406_inline25.gif"/></alternatives></inline-formula>. If we approximate the estimated parameter <inline-formula><alternatives><inline-graphic xlink:href="024406_inline26.gif"/></alternatives></inline-formula> with <inline-formula><alternatives><inline-graphic xlink:href="024406_inline27.gif"/></alternatives></inline-formula>, by using the average connectivity <italic>&#x003C1;</italic><sub><italic>o</italic></sub>, a synaptic weight plasticity rule is given by stochastic gradient descending as
<disp-formula id="eqn11">
<alternatives>
<graphic xlink:href="024406_eqn11.gif"/>
</alternatives>
</disp-formula></p>
<p>As we were considering population representation, in which the total number of output neuron is larger than the total number of external states, there is an redundancy in representation. To make use of most of population, homeostatic constraint is necessary. For homeostatic plasticity, we set a constraint on the output firing rate. By combining two terms, synaptic weight plasticity rule is given as
<disp-formula id="eqn12">
<alternatives>
<graphic xlink:href="024406_eqn12.gif"/>
</alternatives>
</disp-formula></p>
<p>By changing the strength of homeostatic plasticity <italic>b</italic><sub><italic>h</italic></sub>, the network changes its behavior. The learning rate is divided by <italic>&#x003B3;</italic>, because the mean of <italic>w</italic> is proportional to <inline-formula><alternatives><inline-graphic xlink:href="024406_inline28.gif"/></alternatives></inline-formula>. Although, this learning rule is unsupervised, each output neuron naturally selects an external state in self-organisation manner.</p>
</sec>
<sec id="s5a3">
<title>Synaptic connection learning</title>
<p>Wiring plasticity of synaptic connection can be given in a similar manner. As shown in <bold><xref rid="fig3" ref-type="fig">Fig 3E</xref></bold>, if the synaptic connection structure of network is correlated with the external model, the learning performance gets better. Therefore, by considering <inline-formula><alternatives><inline-graphic xlink:href="024406_inline29.gif"/></alternatives></inline-formula>, the update rule of connection probability is given as
<disp-formula id="eqn13">
<alternatives>
<graphic xlink:href="024406_eqn13.gif"/>
</alternatives>
</disp-formula></p>
<p>Here, we approximated <italic>w</italic><sub><italic>ij</italic></sub> with its average value <italic>w</italic><sub><italic>o</italic></sub>. In this implementation, if synaptic weight is also plastic, convergence of <italic>D</italic><sub><italic>KL</italic></sub> is no longer guaranteed. However, as shown in <bold><xref rid="fig2" ref-type="fig">Fig 2E</xref></bold>, redundant representation yields better performance, thus this approximation is reasonable. To keep the detailed balance of connection probability, creation probability <italic>c</italic><sub><italic>p</italic></sub>(<italic>&#x003C1;</italic>) and elimination probability <italic>e</italic><sub><italic>p</italic></sub>(<italic>&#x003C1;</italic>) need to satisfy
<disp-formula id="ueqn3">
<alternatives>
<graphic xlink:href="024406_ueqn3.gif"/>
</alternatives>
</disp-formula></p>
<p>The simplest functions that satisfy above equation is <italic>c</italic><sub><italic>p</italic></sub>(<italic>&#x003C1;</italic>) &#x2261; <italic>&#x003C1;</italic>/<italic>&#x003C4;</italic><sub><italic>c</italic></sub>, <italic>e</italic><sub><italic>p</italic></sub>(<italic>&#x003C1;</italic>) &#x2261; (1 &#x2212; <italic>&#x003C1;</italic>)<italic>/&#x003C4;</italic><sub><italic>c</italic></sub>. In the simulation, we implemented this rule by changing <italic>c</italic><sub><italic>ij</italic></sub> from 1 to 0 with probability = (1 &#x2212; <italic>&#x003C1;</italic>)<italic>/&#x003C4;</italic><sub><italic>c</italic></sub> for every connection with <italic>c</italic><sub><italic>ij</italic></sub> = 1, and shift <italic>c</italic><sub><italic>ij</italic></sub> from 0 to 1 with probability <italic>&#x003C1;/&#x003C4;</italic><sub><italic>c</italic></sub> for non-existing connection (<italic>c</italic><sub><italic>ij</italic></sub> = 0) at every time step.</p>
</sec>
<sec id="s5a4">
<title>Dual Hebbian rule and estimated transfer entropy</title>
<p>The results in the main texts suggest that non-random synaptic connection structure can be beneficial either when that increases estimated transfer entropy or is correlated with the structure of the external model. To derive dual Hebbian rule, we used the latter property, yet in the simulation, estimated transfer entropy also increased by the dual Hebbian rule. Here, we consider relationship of two objective functions. Estimation of the external state from the sampled inputs is approximated as
<disp-formula id="eqn14">
<alternatives>
<graphic xlink:href="024406_eqn14.gif"/>
</alternatives>
</disp-formula></p>
<p>Therefore, by considering stochastic gradient descending, an update rule of <italic>&#x003C1;</italic><sub><italic>ij</italic></sub> is given as
<disp-formula id="eqn15">
<alternatives>
<graphic xlink:href="024406_eqn15.gif"/>
</alternatives>
</disp-formula></p>
<p>If we compare this equation with the equation for dual Hebbian rule, both of them are monotonically increasing function of <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline30.gif"/></alternatives></inline-formula></italic> and have the same dependence on <inline-formula><alternatives><inline-graphic xlink:href="024406_inline31.gif"/></alternatives></inline-formula> although normalization terms are different. Thus, under an adequate normalization, the inner product of change direction is on average positive. Therefore, although dual Hebbian learning rule does not maximize the estimated maximum transfer entropy, the rule rarely diminish it.</p>
</sec>
<sec id="s5a5">
<title>Gaussian model</title>
<p>We constructed mean response probabilities <inline-formula><alternatives><inline-graphic xlink:href="024406_inline32.gif"/></alternatives></inline-formula> by following 2 steps. First, non-normalized response probabilities <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline33.gif"/></alternatives></inline-formula></italic> were chosen from a truncated normal distribution <italic>N</italic> (<italic>&#x000B5;</italic><sub><italic>M</italic></sub><italic>, &#x003C3;</italic><sub><italic>M</italic></sub>) defined on [0, &#x221E;). Second, we defined <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline34.gif"/></alternatives></inline-formula></italic> by <inline-formula><alternatives><inline-graphic xlink:href="024406_inline35.gif"/></alternatives></inline-formula>, where <inline-formula><alternatives><inline-graphic xlink:href="024406_inline36.gif"/></alternatives></inline-formula>. When the noise follows a Gaussian distribution, the response functions in <xref ref-type="disp-formula" rid="eqn5">equation (5)</xref> are given as
<disp-formula id="eqn16">
<alternatives>
<graphic xlink:href="024406_eqn16.gif"/>
</alternatives>
</disp-formula></p>
<p>Because <inline-formula><alternatives><inline-graphic xlink:href="024406_inline37.gif"/></alternatives></inline-formula> is given as <inline-formula><alternatives><inline-graphic xlink:href="024406_inline38.gif"/></alternatives></inline-formula>. By substituting above values into the original equations, the neural dynamics is given as
<disp-formula id="eqn17">
<alternatives>
<graphic xlink:href="024406_eqn17.gif"/>
</alternatives>
</disp-formula></p>
<p>Similarly, dual Hebbian rule becomes
<disp-formula id="eqn18">
<alternatives>
<graphic xlink:href="024406_eqn18.gif"/>
</alternatives>
</disp-formula>
<disp-formula id="eqn19">
<alternatives>
<graphic xlink:href="024406_eqn19.gif"/>
</alternatives>
</disp-formula></p>
</sec>
<sec id="s5a6">
<title>Poisson model</title>
<p>For Poisson model, we defined mean response probabilities <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline39.gif"/></alternatives></inline-formula></italic> from a log-normal distribution instead of a normal distribution. Non-normalized values were sampled from a truncated log-normal distribution <inline-formula><alternatives><inline-graphic xlink:href="024406_inline40.gif"/></alternatives></inline-formula> defined on <inline-formula><alternatives><inline-graphic xlink:href="024406_inline41.gif"/></alternatives></inline-formula>. Normalization was performed as <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline42.gif"/></alternatives></inline-formula></italic> for <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline43.gif"/></alternatives></inline-formula></italic>, where <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline46.gif"/></alternatives></inline-formula></italic>. Because the noise follows a Poisson distribution <italic>p</italic>(<italic>r&#x007C;&#x003B8;</italic>) = exp [<italic>-q</italic> &#x002B; <italic>r</italic> log <italic>q -</italic> log <italic>r</italic>!], the response functions are given as
<disp-formula id="eqn20">
<alternatives>
<graphic xlink:href="024406_eqn20.gif"/>
</alternatives>
</disp-formula></p>
<p>As a result, <italic>a</italic>(<italic>q</italic>) is defined as <italic>a</italic>(<italic>q</italic>) = <italic>A</italic> (<italic>h</italic><sup>-1</sup>(<italic>q</italic>)) = <italic>e</italic><sup><italic>q</italic></sup>. By substituting them to the original equations, the neural dynamics also follows <xref ref-type="disp-formula" rid="eqn17">equation (17)</xref>. If connection is all-to-all, by setting <inline-formula><alternatives><inline-graphic xlink:href="024406_inline47.gif"/></alternatives></inline-formula> for <italic>i &#x02208;</italic> &#x003A9;<sub><italic>&#x000B5;</italic></sub>, optimal inference is achievable. Here, we normalized <italic>&#x003B8;</italic><sub><italic>&#x000B5;j</italic></sub> by <italic>&#x003B8;</italic><sub><italic>o</italic></sub>, which is defined as <inline-formula><alternatives><inline-graphic xlink:href="024406_inline48.gif"/></alternatives></inline-formula>, in order to keep synaptic weights in non-negative values.</p>
<p>Learning rules for synaptic weight and connection are given as
<disp-formula id="eqn21">
<alternatives>
<graphic xlink:href="024406_eqn21.gif"/>
</alternatives>
</disp-formula>
<disp-formula id="eqn22">
<alternatives>
<graphic xlink:href="024406_eqn22.gif"/>
</alternatives>
</disp-formula></p>
<p>Note that the first term of the synaptic weight learning rule coincides with a previously proposed optimal learning rule for spiking neurons [<xref rid="c29" ref-type="bibr">29</xref>] [<xref rid="c50" ref-type="bibr">50</xref>]. In calculation of model error, we error was calculated as <inline-formula><alternatives><inline-graphic xlink:href="024406_inline49.gif"/></alternatives></inline-formula>, where estimated parameter <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline50.gif"/></alternatives></inline-formula></italic> was given by <inline-formula><alternatives><inline-graphic xlink:href="024406_inline51.gif"/></alternatives></inline-formula> Non-normalized estimator <inline-formula><alternatives><inline-graphic xlink:href="024406_inline52.gif"/></alternatives></inline-formula> is calculated as <inline-formula><alternatives><inline-graphic xlink:href="024406_inline53.gif"/></alternatives></inline-formula>. In <bold><xref rid="figS1" ref-type="fig">S1 Fig</xref> F</bold>, estimation from connectivity was calculated from <inline-formula><alternatives><inline-graphic xlink:href="024406_inline54.gif"/></alternatives></inline-formula>, and similarly, estimation from weights was calculated by <inline-formula><alternatives><inline-graphic xlink:href="024406_inline55.gif"/></alternatives></inline-formula>.</p>
<p>For parameters, we used <inline-formula><alternatives><inline-graphic xlink:href="024406_inline56.gif"/></alternatives></inline-formula>, and for other parameters, we used same values with the Gaussian model.</p>
</sec>
</sec>
<sec id="s5b">
<title>Analytical evaluation</title>
<sec id="s5b1">
<title>Performance</title>
<p>In Gaussian model, we can analytically evaluate the performance in two coding schemes. As the dynamics of output neurons follows
<disp-formula id="ueqn4">
<alternatives>
<graphic xlink:href="024406_ueqn4.gif"/>
</alternatives>
</disp-formula>
membrane potential variable <italic>u</italic><sub><italic>i</italic></sub>, which is defined as
<disp-formula id="eqn23">
<alternatives>
<graphic xlink:href="024406_eqn23.gif"/>
</alternatives>
</disp-formula>
determines firing rates of each neuron. Due to normalization <inline-formula><alternatives><inline-graphic xlink:href="024406_inline57.gif"/></alternatives></inline-formula>, mean and variance &#x007B;<italic>&#x03B8;</italic><sub><italic>j&#x03BC;</italic></sub>&#x007D; are given as
<disp-formula id="eqn24">
<alternatives>
<graphic xlink:href="024406_eqn24.gif"/>
</alternatives>
</disp-formula>
where <italic>&#x000B5;</italic><sub><italic>M</italic></sub> and <italic>&#x003C3;</italic><sub><italic>M</italic></sub> are the mean and variance of the original non-normalized truncated Gaussian distribution. Because both <italic>r</italic><sub><italic>X</italic>,<italic>j</italic></sub> and <italic>&#x003B8;</italic><sub><italic>j&#x000B5;</italic></sub> approximately follow Gaussian distribution, <italic>u</italic><sub><italic>i</italic></sub> is expected to follow Gaussian. Therefore, by evaluating its mean and variance, we can characterize the distribution of <italic>u</italic><sub><italic>i</italic></sub> for a given external state [<xref rid="c51" ref-type="bibr">51</xref>].</p>
</sec>
<sec id="s5b2">
<title>In weight coding</title>
<p>In weight coding scheme, <italic>w</italic><sub><italic>ij</italic></sub> and <italic>c</italic><sub><italic>ij</italic></sub> are defined as
<disp-formula id="ueqn5">
<alternatives>
<graphic xlink:href="024406_ueqn5.gif"/>
</alternatives>
</disp-formula>
where <inline-formula><alternatives><inline-graphic xlink:href="024406_inline58.gif"/></alternatives></inline-formula>. If <italic>s</italic><sup><italic>t</italic></sup> = <italic>&#x000B5;</italic>,
<disp-formula id="eqn25">
<alternatives>
<graphic xlink:href="024406_eqn25.gif"/>
</alternatives>
</disp-formula></p>
<p>Similarly, the variance of <italic>u</italic><sub><italic>i</italic></sub> is
<disp-formula id="eqn26">
<alternatives>
<graphic xlink:href="024406_eqn26.gif"/>
</alternatives>
</disp-formula></p>
<p>If <italic>s</italic><sup><italic>t</italic></sup> <italic>&#x2260; &#x000B5;</italic>, as <italic>w</italic><sub><italic>ij</italic></sub> and <italic>r</italic><sub><italic>x</italic>,<italic>j</italic></sub> are independent,
<disp-formula id="eqn27">
<alternatives>
<graphic xlink:href="024406_eqn27.gif"/>
</alternatives>
</disp-formula></p>
<p>In addition to that, due to feedforward connection, output neurons show noise correlation. If output neuron <italic>i</italic> belongs to <italic>i &#x02208;</italic> &#x003A9;<sub><italic>&#x000B5;</italic></sub> where <italic>s</italic><sup><italic>t</italic></sup> = <italic>&#x000B5;</italic>, whereas <italic>l</italic> &#x02209; &#x003A9;<sub><italic>&#x000B5;</italic></sub>, the covariance between <italic>u</italic><sub><italic>i</italic></sub> and <italic>u</italic><sub><italic>l</italic></sub> satisfies
<disp-formula id="eqn28">
<alternatives>
<graphic xlink:href="024406_eqn28.gif"/>
</alternatives>
</disp-formula></p>
<p>Therefore, approximately (<italic>u</italic><sub><italic>i</italic></sub><italic>, u</italic><sub><italic>l</italic></sub>) follows a multivariable Gaussian distributions
<disp-formula id="ueqn6">
<alternatives>
<graphic xlink:href="024406_ueqn6.gif"/>
</alternatives>
</disp-formula></p>
<p>In maximum likelihood estimation, the estimation fails if a non-selective output neuron shows higher firing rate than the selective neuron. Probability for such a event when there are two output neuron is
<disp-formula id="eqn29">
<alternatives>
<graphic xlink:href="024406_eqn29.gif"/>
</alternatives>
</disp-formula></p>
<p>In the simulation, there are <italic>p</italic> &#x2212; 1 distractors per one selective output neuron. Thus, approximately, accuracy of estimation was evaluated by (1 &#x2212; &#x02208;<sub><italic>w</italic></sub>)<sup><italic>p</italic>&#x2212;1</sup>. In <bold><xref rid="fig2" ref-type="fig">Fig 2B</xref></bold>, we numerically calculated this value for the analytical estimation.</p>
</sec>
<sec id="s5b3">
<title>In connectivity coding</title>
<p><bold>In connectivity coding</bold>, <italic>w</italic><sub><italic>ij</italic></sub> and <italic>c</italic><sub><italic>ij</italic></sub> are given as
<disp-formula id="eqn30">
<alternatives>
<graphic xlink:href="024406_eqn30.gif"/>
</alternatives>
</disp-formula></p>
<p>From a similar calculation done above,
<disp-formula id="ueqn7">
<alternatives>
<graphic xlink:href="024406_ueqn7.gif"/>
</alternatives>
</disp-formula></p>
<p>If we compare the two coding schemes, mean and covariance are the same for two coding schemes, and as <italic>&#x003B3;</italic> satisfies<inline-formula><alternatives><inline-graphic xlink:href="024406_inline59.gif"/></alternatives></inline-formula>, variance of non-selective output neuron are similar. The main difference is the second term of signal variance. In the weight coding, signal variance is proportional to 1<italic>/&#x003C1;</italic>, on the other hands, in the connectivity coding, the second term of signal variance is negative, and does not depend on the connectivity. As a result, in the adequately sparse regime, firing rate variability of selective output neuron become smaller in connectivity coding, and the estimation accuracy is better. In the sparse limit, the first term of variance becomes dominant and both schemes do not work well, consequently, the advantage for connectivity coding disappears. Coefficient of variation calculated for signal terms is indeed smaller in connectivity coding scheme (blue and red lines in <bold><xref rid="fig2" ref-type="fig">Fig 2C</xref></bold>), and the same tendency is observed in simulation (cyan and orange lines in <bold><xref rid="fig2" ref-type="fig">Fig 2C</xref></bold>).</p>
</sec>
<sec id="s5b4">
<title>Spine dynamics</title>
<p>In the Gaussian model, because the response probability of input neurons approximately follows a Gaussian distribution, at the equilibrium state, connection probabilities should follow:
<disp-formula id="eqn31">
<alternatives>
<graphic xlink:href="024406_eqn31.gif"/>
</alternatives>
</disp-formula></p>
<p>If we ignore fluctuation of <italic>&#x003C1;</italic> caused by stochastic firing, life expectancy <italic>T</italic> of a spine with connection probability <italic>&#x003C1;</italic> follows,
<disp-formula id="eqn32">
<alternatives>
<graphic xlink:href="024406_eqn32.gif"/>
</alternatives>
</disp-formula>
where <italic>Z</italic>(<italic>&#x003C1;</italic>) is a normalization factor. Thus, spine age distribution is given as,
<disp-formula id="eqn33">
<alternatives>
<graphic xlink:href="024406_eqn33.gif"/>
</alternatives>
</disp-formula></p>
<p>By Bayes rule, connection probability distribution <italic>&#x003C1;</italic> for a given spine age <italic>d</italic> is
<disp-formula id="eqn34">
<alternatives>
<graphic xlink:href="024406_eqn34.gif"/>
</alternatives>
</disp-formula></p>
<p><bold><xref rid="fig6" ref-type="fig">Fig 6D</xref></bold> shows the mean connection probability for various spine ages. As seen in previous experimental studies, older spines tend to have larger connection probability. In the evaluation of analytical results, we used an approximation
<disp-formula id="ueqn8">
<alternatives>
<graphic xlink:href="024406_ueqn8.gif"/>
</alternatives>
</disp-formula>
with &#x00394;<italic>t</italic> = 10<sup>3</sup>. Similarly, 5 days survival rate for various spine age <italic>d</italic> was calculated as,
<disp-formula id="eqn35">
<alternatives>
<graphic xlink:href="024406_eqn35.gif"/>
</alternatives>
</disp-formula>
where <italic>T</italic><sub><italic>o</italic></sub> is time steps corresponding to one day. As expected, 5 days survival rate was higher for older spines in both analytical calculation and simulation (<bold><xref rid="fig6" ref-type="fig">Fig 6E</xref></bold>).</p>
</sec>
</sec>
<sec id="s5c">
<title>Details of simulation</title>
<sec id="s5c1">
<title>Model settings</title>
<p>In the simulation, the external variable <italic>s</italic><sup><italic>t</italic></sup> was chosen from 10 discrete variables (<italic>p</italic> = 10) with equal probability (Pr[<italic>s</italic><sup><italic>t</italic></sup> = <italic>q</italic>] = 1<italic>/p</italic>, for all <italic>q</italic>). The mean response probability <italic>&#x003B8;</italic><sub><italic>j&#x000B5;</italic></sub> was given first by randomly chosen parameters <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline60.gif"/></alternatives></inline-formula></italic> from the truncated normal distribution <italic>N</italic> (<italic>&#x000B5;</italic><sub><italic>M</italic></sub><italic>, &#x003C3;</italic><sub><italic>M</italic></sub>) in [0, &#x221E;), and then normalized using <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline61.gif"/></alternatives></inline-formula></italic>, where <inline-formula><alternatives><inline-graphic xlink:href="024406_inline62.gif"/></alternatives></inline-formula>. Mean weight <italic>w</italic><sub><italic>o</italic></sub> was defined as <inline-formula><alternatives><inline-graphic xlink:href="024406_inline63.gif"/></alternatives></inline-formula>. The normalization factor <italic>h</italic><sub><italic>w</italic></sub> was defined as <inline-formula><alternatives><inline-graphic xlink:href="024406_inline64.gif"/></alternatives></inline-formula> in <bold>Figs 1-4</bold>, where <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline65.gif"/></alternatives></inline-formula></italic>, and as <italic>h</italic><sub><italic>w</italic></sub> = <italic>r</italic><sup><italic>o</italic></sup> <italic>/&#x003B3;</italic> in <bold>Figs 5-7</bold>, as the mean of <italic>&#x003B8;</italic> depends on <italic>&#x03BA;</italic><sub><italic>m</italic></sub>. Average connectivity <inline-formula><alternatives><inline-graphic xlink:href="024406_inline66.gif"/></alternatives></inline-formula> was calculated from the initial connection matrix of each simulation. In the calculation of the dynamics, for the membrane parameter <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline67.gif"/></alternatives></inline-formula></italic>, a boundary condition <italic>v</italic><sub><italic>i</italic></sub> <italic>&#x0003E;</italic> max<sub><italic>l</italic></sub>&#x007B;<italic>v</italic><sub><italic>l</italic></sub> <italic>- v</italic><sub><italic>d</italic></sub>&#x007D; was introduced for numerical convenience, where <italic>v</italic><sub><italic>d</italic></sub> = &#x2212;60. In addition, synaptic weight <italic>w</italic> was bounded to a non-negative value (<italic>w &#x0003E;</italic> 0), and the connection probability was defined as <italic>&#x003C1; &#x02208;</italic> [0, 1]. For simulations with synaptic weight learning, initial weights were defined as <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline68.gif"/></alternatives></inline-formula></italic>, where <inline-formula><alternatives><inline-graphic xlink:href="024406_inline69.gif"/></alternatives></inline-formula>, and <italic>&#x003B6;</italic> is a Gaussian random variable. Similarly, in the simulation with structural plasticity, the initial condition for the synaptic connection matrix was defined as<inline-formula><alternatives><inline-graphic xlink:href="024406_inline70.gif"/></alternatives></inline-formula>. In both the dual Hebbian rule and the semi-dual Hebbian rule, the synaptic weight of a newly created spine was given as<inline-formula><alternatives><inline-graphic xlink:href="024406_inline71.gif"/></alternatives></inline-formula>, for a random Gaussian variable <italic>&#x003B6; &#x02190; N</italic> (0, 1). In <bold><xref rid="fig7" ref-type="fig">Fig 7</xref></bold>, simulations were initiated at -20 days (i.e., 2 <italic>&#x00D7;</italic> 10<sup>6</sup> steps before stimulus onset) to ensure convergence for the control condition. For model parameters, <italic>&#x000B5;</italic><sub><italic>M</italic></sub> = 1.0, <italic>&#x003C3;</italic><sub><italic>M</italic></sub> = 1.0, <italic>&#x003C3;</italic><sub><italic>x</italic></sub> = 1.0, <italic>M</italic> = 200, <italic>N</italic> = 100, <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline72.gif"/></alternatives></inline-formula></italic>, and <italic><inline-formula><alternatives><inline-graphic xlink:href="024406_inline73.gif"/></alternatives></inline-formula></italic> were used, and for learning-related parameters, <italic>&#x003B7;</italic><sub><italic>x</italic></sub> = 0.01, <italic>b</italic><sub><italic>h</italic></sub> = 0.1, <italic>&#x003B7;</italic><sub><italic>&#x003C1;</italic></sub> = 0.001, <italic>&#x003C4;</italic><sub><italic>c</italic></sub> = 10<sup>6</sup>, <italic>T</italic><sub>2</sub> = 10<sup>5</sup>, and <italic>&#x03BA;</italic><sub><italic>m</italic></sub> = 0.5 were used. In <bold>Figs 6 and 7</bold>, except <bold><xref rid="fig6" ref-type="fig">Fig 6A</xref></bold>, <italic>&#x003B7;</italic><sub><italic>&#x003C1;</italic></sub> = 0.0001, <italic>&#x003C4;</italic><sub><italic>c</italic></sub> = 3 <italic>&#x00D7;</italic> 10<sup>5</sup>, and <italic>&#x003B3;</italic> = 0.6 were used, unless otherwise stated.</p>
</sec>
<sec id="s5c2">
<title>Accuracy of estimation</title>
<p>The accuracy was measured with the bootstrap method. By using data from <italic>t - T</italic><sub><italic>o</italic></sub> <italic>&#x02264; t</italic>&#x2032; <italic>&#x003C; t</italic>, the selectivity of output neurons was first decided. &#x003A9;<sub><italic>&#x000B5;</italic></sub> was defined as a set of output neurons that represents external state <italic>&#x000B5;</italic>. Neuron <italic>i</italic> belongs to set &#x003A9;<sub><italic>&#x000B5;</italic></sub> if <italic>i</italic> satisfies
<disp-formula id="eqn36">
<alternatives>
<graphic xlink:href="024406_eqn36.gif"/>
</alternatives>
</disp-formula>
where operator [<italic>X</italic>]<sub><italic>tof</italic></sub> returns 1 if <italic>X</italic> is true; otherwise, it returns 0. By using this selectivity, based on data from <italic>t</italic> &#x003C; = <italic>t</italic>&#x2032; <italic>&#x003C; t</italic> &#x002B; <italic>T</italic><sub><italic>o</italic></sub>, the accuracy was estimated as
<disp-formula id="eqn37">
<alternatives>
<graphic xlink:href="024406_eqn37.gif"/>
</alternatives>
</disp-formula></p>
<p>In the simulation, <italic>T</italic><sub><italic>o</italic></sub> = 10<sup>3</sup> was used because this value is sufficiently slow compared with weight change but sufficiently long to suppress variability.</p>
</sec>
<sec id="s5c3">
<title>Model error</title>
<p>Using the same procedure, model error was estimated as
<disp-formula id="eqn38">
<alternatives>
<graphic xlink:href="024406_eqn38.gif"/>
</alternatives>
</disp-formula>
where <inline-formula><alternatives><inline-graphic xlink:href="024406_inline74.gif"/></alternatives></inline-formula> represents the estimated parameter. <inline-formula><alternatives><inline-graphic xlink:href="024406_inline75.gif"/></alternatives></inline-formula> was estimated by
<disp-formula id="eqn39">
<alternatives>
<graphic xlink:href="024406_eqn39.gif"/>
</alternatives>
</disp-formula></p>
<p>In <bold><xref rid="fig5" ref-type="fig">Fig 5E</xref></bold>, the estimation of the internal model from connectivity was calculated by
<disp-formula id="eqn40">
<alternatives>
<graphic xlink:href="024406_eqn40.gif"/>
</alternatives>
</disp-formula></p>
<p>Similarly, the estimation from the synaptic weight was performed with
<disp-formula id="eqn41">
<alternatives>
<graphic xlink:href="024406_eqn41.gif"/>
</alternatives>
</disp-formula></p>
</sec>
<sec id="s5c4">
<title>Transfer entropy</title>
<p>Entropy reduction caused by partial information on input firing rates was evaluated by transfer entropy:
<disp-formula id="eqn42">
<alternatives>
<graphic xlink:href="024406_eqn42.gif"/>
</alternatives>
</disp-formula>
where
<disp-formula id="eqn43">
<alternatives>
<graphic xlink:href="024406_eqn43.gif"/>
</alternatives>
</disp-formula></p>
<p>Output group &#x003A9;<sub><italic>&#x000B5;</italic></sub> was determined as described above. Here, the true model was used instead of the estimated model to evaluate the maximum transfer entropy achieved by the network.</p>
</sec>
</sec>
<sec id="s5d">
<title sec-type="availability">Code availability</title>
<p>C&#x002B;&#x002B; codes of the simulation program are available at <ext-link ext-link-type="uri" xlink:href="http://modeldb.yale.edu/181913">http://modeldb.yale.edu/181913</ext-link> with access code &#x0201D;wpgensfsp&#x0201D;.</p>
</sec>
</sec>
<sec id="s6">
<title>Supporting Information</title>
<fig id="figS1" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 1.</label>
<caption><title>Results in Poisson model.</title>
<p>(<bold>A</bold>) Relationship between the performance and the degree of weight coding (<italic>&#x03BA;</italic><sub><italic>w</italic></sub>) and connectivity coding (<italic>&#x03BA;</italic><sub><italic>c</italic></sub>). (<bold>B</bold>) An example of output neuron activity before (top) and after (bottom) synaptic weight learning at connectivity <italic>&#x003C1;</italic> = 0.25. (<bold>C</bold>) Relationship between learning curve and connection structure when connectivity <italic>&#x003C1;</italic> = 0.5, and the strength of homeostatic plasticity <italic>b</italic><sub><italic>h</italic></sub> = 1.0. The parameter <italic>&#x003BB;</italic> represents similarity between the connection structure and the external model. (<bold>D</bold>) Synaptic weight matrices before (left) and after (right) learning. both X-neurons and Y-neurons were sorted based on their preferred external states. (<bold>E</bold>) Accuracy of estimation at various time scale of rewiring <italic>&#x003C4;</italic><sub><italic>c</italic></sub>. (<bold>F</bold>) Model error calculated from connectivity (left) and synaptic weights (right). (<bold>G</bold>) Comparison of performance among the model without wiring plasticity (cyan), and dual Hebbian model(orange). Corresponding results in the Gaussian model are described in <bold><xref rid="fig2" ref-type="fig">Fig 2E</xref></bold>, <bold><xref rid="fig3" ref-type="fig">Fig 3A</xref></bold>, <bold><xref rid="fig3" ref-type="fig">Fig 3E</xref></bold>, <bold><xref rid="fig4" ref-type="fig">Fig 4E</xref></bold>, <bold><xref rid="fig4" ref-type="fig">Fig 4F</xref></bold>, <bold><xref rid="fig5" ref-type="fig">Fig 5E</xref>,F</bold>, <bold><xref rid="fig6" ref-type="fig">Fig 6A</xref></bold>, respectively.</p></caption>
<graphic xlink:href="024406_figS1.tif"/>
</fig>
<fig id="figS2" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 2.</label>
<caption><title>Results in simulation with different training protocols.</title>
<p>(<bold>A</bold>) Effect of similarity between the control condition and training on the new spine survival rate. The value of <italic>&#x03BA;</italic><sub><italic>m</italic></sub> was changed as in <bold><xref rid="fig5" ref-type="fig">Fig 5C</xref></bold> to alter the similarity between the two conditions. Note that <italic>&#x03BA;</italic><sub><italic>m</italic></sub> = 0 in <bold><xref rid="fig7" ref-type="fig">Fig 7</xref></bold>. (<bold>B</bold>) Relationship between task performance and spine dynamics in the absence of enhanced spine elimination during training. (<bold>C</bold>) Spine survival rates for short-training (2 d) and long-training (30 d) simulations. Pre-existing and new spines were defined as in <bold><xref rid="fig7" ref-type="fig">Fig 7A</xref>,B</bold>.</p></caption>
<graphic xlink:href="024406_figS2.tif"/>
</fig>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>The authors thank Drs. Haruo Kasai and Taro Toyoizumi for their comments on the manuscript.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Bliss</surname> <given-names>TV</given-names></string-name>, <string-name><surname>Collingridge</surname> <given-names>GL</given-names></string-name>. <article-title>A synaptic model of memory: long-term potentiation in the hippocampus</article-title>. <source>Nature</source>. <year>1993</year>;<volume>361</volume>: <fpage>31</fpage>&#x2013;<lpage>39</lpage>. doi:<pub-id pub-id-type="doi">10.1038/361031a0</pub-id></mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="book"><string-name><surname>Dayan</surname> <given-names>P</given-names></string-name>, <string-name><surname>Abbott</surname> <given-names>LF</given-names></string-name>. <source>Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems</source>. <edition>1 edition</edition>. <publisher-loc>Cambridge, Mass</publisher-loc>.: <publisher-name>The MIT Press</publisher-name>; <year>2005</year>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Song</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sj&#x00F6;str&#x00F6;m</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Reigl</surname> <given-names>M</given-names></string-name>, <string-name><surname>Nelson</surname> <given-names>S</given-names></string-name>, <string-name><surname>Chklovskii</surname> <given-names>DB.</given-names></string-name> <article-title>Highly Non-random Features of Synaptic Connectivity in Local Cortical Circuits</article-title>. <source>PLoS Biol</source>. <year>2005</year>;<volume>3</volume>: <fpage>e68</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pbio.0030068</pub-id></mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Buzsaki</surname> <given-names>G</given-names></string-name>, <string-name><surname>Mizuseki</surname> <given-names>K.</given-names></string-name> <article-title>The log-dynamic brain: how skewed distributions affect network operations</article-title>. <source>Nat Rev Neurosci</source>. <year>2014</year>;<volume>15</volume>: <fpage>264</fpage>&#x2013;<lpage>278</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nrn3687</pub-id></mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Ikegaya</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Sasaki</surname> <given-names>T</given-names></string-name>, <string-name><surname>Ishikawa</surname> <given-names>D</given-names></string-name>, <string-name><surname>Honma</surname> <given-names>N</given-names></string-name>, <string-name><surname>Tao</surname> <given-names>K</given-names></string-name>, <string-name><surname>Takahashi</surname> <given-names>N</given-names></string-name>, <etal>et al.</etal> <article-title>Interpyramid Spike Transmission Stabilizes the Sparseness of Recurrent Network Activity</article-title>. <source>Cereb Cortex</source>. <year>2013</year>;<volume>23</volume>: <fpage>293</fpage>&#x2013;<lpage>304</lpage>. doi:<pub-id pub-id-type="doi">10.1093/cercor/bhs006</pub-id></mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Lefort</surname> <given-names>S</given-names></string-name>, <string-name><surname>Tomm</surname> <given-names>C</given-names></string-name>, <string-name><surname>Floyd Sarria</surname> <given-names>J-C</given-names></string-name>, <string-name><surname>Petersen</surname>, <given-names>CCH</given-names></string-name>. <article-title>The Excitatory Neuronal Network of the C2 Barrel Column in Mouse Primary Somatosensory Cortex</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>61</volume>: <fpage>301</fpage>&#x2013;<lpage>316</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2008.12.020</pub-id></mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Yasumatsu</surname> <given-names>N</given-names></string-name>, <string-name><surname>Matsuzaki</surname> <given-names>M</given-names></string-name>, <string-name><surname>Miyazaki</surname> <given-names>T</given-names></string-name>, <string-name><surname>Noguchi</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kasai</surname> <given-names>H.</given-names></string-name> <article-title>Principles of long-term dynamics of dendritic spines</article-title>. <source>J Neurosci Off J Soc Neurosci</source>. <year>2008</year>;<volume>28</volume>: <fpage>13592</fpage>&#x2013;<lpage>13608</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUR0SCI.0603-08.2008</pub-id></mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Brunel</surname> <given-names>N</given-names></string-name>, <string-name><surname>Hakim</surname> <given-names>V</given-names></string-name>, <string-name><surname>Isope</surname> <given-names>P</given-names></string-name>, <string-name><surname>Nadal</surname> <given-names>J-P</given-names></string-name>, <string-name><surname>Barbour</surname> <given-names>B.</given-names></string-name> <article-title>Optimal Information Storage and the Distribution of Synaptic Weights: Perceptron versus Purkinje Cell</article-title>. <source>Neuron</source>. <year>2004</year>;<volume>43</volume>: <fpage>745</fpage>&#x2013;<lpage>757</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2004.08.023</pub-id></mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Hiratani</surname> <given-names>N</given-names></string-name>, <string-name><surname>Teramae</surname> <given-names>J-N</given-names></string-name>, <string-name><surname>Fukai</surname> <given-names>T.</given-names></string-name> <article-title>Associative memory model with long-tail-distributed Hebbian synaptic connections</article-title>. <source>Front ComputNeurosci</source>. <year>2013</year>;<volume>6</volume>. doi:<pub-id pub-id-type="doi">10.3389/fncom.2012.00102</pub-id></mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Caporale</surname> <given-names>N</given-names></string-name>, <string-name><surname>Dan</surname> <given-names>Y.</given-names></string-name> <article-title>Spike Timing-Dependent Plasticity: A Hebbian Learning Rule</article-title>. <source>Annu Rev Neurosci</source>. <year>2008</year>;<volume>31</volume>: <fpage>25</fpage>&#x2013;<lpage>46</lpage>. doi:<pub-id pub-id-type="doi">10.1146/annurev.neuro.31.060407.125639</pub-id></mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Feldman</surname> <given-names>DE</given-names></string-name>. <article-title>Synaptic Mechanisms for Plasticity in Neocortex</article-title>. <source>Annu Rev Neurosci</source>. <year>2009</year>;<volume>32</volume>: <fpage>33</fpage>&#x2013;<lpage>55</lpage>. doi:<pub-id pub-id-type="doi">10.1146/annurev.neuro.051508.135516</pub-id></mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Maass</surname> <given-names>W</given-names></string-name>, <string-name><surname>Natschlager</surname> <given-names>T</given-names></string-name>, <string-name><surname>Markram</surname> <given-names>H.</given-names></string-name> <article-title>Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations</article-title>. <source>Neural Comput</source>. <year>2002</year>;<volume>14</volume>: <fpage>2531</fpage>&#x2013;<lpage>2560</lpage>. doi:<pub-id pub-id-type="doi">10.1162/089976602760407955</pub-id></mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Ganguli</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sompolinsky</surname> <given-names>H.</given-names></string-name> <article-title>Compressed sensing, sparsity, and dimensionality in neuronal information processing and data analysis</article-title>. <source>Annu Rev Neurosci</source>. <year>2012</year>;<volume>35</volume>: <fpage>485</fpage>&#x2013;<lpage>508</lpage>. doi:<pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150410</pub-id></mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Chklovskii</surname> <given-names>DB</given-names></string-name>, <string-name><surname>Mel</surname> <given-names>BW</given-names></string-name>, <string-name><surname>Svoboda</surname> <given-names>K.</given-names></string-name> <article-title>Cortical rewiring and information storage</article-title>. <source>Nature</source>. <year>2004</year>;<volume>431</volume>: <fpage>782</fpage>&#x2013;<lpage>788</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nature03012</pub-id></mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Holtmaat</surname> <given-names>A</given-names></string-name>, <string-name><surname>Svoboda</surname> <given-names>K.</given-names></string-name> <article-title>Experience-dependent structural synaptic plasticity in the mammalian brain</article-title>. <source>Nat Rev Neurosci</source>. <year>2009</year>;<volume>10</volume>: <fpage>647</fpage>&#x2013;<lpage>658</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nrn2699</pub-id></mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Holtmaat</surname> <given-names>AJGD</given-names></string-name>, <string-name><surname>Trachtenberg</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Wilbrecht</surname> <given-names>L</given-names></string-name>, <string-name><surname>Shepherd</surname> <given-names>GM</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Knott</surname> <given-names>GW</given-names></string-name>, <etal>et al.</etal> <article-title>Transient and persistent dendritic spines in the neocortex in vivo</article-title>. <source>Neuron</source>. <year>2005</year>;<volume>45</volume>: <fpage>279</fpage>&#x2013;<lpage>291</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2005.01.003</pub-id></mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Zuo</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Lin</surname> <given-names>A</given-names></string-name>, <string-name><surname>Chang</surname> <given-names>P</given-names></string-name>, <string-name><surname>Gan</surname> <given-names>W-B.</given-names></string-name> <article-title>Development of Long-Term Dendritic Spine Stability in Diverse Regions of Cerebral Cortex</article-title>. <source>Neuron</source>. <year>2005</year>;<volume>46</volume>: <fpage>181</fpage>&#x2013;<lpage>189</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2005.04.001</pub-id></mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Yang</surname> <given-names>G</given-names></string-name>, <string-name><surname>Pan</surname> <given-names>F</given-names></string-name>, <string-name><surname>Gan</surname> <given-names>W-B.</given-names></string-name> <article-title>Stably maintained dendritic spines are associated with lifelong memories</article-title>. <source>Nature</source>. <year>2009</year>;<volume>462</volume>: <fpage>920</fpage>&#x2013;<lpage>924</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nature08577</pub-id></mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Xu</surname> <given-names>T</given-names></string-name>, <string-name><surname>Yu</surname> <given-names>X</given-names></string-name>, <string-name><surname>Perlik</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Tobin</surname> <given-names>WF</given-names></string-name>, <string-name><surname>Zweig</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Tennant</surname> <given-names>K</given-names></string-name>, <etal>et al.</etal> <article-title>Rapid formation and selective stabilization of synapses for enduring motor memories</article-title>. <source>Nature</source>. <year>2009</year>;<volume>462</volume>: <fpage>915</fpage>&#x2013;<lpage>919</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nature08389</pub-id></mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Poirazi</surname> <given-names>P</given-names></string-name>, <string-name><surname>Mel</surname> <given-names>BW</given-names></string-name>. <article-title>Impact of active dendrites and structural plasticity on the memory capacity of neural tissue</article-title>. <source>Neuron</source>. <year>2001</year>;<volume>29</volume>: <fpage>779</fpage>&#x2013;<lpage>796</lpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Stepanyants</surname> <given-names>A</given-names></string-name>, <string-name><surname>Hof</surname> <given-names>PR</given-names></string-name>, <string-name><surname>Chklovskii</surname> <given-names>DB.</given-names></string-name> <article-title>Geometry and Structural Plasticity of Synaptic Connectivity</article-title>. <source>Neuron</source>. <year>2002</year>;<volume>34</volume>: <fpage>275</fpage>&#x2013;<lpage>288</lpage>. doi:<pub-id pub-id-type="doi">10.1016/S0896-6273(02)00652-9</pub-id></mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Knoblauch</surname> <given-names>A</given-names></string-name>, <string-name><surname>Palm</surname> <given-names>G</given-names></string-name>, <string-name><surname>Sommer</surname> <given-names>FT</given-names></string-name>. <article-title>Memory capacities for synaptic and structural plasticity</article-title>. <source>Neural Comput</source>. <year>2010</year>;<volume>22</volume>: <fpage>289</fpage>&#x2013;<lpage>341</lpage>. doi:<pub-id pub-id-type="doi">10.1162/neco.2009.08-07-588</pub-id></mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Beck</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>WJ</given-names></string-name>, <string-name><surname>Kiani</surname> <given-names>R</given-names></string-name>, <string-name><surname>Hanks</surname> <given-names>T</given-names></string-name>, <string-name><surname>Churchland</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Roitman</surname> <given-names>J</given-names></string-name>, <etal>et al.</etal> <article-title>Probabilistic Population Codes for Bayesian Decision Making</article-title>. <source>Neuron</source>. <year>2008</year>;<volume>60</volume>: <fpage>1142</fpage>&#x2013;<lpage>1152</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2008.09.021</pub-id></mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Lochmann</surname> <given-names>T</given-names></string-name>, <string-name><surname>Deneve</surname> <given-names>S.</given-names></string-name> <article-title>Neural processing as causal inference</article-title>. <source>CurrOpinNeurobiol</source>. <year>2011</year>;<volume>21</volume>: <fpage>774</fpage>&#x2013;<lpage>781</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.conb.2011.05.018</pub-id></mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Haefner</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Gerwinn</surname> <given-names>S</given-names></string-name>, <string-name><surname>Macke</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Bethge</surname> <given-names>M.</given-names></string-name> <article-title>Inferring decoding strategies from choice probabilities in the presence of correlated variability</article-title>. <source>Nat Neurosci</source>. <year>2013</year>;<volume>16</volume>: <fpage>2357242</fpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.3309</pub-id></mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Perin</surname> <given-names>R</given-names></string-name>, <string-name><surname>Berger</surname> <given-names>TK</given-names></string-name>, <string-name><surname>Markram</surname> <given-names>H.</given-names></string-name> <article-title>A synaptic organizing principle for cortical neuronal groups</article-title>. <source>Proc Natl Acad Sci</source>. <year>2011</year>;<volume>108</volume>: <fpage>5419</fpage>&#x2013;<lpage>5424</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1016051108</pub-id></mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Yoshimura</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Dantzker</surname>, <given-names>JLM</given-names></string-name>, <string-name><surname>Callaway</surname> <given-names>EM</given-names></string-name>. <article-title>Excitatory cortical neurons form fine-scale functional networks</article-title>. <source>Nature</source>. <year>2005</year>;<volume>433</volume>: <fpage>868</fpage>&#x2013;<lpage>873</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nature03252</pub-id></mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Dayan</surname> <given-names>P</given-names></string-name>, <string-name><surname>Hinton</surname> <given-names>GE</given-names></string-name>, <string-name><surname>Neal</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Zemel</surname> <given-names>RS</given-names></string-name>. <article-title>The Helmholtz machine</article-title>. <source>Neural Comput</source>. <year>1995</year>;<volume>7</volume>: <fpage>889</fpage>&#x2013;<lpage>904</lpage>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Nessler</surname> <given-names>B</given-names></string-name>, <string-name><surname>Pfeiffer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Buesing</surname> <given-names>L</given-names></string-name>, <string-name><surname>Maass</surname> <given-names>W.</given-names></string-name> <article-title>Bayesian Computation Emerges in Generic Cortical Microcircuits through Spike-Timing-Dependent Plasticity</article-title>. <source>PLoSComput Biol</source>. <year>2013</year>;<volume>9</volume>: <fpage>e1003037</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1003037</pub-id></mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Turrigiano</surname> <given-names>GG</given-names></string-name>, <string-name><surname>Nelson</surname> <given-names>SB</given-names></string-name>. <article-title>Homeostatic plasticity in the developing nervous system</article-title>. <source>Nat Rev Neurosci</source>. <year>2004</year>;<volume>5</volume>: <fpage>97</fpage>&#x2013;<lpage>107</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nrn1327</pub-id></mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Sengupta</surname> <given-names>B</given-names></string-name>, <string-name><surname>Stemmler</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Friston</surname> <given-names>KJ</given-names></string-name>. <article-title>Information and Efficiency in the Nervous System-A Synthesis</article-title>. <source>PLoS Comput Biol</source>. <year>2013</year>;<volume>9</volume>: <fpage>e1003157</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1003157</pub-id></mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Faisal</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Selen</surname>, <given-names>LPJ</given-names></string-name>, <string-name><surname>Wolpert</surname> <given-names>DM</given-names></string-name>. <article-title>Noise in the nervous system</article-title>. <source>Nat Rev Neurosci</source>. <year>2008</year>;<volume>9</volume>: <fpage>292</fpage>&#x2013;<lpage>303</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nrn2258</pub-id></mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Kasai</surname> <given-names>H</given-names></string-name>, <string-name><surname>Hayama</surname> <given-names>T</given-names></string-name>, <string-name><surname>Ishikawa</surname> <given-names>M</given-names></string-name>, <string-name><surname>Watanabe</surname> <given-names>S</given-names></string-name>, <string-name><surname>Yagishita</surname> <given-names>S</given-names></string-name>, <string-name><surname>Noguchi</surname> <given-names>J.</given-names></string-name> <article-title>Learning rules and persistence of dendritic spines</article-title>. <source>Eur J Neurosci</source>. <year>2010</year>;<volume>32</volume>: <fpage>241</fpage>&#x2013;<lpage>249</lpage>. doi:<pub-id pub-id-type="doi">10.1111/j.1460-9568.2010.07344.x</pub-id></mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Matsuzaki</surname> <given-names>M</given-names></string-name>, <string-name><surname>Honkura</surname> <given-names>N</given-names></string-name>, <string-name><given-names>Ellis-Davies</given-names> <surname>Gcr</surname></string-name>, <string-name><surname>Kasai</surname> <given-names>H.</given-names></string-name> <article-title>Structural basis of long-term potentiation in single dendritic spines</article-title>. <source>Nature</source>. <year>2004</year>;<volume>429</volume>: <fpage>761</fpage>&#x2013;<lpage>766</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nature02617</pub-id></mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Wiegert</surname> <given-names>JS</given-names></string-name>, <string-name><surname>Oertner</surname> <given-names>TG</given-names></string-name>. <article-title>Long-term depression triggers the selective elimination of weakly integrated synapses</article-title>. <source>Proc Natl Acad Sci USA</source>.<year>2013</year>;<volume>110</volume>: <fpage>E4510</fpage>&#x2013;<lpage>4519</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1315926110</pub-id></mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Salinas</surname> <given-names>E</given-names></string-name>, <string-name><surname>Romo</surname> <given-names>R.</given-names></string-name> <article-title>Conversion of Sensory Signals into Motor Commands in Primary Motor Cortex</article-title>. <source>J Neurosci</source>. <year>1998</year>;<volume>18</volume>: <fpage>499</fpage>&#x2013;<lpage>511</lpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Sul</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Jo</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>D</given-names></string-name>, <string-name><surname>Jung</surname> <given-names>MW</given-names></string-name>. <article-title>Role of rodent secondary motor cortex in value-based action selection</article-title>. <source>Nat Neurosci</source>. <year>2011</year>;<volume>14</volume>: <fpage>1202</fpage>&#x2013;<lpage>1208</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.2881</pub-id></mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Masamizu</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Tanaka</surname> <given-names>YR</given-names></string-name>, <string-name><surname>Tanaka</surname> <given-names>YH</given-names></string-name>, <string-name><surname>Hira</surname> <given-names>R</given-names></string-name>, <string-name><surname>Ohkubo</surname> <given-names>F</given-names></string-name>, <string-name><surname>Kitamura</surname> <given-names>K</given-names></string-name>, <etal>et al.</etal> <article-title>Two distinct layer-specific dynamics of cortical ensembles during learning of a motor task</article-title>. <source>Nat Neurosci</source>. <year>2014</year>;<volume>17</volume>: <fpage>987</fpage>&#x2013;<lpage>994</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.3739</pub-id></mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Araya</surname> <given-names>R</given-names></string-name>, <string-name><surname>Vogels</surname> <given-names>TP</given-names></string-name>, <string-name><surname>Yuste</surname> <given-names>R.</given-names></string-name> <article-title>Activity-dependent dendritic spine neck changes are correlated with synaptic strength</article-title>. <source>Proc Natl Acad Sci</source>. <year>2014</year>;<volume>111</volume>: <fpage>E28957</fpage>&#x2013;<lpage>E2904</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1321869111</pub-id></mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Caroni</surname> <given-names>P</given-names></string-name>, <string-name><surname>Donato</surname> <given-names>F</given-names></string-name>, <string-name><surname>Muller</surname> <given-names>D.</given-names></string-name> <article-title>Structural plasticity upon learning: regulation and functions</article-title>. <source>Nat Rev Neurosci</source>. <year>2012</year>;<volume>13</volume>: <fpage>478</fpage>&#x2013;<lpage>490</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nrn3258</pub-id></mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Munz</surname> <given-names>M</given-names></string-name>, <string-name><surname>Gobert</surname> <given-names>D</given-names></string-name>, <string-name><surname>Schohl</surname> <given-names>A</given-names></string-name>, <string-name><surname>Poquerusse</surname> <given-names>J</given-names></string-name>, <string-name><surname>Podgorski</surname> <given-names>K</given-names></string-name>, <string-name><surname>Spratt</surname> <given-names>P</given-names></string-name>, <etal>et al.</etal> <article-title>Rapid Hebbian axonal remodeling mediated by visual stimulation</article-title>. <source>Science</source>. <year>2014</year>;<volume>344</volume>: <fpage>904</fpage>&#x2013;<lpage>909</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1251593</pub-id></mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Matsui</surname> <given-names>A</given-names></string-name>, <string-name><surname>Tran</surname> <given-names>M</given-names></string-name>, <string-name><surname>Yoshida</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Kikuchi</surname> <given-names>SS</given-names></string-name>, <collab>U M</collab>, <string-name><surname>Ogawa</surname> <given-names>M</given-names></string-name>, <etal>et al.</etal> <article-title>BTBD3 controls dendrite orientation toward active axons in mammalian neocortex</article-title>. <source>Science</source>. <year>2013</year>;<volume>342</volume>: <fpage>1114</fpage>&#x2013;<lpage>1118</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1244505</pub-id></mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><surname>Knott</surname> <given-names>GW</given-names></string-name>, <string-name><surname>Holtmaat</surname> <given-names>A</given-names></string-name>, <string-name><surname>Wilbrecht</surname> <given-names>L</given-names></string-name>, <string-name><surname>Welker</surname> <given-names>E</given-names></string-name>, <string-name><surname>Svoboda</surname> <given-names>K.</given-names></string-name> <article-title>Spine growth precedes synapse formation in the adult neocortex in vivo</article-title>. <source>Nat Neurosci</source>. <year>2006</year>;<volume>9</volume>: <fpage>1117</fpage>&#x2013;<lpage>1124</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn1747</pub-id></mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>Yang</surname> <given-names>G</given-names></string-name>, <string-name><surname>Lai</surname>, <given-names>CSW</given-names></string-name>, <string-name><surname>Cichon</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>L</given-names></string-name>, <string-name><surname>Li</surname> <given-names>W</given-names></string-name>, <string-name><surname>Gan</surname> <given-names>W-B.</given-names></string-name> <article-title>Sleep promotes branch-specific formation of dendritic spines after learning</article-title>. <source>Science</source>. <year>2014</year>;<volume>344</volume>: <fpage>1173</fpage>&#x2013;<lpage>1178</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1249098</pub-id></mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>O&#x2019;Donnell</surname> <given-names>C</given-names></string-name>, <string-name><surname>Nolan</surname> <given-names>MF</given-names></string-name>, <string-name><given-names>van Rossum</given-names> <surname>Mcw</surname></string-name>. <article-title>Dendritic Spine Dynamics Regulate the Long-Term Stability of Synaptic Plasticity</article-title>. <source>J Neurosci</source>. <year>2011</year>;<volume>31</volume>: <fpage>161427</fpage>&#x2013;<lpage>16156</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.2520-11.2011</pub-id></mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><surname>Zheng</surname> <given-names>P</given-names></string-name>, <string-name><surname>Dimitrakakis</surname> <given-names>C</given-names></string-name>, <string-name><surname>Triesch</surname> <given-names>J.</given-names></string-name> <article-title>Network self-organization explains the statistics and dynamics of synaptic connection strengths in cortex</article-title>. <source>PLoSComput Biol</source>. <year>2013</year>;<volume>9</volume>: <fpage>e1002848</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1002848</pub-id></mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>Fauth</surname> <given-names>M</given-names></string-name>, <string-name><surname>Worg&#x00F6;tter</surname> <given-names>F</given-names></string-name>, <string-name><surname>Tetzlaff</surname> <given-names>C.</given-names></string-name> <article-title>The Formation of Multi-synaptic Connections by the Interaction of Synaptic and Structural Plasticity and Their Functional Consequences</article-title>. <source>PLoS Comput Biol</source>. <year>2015</year>;<volume>11</volume>. doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1004031</pub-id></mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><surname>Chen</surname> <given-names>BL</given-names></string-name>, <string-name><surname>Hall</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Chklovskii</surname> <given-names>DB</given-names></string-name>. <article-title>Wiring optimization can relate neuronal structure and function</article-title>. <source>Proc Natl Acad Sci USA</source>. <year>2006</year>;<volume>103</volume>: <fpage>4723</fpage>&#x2013;<lpage>4728</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.0506806103</pub-id></mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><surname>Fusi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Asaad</surname> <given-names>WF</given-names></string-name>, <string-name><surname>Miller</surname> <given-names>EK</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>X-J.</given-names></string-name> <article-title>A neural circuit model of flexible sensorimotor mapping: learning and forgetting on multiple timescales</article-title>. <source>Neuron</source>. <year>2007</year>;<volume>54</volume>: <fpage>319</fpage>&#x2013;<lpage>333</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2007.03.017</pub-id></mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Habenschuss</surname> <given-names>S</given-names></string-name>, <string-name><surname>Puhr</surname> <given-names>H</given-names></string-name>, <string-name><surname>Maass</surname> <given-names>W.</given-names></string-name> <article-title>Emergence of Optimal Decoding of Population Codes Through STDP</article-title>. <source>Neural Comput</source>. <year>2013</year>;<volume>25</volume>, <fpage>1371</fpage>&#x2013;<lpage>1407</lpage>. doi:<pub-id pub-id-type="doi">10.1162/NECO_a_00446</pub-id></mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><surname>Babadi</surname> <given-names>B</given-names></string-name>, <string-name><surname>Sompolinsky</surname> <given-names>H.</given-names></string-name> <article-title>Sparseness and Expansion in Sensory Representations</article-title>. <source>Neuron</source> <year>2014</year>;<volume>83</volume>, <fpage>1213</fpage>&#x2013;<lpage>1226</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuron.2014.07.035</pub-id></mixed-citation></ref>
</ref-list>
</back>
</article>