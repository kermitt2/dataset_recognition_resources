<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/137851</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The algorithmic neuroanatomy of action-outcome learning</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5018-1239</contrib-id>
<name>
<surname>Morris</surname>
<given-names>Richard W.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Dezfouli</surname>
<given-names>Amir</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Griffiths</surname>
<given-names>Kristi R</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Le Pelley</surname>
<given-names>Mike E</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Balleine</surname>
<given-names>Bernard W</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>School of Psychology, University of New South Wales (UNSW)</institution>, NSW, <country>Australia</country></aff>
<aff id="a2"><label>2</label><institution>Brain &#x0026; Mind Centre, University of Sydney</institution>, Camperdown, NSW, <country>Australia</country></aff>
<aff id="a3"><label>3</label><institution>ARC Centre of Excellence in Cognition and its Disorders, Macquarie University</institution>, NSW, <country>Australia</country></aff>
<aff id="a4"><label>4</label><institution>Brain Dynamics Centre, Westmead Millennium Institute</institution>, NSW, <country>Australia</country></aff>
</contrib-group>
<author-notes>
<corresp>Correspondence: Bernard Balleine Decision Neuroscience Laboratory Level 4 Matthews Building School of Psychology University of NSW Kensington, NSW 2052 Australia Tel: &#x002B;61 2 9385 1857 Email: <email>bernard.balleine@unsw.edu.au</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2017</year>
</pub-date>
<elocation-id>137851</elocation-id>
<history>
<date date-type="received">
<day>14</day>
<month>5</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>14</day>
<month>5</month>
<year>2017</year>
</date>
</history><permissions><copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="137851.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Although it is well known that animals can encode the consequences of their actions and can use this information to control action selection and evaluation, it is not known what learning rules control action-outcome (AO) learning. Here we trained participants to encode specific AO associations whilst undergoing functional imaging (fMRI) and used computational modelling to evaluate competing models. This analysis revealed that a Kalman filter, which learned the unique causal effect of each action, best characterized AO learning and found the medial prefrontal cortex differentiated the unique effect of actions from background effects. We subsequently extended these findings to show that mPFC participated in a circuit with parietal cortex and caudate nucleus to segregate distinct contributions to AO learning. The results extend our understanding of goal-directed learning and demonstrate that sensitivity to the causal relationship between actions and outcomes guides goal-directed learning rather than contiguous state-action relations.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>reinforcement learning</kwd>
<kwd>causal</kwd>
<kwd>computational neuroscience</kwd>
<kwd>fMRI</kwd>
</kwd-group>
<counts>
<page-count count="36"/>
</counts>
</article-meta>
</front>
<body>
<p>There is now considerable evidence that animals are capable of encoding the consequences of their actions and that they use that information to select, evaluate and initiate future actions.<sup><xref rid="c1" ref-type="bibr">1</xref>&#x2013;<xref rid="c5" ref-type="bibr">5</xref></sup> Although it is clear, therefore, that such learning involves encoding of the action-outcome relationship, the learning rules that govern that learning have yet to be established. Normatively, this relationship has been described by the formalism &#x0394;P (<xref ref-type="fig" rid="fig1">Figure 1A</xref>); which captures the effect of the action on outcome delivery over and above any background effects<sup><xref rid="c6" ref-type="bibr">6</xref>,<xref rid="c7" ref-type="bibr">7</xref></sup>. Nevertheless, how we distinguish these effects is unclear.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p>The action-outcome relationship in <bold>A)</bold> contingency space, where &#x0394;P = P(O&#x007C;A) &#x2013; P(O&#x007C; &#x007E; A), that is, a positive &#x0394;P exists when the conditional probability of an outcome given an action is greater than the probability of the outcome in the absence of the action (Con) while &#x0394;P approaches zero as these conditional probabilities become equal (e.g., Deg); <bold>B)</bold> Outcome-specific degradation schedule where P(O1&#x007C;A1) = P(O2&#x007C;A2) while the addition of noncontingent outcomes (O2) produces differences in &#x0394;P (i.e., &#x0394;P = 0.4 and 0 for Con and Deg, respectively); <bold>C)</bold> Signaled schedule where the cue (pink) marks the noncontingent outcomes <bold>D)</bold> The action-outcome relationships presented onscreen (counterbalanced) in the degradation test in the MRI and <bold>E)</bold> the signaled follow-up test after the MRI, where a 1-s visual cue (yellow) indicated the delivery of each noncontingent outcome</p></caption>
<graphic xlink:href="137851_fig1.tif"/>
</fig>
<p>There are, in fact, very few mechanistic models of how we and other animals detect and encode the AO contingency. Historically, models of associative learning have been proposed as an algorithmic account of causal learning, including learning about actions.<sup><xref rid="c8" ref-type="bibr">8</xref>,<xref rid="c9" ref-type="bibr">9</xref></sup> These models, and their modern varients,<sup><xref rid="c1" ref-type="bibr">1</xref></sup> assume that background conditioning plays a key role because the background competes with actions via a summed error term, to uniquely predict the outcome. More recently model-based reinforcement learning models (MBRL), which forgo a summed error-term and instead incorporate a model of the task structure, have been proposed as a general account of goal-directed action.<sup><xref rid="c4" ref-type="bibr">4</xref>,<xref rid="c10" ref-type="bibr">10</xref>,<xref rid="c11" ref-type="bibr">11</xref></sup> MBRL represents the task structure in a covariance matrix that represents the contiguity, rather than the unique causal relationship, between actions and outcomes (i.e., the state-action transitions).</p>
<p>Here we evaluated the neural computations of learning about causal actions (AO learning) in the human brain. We found a simple Kalman filter<sup><xref rid="c12" ref-type="bibr">12</xref>&#x2013;<xref rid="c14" ref-type="bibr">14</xref></sup> that combined prediction-error learning with the covariance structure of the environment explained the acquisition of AO learning better than models based on covariance or prediction-error alone. This iterative model attributed prediction-errors to different causal variables adjusted by their covariance, in order to uniquely predict the outcome. Critically, model-based fMRI revealed activity in the medial prefrontal cortex (mPFC) and the dorsal anterior cingulate cortex (dACC) tracked changes in the predictive value of actions and the background with respect to specific outcomes, separately. Furthermore, we found the mPFC participated in a network with the striatum and posterior parietal cortex to segregate the influence of different causes via their covariance, a unique prediction of the Kalman algorithm. These findings reveal, for the first time, an integrated corticostriatal network that combines prediction-errors with covariance in order to learn how our actions control our environment.</p>
<sec id="s1">
<title>Results</title>
<p>We performed two fMRI experiments to probe the computational mechanisms by which the brain learns the AO contingency and distinguishes it from any background effects. In typical neuroscience experiments, actions and their consequences are offered in discrete trials where there is no ambiguity about unique effects. Instead we used a free-operant task without a discrete trial structure, along with noncontingent outcomes, which required participants to infer the causal effects of their actions.</p>
<sec id="s1a">
<title>AO contingency degradation revealed people learned the unique effects of their actions</title>
<p>Experiment 1 involved training hungry participants with two actions for distinct food outcomes, selected before the task by each participant (e.g., button 1 = M&#x0026;Ms, button 2 = BBQ flavored crackers). Training occurred according to a &#x2018;constant probability&#x2019; schedule.<sup><xref rid="c15" ref-type="bibr">15</xref></sup> During the fMRI test, at the end of every second the schedule recorded whether a button was pressed and then presented an outcome onscreen with a conditional probability P(Oi&#x007C;Ai) = 0.2, for each action. (Actual food outcomes were provided at the end of all testing). In order to selectively degrade the causal relationship of one AO contingency while equating the reward value of both actions, the schedule also presented one of the outcomes onscreen if neither button had been pressed, i.e., P(O1&#x007C; &#x007E;A1, &#x007E;A2) = 0.2. Under this outcome-specific degradation schedule, delivery of the noncontingent outcome diminishes the reward value of both actions equally (since reward can now be obtained without taking either action). However the noncontingent outcome will selectively reduce the <italic>causal</italic> relationship of only one action (A1) and not the other (A2), because the noncontingent outcome is indistinguishable from the outcomes caused by one action, but easily distinguishable from the outcomes caused by the other action (see methods).<sup><xref rid="c16" ref-type="bibr">16</xref></sup></p>
<p><xref ref-type="fig" rid="fig2">Figure 2A</xref> illustrates that a preference for the non-degraded action (Con) clearly emerged over time as people were exposed to the differences between each AO contingency. <xref ref-type="fig" rid="fig2">Figure 2B</xref> (left panel) shows that overall the mean number of Con actions was greater than the degraded actions (Deg). Causal ratings collected at the end of each two minute block also confirmed people judged the Con action more causal than the Deg action, shown in <xref ref-type="fig" rid="fig2">Figure 2C</xref> (left panel).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Experiment 1 behavioural results (<italic>N</italic> = 30) <bold>A)</bold> Mean (shaded = SEM) probability of each action over time shows that on average the contingent action was gradually selected over the degraded action <bold>B)</bold> The mean (errorbars SEM) percent of contingent actions was significantly greater than degraded actions when free outcomes were unsignalled, paired t-test <italic>t</italic><sub>29</sub> = 4.15, &#x002A;&#x002A;&#x002A;<italic>p</italic> = .0002. However when free outcomes were signalled then degraded actions were restored, paired t-test <italic>t</italic><sub>29</sub> = 0.75, <italic>p</italic> = .46. <bold>C)</bold> Mean (errorbars SEM) causal judgments of the contingent action were greater than the degraded action when free outcomes were unsignalled, paired t-test <italic>t</italic><sub>29</sub> = 3.94, &#x002A;&#x002A;&#x002A;<italic>p</italic> &#x003C; .0004, and this difference was removed when the free outcomes were signalled, paired t-test <italic>t</italic><sub>29</sub> = 0.88, <italic>p</italic> = .39.</p></caption>
<graphic xlink:href="137851_fig2.tif"/>
</fig>
<p>The selective impact of noncontingent outcomes on actions and causal judgments reveals our sensitivity to the unique effect of our actions, even when the probability of reinforcement among actions is equal. The noncontingent outcomes made it more difficult for participants to distinguish the outcome they caused from the outcome that would have occurred anyway. As a result, the perceived causal efficacy of that action was reduced. We conducted a follow-up test after the fMRI scan, under the same contingencies, with the addition that each noncontingent outcome was now signaled by a yellow-light cue (<xref ref-type="fig" rid="fig1">Figure 1B</xref> &#x0026; D). The signal reduces the uncertainty about the noncontingent outcomes, which allows participants to once again distinguish the unique effect of their own actions. The results found the addition of the signal restored responding (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, Signaled) as well as causal judgments of the degraded action (Deg) to the same level as the Con action (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, Signaled). The restoration of actions and causal judgments by the signal implies that learning about the base-rate or &#x2018;background conditioning&#x2019; plays a key role in learning the unique causal strength of our actions.</p>
</sec>
<sec id="s1b">
<title>Action-selection reflected causal learning rather than reinforcement learning</title>
<p><xref ref-type="fig" rid="fig3">Figure 3A</xref> shows the correlation between causal judgments and causal actions for each person in Experiment 1 was high and significant, consistent with causal learning guiding action-selection rather than the frequency of reinforcement or immediate temporal effect of reward. To check our experimental control over each contingency in this free-response task, we confirmed the noncontingent outcomes selectively degraded the experienced contingency of the degraded action: post-hoc analysis revealed the mean contingencies experienced for the Con and Deg action were &#x0394;P = 0.18 and &#x0394;P = 0.07 respectively, paired t-test <italic>t</italic><sub>29</sub> = 12.06, <italic>p</italic> = .8E-12. The positive contingency (ignoring noncontingent outcomes) for the Deg action was P(O1&#x007C;A1) = 0.17, and very similar to the Con action (P(O2&#x007C;A2) = 0.18, paired t-test <italic>t</italic><sub>29</sub> = 0.90, <italic>p</italic> = .37), confirming that serendipitous differences in the positive contingency were not responsible for the results. Importantly, each person received noncontingent outcomes in each block, with the exception of one person who received noncontingent outcomes in only 4 out of 6 blocks. We also checked whether any serendipitous reward contingency existed for the Con action. <xref ref-type="fig" rid="fig3">Figure 3B</xref> (blue) shows the correlation between the number of Con actions and the total number of outcomes (contingent &#x002B; noncontingent) delivered was close to zero across participants, confirming there was no serendipitous reward contingency that may have influenced preference for the Con action. Conversely there was no negative contingency between Deg actions and total outcomes (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, red). Furthermore, the distribution of delays between each outcome and the preceding action did not differ within a 10-s interval for Con and Deg actions (<xref ref-type="fig" rid="fig3">Figure 3C</xref>), confirming that the immediate temporal contiguity with reward was not differentially influencing action-selection. Finally, pre-test preference ratings of the snack food outcomes confirmed both rewards were equally liked. The mean (95% confidence interval) ratings on a 7-point Likert scale were 5.8 CI[5.5, 6.2] and 6.3 CI[5.9, 6.6], for BBQ crackers and M&#x0026;M respectively. Thus, action-selection did not reflect any post hoc or serendipitous differences in reward contingency or contiguity.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Causality explains action selection better than reward in Experiment 1 (<italic>N</italic> = 30) <bold>A)</bold> The difference (in z-score units) in contingent over degraded actions and causal judgments among participants was correlated, <italic>F</italic><sub>1,28</sub> = 26.20, &#x002A;&#x002A;&#x002A;<italic>p</italic> = .00002 (dotted lines 95% CI). <bold>B)</bold> No correlation existed between the number of contingent actions (blue, <italic>F</italic><sub>1,28</sub> = 0.07, <italic>p</italic> = .79), or degraded actions (red, <italic>F</italic><sub>1,28</sub> = 0.30, <italic>p</italic> = .59) and the total number of outcomes among participants. <bold>C)</bold> Frequency histogram of the experienced delays between actions and reward shows the distribution was similar for both actions, Kolmogorov-Smirnov <italic>D</italic><sub>78</sub> = 0.09, <italic>p</italic> = 0.99</p></caption>
<graphic xlink:href="137851_fig3.tif"/>
</fig>
</sec>
<sec id="s1c">
<title>Modelling revealed a Bayesian prediction-error best explained AO learning</title>
<p>We simulated and fit three models: a prediction-error model with a summed prediction-error term, a MBRL model with a covariance matrix, and a Kalman algorithm with both, to determine which best explained behavioral performance in Experiment 1. The prediction-error model assumed actions and background cues competed for causal strength via a summed error-term (see methods). Simulation (Supplementary figure 1) confirmed this model resolved the unique effect of the causal action (i.e., converged to &#x0394;P). By contrast, the MBRL used a transition matrix (updated via a state prediction-error) to represent the covariance between each action, outcome and background. Simulation confirmed the covariance learned by this model was insufficient to distinguish the causal action. In particular, the reward value of the free outcomes outcompeted both actions equally in the MBRL, which did not learn or prefer the causal action. Finally, the Kalman algorithm assumed actions compete with background effects via a summed prediction-error term, however the amount learned about each is adjusted by the covariance between them. That is, when causal variables covary (i.e., positive covariance), the model cannot distinguish their separate influence and so adjusts them together. However with negative covariance then the effects can be distinguished and changes in the belief of one cause will inversely affect the other (see <xref ref-type="fig" rid="fig4">Figure 4</xref>). By combining the prediction-error term with the covariance, the Kalman filter distinguished causal actions across the widest range of parameter values (Supplementary figure 2).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><p>The Kalman algorithm changes beliefs over time according to a prediction-error adjusted for covariance <bold>A)</bold> Under a degradation schedule, belief in the action and background initially increase together as actions co-occur with outcomes. However with free outcomes, the belief in the background diverges from the action. <bold>B)</bold> Changes in the background and action occur in the same direction when covariance between the action and background is positive, but when the covariance is negative then beliefs move in opposite directions. <bold>C)</bold> Action and outcome events over time in this example</p></caption>
<graphic xlink:href="137851_fig4.tif"/>
</fig>
<p>Behavioral fitting indicated action selection was better explained by the Kalman algorithm than a prediction-error model or MBRL. Data from Experiment 1 was used to calculate the posterior probability of the Kalman algorithm, as well as a prediction-error model, a MBRL model (using a transition matrix), and a null model. The null model used the asymptote AO contingencies of each block as action values, thus it was not a learning model but a static model with no temporal dynamics. Comparison with the null model determines whether each learning model can explain how choices depend on the sequence of trial-by-trial feedback. <xref ref-type="table" rid="tbl1">Table 1</xref> shows the negative log likelihoods and relative Bayes factors of each model relative to the null. After fitting each participants&#x2019; data by maximum-likelihood estimation, the results of a likelihood ratio test (LRT) indicated all learning models predicted significantly more behavior than chance. However, the relative Bayes factor (vs the null model) shows only the optimal Kalman algorithm had a positive value, indicating only this model predicted the acquisition of causal learning over time. This model also explained more choices and the behavior of more individuals than the other models, with a Pseudo-R2 of 0.24 and a positive evidence ratio of 2 (20/10 favoring H<sub>1</sub>/H<sub>0</sub>), which were higher than the respective values for the other learning models. Thus, the majority of subjects and the total evidence favors the simple Kalman algorithm over a static model with no temporal dynamics but perfect asymptote performance. This was not the case for the optimal MBRL, or the prediction-error model (or a causal induction model, see Supplementary Materials), which all explained more variance than chance but had a negative GBF relative to the null and a PER less than 1 (<xref ref-type="table" rid="tbl1">Table 1</xref>).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><p>Model evidence and comparison scores</p></caption>
<graphic xlink:href="137851_tbl1.tif"/>
</table-wrap>
</sec>
<sec id="s1d">
<title>Model-based fMRI revealed the mPFC distinguishes causal actions from background effects</title>
<p>We evaluated whether the brain learned about causal actions, as described by the Kalman filter, by regressing the model-derived learning estimates against the image data collected in Experiment 1. Independent learning estimates for actions and background (&#x0394;AO and &#x0394;XO; see Methods) were included as parametric modulators of a stick (delta) function of response and outcome times. We included outcomes in the delta function in order to include times when the action was present as well as absent (the background was assumed to be always present). Whole-brain analysis revealed learning about actions and the background occurred in distinct regions of the mPFC (<xref ref-type="fig" rid="fig5">Figure 5A</xref> &#x0026; B). Action learning (&#x0394;AO) appeared in a medial region of the superior frontal gyrus (BA9, global peak MNI co-ordinates: -15 &#x002B;47 &#x002B;40, Z = 4.71, <italic>F</italic><sub>1,29</sub> = 37.12, FWE = .031). At the same time, learning related changes to the background estimates (&#x0394;XO) appeared in the dorsal anterior cingulate cortex (BA32, global peak MNI: -9 &#x002B;41 &#x002B;22, Z = 5.19, <italic>F</italic><sub>1,29</sub> = 50.20, FWE = .004), as well as smaller changes in the left caudate (MNI: -15 &#x002B;14 &#x002B;7, Z = 4.88, <italic>F</italic><sub><italic>1,29</italic></sub> = 41.80, FWE = .017), and cuneus (MNI: -3 -64 &#x002B;34, Z = 4.39, <italic>F</italic><sub><italic>1,29</italic></sub> = 37.28, FWE = .04). No other region survived multiple comparison correction in this whole-brain analysis.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Corticostriatal network for causal learning in Experiment 1, <italic>N</italic> = 30. <bold>A</bold>) Model-derived learning variables were tracked in the medial prefrontal cortex: Model updates to actions (<italic>&#x0394;</italic>AO) occurred in the medial prefrontal cortex (BA9) (violet voxels FWE cluster <italic>p</italic> = .007). At the same time, model updates to the background (<italic>&#x0394;</italic>XO) occurred in the dorsal anterior cingulate cortex (ACC), (blue voxels FWE cluster &#x003C; .001); <bold>B</bold>) Cut-away representation showing the spatial relationship of the corticostriatal network, including model covariance in the right posterior parietal cortex (BA40) (red voxels FWE cluster <italic>p</italic> =). <bold>C</bold>) ROI analysis in the striatum: Red voxels (image threshold <italic>p</italic> &#x003C; .05 svc) in the anterior caudate tracked the covariance, while green voxels (image threshold <italic>p</italic> &#x003C; .05 svc) in the caudate body tracked summed prediction-errors. Overlap is indicted in yellow. <bold>D</bold>) Sagittal view of ROI results. <bold>E</bold>) DCM showing the caudate integrates information from separate regions in the mPFC (dACC and BA9), modulated by the covariance between potential causes; <bold>F</bold>) An alternate DCM showing the caudate segregating information in the mPFC, modulated by the covariance between potential causes <bold>G</bold>) The probability the data supports the Segregate model (SEG) is more likely than the Integrate model (INT). <bold>H</bold>) Posterior probability of each model (INT vs SEG) generating the observed data</p></caption>
<graphic xlink:href="137851_fig5.tif"/>
</fig>
<p>The results described so far indicate the background expectancy produced by the noncontingent outcome plays a key role in learning the unique effect of our actions. According to prediction-error models (including the Kalman algorithm), this background expectancy will be violated whenever the noncontingent outcome does not follow an action. This implies that a negative AO contingency will be learned under certain conditions (i.e., inhibitory learning). For example, in Experiment 1 we explicitly arranged that O1 sometimes occurs after A1 but never after A2 (in order to equate the reward value of both actions), which results in a negative contingency between A2-O1. We tested a regressor of changes to this negative AO contingency, as learned by the Kalman algorithm, in the whole-brain. BOLD activity in a ventral medial prefrontal region, including the anterior cingulate (BA32) and medial orbitofrontal cortex (BA10), learned the negative AO contingency, global peak MNI: -3 &#x002B;50 -11, Z = 3.52, <italic>F</italic><sub><italic>1,29</italic></sub> = 18.08, FWE = .011 (Supplementary figure 3). These imaging results are consistent with recent reports in rodents that the medial orbitofrontal cortex is critical for learning about unobserved outcomes, and in particular a negative AO contingency.<sup><xref rid="c17" ref-type="bibr">17</xref></sup></p>
</sec>
<sec id="s1e">
<title>Model-based fMRI showed the posterior parietal cortex tracks covariance between causes</title>
<p>The results so far indicate causal actions are distinguished from the background in the mPFC. A unique feature of the Kalman filter is that the covariance term distinguishes the influence of candidate causes by updating &#x0394;AO and &#x0394;XO together when the covariance is positive and updating them in opposite directions when the covariance is negative (<xref ref-type="fig" rid="fig4">Figure 4</xref>). We tested whether any brain regions tracked the covariance between actions and background by entering the covariance values as a parametric modulator. A whole-brain analysis revealed bilateral activity in posterior parietal cortex (BA40) was significantly associated with the covariance term, left global peak MNI: -57 -55 &#x002B;37, <italic>Z = 5.83, F</italic><sub><italic>1,29</italic></sub> = 72.78, FWE &#x003C; .001 and right peak MNI: &#x002B;42 -67 &#x002B;43, Z = 5.34, <italic>F</italic><sub><italic>1,29</italic></sub> = 54.67, FWE = .002 (<xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p>
</sec>
<sec id="s1f">
<title>ROI analysis showed prediction-error and covariance converge in caudate</title>
<p>Substantial evidence exists that the ventral striatum tracks or receives reward prediction-errors,<sup><xref rid="c18" ref-type="bibr">18</xref>&#x2013;<xref rid="c20" ref-type="bibr">20</xref></sup> while dorsal striatal regions track action values.<sup><xref rid="c18" ref-type="bibr">18</xref></sup> In the present example of AO learning, the prediction-error represents the deviations between the observed outcome and the summed total causal expectancy (action &#x002B; background). We tested whether the striatum tracks this summed error term, by including it as a parametric modulator in an anatomical ROI analysis of the striatum. <xref ref-type="fig" rid="fig5">Figure 5C</xref> &#x0026; 5D shows BOLD responses in a posterior region of the caudate body (green) tracked the summed errors (ROI peak MNI:: &#x002B;15 &#x002B;11 &#x002B;4, <italic>Z</italic> = 4.17, <italic>t</italic><sub>29</sub> = 4.94, FWE = .002 svc) while activity in the anterior caudate (red) was associated with the covariance between actions and background (ROI peak MNI: -15 &#x002B;23 &#x002B;7, <italic>Z</italic> = 3.42, <italic>t</italic><sub>29</sub> = 3.84, FWE = .029 svc) These regions were more medial and dorsal than those implicated in reward prediction-error signals but similar to regions implicated in instrumental learning.<sup><xref rid="c18" ref-type="bibr">18</xref></sup> Thus, the caudate appears to receive sufficient information to segregate the influence of different events and may play an important role in selectively distinguishing causal actions from background effects.</p>
</sec>
<sec id="s1g">
<title>DCM revealed the caudate segregates the effect of the action from background effects</title>
<p>To further determine the caudate&#x2019;s role in distinguishing control, we performed a dynamic causal model (DCM) analysis.<sup><xref rid="c21" ref-type="bibr">21</xref></sup> We tested two possibilities shown in <xref ref-type="fig" rid="fig5">Figure 5E</xref> &#x0026; 5F. In Model 1 the caudate is a site of convergence of the updated values from the prefrontal cortex to enable action-selection. In Model 2 the caudate segregates the prediction-error to update the estimates of the action and background separately. Bayesian model selection revealed the relative log-evidence for model 2 was 85.31, which corresponds to strong evidence in favor of segregation. A random-effects analysis (<xref ref-type="fig" rid="fig5">Figure 5G</xref> &#x0026; 5H) revealed a large majority of participants were significantly more consistent with the segregate model than the integration model (the exceedance probability was 99 percent), and it was more likely to be true for any random subject (the posterior probability of the segregate model was 80 percent).</p>
</sec>
<sec id="s1h">
<title>PPI showed cortex and caudate interact when causal actions must be distinguished by their covariance</title>
<p>Experiment 2 replicated the key fMRI results in an independent sample of naive participants, using a design that allowed us to assess the effect of distinguishing the free outcomes on the corticostriatal network we identified above. The experiment used a single AO contingency and varied whether or not the free outcomes were distinguishable from the earned outcomes in a block-by-block fashion, to test the interaction between causality and caudate activity. For half the blocks we used the same outcome for both free and earned outcomes (i.e., as in Experiment 1), while in the other half the earned outcomes were different from the free outcomes. Using distinct outcomes in half the blocks allowed the participant to discern the causal effect of their actions (i.e., equivalent to signaling the free outcomes, as in the follow-up to Experiment 1). <xref ref-type="fig" rid="fig6">Figure 6A</xref> shows that degradation reduced total actions and causal ratings, however providing distinct free outcomes restored causal actions and judgments. As before, we fitted the Kalman algorithm to the data using maximum likelihood estimation. The optimal model predicted significantly more choices than chance, the mean group average likelihood per trial was 57 percent (95% CI: 53-60). A functional ROI (fROI) analysis using masks generated from the significant results in the mPFC and caudate of Experiment 1 confirmed that learning about the background (&#x0394;XO) occurred in the same dorsal ACC region (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, blue), ROI peak MNI: -3 &#x002B;36 &#x002B;38, <italic>Z</italic> = 3.94, <italic>t</italic><sub>19</sub> = 5.06, FWE = .02. Meanwhile learning values for the action (&#x0394;AO) occurred in the same region of BA9 (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, violet), ROI peak MNI: -2 &#x002B;47 &#x002B;46, <italic>Z</italic> = 3.04, <italic>t</italic><sub>19</sub> = 3.56, <italic>p</italic> = .001. Covariance between the action and background was tracked in the caudate (<xref ref-type="fig" rid="fig6">Figure 6C</xref>, right), ROI peak MNI: -12 &#x002B;20 &#x002B;1, <italic>Z</italic> = 3.91, <italic>t</italic><sub>19</sub> = 5.01, FWE = .002. We used a whole-brain PPI analysis to determine whether any cortical regions interacted with the caudate when free outcomes were indistinguishable from earned outcomes. A single region in the right parietal junction interacted with the caudate when free outcomes were the same (versus different), shown in <xref ref-type="fig" rid="fig6">Figure 6E</xref>, global peak MNI: &#x002B;54 - 58 &#x002B;30, <italic>Z</italic> = 4.68, <italic>F</italic><sub><italic>1,19</italic></sub> = 24.90, FWE = .01. This region overlapped with the right posterior parietal cortex (BA40) identified in Experiment 1 (<xref ref-type="fig" rid="fig6">Figure 6E</xref>, red). Together these results implicate the posterior parietal cortex in tracking the covariance with the background, and interacting with the caudate when the unique effect of our actions must be distinguished from the background.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6.</label>
<caption><p>Experiment 2 results (<italic>N</italic> = 20) <bold>A)</bold> Mean (errorbar SEM) total responses were signficantly higher when the free outcomes were different from earned outcomes, outcome main effect <italic>F</italic><sub>1,19</sub> = 12.57, <italic>p</italic> = .02. This difference decreased as &#x0394;P increased, outcome by contingency interaction <italic>F</italic><sub>1.5,38</sub> = 7.24, <italic>p</italic> = .005. <bold>B)</bold> Mean (errorbar SEM) causal judgments were higher when free outcomes were different from earned outcomes (<italic>F</italic><sub>1,19</sub> = 33.82, <italic>p</italic> = .6E-4) and this difference decreased as &#x0394;P increased, interaction <italic>F</italic><sub>2,38</sub> = 10.07, <italic>p</italic> = .002. <bold>C)</bold> Updates to the action &#x0394;AO occurred in the BA9 fROI (violet, image threshold <italic>p</italic> &#x003C; .001 svc) while updates to the background &#x0394;XO occurred in the dorsal ACC fROI (blue, image threshold <italic>p</italic> &#x003C; .001 svc) and the covariance was tracked in the caudate fROI (green, image threshold p &#x003C; .001 svc). <bold>D)</bold> Illustrative results from a single subject showing the caudate and posterior parietal cortex interacted with the causal condition <bold>E)</bold> Right parietal junction activity interacted with caudate activity when noncontingent outcomes were indistinguishable from contingent outcomes (covariance from Experiment 1 shown in red for comparison), image thresold <italic>p</italic> &#x003C; .001 unc.</p></caption>
<graphic xlink:href="137851_fig6.tif"/>
</fig>
</sec>
</sec>
<sec id="s2">
<title>Discussion</title>
<p>We sought to establish the learning rules that govern AO learning in instrumental conditioning and their neural bases. We found that the medial prefrontal cortex participates in a circuit that detects and segregates the unique causal effects of our actions from other background effects, and more importantly, that this segregation was generated by a Bayesian prediction-error, described by a Kalman filter, which uses a summed prediction-error term along with the covariance between potential causes to distinguish the unique effect of actions from background effects. Furthermore, the caudate appears to be a key point of integration of the covariance term and prediction-error; it segregates the summed prediction-error into separate update values for each causal belief. Thus, this model represents a simple, iterative Bayesian model of change, that unlike other computational-level models,<sup><xref rid="c22" ref-type="bibr">22</xref></sup> provides an algorithmic account of AO learning that can be instantiated in the neural code.<sup><xref rid="c23" ref-type="bibr">23</xref></sup></p>
<p>Many results have emphasized the critical role of the medial prefrontal cortex in AO learning, however the exact nature of this role has been unspecified. Indeed, there is a wealth of evidence that in the rat, the prelimbic region of the medial prefrontal cortex is critical for the acquisition of goal-directed actions.<sup><xref rid="c2" ref-type="bibr">2</xref>,<xref rid="c24" ref-type="bibr">24</xref>&#x2013;<xref rid="c29" ref-type="bibr">29</xref></sup> Computational models of medial prefrontal cortex function in humans such as the PRO model<sup><xref rid="c1" ref-type="bibr">1</xref></sup> assume the <italic>dorsal anterior cingulate</italic> signals negative prediction-errors during AO learning, consistent with other prediction-error models.<sup><xref rid="c8" ref-type="bibr">8</xref></sup> Such models only offer a partial explanation of the results we observed here: The update to the background effect (&#x0394;XO) represents the learned probability of the noncontingent outcome that must be adjusted against the probability of the contingent outcome. This is consistent with negative prediction-errors when positive changes to the background (&#x002B;&#x0394;XO) occur in the context of a negative covariance term, and so result in negative changes to the AO contingency (&#x2013;&#x0394;AO). At other times, positive changes to the background probability (&#x002B;&#x0394;XO) will occur in line with changes to the AO contingency, consistent with positive prediction-errors. This occurs when the covariance is positive, for instance, in situations in which we cannot distinguish the unique effect of our actions from potential background causes.</p>
<p>Our results also distinguished a separate region of the mPFC, near the medial surface of BA9, whose activity represented updates to the unique effect of the action (&#x0394;AO) and replicated the involvement of the mPFC in an independent sample, highlighting the reliability of the current findings (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). We further showed using PPI (<xref ref-type="fig" rid="fig6">Figure 6D</xref> &#x0026; E) that the caudate interacts with the parietal cortex when free outcomes are indistinguishable from the earned outcome (i.e., when there was no signal or the free outcome was the same as the earned outcome). The selective interaction between parietal cortex and caudate arises under the additional demands when there is no observable information to distinguish control. When noncontingent outcomes are indistinguishable, the covariance between our actions and the background is the only information that can be used to distinguish them. The right posterior parietal junction identified in the PPI was the same cortical region identified in Experiment 1 as tracking the covariance term (along with the caudate in the subcortex). It also overlaps with the cortical region previously implicated in learning the transition matrix during model-based reinforcement learning,<sup><xref rid="c30" ref-type="bibr">30</xref></sup> suggesting, across studies and laboratories, that this region represents the covariance structure of the environment. While PPI does not indicate the direction of influence, these results are consistent with a network extending from the parietal cortex to caudate to mPFC, which tracks the covariance between actions and other events and then segregates the error-term to learn about and distinguish between the influence of different causes.</p>
<p>We found the competitive allocation of causal belief to actions relative to the background is a form of selective learning closely related to cue-competition models in associative learning.<sup><xref rid="c8" ref-type="bibr">8</xref></sup> An important difference is the covariance matrix of the Kalman filter, in which the off-diagonal terms track the covariation between events. In our model, the covariance allowed the learner to distinguish or segregate the effects of an action from the background in the absence of that action, as shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>. This allowed the model to reason counterfactually about what would have happened if an action had not occurred. In this manner, the covariance is analogous to heuristically motivated formalizations of within-event learning (e.g., negative alpha) which allows learning about absent events in recent versions of cue-competition models.<sup><xref rid="c31" ref-type="bibr">31</xref>,<xref rid="c32" ref-type="bibr">32</xref></sup> Our simple Kalman filter thus combines key features of contemporary associative learning and model-based reinforcement learning.</p>
<p>These results also make an important contribution to the common claim that goal-directed learning is analogous to MBRL. In general, MBRL is concerned with building a model of the environment, given the state caused by each action (i.e., the covariance or transition matrix). In such models, &#x201C;state prediction-errors&#x201D; and the covariance matrix they update<sup><xref rid="c30" ref-type="bibr">30</xref></sup> only describe the contiguity between states and, as a result, the covariance matrix cannot learn or accurately represent a causal relationship. By contrast, causal learning is not concerned with the transition probabilities between different states, but rather the trade-offs between competing contingencies to determine whether that state was cause by an action or not. This is a primary difference between causal models and MBRL. The question of which action caused which state is arguably more fundamental to goal-directed learning. For example, the prediction-errors we observed here are critically different from &#x201C;state prediction-errors&#x201D; because they are adjusted for the probability that another state (the background) may also cause the outcome. This adjustment leads to a representation of the unique causal strength of our actions. Hence, an important implication of this proposal is that MBRL <italic>per se</italic> may be sufficient for maximizing reward but it does not provide a complete account of goal-directed learning since it is unable to calculate, and so is insensitive to, the causal relationship between actions and states.</p>
<p>Unlike some other computational models of causal learning, the causal estimates learned by our Kalman filter converge to &#x0394;P, a normative measure of causal strength. Other researchers have argued that &#x0394;P does not provide the best approximation of human causal inference because changing the base-rate probability of an outcome while holding &#x0394;P constant modulates causal judgements (e.g., &#x201C;the base-rate illusion&#x201D;). However the base-rate illusion is considerably weaker in free-response, instrumental learning where trials are not explicitly segmented.<sup><xref rid="c33" ref-type="bibr">33</xref>&#x2013;<xref rid="c36" ref-type="bibr">36</xref></sup> Furthermore, when learning about causal effects, active intervention is a more reliable guide to causal relations than is sheer observation, largely because actions constitute one basic way to control for possible alternative causes.<sup><xref rid="c37" ref-type="bibr">37</xref>&#x2013;<xref rid="c40" ref-type="bibr">40</xref></sup> Humans are able to reason suppositionally or counterfactually about what would be expected to happen if an intervention is made or not made, and midbrain dopamine neuron firing<sup><xref rid="c41" ref-type="bibr">41</xref></sup> along with our Bayesian model reflects these counterfactual action values. For these reasons, AO learning may not suffer the same biases as other forms of causal learning that are based on passive observation.</p>
<p>In conclusion, learning about the causal effects of our actions, as required for goal-directed learning and as investigated here, appears to reflect features of traditional associative models such as competition for predictive value, as well as modern conventions such as environmental structure (covariance). In our hands, these features were combined in a highly simplified, iterative Kalman filter that learned a probability distribution over action-outcome contingencies to provide a novel account of AO learning. In our results there was impressive agreement across experiments and replications that distinct regions of a corticostriatal network distinguished the unique causal effect of actions from those of the background. More generally, the results revealed how our neuroanatomy performs Bayesian computations, consistent with growing evidence that the brain learns and make decisions on the basis of probability distributions.<sup><xref rid="c23" ref-type="bibr">23</xref>,<xref rid="c42" ref-type="bibr">42</xref>&#x2013;<xref rid="c44" ref-type="bibr">44</xref></sup></p>
</sec>
<sec id="s3">
<title>Methods</title>
<sec id="s3a">
<title>Participants</title>
<p>In Experiment 1, 31 right-handed English speaking volunteers, aged between 19 and 51 (mean age 30.5) were scanned. One participant was removed due to excessive head movement (&#x003E; 2 mm), thus n = 30 (18 females). On the basis of a power analysis (Supplementary Material), scans from 23 right-handed, English speaking volunteers, aged between 17 and 32 (mean age 26) were considered for Experiment 2. Three participants were removed due to excessive head movement (&#x003E; 2 mm), thus <italic>n</italic> = 20 (11 females). All participants were free of food allergies, neurological or psychiatric disease, and psychotropic drugs, and reported strong liking of the snack foods we provided as reward. Informed consent to participate was obtained and the study was approved by the Human Research Ethics Committee at the University of Sydney (HREC no. 12812). Participants were reimbursed &#x0024;45 AUD in shopping vouchers, in addition to the snack foods they earned during the test session.</p>
</sec>
<sec id="s3b">
<title>AO contingency degradation task</title>
<p>In each experiment, participants were instructed not to eat three hours prior the appointment. Pre-testing involved obtaining preference ratings on a 7-point scale for each of three snacks (M&#x0026;Ms, BBQ flavored crackers, chocolate cookies), from which the two most similarly preferred snacks were selected for the experiment.</p>
<p>Experiment 1 involved learning two AO contingencies concurrently. Participants were instructed they could liberate snack foods (BBQ flavored crackers and M&#x0026;Ms) from a vending machine by tilting it to the left or right (by pressing either a left or right button), and that sometimes the vending machine would also release a snack for free. They were also instructed to find the best action for releasing snacks. Outcomes were indicated by the presentation of a visual stimulus depicting the snack for 1-s duration (a particular snack food, e.g., M&#x0026;M or BBQ cracker), during which further outcomes could not be earned. The relationship between actions and outcomes were constant across blocks for each participant (e.g., left = M&#x0026;M and right = BBQ crackers for all blocks). Each block lasted 120-s, and the software controlling the task PsychoPy2 v1.8, <sup><xref rid="c45" ref-type="bibr">45</xref>,<xref rid="c46" ref-type="bibr">46</xref></sup> divided each block into 120 one-second intervals to determine the outcome rate. Participants were unaware of the 1-s intervals, and they responded freely using the index finger on their right hand to press the left or right button on a Lumina MRI-compatible response pad (LU-400, Cedrus Corporation, CA). An action (tilt left or tilt right) earned a particular outcome with a probability <italic>P</italic> = 0.2 if that action had occurred in the preceding 1-s interval. If both actions occurred in the preceding interval then only the most recent action was considered for reinforcement. A free outcome was delivered with <italic>P</italic> = 0.2 if neither action had been made. This schedule ensured two important features: 1) that there was no serendipitous contingency between an action and a free outcome, which would result in a higher reward contingency for the contingent action<sup><xref rid="c16" ref-type="bibr">16</xref></sup>, and 2) the earned outcome appeared at a varying interval up to one second after a successful action, which is sufficient to introduce ambiguity into the perceived AO contingency.<sup><xref rid="c47" ref-type="bibr">47</xref></sup> Participants completed six blocks; the outcome (BBQ cracker or M&#x0026;M) that was subject to contingency degradation was counterbalanced across blocks (ABBAAB). At the end of each block, participants rated how causal each action was with respect to each outcome on a Likert scale from 1 (not at all) to 7 (very causal). A follow-up test was conducted after the scan. The test setting, duration and programmed AO contingencies in the follow-up test were exactly the same as in the scanner, with the addition of a 1-s yellow light cue displayed on the front of the virtual vending machine immediately prior to the delivery of each free reward. At the end of all testing, participants received all snacks that had been delivered onscreen during test.</p>
<p>Experiment 2 involved learning a single AO contingency. The session was arranged in 12 blocks of 60-s duration, and in each block the participant responded freely for a single snack food reward (counterbalanced between BBQ crackers and M&#x0026;Ms). As before, in each block the positive contingency was <italic>P</italic>(O&#x007C;A) = 0.2 in every second a response was made. The probability of a free outcome in every second when no response was made, i.e., <italic>P</italic>(O&#x007C;&#x007E;A), varied between 0, 0.1 and 0.2 across blocks in a counterbalanced order. Conversely, &#x0394;P varied from 0.2, 0.1 to 0. In addition, Experiment 2 varied whether the free outcome was the same snack or a different snack as the earned outcome in each block, in an ABBA order. In half the blocks the earned and free outcomes were different which effectively signaled the free outcomes and allowed the participant to discern the causal effect of their actions. For the other half of blocks the free outcomes were the same snack food as the earned outcome, thus making it difficult to discern unique causal effects.</p>
</sec>
<sec id="s3c">
<title>Behavior data analysis</title>
<p>The behavioral data consisted of the rate of responding during each block and the causal ratings obtained at the end each block. Experiment 1 tested for differences between Con and Deg actions in the proportion of total responses, as well as mean causal ratings. In each case, a Shapiro-Wilk test confirmed the data did not violate the assumption of normality and differences were assessed by paired t-tests (two-tailed). Experiment 2 tested the main effect of the outcome condition (Same versus Different free outcomes) and its linear interaction with the contingency condition (&#x0394;P = 0.2, 0.1, and 0.0), using a 2 &#x00D7; 3 repeated measures ANOVA (two-tailed). Mauchly&#x2019;s test was used to detect violations of sphericity, in which case the Greenhouse-Geisser correction was applied.</p>
</sec>
<sec id="s3d">
<title>Model-based analysis</title>
<p>For each of the learning models described below, the real-time occurrence of outcomes was modelled with a logistic function f(x) = 1 / (1 &#x002B; e<sup>&#x221E; (D-k)</sup>) to produce a binary result (0,1) determining whether the outcome will or will not be associated with the prior action. D is the delay between the previous action and outcome, and k is the temporal threshold included as a free parameter in the model-fitting described below.</p>
<sec id="s3d1">
<title>Prediction-error learning</title>
<p>The prediction-error model adopted a standard delta rule exemplified in the Rescorla-Wagner model<sup><xref rid="c8" ref-type="bibr">8</xref></sup> and adapted for vector-valued predictions in modern reincarnations.<sup><xref rid="c1" ref-type="bibr">1</xref></sup> This allows multiple action outcomes to be predicted simultaneously, each with its own summed error-term. In this model, the predicted outcome <bold>&#x00D4;</bold> is a weighted sum of actions and background cues (<bold>&#x00D4;</bold> = <italic>V</italic><bold>a</bold>). Updates to the weights (<italic>V)</italic> occur by the prediction error:
<disp-formula id="eqn1">
<alternatives>
<graphic xlink:href="137851_eqn1.gif"/>
</alternatives>
</disp-formula>
where &#x03B1; is a free parameter controlling the learning rate. In this way, the model replicates the prediction-error learning of the Kalman algorithm below, but without adjustment by a covariance matrix so all changes are restricted to actions on the current trial.</p>
</sec>
<sec id="s3d2">
<title>Model-Based Reinforcement Learning</title>
<p>The MBRL model was adapted from the FORWARD model described in Glascher et al (2010), which uses experience with state transitions to update an estimated matrix of transition probabilities. The transition matrix (<italic>T</italic>) held the current estimate of the probability of transitioning from action <bold>a</bold> (a binary vector indicating one of three possibilities: make action A1, make action A2, or Wait) to an outcome state <bold>o</bold> (a binary vector indicating one of three possibilities: outcome O1 delivered, outcome O2 delivered, or no outcome delivered [background state]). Wait actions occurred at the end of every second in which no other action occurred. In the <italic>T</italic> matrix, the different actions were represented in different columns, while the different outcomes were represented in different rows. The transitions were initialized to uniform distributions connecting each action and outcome. Upon each step, having taken action <bold>a</bold> and arrived in outcome state <bold>o</bold>, the FORWARD learner estimates the expected outcomes on the basis of the current transition matrix (<bold>&#x00D4;</bold> = <italic>T</italic><bold>a</bold>), and computes a state prediction-error <italic>&#x0394;T</italic>:
<disp-formula id="eqn2">
<alternatives>
<graphic xlink:href="137851_eqn2.gif"/>
</alternatives>
</disp-formula></p>
<p>Updates to the transition matrix <italic>T</italic> of the observed transition occur via &#x0394;<italic>T</italic>
<disp-formula id="eqn3">
<alternatives>
<graphic xlink:href="137851_eqn3.gif"/>
</alternatives>
</disp-formula>
where <italic>i</italic> is the column corresponding to the taken action.</p>
</sec>
<sec id="s3d3">
<title>Kalman algorithm</title>
<p>The aim of the Kalman algorithm was to learn the unique causal weight of each action over and above the background (i.e., &#x0394;P). The algorithm builds a probabilistic representation of the causal weights (<bold>w</bold>) of each input (actions and background cues) predicting each outcome (<bold>o</bold>), representing causal beliefs. The causal beliefs are represented by a multivariate normal density <italic>N</italic>(<bold>w</bold>&#x007C;<bold>&#x00B5;</bold><italic>,C</italic>) with a prior Gaussian distribution, and after observing each outcome the causal weights are updated by changes in the mean and variance of each distribution (see below). The mean <bold>&#x00B5;</bold> represents the belief in the unique causal strength while the variance <italic>C</italic> captures the uncertainty around that belief. When the variance is large, there is large uncertainty regarding the true causal strength. The updating equations for the mean and variance of the causal weight have the following form:
<disp-formula id="eqn4">
<alternatives>
<graphic xlink:href="137851_eqn4.gif"/>
</alternatives>
</disp-formula>
<disp-formula id="eqn5">
<alternatives>
<graphic xlink:href="137851_eqn5.gif"/>
</alternatives>
</disp-formula>
where <italic>v</italic> is a free parameter capturing outcome variance. For each second, the execution of an action and the constant background context are represented as a binary input vector (<bold>a</bold>) and an outcome vector (<bold>o</bold>) represents the delivery of the outcomes (O1 and O2). The first term (<italic>v</italic> &#x002B; <bold>a<sup>T</sup></bold><italic>C</italic><bold>a</bold>)<sup>&#x2212;</sup><sup><xref rid="c1" ref-type="bibr">1</xref></sup> represents the total certainty (inverse sum of outcome uncertainty and belief uncertainty) and it governs the learning rate. <bold>&#x00B5;<sup>T</sup>a</bold> is a vector representing the learned causal weights on the basis of the current inputs (<bold>a</bold>). The difference between the observed outcome and learned causal weights (<bold>o</bold> &#x2013; <bold>&#x00B5;<sup>T</sup>a</bold>) is a vector of outcome-specific prediction errors, each of which represents a summed error-term for O1 and O2. The rightmost term <italic>C</italic><bold>a</bold> is the product of the covariance matrix and input vector, and it allows for changes to the mean belief about actions otherwise correlated with the background context but absent on the present trial. In this manner, the Kalman filter is able to distinguish the unique influence of actions from the context during noncontingent outcomes. Importantly, the covariation between each action and the background are tracked in the off-diagonal elements of <italic>C</italic>, which allowed us to test a unique prediction of this model. Thus changes to the mean beliefs (&#x0394;<bold>&#x00B5;</bold>) depend on the prediction-error as well as the covariance matrix <italic>C</italic>.</p>
</sec>
<sec id="s3d4">
<title>Null model</title>
<p>For comparison, we also described a null model without any temporal dynamics but ideal asymptote performance. The null model assumed that the probability of taking each action was proportional to the final <italic>&#x0394;P</italic> obtained in each block, so <italic>Q</italic><sub>right</sub> = <italic>&#x0394;P</italic><sub>right</sub> and <italic>Q</italic><sub>left</sub> = <italic>&#x0394;P</italic><sub>left</sub>.</p>
</sec>
<sec id="s3d5">
<title>Policy</title>
<p>The policy of each model was the same. In each learning model, each action had a unique causal relationship with two outcomes representing the belief regarding that particular AO contingency (e.g., <bold>&#x00B5;</bold><sub><italic>a,o</italic></sub>). So for each action, we selected the highest causal belief associated with that action <italic>Q</italic><sub>a</sub> = arg max<sub>a</sub> <bold>&#x00B5;</bold><sub><italic>a,o</italic></sub> and then used it to probabilistically explain the action choices of each participant using the softmax rule:
<disp-formula id="eqn6">
<alternatives>
<graphic xlink:href="137851_eqn6.gif"/>
</alternatives>
</disp-formula></p>
</sec>
<sec id="s3d6">
<title>Bayesian model comparison</title>
<p>We generated observation models based on the three learning models described above as well as the null model, and fit them to each subject&#x2019;s behaviour separately using maximum-likelihood estimation. A non-linear optimization was achieved using the fmincon function in MATLAB R2014B (The Mathworks Inc., MA, USA) over 100 random starting values for each subject. We measured the overall goodness of fit using the average likelihood per trial of the best fit model for each subject. The average likelihood per trial was calculated as the exponent of the sum of log likelihoods divided by the number of trials (responses) for each subject. We compared models by aggregating the probability of the data given the model over each subject&#x2019;s fit (single-subject Bayesian Information Criterion [BIC] score) to estimate the model evidence for the full dataset. The aggregate for each model was then compared to compute a Group Bayes Factor (GBF). We also report the number of subjects for whom individual model comparison gave the same answer as the GBF and the positive evidence ratio (PER), where &#x201C;positive&#x201D; evidence for one model versus another exists if the log Bayes factor is larger than three (Kass &#x0026; Raftery, 1995).</p>
</sec>
</sec>
<sec id="s3e">
<title>Image data analysis</title>
<p>MRI data were acquired on a 3-Tesla GE Discovery using a 32-channel head coil. A T1-weighted high-resolution structural scan was acquired for each subject for screening and registration with a 1-mm<sup><xref rid="c3" ref-type="bibr">3</xref></sup> voxel resolution (TR: 7200 ms, TE: 2.7 ms, 176 sagittal slices, 1-mm thick, no gap, 256 &#x00D7; 256 &#x00D7; 256 matrix). For BOLD acquisition, we acquired echo planar image (EPI) volumes comprising 52 axial slices in an ascending interleaved acquisition order (TR: 2910 ms, TE: 20 ms, FA: 90 degrees, FOV: 240 mm, matrix: 128 &#x00D7; 128, acceleration factor: 2, slice gap: 0.2 mm) with a voxel resolution of 1.88 &#x00D7; 1.88 &#x00D7; 2.0 mm. Slices were angled 15 degrees from AC-PC to reduce signal loss in the OFC. In Experiment 1, 343 EPIs were acquired while in Experiment 2, 260 EPIs were acquired.</p>
<p>Data were analysed using SPM8 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">www.fil.ion.ucl.ac.uk/spm</ext-link>). Preprocessing and statistical analysis were conducted separately for each experiment. The first four images were automatically discarded to allow for T1 equilibrium effects, then images were slice time corrected to the middle slice and realigned with the first volume. The structural image was coregistered to the mean functional image, segmented and warped to MNI space. The warp parameters were then used to normalise the resampled functional images (2 mm<sup>3</sup>). Images were then smoothed with a Gaussian kernel of 8-mm full-width half maximum to improve sensitivity for group analysis.</p>
</sec>
<sec id="s3f">
<title>Model-based fMRI analysis</title>
<p>For each first-level GLM analyses, we constructed a vector of delta values for action causal beliefs (&#x0394;AO) and background causal beliefs (&#x0394;XO), generated with the parameters provided by the group maximum likelihood estimation (MLE).<sup><xref rid="c48" ref-type="bibr">48</xref></sup> For &#x0394;AO, the delta values were taken for the current action contingency (i.e., A1-O1 or A2-O2), while for &#x0394;XO the delta values were taken for the current outcome (B-O1 or B-O2). To test for brain activity tracking the unique changes in each vector, we entered &#x0394;AO and &#x0394;XO as parametric modulators of a stick function that included both response and reward times in an event-related design. While these update signals will fluctuate independently, there will be some collinearity when the covariance is zero. Collinearity is a problem when trying to determine unique effects associated with each regressor. However, the variance inflation factor can be used to indicate if a collinearity problem is present. The variance inflation factor was 1.23, which is within the bounds of a conservative threshold &#x003C; 5.<sup><xref rid="c49" ref-type="bibr">49</xref></sup> Nevertheless, to remove any residual collinearity between these regressors, each regressor was entered as the second modulator to ensure it was adjusted for the prior regressor using the default orthogonalize routine in SPM.<sup><xref rid="c49" ref-type="bibr">49</xref></sup> Each GLM also contained rating periods and six movement parameters. Betas were estimated with a 128-s high-pass filter and AR1 correction for auto-correlation. The resulting beta images were included in a group-level random effects analysis in SPM one-sample t-tests. SPM F-contrasts (two-tailed) were used to create whole-brain statistical parametric maps, corrected for multiple comparisons using a voxel-level FWE-p &#x003C; .05. SPM t-contrasts (one-tailed) were used in each ROI analysis, corrected for multiple comparisons using FWE (svc) in the case of anatomical ROIs (Experiment 1) and uncorrected at <italic>p</italic> &#x003C; .001 (svc) in the case of independent functional ROIs (Experiment 2).</p>
<sec id="s3f1">
<title>DCM analysis</title>
<p>Each of the volumes-of-interest (VOI) for the DCM analysis was spatially defined according to the group results of the relevant GLM analysis. The BA9 VOI was defined by the significant cluster from the analysis of &#x0394;AO in the group results. This significant group cluster was used to construct a binary mask and this mask was then used to define the VOI and extract the first eigenvector for all individuals, adjusted for the &#x0394;AO and &#x0394;XO regressors. All subthreshold voxels within the mask were included which were <italic>p</italic> &#x003C; .5 (uncorrected), which roughly corresponds to all voxel activity positively related to &#x0394;AO. The mPFC VOI was extracted in the same manner but using the significant cluster from the analysis of &#x0394;XO in the group analysis. The caudate VOI was defined by a group ROI analysis of press rate restricted to the striatum (<italic>p</italic> &#x003C; .05, small volume corrected), with a single cluster of 48 voxels in the right caudate (peak MNI: &#x002B;15 &#x002B;10 &#x002B;6), and otherwise extracted in the same manner as other VOIs.</p>
</sec>
<sec id="s3f2">
<title>PPI analysis</title>
<p>The psychological term was the block condition, whether or not the free rewards were distinguishable within that block. The physiological term was the timeseries from the anterior caudate in each participant (n = 20) using a group fROI mask from the GLM analysis of the covariance. We constructed the interaction term in SPM8 (per defaults) and included all three terms in the first-level GLM. Finally we tested for regions of interaction in the whole-brain, corrected for multiple comparisons FDR -<italic>q</italic> &#x003C; .05.</p>
</sec>
</sec>
<sec id="s3g" sec-type="availability">
<title>Data availability</title>
<p>The analyses in this report were conducted by RWM (unblinded). Data is available upon request. Unthresholded statistical maps are available for viewing and download at <ext-link ext-link-type="uri" xlink:href="http://neurovault.org/collections/VXWZKTWE/">http://neurovault.org/collections/VXWZKTWE/</ext-link>. Experimental programs and Matlab code to generate simulations can be downloaded from <ext-link ext-link-type="uri" xlink:href="http://balleinelab.com">http://balleinelab.com</ext-link></p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>This study was supported by a Laureate Fellowship from the Australian Research Council (ARC; #FL0992409) awarded to BWB. RWM was supported by National Health and Medical Research Council (NHMRC) Project Grant #1069487, and the ARC Centre of Excellence in Cognition and its Disorders (Macquarie University). MJG was supported by the NHMRC R.D. Wright Biomedical Career Development Fellowship (APP1061875). MLP was supported by an ARC Future Fellowship (FT100100260) and BWB by a Senior Principal Research Fellowship from the NHMRC. The programs used in the behavioral tasks are available for download at <ext-link ext-link-type="uri" xlink:href="http://balleinelab.com">http://balleinelab.com</ext-link>.</p>
<p>The authors declare no biomedical financial interests or potential conflicts of interest.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Alexander</surname> <given-names>WH</given-names></string-name> &#x0026; <string-name><surname>Brown</surname> <given-names>JW</given-names></string-name> (<year>2011</year>). <article-title>Medial prefrontal cortex as an action-outcome predictor</article-title>. <source>Nat Neurosci</source> <volume>14</volume>, <fpage>1338</fpage>&#x2013;<lpage>1344</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Balleine</surname> <given-names>BW</given-names></string-name> &#x0026; <string-name><surname>Dickinson</surname> <given-names>A</given-names></string-name> (<year>1998</year>). <article-title>Goal-directed instrumental action: contingency and incentive learning and their cortical substrates</article-title>. <source>Neuropharmacology</source> <volume>37</volume>, <fpage>407</fpage>&#x2013;<lpage>419</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Balleine</surname> <given-names>BW</given-names></string-name> &#x0026; <string-name><surname>O&#x2019;Doherty</surname> <given-names>JP</given-names></string-name> (<year>2010</year>). <article-title>Human and rodent homologies in action control: corticostriatal determinants of goal-directed and habitual action</article-title>. <source>Neuropsychopharm</source> <volume>35</volume>, <fpage>48</fpage>&#x2013;<lpage>69</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Doll</surname> <given-names>BB</given-names></string-name>, <string-name><surname>Simon</surname> <given-names>DA</given-names></string-name> &#x0026; <string-name><surname>Daw</surname> <given-names>ND</given-names></string-name> (<year>2012</year>). <article-title>The ubiquity of model-based reinforcement learning</article-title>. <source>Curr Opin Neurobiol</source> <volume>22</volume>, <fpage>1075</fpage>&#x2013;<lpage>1081</lpage>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Tanaka</surname> <given-names>SC</given-names></string-name>, <string-name><surname>Balleine</surname> <given-names>BW</given-names></string-name> &#x0026; <string-name><surname>O&#x2019;Doherty</surname> <given-names>JP</given-names></string-name> (<year>2008</year>). <article-title>Calculating consequences: brain systems that encode the causal effects of actions</article-title>. <source>J Neurosci</source> <volume>28</volume>, <fpage>6750</fpage>&#x2013;<lpage>6755</lpage>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Allan</surname> <given-names>LG</given-names></string-name> (<year>1980</year>). <article-title>A Note on Measurement of Contingency between 2 Binary Variables in Judgment Tasks</article-title>. <source>Bulletin of the Psychonomic Society</source> <volume>15</volume>, <fpage>147</fpage>&#x2013;<lpage>149</lpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Maier</surname> <given-names>SF</given-names></string-name> &#x0026; <string-name><surname>Seligman</surname> <given-names>MEP</given-names></string-name> (<year>1976</year>). <article-title>Learned Helplessness - Theory and Evidence</article-title>. <source>Journal of Experimental Psychology-General</source> <volume>105</volume>, <fpage>3</fpage>&#x2013;<lpage>46</lpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="other"><string-name><surname>Rescorla</surname> <given-names>RA</given-names></string-name> &#x0026; <string-name><surname>Wagner</surname> <given-names>AR.</given-names></string-name> in <article-title><italic>Classical Conditioning II</italic> (eds</article-title> <person-group person-group-type="editor"><string-name><given-names>A. H.</given-names> <surname>Black</surname></string-name> &#x0026; <string-name><given-names>W. F.</given-names> <surname>Prokasy</surname></string-name></person-group>) <fpage>64</fpage>&#x2013;<lpage>99</lpage> (Appleton-Century-Crofts, 1972).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Dickinson</surname> <given-names>A</given-names></string-name> (<year>2001</year>). <article-title>Causal learning: an associative analysis</article-title>. <source>Quarterly Journal of Experimental Psychology</source> <volume>54B</volume>, <fpage>3</fpage>&#x2013;<lpage>25</lpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Daw</surname> <given-names>ND</given-names></string-name>, <string-name><surname>Niv</surname> <given-names>Y</given-names></string-name> &#x0026; <string-name><surname>Dayan</surname> <given-names>P</given-names></string-name> (<year>2005</year>). <article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title>. <source>Nat Neurosci</source> <volume>8</volume>, <fpage>1704</fpage>&#x2013;<lpage>1711</lpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Solway</surname> <given-names>A</given-names></string-name> &#x0026; <string-name><surname>Botvinick</surname> <given-names>MM</given-names></string-name> (<year>2012</year>). <article-title>Goal-directed decision making as probabilistic inference: a computational framework and potential neural correlates</article-title>. <source>Psychol Rev</source> <volume>119</volume>, <fpage>120</fpage>&#x2013;<lpage>154</lpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Dayan</surname> <given-names>P</given-names></string-name>, <string-name><surname>Kakade</surname> <given-names>S</given-names></string-name> &#x0026; <string-name><surname>Montague</surname> <given-names>P</given-names></string-name> (<year>2000</year>). <article-title>Learning and selective attention</article-title>. <source>Nature neuroscience</source> <volume>3 Suppl</volume>, <fpage>1218</fpage>&#x2013;<lpage>1223</lpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Sutton</surname> <given-names>RS</given-names></string-name> (<year>1992</year>). <article-title>Gain adaptation beats least squares?</article-title> <source>Proceedings of the Seventh Yale Workshop on Adaptive and Learning Systems</source>, <fpage>161</fpage>&#x2013;<lpage>166</lpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Meinhold</surname> <given-names>RJ</given-names></string-name> &#x0026; <string-name><surname>Singpurwalla</surname> <given-names>ND</given-names></string-name> (<year>1983</year>). <article-title>Understanding the Kalman Filter</article-title>. <source>American Statistician</source> <volume>37</volume>, <fpage>123</fpage>&#x2013;<lpage>127</lpage>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Hammond</surname> <given-names>LJ</given-names></string-name> (<year>1980</year>). <article-title>The effect of contingency upon the appetitive conditioning of free-operant behavior</article-title>. <source>J Exp Anal Behav</source> <volume>34</volume>, <fpage>297</fpage>&#x2013;<lpage>304</lpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Dickinson</surname> <given-names>A</given-names></string-name> &#x0026; <string-name><surname>Mulatero</surname> <given-names>CW</given-names></string-name> (<year>1989</year>). <article-title>Reinforcer Specificity of the Suppression of Instrumental Performance on a Non-Contingent Schedule</article-title>. <source>Behavioural Processes</source> <volume>19</volume>, <fpage>167</fpage>&#x2013;<lpage>180</lpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Bradfield</surname> <given-names>LA</given-names></string-name>, <string-name><surname>Dezfouli</surname> <given-names>A</given-names></string-name>, <string-name><surname>van Holstein</surname> <given-names>M</given-names></string-name>, <string-name><surname>Chieng</surname> <given-names>B</given-names></string-name> &#x0026; <string-name><surname>Balleine</surname> <given-names>BW</given-names></string-name> (<year>2015</year>). <article-title>Medial Orbitofrontal Cortex Mediates Outcome Retrieval in Partially Observable Task Situations</article-title>. <source>Neuron</source> <volume>88</volume>, <fpage>1268</fpage>&#x2013;<lpage>1280</lpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>O&#x2019;Doherty</surname> <given-names>J</given-names></string-name>, <string-name><surname>Dayan</surname> <given-names>P</given-names></string-name>, <string-name><surname>Schultz</surname> <given-names>J</given-names></string-name>, <string-name><surname>Deichmann</surname> <given-names>R</given-names></string-name>, <string-name><surname>Friston</surname> <given-names>K</given-names></string-name> <etal>et al.</etal> (<year>2004</year>). <article-title>Dissociable roles of ventral and dorsal striatum in instrumental conditioning</article-title>. <source>Science</source> <volume>304</volume>, <fpage>452</fpage>&#x2013;<lpage>454</lpage>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Pessiglione</surname> <given-names>M</given-names></string-name>, <string-name><surname>Seymour</surname> <given-names>B</given-names></string-name>, <string-name><surname>Flandin</surname> <given-names>G</given-names></string-name>, <string-name><surname>Dolan</surname> <given-names>RJ</given-names></string-name> &#x0026; <string-name><surname>Frith</surname> <given-names>CD</given-names></string-name> (<year>2006</year>). <article-title>Dopamine-dependent prediction errors underpin reward-seeking behaviour in humans</article-title>. <source>Nature</source> <volume>442</volume>, <fpage>1042</fpage>&#x2013;<lpage>1045</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Tobler</surname> <given-names>PN</given-names></string-name>, <string-name><surname>O&#x2019;Doherty</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Dolan</surname> <given-names>RJ</given-names></string-name> &#x0026; <string-name><surname>Schultz</surname> <given-names>W</given-names></string-name> (<year>2006</year>). <article-title>Human neural learning depends on reward prediction errors in the blocking paradigm</article-title>. <source>J Neurophysiol</source> <volume>95</volume>, <fpage>301</fpage>&#x2013;<lpage>310</lpage>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Daunizeau</surname> <given-names>J</given-names></string-name>, <string-name><surname>David</surname> <given-names>O</given-names></string-name> &#x0026; <string-name><surname>Stephan</surname> <given-names>KE</given-names></string-name> (<year>2011</year>). <article-title>Dynamic causal modelling: a critical review of the biophysical and statistical foundations</article-title>. <source>Neuroimage</source> <volume>58</volume>, <fpage>312</fpage>&#x2013;<lpage>322</lpage>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Jacobs</surname> <given-names>RA</given-names></string-name> &#x0026; <string-name><surname>Kruschke</surname> <given-names>JK</given-names></string-name> (<year>2011</year>). <article-title>Bayesian learning theory applied to human cognition</article-title>. <source>Wiley Interdiscip Rev Cogn Sci</source> <volume>2</volume>, <fpage>8</fpage>&#x2013;<lpage>21</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Guest</surname> <given-names>O</given-names></string-name> &#x0026; <string-name><surname>Love</surname> <given-names>BC</given-names></string-name> (<year>2016</year>). <article-title>What the Success of Brain Imaging Implies about the Neural Code</article-title>. <source>bioRxiv</source> <fpage>24</fpage></mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Yin</surname> <given-names>HH</given-names></string-name>, <string-name><surname>Knowlton</surname> <given-names>BJ</given-names></string-name> &#x0026; <string-name><surname>Balleine</surname> <given-names>BW</given-names></string-name> (<year>2005</year>). <article-title>Blockade of NMDA receptors in the dorsomedial striatum prevents action-outcome learning in instrumental conditioning</article-title>. <source>Eur J Neurosci</source> <volume>22</volume>, <fpage>505</fpage>&#x2013;<lpage>512</lpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Ostlund</surname> <given-names>SB</given-names></string-name> &#x0026; <string-name><surname>Balleine</surname> <given-names>BW</given-names></string-name> (<year>2007</year>). <article-title>The contribution of orbitofrontal cortex to action selection</article-title>. <source>Ann N Y Acad Sci</source> <volume>1121</volume>, <fpage>174</fpage>&#x2013;<lpage>192</lpage>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Ostlund</surname> <given-names>SB</given-names></string-name> &#x0026; <string-name><surname>Balleine</surname> <given-names>BW</given-names></string-name> (<year>2007</year>). <article-title>Orbitofrontal cortex mediates outcome encoding in Pavlovian but not instrumental conditioning</article-title>. <source>J Neurosci</source> <volume>27</volume>, <fpage>4819</fpage>&#x2013;<lpage>4825</lpage>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Corbit</surname> <given-names>LH</given-names></string-name> &#x0026; <string-name><surname>Balleine</surname> <given-names>BW</given-names></string-name> (<year>2003</year>). <article-title>The role of prelimbic cortex in instrumental conditioning</article-title>. <source>Behav Brain Res</source> <volume>146</volume>, <fpage>145</fpage>&#x2013;<lpage>157</lpage>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Dias-Ferreira</surname> <given-names>E</given-names></string-name>, <string-name><surname>Sousa</surname> <given-names>JC</given-names></string-name>, <string-name><surname>Melo</surname> <given-names>I</given-names></string-name>, <string-name><surname>Morgado</surname> <given-names>P</given-names></string-name>, <string-name><surname>Mesquita</surname> <given-names>AR</given-names></string-name> <etal>et al.</etal> (<year>2009</year>). <article-title>Chronic stress causes frontostriatal reorganization and affects decision-making</article-title>. <source>Science</source> <volume>325</volume>, <fpage>621</fpage>&#x2013;<lpage>625</lpage>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Naneix</surname> <given-names>F</given-names></string-name>, <string-name><surname>Marchand</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Di Scala</surname> <given-names>G</given-names></string-name>, <string-name><surname>Pape</surname> <given-names>JR</given-names></string-name> &#x0026; <string-name><surname>Coutureau</surname> <given-names>E</given-names></string-name> (<year>2009</year>). <article-title>A role for medial prefrontal dopaminergic innervation in instrumental conditioning</article-title>. <source>J Neurosci</source> <volume>29</volume>, <fpage>6599</fpage>&#x2013;<lpage>6606</lpage>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Glascher</surname> <given-names>J</given-names></string-name>, <string-name><surname>Daw</surname> <given-names>N</given-names></string-name>, <string-name><surname>Dayan</surname> <given-names>P</given-names></string-name> &#x0026; <string-name><surname>O&#x2019;Doherty</surname> <given-names>JP</given-names></string-name> (<year>2010</year>). <article-title>States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning</article-title>. <source>Neuron</source> <volume>66</volume>, <fpage>585</fpage>&#x2013;<lpage>595</lpage>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Dickinson</surname> <given-names>A</given-names></string-name> &#x0026; <string-name><surname>Burke</surname> <given-names>J</given-names></string-name> (<year>1996</year>). <article-title>Within-compound associations mediate the retrospective revaluation of causality judgements</article-title>. <source>Quarterly Journal of Experimental Psychology Section B-Comparative and Physiological Psychology</source> <volume>49</volume>, <fpage>60</fpage>&#x2013;<lpage>80</lpage>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Van Hamme</surname> <given-names>LJ</given-names></string-name> &#x0026; <string-name><surname>Wasserman</surname> <given-names>EA</given-names></string-name> (<year>1994</year>). <article-title>Cue Competition in Causality Judgments - the Role of Nonpresentation of Compound Stimulus Elements</article-title>. <source>Learning and Motivation</source> <volume>25</volume>, <fpage>127</fpage>&#x2013;<lpage>151</lpage>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Vallee-Tourangeau</surname> <given-names>F</given-names></string-name>, <string-name><surname>Murphy</surname> <given-names>RA</given-names></string-name> &#x0026; <string-name><surname>Baker</surname> <given-names>AG</given-names></string-name> (<year>2005</year>). <article-title>Contiguity and the outcome density bias in action-outcome contingency judgements</article-title>. <source>Quarterly Journal of Experimental Psychology Section B-Comparative and Physiological Psychology</source> <volume>58</volume>, <fpage>177</fpage>&#x2013;<lpage>192</lpage>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Wasserman</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Chatlosh</surname> <given-names>DL</given-names></string-name> &#x0026; <string-name><surname>Neunaber</surname> <given-names>DJ</given-names></string-name> (<year>1983</year>). <article-title>Perception of Causal Relations in Humans - Factors Affecting Judgments of Response Outcome Contingencies under Free-Operant Procedures</article-title>. <source>Learning and Motivation</source> <volume>14</volume>, <fpage>406</fpage>&#x2013;<lpage>432</lpage>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Wasserman</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Elek</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Chatlosh</surname> <given-names>DL</given-names></string-name> &#x0026; <string-name><surname>Baker</surname> <given-names>AG</given-names></string-name> (<year>1993</year>). <article-title>Rating Causal Relations - Role of Probability in Judgments of Response Outcome Contingency</article-title>. <source>Journal of Experimental Psychology-Learning Memory and Cognition</source> <volume>19</volume>, <fpage>174</fpage>&#x2013;<lpage>188</lpage>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Vallee-Tourangeau</surname> <given-names>F</given-names></string-name> &#x0026; <string-name><surname>Murphy</surname> <given-names>RA</given-names></string-name> (<year>1999</year>). <article-title>Action-effect contingency judgment tasks foster normative causal reasoning</article-title>. <source>Proceedings of the Twenty First Annual Conference of the Cognitive Science Society</source>, <fpage>821</fpage>&#x2013;<lpage>821</lpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="book"><string-name><surname>Cheng</surname> <given-names>PW</given-names></string-name> &#x0026; <string-name><surname>Buehner</surname> <given-names>M.</given-names></string-name> in <source>Oxford Handbook of Thinking and Reasoning</source> (eds <person-group person-group-type="editor"><string-name><given-names>K. J.</given-names> <surname>Holyoak</surname></string-name> &#x0026; <string-name><given-names>R. G.</given-names> <surname>Morrison</surname></string-name></person-group>) Ch. 12, <fpage>210</fpage>&#x2013;<lpage>233</lpage> (<publisher-name>Oxford University Press</publisher-name>, <year>2012</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Holyoak</surname> <given-names>KJ</given-names></string-name> &#x0026; <string-name><surname>Cheng</surname> <given-names>PW</given-names></string-name> (<year>2011</year>). <article-title>Causal learning and inference as a rational process: the new synthesis</article-title>. <source>Annu Rev Psychol</source> <volume>62</volume>, <fpage>135</fpage>&#x2013;<lpage>163</lpage>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="book"><string-name><surname>Pearl</surname> <given-names>J</given-names></string-name> (<year>2009</year>). <chapter-title><italic>Causality: Models, Reasoning, and Inference</italic></chapter-title>. <edition>2nd</edition> edn, <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Steyvers</surname> <given-names>M</given-names></string-name>, <string-name><surname>Tenenbaum</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Wagenmakers</surname> <given-names>EJ</given-names></string-name> &#x0026; <string-name><surname>Blum</surname> <given-names>B</given-names></string-name> (<year>2003</year>). <article-title>Inferring causal networks from observations and interventions</article-title>. <source>Cognitive Science</source> <volume>27</volume>, <fpage>453</fpage>&#x2013;<lpage>489</lpage>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Kishida</surname> <given-names>KT</given-names></string-name>, <string-name><surname>Saez</surname> <given-names>I</given-names></string-name>, <string-name><surname>Lohrenz</surname> <given-names>T</given-names></string-name>, <string-name><surname>Witcher</surname> <given-names>MR</given-names></string-name>, <string-name><surname>Laxton</surname> <given-names>AW</given-names></string-name> <etal>et al.</etal> (<year>2016</year>). <article-title>Subsecond dopamine fluctuations in human striatum encode superposed error signals about actual and counterfactual reward</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>113</volume>, <fpage>200</fpage>&#x2013;<lpage>205</lpage>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Goussev</surname> <given-names>V</given-names></string-name> (<year>2004</year>). <article-title>Does the brain implement the Kalman filter?</article-title> <source>Behavioral and Brain Sciences</source> <volume>27</volume>, <fpage>404</fpage>&#x2013;<lpage>405</lpage>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><surname>Morris</surname> <given-names>RW</given-names></string-name>, <string-name><surname>Dezfouli</surname> <given-names>A</given-names></string-name>, <string-name><surname>Griffiths</surname> <given-names>KR</given-names></string-name> &#x0026; <string-name><surname>Balleine</surname> <given-names>BW</given-names></string-name> (<year>2014</year>). <article-title>Action-value comparisons in the dorsolateral prefrontal cortex control choice between goal-directed actions</article-title>. <source>Nat Commun</source> <volume>5</volume>, <fpage>4390</fpage>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>Pouget</surname> <given-names>A</given-names></string-name>, <string-name><surname>Beck</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>WJ</given-names></string-name> &#x0026; <string-name><surname>Latham</surname> <given-names>PE</given-names></string-name> (<year>2013</year>). <article-title>Probabilistic brains: knowns and unknowns</article-title>. <source>Nat Neurosci</source> <volume>16</volume>, <fpage>1170</fpage>&#x2013;<lpage>1178</lpage>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>Peirce</surname> <given-names>JW</given-names></string-name> (<year>2007</year>). <article-title>PsychoPy&#x2014;Psychophysics software in Python</article-title>. <source>Journal of Neuroscience Methods 162</source>, <fpage>8</fpage>&#x2013;<lpage>13</lpage>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><surname>Peirce</surname> <given-names>JW</given-names></string-name> (<year>2009</year>). <article-title>Generating stimuli for neuroscience using PsychoPy</article-title>. <source>Frontiers in Neuroinformatics</source></mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>Shanks</surname> <given-names>DR</given-names></string-name> &#x0026; <string-name><surname>Dickinson</surname> <given-names>A</given-names></string-name> (<year>1991</year>). <article-title>Instrumental judgment and performance under variations in action-outcome contingency and contiguity</article-title>. <source>Mem Cognit</source> <volume>19</volume>, <fpage>353</fpage>&#x2013;<lpage>360</lpage>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="book"><string-name><surname>Daw</surname> <given-names>ND.</given-names></string-name> in <source>Decision making, affect, and learning: Attention and performance XXIII</source> Vol. <volume>23</volume> (eds <person-group person-group-type="editor"><string-name><given-names>M R.</given-names> <surname>Delgado</surname></string-name>, <string-name><given-names>E. A.</given-names> <surname>Phelps</surname></string-name>, &#x0026; <string-name><given-names>T. W.</given-names> <surname>Robbins</surname></string-name></person-group>) Ch. 1, <fpage>3</fpage>&#x2013;<lpage>38</lpage> (<publisher-name>Oxford University Press</publisher-name>, <year>2011</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><surname>Mumford</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Poline</surname> <given-names>JB</given-names></string-name> &#x0026; <string-name><surname>Poldrack</surname> <given-names>RA</given-names></string-name> (<year>2015</year>). <article-title>Orthogonalization of regressors in FMRI models</article-title>. <source>PLoS One</source> <volume>10</volume>, <fpage>e0126255</fpage>.</mixed-citation></ref>
</ref-list>
<sec>
<title>Supplementary Figure</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Figure S1.</label>
<caption><p>Simulation results (mean and SEM of 100 iterations) showing the temporal dynamics of learning and behaviour for each model at representative parameter values. <bold>A)</bold> Beliefs (<bold>u</bold>) and behaviour in the Kalman algorithm distinguish causal actions over time throughout parameter space <bold>B)</bold> <italic>V</italic>-values and behaviour of the PE model distinguish causal actions over time in a narrower range of paramter values <bold>C)</bold> State-action transitions (T) and behaviour in MBRL fails to distinguish causal actions</p></caption>
<graphic xlink:href="137851_figS1.tif"/>
</fig>
<fig id="figS2" position="float" fig-type="figure">
<label>Figure S2.</label>
<caption><p>Simulation results showing the parameter space (of learning rate &#x0026; tau) over which each model distinguishes the causal action during degradation. <bold>A)</bold> Beliefs (<bold>u</bold>) in the Kalman algorithm distinguish causal actions throughout parameter space, with the best distinction at low parameter values <bold>B)</bold> Behavior of the Kalman algorithm distinguishes causal actions throughout parameter space <bold>C)</bold> <italic>V</italic>-values in the PE model distinguish causal actions at low values of tau <bold>D)</bold> Behavior of the PE model partially distinguishes causal actions at low learning rates <bold>E)</bold> State-action transitions (T) in MBRL fail to distinguish causal actions <bold>F)</bold> Behavior of the MBRL fails to distinguish causal actions</p></caption>
<graphic xlink:href="137851_figS2.tif"/>
</fig>
<fig id="figS3" position="float" fig-type="figure">
<label>Figure S3.</label>
<caption><p>Responses to the alternate AO contingency in the ventromedial prefrontal cortex (N = 30), <italic>F</italic><sub>1,29</sub> = 18.08, FWE = .011, including <bold>A)</bold> the medial orbitofrontal cortex and <bold>B)</bold> the anterior cingulate. Image thresholded at p &#x003C; .001, uncorrected.</p></caption>
<graphic xlink:href="137851_figS3.tif"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>Figure S4.</label>
<caption><p>Power analysis of Experiment 1, using the mPFC response during &#x0394;AO, indicated N &#x003E; 20 would achieve 95 percent power</p></caption>
<graphic xlink:href="137851_figS4.tif"/>
</fig>
</sec>
</back>
</article>