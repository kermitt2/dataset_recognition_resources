<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/126763</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Sensitivity to statistical structure facilitates perceptual analysis of complex auditory scenes</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="no">
<name>
<surname>Aman</surname>
<given-names>Lucie</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="no">
<name>
<surname>Andreou</surname>
<given-names>Lefkothea-Vasiliki</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7808-3593</contrib-id>
<name>
<surname>Chait</surname>
<given-names>Maria</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Ear Institute, University College London</institution>, London, <country>UK</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1">Corresponding Author: Dr. Maria Chait Ear Institute, University College London 332 Gray&#x2019;s Inn Road, London WC1X 8EE, UK <email>m.chait@ucl.ac.uk</email>
</corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2017</year>
</pub-date>
<elocation-id>126763</elocation-id>
<history>
<date date-type="received">
<day>11</day>
<month>4</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>11</day>
<month>4</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="126763.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<p>The notion that sensitivity to the statistical structure of the environment is pivotal to perception has recently garnered considerable attention. Here we investigated this issue in the context of hearing. Building on previous work (Sohoglu &#x0026; Chait, <xref ref-type="bibr" rid="c30">2016b</xref>), stimuli were artificial &#x2018;sound-scapes&#x2019; populated by multiple (up to 14) simultaneous sources (&#x2018;auditory objects&#x2019;) comprised of tone-pip sequences, each with a distinct frequency and pattern of amplitude modulation. Sequences were either temporally regular or random.</p>
<p>We show that listeners&#x2019; ability to detect abrupt appearance or disappearance of a source is facilitated when scene sources were characterized by a temporally regular fluctuation pattern. The patterning of the changing source as well as that of the background (non-changing) sources contribute independently to this effect. Remarkably, listeners benefit from regularity even when they are not consciously aware of it. These findings establish that perception of complex acoustic scenes relies on the availability of detailed representations of the regularities automatically extracted from each scene source.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>Temporal regularity</kwd>
<kwd>Auditory scene analysis</kwd>
<kwd>change detection</kwd>
<kwd>time perception</kwd>
<kwd>change deafness</kwd>
<kwd>predictive coding</kwd>
</kwd-group>
<counts>
<page-count count="28"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Due to their idiosyncratic physical constraints, most animate objects produce statistically structured, temporally predictable, sensory signals (e.g. vocalizations, locomotion). Accumulating evidence from both vision and audition, suggest that observers are sensitive to this patterning and use it to understand, and efficiently interact with their surroundings (e.g. <xref ref-type="bibr" rid="c27">Rohenkohl et al, 2012</xref>; <xref ref-type="bibr" rid="c24">N&#x00E4;&#x00E4;t&#x00E4;nen et al, 2011</xref>; <xref ref-type="bibr" rid="c22">Leaver et al, 2009</xref>; <xref ref-type="bibr" rid="c21">Lange, 2013</xref>; <xref ref-type="bibr" rid="c1">Andreou et al, 2011</xref>; <xref ref-type="bibr" rid="c35">Yaron et al, 2012</xref>; <xref ref-type="bibr" rid="c25">Nelken, 2012</xref>; <xref ref-type="bibr" rid="c9">Costa-Faidella et al, 2011</xref>; <xref ref-type="bibr" rid="c34">Winkler et al, 2009</xref>).</p>
<p>Specifically in the context of hearing, It has been proposed that the auditory system&#x2019;s capacity to rapidly detect predictable patterns in the unfolding sound input may facilitate listening in crowded environments (<xref ref-type="bibr" rid="c34">Winkler et al, 2009</xref>;; <xref ref-type="bibr" rid="c1">Andreou et al, 2011</xref>). We recently provided evidence for this proposal in a magnetoencephalography (MEG) study (<xref ref-type="bibr" rid="c30">Sohoglu &#x0026; Chait, 2016b</xref>), using a stimulus that mirrors the complexity of crowded natural acoustic scenes and an ecologically relevant listening task. Listeners were presented with artificial acoustic scenes (<xref ref-type="fig" rid="fig1">Figure 1</xref>) comprising several concurrent sound-streams (&#x2018;sources&#x2019;), each consisting of a sequence of tone pips of a particular frequency and with rates commensurate with those characterizing many natural sounds. Their task was to detect occasional changes (appearance of a source) within those &#x2018;soundscapes&#x2019;. The temporal structure of the streams comprising the scene was manipulated such they were either temporally regular or random (repeated with a random inter-pip-interval). we demonstrated that activity in auditory cortex rapidly (within 400 ms of scene onset) distinguishes scenes comprised of temporally-regular (REG) vs. temporally-random (RAND) streams. This is manifested by increased sustained activity to REG relative to RAND scenes. Over and above this, appearance of a source in REG scenes is associated with increased responses relative to RAND scenes, mirroring the behavioural data which showed increased accuracy and quicker response times in that condition.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption>
<p>Example of the three variations (&#x2018;change appear&#x2019;, &#x2018;Change disappear&#x2019;, and &#x2018;no change&#x2019;) of a scene with 8 sources. Regular (REG) scenes are on the left, random (RAND) scenes on the right. The plots represent &#x2018;auditory&#x2019; spectrograms, generated with a filter bank of 1/ERB wide channels equally spaced on a scale of ERB-rate. Channels are smoothed to obtain a temporal resolution similar to the Equivalent Rectangular Duration.</p>
</caption>
<graphic xlink:href="126763_fig1.tif"/>
</fig>
<p>The increased sustained activity after scene onset is interpreted as reflecting a mechanism that infers the precision (predictability) of sensory input and uses this information to up-regulate neural processing towards more reliable sensory signals (<xref ref-type="bibr" rid="c14">Feldman and Friston, 2010</xref>; <xref ref-type="bibr" rid="c36">Zhao et al., 2013</xref>; <xref ref-type="bibr" rid="c4">Auksztulewicz and Friston, 2015</xref>; <xref ref-type="bibr" rid="c6">Barascud et al., 2016</xref>; Southwell et al, in press). According to this &#x2018;precisionweighting&#x2019; account, if the auditory system can form precise models about the content of ongoing scenes, novel events that violate those models (e.g. the appearance events studied in Sohoglu et al 2016) would evoke greater neural responses and be perceived as more salient, consistent with the observed change-evoked brain responses and the behavioural advantage associated with REG scenes.</p>
<p>The present series of behavioural experiments aims to understand the factors which underlie listeners&#x2019; sensitivity to the statistical structure of complex soundscapes. In Experiment 1, we replicate the behavioural results in Sohoglu &#x0026; Chait (2016) and further extend them to item disappearance. In Experiment 2a,b we investigate more complex forms of temporal regularity. In Experiments 3 and 4 we seek to determine whether the effect of regularity is driven by sensitivity to the temporal regularity of the changing (appearing or disappearing) <bold>source</bold> per se, or by that of the <bold>context</bold> (the other, non-changing, sources in the scene).</p>
</sec>
<sec id="s2">
<title>EXPERIMENT 1</title>
<sec id="s2a">
<title>Materials and Methods</title>
<sec id="s2a1">
<title>Stimuli:</title>
<p>The artificial &#x2018;sound-scapes&#x2019; used here simulate challenges faced by listeners in natural acoustic scenes, in which many concurrent sound sources, each with a distinctive temporal pattern, are heard simultaneously (<xref ref-type="bibr" rid="c28">Snyder et al, 2012</xref>; <xref ref-type="bibr" rid="c13">Eramudugolla et al, 2005</xref>). Unlike natural sounds, however, the present stimuli are designed such that sources occupy distinct spectral ranges and hence do not energetically mask each other. This enables us to (1) create the optimal conditions for the streams to be perceived as independent auditory objects (2) measure the effect of growing scene size (number of concurrent sources present) independently of increased inter-source masking. Stimuli (<xref ref-type="fig" rid="fig1">Figure 1</xref>) were 2000-4000 ms long artificial &#x2018;scenes&#x2019; populated by multiple (4, 8 or 14) streams of pure-tones designed to model sound sources. Each source is characterized by a different carrier frequency (drawn from a pool of fixed values spaced at 2&#x002A;ERB between 100 and 4846 Hz; <xref ref-type="bibr" rid="c23">Moore &#x0026; Glasberg, 1983</xref>), and is furthermore amplitude modulated (AM) by a square wave &#x2013; such that the source can be seen as a sequence of tone pips. In a previous series of experiments (<xref ref-type="bibr" rid="c8">Cervantes Constantino et al, 2012</xref>), we demonstrated that these stimuli are perceived as a composite &#x2018;sound-scape&#x2019; in which individual streams can be perceptually segregated and selectively attended to, and are therefore good models for natural acoustic scenes. Importantly, the large spectral separation between neighbouring streams (at least 2&#x002A; ERB) minimizes peripheral masking, enabling the investigation of the effects of increasing scene size without the confound of increasing inter-stream sensory masking.</p>
<p>In the &#x2018;regular&#x2019; scenes (REG), the duration of a tone pip (values uniformly distributed between 20 and 160 ms) and the silent interval between pips (values uniformly distributed between 2 and 160 ms) are chosen independently and then fixed for the duration of the scene so that the pattern is regular (see <xref ref-type="fig" rid="fig1">Figure 1</xref>, left column). This pattern mimics the regularly modulated temporal properties of many natural sounds. In &#x2018;Random&#x2019; (RAND) scenes, tone duration remains fixed throughout the scene, but the silent intervals between successive pips are varied randomly (values uniformly distributed between 2-160 ms) resulting in an irregular pattern (See <xref ref-type="fig" rid="fig1">Figure 1</xref>, right column).</p>
<p>Scenes in which each source is active throughout the stimulus are referred to as &#x2018;no change&#x2019; stimuli (NC). Additionally, we synthesized scenes in which a source became active (appeared) or inactive (disappeared) at some intermediate time during the scene. These are referred to as &#x2018;change appear&#x2019; (CA) and &#x2018;change disappear&#x2019; (CD) stimuli, respectively. The timing of change varied randomly (uniformly distributed between 1000 ms and 2000 ms post scene onset), but with the following constraints: The nominal time of change for CA objects coincided with the onset of the first tone while for CD objects the nominal time of change was set to the offset of the last tone augmented by the inter-tone interval, i.e. at the expected onset of the next tone, which is the earliest time at which the disappearance could be detected. For disappearing sources in RAND scenes it is impossible to define change time in this way (because there is no regular temporal structure). For the purpose of measuring RT, the CD change time in RAND scenes was set to the offset of the last tone-pip augmented by the mean inter-pip-interval (85 ms). Because the distribution of inter-pip-intervals was identical in REG and RAND conditions, if temporal structure does not play a role in change detection, RT should be identical in both conditions.</p>
<p>The set of carrier frequencies and modulation patterns was chosen randomly for each scene, but to enable a controlled comparison between conditions, NC, CA and CD stimuli were generated as triplets sharing the same carrier frequencies and modulation patterns (but differing by the appearance or disappearance of a source; see <xref ref-type="fig" rid="fig1">Fig. 1</xref>). They were then presented in random order during the experiment, blocked by change type (NC and CA or NC and CD) and scene type (RAND or REG). Each block contained equal numbers of no change (NC) or change (CA or CD) scenes such that the occurrence of change (and change time) were unpredictable.</p>
<p>Stimuli were synthesized with a sampling rate of 44100 Hz and shaped with a 30 ms raised cosine onset and offset ramp. They were presented with an EDIROL UA-4FX sound card (Roland Corporation) over headphones (Sennheiser HD 555) at a comfortable listening level (&#x223C;60-70 dB SPL), self-adjusted by each participant. Stimulus presentation was controlled using the <italic>Cogent</italic> software (<ext-link ext-link-type="uri" xlink:href="http://www.vislab.ucl.ac.uk/cogent.php">http://www.vislab.ucl.ac.uk/cogent.php</ext-link>).</p>
</sec>
<sec id="s2a2">
<title>Procedure:</title>
<p>the experiment was conducted in an acoustically-shielded booth (IAC, Winchester, UK). Experimental sessions lasted about 2 hours and consisted of a short practice session with feedback, followed by the main experiment without feedback, divided into runs of approximately 10 minutes each. Subjects were instructed to fixate at a cross presented on the computer screen, and perform a change detection task whereby they pressed a keyboard button as fast as possible when they detected a change in the presented stimulus. They were allowed a short rest between runs.</p>
</sec>
<sec id="s2a3">
<title>Analysis:</title>
<p>Dependent measures are hit rates (number of changes correctly identified), d&#x2019; scores and response times (RT; measured between the nominal time of change and the subject&#x2019;s key press). The &#x03B1; level was <italic>a priori</italic> set to 0.05.</p>
</sec>
<sec id="s2a4">
<title>Participants:</title>
<p>Ten paid participants took part in the experiment (5 female; mean age &#x003D; 25 years). All reported normal hearing and no history of neurological or audiological disorder. Experimental procedures (here and in subsequent experiments) were carried out in accordance with the protocols approved by the research ethics committee of University College London, and written informed consent was obtained from each participant. The sample size (10 participants; here and in subsequent experiments) is based on previous experience in the lab with similar data. As can be seen below the effects are stable and consistent across participants, suggesting the sample size is appropriate.</p>
</sec>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<p><xref ref-type="fig" rid="fig2">Figure 2</xref> shows change detection performance for &#x2018;appearing&#x2019; (CA; in red) and &#x2018;disappearing&#x2019; (CD; in blue) events. For <bold>hit rates</bold> (<xref ref-type="fig" rid="fig2">Fig 2A</xref>), a repeated measures ANOVA with scene type (REG vs. RAND), change type (CA vs. CD) and scene size (4, 8, or 12 objects) as factors revealed main effects of scene type (F(1,9)=53.1, p&#x003C;0.0001), change type (F(1,9)&#x003D;87.9, p&#x003C;0.0001) and scene size (F(2,18)&#x003D;145.3, p&#x003C;0.0001) as well as the following interactions: scene type &#x00D7; change type (F(1,9)&#x003D; 9.2, p&#x003D;0.014; for CD scenes, the difference between REG and RAND was greater than for CA), scene type &#x00D7; scene size (F(2,18)&#x003D;6.9, p&#x003D;0.006; for REG scenes, the effect of scene size was smaller than for RAND scenes) and change type &#x00D7; scene size (F(2,18)&#x003D;28.5, p&#x003C;0.0001; due to CD showing a steeper decline in performance with scene size than CA).</p>
<p>The data are consistent with previous reports that changes associated with appearance of objects in the scene are easier to detect than those associated with disappearances (see discussion in <xref ref-type="bibr" rid="c8">Cervantes Constantino et al, 2012</xref>; <xref ref-type="bibr" rid="c29">Sohoglu &#x0026; Chait 2016a</xref>). Given the spectral separation between objects, the steep drop in performance for larger scenes is likely associated with the growing computational load of monitoring multiple streams in parallel (rather than inter-component masking). Importantly, the main, novel result is that listeners&#x2019; capacity to detect changes (both appearances and disappearances of objects within the scene) depends on temporal regularity.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption>
<p>Results of Experiment 1. Error bars are 1 standard error (SE). In all measures (hit rate, d&#x2019; and response time) performance is significantly reduced in RAND relative to REG scenes.</p>
</caption>
<graphic xlink:href="126763_fig2.tif"/>
</fig>
<p><bold><italic>d&#x2019;</italic>sensitivity</bold> scores mirror the hit rate results. A repeated measures ANOVA with scene type, change type and scene size as factors revealed main effects of scene type (F(1,9)=45.3, p&#x003C;0.0001), change type (F(1,9)&#x003D;10.8, p&#x003D;0.009) and scene size (F(2,18)&#x003D;64.3, p&#x003C;0.0001) with no interactions. The <bold>response time</bold> data demonstrated a pattern similar to that for the detection performance. Listeners were slower to detect disappearance (relative to appearance) events, and, importantly, for both CA and CD, reaction times were significantly slower in RAND, relative to REG, scenes. A repeated measures ANOVA revealed main effects of scene type (F(1,9)&#x003D;27.4, p&#x003D;0.001), change type (F(1,9)&#x003D;71.5, p&#x003C;0.0001) and scene size (F(2,18)&#x003D;85, p&#x003C;0.0001) as well as the following interactions: scene type &#x00D7; change type (F(1,9)&#x003D;21, p&#x003D;0.001; due to performance on CD showing a larger increase in RT than that on CA) and change type &#x00D7; scene size (F(2,18)&#x003D;10.5, p&#x003D;0.001).</p>
<p>Thus the results replicate the behavioural results from Sohoglu &#x0026; Chait (2016; for CA) and further extend them to demonstrate that CD performance is also improved in REG relative to RAND scenes.</p>
</sec>
<sec id="s4">
<title>EXPERIMENT 2A,B</title>
<p>Experiments 2A, B aim to determine whether the advantage of regularity persists for more complex (non-isochronous) temporal patterns.</p>
<sec id="s4a">
<title>Materials and Methods</title>
<sec id="s4a1">
<title>Stimuli:</title>
<p>Stimuli were identical to those in Experiment 1, above, except that REG scenes were characterized by increasingly complex temporal patterns. In <bold>Experiment 2A</bold> REG sources were constructed by randomly choosing two intertone-interval durations (T1, and T2 in <xref ref-type="fig" rid="fig3">Figure 3A</xref>; values uniformly distributed between 20 and 160 ms) which then repeated regularly. Henceforth, this condition will be referred to as REG2. In <bold>Experiment 2B</bold> scene sources contained three randomly selected, regularly repeating inter-tone-intervals. This condition is referred to as REG3. In both experiments, only inter-tone intervals were manipulated. Tone-pip durations were randomly chosen for each source, from within the same range as above, and then fixed for the duration of the stimulus. Inter-tone-interval durations and tone-pip durations were chosen anew for each source in each trial.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption>
<p>Experiments 2a and 2b. A Schematic representations of the regular patterns used. Scene sources in Experiment 1 (REG1) contained a fixed inter-tone-interval (T1) that was randomly chosen for each source in each trial. Those in Experiment 2a (REG2) contained two different, regularly repeating, inter-tone-intervals (T1 and T2). T1 and T2 were randomly chosen for each source in each trial. REG patterns in Experiment 2b (REG3) contained three different, regularly repeating, inter-tone-intervals (T1, T2 and T3). These were randomly chosen for each source in each trial. B Results of Experiment 3a (Left) and Experiment 3b (Right) expressed in terms of d&#x2019; scores (top) and reaction times (bottom). Error bars are 1 standard error (SE). As in Experiment 1, performance is significantly reduced in RAND relative to REG scenes.</p>
</caption>
<graphic xlink:href="126763_fig3.tif"/>
</fig>
</sec>
<sec id="s4a2">
<title>Participants:</title>
<p>Eleven subjects (mean age 22.9 years; 7 females) participated in Experiment 2A and 10 subjects (mean age 22.1; 9 females) participated in Experiment 2B. An additional participant was excluded from the analysis due to very low performance scores (d&#x2019;&#x003D;1.5 in the easiest condition [REG CA scene size 4]; for the rest of the subjects d&#x2019;&#x003E;3.3). Four subjects participated in both experiments.</p>
<p>Results are presented in <xref ref-type="fig" rid="fig3">Figure 3</xref>. Hit rate and d&#x2019; results were qualitatively identical and hence only d&#x2019; data are presented. Performance in both experiments resembled that in Experiment 1. REG2: a repeated measures ANOVA on <bold>d&#x2019;</bold> with scene type (REG vs. RAND), change type (CA vs. CD) and scene size (4, 8, or 14 objects) as factors revealed main effects of scene type (F(1,10)&#x003D;32.3 p&#x003C;0.0001), change type (F(1,10)&#x003D;92.6 p&#x003C;0.0001) and scene size (F(2,20)&#x003D;166.5 p&#x003C;0.0001) with no interactions. An identical pattern is observed for REG3: main effects of scene type (F(1,9)&#x003D;16.6 p&#x003D;0.003), change type (F(1,9)&#x003D;79.6 p&#x003C;0.0001) and scene size (F(2,18)&#x003D;82.9 p&#x003C;0.0001) with no interactions.</p>
<p>Similarly, <bold>response time</bold> data revealed effects comparable to those in Experiment 1. REG2: a repeated measures ANOVA on RT data revealed main effects of scene type (F(1,10)&#x003D;28.87 p&#x003C;0.0001), change type (F(1,10)&#x003D;88.1 p&#x003C;0.0001) and scene size (F(2,20)&#x003D;39.67 p&#x003C;0.0001) with no interactions. REG3: main effects of scene type (F(1,9)&#x003D;85 p&#x003C;0.0001), change type (F(1,9)&#x003D;351.6 p&#x003C;0.0001) and scene size (F(2,18)&#x003D;32.5 p&#x003C;0.0001) as well as the following interactions: change type &#x00D7; scene size (F(2,18)&#x003D;11 p&#x003D;0.002; due to CD showing larger decline in performance with scene size than CA) and scene type &#x00D7; change type (F(1,9)&#x003D;17.3 p&#x003D;0.002). Post hoc tests revealed significant effects of scene type for both CA and CD (F(1,9)&#x003D;11.8 p&#x003D;0.007; F(1,9)&#x003D;54 p&#x003C;0.0001), suggesting the interaction is due to differing marginal means.</p>
<p>An across group ANOVA was also conducted to compare performance for REG2 and REG3. For d&#x2019; this revealed main effects of scene type (F(1,19)&#x003D;46.037 p&#x003C;0.0001), change type (F(1,19)&#x003D;168.8 p&#x003C;0.0001) and scene size (F(2,38)&#x003D;239 p&#x003C;0.0001) and an interaction of group (Exp2A vs Exp2B) &#x00D7; Scene size (F(2,38)&#x003D;4.2 p&#x003D;0.024. The interaction is due to a smaller scene size effect in the Exp2B group, which is likely attributable to individual differences. A similar test for RT demonstrated main effects of scene type (F(1,19)&#x003D;91 p&#x003C;0.0001), change type (F(1,19)&#x003D;295.6 p&#x003C;0.0001) and scene size (F(2,38)&#x003D;70.9 p&#x003C;0.0001) as well as the following interactions: change type &#x00D7; scene size (F(2,38)&#x003D;10.4 p&#x003C;0.0001); scene type &#x00D7; change type (F(1,9)&#x003D;18.58 p&#x003C;0.0001); scene type &#x00D7; change type &#x00D7; group (F(1,19)&#x003D;6.61 p&#x003D;0.019);</p>
<p>Overall, the data demonstrate that, on all measures, change detection performance was significantly improved in REG relative to RAND scenes. This suggests that listeners are able to acquire, and make use of, relatively complex regular patterns to facilitate the detection of changes in the scene.</p>
</sec>
</sec>
</sec>
<sec id="s5">
<title>EXPERIMENT 3</title>
<p>Next we sought to determine whether the effect of regularity is driven by sensitivity to the temporal regularity of the changing (appearing or disappearing) <bold>source</bold> per se, or by that of the <bold>context</bold> (the other, non-changing, scene elements).</p>
<sec id="s5a">
<title>Martials and methods</title>
<sec id="s5a1">
<title>Stimuli:</title>
<p>&#x2018;Regular&#x2019; (REG) and &#x2018;Random&#x2019; (RAND) scenes were created as before (Experiment 1) with the exception that the regularity of the changing (appearing or disappearing) source was manipulated independently of the regularity of the rest of the sources in the scene, resulting in 4 configurations for each change type (CA, CD or NC): REG_REG, REG_RAND, RAND_REG and RAND_RAND (in each case the first term refers to the regularity of the scene context, the second to the regularity status of the changing source). Two scene sizes &#x2013; 8 and 14 &#x2013; were used. Stimuli were blocked by context and change type (CA vs CD).</p>
</sec>
<sec id="s5a2">
<title>Participants:</title>
<p>Ten participants (6 female; mean age &#x003D; 29.4 years) took part in the experiment.</p>
</sec>
</sec>
<sec id="s6">
<title>Results</title>
<p>Results are in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Hit rate and d&#x2019; results were qualitatively identical and hence only d&#x2019; data are presented. A repeated measures ANOVA on <bold>d&#x2019;</bold> data, with context regularity (REG vs. RAND), source regularity (REG vs. RAND), change type (CA vs. CD) and scene size (8 or 14 objects) as factors, showed main effects of context regularity (F(1,9)=92.8 p&#x003C;0.0001), source regularity (F(1,9)&#x003D;21.2 p&#x003D;0.001), change type (F(1,9)&#x003D;66.8 p&#x003C;0.0001) and scene size (F(1,9)&#x003D;87.4 p&#x003C;0.0001) as well as the following interactions: change type x scene size (F(1,9)&#x003D;6.28 p&#x003D;0.033; also seen in Experiment 1) and context regularity x change type (F(1,9)&#x003D;25.6 p&#x003D;0.001). To understand this interaction, separate ANOVAs were run for CA and CD. For CA, this revealed main effects of context regularity (F(1,9)&#x003D;122.6 p&#x003C;0.0001) and scene size (F(1,9)&#x003D;44.2 p&#x003C;0.0001) but no effect of source regularity (F(1,9)&#x003D;1.8 p&#x003D;0.205). In contrast, for CD all three main effects were significant : F(1,9)&#x003D;31.5 p&#x003C;0.0001, F(1,9)&#x003D;47.6 p&#x003C;0.0001 and F(1,9)&#x003D;35.3 p&#x003C;0.0001 respectively.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption>
<p>Results of Experiment 3. Appearance changes (CA; red colours) are on the left and disappearance changes (CD; blue colours) are on the right. REG context conditions are in darker colours; RAND context conditions are in lighter colours. REG source conditions are plotted with solid lines; RAND source conditions are plotted with dashed lines. Error bars are 1 standard error (SE).The results demonstrate that the patterning of the changing source as well as that of the background (nonchanging) sources both contribute independently to the advantage of regularity in the context of change detection.</p>
</caption>
<graphic xlink:href="126763_fig4.tif"/>
</fig>
<p>The data demonstrate that whilst context regularity influenced both CA and CD, source regularity only affected CD performance. This is consistent with the hypothesis that the detectability of CA depends primarily on the first event within the appearing stream (and as such shouldn&#x2019;t be affected by the patterning of the sequence) whereas successful coding of temporal structure is critical for the rapid detection of source disappearance. Indeed, to efficiently determine that a source has disappeared from the scene, an ideal observer must &#x2018;acquire&#x2019; the pattern of onsets and offsets associated with that channel, and respond as soon as an expected tone pip fails to arrive. Importantly, since the identity of the changing source varied randomly from trial to trial, to achieve optimal performance one must be able to represent the temporal structure of <italic>all objects</italic> within the scene. That listeners were indeed consistently better and substantially faster at detecting CD events in REG scenes demonstrates that, listeners do, at least to some extent, acquire the temporal structure of all on-going scene elements and use this information during scene perception.</p>
<p>An analysis of false positive rates (data not shown) revealed an expected main effect of scene size (F(1,9)&#x003D;15.24 p&#x003D;0.004) and an interaction between change type and scene size (F(1,9)&#x003D;6.7 p&#x003D;0.029). We also observed a main effect of context (F(1,9)&#x003D;13.8 p&#x003D;0.005), revealing significantly more false positive in RAND, relative to REG scenes, across change types. No other main effects or interactions were significant (p&#x003E;0.1 for all).</p>
<p>For <bold>response times</bold>, a repeated measures ANOVA showed main effects of context regularity (F(1,9)&#x003D;68.4 p&#x003C;0.0001), source regularity (F(1,9)&#x003D;26.1 p&#x003D;0.001), change type (F(1,9)&#x003D;98.4 p&#x003C;0.0001) and scene size (F(1,9)&#x003D;53.6 p&#x003C;0.0001) as well as the following interactions: change type &#x00D7; scene size (F(1,9)&#x003D;16.3 p&#x003D;0.003; also seen in Experiment 1, above, and due to CD showing larger decline in performance with scene size than CA) and context regularity &#x00D7; change type (F(1,9)&#x003D;10.8 p&#x003D;0.009). To understand the latter interaction, separate ANOVAs were run for CA and CD. In both cases all three main effects were significant, with no interactions. CA: main effect of context regularity: F(1,9)&#x003D;17.9 p&#x003D;0.002, main effect of source regularity: F(1,9)&#x003D;11 p&#x003D;0.009, main effect of scene size: F(1,9)&#x003D;28.9 p&#x003C;0.0001. CD: F(1,9)&#x003D;23.7 p&#x003D;0.001, F(1,9)&#x003D;19.8 p&#x003D;0.002, and F(1,9)&#x003D;42.7 p&#x003C;0.0001, respectively. Therefore, RT as a measure of performance suggests that both CA and CD detection are affected by the regularity of the context as well as that of the changing component itself. The source regularity effect for CA, which is observed in RT but not d&#x2019;, suggests that whilst component regularity does not affect CA detectability per se, it does speed-up its detection.</p>
</sec>
</sec>
<sec id="s7">
<title>EXPERIMENT 4</title>
<p>The design of experiment 3 was possibly confounded in the sense that regular sources in a random context might have perceptually stood out, even before the actual change event, thus facilitating the scanning for possible changes.</p>
<p>Here we investigated the extent to which listeners are sensitive to such situations: do REG streams in a RAND context (or vice versa - RAND streams in REG scenes) pop out?</p>
<sec id="s7a">
<title>Materials and methods</title>
<sec id="s7a1">
<title>Stimuli:</title>
<p>This experiment used only NC stimuli. &#x2018;REG context&#x2019; scenes contained all regular streams (&#x2018;Foil scenes&#x2019;) or one random stream among regular streams (&#x2018;target&#x2019; scenes&#x2019;; 50&#x0025;). Conversely, &#x2018;RAND context&#x2019; scenes contained all random streams or (in 50&#x0025; of the signals) one regular stream among random streams (&#x2018;target&#x2019; scenes). Within each context condition, scenes were generated in target/foil pairs such that each duo consisted of identical sources (in terms of frequency and temporal properties) only differing by the temporal structure of the target stream (See <xref ref-type="fig" rid="fig5">Figure 5A</xref>). The stimuli were then presented to the listeners in random order, blocked by context type (REG or RAND). Three scene sizes (4, 8 and 14 sources) were used. Participants were instructed to detect the odd sources (regular among random or vice versa &#x2013; &#x2018;target&#x2019; scenes).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption>
<p>Experiment 4. An Example of REG and RAND context scenes (left and right, respectively) with 4 sources. &#x2018;Foil&#x2019; scenes (bottom) contain all REG or all RAND sources; &#x2018;Target&#x2019; scenes (top) contain an odd source &#x2013; regular among random or vice versa, indicated with arrows. The plots represent &#x2018;auditory&#x2019; spectrograms, generated with a filter bank of 1/ERB wide channels equally spaced on a scale of ERB-rate. Channels are smoothed to obtain a temporal resolution similar to the Equivalent Rectangular Duration. B Results of Experiment 4. The REG context condition is plotted with a solid line, the RAND context condition is plotted with a dashed line. The results demonstrate that it is consistently easier to detect a random stream among regular streams (REG context) than vice versa.</p>
</caption>
<graphic xlink:href="126763_fig5.tif"/>
</fig>
</sec>
<sec id="s7a2">
<title>Participants:</title>
<p>Nine subjects participated in the experiment (4 female; mean age &#x003D; 28.7 years). The data from an additional participant were not useable due to a technical error.</p>
</sec>
</sec>
<sec id="s8">
<title>Results</title>
<p><xref ref-type="fig" rid="fig5">Figure 5B</xref> shows the results of Experiment 4. Hit rate and d&#x2019; results were qualitatively identical and hence only d&#x2019; data are presented. A repeated measures ANOVA on <bold>d&#x2019;</bold> scores, with context (REG versus RAND) and scene size (4, 8 or 14 objects) as factors, revealed main effects of context (F(1,8)&#x003D;37.8, p&#x003C;0.001) and scene size (F(2,16)&#x003D;52.3, p&#x003C;0.001); no interactions.</p>
<p>The relatively steep decline with scene size suggests that regularity does not strictly &#x2018;pop-out&#x2019; but is rather discovered via some search-based process. That it is overall easier to find a random source in a REG context, rather than a regular source in a RAND context, is in line with the account proposed above: listeners acquire the temporal patterning associated with each source in a REG context scene such that events that do not conform with these patterns (associated with the presence of a RAND stream) are relatively easy to detect. Conversely, in a RAND context, where most events are unpredictable, it is more difficult to spot the one stream that follows a regular pattern.</p>
<p>Importantly, the results demonstrate that at the largest scene size (14) participants are at floor for detecting a regular item in a RAND context (one sample T test against 0: t&#x003D;1.4, p&#x003D;0.19). Combined with the findings from Experiment 3, above, the data demonstrate that despite not being aware of the regularity of the sources, participants implicitly used this information for change detection: even with 14 concurrent sources in the scene, subjects&#x2019; change detection performance benefitted when a disappearing stream was regular, relative to when it was random. Indeed, all the results of Experiment 3, both in terms of d&#x2019; and RT, remain significant when running the ANOVA on scene size 14 only (p&#x003C;0.01 for all). This reveals that even in very crowded scenes, participants are able to utilize temporal regularity, of which they are not consciously aware, to efficiently detect change events.</p>
</sec>
</sec>
<sec id="s9">
<title>Discussion</title>
<p>The auditory system is tuned to changes in the acoustic environment (<xref ref-type="bibr" rid="c8">Cervantes Constantino et al, 2012</xref>; <xref ref-type="bibr" rid="c12">Demany et al, 2010</xref>). We therefore chose a change detection task as a neuro-ethologically relevant means by which the role of sensitivity to temporal structure in the course of auditory scene analysis can be studied.</p>
<p>We demonstrate that listeners&#x2019; ability to detect changes, manifested as the abrupt appearance or disappearance of a source, is facilitated when scene sources are characterized by a temporally regular fluctuation pattern (experiment 1). The patterning of the changing source as well as that of the background (non-changing) sources both contribute independently to this effect (Experiment 3). The advantage of regularity, relative to random temporal patterning, is observable even when using complex, non-isochronous, temporal patterns (Experiment 2). These findings establish that perception of complex acoustic scenes relies on the availability of detailed representations of the regularities automatically extracted from each scene source.</p>
<sec id="s9a">
<title>Are listeners coding local (frequency specific) or global temporal regularities?</title>
<p>An important point concerns whether the patterns were detected within each frequency channel separately, or identified as a complex, temporal regularity across the entire frequency range. For example, it is possible that listeners are sensitive to a global feature such as spectral variability (a spectrum whose shape is fluctuating regularly vs. whose shape is fluctuating irregularly). To study this possibility, we quantified spectral variability as the variability of the spectral centroid over time. The spectral centroid was computed from a spectrogram, generated with a filter bank of 1/ERB-wide channels (smoothed to obtain a temporal resolution similar to the Equivalent Rectangular Duration; equi-spaced on a scale of ERB-rate). As expected, this measure of variability indeed varied with scene size and regularity, such that RAND scenes were overall more variable then REG scenes and it is possible that listeners used this cue. Critically, however, sensitivity to spectral variability <italic>cannot</italic> explain the &#x2018;source effect&#x2019; observed in Experiment 3. The introduction of a single REG source within a context of RAND sources does not change spectral variability for scenes of 14 components, and does so marginally for scenes of 8 components. Therefore the improved performance in this condition is consistent with listeners tracking, at least a subset of, individual sources.</p>
<p>Key elements of the paradigm including the use of multiple, random-phase streams that are widely spaced in frequency were explicitly implemented to encourage listeners to process the signals as multiple concurrent &#x2018;auditory objects&#x2019;. Because the regular patterns characterizing each source are simple, relatively to the much more complex aggregate pattern, it is parsimonious to assume that patterns were extracted within each component separately (e.g. <xref ref-type="bibr" rid="c10">Dau et al 1997a</xref>,<xref ref-type="bibr" rid="c11">b</xref>). The experimental results, demonstrating very rapid response times in REG scenes provide further support for this assumption.</p>
<p>Mounting evidence suggests that listeners are tuned to the temporal structure of sound sequences and use this information to anticipate and improve their interaction with expected events, even in the absence of directed attention (<xref ref-type="bibr" rid="c5">Barnes &#x0026; Jones, 2000</xref>; <xref ref-type="bibr" rid="c16">Geiser et al, 2012</xref>; <xref ref-type="bibr" rid="c2">Andreou et al, 2015</xref>). Accumulating work using the &#x2018;omission MMN&#x2019; paradigm, demonstrates that the neural machinery is &#x2018;pre-activated&#x2019; to process predicted signals (e.g. Hughes et al, 2001; <xref ref-type="bibr" rid="c7">Bendixen et al, 2009</xref>; Janata et al, 2001). Importantly, we demonstrate that the auditory system&#x2019;s ability to track the temporal structure of on-going sound input and register when it is violated persists even when the scene is heavily populated with concurrent objects and the identity of the changing component is in advance unknown.</p>
</sec>
<sec id="s9b">
<title>Do regular patterns attract attention?</title>
<p>It has been suggested that attention can be understood as a process that infers the level of predictability of sensory signals such that highly predictable sensory streams capture attention in a bottom-up manner (<xref ref-type="bibr" rid="c20">Jones et al, 2002</xref>; <xref ref-type="bibr" rid="c3">Arnal &#x0026; Giraud, 2012</xref>; <xref ref-type="bibr" rid="c14">Feldman &#x0026; Friston, 2010</xref>). In line with this hypothesis, <xref ref-type="bibr" rid="c36">Zhao et al (2013)</xref> demonstrated that stimulus predictability biases attention: A visual search task was facilitated at a location which previously contained a regularity. This occurred even though participants reported not being aware of the regular pattern.</p>
<p>Here we similarly show that listeners benefitted from regularity regardless of not being consciously aware of it: Listeners were at floor when asked to determine whether a regular source was present in a scene containing 14 concurrent random streams (Experiment 4), but exhibited a sizeable improvement to change detection performance (an RT difference of 110ms) when that source disappeared (Experiment 3). This suggests that the temporal structure of that source was automatically tracked by the auditory system and used to facilitate scene analysis.</p>
<p>Furthermore, the results of Experiment 4 demonstrate that regular patterns in a background of random patterns do not pop-out and are in fact always harder to detect than vice versa. On the whole, the results suggest that while regularity plays a key role in shaping our perception of our surroundings, this does not translate to explicit attentional capture (see also Southwell et al, 2017).</p>
</sec>
<sec id="s9c">
<title>Computational mechanisms</title>
<p>In an MEG study (<xref ref-type="bibr" rid="c30">Sohoglu &#x0026; Chait, 2016b</xref>) we recorded responses to REG and RAND scenes (as in Experiment 1, here) in the context of an appearance (CA) detection task. The behavioural advantage associated with REG scenes was accompanied by increased responses in auditory cortex and parietal cortex both before, as well as after, the change. This was interpreted as reflecting the operation of mechanisms which rapidly infer the precision (predictability) of sensory input and upregulate responses to reliable sensory information, such that violations of these patterns (e.g, in the form of an appearing or disappearing sources) evoke higher prediction errors.</p>
<p>The behavioural effects observed here support this interpretation: The &#x2018;context&#x2019; effects shown in Experiment 3 (where listeners were better at detecting changes in scenes where the &#x2018;background, non-changing, sources were regular) and the demonstration that listeners are consistently better at spotting random sequences within scenes that otherwise comprised of regular components, than vice versa (Experiment 4) demonstrate increased sensitivity to deviants in REG than RAND context.</p>
<p>Overall, these results are consistent with an account according to which the auditory system rapidly discovers regular structure in the unfolding sensory input. Transients (onsets of individual tones) in regular streams are therefore predictable and easier to ignore (<xref ref-type="bibr" rid="c1">Andreou et al, 2011</xref>; Southwell et al, in 2007), rendering unexpected events as more salient. These findings are in line with a theoretical framework of brain function which views the brain as a regularity extractor (<xref ref-type="bibr" rid="c15">Friston, 2005</xref>; <xref ref-type="bibr" rid="c17">Hohwy, 2013</xref>; Winkler &#x0026; Schroger, 2015; <xref ref-type="bibr" rid="c34">Winkler et al, 2009</xref>; <xref ref-type="bibr" rid="c36">Zhao et al, 2013</xref>; Wacgone et al, 2011; <xref ref-type="bibr" rid="c6">Barascud et al, 2016</xref>).</p>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Andreou</surname>, <given-names>L.V.</given-names></string-name>, <string-name><surname>Kashino</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Chait</surname>, <given-names>M.</given-names></string-name> (<year>2011</year>). <article-title>The role of temporal regularity in stream segregation</article-title>. <source>Hearing Research</source>, <volume>280</volume>, <fpage>228</fpage>&#x2013;<lpage>35</lpage></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Andreou</surname>, <given-names>L.V</given-names></string-name>, <string-name><surname>Griffiths</surname>, <given-names>T.D.</given-names></string-name>, &#x0026; <string-name><surname>Chait</surname>, <given-names>M.</given-names></string-name> (<year>2015</year>) <article-title>Sensitivity to the temporal structure of rapid sound sequences - An MEG study</article-title>. <source>Neuroimage</source>. <volume>110</volume>, <fpage>194</fpage>&#x2013;<lpage>204</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Arnal</surname>, <given-names>L.H.</given-names></string-name>, &#x0026; <string-name><surname>Giraud</surname>, <given-names>A.L.</given-names></string-name> (<year>2012</year>) <article-title>Cortical oscillations and sensory predictions</article-title>. <source>Trends Cogn Sci</source>. <volume>16</volume>, <fpage>390</fpage>&#x2013;<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Auksztulewicz</surname>, <given-names>R.</given-names></string-name>, &#x0026; <string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name> (<year>2015</year>) <article-title>Attentional Enhancement of Auditory Mismatch Responses: a DCM/MEG Study</article-title>. <source>Cereb Cortex</source>:<fpage>1</fpage>&#x2013;<lpage>11</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Barnes</surname>, <given-names>R.</given-names></string-name>, &#x0026; <string-name><surname>Jones</surname>, <given-names>M.R.</given-names></string-name>, (<year>2000</year>). <article-title>Expectancy, attention, and time</article-title>. <source>Cogn Psychol</source>. <volume>41</volume>, <fpage>254</fpage>&#x2013;<lpage>311</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Barascud</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Pearce</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Griffiths</surname>, <given-names>T. D.</given-names></string-name>, <string-name><surname>Friston</surname>, <given-names>K. J.</given-names></string-name> &#x0026; <string-name><surname>Chait</surname>, <given-names>M.</given-names></string-name> (<year>2016</year>) <article-title>Brain responses in humans reveal ideal observer-like sensitivity to complex acoustic patterns</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>113</volume>, <fpage>E616</fpage>&#x2013;<lpage>E625</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Bendixen</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Schr&#x00F6;ger</surname>, <given-names>E.</given-names></string-name>, &#x0026; <string-name><surname>Winkler</surname>, <given-names>I.</given-names></string-name> (<year>2009</year>). <article-title>I heard that coming: event-related potential evidence for stimulus-driven prediction in the auditory system</article-title>. <source>J Neurosci</source>, <volume>29</volume>, <fpage>8447</fpage>&#x2013;<lpage>51</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Cervantes Constantino</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Pinggera</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Paranamana</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kashino</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Chait</surname>, <given-names>M.</given-names></string-name> (<year>2012</year>) <article-title>Detection of Appearing and Disappearing Objects in Complex Acoustic Scenes</article-title>. <source>PLoS ONE</source>, <volume>7</volume>(<issue>9</issue>), <fpage>e46167</fpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Costa-Faidella</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Baldeweg</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Grimm</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Escera</surname>, <given-names>C.</given-names></string-name> (<year>2011</year>). <article-title>Interactions between &#x201C;what&#x201D; and &#x201C;when&#x201D; in the auditory system: temporal predictability enhances repetition suppression</article-title>. <source>J Neurosci</source>, <volume>3</volume>, <fpage>18590</fpage>&#x2013;<lpage>7</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Dau</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kollmeier</surname>, <given-names>B.</given-names></string-name>, &#x0026; <string-name><surname>Kohlrausch</surname> <given-names>A.</given-names></string-name> (<year>1997a</year>) <article-title>Modeling auditory processing of amplitude modulation. I. Detection and masking with narrow-band carriers</article-title>. <source>J Acoust Soc Am</source>. <volume>102</volume>, <fpage>2892</fpage>&#x2013;<lpage>905</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Dau</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kollmeier</surname>, <given-names>B.</given-names></string-name>, &#x0026; <string-name><surname>Kohlrausch</surname>, <given-names>A.</given-names></string-name>, (<year>1997b</year>) <article-title>Modeling auditory processing of amplitude modulation. II. Spectral and temporal integration</article-title>. <source>J Acoust Soc Am</source>. <volume>102</volume>, <fpage>2906</fpage>&#x2013;<lpage>19</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Demany</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Semal</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Cazalets</surname>, <given-names>J.R.</given-names></string-name>, &#x0026; <string-name><surname>Pressnitzer</surname>, <given-names>D.</given-names></string-name> (<year>2010</year>). <article-title>Fundamental differences in change detection between vision and audition</article-title>. <source>Experimental Brain Research</source>, <volume>203</volume>, <fpage>261</fpage>&#x2013;<lpage>270</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Eramudugolla</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Irvine</surname>, <given-names>D.R.F</given-names></string-name>, <string-name><surname>McAnally</surname>, <given-names>K.I.</given-names></string-name>, <string-name><surname>Martin</surname>, <given-names>R.L.</given-names></string-name>, &#x0026; <string-name><surname>Mattingley</surname>,. <given-names>JB.</given-names></string-name> (<year>2005</year>). <article-title>Directed attention eliminates &#x201C;change deafness&#x201D; in complex auditory scenes</article-title>. <source>Current Biology</source>, <volume>15</volume>, <fpage>1108</fpage>&#x2013;<lpage>1113</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Feldman</surname>, <given-names>H.</given-names></string-name>, &#x0026; <string-name><surname>Friston</surname>, <given-names>K.J.</given-names></string-name> (<year>2010</year>) <article-title>Attention, uncertainty, and free-energy</article-title>. <source>Front Hum Neurosci</source> <volume>4</volume>: <fpage>215</fpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Friston</surname>, <given-names>K.</given-names></string-name> <year>2005</year> <article-title>A theory of cortical responses</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source> <volume>360</volume>, <fpage>815</fpage>&#x2013;<lpage>836</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Geiser</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Notter</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Gabrieli</surname>, <given-names>J.D.</given-names></string-name> (<year>2012</year>). <article-title>Acorticostriatal neural system Enhances auditory perception through temporal context processing</article-title>. <source>J Neurosci</source>. <volume>32</volume>, <fpage>6177</fpage>&#x2013;<lpage>6182</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="book"><string-name><surname>Hohwy</surname>, <given-names>J.</given-names></string-name> (<year>2013</year>). <source>The predictive mind</source>. <publisher-name>Oxford university Press</publisher-name></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Hughes</surname>, <given-names>H.C</given-names></string-name> <etal>et al</etal>. (<year>2001</year>) <article-title>Responses of human auditory association cortex to the omission of an expected acoustic event</article-title>. <source>Neuroimage</source>,<volume>13</volume>, <fpage>1073</fpage>&#x2013;<lpage>89</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Janata</surname>, <given-names>P.</given-names></string-name> (<year>2001</year>) <article-title>Brain electrical activity evoked by mental formation of auditory expectations and images</article-title>. <source>Brain Topogr</source>. <volume>13</volume>, <fpage>169</fpage>&#x2013;<lpage>93</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Jones</surname>, <given-names>M.R.</given-names></string-name>, <string-name><surname>Moynihan</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>MacKenzie</surname>, <given-names>N.</given-names></string-name>, &#x0026; <string-name><surname>Puente</surname>, <given-names>J.</given-names></string-name> (<year>2002</year>). <article-title>Temporal aspects of stimulus-driven attending in dynamic arrays</article-title>. <source>Psychol. Sci</source>. <volume>13</volume>, <fpage>313</fpage>&#x2013;<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Lange</surname>, <given-names>K.</given-names></string-name> (<year>2013</year>) <article-title>The ups and downs of temporal orienting: a review of auditory temporal orienting studies and a model associating the heterogeneous findings on the auditory N1 with opposite effects of attention and prediction</article-title>. <source>Front Hum Neurosci</source>. <volume>11</volume>, <issue>7</issue>: <fpage>263</fpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Leaver</surname>, <given-names>A.M.</given-names></string-name>, <string-name><surname>Van Lare</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zielinski</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Halpern</surname>, <given-names>A.R.</given-names></string-name>, &#x0026; <string-name><surname>Rauschecker</surname>, <given-names>J.P.</given-names></string-name> (<year>2009</year>) <article-title>Brain activation during anticipation of sound sequences</article-title>. <source>J Neurosci</source>. <volume>29</volume>, <fpage>2477</fpage>&#x2013;<lpage>85</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Moore</surname>, <given-names>B.C.J.</given-names></string-name>, &#x0026; <string-name><surname>Glasberg</surname>, <given-names>B.R.</given-names></string-name> (<year>1983</year>). <article-title>Suggested formulae for calculating auditory-filter bandwidths and excitation patterns</article-title>. <source>J. Acoust. Soc. Am</source>, <volume>74</volume>, <fpage>750</fpage>&#x2013;<lpage>753</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>N&#x00E4;&#x00E4;t&#x00E4;nen</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Kujala</surname>, <given-names>T.</given-names></string-name>, &#x0026; <string-name><surname>Winkler</surname>, <given-names>I.</given-names></string-name> (<year>2011</year>). <article-title>Auditory processing that leads to conscious perception: a unique window to central auditory processing opened by the mismatch negativity and related responses</article-title>. <source>Psychophysiology</source>, <volume>48</volume>, <fpage>4</fpage>&#x2013;<lpage>22</lpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Nelken</surname>, <given-names>I.</given-names></string-name> (<year>2012</year>). <article-title>Predictive information processing in the brain: the neural perspective</article-title>. <source>Int J Psychophysiol</source>, <volume>83</volume>, <fpage>253</fpage>&#x2013;<lpage>5</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Nobre.</surname> <given-names>A</given-names></string-name>, <string-name><surname>Correa</surname>, <given-names>A.</given-names></string-name>, &#x0026; <string-name><surname>Coull</surname>, <given-names>J.</given-names></string-name> (<year>2007</year>). <article-title>The hazards of time</article-title>. <source>Curr Opin Neurobiol</source>., <volume>17</volume>, <fpage>465</fpage>&#x2013;<lpage>70</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Rohenkohl</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Cravo</surname>, <given-names>A.M.</given-names></string-name>, <string-name><surname>Wyart</surname>, <given-names>V.</given-names></string-name>, &#x0026; <string-name><surname>Nobre</surname>, <given-names>A.C.</given-names></string-name> (<year>2012</year>) <article-title>Temporal expectation improves the quality of sensory information</article-title>. <source>J Neurosci</source>. <volume>32</volume>, <fpage>8424</fpage>&#x2013;<lpage>8</lpage></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Snyder</surname>, <given-names>J.S.</given-names></string-name>, <string-name><surname>Gregg</surname>, <given-names>M.K.</given-names></string-name>, <string-name><surname>Weintraub</surname>, <given-names>D.M.</given-names></string-name>, &#x0026; <string-name><surname>Alain</surname>, <given-names>C.</given-names></string-name> (<year>2012</year>). <article-title>Attention, awareness, and the perception of auditory scenes</article-title>. <source>Front Psychol</source>., <volume>3</volume>, <fpage>15</fpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Sohoglu</surname>, <given-names>E.</given-names></string-name>, &#x0026; <string-name><surname>Chait</surname>, <given-names>M.</given-names></string-name> (<year>2016a</year>) <article-title>Neural dynamics of change detection in crowded acoustic scenes</article-title>. <source>Neuroimage</source> <volume>126</volume>: <fpage>164</fpage>&#x2013;<lpage>172</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Sohoglu</surname> <given-names>E</given-names></string-name> &#x0026; <string-name><surname>Chait</surname> <given-names>M</given-names></string-name> (<year>2016b</year>) <article-title>Detecting and representing predictable structure during auditory scene analysis</article-title>. <source>eLife</source>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="other"><string-name><surname>Southwell</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Baumann</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gal</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Barascud</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Friston</surname>, <given-names>K.J.</given-names></string-name>, &#x0026; <string-name><surname>Chait</surname>, <given-names>M.</given-names></string-name> (in press) <article-title>Is predictability salient? A study of attentional capture by auditory patterns</article-title>. <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Wacongne</surname>, <given-names>C.</given-names></string-name>, <etal>et al</etal> (<year>2011</year>) <article-title>Evidence for a hierarchy of predictions and prediction errors in human cortex</article-title>. <source>Proc Natl Acad Sci U S A</source>., <volume>108</volume>, <fpage>20754</fpage>&#x2013;<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Winkler</surname>, <given-names>I.</given-names></string-name>, &#x0026; <string-name><surname>Schr&#x00F6;ger</surname>, <given-names>E.</given-names></string-name> (<year>2015</year>) <article-title>Auditory perceptual objects as generative models: Setting the stage for communication by sound</article-title>. <source>Brain Lang</source>. <volume>148</volume>, <fpage>1</fpage>&#x2013;<lpage>22</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Winkler</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Denham</surname>, <given-names>S.L.</given-names></string-name>, &#x0026; <string-name><surname>Nelken</surname>, <given-names>I.</given-names></string-name> (<year>2009</year>). <article-title>Modeling the auditory scene: predictive regularity representations and perceptual objects</article-title>. <source>Trends Cogn. Sci</source>. <volume>13</volume>, <fpage>532</fpage>&#x2013;<lpage>40</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Yaron</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Hershenhoren</surname>, <given-names>I.</given-names></string-name>, &#x0026; <string-name><surname>Nelken</surname>, <given-names>I.</given-names></string-name> (<year>2012</year>). <article-title>Sensitivity to Complex Statistical Regularities in Rat Auditory Cortex</article-title>. <source>Neuron</source>, <volume>76</volume>,<fpage>603</fpage>&#x2013;<lpage>615</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Zhao</surname> <given-names>J</given-names></string-name>, <string-name><surname>Al-Aidroos</surname> <given-names>N</given-names></string-name>, <string-name><surname>Turk-Browne</surname> <given-names>NB</given-names></string-name> (<year>2013</year>) <article-title>Attention Is Spontaneously Biased Toward Regularities</article-title>. <source>Psychol Sci</source> <volume>24</volume>: <fpage>667</fpage>&#x2013;<lpage>677</lpage>.</mixed-citation></ref>
</ref-list>
<sec>
<title>Author contribution statement:</title>
<p>MC designed the experiments; LA and LVA conducted and analysed the experiments; All authors wrote the manuscript.</p>
</sec>
<ack>
<title>Acknowledgements:</title>
<p>This study was supported by a BBSRC project grant to MC.</p>
</ack>
</back>
</article>