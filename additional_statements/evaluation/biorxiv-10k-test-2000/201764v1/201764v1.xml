<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/201764</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Deep convolutional models improve predictions of macaque V1 responses to natural images</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Cadena</surname><given-names>Santiago A.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="corresp" rid="cor1">&#x0040;</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Denfield</surname><given-names>George H.</given-names></name>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Walker</surname><given-names>Edgar Y.</given-names></name>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Gatys</surname><given-names>Leon A.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Tolias</surname><given-names>Andreas S.</given-names></name>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Bethge</surname><given-names>Matthias</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Ecker</surname><given-names>Alexander S.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a6">6</xref>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Centre for Integrative Neuroscience and Institute for Theoretical Physics, University of T&#x00FC;bingen</institution>, <country>Germany</country></aff>
<aff id="a2"><label>2</label><institution>Max Planck Institute for Biological Cybernetics</institution>, T&#x00FC;bingen, <country>Germany</country></aff>
<aff id="a3"><label>3</label><institution>Bernstein Center for Computational Neuroscience</institution>, T&#x00FC;bingen, <country>Germany</country></aff>
<aff id="a4"><label>4</label><institution>Department of Neuroscience, Baylor College of Medicine</institution>, Houston, TX, <country>USA</country></aff>
<aff id="a5"><label>5</label><institution>Department of Electrical and Computer Engineering, Rice University</institution>, Houston, TX, <country>USA</country></aff>
<aff id="a6"><label>6</label><institution>Center for Neuroscience and Artificial Intelligence</institution>, BCM. Houston, TX, <country>USA</country></aff>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>&#x002A;</label><p>These authors contributed equally</p></fn>
<corresp id="cor1"><label>&#x0040;</label>Corresponding author: <email>santiago.cadena@bethgelab.org</email></corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2017</year></pub-date>
<elocation-id>201764</elocation-id>
<history>
<date date-type="received">
<day>11</day>
<month>10</month>
<year>2017</year>
</date>
<date date-type="rev-recd">
<day>11</day>
<month>10</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>11</day>
<month>10</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="201764.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract><title>Abstract</title>
<p>Despite great efforts over several decades, our best models of primary visual cortex (V1) still predict neural responses quite poorly when probed with natural stimuli, highlighting our limited understanding of the nonlinear computations in V1. At the same time, recent advances in machine learning have shown that deep neural networks can learn highly nonlinear functions for visual information processing. Two approaches based on deep learning have recently been successfully applied to neural data: transfer learning for predicting neural activity in higher areas of the primate ventral stream and data-driven models to predict retina and V1 neural activity of mice. However, so far there exists no comparison between the two approaches and neither of them has been used to model the early primate visual system. Here, we test the ability of both approaches to predict neural responses to natural images in V1 of awake monkeys. We found that both deep learning approaches outperformed classical linear-nonlinear and wavelet-based feature representations building on existing V1 encoding theories. On our dataset, transfer learning and data-driven models performed similarly, while the data-driven model employed a much simpler architecture. Thus, multi-layer CNNs set the new state of the art for predicting neural responses to natural images in primate V1. Having such good predictive <italic>in-silico</italic> models opens the door for quantitative studies of yet unknown nonlinear computations in V1 without being limited by the available experimental time.</p>
</abstract>
<counts>
<page-count count="16"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1"><label>1</label><title>Introduction</title>
<p>An essential step towards understanding visual processing in the brain is building models that accurately predict neural responses to arbitrary stimuli (<xref ref-type="bibr" rid="c6">Carandini et al., 2005</xref>). Primary visual cortex (V1) has been a strong focus of sensory neuroscience ever since Hubel and Wiesel&#x2019;s seminal studies demonstrated that neurons in V1 respond selectively to distinct image features like local orientation and contrast (<xref ref-type="bibr" rid="c20">Hubel and Wiesel, 1959</xref>, <xref ref-type="bibr" rid="c21">1968</xref>). Our current standard model of V1 is based on linear-nonlinear models (LN) (<xref ref-type="bibr" rid="c22">Jones and Palmer, 1987</xref>; <xref ref-type="bibr" rid="c18">Heeger, 1992</xref>) and energy models (<xref ref-type="bibr" rid="c1">Adelson and Bergen, 1985</xref>) to explain simple and complex cells, respectively. While these simple models explain responses to simple stimuli such as gratings reasonably well, they fail to account for neural responses to natural stimuli (<xref ref-type="bibr" rid="c30">Olshausen and Field, 2005</xref>; <xref ref-type="bibr" rid="c37">Talebi and Baker, 2012</xref>).</p>
<p>Simple LN models fail because natural stimuli unlock nonlinear subunits that cannot be captured by a linear transformation of the stimulus. To address this issue, LN-LN cascade models have been proposed, which either learn (convolutional) subunits (<xref ref-type="bibr" rid="c33">Rust et al., 2005</xref>; <xref ref-type="bibr" rid="c38">Touryan et al., 2005</xref>; <xref ref-type="bibr" rid="c39">Vintch et al., 2015</xref>) or use handcrafted wavelet representations (<xref ref-type="bibr" rid="c40">Willmore et al., 2008</xref>). These cascade models outperform simple LN models, but they currently do not capture the full range of nonlinearities observed in V1, such as gain control mechanisms and potentially other not-yet-understood nonlinear response properties. Because experimental time is limited, LN-LN models have to be designed carefully to keep the number of parameters tractable, limiting their expressiveness to energy models for direction-selective and complex cells.</p>
<p>Recent advances in machine learning and computer vision using deep neural networks (&#x2018;deep learning&#x2019;) have opened a new door to learn much more complex non-linear models of neural responses. We identify two main approaches that we refer to as goal-driven, and data-driven.</p>
<p>The goal-driven approach is based on transfer learning (<xref ref-type="bibr" rid="c10">Donahue et al., 2014</xref>), a paradigm that has been very successful in deep learning. Convolutional neural networks (CNNs) have reached human-level performance on visual tasks like object classification by training on more than one million images (<xref ref-type="bibr" rid="c27">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="c36">Simonyan and Zisserman, 2014</xref>; <xref ref-type="bibr" rid="c17">He et al., 2016</xref>; <xref ref-type="bibr" rid="c19">Huang et al., 2016</xref>). These CNNs optimized for visual tasks have proven extremely useful as nonlinear feature spaces for tasks where less labeled data is available (e.g. <xref ref-type="bibr" rid="c28">K&#x00FC;mmerer et al. 2014</xref>). This transfer to a new task is achieved by (linearly) reading out the network&#x2019;s internal representations of the input. Yamins, DiCarlo and colleagues showed that using deep networks trained on large-scale object recognition as nonlinear feature spaces for neural system identification works remarkably well in higher areas of the ventral stream, such as V4 and IT (<xref ref-type="bibr" rid="c43">Yamins et al., 2014</xref>).</p>
<p>The deep data-driven approach is based on fitting all model parameters directly to neural data (<xref ref-type="bibr" rid="c2">Antol&#x00ED;k et al., 2016</xref>; <xref ref-type="bibr" rid="c3">Batty et al., 2016</xref>; <xref ref-type="bibr" rid="c29">McIntosh et al., 2016</xref>; <xref ref-type="bibr" rid="c25">Klindt et al., 2017</xref>). The critical advance of deep models in neural system identification is that they can have many more parameters than classical LN cascade models discussed above, because they exploit computational similarities between different neurons. While previous approaches treated each neuron as an individual multivariate regression problem, modern CNN-based approaches learn one model for an entire population of neurons, thereby exploiting two key properties of local neural circuits: (1) they share the same presynaptic circuitry (for V1: retina and LGN) and (2) many neurons perform essentially the same computation, but at different locations (topographic organization, implemented by convolutional weight sharing).</p>
<p>While both, goal-driven and data-driven approaches have been shown to outperform LN models, it is currently unknown how their performance compares. Moreover, neither approach has been evaluated in monkey V1 (see <xref ref-type="bibr" rid="c23">Kindel et al. 2017</xref> for concurrent work). Here, we address this, and show that both methods perform similarly well outperforming classic LN cascade models by a substantial margin, offering an alternative to study unknown V1 nonlinear properties.</p>
</sec>
<sec id="s2"><label>2</label><title>Materials and Methods</title>
<sec id="s2a"><label>2.1</label><title>Electrophysiological recordings</title>
<p>We performed non-chronic recordings from two adult male rhesus monkeys (aged 8 and 11; weighing 10.9kg and 12.1kg) with a 32-channel linear silicon probe (NeuroNexus V1x32-Edge-10mm-60-177). The surgical methods and recording protocol were described previously (<xref ref-type="bibr" rid="c9">Denfield et al., 2017</xref>). Briefly, form-specific titanium recording chambers and headposts were implanted under full anesthesia and aseptic conditions. The bone was originally left intact, and only prior to recordings small trephinations (2 mm) were made over medial primary visual cortex at eccentricities ranging from 1.4 to 3.0 degrees of visual angle. Recordings were done within two weeks of each trephination. Probes were lowered using a Narishige Microdrive (MO-97) and a guide tube to penetrate the dura. Care was taken lower the probe slowly, not to penetrate the cortex with the guide tube and to minimize tissue compression (for a detailed description of the procedure, see <xref ref-type="bibr" rid="c9">Denfield et al. 2017</xref>). All experimental procedures complied with guidelines of the NIH and were approved by the Baylor College of Medicine Institutional Animal Care and Use Committee (permit number: AN-4367).</p>
</sec>
<sec id="S2b"><label>2.2</label><title>Data acquisition and spike sorting</title>
<p>Electrophysiological data were collected continuously as broadband signal (0.5-16,000 Hz) digitized at 24 bits as described previously (<xref ref-type="bibr" rid="c12">Ecker et al., 2010</xref>). Our spike sorting methods are based on (<xref ref-type="bibr" rid="c11">Ecker et al. 2014</xref>, code available at <ext-link ext-link-type="uri" xlink:href="https://github.com/aecker/moksm">https://github.com/aecker/moksm</ext-link>), but with adaptations to the novel type of silicon probe as described previously (<xref ref-type="bibr" rid="c9">Denfield et al., 2017</xref>). Briefly, we split the linear array of 32 channels into 14 groups of 6 adjacent channels (with a stride of two), which we treated as virtual electrodes for spike detection and sorting. Spikes were detected when channel signals crossed a threshold of five times the standard deviation of the noise. After spike alignment, we extracted the first three principal components of each channel, resulting in an 18-dimensional feature space used for spike sorting. We fitted a Kalman filter mixture model (<xref ref-type="bibr" rid="c5">Calabrese and Paninski, 2011</xref>; <xref ref-type="bibr" rid="c34">Shan et al., 2017</xref>) to track waveform drift typical for non-chronic recordings. The shape of each cluster was modeled with a multivariate t-distribution (<italic>df</italic> &#x003D; 5) with a ridge regularized covariance matrix. The number of clusters was determined based on a penalized average likelihood with a constant cost per additional cluster (<xref ref-type="bibr" rid="c11">Ecker et al., 2014</xref>). Subsequently, we used a custom graphical user interface to manually verify single-unit isolation by assessing the stability of the units (based on drifts and health of the cells throughout the session), identifying a refractory period, and inspecting the scatter plots of the pairs of channel principal components.</p>
</sec>
<sec id="s2c"><label>2.3</label><title>Visual stimulation and eye tracking</title>
<p>Visual stimuli were rendered by a dedicated graphics workstation and displayed on a CRT monitor with a 100 Hz refresh rate. The monitors were gamma corrected to have a linear luminance response profile. A camera-based, custom-built eye tracking system verified that monkeys maintained fixation within &#x007E; 0.42 degrees around the target. Offline analysis showed that monkeys typically fixated much more accurately. The monkeys were trained to fixate on a red target of &#x007E; 0.15 degrees in the middle of the screen. After they maintained fixation for 300 ms, a visual stimulus appeared. If the monkeys fixated throughout the entire stimulus period, they received a drop of juice at the end of the trial.</p>
</sec>
<sec id="s2d"><label>2.4</label><title>Receptive field mapping</title>
<p>At the beginning of each session, we first mapped receptive fields. We used a sparse random dot stimulus for receptive field mapping. A single dot of size 0.12 degrees of visual field was presented on a uniform gray background, changing location and color (black or white) randomly every 30 ms. Each trial lasted 2 seconds. We obtained multi-unit receptive field profiles for every channel using reverse correlation. We then estimated the population receptive field location by fitting a 2D Gaussian to the spike-triggered average across channels at the time lag that maximizes the signal-to-noise-ratio. We subsequently placed our natural image stimulus at this location.</p>
</sec>
<sec id="s2e"><label>2.5</label><title>Natural image stimulus</title>
<p>We used an approach similar to <xref ref-type="bibr" rid="c13">Freeman et al. 2013</xref>. We generated stimuli with different degrees of &#x201C;naturalness&#x201D; by capturing different levels of higher order correlations from a local to a global scale. This was achieved by using a parametric model for texture synthesis proposed by <xref ref-type="bibr" rid="c14">Gatys et al. 2015</xref> that uses the pre-trained feature maps of VGG-19 (<xref ref-type="bibr" rid="c36">Simonyan and Zisserman, 2014</xref>). Briefly, the algorithm consists of analysis and synthesis stages. During analysis, the summary statistics&#x2014;given by the correlation matrix between feature maps (also, Gram matrix)&#x2014; are computed for each layer in the net. During synthesis, by starting with a random white noise image, pixels are pushed (usually via gradient descend) in a direction that leads to Gram matrices matching those of the original image.</p>
<p>For visual stimuli, we randomly selected and gray-scaled 1450 images from ImageNet (<xref ref-type="bibr" rid="c32">Russakovsky et al., 2015</xref>). Additionally, for every image, we synthesized four new types of images using the parametric texture model (<xref ref-type="bibr" rid="c14">Gatys et al., 2015</xref>). For displaying and further analyses, we cropped the central 140 pixels of each image. For texture synthesis, we matched all the Gram matrices cumulatively up to conv1, conv2 and conv3 and conv4. (e.g. the conv3 model matches Gram matrices for layers conv1_1, conv2_1 and conv3_1 of VGG-19). <xref ref-type="fig" rid="fig1">Figure 1A</xref> shows three example images with their respective texturized versions.</p>
<fig id="fig1" position="float" fig-type="figure"><label>Figure 1.</label>
<caption><title>Stimulus paradigm</title>
<p><bold>A</bold>. Classes of images shown in the experiment. We used grayscale natural images (labeled &#x2018;original&#x2019;) from the ImageNet dataset (<xref ref-type="bibr" rid="c32">Russakovsky et al., 2015</xref>) along with textures synthesized from these images using the texture synthesis algorithm described by <xref ref-type="bibr" rid="c14">Gatys et al. 2015</xref>. Each row shows four synthesized versions of three example original images using different convolutional layers (see Materials and Methods for details). Lower convolutional layers capture more local statistics compared to higher ones. <bold>B</bold>. Stimulus sequence. In each trial, we showed a randomized sequence of images (each displayed for 60 ms covering 2 degrees of visual angle) centered on the receptive fields of the recorded neurons while the monkey sustained fixation on a target. The images were masked with a circular mask with cosine fadeout.</p></caption>
<graphic xlink:href="201764_fig1.tif"/>
</fig>
<p>The entire data set contains 1450 &#x00D7; 5 &#x003D; 7250 images (original plus synthesized). During each trial, 29 images were displayed, each for 60 ms, with no blanks in between (<xref ref-type="fig" rid="fig1">Figure 1 B</xref>). Each image was masked by a circular mask with a diameter of 2 degrees and a soft fade-out starting at a diameter of 1 degree:
<disp-formula id="ueqn1"><alternatives><graphic xlink:href="201764_ueqn1.gif"/></alternatives></disp-formula></p>
<p>Images were randomized such that consecutive images were not of the same type or synthesized from the same image. A full pass through the dataset took 250 successful trials, after which it was traversed again in a new random order. Images were repeated between one and four times, depending on how many trials the monkeys completed in each session.</p>
</sec>
<sec id="s2f"><label>2.6</label><title>GLM with pre-trained CNN features</title>
<p>Our proposed model consists of two parts: feature extraction and a generalized linear model (GLM; <xref ref-type="fig" rid="fig3">Fig. 3</xref>). The features are the output maps of intermediate convolutional layers of VGG-19 (<xref ref-type="bibr" rid="c36">Simonyan and Zisserman, 2014</xref>) to a stimulus image. We fit a separate GLM for each convolutional layer of VGG-19. We used a normalized version of VGG-19, where the weights have been rescaled such that the average activation of each feature map over a large set of natural images is equal to one (<xref ref-type="bibr" rid="c15">Gatys et al., 2016</xref>). The original 140 px images were first cropped to omit the 30 px border and then downsampled by a factor of two, resulting in images of size 40 px. The output of the convolutional layers is a set of <italic>K</italic> feature maps (denoted as depth in <xref ref-type="fig" rid="fig3">Fig. 3</xref>).</p>
<p>The GLM consists of linear fully connected weights <italic>w</italic><sub><italic>ijk</italic></sub> for each neuron that compute a dot product with the input feature maps <italic>&#x03C8;</italic><sub><italic>ijk</italic></sub>(<italic>x</italic>), an exponential nonlinearity, and Poisson noise. Here, <italic>i</italic> and <italic>j</italic> index space, while <italic>k</italic> indexes feature maps. The weights have the same dimensionality as the feature maps. The spiking rate of a given neuron <italic>r</italic> will follow:
<disp-formula id="eqn1"><alternatives><graphic xlink:href="201764_eqn1.gif"/></alternatives></disp-formula></p>
<p>Additionally, three regularization terms were applied to the weights:
<list list-type="order">
<list-item>
<p><bold>Sparsity</bold>: Most weights need to be zero since we expect the spatial pooling to be localized. We use the L1 norm of the weights:
<disp-formula id="eqn2"><alternatives><graphic xlink:href="201764_eqn2.gif"/></alternatives></disp-formula></p>
</list-item>
<list-item>
<p><bold>Spatial Smoothness</bold>: Together with sparseness, spatial smoothness encourages spatial locality by imposing continual regular changes in space. We computed this by an L2 penalty on the Laplacian of the weights:
<disp-formula id="eqn3"><alternatives><graphic xlink:href="201764_eqn3.gif"/></alternatives></disp-formula></p>
</list-item>
<list-item>
<p><bold>Group Sparsity</bold>: Encourages our model to pool from a reduced set of feature maps to explain each neuron&#x2019;s responses:
<disp-formula id="eqn4"><alternatives><graphic xlink:href="201764_eqn4.gif"/></alternatives></disp-formula></p>
</list-item>
</list>
</p>
<p>Considering the recorded image-response as (<italic>x</italic>, <italic>y</italic>) for one neuron, the resulting loss function is given
by:
<disp-formula id="eqn5"><alternatives><graphic xlink:href="201764_eqn5.gif"/></alternatives></disp-formula></p>
<p>We fit the model by minimizing the loss using the Adam optimizer (<xref ref-type="bibr" rid="c24">Kingma and Ba, 2014</xref>) on a training set consisting of 80&#x0025; of the data, and reported performance on the remaining 20&#x0025;. We cross-validated the parameters &#x03BB;<sub><italic>sparse</italic></sub>, &#x03BB;<sub><italic>lap</italic></sub>, &#x03BB;<sub><italic>group</italic></sub> for each neuron independently by performing a grid search over four logarithmically spaced values for each parameter. The validation was done on 20&#x0025; of the training data. The same split of data for training, validation, and testing was used to fit all models in this study.</p>
</sec>
<sec id="s2g"><label>2.7</label><title>Convolutional neural network model</title>
<p>We followed the results of <xref ref-type="bibr" rid="c25">Klindt et al. 2017</xref> and use their best-performing architecture that obtained state-of-the-art performance on a public dataset (<xref ref-type="bibr" rid="c2">Antol&#x00ED;k et al., 2016</xref>). As our VGG-based model, this model also consisted of convolutional feature extraction followed by a GLM, the difference being that here the convolutional feature space was learned from neural data instead of having been trained on object recognition. The feature extraction architecture consisted of three convolutional layers with filters of receptive field size 13 &#x00D7; 13 px for the first layer and 3 &#x00D7; 3 px for the subsequent layers. Each layer had 32 feature maps (<xref ref-type="fig" rid="fig5">Fig. 5</xref>). As in the original publication (<xref ref-type="bibr" rid="c25">Klindt et al., 2017</xref>) we regularized the convolutional filters by imposing smoothness constraints on the first layer and group sparseness on the second and third. A notable difference to our VGG-based GLM is that here the readout weights are factorized in space and feature maps:
<disp-formula id="ueqn2"><alternatives><graphic xlink:href="201764_ueqn2.gif"/></alternatives></disp-formula>
where <italic>u</italic><sub><italic>ij</italic></sub> is a spatial mask and <italic>v</italic><sub><italic>k</italic></sub> a set of feature pooling weights. We used an exponential linear unit (ELU; <xref ref-type="bibr" rid="c8">Clevert et al. 2015</xref>) as the output nonlinearity.</p>
</sec>
<sec id="s2h"><label>2.8</label><title>Other baseline models</title>
<p>The performance of the two convolutional models was compared with two alternative models: a regularized linear nonlinear Poisson model (LNP; <xref ref-type="bibr" rid="c35">Simoncelli et al. 2004</xref>) and the Berkeley wavelet transform (BWT) linearized model (<xref ref-type="bibr" rid="c40">Willmore et al., 2008</xref>). Images were down-sampled to 40 px as in our proposed model.</p>
<p>The LNP model was fitted using two regularization terms: smoothness and sparseness. Their corresponding parameters were cross-validated independently for each cell as above.</p>
<p>The BWT model (<xref ref-type="bibr" rid="c40">Willmore et al., 2008</xref>, <xref ref-type="bibr" rid="c41">2010</xref>) uses a set of scaled, oriented, frequency-and phase-shifted wavelets to decompose the original image. We used the publicly available implementation from StrfLab (<xref ref-type="bibr" rid="c40">Willmore et al., 2008</xref>) and we set the temporal size and temporal velocities to one. We picked the following parameters for the Gabor wavelet bank that lead to best performance on test set in order for it to be competitive with the other methods: 16 evenly spaced orientations, 5 frequency divisions between 0.5 and 6 cycles per degree, 0.5 ratio between the Gaussian window and spatial frequency, 2.5 standard deviation of the Gaussian window of spatial separation of each wavelet. A log link and Poisson noise were used to fit the regression weights on top of the feature space.</p>
</sec>
<sec id="s2i"><label>2.9</label><title>Performance evaluation</title>
<p>We measured the performance of all models with the fraction of explainable variance explained <italic>FEV</italic>. That is, the ratio between the variance accounted for by the model (variance explained) and the explainable variance. The explainable variance is lower than the total variance, because observation noise prevents even a perfect model from accounting for all variance. We estimated the amount of observation noise by averaging the variance across images of responses to the same stimulus: <italic>E</italic><sub>j</sub>[<italic>Var</italic><sub><italic>i</italic></sub>[<italic>y</italic><sub><italic>i</italic></sub>&#x007C;<italic>x</italic><sub><italic>j</italic></sub>]]. If our model predicted an average response of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="201764_inline1.gif"/></alternatives></inline-formula>, then <italic>FEV</italic> is computed as in <xref ref-type="disp-formula" rid="eqn6">equation 6</xref> for the observed spike counts <italic>y</italic>
<disp-formula id="eqn6"><alternatives><graphic xlink:href="201764_eqn6.gif"/></alternatives></disp-formula></p>
</sec>
</sec>
<sec id="s3"><label>3</label><title>Results</title>
<p>We measured the spiking activity of populations of neurons in V1 of two awake, fixating rhesus macaques using a 32-channel linear array spanning all cortical layers (<xref ref-type="fig" rid="fig2">Fig. 2A</xref>). We isolated 307 neurons in 23 sessions. Our stimuli consisted of synthesized images that capture different levels of high order correlations present in natural images (see Methods). Each stimulus was shown for 60ms, with no in-between blanks, and was centered on the mapped population receptive field of the neurons. The images were scaled to have the same contrast.</p>
<fig id="fig2" position="float" fig-type="figure"><label>Figure 2.</label>
<caption><title>V1 electrophysiological responses.</title>
<p><bold>A</bold>. Isolated single unit activity. We performed acute recordings with a 32-channel, linear array (NeuroNexus V1x32-Edge-10mm-60-177, layout shown in the left) to record in primary visual cortex of two awake, fixating macaques. The channel mean-waveform footprints of the spiking activity of 23 well-isolated neurons in one example session are shown in the central larger panel. The upper panel shows color-matched autocorrelo-grams. <bold>B</bold>. Peri-stimulus time histograms (PSTH) of four example neurons from A. Spike counts where binned with t &#x003D; 1 ms, aligned to the onset of each stimulus image, and averaged over trials. The 60 ms interval where the image was displayed is shown in red. We ignored the temporal profile of the response and extracted spike counts for each image on the 40-100 ms interval after image onset (shown in light gray). <bold>C</bold>. The Response Triggered Average (RTA) calculated by reverse correlation of the extracted responses.</p></caption>
<graphic xlink:href="201764_fig2.tif"/>
</fig>
<p>The entire set of stimuli has 7250 unique images and was shown one to four times for each session. We used only sessions with two or more repetitions, and applied a selection criterion to the neurons based on how much of their variability was induced by the stimulus. We estimated the observation noise by averaging the variance of responses to repeated presentations of the images. By subtracting the average trial-to-trial variance to repeated presentations from the total variance of the responses for every neuron, we obtain an estimate of the explainable variance. We discarded neurons with a ratio of explainable-to-total variance smaller than 0.15, yielding 166 isolated neurons recorded in 17 sessions. 51 neurons belonged to sessions with two repetitions and 115 to those with four.</p>
<sec id="s3a"><label>3.1</label><title>Generalized linear model with pre-trained CNN features</title>
<p>We used the network VGG-19 (<xref ref-type="bibr" rid="c36">Simonyan and Zisserman, 2014</xref>) to extract a nonlinear feature space for a generalized linear model (GLM). VGG-19 is a CNN trained on the large image classification task ImageNet (ILSVRC2012) that takes an RGB image as input and infers the class of the dominant object in the image (among 1000 possible classes). Its architecture consists of a hierarchy of linear-nonlinear transformations (layers) where the input is spatially convolved with a set of filters and then passed through a rectifying nonlinearity (<xref ref-type="fig" rid="fig3">Fig. 3</xref>). The output of such layers produces a number of feature maps that serve as input for the next layer. Additionally, the network has pooling layers where the feature maps are down-sampled by taking the local maximum values of neighboring pixels. There are 16 convolutional layers that can be grouped into five groups with 2, 2, 4, 4, 4 convolutional layers and 64, 128, 256, 512, 512 output feature maps, respectively, and a pooling layer in between each group.</p>
<fig id="fig3" position="float" fig-type="figure"><label>Figure 3.</label>
<caption><title>Our proposed model</title>
<p>For each of the 16 convolutional layers of VGG-19 (<xref ref-type="bibr" rid="c36">Simonyan and Zisserman, 2014</xref>), we extract the output feature maps of the images shown to the monkey. We then train for each neuron a Generalized Linear Model with Poisson noise and log link on top of this representation to predict the observed spike counts from monkey V1. The linear readouts have the same size as the feature maps and their resulting dot product is fed to the exponential nonlinearity. The learning objective was Maximum Likelihood with three regularization terms on the weights for sparseness, spatial smoothness, and group sparseness (see Methods). This facilitated identifying a reduced set of feature maps to pool from, as well as the location of each neuron&#x2019;s receptive field.</p></caption>
<graphic xlink:href="201764_fig3.tif"/>
</fig>
<p>For each convolutional layer of VGG-19, we fit a GLM that uses this layer&#x2019;s representation of the stimulus as a nonlinear feature space. To do so, we fed all images in our stimulus set through the network and extracted the feature maps of every convolutional layer (<xref ref-type="fig" rid="fig3">Fig. 3</xref>). We then learned a set of linear weights followed by an exponential nonlinearity to predict each neuron&#x2019;s response (<xref ref-type="fig" rid="fig3">Fig. 3</xref>). Since the convolutional feature spaces are larger than the number of pixels in the image, regularization of the readout weights is particularly important. We used three regularization terms for the weights. (1) Sparseness, because receptive fields are localized, we expect most weights to be zero; (2) smoothness, to encourage a regular spatial continuity of the receptive fields; and (3) group sparsity, which encourages the model to pool only from a small number of feature maps. We fit this model for each convolutional layer of VGG-19 to maximize the likelihood of the predicted response under a Poisson noise model and cross-validated over the three regularization terms for each cell independently.</p>
<p>To measure our model&#x2019;s performance and compare it to others, we computed the fraction of explain-able variance explained (<italic>FEV</italic>). This metric, which ranges from 0 to 1, measures what fraction of the stimulus-driven response is explained by the model, ignoring the unexplainable trial-to-trial variability in the neurons&#x2019; responses (for details see Methods).</p>
</sec>
<sec id="s3b"><label>3.2</label><title>Intermediate layers of VGG best predict V1 responses</title>
<p>The model based on the fifth (out of sixteen) layers&#x2019; features (called &#x2018;conv3_1&#x2019;, <xref ref-type="fig" rid="fig3">Fig. 3</xref>) best predicted neuronal responses to novel images not seen during training (<xref ref-type="fig" rid="fig4">Fig. 4</xref>). This model predicted on average 50.1&#x0025; of the explainable variance. In contrast, performance for the very first layer was poor (31&#x0025; <italic>FEV</italic>), but increased monotonically up to conv3_1. Afterwards, the performance again decreased continually up the hierarchy (<xref ref-type="fig" rid="fig4">Fig. 4</xref>). These results followed our intuition that early to intermediate processing stages in a hierarchical model should match primary visual cortex, given that V1 is the third processing stage in the visual hierarchy after the retina and LGN.</p>
<fig id="fig4" position="float" fig-type="figure"><label>Figure 4.</label>
<caption><title>Model performance on test set</title>
<p>Average fraction of explainable variance explained (<italic>FEV</italic>) on test set. Conv3_1 shows on average the highest predictive performance for both models trained with a fixed input size for all layers, and rescaled inputs to match units&#x2019; receptive field sizes across layers.</p></caption>
<graphic xlink:href="201764_fig4.tif"/>
</fig>
<p>One potential concern is that the performance curve may be related more to receptive field size of the units-the size of the input region which a unit depends on-than to actual nonlinear response properties. Each VGG layer convolves its inputs with a 3 &#x00D7; 3 px kernel, leading to growing receptive field sizes along the hierarchy. For example, the receptive field size of units in the first layer (&#x2018;conv1_1&#x2019;) is 3 &#x00D7; 3 px (which covers only 0.08 degrees of visual angle). Because nonlinear features are extracted at the scale of the units&#x2019; receptive field, it may be important to match receptive field sizes between each VGG layer and V1. To address this concern, we resized the input image for each layer model such that the receptive field size of units in the layer that provided the feature space roughly matched 2 degrees of visual angle (the field of view that V1 neurons were stimulated with). In this way, the image patch each VGG unit saw was equivalent across layers. This procedure was done for the first nine convolutional layers as performance was already steadily decreasing. The resulting performance across layers agreed with the previous results (conv3_1 was still the best performing layer; <xref ref-type="fig" rid="fig4">Fig 4</xref>, dashed gray line).</p>
</sec>
<sec id="s3c"><label>3.3</label><title>Data-driven CNN model and GLM with pre-trained CNN set state of the art</title>
<p>We next asked how the predictive performance of our VGG-based model compared to other quantitative models of monkey V1. We therefore compared it to a classical linear-nonlinear Poisson (LNP) model, a wavelet-based model and a multi-layer CNN fit directly to the data.</p>
<p>We regularized the LNP model by selecting for smoothness and sparseness of the linear filters via cross-validation (see Methods). The wavelet-based model uses the Berkeley Wavelet Transform (BWT, <xref ref-type="bibr" rid="c40">Willmore et al. 2008</xref>, <xref ref-type="bibr" rid="c41">2010</xref>), a handcrafted nonlinear feature space based on orthogonal wavelets that resemble Gabor functions. This model is the current state of the art in the neural prediction challenge for monkey V1 responses to natural images (<ext-link ext-link-type="uri" xlink:href="http://neuralprediction.berkeley.edu">http://neuralprediction.berkeley.edu</ext-link>). Because recent work has shown that multi-layer convolutional neural networks can be fit directly to neural data on natural image datasets (<xref ref-type="bibr" rid="c2">Antol&#x00ED;k et al., 2016</xref>; <xref ref-type="bibr" rid="c23">Kindel et al., 2017</xref>; <xref ref-type="bibr" rid="c25">Klindt et al., 2017</xref>), we also fit a three-layer CNN identical to that of <xref ref-type="bibr" rid="c25">Klindt et al. 2017</xref>. This model is illustrated in <xref ref-type="fig" rid="fig5">Fig. 5</xref>. For more details on the models, see Methods.</p>
<fig id="fig5" position="float" fig-type="figure"><label>Figure 5.</label>
<caption><title>Convolutional neural network architecture.</title>
<p>Following the approach of <xref ref-type="bibr" rid="c25">Klindt et al. 2017</xref>, we trained a three-layer convolutional neural network to produce a feature space fed to a GLM-like model. In contrast to the VGG-based model, both feature space and readout weights are trained only on the neural data.</p></caption>
<graphic xlink:href="201764_fig5.tif"/>
</fig>
<p>We compared the models for a number of cells from a representative recording (<xref ref-type="fig" rid="fig6">Fig. 6A</xref>) and found a diversity of cells. For simple-like cells - cells for which the LNP had a high predictive power - all models performed approximately equally. However, the nonlinear feature spaces were able to better represent nonlinear (e.g. complex) cells, for which the BWT had a much larger performance than LNP (<xref ref-type="fig" rid="fig6">Fig. 6A</xref>).</p>
<fig id="fig6" position="float" fig-type="figure"><label>Figure 6.</label>
<caption><title>Deep models are the new state of the art.</title>
<p><bold>A</bold> Randomly selected cells. The normalized explainable variance (oracle) per cell is shown in gray. For each cell from left to right, the variance explained of: regularized LNP (<xref ref-type="bibr" rid="c35">Simoncelli et al., 2004</xref>), BWT (<xref ref-type="bibr" rid="c40">Willmore et al., 2008</xref>), three-layer CNN trained on neural responses, and VGG conv3_1 model (ours). <bold>B</bold> CNN and VGG conv3_1 models outperform for most cells LNP and BWT. Black line denotes the identity. The performance is given in <italic>FEV</italic> (see Methods). <bold>C</bold>. VGG conv3_1 features perform slightly better than the three-layer CNN. <bold>D</bold>. Average performance of the four models given in mean fraction of explainable variance explained (<italic>FEV</italic>).</p></caption>
<graphic xlink:href="201764_fig6.tif"/>
</fig>
<p>The two deep learning approaches outperformed the other models of V1 (<xref ref-type="fig" rid="fig6">Fig. 6B, D</xref>). The LNP model achieved 17&#x0025; <italic>FEV</italic>, the Berkeley Wavelet Transform model 39&#x0025; <italic>FEV</italic>. The performance of the VGG-based model was comparable to that of the CNN trained directly on the data (<xref ref-type="fig" rid="fig6">Fig. 6C, D</xref>). On average, the VGG-based model yielded on average a slightly higher performance (50.1&#x0025; <italic>FEV</italic>) than the data-driven CNN (46&#x0025; <italic>FEV</italic>), but this difference was not significant (p &#x003D; 0.09, t-test). In addition, a more extensive search of hyperparameters of the data-driven CNN architecture might lead to better performance.</p>
</sec>
</sec>
<sec id="s4"><label>4</label><title>Discussion</title>
<p>We fit two models based on convolutional neural networks to V1 responses to natural stimuli in awake, fixating monkeys: a goal-driven model, which uses the representations learned by a CNN trained on object recognition (VGG), and a data-driven model, which learns both the convolutional and readout parameters using stimulus-response pairs with multiple neurons simultaneously. Both approaches yielded comparable performance and outperformed the widely used LNP (<xref ref-type="bibr" rid="c35">Simoncelli et al., 2004</xref>) and the wavelet-decomposition model (BWT; <xref ref-type="bibr" rid="c40">Willmore et al. 2008</xref>), which held the previous state of the art in prediction of V1 responses to natural images. For the goal-driven model, we found that features of intermediate layers of VGG (layer conv3_1) explain V1 best.</p>
<p>The most successful system identification approaches to date all build on feature spaces shared by all neurons. There are three main ways in which this feature space can be chosen. First, handcrafted features have been used, which are based on existing neural encoding theories (e.g. BWT, <xref ref-type="bibr" rid="c40">Willmore et al. 2008</xref>; or HMAX, <xref ref-type="bibr" rid="c31">Riesenhuber and Poggio 1999</xref>). Second, more recent studies have learned shared feature spaces by jointly fitting the stimulus-response of all neurons in the dataset (<xref ref-type="bibr" rid="c2">Antol&#x00ED;k et al., 2016</xref>; <xref ref-type="bibr" rid="c3">Batty et al., 2016</xref>; <xref ref-type="bibr" rid="c23">Kindel et al., 2017</xref>; <xref ref-type="bibr" rid="c25">Klindt et al., 2017</xref>; <xref ref-type="bibr" rid="c29">McIntosh et al., 2016</xref>). Third, inspired by the success of transfer learning in the machine learning community, researchers have borrowed representations optimized to solve a visual task like object recognition and used them to predict responses in high-level areas of the ventral stream (<xref ref-type="bibr" rid="c43">Yamins et al., 2014</xref>). We compared these three approaches quantitatively and showed that the last two have comparable and the highest predictive performance on our monkey V1 dataset.</p>
<p>This result has two important implications that we want to briefly discuss. First, the fact that deep models substantially outperformed the handcrafted feature spaces (BWT) shows that we still do not fully understand the computations performed by V1 - or at least that there still does not exist an explicit model applicable to natural images. Second, an architecture optimized for fitting the neural data (CNN model) did not outperform a feature space trained on a different task (object recognition). On the one hand, this result underscores the power of transfer learning and stresses the relevance of nonlinearities learned by VGG for predicting neural responses along the ventral visual stream. On the other hand, the fact that the data-driven model reached comparable performance with a shallower and less complex architecture, shows that the VGG feature space is not identical to that of V1. If it was, the VGG-based model should have outperformed the data-driven approach. Note, though, that these two approaches are each at one end of a spectrum: a hybrid approach, where a pre-trained feature space is used as initialization and subsequently fine-tuned may provide the right balance of inductive bias and flexibility, perhaps leading to even higher performance than either the data-driven or transfer-learning-based approaches. Moreover, it is possible that increasing the entropy of the stimulus set or the number of neurons (or both) could improve performance of purely data-driven models. We leave these questions for future work.</p>
<p>Our work contributes to a growing body of research where goal-driven deep learning models (<xref ref-type="bibr" rid="c42">Yamins and DiCarlo, 2016</xref>) have shown unprecedented predictive performance of higher areas of the visual stream (<xref ref-type="bibr" rid="c4">Cadieu et al., 2014</xref>; <xref ref-type="bibr" rid="c43">Yamins et al., 2014</xref>), and a hierarchical correspondence between deep networks and the ventral stream (<xref ref-type="bibr" rid="c16">G&#x00FC;&#x00E7;gl&#x00FC; and van Gerven, 2015</xref>; <xref ref-type="bibr" rid="c7">Cichy et al., 2016</xref>). Studies based on fMRI have established a correspondence between early layers of CNNs trained on object recognition and V1 (<xref ref-type="bibr" rid="c16">G&#x00FC;&#x00E7;gl&#x00FC; and van Gerven, 2015</xref>; <xref ref-type="bibr" rid="c26">Kriegeskorte, 2015</xref>), somewhat in contrast to our findings that intermediate layers performed best. However, these studies are based on fMRI data, which is an average of many neurons&#x2019; responses and therefore possibly more linear than individual neurons&#x2019; responses. Moreover, they used a shallower and less well performing CNN (AlexNet, <xref ref-type="bibr" rid="c27">Krizhevsky et al. 2012</xref>), which has larger, Gabor-shaped receptive fields in its early layers.</p>
<p>Interestingly, the features of multiple VGG layers performed similarly well, with only a shallow peak at layer conv3_1 (<xref ref-type="fig" rid="fig4">Fig. 4</xref>). This result is to be expected, as it has been observed that in deep neural networks the features of subsequent convolutional layers are highly redundant. That is, one can predict the feature maps in any given layer very well by those of a previous layer. More recent state-of-the-art architectures for object recognition avoid this type of redundancy by enabling &#x2018;shortcut&#x2019; connections that skip layers. These skip connections encourage each layer to extract &#x2018;new&#x2019; information instead of mainly carrying along information that has already been extracted. Recent examples for such architectures are Densely Connected CNNs (<xref ref-type="bibr" rid="c19">Huang et al., 2016</xref>), and the residual paths of Residual Networks (<xref ref-type="bibr" rid="c17">He et al., 2016</xref>). Some of these novel architectures hold more similarities with known cortical circuitry. They may be exploited in the future to extract features for neural system identification in the same way and could potentially be more interpretable.</p>
<p>Although deep models capture nonlinearities that go beyond complex cells, they lack a minimalistic description that could be meaningfully linked to biology. However, their success over other models makes them good candidates for an <italic>in-silico</italic> investigation. The advantage of having a good predictive <italic>in-silico</italic> model is that, unlike a real brain, one can probe it with completely arbitrary stimuli and run unlimited experiments. Thus, we argue that the chances of finding simple descriptions of the nonlinear computations performed by the brain are much larger when probing a highly predictive model than when measuring activity in the brain directly without a predictive model.</p>
</sec>
</body><back><ack><title>Acknowledgments</title>
<p>Research reported in this publication was supported by the German Research Foundation (DFG) grant EC 479/1-1 to A.S.E; the Bernstein Center for Computational Neuroscience (FKZ 01GQ1002); the German Excellency Initiative through the Centre for Integrative Neuroscience T&#x00FC;bingen (EXC307); the National Eye Institute of the National Institutes of Health under Award Numbers R01EY026927 (A.S.T.), DP1 EY023176 (A.S.T.), and NIH-Pioneer award DP1-OD008301 (A.S.T). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. This research was also supported by NEI/NIH Core Grant for Vision Research (EY-002520-37), NEI training grant T32EY00700140 (G.H.D) and F30EY025510 (E.Y.W.). L.A.G was supported by German National Academic Foundation. This research was also supported by Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government. The authors have no conflicts of interest to report.</p>
</ack>
<ref-list><title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Adelson</surname> <given-names>EH</given-names></string-name>, <string-name><surname>Bergen</surname> <given-names>JR</given-names></string-name> (<year>1985</year>) <article-title>Spatiotemporal energy models for the perception of motion</article-title>. <source>JOSA A</source> <volume>2</volume>:<fpage>284</fpage>&#x2013;<lpage>299</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Antol&#x00ED;k</surname> <given-names>J</given-names></string-name>, <string-name><surname>Hofer</surname> <given-names>SB</given-names></string-name>, <string-name><surname>Bednar</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Mrsic-Flogel</surname> <given-names>TD</given-names></string-name> (<year>2016</year>) <article-title>Model constrained by visual hierarchy improves prediction of neural responses to natural scenes</article-title>. <source>PLOS Comput Biol</source> <volume>12</volume>:<fpage>e1004927</fpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="other"><string-name><surname>Batty</surname> <given-names>E</given-names></string-name>, <string-name><surname>Merel</surname> <given-names>J</given-names></string-name>, <string-name><surname>Brackbill</surname> <given-names>N</given-names></string-name>, <string-name><surname>Heitman</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sher</surname> <given-names>A</given-names></string-name>, <string-name><surname>Litke</surname> <given-names>A</given-names></string-name>, <string-name><surname>Chichilnisky</surname> <given-names>E</given-names></string-name>, <string-name><surname>Paninski</surname> <given-names>L</given-names></string-name> (<year>2016</year>) <source>Multilayer recurrent network models of primate retinal ganglion cell responses</source>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Cadieu</surname> <given-names>CF</given-names></string-name>, <string-name><surname>Hong</surname> <given-names>H</given-names></string-name>, <string-name><surname>Yamins</surname> <given-names>DL</given-names></string-name>, <string-name><surname>Pinto</surname> <given-names>N</given-names></string-name>, <string-name><surname>Ardila</surname> <given-names>D</given-names></string-name>, <string-name><surname>Solomon</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Majaj</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>DiCarlo</surname> <given-names>JJ</given-names></string-name> (<year>2014</year>) <article-title>Deep neural networks rival the representation of primate it cortex for core visual object recognition</article-title>. <source>PLoS Comput Biol</source> <volume>10</volume>:<fpage>e1003963</fpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Calabrese</surname> <given-names>A</given-names></string-name>, <string-name><surname>Paninski</surname> <given-names>L</given-names></string-name> (<year>2011</year>) <article-title>Kalman filter mixture model for spike sorting of non-stationary data</article-title>. <source>Journal of neuroscience methods</source> <volume>196</volume>:<fpage>159</fpage>&#x2013;<lpage>169</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Carandini</surname> <given-names>M</given-names></string-name>, <string-name><surname>Demb</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Mante</surname> <given-names>V</given-names></string-name>, <string-name><surname>Tolhurst</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Dan</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Olshausen</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Rust</surname> <given-names>NC</given-names></string-name> (<year>2005</year>) <article-title>Do we know what the early visual system does?</article-title> <source>The Journal of neuroscience</source> <volume>25</volume>:<fpage>10577</fpage>&#x2013;<lpage>10597</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="other"><string-name><surname>Cichy</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Khosla</surname> <given-names>A</given-names></string-name>, <string-name><surname>Pantazis</surname> <given-names>D</given-names></string-name>, <string-name><surname>Torralba</surname> <given-names>A</given-names></string-name>, <string-name><surname>Oliva</surname> <given-names>A</given-names></string-name> (<year>2016</year>) <article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title>. <source>Scientific reports</source> <fpage>6</fpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="other"><string-name><surname>Clevert</surname> <given-names>DA</given-names></string-name>, <string-name><surname>Unterthiner</surname> <given-names>T</given-names></string-name>, <string-name><surname>Hochreiter</surname> <given-names>S</given-names></string-name> (<year>2015</year>) <article-title>Fast and accurate deep network learning by exponential linear units (elus)</article-title>. arXiv preprint <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1511.07289">arXiv:1511.07289</ext-link>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="other"><string-name><surname>Denfield</surname> <given-names>GH</given-names></string-name>, <string-name><surname>Ecker</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Shinn</surname> <given-names>TJ</given-names></string-name>, <string-name><surname>Bethge</surname> <given-names>M</given-names></string-name>, <string-name><surname>Tolias</surname> <given-names>AS</given-names></string-name> (<year>2017</year>) <article-title>Attentional fluctuations induce shared variability in macaque primary visual cortex</article-title>. <source>bioRxiv</source> p. <fpage>189</fpage>&#x2013;<lpage>282</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="confproc"><string-name><surname>Donahue</surname> <given-names>J</given-names></string-name>, <string-name><surname>Jia</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Vinyals</surname> <given-names>O</given-names></string-name>, <string-name><surname>Hoffman</surname> <given-names>J</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>N</given-names></string-name>, <string-name><surname>Tzeng</surname> <given-names>E</given-names></string-name>, <string-name><surname>Darrell</surname> <given-names>T</given-names></string-name> (<year>2014</year>) <article-title>Decaf: A deep convolutional activation feature for generic visual recognition</article-title> In <conf-name>International conference on machine learning</conf-name>, pp. <fpage>647</fpage>&#x2013;<lpage>655</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Ecker</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Berens</surname> <given-names>P</given-names></string-name>, <string-name><surname>Cotton</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Subramaniyan</surname> <given-names>M</given-names></string-name>, <string-name><surname>Denfield</surname> <given-names>GH</given-names></string-name>, <string-name><surname>Cadwell</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Smirnakis</surname> <given-names>SM</given-names></string-name>, <string-name><surname>Bethge</surname> <given-names>M</given-names></string-name>, <string-name><surname>Tolias</surname> <given-names>AS</given-names></string-name> (<year>2014</year>) <article-title>State dependence of noise correlations in macaque primary visual cortex</article-title>. <source>Neuron</source> <volume>82</volume>:<fpage>235</fpage>&#x2013;<lpage>248</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Ecker</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Berens</surname> <given-names>P</given-names></string-name>, <string-name><surname>Keliris</surname> <given-names>GA</given-names></string-name>, <string-name><surname>Bethge</surname> <given-names>M</given-names></string-name>, <string-name><surname>Logothetis</surname> <given-names>NK</given-names></string-name>, <string-name><surname>Tolias</surname> <given-names>AS</given-names></string-name> (<year>2010</year>) <article-title>Decorrelated neuronal firing in cortical microcircuits</article-title>. <source>science</source> <volume>327</volume>:<fpage>584</fpage>&#x2013;<lpage>587</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Freeman</surname> <given-names>J</given-names></string-name>, <string-name><surname>Ziemba</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name>, <string-name><surname>Movshon</surname> <given-names>JA</given-names></string-name> (<year>2013</year>) <article-title>A functional and perceptual signature of the second visual area in primates</article-title>. <source>Nature neuroscience</source> <volume>16</volume>:<fpage>974</fpage>&#x2013;<lpage>981</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Gatys</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ecker</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Bethge</surname> <given-names>M</given-names></string-name> (<year>2015</year>) <source>Texture synthesis using convolutional neural networks In Advances in Neural Information Processing Systems</source>, pp. <fpage>262</fpage>&#x2013;<lpage>270</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="confproc"><string-name><surname>Gatys</surname> <given-names>LA</given-names></string-name>, <string-name><surname>Ecker</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Bethge</surname> <given-names>M</given-names></string-name> (<conf-date>2016</conf-date>) <article-title>Image style transfer using convolutional neural networks</article-title> In <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>, pp. <fpage>2414</fpage>&#x2013;<lpage>2423</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>G&#x00FC;&#x00E7;gl&#x00FC;</surname> <given-names>U</given-names></string-name>, <string-name><surname>van Gerven</surname> <given-names>MA</given-names></string-name> (<year>2015</year>) <article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title>. <source>The Journal of Neuroscience</source> <volume>35</volume>:<fpage>10005</fpage>&#x2013;<lpage>10014</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="confproc"><string-name><surname>He</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname> <given-names>X</given-names></string-name>, <string-name><surname>Ren</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sun</surname> <given-names>J</given-names></string-name> (<conf-date>2016</conf-date>) <article-title>Deep residual learning for image recognition</article-title> In <conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name>, pp. <fpage>770</fpage>&#x2013;<lpage>778</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Heeger</surname> <given-names>DJ</given-names></string-name> (<year>1992</year>) <article-title>Half-squaring in responses of cat striate cells</article-title>. <source>Visual neuroscience</source> <volume>9</volume>:<fpage>427</fpage>&#x2013;<lpage>443</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="other"><string-name><surname>Huang</surname> <given-names>G</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Weinberger</surname> <given-names>KQ</given-names></string-name>, <string-name><surname>van der Maaten</surname> <given-names>L</given-names></string-name> (<year>2016</year>) <article-title>Densely connected convolutional networks</article-title>. <source>arXiv preprint</source> <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1608.06993">arXiv:1608.06993</ext-link>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Hubel</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Wiesel</surname> <given-names>TN</given-names></string-name> (<year>1959</year>) <article-title>Receptive fields of single neurones in the cat&#x2019;s striate cortex</article-title>. <source>The Journal of physiology</source> <volume>148</volume>:<fpage>574</fpage>&#x2013;<lpage>591</lpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Hubel</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Wiesel</surname> <given-names>TN</given-names></string-name> (<year>1968</year>) <article-title>Receptive fields and functional architecture of monkey striate cortex</article-title>. <source>The Journal of physiology</source> <volume>195</volume>:<fpage>215</fpage>&#x2013;<lpage>243</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Jones</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Palmer</surname> <given-names>LA</given-names></string-name> (<year>1987</year>) <article-title>An evaluation of the two-dimensional gabor filter model of simple receptive fields in cat striate cortex</article-title>. <source>Journal of neurophysiology</source> <volume>58</volume>:<fpage>1233</fpage>&#x2013;<lpage>1258</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="other"><string-name><surname>Kindel</surname> <given-names>WF</given-names></string-name>, <string-name><surname>Christensen</surname> <given-names>ED</given-names></string-name>, <string-name><surname>Zylberberg</surname> <given-names>J</given-names></string-name> (<year>2017</year>) <article-title>Using deep learning to reveal the neural code for images in primary visual cortex</article-title>. <source>arXiv preprint</source> <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1706.06208">arXiv:1706.06208</ext-link>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="other"><string-name><surname>Kingma</surname> <given-names>D</given-names></string-name>, <string-name><surname>Ba</surname> <given-names>J</given-names></string-name> (<year>2014</year>) <article-title>Adam: A method for stochastic optimization</article-title>. <source>arXiv preprint</source> <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1412.6980">arXiv:1412.6980</ext-link>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="other"><string-name><surname>Klindt</surname> <given-names>D</given-names></string-name>, <string-name><surname>Ecker</surname> <given-names>AS</given-names></string-name>, <string-name><surname>Euler</surname> <given-names>T</given-names></string-name>, <string-name><surname>Bethge</surname> <given-names>M</given-names></string-name> (<year>2017</year>) <article-title>Neural system identification for large populations separating &#x201C;what&#x201D; and &#x201C;where&#x201D;</article-title> In <source>Advances in Neural Information Processing Systems (accepted)</source>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Kriegeskorte</surname> <given-names>N</given-names></string-name> (<year>2015</year>) <article-title>Deep neural networks: a new framework for modeling biological vision and brain information processing</article-title>. <source>Annual Review of Vision Science</source> <volume>1</volume>:<fpage>417</fpage>&#x2013;<lpage>446</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="other"><string-name><surname>Krizhevsky</surname> <given-names>A</given-names></string-name>, <string-name><surname>Sutskever</surname> <given-names>I</given-names></string-name>, <string-name><surname>Hinton</surname> <given-names>GE</given-names></string-name> (<year>2012</year>) <article-title>Imagenet classification with deep convolutional neural networks</article-title> In <source>Advances in neural information processing systems</source> pp. <fpage>1097</fpage>&#x2013;<lpage>1105</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="website"><string-name><surname>K&#x00FC;mmerer</surname> <given-names>M</given-names></string-name>, <string-name><surname>Theis</surname> <given-names>L</given-names></string-name>, <string-name><surname>Bethge</surname> <given-names>M</given-names></string-name> (<year>2014</year>) <article-title>Deep gaze i: Boosting saliency prediction with feature maps trained on imagenet</article-title>. <source>arXiv preprint</source> <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1411.1045">arXiv:1411.1045</ext-link>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>McIntosh</surname> <given-names>L</given-names></string-name>, <string-name><surname>Maheswaranathan</surname> <given-names>N</given-names></string-name>, <string-name><surname>Nayebi</surname> <given-names>A</given-names></string-name>, <string-name><surname>Ganguli</surname> <given-names>S</given-names></string-name>, <string-name><surname>Baccus</surname> <given-names>S</given-names></string-name> (<year>2016</year>) <article-title>Deep learning models of the retinal response to natural scenes</article-title> In <source>Advances in Neural Information Processing Systems</source>, pp. <fpage>1369</fpage>&#x2013;<lpage>1377</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Olshausen</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Field</surname> <given-names>DJ</given-names></string-name> (<year>2005</year>) <article-title>How close are we to understanding v1?</article-title> <source>Neural computation</source> <volume>17</volume>:<fpage>1665</fpage>&#x2013;<lpage>1699</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Riesenhuber</surname> <given-names>M</given-names></string-name>, <string-name><surname>Poggio</surname> <given-names>T</given-names></string-name> (<year>1999</year>) <article-title>Hierarchical models of object recognition in cortex</article-title>. <source>Nature neuro science</source> <volume>2</volume>:<fpage>1019</fpage>&#x2013;<lpage>1025</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Russakovsky</surname> <given-names>O</given-names></string-name>, <string-name><surname>Deng</surname> <given-names>J</given-names></string-name>, <string-name><surname>Su</surname> <given-names>H</given-names></string-name>, <string-name><surname>Krause</surname> <given-names>J</given-names></string-name>, <string-name><surname>Satheesh</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>S</given-names></string-name>, <string-name><surname>Huang</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Karpathy</surname> <given-names>A</given-names></string-name>, <string-name><surname>Khosla</surname> <given-names>A</given-names></string-name>, <string-name><surname>Bernstein</surname> <given-names>M</given-names></string-name> <etal>et al.</etal> (<year>2015</year>) <article-title>Imagenet large scale visual recognition challenge</article-title>. <source>International Journal of Computer Vision</source> <volume>115</volume>:<fpage>211</fpage>&#x2013;<lpage>252</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Rust</surname> <given-names>NC</given-names></string-name>, <string-name><surname>Schwartz</surname> <given-names>O</given-names></string-name>, <string-name><surname>Movshon</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name> (<year>2005</year>) <article-title>Spatiotemporal elements of macaque v1 receptive fields</article-title>. <source>Neuron</source> <volume>46</volume>:<fpage>945</fpage>&#x2013;<lpage>956</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Shan</surname> <given-names>KQ</given-names></string-name>, <string-name><surname>Lubenov</surname> <given-names>EV</given-names></string-name>, <string-name><surname>Siapas</surname> <given-names>AG</given-names></string-name> (<year>2017</year>) <article-title>Model-based spike sorting with a mixture of drifting t-distributions</article-title>. <source>bioRxiv</source> p. <fpage>109</fpage>&#x2013;<lpage>850</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name>, <string-name><surname>Paninski</surname> <given-names>L</given-names></string-name>, <string-name><surname>Pillow</surname> <given-names>J</given-names></string-name>, <string-name><surname>Schwartz</surname> <given-names>O</given-names></string-name> (<year>2004</year>) <article-title>Characterization of neural responses with stochastic stimuli</article-title>. <source>The cognitive neurosciences</source> <volume>3</volume>:<fpage>327</fpage>&#x2013;<lpage>338</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="website"><string-name><surname>Simonyan</surname> <given-names>K</given-names></string-name>, <string-name><surname>Zisserman</surname> <given-names>A</given-names></string-name> (<year>2014</year>) <article-title>Very deep convolutional networks for large-scale image recognition</article-title>. <source>arXiv preprint</source> <ext-link ext-link-type="arxiv" xlink:href="http://arxiv.org/abs/arXiv:1409.1556">arXiv:1409.1556</ext-link>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Talebi</surname> <given-names>V</given-names></string-name>, <string-name><surname>Baker</surname> <given-names>CL</given-names></string-name> (<year>2012</year>) <article-title>Natural versus synthetic stimuli for estimating receptive field models: a comparison of predictive robustness</article-title>. <source>The Journal of Neuroscience</source> <volume>32</volume>:<fpage>1560</fpage>&#x2013;<lpage>1576</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Touryan</surname> <given-names>J</given-names></string-name>, <string-name><surname>Felsen</surname> <given-names>G</given-names></string-name>, <string-name><surname>Dan</surname> <given-names>Y</given-names></string-name> (<year>2005</year>) <article-title>Spatial structure of complex cell receptive fields measured with natural images</article-title>. <source>Neuron</source> <volume>45</volume>:<fpage>781</fpage>&#x2013;<lpage>791</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Vintch</surname> <given-names>B</given-names></string-name>, <string-name><surname>Movshon</surname> <given-names>JA</given-names></string-name>, <string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name> (<year>2015</year>) <article-title>A convolutional subunit model for neuronal responses in macaque v1</article-title>. <source>The Journal of Neuroscience</source> <volume>35</volume>:<fpage>14829</fpage>&#x2013;<lpage>14841</lpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Willmore</surname> <given-names>B</given-names></string-name>, <string-name><surname>Prenger</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>MCK</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name> (<year>2008</year>) <article-title>The berkeley wavelet transform: a biologically inspired orthogonal wavelet transform</article-title>. <source>Neural computation</source> <volume>20</volume>:<fpage>1537</fpage>&#x2013;<lpage>1564</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Willmore</surname> <given-names>BD</given-names></string-name>, <string-name><surname>Prenger</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name> (<year>2010</year>) <article-title>Neural representation of natural images in visual area v2</article-title>. <source>The Journal of neuroscience</source> <volume>30</volume>:<fpage>2102</fpage>&#x2013;<lpage>2114</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Yamins</surname> <given-names>DL</given-names></string-name>, <string-name><surname>DiCarlo</surname> <given-names>JJ</given-names></string-name> (<year>2016</year>) <article-title>Using goal-driven deep learning models to understand sensory cortex</article-title>. <source>Nature neuroscience</source> <volume>19</volume>:<fpage>356</fpage>&#x2013;<lpage>365</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="confproc"><string-name><surname>Yamins</surname> <given-names>DL</given-names></string-name>, <string-name><surname>Hong</surname> <given-names>H</given-names></string-name>, <string-name><surname>Cadieu</surname> <given-names>CF</given-names></string-name>, <string-name><surname>Solomon</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Seibert</surname> <given-names>D</given-names></string-name>, <string-name><surname>DiCarlo</surname> <given-names>JJ</given-names></string-name> (<conf-date>2014</conf-date>) <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <conf-name>Proceedings of the National Academy of Sciences</conf-name> <volume>111</volume>:<fpage>8619</fpage>&#x2013;<lpage>8624</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>