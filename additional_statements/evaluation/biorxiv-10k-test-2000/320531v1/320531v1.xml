<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/320531</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Measuring human sensitivity to perturbations within the manifold of natural images</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Fruend</surname>
<given-names>Ingo</given-names>
</name>
<xref ref-type="aff" rid="a1">&#x2756;&#x2709;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Stalker</surname>
<given-names>Elee</given-names>
</name>
<xref ref-type="aff" rid="a1">&#x2756;&#x2709;</xref>
</contrib>
<aff id="a1"><label>&#x2756;&#x2709;</label><institution>Centre for Vision Research and Department of Psychology York University</institution>, Toronto, ON, <country>Canada</country></aff>
<aff id="a2"><label>&#x2756;&#x2709;</label><institution>Department of Psychology York University</institution>, Toronto, ON, <country>Canada</country></aff>
</contrib-group>
<pub-date pub-type="epub">
<year>2018</year>
</pub-date>
<elocation-id>320531</elocation-id>
<history>
<date date-type="received">
<day>11</day>
<month>5</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>11</day>
<month>5</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>11</day>
<month>5</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="320531.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Humans are remarkably well tuned to the statistical properties of natural images. However, quantitative characterization of processing within the domain of natural images has been difficult because most parametric manipulations of a natural image make that image appear less natural. We used generative adversarial networks (GANs) to constrain parametric manipulations to remain within (an approximation of) the manifold of natural images. In the first experiment, 7 observers decided which one of two perturbed images matched an unperturbed comparison image. Observers were significantly more sensitive to perturbations that were constrained to the manifold of natural images than they were to perturbations applied directly in pixel space. Trial by trial errors were consistent with the idea that these perturbations disrupt configural aspects of visual structure used in image segmentation. In a second experiment, 5 observers discriminated paths along the image manifold. Observers were remarkably good at this task, confirming that observers were tuned to fairly detailed properties of the manifold of natural images. We conclude that human tuning to natural images is more general than detecting deviations from natural appearance, and that humans have, to some extent, access to detailed interrelations between natural images.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>natural images</kwd>
<kwd>image recognition</kwd>
<kwd>artificial neural networks</kwd>
<kwd>generative adversarial nets</kwd>
<kwd>noise perturbations</kwd>
</kwd-group>
<counts>
<page-count count="15"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1"><title>Introduction</title>
<p>The images that we experience in our everyday visual environment are highly complex. However, they only comprise a small part of all possible digital images and our visual system seems to be adapted to perform well with these natural images (<xref ref-type="bibr" rid="c36">Simoncelli &#x0026; Olshausen, 2001</xref>; <xref ref-type="bibr" rid="c13">Geisler, 2008</xref>).</p>
<p>Humans are very quick at making simple decisions in natural images (<xref ref-type="bibr" rid="c38">Thorpe et al., 2001</xref>). We can easily complete missing image components with the most natural looking value (<xref ref-type="bibr" rid="c5">Bethge et al., 2007</xref>) or detect visual deviations from naturalness (<xref ref-type="bibr" rid="c14">Gerhard et al., 2013</xref>; <xref ref-type="bibr" rid="c42">Wallis et al., 2017</xref>; <xref ref-type="bibr" rid="c10">Fr&#x00FC;nd &#x0026; Elder, 2013</xref>). This precise tuning seems to be mostly restricted to foveal areas and humans are much less sensitive to deviations from naturalness in the periphery (<xref ref-type="bibr" rid="c41">Wallis &#x0026; Bex, 2012</xref>). In fact, two physically different images that match in only a coarse set of image statistics in the periphery, typically appear to be the same (<xref ref-type="bibr" rid="c8">Freeman &#x0026; Simoncelli, 2011</xref>).</p>
<p>Most of these studies are concerned with human sensitivity to deviations from naturalness. It is however, less clear how to characterize human performance within the manifold of natural images. One challenge is that most experimental manipulations of natural images make the image itself appear less natural. For example, <xref ref-type="bibr" rid="c6">Bex (2010</xref>) manipulates images by local deformations. He finds that sensitivity to these deformations is tuned to the spatial frequency at which they occur. Although the deformations used by <xref ref-type="bibr" rid="c6">Bex (2010</xref>) only moderately alter the power spectrum of natural images, they considerably disrupt higher order statistical properties such as the phase spectrum of the images (<xref ref-type="bibr" rid="c43">Wichmann et al., 2006</xref>). Others, have manipulated images by introducing &#x201C;dead leaves&#x201D;&#x2014;small homogeneous patches&#x2014;in different locations (<xref ref-type="bibr" rid="c41">Wallis &#x0026; Bex, 2012</xref>) or manipulating the correlation structure of the images (<xref ref-type="bibr" rid="c27">McDonald &#x0026; Tadmor, 2006</xref>). For small patches presented in the visual periphery, observers can often not detect these manipulations. However, we argue that these manipulations still make the image appear less natural rather than studying visual processing within the domain of natural images. A possible solution to this problem is to use selective sampling (<xref ref-type="bibr" rid="c35">Sebastian et al., 2017</xref>): Instead of attempting to manipulate the image directly, one chooses another natural image that <italic>by chance</italic> shows the desired manipulation. Although this approach guarantees that the resulting &#x201C;manipulations&#x201D; remain natural, it is highly dependent on the indexing mechanism used to select the &#x201C;manipulated image&#x201D;. This dependence creates a difficulty in generalizing the selective sampling approach to higher levels of visual processing.</p>
<p>Recent advances in machine learning might provide a means to constrain image manipulations to the domain of natural images. Here, we focus on a class of very powerful generative image models, known as generative adversarial nets (<xref ref-type="bibr" rid="c16">GANs, Goodfellow et al., 2014</xref>; <xref ref-type="bibr" rid="c30">Radford et al., 2016</xref>; <xref ref-type="bibr" rid="c3">Arjovsky et al., 2017</xref>; <xref ref-type="bibr" rid="c18">Gulrajani et al., 2017</xref>; <xref ref-type="bibr" rid="c28">Miyato et al., 2018</xref>; <xref ref-type="bibr" rid="c20">Hjelm et al., 2018</xref>). GANs learn a mapping&#x2014;called the generator&#x2014;from an isotropic Gaussian distribution to the space of images. One key insight with GANs is the use of an auxiliary classification function&#x2014;often called the critic&#x2014;to judge how good the generator mapping is. Specifically, the critic attempts to predict if a given image has been generated by mapping isotropic Gaussian noise through the generator, or if the image is an instance from the training database. Generator and critic are trained in alternation, where the generator is trained to increase the errors of the critic and the critic is trained to decrease its own error (see for example <xref ref-type="bibr" rid="c16">Goodfellow et al., 2014</xref>, for details). In general, generator and critic can be any possible transformation, but they are typically implemented as artificial neural networks with multiple hidden layers (<xref ref-type="bibr" rid="c16">Goodfellow et al., 2014</xref>; <xref ref-type="bibr" rid="c30">Radford et al., 2016</xref>). Although never studied quantitatively, images generated from GANs look quite similar to natural images and manipulations in a GAN&#x2019;s latent space seem to correspond in a meaningful way to perceptual experience. For example, <xref ref-type="bibr" rid="c30">Radford et al. (2016</xref>) start with a picture of a smiling woman, subtract the average latent representation of a neutral woman&#x2019;s face and add a neutral man&#x2019;s face to arrive at a picture of a smiling man. Similarly, <xref ref-type="bibr" rid="c45">Zhu et al. (2016</xref>) illustrate that projecting perceptually meaningful constraints back to a GAN&#x2019;s latent space allows creation of random images with specified features (e.g. edges or colored patches) in the specified locations. Together, this suggests that GANs recover a reasonably good approximation to the manifold of natural images.</p>
<p>In this study, we used GANs to manipulate images and observers made perceptual judgments about these images. In experiment 1, the observers viewed a target image and decided which one of two noisy comparison images corresponded to that target. Crucially, noise was either applied directly in pixel space or it was restricted to remain within an approximation to the manifold of natural images by applying it in the latent space of a GAN. We found that this task was more difficult if noise was applied in the latent space of a GAN, suggesting that noise in the GAN&#x2019;s latent space actually changes image features that are relevant for image recognition, while noise that was directly applied in pixel space only resulted in degradation of the image without necessarily changing image content. In experiment 2, observers were asked to detect changes of direction in videos that were constructed by moving along smooth paths through a GAN&#x2019;s latent space and we found that observers performed significantly above chance even for very small directional changes, suggesting that GANs not only recover a good approximation to the manifold of natural images, but that they also recover a perceptually meaningful parameterization of this manifold.</p></sec>
<sec id="s2"><title>Experiment 1: Sensitivity to perturbations within the natural image manifold</title>
<sec id="s2a"><title>Method</title>
<sec id="s2a1"><title>Training generative adversarial nets</title>
<p>We trained a Wasserstein-GAN (<xref ref-type="bibr" rid="c3">Arjovsky et al., 2017</xref>) on the 60 000 32 &#x00D7; 32 images contained in the CIFAR10 dataset (<xref ref-type="bibr" rid="c26">Krizhevsky, 2009</xref>) using gradient penalty as proposed by (<xref ref-type="bibr" rid="c18">Gulrajani et al., 2017</xref>). See <xref ref-type="fig" rid="fig1">Figure 1A</xref> for example training images. In short, a GAN consists of a generator network <italic>G</italic> that maps a latent vector <italic>z</italic> to image space and a critic network <italic>D</italic> that takes an image as input and predicts whether that image is a real image from the training dataset or an image that was generated by mapping a latent vector through the generator network (see <xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="bibr" rid="c18">Gulrajani et al., 2017</xref> for details of the architecture of the two networks). The generator network and the critic network were trained in alternation using stochastic gradient descent. Specifically, training alternated between 5 updates of the critic network and one update of the generator network. Updates of the critic network were chosen to minimize the loss<disp-formula>
<alternatives><graphic xlink:href="320531_ueqn1.gif"/></alternatives>
</disp-formula>and updates of the generator were chosen to maximize this loss. Here, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline1.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline2.gif"/></alternatives></inline-formula> denote averages over a batch of 64 latent vectors <italic>z</italic> or training images <italic>y</italic> respectively. Furthermore, &#x2207;<italic><sub>y</sub></italic> denotes the gradient with respect to image pixels <italic>y</italic>, which was evaluated at random points along straight line interpolations between real and generated images (see <xref ref-type="bibr" rid="c18">Gulrajani et al., 2017</xref> for details). We set <italic>&#x03BB;</italic> &#x003D; 10 during training.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Example training images and samples</title>
<p><bold>A.</bold> Training images from the CIFAR10 database used to train the GAN model. <bold>B.</bold> Example samples generated from the trained GAN. Similar images were used in the experiment.</p></caption>
<graphic xlink:href="320531_fig1.tif"/>
</fig>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Architecture of the generative adversarial network</title>
<p><bold>A.</bold> Architecture of the generator network. Information flow is from top to bottom, network layers are &#x201C;Linear <italic>k</italic>, <italic>m</italic>&#x201D;: Affine transformation from <italic>k</italic> features to <italic>m</italic> features, &#x201C;Conv <italic>k, m, n</italic> &#x00D7; <italic>n</italic>&#x201D;: Convolutional layer from <italic>k</italic> channels to <italic>m</italic> channels using a kernel size of <italic>n</italic> &#x00D7; <italic>n</italic>, &#x201C;DeConv <italic>k</italic>, <italic>m</italic>, <italic>n</italic> &#x00D7; <italic>n</italic>&#x201D;: like convolution but up-upsampling before the convolution to increase spatial resolution and image size, &#x201C;BatchNorm&#x201D;: Batch normalization (<xref ref-type="bibr" rid="c22">Ioffe &#x0026; Szegedy, 2015</xref>), &#x201C;ReLU&#x201D;: rectified linear unit ReLU(<italic>x</italic>) &#x003D; max(0,<italic>x</italic>) (<xref ref-type="bibr" rid="c15">Glorot et al., 2011</xref>). The generator network maps a sample <italic>z</italic> from an isotropic 128 dimensional Gaussian to a 32 &#x00D7; 32 pixel colour image. <bold>B.</bold> Architecture of the critic network. Architecture components not used in A. are &#x201C;Leaky ReLU&#x201D; (<xref ref-type="bibr" rid="c19">He et al., 2015</xref>). The critic network receives as input either an image <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline3.gif"/></alternatives></inline-formula> generated by the generator network or a real training image <italic>y</italic>, and it decides if the input image is real or not.</p></caption>
<graphic xlink:href="320531_fig2.tif"/>
</fig>
<p>Networks with different numbers of hidden states (parameter <italic>N</italic> in <xref ref-type="fig" rid="fig2">Figure 2</xref>) were trained for 200 000 epochs using an ADAM optimizer (<xref ref-type="bibr" rid="c25">Kingma &#x0026; Ba, 2015</xref>) with learning rate 10<sup>&#x2212;4</sup> and <italic>&#x03B2;</italic><sub>0</sub> &#x003D; 0, <italic>&#x03B2;</italic><sub>1</sub> &#x003D; 0.9. Specifically, we trained networks with <italic>N</italic> &#x003D; 40, 50, 60, 64, 70, 80, 90,128 (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). Wasserstein-2 error (<xref ref-type="bibr" rid="c3">Arjovsky et al., 2017</xref>) on a validation set (the CIFAR10 test dataset) was lowest with <italic>N</italic> &#x003D; 90 in agreement with visual inspection of sample quality, so we chose a network with <italic>N</italic> &#x003D; 90 for all remaining analyses. Example images generated from this final network are shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref>.</p></sec>
<sec id="s2a2"><title>Observers</title>
<p>Seven observers participated in the experiment. Two of them were authors, the remaining five were students from various labs at the Centre for Vision Research at York University, Toronto, Ontario. One additional observer participated in the first session, but withdrew from the experiment afterwards and their data was excluded from the analysis. All observers reported normal or corrected-to-normal vision. Prior to participation, all observers provided written informed consent to participate and all procedures were approved by the York University Ethics Board.</p></sec>
<sec id="s2a3"><title>Procedure</title>
<p>Each individual observer judged image samples in a spatial two-alternatives forced-choice image matching paradigm (see <xref ref-type="fig" rid="fig3">Figure 3</xref>). On every trial, the observer saw three images; a target image at the center, one comparison stimulus on the left and another on the right. One of the comparison stimuli was a perturbed version of the target image, while the other was an equally perturbed version of another sample from the GAN. The observer was required to indicate which of the two comparison images matched the central target image by pressing a corresponding button on a computer keyboard (left arrow key if the left comparison stimulus matched, right arrow key if the right comparison stimulus matched). Stimuli were presented for up to 6s or until the observer made a response, resulting in practically unlimited viewing time. Before each trial, a fixation point appeared on the screen for 500ms. Each observer performed 5 sessions and each session consisted of 80 trials for each noise level and type, resulting in a total of 1300 trials per observer (except O2, who performed 1437 trials).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Design of the image matching experiment</title>
<p>On every trial, the observer saw three images; a target image at the center, one comparison stimulus on the left and another on the right. Comparison stimuli were perturbed by different types of noise. Pixel noise (left column) was added as independent Gaussian noise to every pixel, Fourier noise (middle column) with the same power spectrum as the original image, but with random phases, was added to the original image, Latent noise (right column) was independent Gaussian noise applied in the latent space of the GAN that was used to generate the images. The top row shows example experimental displays with low noise, the bottom row shows example experimental displays with high noise (amounts given on left). For illustration, the left stimulus is a random perturbed stimulus and the right stimulus is the perturbed target in every example display. During the experiment, the identities and locations of the comparison stimuli were randomized.</p></caption>
<graphic xlink:href="320531_fig3.tif"/>
</fig></sec>
<sec id="s2a4"><title>Stimuli</title>
<p>All stimuli were samples from a GAN, converted to gray scale by averaging the red, green and blue channels of the sample image. The target stimulus was always noise-free, while the two comparison stimuli were perturbed by one of three noise types (see <xref ref-type="fig" rid="fig3">Figure 3</xref>). Pixel noise was constructed by adding independent Gaussian noise to each pixel of the respective image. Fourier noise was constructed in the Fourier domain by replacing the image&#x2019;s phase component by random numbers. This resulted in an image with the same power spectrum as the source image but with completely random phases. A Fourier-perturbed image was constructed by adding a multiple of this power-spectrum-matched noise to the source image. Latent noise was constructed by manipulating the latent vector <italic>z</italic> from which an image was generated. To generate perturbed images with a predefined difference in pixel space, we started by adding independent Gaussian noise <italic>&#x03B6;</italic> to <italic>z</italic> and determining the corresponding image <italic>G</italic>(<italic>z</italic> &#x002B; <italic>&#x03B6;</italic>). We then used gradient descent on<disp-formula>
<alternatives><graphic xlink:href="320531_ueqn2.gif"/></alternatives>
</disp-formula>to adjust <italic>&#x03B6;</italic> such that the final difference between the target and the perturbed target had a predefined pixel space difference of <italic>&#x03B4;</italic>.</p>
<p>Stimuli were presented on a medium gray background (54.1 cd/m<sup>2</sup>) on a Sony Triniton Multiscan G520 CRT monitor in a dimly illuminated room. The monitor was carefully linearized using a Minolta LS-100 photometer. Maximum stimulus luminance was 106.9 cd/m<sup>2</sup>, minimum stimulus luminance was 1.39 cd/m<sup>2</sup>. If the nominal stimulus luminance exceeded that range, it was clipped (for subsequent analyses, we also used the clipped stimuli). On every frame, the stimuli were re-rendered using random dithering to generate a quasi-continuous luminance resolution (<xref ref-type="bibr" rid="c2">Allard &#x0026; Faubert, 2008</xref>). At a viewing distance of approximately 87cm, each stimulus image subtended approximately 0.65 degrees of visual angle and were separated by approximately 0.13 degrees of visual angle. One pixel subtended approximately 0.02 degrees of visual angle.</p></sec>
<sec id="s2a5"><title>Data analysis</title>
<p>For every observer, we estimated a psychometric function parameterised as<disp-formula>
<alternatives><graphic xlink:href="320531_ueqn3.gif"/></alternatives>
</disp-formula>where <italic>&#x03B3;</italic> &#x003D; 0.5 is the probability to guess the stimulus correctly by chance, <italic>&#x03BB;</italic> is the lapse probability, <italic>&#x03C3;</italic> is the logistic function and <italic>a</italic> and b govern the offset and the slope of the psychometric function. We adopted a Bayesian perspective on estimation of the psychometric function (<xref ref-type="bibr" rid="c11">Fr&#x00FC;nd et al., 2011</xref>) and used weak priors <italic>&#x03BB;</italic> &#x007E; Beta(1.5, 20), <italic>a</italic> &#x007E; <italic>N</italic>(0,100), <italic>b</italic> &#x007E; (0,100). Mean <italic>a posteriori</italic> estimates of the critical noise level <italic>x<sub>c</sub></italic> at which <italic>&#x03C8;</italic>(<italic>x</italic>) &#x003D; 0.75 and the slope of the psychometric function at <italic>x<sub>c</sub></italic> were obtained using numerical integration of the posterior (<xref ref-type="bibr" rid="c34">Sch&#x00FC;tt et al., 2016</xref>).</p>
<p>To understand how the structure of the GAN&#x2019;s latent space determined image matching performance, we re-analysed data from the latent condition. For this re-analysis, we assumed that observers would pick the perturbed stimulus that is closer to the target with respect to some distance measure. More specifically, let <italic>t</italic> denote the noise-free target stimulus and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline4.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline5.gif"/></alternatives></inline-formula> denote the perturbed target and distractor stimuli respectively. If an observer is picking the stimulus that is closer to the target, then <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline6.gif"/></alternatives></inline-formula> should show a positive correlation with the observer&#x2019;s trial by trial response accuracy. Here, <italic>d</italic> is a suitably defined distance measure. We used either the Euclidean distance <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline7.gif"/></alternatives></inline-formula>, the radial distance <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline8.gif"/></alternatives></inline-formula>, or the cosine distance <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline9.gif"/></alternatives></inline-formula>, where ||<italic>x</italic>|| denotes the Euclidean norm of a vector <italic>x</italic> and &#x3008;<italic>x</italic>, <italic>y</italic>&#x3009; denotes the scalar product of vectors <italic>x</italic> and <italic>y</italic>. These distances were applied in either the GAN&#x2019;s latent space or directly in pixel space, after concatenating the respective stimulus&#x2019; pixel intensities into one long vector. We then determined receiver operating curves (ROC) for predicting correct vs incorrect responses based on <italic>c</italic>. The area under the ROC is a measure for how well the respective distance measure predicts the observer&#x2019;s trial by trial responses (<xref ref-type="bibr" rid="c17">Green &#x0026; Swets, 1966</xref>). To test if the area under the curve (AUC) was significantly different from chance, we performed a permutation test randomly re-shuffling the correct/incorrect labels 1000 times and taking the 95-th percentile of the resulting distribution as the critical value. We also used permutation tests to determine if the AUC for two different distance measures was significantly different. For the pairwise comparisons, there are 128 possible re-assignments of AUC values to the two conditions, and we computed all of them. The <italic>p</italic>-values for these post-hoc comparisons were corrected for multiple comparisons to control for inflation of the false discovery rate (<xref ref-type="bibr" rid="c4">Benjamini &#x0026; Hochberg, 1995</xref>).</p>
<p>To gain insight into the image features that determined the observers&#x2019; responses, we applied the ROC analysis to a number of image features as well. Firstly, we calculated the luminance histogram (50 bins) for each image and calculated the distance difference <italic>c</italic> between luminance histograms of the respective images. Secondly, to determine local dominant orientation at each pixel we first filtered the image with horizontal and vertical Scharr filters (<xref ref-type="bibr" rid="c33">Scharr, 2000</xref>) as implemented in scikit-image (<xref ref-type="bibr" rid="c39">van der Walt et al., 2014</xref>) giving local horizontal structure <italic>h</italic> and vertical structure <italic>v</italic>. The local orientation <italic>&#x03D5;</italic> was extracted from these two responses such that <italic>h</italic> &#x003D; <italic>r</italic> cos(<italic>&#x03D5;</italic>) and <italic>v</italic> &#x003D; <italic>r</italic> sin(<italic>&#x03D5;</italic>), where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline10.gif"/></alternatives></inline-formula>. We then determined the histogram (3 bins) of the local orientations across the image and calculated <italic>c</italic> as the distance difference between these orientation histograms. As a third feature, we calculated the edge densities of the two images, using the canny edge detector from scikit-image with a standard deviation of 2 pixels and calculating the fraction of pixels labeled as edges by this algorithm. As a fourth feature we determined the slope of the power spectrum in double logarithmic coordinates.</p>
<p>Finally, we used a standard method for image segmentation (<xref ref-type="bibr" rid="c7">Felsenzwalb &#x0026; Huttenlocher, 2004</xref>) to calculate segmentations of the images <italic>t</italic>, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline11.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline12.gif"/></alternatives></inline-formula>. We used the method implemented in scikit-image (<xref ref-type="bibr" rid="c39">van der Walt et al., 2014</xref>). Briefly, this algorithm iteratively merges neighbouring pixels or pixel groups if the differences across their borders are small compared to the differences within them. Each segmentation consists of a number of discrete labels assigned to the pixels of the original image. If two pixels belong to the same segmented region, the two labels associated with them should be the same. However, different segmentations may assign different labels to the same region. Thus, two segmentations <italic>s</italic> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline13.gif"/></alternatives></inline-formula> would be similar, if for many pairs (<italic>i</italic>, <italic>j</italic>) of pixels <italic>s<sub>i</sub></italic> &#x003D; <italic>s<sub>j</sub></italic> implies <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline14.gif"/></alternatives></inline-formula>. When calculating the distance between two segmentations <italic>s</italic> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline15.gif"/></alternatives></inline-formula>, we therefore count the number of pixel pairs for which <italic>s<sub>i</sub></italic> &#x003D; <italic>s<sub>j</sub></italic> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline16.gif"/></alternatives></inline-formula> and we normalize by the number of pixel pairs that are assigned to the same region by at least one of the two segmentations. This is captured by the distance measure<disp-formula>
<alternatives><graphic xlink:href="320531_ueqn4.gif"/></alternatives>
</disp-formula>where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline17.gif"/></alternatives></inline-formula> if the expression <italic>A</italic> is true and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="320531_inline18.gif"/></alternatives></inline-formula> otherwise and the sums go over all pairs of pixels <italic>i, j</italic>. If the two segmentations define exactly the same regions (but possibly with different labels), <italic>d</italic><sub>segm</sub> will be 0, if the two segmentations are completely different, in the sense that one has only one region (the entire image) and the other assigns each pixel to its own region, then <italic>d</italic><sub>segm</sub> will be 1.</p></sec></sec>
<sec id="s2b"><title>Results</title>
<sec id="s2b1"><title>Lower tolerance for noise applied within the natural image manifold</title>
<p>In the first experiment, seven observers were required to judge which one of two noisy comparison images matched a centrally presented target stimulus (see <xref ref-type="fig" rid="fig3">Figure 3</xref>). <xref ref-type="fig" rid="fig4">Figure 4A</xref> shows psychometric functions for one example observer as a function of noise level. In general, higher noise levels were associated with less correct responses. The observer&#x2019;s performance, indicated by level of response correctness, was least affected by noise that was applied independently to each pixel (critical noise level with 75&#x0025; correct performance at 14.8 &#x00B1; 1.43dB, posterior mean and standard deviation, blue dots and line in <xref ref-type="fig" rid="fig4">Figure 4A</xref>). Observer performance was more affected by noise that was applied in the pixel domain but matched the power spectrum of the original image (critical noise level at 7.31 &#x00B1; 0.54dB, green dots and line in <xref ref-type="fig" rid="fig4">Figure 4A</xref>). Finally, the level of response correctness was most affected by noise that was applied in the latent space of the GAN and thus stayed within the manifold of natural images (critical noise level at 1.60 &#x00B1; 0.60 dB, red dots and line in <xref ref-type="fig" rid="fig4">Figure 4A</xref>). Thus, this observer&#x2019;s level of response correctness was most affected by noise applied within the manifold of natural images. Furthermore, the psychometric function for this observer was considerably steeper in the latent noise condition (slope at critical noise level was &#x2212;0.11 &#x00B1; 0.09/dB) than in the other two conditions (slopes at critical noise level were &#x2212;0.015 &#x00B1; 0.006/dB for pixel noise and &#x2212;0.013 &#x00B1; 0.0015/dB for Fourier noise), suggesting that the observer was relatively insensitive to low amplitude latent noise and then abruptly became unable to do the task, consistent with the idea that for latent noise above a certain level a categorical change happens, while noise in the pixel domain results in a more gradual decrease in image quality.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Higher sensitivity to noise perturbations applied within the manifold</title>
<p><bold>A.</bold> Psychometric function for one example observer. Each dot represents between 50 and 60 trials, solid lines are mean a-posteriori estimates of the psychometric function (see Methods). All noise levels were quantified as root mean square difference to the target in pixel space and were normalized to the background luminance. <bold>B.</bold> Average critical noise levels corresponding to 75&#x0025; correct performance. Height of the bars denotes the mean across 7 observers, error bars indicate s.e.m. across observers. Colors are the same as in part A. <bold>C.</bold> Average slope of the psychometric function at the critical noise level. Height of the bars denotes the mean across 7 observers, error bars indicate s.e.m. across observers. Colors are the same as in part A.</p></caption>
<graphic xlink:href="320531_fig4.tif"/>
</fig>
<p>On average across all 7 observers, the critical noise level was highest for independent pixel noise (8.91&#x00B1;1.22dB, mean&#x00B1;s.e.m.) and it was comparable for Fourier noise (6.77&#x00B1;0.83dB, paired <italic>t</italic>-test pixel vs. Fourier noise: <italic>t</italic>(6) &#x003D; &#x2212;1.59, n.s.) and decreased significantly for latent noise (3.47&#x00B1;0.36dB, paired <italic>t</italic>-test pixel vs. latent noise: <italic>t</italic>(6) &#x003D; 3.59, <italic>p</italic> &#x003D; 0.011, Fourier vs. latent noise: <italic>t</italic>(6) &#x003D; 3.89, <italic>p</italic> &#x003D; 0.0080) respectively (see <xref ref-type="fig" rid="fig4">Figure 4B</xref>). Thus overall, observers were most affected by noise that was approximately applied within the manifold of natural images by perturbing the GAN&#x2019;s latent representation of the stimulus. We verified that this result also held for every individual observer. We further found that psychometric functions tended to fall off more steeply when noise was applied in the GAN&#x2019;s latent space (average slope at critical noise level for latent noise was -0.066&#x00B1;0.0084/dB, see <xref ref-type="fig" rid="fig4">Figure 4C</xref>) than when noise was applied in pixel space (average slope at critical noise level for pixel noise was -0.024&#x00B1;0.0041/dB, for Fourier noise -0.017&#x00B1;0.0027/dB), replicating the observations from <xref ref-type="fig" rid="fig4">Figure 4A</xref>.</p>
<p>To summarize, we found that, in general, noise that was approximately applied within the manifold of natural images was more effective at interrupting observers&#x2019; performance than noise applied outside of the manifold of natural images in pixel space. We performed two additional sets of analysis to determine (i) if latent space or pixel space image differences were more predictive of observer&#x2019;s trial by trial behaviour and (ii) which image features were responsible for the decline in performance in the latent noise condition.</p></sec>
<sec id="s2b2"><title>Distance in latent space correlates with image matching performance</title>
<p>We modeled observer behaviour in the image matching task, by assuming that on every trial, the observer picks the comparison stimulus that appears closer to the target with respect to some appropriate distance measure. We looked at three different candidate distance measures and applied them both in the GAN&#x2019;s latent space and directly in pixel space. To evaluate the relevance of each distance measure, we used the area under the receiver operating curve (AUC) for discrimination between correct and incorrect trials.</p>
<p><xref ref-type="fig" rid="fig5">Figure 5A</xref> shows average AUC-values for different hypothetical distance measures applied either in latent space or in pixel space. The simplest distance measure is the Euclidean distance, marked by the blue bars in <xref ref-type="fig" rid="fig5">Figure 5A</xref> (darker blue for Euclidean distance in latent space, lighter blue for Euclidean distance in pixel space). Note that Euclidean distance in pixel space is equivalent to the RMS difference between stimuli that was used as a unified measure of perturbation strength in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Euclidean distance in latent space was at least as predictive as Euclidean distance in pixel space (latent space: 0.83&#x00B1;0.0092, average AUC &#x00B1; s.e.m., pixel space: 0.82&#x00B1;0.014, permutation test <italic>p</italic> &#x003D; 0.17).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Explaining trial by trial performance</title>
<p><bold>A.</bold> Area under the ROC curve for different hypothetical distances applied in latent space or pixel space. Distance measures applied in the GAN&#x2019;s latent space, corresponded to distances within the manifold of natural images (darker colors). Color hue codes for the type of distance measure used. Error bars indicate 95&#x0025; bootstrap confidence intervals. The light gray line marks the average critical value for chance performance. <bold>B.</bold> Same as A. but for image features. &#x201C;Lum&#x201D; corresponds to comparisons of luminance histograms, &#x201C;Ori&#x201D; corresponds to comparisons of orientation histograms, &#x201C;Edg&#x201D; corresponds to edge density, &#x201C;Pow&#x201D; corresponds to the slope of the image&#x2019;s power spectrum, and &#x201C;Seg&#x201D; corresponds to comparisons of image segmentations (see Methods for detail).</p></caption>
<graphic xlink:href="320531_fig5.tif"/>
</fig>
<p>We performed the same analysis using the difference between the norms of the either the latent vector or the pixel vector (green bars in <xref ref-type="fig" rid="fig5">Figure 5A</xref>). In pixel space, radial distance is equivalent to an observer who simply compares the RMS contrast of the images. In latent space, the norm of the latent vector seems to be related to contrast as well, but the relationship is more complex. Radial distance receives considerably lower AUC than Euclidean distance in both latent and pixel space and was a much less reliable predictor of trial by trial performance. Radial distance in latent space was significantly less predictive than radial distance in pixel space (latent space: 0.57&#x00B1;0.022, pixel space: 0.68&#x00B1;0.016, permutation test <italic>p</italic> &#x003C; 0.05 corrected) and for four out of 7 observers, radial distance in latent space did not predict trial by trial choices significantly better than chance.</p>
<p>Finally, we analyzed how well cosine distance explained observers&#x2019; responses (red bars in <xref ref-type="fig" rid="fig5">Figure 5A</xref>). Cosine distance is interesting for two reasons. Firstly, cosine distance is equivalent to Euclidean distance except for the influence of the radial component. Secondly, cosine distance can be interpreted as an observer who uses the target stimulus as a template and linearly matches that template against both comparison stimuli to pick the stimulus that matches best<sup><xref ref-type="fn" rid="fn1">1</xref></sup>. Cosine distance applied in latent space was a better predictor than if it was applied in pixel space (latent space: 0.82&#x00B1;0.012, pixel space: 0.78&#x00B1;0.019, permutation test <italic>p</italic> &#x003D; 0.05 corrected).</p>
<p>Taken together, these results suggest that the latent space of GANs captures processing beyond simple contrast differences.</p></sec>
<sec id="s2b3"><title>Distortions of mid-level features explain trial by trial performance</title>
<p>In order to determine which image features were responsible for the decline in performance with perturbations in the latent space of GANs, we applied the same analysis to different image features (see <xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p>
<p>We found that differences in the luminance distribution of the images are clearly predictive of trial by trial behaviour (average AUC: 0.67&#x00B1;0.018, AUC was larger than 95-th percentile of the null distribution in all seven observers). Yet, other features, such as the difference in local orientation (average AUC: 0.70&#x00B1;0.010) or differences in the images edge density (average AUC: 0.67&#x00B1;0.011) were equally good predictors of the observers&#x2019; trial by trial behaviour (permutation test not significant after correction for multiple comparisons).</p>
<p>One of the quantities that might have been relevant for our observers is the slope of the power spectrum (see for example <xref ref-type="bibr" rid="c1">Alam et al., 2014</xref>). We evaluated to what extent this feature contributed to our observers&#x2019; decisions and found that it is largely irrelevant at explaining the observers&#x2019; trial by trial behaviour: In three out of seven observers the AUC of this feature was not significantly different from chance performance and the average AUC for the slope of the images&#x2019; power spectrum was significantly less than that for any other feature we studied.</p>
<p>In order to quantify how well differences in the mid-level structure of the perturbed images could explain trial by trial responses, we calculated segmentations of all images using a standard segmentation algorithm (<xref ref-type="bibr" rid="c7">Felsenzwalb &#x0026; Huttenlocher, 2004</xref>). Although we do not believe that humans necessarily segment images using graph based optimization as the algorithm does, we believe that this approach provides at least a coarse approximation to the mid-level structure of the images. Differences in segmentation were considerably more predictive than differences in any other feature distribution (mean AUC for segmentation 0.82&#x00B1;0.011, permutation test <italic>p</italic> &#x003C; 0.05 corrected). In fact, differences in segmentation were about as predictive of trial by trial behaviour as cosine distance in latent space (permutation test <italic>p</italic> &#x003D; 0.60), suggesting that indeed distortions of the images&#x2019; mid-level structure might be responsible for the decline in image matching performance when noise was constrained to stay within the manifold of natural images by applying it in the GAN&#x2019;s latent space.</p></sec></sec></sec>
<sec id="s3"><label>Experiment 2:</label><title>Sensitivity to directions in the natural image manifold</title>
<p>In Experiment 1, we found that human observers are particularly sensitive to image perturbations that stay within the manifold of natural images. This was achieved by perturbing stimuli along a parameterization of the manifold of natural images as recovered by a GAN. We wondered if observers were also sensitive to other aspects of this parameterization, such as for example direction. We therefore asked observers to discriminate between videos that were created by walking along either straight paths in latent space (i.e. which contained no change in direction) or paths that had a sudden turn (i.e. which contained a change in direction).</p>
<sec id="s3a"><title>Method</title>
<sec id="s3a1"><title>Observers</title>
<p>Five observers participated in the second experiment. All five of them had participated in the first experiment as well. The procedures were approved by the Ethics Board of York University, ON, Canada and observers provided informed consent before participating. One observer (O3) accidentally did one session with incorrect angle logging. As the correct angles could not be recovered, we decided to exclude the corresponding trials from the analysis.</p></sec>
<sec id="s3a2"><title>Procedure</title>
<p>The experiment was a single interval design. On every trial, the observers either saw a video without a turn in latent space direction or a video that contained a turn in latent space direction and they had to decide if the presented video contained a turn or not. The probabilities for turn and no-turn videos were each 0.5. In order to avoid bias about the image features that would indicate a turn in latent space, observers were instructed that there would be two classes of videos and that we were not able to describe the difference unambiguously in words. Instead, each observer first saw 100 trials with path angles of 90&#x00B0; (vs straight) and received trial by trial feedback about their performance. After that, each observer performed two blocks of 100 trials for each path angle for the main experiment. For each block, the size of the possible path angle was kept constant. To allow the observers to calibrate their decision criterion to the size of turns presented in the respective block, we provided trial by trial feedback during the first 20 trials of each block and only analyzed the remaining 80 trials. Thus in total, we analyzed 160 trials per observer per path angle.</p></sec>
<sec id="s3a3"><title>Stimuli</title>
<p>Each video consisted of 60 frames. The first frame corresponded to a random point on the sphere with radius 10. Successive frames were then constructed by taking steps of norm 0.5 in a random direction tangential to the sphere with radius 10 (see <xref ref-type="fig" rid="fig6">Figure 6A</xref>). Straight paths were constructed by simply taking 60 successive steps in the same direction. Paths with a turn were constructed by changing the direction of steps by an angle a of 15, 30, 60, or 90 degrees after the first 30 frames. We will refer to this angle as the &#x201C;path angle&#x201D; in the following. At a frame rate of 60Hz, each video had a duration of 1s and if the video contained a turn in latent space, that turn happened after 500ms. Otherwise, the setup for experiment 2 was the same as in experiment 1.</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Figure 6:</label>
<caption><title>Observers can detect turns in paths along the manifold of natural images</title>
<p><bold>A.</bold> Illustration of stimulus construction. Two latent axes <italic>z<sub>&#x2113;</sub></italic> and <italic>z<sub>k</sub></italic> are shown for illustration, the actual latent space had a dimension of 128. The light gray circle marks the sphere of radius 10 in latent space. Shown are a straight path (dotted line, standard stimulus in the experiment) and a path that turns half way at an angle <italic>&#x03B1;</italic> (solid line, target stimulus in the experiment). <bold>B.</bold> Average sensitivity for detecting a turn in an image sequence along a path along the manifold. Error bars mark 95&#x0025; confidence intervals.</p></caption>
<graphic xlink:href="320531_fig6.tif"/>
</fig></sec>
<sec id="s3a4"><title>Data analysis</title>
<p>To evaluate observers&#x2019; sensitivity to path angles, we calculated <italic>d</italic>&#x2032; at each path angle. For single observers, confidence intervals for <italic>d</italic>&#x2032; were determined by bootstrap with 1000 samples.</p></sec></sec>
<sec id="s3b"><title>Results</title>
<p><xref ref-type="fig" rid="fig6">Figure 6B</xref> shows average discrimination performance as a function of path angle. Not surprisingly, observers were best at detecting turns of 90 degrees in latent space (average <italic>d</italic>&#x2032; &#x003D; 2.23 &#x00B1; 0.59). However, even for path angles of 15 degrees, the smallest path angles tested, three out of five observers performed above chance (average <italic>d</italic>&#x2032; &#x003D; 0.20 &#x00B1; 0.090, one-sided <italic>t</italic>-test against zero: <italic>t</italic>(4) &#x003D; 2.25, <italic>p</italic> &#x003D; 0.0436). This indicates that even small changes in latent space direction were detected by the observers.</p>
<p>Overall, observers were remarkably good at discriminating between different directions taken by movies in latent space. In fact half of the observers responded correctly in more than 80&#x0025; of the trials (average 78.1&#x00B1;2.56&#x0025;, mean &#x00B1; s.e.m.) at the largest turn.</p></sec></sec>
<sec id="s4"><title>General Discussion</title>
<p>We found that observers are sensitive to image manipulations that are restricted to remain within the manifold of natural images. Errors made within the manifold of natural images seem to be related to changes in the configural structure of the images more than to changes in local image features. We further find that observers are sensitive not only to noise within the manifold, but also to more subtle aspects of paths along the manifold (see Experiment 2).</p>
<sec id="s4a"><title>Global vs local image models</title>
<p>Our results seem to contradict reports which find that observers are very sensitive to deviations from naturalness (<xref ref-type="bibr" rid="c14">Gerhard et al., 2013</xref>): How would observers be equally sensitive to changes within the manifold of natural images and deviations away from that manifold? We find that sensitivity to perturbations within the manifold of natural images can be well predicted by the part and object structure of the image that is captured by standard segmentation algorithms (see <xref ref-type="fig" rid="fig5">Figure 5</xref>), while deviations from naturalness might be more related to the fine structure of local statistics. This is also visible in <xref ref-type="fig" rid="fig1">Figure 1</xref>: The two image classes in parts A. and B. appear very similar at first glance and only upon closer inspection does it become clear that the examples in part B. are mostly meaningless. We believe that this point could be taken both as an advantage and a disadvantage. Clearly, the samples in <xref ref-type="fig" rid="fig1">Figure 1B</xref>. do not match every aspect of the natural images in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. (although better matches can be achieved if the GAN is restricted to more narrow classes of objects, see for example <xref ref-type="bibr" rid="c30">Radford et al., 2016</xref>; <xref ref-type="bibr" rid="c18">Gulrajani et al., 2017</xref>). However, the images capture a lot of the global and highly non-stationary properties of natural images, that texture models based on stationarity do not capture (<xref ref-type="bibr" rid="c29">Portilla &#x0026; Simoncelli, 2000</xref>; <xref ref-type="bibr" rid="c12">Gatys et al., 2015</xref>). We therefore believe that our approach is complementary to the local approach taken by studies that investigated texture processing (e.g. <xref ref-type="bibr" rid="c8">Freeman &#x0026; Simoncelli, 2011</xref>; <xref ref-type="bibr" rid="c14">Gerhard et al., 2013</xref>; <xref ref-type="bibr" rid="c40">Wallis et al., 2016</xref>, <xref ref-type="bibr" rid="c42">2017</xref>)</p></sec>
<sec id="s4b"><title>Small images</title>
<p>The images employed here are relatively small. Our images were only 32&#x00D7;32 pixels in size. In contrast, <xref ref-type="bibr" rid="c42">Wallis et al. (2017</xref>) used image patches that were 128 &#x00D7; 128 pixels to compare between texture images created from a deep neural network model and real photographs of textures. Other studies have used a range of images sizes (<xref ref-type="bibr" rid="c1">Alam et al., 2014</xref>; <xref ref-type="bibr" rid="c35">Sebastian et al., 2017</xref>; <xref ref-type="bibr" rid="c6">Bex, 2010</xref>, in increasing order of image size), but our images are closer to the range of image sizes used as patches of images (<xref ref-type="bibr" rid="c14">Gerhard et al., 2013</xref>) rather than entire images. However, training generative adversarial networks on larger images with similar image variability as the CIFAR10 network currently typically requires training class conditional networks as for example done by <xref ref-type="bibr" rid="c28">Miyato et al. (2018</xref>) when training on the entire ImageNet dataset (<xref ref-type="bibr" rid="c32">Russakovsky et al., 2015</xref>). Although this would have in principle been possible in this study as well, it would have implied that separate manifolds would be used for different classes and it would have made interpretation of our results considerably more complex. We therefore decided to restrict ourselves to smaller images.</p></sec>
<sec id="s4c"><title>Dataset bias</title>
<p>Many publicly available data bases appear to be systematically biased (<xref ref-type="bibr" rid="c44">Wichmann et al., 2010</xref>): The images in these databases are pre-segmented in the sense that a photographer selected a viewpoint that they considered particularly &#x201C;interesting&#x201D; or in that they selected which objects to put in focus. Although pictures from these databases may appear natural, conclusions drawn from these data bases may be misleading (<xref ref-type="bibr" rid="c44">Wichmann et al., 2010</xref>). The example pictures in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. clearly show such photographer bias. In every one of these examples, the perspective is clearly focused on one specific object, while typical natural scenes often contain multiple objects and a lot of not explicitly defined random texture (i.e. background). As outlined above, we believe that our approach is less prone to such dataset bias because it predominantly depends on the GAN&#x2019;s ability to capture the global structure of the images. Although this global structure may be imposed by the photographer, it seems that a similar global structure might also be imposed by eye movements centering objects on or close to the fovea (<xref ref-type="bibr" rid="c23">Kayser et al., 2006</xref>; &#x2019;t <xref ref-type="bibr" rid="c37">Hart et al., 2013</xref>).</p></sec>
<sec id="s4d"><title>Non-object images</title>
<p>Upon closer inspection, the examples in <xref ref-type="fig" rid="fig1">Figure 1B</xref>. do not look exactly like real objects. Although each one of the examples can clearly be segmented into foreground and background, it is not always possible to actually name the foreground objects in <xref ref-type="fig" rid="fig1">Figure 1B</xref>., while this seems to be easier for the training examples in <xref ref-type="fig" rid="fig1">Figure 1A</xref>., despite the relatively low resolution of the images. Thus, the samples from the GAN used in the present study would probably be easy to discriminate from real images if they were directly compared to real images (<xref ref-type="bibr" rid="c14">Gerhard et al., 2013</xref>; <xref ref-type="bibr" rid="c42">Wallis et al., 2017</xref>). It should thus be noted that the image representation learned by the GAN used in this study is only an approximation to the manifold of natural images (note however, that other studies training on the CIFAR10 dataset show samples of similar quality, for example <xref ref-type="bibr" rid="c18">Gulrajani et al., 2017</xref>; <xref ref-type="bibr" rid="c31">Roth et al., 2017</xref>). Although image manipulations in this approximation appear to be convincing if the GAN has been trained on more restricted sets of training images (see for example <xref ref-type="bibr" rid="c45">Zhu et al., 2016</xref>), this can not be guaranteed for all the stimuli used in this experiment. To this date, it is unclear how exactly the GAN samples used in this study match the perceived properties of the training data (even though the training data themselves might be biased, see Section &#x201C;Dataset bias&#x201D;).</p>
<p>We believe however, that natural-appearing non-objects are still an interesting class of stimuli. For example, <xref ref-type="bibr" rid="c21">Huth et al. (2012</xref>) report that semantic content is important for shaping the response properties of large parts of anterior visual cortex, suggesting that many areas that are traditionally thought of as visual, are also semantic areas. Attempts to further test this claim have been restricted to correlational approaches (<xref ref-type="bibr" rid="c24">Khaligh-Razavi &#x0026; Kriegeskorte, 2014</xref>), partly because it is difficult to generate &#x201C;non-object&#x201D; stimuli that otherwise fully match the properties of object stimuli (see also <xref ref-type="bibr" rid="c9">Fr&#x00FC;nd et al., 2008</xref>). Comparing responses to training images with objects (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) to images generated from a GAN but without the full semantic information (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) could help resolve this point.</p></sec>
<sec id="s4e"><title>Conclusion</title>
<p>In this study, we explored the potential of studying vision within the manifold of natural images. To do so, we employed a generative adversarial network to constrain perturbations to remain within the manifold of natural images and we find that observers are remarkably sensitive to image manipulations that are constrained in this way. We observe that perturbations within the manifold of natural images tend to disrupt more global image structures such as figure-ground segmentation structure. This might prove useful in future studies that investigate such processes under more naturalistic conditions. The fact that GANs provide an approximate parametrization to the manifold of natural images encourages further use of these powerful image models to study vision under complex naturalistic stimulus conditions.</p></sec></sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Alam</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Vilankar</surname>, <given-names>K. P.</given-names></string-name>, <string-name><surname>Field</surname>, <given-names>D. J.</given-names></string-name>, &#x0026; <string-name><surname>Chandler</surname>, <given-names>D. M.</given-names></string-name> (<year>2014</year>). <article-title>Local masking in natural images: A database and analysis</article-title>. <source>Journal of Vision</source>, <volume>14</volume>(8)(<issue>22</issue>), <fpage>1</fpage>&#x2013;<lpage>38</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Allard</surname>, <given-names>R.</given-names></string-name>, &#x0026; <string-name><surname>Faubert</surname>, <given-names>J.</given-names></string-name> (<year>2008</year>). <article-title>The noisy-bit method for digital displays: Converting a 256 luminance resolution into a continuous resolution</article-title>. <source>Behavior Research Methods</source>, <volume>40</volume>(<issue>3</issue>), <fpage>735</fpage>&#x2013;<lpage>743</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="other"><string-name><surname>Arjovsky</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Chintala</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Bottou</surname>, <given-names>L.</given-names></string-name> (<year>2017</year>). <source>Wasserstein GAN</source>. arXiv:1701.07875.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Benjamini</surname>, <given-names>Y.</given-names></string-name>, &#x0026; <string-name><surname>Hochberg</surname>, <given-names>Y.</given-names></string-name> (<year>1995</year>). <article-title>Controlling the false discovery rate: A practical and powerful approach to multiple testing</article-title>. <source>Journal of the Royal Statistical Society. Series B (Methodological)</source>, <volume>57</volume>(<issue>1</issue>), <fpage>289</fpage>&#x2013;<lpage>300</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wiecki</surname>, <given-names>T. V.</given-names></string-name>, &#x0026; <string-name><surname>Wichmann</surname>, <given-names>F. A.</given-names></string-name> (<year>2007</year>). <article-title>The independent components of natural images are peperceptual dependent</article-title>. <source>In Proceedings of spie, human vision and electronic imaging xii</source> (Vol. <volume>6492</volume>).</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Bex</surname>, <given-names>P. J.</given-names></string-name> (<year>2010</year>). <article-title>(In) Sensitivity to spatial distortion in natural scenes</article-title>. <source>Journal of Vision</source>, <volume>10</volume>(2)(<issue>23</issue>), <fpage>1</fpage>&#x2013;<lpage>15</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Felsenzwalb</surname>, <given-names>P. F.</given-names></string-name>, &#x0026; <string-name><surname>Huttenlocher</surname>, <given-names>D. P.</given-names></string-name> (<year>2004</year>). <article-title>Efficient graph-based image segmentation</article-title>. <source>International Journal of Computer Vision</source>, <volume>59</volume>(<issue>2</issue>), <fpage>167</fpage>&#x2013;<lpage>181</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Freeman</surname>, <given-names>J.</given-names></string-name>, &#x0026; <string-name><surname>Simoncelli</surname>, <given-names>E.</given-names></string-name> (<year>2011</year>). <article-title>Metamers of the ventral stream</article-title>. <source>Nature Neuroscience</source>, <volume>14</volume>, <fpage>1195</fpage>&#x2013;<lpage>1201</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Fr&#x00FC;nd</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Busch</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Schadow</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Gruber</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>K&#x00F6;rner</surname>, <given-names>U.</given-names></string-name>, &#x0026; <string-name><surname>Herrmann</surname>, <given-names>C. S.</given-names></string-name> (<year>2008</year>). <article-title>Time pressure modulates electrophysiological correlates of early visual processing</article-title>. <source>PLoS One</source>, <volume>3</volume>(<issue>2</issue>), <fpage>e1675</fpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Fr&#x00FC;nd</surname>, <given-names>I.</given-names></string-name>, &#x0026; <string-name><surname>Elder</surname>, <given-names>J.</given-names></string-name> (<year>2013</year>). <article-title>Statistical coding of natural closed contours</article-title>. <source>Journal of Vision</source>, <volume>13</volume>(<issue>9</issue>), <fpage>119</fpage>&#x2013;<lpage>119</lpage>. (VSS abstract)</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Fr&#x00FC;nd</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Haenel</surname>, <given-names>N. V.</given-names></string-name>, &#x0026; <string-name><surname>Wichmann</surname>, <given-names>F. A.</given-names></string-name> (<year>2011</year>). <article-title>Inference for psychometric functions in the presence of nonstationary behavior</article-title>. <source>Journal of Vision</source>, <volume>11</volume>(6)(<issue>16</issue>), <fpage>1</fpage>&#x2013;<lpage>19</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="book"><string-name><surname>Gatys</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name>, &#x0026; <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name> (<year>2015</year>). <chapter-title>Texture synthesis using convolutional neural networks</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>C.</given-names> <surname>Cortes</surname></string-name>, <string-name><given-names>N. D.</given-names> <surname>Lawrence</surname></string-name>, <string-name><given-names>D. D.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Sugiyama</surname></string-name>, &#x0026; <string-name><given-names>R.</given-names> <surname>Garnett</surname></string-name></person-group> (Eds.), <source>Advances in neural information processing systems</source> <volume>28</volume> (p. <fpage>262</fpage>&#x2013;<lpage>270</lpage>).</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Geisler</surname>, <given-names>W. S.</given-names></string-name> (<year>2008</year>). <article-title>Visual perception and the statistical properties of natural scenes</article-title>. <source>Annual Review of Psychology</source>, <volume>59</volume>, <fpage>167</fpage>&#x2013;<lpage>192</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Gerhard</surname>, <given-names>H. E.</given-names></string-name>, <string-name><surname>Wichmann</surname>, <given-names>F. A.</given-names></string-name>, &#x0026; <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name> (<year>2013</year>). <article-title>How sensitive is the human visual system to the local statistics of natural images</article-title>? <source>PLoS Computational Biology</source>, <volume>9</volume>(<issue>1</issue>), <fpage>e1002873</fpage>., doi:<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pcbi.1002873">https://doi.org/10.1371/journal.pcbi.1002873</ext-link></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="book"><string-name><surname>Glorot</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Bordes</surname>, <given-names>A.</given-names></string-name>, &#x0026; <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name> (<year>2011</year>). <chapter-title>Deep sparse rectifier neural networks</chapter-title>. In <source>Proceedings of the 14th international conference on artificial intelligence and statistics</source> (Vol. <volume>15</volume> of <publisher-name>JMLR</publisher-name>).</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="book"><string-name><surname>Goodfellow</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Pouget-Abadie</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Mirza</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Xu</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Warde-Farley</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Ozair</surname>, <given-names>S.</given-names></string-name>, <etal>et al.</etal> (<year>2014</year>). <chapter-title>Generative adversarial nets</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>Z.</given-names> <surname>Gharamani</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Welling</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Cortex</surname></string-name>, <string-name><given-names>N. D.</given-names> <surname>Lawrence</surname></string-name>, &#x0026; <string-name><given-names>K. Q.</given-names> <surname>Weinberger</surname></string-name></person-group> (Eds.), <source>Advances in neural information processing systems</source> <volume>27</volume>. <publisher-name>Curran Associates, Inc</publisher-name>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="book"><string-name><surname>Green</surname>, <given-names>D. M.</given-names></string-name>, &#x0026; <string-name><surname>Swets</surname>, <given-names>J. A.</given-names></string-name> (<year>1966</year>). <source>Signal detection theory and psychophysics</source>. <publisher-name>Wiley</publisher-name>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="other"><string-name><surname>Gulrajani</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Ahmed</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Arjovsky</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Dumoulin</surname>, <given-names>V.</given-names></string-name>, &#x0026; <string-name><surname>Courville</surname>, <given-names>A.</given-names></string-name> (<year>2017</year>). <source>Improved training of Wasserstein GANs</source>. arXiv:1704.00028.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="other"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Sun</surname>, <given-names>J.</given-names></string-name> (<year>2015</year>). <article-title>Delving deep into rectifiers: Surpassing human-level performance on imaimage classification</article-title>. In <source>International conference on computer vision (iccv)</source>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="other"><string-name><surname>Hjelm</surname>, <given-names>R. D.</given-names></string-name>, <string-name><surname>Jacob</surname>, <given-names>A. P.</given-names></string-name>, <string-name><surname>Che</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Trischler</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cho</surname>, <given-names>K.</given-names></string-name>, &#x0026; <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name> (<year>2018</year>). <article-title>Boundary-seeking generative adversarial networks</article-title>. <source>International Conference on Learning Representations</source>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Huth</surname>, <given-names>A. G.</given-names></string-name>, <string-name><surname>Nishimoto</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Vu</surname>, <given-names>A. T.</given-names></string-name>, &#x0026; <string-name><surname>Gallant</surname>, <given-names>J. L.</given-names></string-name> (<year>2012</year>). <article-title>A continuous semantic space describes the representation of thousands of object and action categories across the human brain</article-title>. <source>Neuron</source>, <volume>76</volume>, <fpage>1210</fpage>&#x2013;<lpage>1224</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Ioffe</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Szegedy</surname>, <given-names>C.</given-names></string-name> (<year>2015</year>). <article-title>Batch normalization: Accelerating deep network training by reducing internal covariate shift</article-title>. In <string-name><given-names>F.</given-names> <surname>Bach</surname></string-name> &#x0026; B. D (Eds.), <source>Proceedings of the 32nd international conference on machine learning</source> (Vol. <volume>37</volume>).</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Kayser</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Nielsen</surname>, <given-names>K. J.</given-names></string-name>, &#x0026; <string-name><surname>Logothetis</surname>, <given-names>N. K.</given-names></string-name> (<year>2006</year>). <article-title>Fixations in natural scenes: Interaction of image structure and image content</article-title>. <source>Vision Research</source>, <volume>46</volume>, <fpage>2535</fpage>&#x2013;<lpage>2545</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Khaligh-Razavi</surname>, <given-names>S.-M.</given-names></string-name>, &#x0026; <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> (<year>2014</year>). <article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title>. <source>PLoS Computational Biology</source>, <volume>10</volume>(<issue>11</issue>), <fpage>e1003915</fpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="other"><string-name><surname>Kingma</surname>, <given-names>D. P.</given-names></string-name>, &#x0026; <string-name><given-names>Ba. L.</given-names>, <surname>Jimmy</surname></string-name>. (<year>2015</year>). <article-title>Adam: A method for stochastic optimization</article-title>. In <source>International conference on learning representations</source>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="book"><string-name><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name> (<year>2009</year>). <source>Learning multiple layers of features from tiny images (Tech. Rep.)</source>. <publisher-name>University of Toronto</publisher-name>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>McDonald</surname>, <given-names>J. S.</given-names></string-name>, &#x0026; <string-name><surname>Tadmor</surname>, <given-names>Y.</given-names></string-name> (<year>2006</year>). <article-title>The perceived contrast of texture patches embedded in natural images</article-title>. <source>Vision Research</source>, <volume>46</volume>, <fpage>3098</fpage>&#x2013;<lpage>3104</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="other"><string-name><surname>Miyato</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Kataoka</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Koyama</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Yoshida</surname>, <given-names>Y.</given-names></string-name> (<year>2018</year>). <article-title>Spectral normalization for generative adversarial networks</article-title>. <source>International Conference on Learning Representations</source>. (accepted as oral presentation)</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Portilla</surname>, <given-names>J.</given-names></string-name>, &#x0026; <string-name><surname>Simoncelli</surname>, <given-names>E.</given-names></string-name> (<year>2000</year>). <article-title>A parametric texture model based on joint statistics of complex wavelet coefficients</article-title>. <source>International Journal of Computer Vision</source>, <volume>40</volume>(<issue>1</issue>), <fpage>49</fpage>&#x2013;<lpage>71</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="other"><string-name><surname>Radford</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Luke</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Chintala</surname>, <given-names>S.</given-names></string-name> (<year>2016</year>). <article-title>Unsupervised representation learning with deep convolutional generative adversarial networks</article-title>. In <source>International conference on learning representations</source>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="book"><string-name><surname>Roth</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Lucchi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Nowozin</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Hofmann</surname>, <given-names>T.</given-names></string-name> (<year>2017</year>). <chapter-title>Stabilizing training of generative adversarial networks through regularization</chapter-title>. In <string-name><given-names>I.</given-names> <surname>Guyon</surname></string-name> <etal>et al.</etal> (Eds.), <source>Advances in neural information processing systems</source> <volume>30</volume> (p. <fpage>2018</fpage>&#x2013;<lpage>2028</lpage>). <publisher-name>Curran Associates, Inc</publisher-name>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Russakovsky</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Deng</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Su</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Krause</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Satheesh</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Ma</surname>, <given-names>S.</given-names></string-name>, <etal>et al.</etal> (<year>2015</year>). <article-title>Imaimage large scale visual recognition callenge</article-title>. <source>International Journal of Computer Vision</source>, <volume>115</volume>(<issue>3</issue>), <fpage>211</fpage>&#x2013;<lpage>252</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="book"><string-name><surname>Scharr</surname>, <given-names>H.</given-names></string-name> (<year>2000</year>). <source>Optimale operatoren in der digitalen bildverarbeitung</source>. <publisher-loc>Unpublished doctoral dissertation, IWR, Fakult&#x00E4;t f&#x00FC;r Physik und Astronomie</publisher-loc>, <publisher-name>Univ. Heidelberg</publisher-name>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Sch&#x00FC;tt</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Harmeling</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Macke</surname>, <given-names>J. H.</given-names></string-name>, &#x0026; <string-name><surname>Wichmann</surname>, <given-names>F. A.</given-names></string-name> (<year>2016</year>). <article-title>Painfree and accurate bayesian estimation of psychometric functions for (potentially) overdispersed data</article-title>. <source>Vision Research</source>, <volume>122</volume>, <fpage>105</fpage>&#x2013;<lpage>123</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="other"><string-name><surname>Sebastian</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Abrams</surname>, <given-names>J.</given-names></string-name>, &#x0026; <string-name><surname>Geisler</surname>, <given-names>W. S.</given-names></string-name> (<year>2017</year>). <article-title>Constrained sampling experiments reveal principles of detection in natural scenes</article-title>. <source>Proc Natl Acad Sci USA</source>, <fpage>E5731</fpage>&#x2013;<lpage>E5740</lpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Simoncelli</surname>, <given-names>E. P.</given-names></string-name>, &#x0026; <string-name><surname>Olshausen</surname>, <given-names>B. A.</given-names></string-name> (<year>2001</year>). <article-title>Natural image statistics and neural representation</article-title>. <source>Annual Review of Neuroscience</source>, <volume>24</volume>, <fpage>1193</fpage>&#x2013;<lpage>1216</lpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>&#x2019;t Hart</surname>, <given-names>B. M.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>H. C. E. F.</given-names></string-name>, <string-name><surname>Roth</surname>, <given-names>C.</given-names></string-name>, &#x0026; <string-name><surname>Einh&#x00E4;user</surname>, <given-names>W.</given-names></string-name> (<year>2013</year>). <article-title>Fixations on objects in natural scenes: dissociating importance from salience</article-title>. <source>Frontiers in Psychology</source>, <volume>4</volume>(<issue>455</issue>), <fpage>1</fpage>&#x2013;<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Thorpe</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Fize</surname>, <given-names>D.</given-names></string-name>, &#x0026; <string-name><surname>Marlot</surname>, <given-names>C.</given-names></string-name> (<year>2001</year>). <article-title>Speed of processing in the human visual system</article-title>. <source>Nature</source>, <volume>381</volume>, <fpage>520</fpage>&#x2013;<lpage>522</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>van der Walt</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sch&#x00F6;nberger</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Nunez-Iglesias</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Boulogne</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Warner</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Yager</surname>, <given-names>N.</given-names></string-name>, <etal>et al.</etal> (<year>2014</year>). <article-title>scikit-image: image processing in Python</article-title>. <source>PeerJ</source>, <volume>2</volume>, <fpage>e453</fpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Wallis</surname>, <given-names>T. S. A.</given-names></string-name>, <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Wichmann</surname>, <given-names>F. A.</given-names></string-name> (<year>2016</year>). <article-title>Testing models of peripheral encoding using metamerism in an oddity paradigm</article-title>. <source>Journal of Vision</source>, <volume>16</volume>(2)(<issue>4</issue>), <fpage>1</fpage>&#x2013;<lpage>30</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Wallis</surname>, <given-names>T. S. A.</given-names></string-name>, &#x0026; <string-name><surname>Bex</surname>, <given-names>P. J.</given-names></string-name> (<year>2012</year>). <article-title>Image correlates of crowding in natural scenes</article-title>. <source>Journal of Vision</source>, <volume>12</volume>(7)(<issue>6</issue>), <fpage>1</fpage>&#x2013;<lpage>19</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Wallis</surname>, <given-names>T. S. A.</given-names></string-name>, <string-name><surname>Funke</surname>, <given-names>C. M.</given-names></string-name>, <string-name><surname>Ecker</surname>, <given-names>A. S.</given-names></string-name>, <string-name><surname>Gatys</surname>, <given-names>L. A.</given-names></string-name>, <string-name><surname>Wichmann</surname>, <given-names>F. A.</given-names></string-name>, &#x0026; <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name> (<year>2017</year>). <article-title>A parametric texture model based on deep convolutional features closely matches texture appearance for humans</article-title>. <source>Journal of Vision</source>, <volume>17</volume>(<issue>12</issue>(5)), <fpage>1</fpage>&#x2013;<lpage>29</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Wichmann</surname>, <given-names>F. A.</given-names></string-name>, <string-name><surname>Braun</surname>, <given-names>D. I.</given-names></string-name>, &#x0026; <string-name><surname>Gegenfurtner</surname>, <given-names>K. R.</given-names></string-name> (<year>2006</year>). <article-title>Phase noise and the classification of natural images</article-title>. <source>Vision Research</source>, <volume>46</volume>, <fpage>1520</fpage>&#x2013;<lpage>1529</lpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Wichmann</surname>, <given-names>F. A.</given-names></string-name>, <string-name><surname>Drewes</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Rosas</surname>, <given-names>P.</given-names></string-name>, &#x0026; <string-name><surname>Gegenfurtner</surname>, <given-names>K. R.</given-names></string-name> (<year>2010</year>). <article-title>Animal detection in natural scenes: Critical features revisited</article-title>. <source>Journal of Vision</source>, <volume>10</volume>(4)(<issue>6</issue>), <fpage>1</fpage>&#x2013;<lpage>27</lpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="other"><string-name><surname>Zhu</surname>, <given-names>J.-Y.</given-names></string-name>, <string-name><surname>Kr&#x00E4;henbuhl</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Shechtman</surname>, <given-names>E.</given-names></string-name>, &#x0026; <string-name><surname>Efros</surname>, <given-names>A. A.</given-names></string-name> (<year>2016</year>). <article-title>Generative visual manipulation on the natural image manifold</article-title>. In <source>Proceedings of european conference on computer vision (eccv)</source>.</mixed-citation></ref>
</ref-list>
<fn-group>
<fn id="fn1"><label>1</label><p>Note that for cosine distance in latent space, linear template matching would be performed in the GAN&#x2019;s latent space, which corresponds to image space in a complex and highly nonlinear way.</p></fn>
</fn-group>
</back>
</article>