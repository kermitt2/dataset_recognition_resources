<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/402263</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Neural Representations of Faces are Tuned to Eye Movements</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7357-4919</contrib-id>
<name><surname>Stacchi</surname><given-names>Lisa</given-names></name>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5753-5493</contrib-id>
<name><surname>Ramon</surname><given-names>Meike</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Lao</surname><given-names>Junpeng</given-names></name>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8260-9949</contrib-id>
<name><surname>Caldara</surname><given-names>Roberto</given-names></name>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<aff><institution>Eye and Brain Mapping Laboratory (iBMLab), Department of Psychology, University of Fribourg</institution>, Fribourg, <country>Switzerland</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label>Corresponding author: Roberto Caldara Department of Psychology University of Fribourg Faucigny 2 1700 Fribourg Switzerland Email: <email>roberto.caldara@unifr.ch</email> Tel: &#x002B;41 (26) 300 76 36</corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2018</year>
</pub-date>
<elocation-id>402263</elocation-id>
<history>
<date date-type="received">
<day>28</day>
<month>8</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>28</day>
<month>8</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>30</day>
<month>8</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="402263.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>ABSTRACT</title>
<p>Eye movements provide a functional signature of how human vision is achieved. Many recent studies have reported idiosyncratic visual sampling strategies during face recognition. Whether these inter-individual differences are mirrored by idiosyncratic <italic>neural</italic> responses has not been investigated yet. Here, we tracked observers&#x2019; eye movements during face recognition; additionally, we obtained an objective index of neural face discrimination through EEG that was recorded while subjects fixated different facial information.</p><p>Across all observers, we found that those facial features that were fixated longer during face recognition elicited stronger neural face discrimination responses. This relationship occurred independently of inter-individual differences in fixation biases. Our data show that eye movements play a functional role during face processing by providing the neural system with information that is diagnostic to a specific observer. The effective processing of face identity involves idiosyncratic, rather than universal representations.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>eye movements</kwd>
<kwd>individual differences</kwd>
<kwd>face discrimination</kwd>
<kwd>EEG</kwd>
<kwd>fast periodic</kwd>
<kwd>visual stimulation</kwd>
</kwd-group>
<counts>
<page-count count="20"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>INTRODUCTION</title>
<p>The visual system continuously processes perceptual inputs to adapt to the world by selectively moving the eyes towards diagnostic information. As a consequence, eye movements do not unfold randomly, and during face processing humans deploy specific gaze strategies. Since Yarbus&#x2019;s seminal report (<xref ref-type="bibr" rid="c33">Yarbus, 1967</xref>), a multitude of studies have reported a distinct triangular fixation pattern encompassing the eye and mouth region during face recognition (<xref ref-type="bibr" rid="c14">Henderson, Williams, &#x0026; Falk, 2005</xref>). For many years, this T-shaped fixation pattern was considered to be universal and shared across observers, suggesting that the presence of a face triggers a unique biologically-determined information extraction pattern.</p>
<p>However, over the last decade, a growing body of work has challenged this view by revealing cross-cultural (e.g., <xref ref-type="bibr" rid="c5">Blais, Jack, Scheepers, Fiset, &#x0026; Caldara, 2008</xref>; <xref ref-type="bibr" rid="c23">Miellet, Vizioli, He, Zhou, &#x0026; Caldara, 2013</xref>), idiosyncratic (<xref ref-type="bibr" rid="c21">Mehoudar, Arizpe, Baker, &#x0026; Yovel, 2014</xref>) and even within-observer (<xref ref-type="bibr" rid="c22">Miellet, Caldara, &#x0026; Schyns, 2011</xref>) differences during face recognition. Contrary to the average T-shaped fixation pattern displayed by Westerners, Easterners deploy a global sampling strategy by directing the majority of their fixations toward the center of the face, while reaching comparable efficiency in face recognition (for a review see <xref ref-type="bibr" rid="c7">Caldara, 2017</xref>). Even among Western observers there is a conspicuous degree of variability in the sampling strategies adopted to achieve face identification (<xref ref-type="bibr" rid="c22">Miellet et al., 2011</xref>). In addition, in line with early observations based on individual participants (<xref ref-type="bibr" rid="c32">Walker-Smith, Gale, &#x0026; Findlay, 1977</xref>), recent studies demonstrate that observers deploy unique sampling strategies (<xref ref-type="bibr" rid="c16">Kanan, Bseiso, Ray, Hsiao, &#x0026; Cottrell, 2015</xref>; <xref ref-type="bibr" rid="c2">Arizpe, Walsh, Yovel, &#x0026; Baker, 2017</xref>), which are stable over time (<xref ref-type="bibr" rid="c21">Mehoudar et al., 2014</xref>), and relevant to behavioral performance (<xref ref-type="bibr" rid="c28">Peterson &#x0026; Eckstein, 2013</xref>). Individuals&#x2019; sampling strategies deviate considerably from the well-established T-shaped pattern reported for Westerners observers, which is merely the result of the group averaging of the idiosyncratic visual sampling strategies of individual observers (<xref ref-type="bibr" rid="c21">Mehoudar et al., 2014</xref>).</p>
<p>Despite the growing literature on the existence of idiosyncratic sampling strategies, their functional role and the underlying neural mechanisms remain poorly understood. Some studies have investigated the impact of the fixated facial information input on neural responses, by recording the electroencephalographic (EEG) signals while observers fixated different facial information (i.e., viewing positions; VPs). This body of work has focused on the N170 ERP (Event Related Potential) component (<xref ref-type="bibr" rid="c4">Bentin, Allison, Puce, Perez, &#x0026; McCarthy, 1996</xref>), the earliest face sensitive neural marker characterized by an occipito-temporal negative deflection of the EEG signal 170ms after stimulus onset. Collectively, the results of these studies demonstrated that VPs differentially modulate the N170, with fixation on the eye region electing larger amplitudes (<xref ref-type="bibr" rid="c9">de Lissa et al., 2014</xref>, <xref ref-type="bibr" rid="c15">Itier, Latinus, &#x0026; Taylor, 2006</xref>; <xref ref-type="bibr" rid="c25">Nemrodov, Anderson, Preston, &#x0026; Itier, 2014</xref>; <xref ref-type="bibr" rid="c30">Rousselet, Ince, van Rijsbergen, &#x0026; Schyns, 2014</xref>). This observation would suggest a possible universal <italic>neural</italic> preference toward this facial information. However, these studies have mainly involved grand-average analyses and did not control for individual fixation preferences. As a consequence, while this analytical approach allows enhancing commonalities across observers, it confounds crucial individual differences, leaving unaddressed the question of whether idiosyncratic fixation biases concur with idiosyncratic neural responses.</p>
<p>Fast-periodic visual stimulation (FPVS) has been increasingly used to examine different aspects of face processing, including e.g. face detection, discrimination, and categorization (<xref ref-type="bibr" rid="c1">Ales, Farzin, Rossion, &#x0026; Norcia, 2012</xref>; <xref ref-type="bibr" rid="c26">Norcia, Appelbaum, Ales, Cottereau, &#x0026; Rossion, 2015</xref>; <xref ref-type="bibr" rid="c29">Rossion, Torfs, Jacques, &#x0026; Liu-Shuang, 2015</xref>; <xref ref-type="bibr" rid="c18">Liu-Shuang, Norcia, &#x0026; Rossion, 2014</xref>). Compared to traditional ERPs, the FPVS response has many advantages: it is far less susceptible to noise artefacts, and its remarkably high signal-to-noise ratio increases the likelihood of detecting subtle differences between experimental manipulations of interest (<xref ref-type="bibr" rid="c26">Norcia et al., 2015</xref>). Such signal properties make the FPVS paradigm paired with EEG recordings ideal to investigate the potential relationship between VP dependency of neural responses and idiosyncratic visual sampling strategies.</p>
<p>To this aim, we tracked the eye movements of observers performing an old/new face recognition task (<xref ref-type="bibr" rid="c5">Blais et al., 2008</xref>) and extracted their fixation patterns. Within the same testing session, we then recorded their neural face discrimination responses by means of a FPVS paradigm, while observers fixated faces on one out of ten viewing positions covering all inner facial features (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>). We then applied a robust statistical data-driven approach to relate the idiosyncratic sampling strategies and the electrophysiological responses across all electrodes independently, without any a-priori assumptions regarding the topography of a potential effect. To account for visual sampling idiosyncrasies, this computation was performed at the individual level. Our data show a strong positive relationship between idiosyncratic sampling strategies and neural face discrimination responses as recorded at different viewing positions, for <italic>all</italic> the observers. In particular, independently of the sampling strategy, the longer a viewing position was fixated under natural viewing conditions, the more likely this VP was to elicit the strongest neural face discrimination response when its fixation was enforced.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig. 2.</label>
<caption><p>FPVS paradigm and viewing positions. (A) Faces were presented at a frequency rate of 6Hz through sinusoidal contrast modulation. Base stimuli consisted of images of the same facial identity; interleaved oddball stimuli conveying different identities were presented every 7<sup>t</sup><sup>h</sup> base stimulus. (B) Illustration of the 10 viewing positions (VPs) fixated by participants. (C) Examples of two trials displaying fixation on the left eye (VP1, top row), or mouth (VP8, bottom row).</p></caption>
<graphic xlink:href="402263_fig2.tif"/>
</fig>
</sec>
<sec id="s2">
<title>METHODS</title>
<sec id="s2a">
<title>Participants</title>
<p>The sample size opted for was motivated by studies using the same FPVS paradigm to index neural face discrimination that were published up to data acquisition (<xref ref-type="bibr" rid="c11">Dzhelyova &#x0026; Rossion, 2014a</xref>, <xref ref-type="bibr" rid="c12">2014b</xref>; <xref ref-type="bibr" rid="c18">Liu-Shuang et al., 2014</xref>; <xref ref-type="bibr" rid="c19">Liu-Shuang, Torfs, &#x0026; Rossion, 2016</xref>; sample size range: 8-12). In Dzhelyova and Rossion&#x2019;s (2014b) study using a within subject design, the observed minimal effect size resulting from a repeated ANOVA was .2 (partial-eta). As the effect size estimation is often overly optimistic in the literature, we planned our experiment based on an effect size of .1 and an estimated sample size of 15 participant which results in a power of .95 to detect an effect. Based on prior experience and the requirement of high quality data from independent methods, we chose to test a total number of 20 participants. Our cohort comprised 20 Western Caucasian observers (11 females, two left-handed, mean age: 25&#x00B1;3 years) with normal or corrected-to-normal vision and no history of psychiatric or neurological disorders. Three observers were excluded due to poor quality of the eye movement data. All participants provided written informed consent and received financial compensation for participation; all procedures were approved by the local ethics committee.</p>
</sec>
<sec id="s2b">
<title>Procedures</title>
</sec>
<sec id="s2c">
<title>Eye-tracking</title>
<sec id="s2c1">
<title>Stimuli and procedure</title>
<p>Stimuli consisted of 112 grey-scaled pictures portraying 56 Western Caucasians (i.e. WC) and 56 East Asians (i.e. EA) respectively obtained from the KDEF (<xref ref-type="bibr" rid="c20">Lundqvist, Flykt, &#x0026; &#x00D6;hman, 1998</xref>) and the AFID (<xref ref-type="bibr" rid="c3">Bang, Kim, &#x0026; Choi, 2001</xref>). Faces were presented at a viewing distance of 75 cm and subtended 12.56&#x00B0; (height from chin to hairline) &#x00D7; 9.72&#x00B0; (width) of visual angle on a VIEWPIxx/3D monitor (1920 &#x00D7; 1080 pixel resolution, 120 Hz refresh rate).</p>
<p>Participants performed an Old-New face recognition task (<xref ref-type="bibr" rid="c5">Blais et al., 2008</xref>), with two blocks, each comprising a learning and a recognition phase of either WC and EA faces. In each learning phase participants were presented with 14 identities (7 female) with a neutral, happy or disgust expression; the recognition phase involved the presentation of these encoded identities alongside of 14 new ones, with a change of facial expression for the learned identities in order to prevent for face image matching strategies instead of genuine face recognition. Participants were required to indicate via button press whether a stimulus had been previously seen or not. During the learning phase, the faces were presented for five seconds; during the recognition phase presentation was terminated upon participants&#x2019; responses. The eye movements were recorded during both the learning and recognition phases.</p>
</sec>
<sec id="s2c2">
<title>Data acquisition and processing</title>
<p>The oculomotor behavior was recorded for each participant using an EyeLink 1000 Desktop Mount with a temporal resolution of 1000 Hz. The raw data are available in the public domain (<xref ref-type="bibr" rid="c31">Stacchi, Ramon, Lao, &#x0026; Caldara, 2018</xref>). Data were registered by using the Psychophysics (<xref ref-type="bibr" rid="c6">Brainard, 1997</xref>) and the EyeLink (<xref ref-type="bibr" rid="c8">Cornelissen, Peters, &#x0026; Palmer, 2002</xref>) Toolbox running in a MatlabR2013b environment. Calibrations and validations were performed at the beginning of the experiment using a nine-point fixation procedure. Additionally, before each trial a fixation cross appeared in the center of the screen and participants were instructed to fixate on it until a new stimulus appeared to ensure eye movements were correctly tracked. A new calibration was performed if the eye drift exceeded 1&#x00B0; of visual angle.</p>
<p>After removing eye blinks and saccades using the algorithm developed by Nystrom et al. (<xref ref-type="bibr" rid="c27">Nystr&#x00F6;m &#x0026; Holmqvist, 2010</xref>), observers&#x2019; eye movement data from the learning phases of the Old-New task were processed to create individual fixation maps. Previous studies have shown that with this paradigm there are no differences in the sampling strategies used to samples WC or EA faces (<xref ref-type="bibr" rid="c5">Blais et al., 2008</xref>; <xref ref-type="bibr" rid="c7">Caldara, 2017</xref>). Therefore, in order to increase the signal-to-noise ratio, fixation maps were extracted from all face presentations. Individuals&#x2019; fixation intensities (based on the cumulative fixation duration) per observers were derived using these fixation maps and pre-defined circular regions of interest (ROIs; see <xref rid="fig1" ref-type="fig">Fig. 1</xref>). The ROIs covered 1.8&#x00B0; of visual angle and were centered on the ten viewing positions fixated during the FPVS experiment.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig. 1.</label>
<caption><p>Illustration of the regions of interest (ROI) surrounding the 10 viewing positions. Observers&#x2019; fixation maps were overlaid onto a ROI mask to compute the fixation intensity per ROI. The ROI were covering 1.8&#x00B0; of visual angle and were centered on 9 equidistant viewing positions (red circles) and on an additional VP corresponding to the center of the stimulus (black circle).</p></caption>
<graphic xlink:href="402263_fig1.tif"/>
</fig>
</sec>
</sec>
<sec id="s2d">
<title>EEG</title>
<sec id="s2d1">
<title>Stimuli and procedure</title>
<p>We used full-front, color images of 50 identities (25 female) from the same set described previously (<xref ref-type="bibr" rid="c18">Liu-Shuang et al., 2014</xref>). All faces conveyed a neutral expression, were cropped to exclude external facial features, and were presented against a grey background. Each original stimulus subtended 11.02&#x00B0; (height) &#x00D7; 8.81&#x00B0; (width) of visual angle at a viewing distance of 70 cm.</p>
<p>Face stimuli were presented at a constant frequency of 6 Hz, with intervening oddball identities every 7<sup>th</sup> base (0.857 Hz) (<xref rid="fig2" ref-type="fig">Fig. 2A</xref>). The experiment comprised 20 trials: ten conditions (the viewing positions participants were required to fixate on; <xref rid="fig2" ref-type="fig">Fig. 2B</xref>), with two 62s trials per condition (trials differed with respect to the gender of the face stimuli). To prevent eye-movements, participants were instructed to maintain fixation on a central cross. The position of face stimuli was manipulated to vary, across trials, the fixated viewing position, hence the facial information. Faces were presented through sinusoidal contrast modulation (see <xref rid="fig2" ref-type="fig">Fig. 2A</xref>). Additionally, two seconds of gradual fade in and fade out were added at the beginning and end of each trial. To maintain subjects&#x2019; attention, the fixation cross briefly (200ms) changed color (red to blue) randomly between seven and eight times within each trial; participants were instructed to report the color change by button press. Subjects were also monitored trough a camera placed in front of them communicating the experimenter computer. Finally, to avoid pixel-wise overlap, stimulus size varied randomly between 80&#x0025; and 120&#x0025; of the original size (visual angle ranged between 8.82-13.22&#x00B0; (height) and 7.05-10.57&#x00B0; (width)).</p>
</sec>
<sec id="s2d2">
<title>Data acquisition and processing</title>
<p>Electrophysiological responses were recorded with Biosemi Active-Two amplifier system (Biosemi, Amsterdam, Netherlands) with 128 Ag/AgCl active electrodes and a sampling rate of 1024Hz. Additional electrodes placed at the outer canthi and below both eyes registered eye movements and blinks; electrode impedance was maintained between &#x00B1;25k&#x2126;. The recorded EEG was analyzed using Letswave 5 (<ext-link ext-link-type="uri" xlink:href="http://nocions.webnode.com/letswave">http://nocions.webnode.com/letswave</ext-link>; (<xref ref-type="bibr" rid="c24">Mouraux &#x0026; Iannetti, 2008</xref>)). The raw data are available in the public domain (<xref ref-type="bibr" rid="c31">Stacchi et al., 2018</xref>). Preprocessing consisted in high-and low-pass filtering the signal (with a 0.1Hz and 100Hz Butterworth band-pass filter (4<sup>th</sup> order). Data were subsequently downsampled to 256Hz and segmented according to condition resulting in 20 66-second epochs, which included two seconds before and after stimulation. Independent component analysis was performed on each participant&#x2019;s data to remove contamination due to eye-movements.</p>
<p>Noisy electrodes were interpolated using the three nearest spatially neighboring channels; this process was applied to no more than 5&#x0025; of all scalp electrodes. Segments were then re-referenced to a common average reference and cropped to an integer number of oddball cycles, excluding two seconds after stimulus onset and two seconds before stimulus offset (&#x223C;58-second epochs; 14932 bins). Epochs were then averaged separately for each subject per condition.</p>
</sec>
<sec id="s2d3">
<title>Frequency domain</title>
<p>Fast Fourier Transform (FFT) was applied to the averaged segments and amplitude was extracted. The data were baseline corrected by subtracting from each frequency&#x2019;s amplitude the average of its surrounding 20 bins excluding the two neighboring ones. Finally, for each subject and condition, the summed baseline-corrected amplitude of the oddball frequency and its significant harmonics provided the index of neural face discrimination. Following previous procedures (<xref ref-type="bibr" rid="c10">Dzhelyova, Jacques, &#x0026; Rossion, 2016</xref>), harmonics were considered significant until the mean z-score across all conditions was no longer above 1.64 (<italic>p</italic>&#x003C;.05). Based on this criterion we considered the first 11 harmonics excluding the 7<sup>th</sup> harmonic, which is confounded with the base stimulation frequency rate.</p>
</sec>
<sec id="s2d4">
<title>Analysis</title>
<p>Using the iMAP4 toolbox (<xref ref-type="bibr" rid="c17">Lao, Miellet, Pernet, Sokhn, &#x0026; Caldara, 2017</xref>) we computed a linear regression to explore the relationship between fixation bias (the z-scored fixation duration) and neural face discrimination (i.e., the FPVS response amplitude) within observer. To this aim we performed a linear mixed-effects model with random effect for intercept and <italic>Fixation duration</italic> grouped by subject. To avoid a-priori assumptions regarding topography of the effect, we regressed the two variable at all scalp electrodes independently.
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="402263_eqn1.gif"/></alternatives>
</disp-formula>
</p>
<p>This computation will determine whether, VP-dependent fixation duration are associated with the amplitude of the neural face discrimination response elicited by each VP, for each subject independently. Importantly, because the analysis is performed at the individual level, there is no a priori expectation on how VPs are ranked. We opted for this approach in light of individual differences in fixation patterns reported previously (<xref ref-type="bibr" rid="c21">Mehoudar et al., 2014</xref>; Arizpe et al., 2016; <xref ref-type="bibr" rid="c16">Kanan et al., 2015</xref>), and similar idiosyncrasies assumed to exist for neural face discrimination responses across VPs. Therefore, the model used here allows each subject to have his/her specific VP-pattern and a relationship emerges if the fixation pattern is predictive of the neural response pattern of the same subject. Finally, as the current work does only focus on individual subjects, we did not perform any analysis involving average fixation maps and average EEG responses. The processed data used to compute this analysis are available in the public domain (<xref ref-type="bibr" rid="c31">Stacchi et al., 2018</xref>)</p>
</sec>
</sec>
</sec>
<sec id="s3">
<title>RESULTS</title>
<sec id="s3a">
<title>Behavior</title>
<p>As expected, subjects&#x2019; performance in the Old-New task, as indexed by d&#x2019;, was significantly better for Western Caucasian (M=1.62, SD=.64) than East Asian faces (M=0.97, SD=.60), t(16)=5.72, <italic>p</italic>&#x003C;.01. Subjects&#x2019; performance was nearly at ceiling for the FPVS orthogonal task consisting in detecting the color-change of the fixation-cross (M=.95, SD=.11). Due to technical issues, one subject&#x2019;s behavioral response at this orthogonal task was not recorded. However, as for all subjects, behavior was monitored trough a webcam.</p>
</sec>
<sec id="s3b">
<title>Eye-movements and FPVS response</title>
<sec id="s3b1">
<title>Description of fixation and neural biases at the group and individual level</title>
<p>The average fixation maps (computed for descriptive purposes and) shown in <xref rid="fig3" ref-type="fig">Figure 3A</xref> demonstrate that <italic>as a group</italic> observers preferentially sampled facial information encompassing the eyes, nasion, nose and mouth. However, because the focus of this work was to investigate the relationship between fixation patterns and neural responses at the individual level, group data were not subject to any further analysis.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig. 3.</label>
<caption><p>Fixation maps and oddball responses. A and C show the grand-average fixation map and FPVS responses respectively, while B and D show the two measures for the same subjects. For illustration, only four subjects are reported.</p></caption>
<graphic xlink:href="402263_fig3.tif"/>
</fig>
<p>At the individual level, the majority of individual observers&#x2019; fixation maps did not <italic>perfectly</italic> conform to the grand average fixation pattern (<xref rid="fig3" ref-type="fig">Figure 3A-B</xref> &#x2013; see also <xref rid="figS1" ref-type="fig">supplementary Figure S1</xref>), clearly demonstrating the existence of idiosyncratic visual sampling strategies (see <xref rid="figS1" ref-type="fig">Figure S1</xref> for all individual observers&#x2019; fixation maps). Mirroring these results, the grand average neural face discrimination response amplitudes varied as a function of VPs, with the greatest amplitudes for the central position (<xref rid="fig3" ref-type="fig">Figure 3C</xref>). However, the neural response amplitudes also markedly differed across individuals (<xref rid="fig3" ref-type="fig">Figure 3D</xref>).</p>
</sec>
<sec id="s3b2">
<title>Regression analysis: Assessing the relationship between fixation and neural biases at the individual level</title>
<p>The data-driven regression between individuals&#x2019; fixation durations and FPVS responses across VPs computed independently on all electrodes revealed a positive relationship at right occipito-temporal (OT) and central-parietal (CP) clusters (see <xref rid="fig4" ref-type="fig">Figure 4A</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig. 4.</label>
<caption><p>The relationship between fixation duration and neural face discrimination responses across VPs observed across all subjects considered individually. (A) Regression F-values (left) and beta maps (right). Original maps were overlaid with a mask and only electrodes exhibiting a significant effect (p&#x003C; 7.81e-05) are shown. (B) On the left, the scatterplot illustrates individual subjects&#x2019; (light grey lines) as wells as the group (black line) effect averaged across the significant occipito-temporal cluster of electrodes. Crucially, the ranking of fixation and neural bias for VPs differed across subjects. On the right, data of two subjects illustrate that the relationship between fixation and neural bias emerges irrespectively of the fixation pattern exhibited (i.e., left-eye for S01 and mouth for S16). Note that here individual subjects&#x2019; correlations are displayed (black line).</p></caption>
<graphic xlink:href="402263_fig4.tif"/>
</fig>
<p>The occipito-temporal cluster includes 12 significant electrodes with the strongest effect at A28 (F(1,169)=30.02, &#x00DF;=.22 [.14 .30], <italic>p</italic>=1.54e-07) and the smallest at A13 (F(1,169)=17.18 &#x00DF;=.20 [.10 .29], <italic>p=</italic>5.37e-05) (<xref rid="tbl1" ref-type="table">Table S1</xref>). The average across the electrodes in the OT significant cluster shows that individual subjects exhibited variable intercepts but similar slopes (see <xref rid="fig4" ref-type="fig">Fig. 4B</xref>). Despite inter-individual variations in the neural face discrimination response amplitude and fixation durations, the relationship was present within <italic>all</italic> the observers (see <xref rid="fig5" ref-type="fig">Fig. 5</xref>).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Fig. 5.</label>
<caption><p>The relationship between individual subjects&#x2019; fixation and neural bias across VPs. For each observer (S1-S17), the VP-dependent fixation duration (x-axis) is plotted against their neural face discrimination response amplitude (y-axis) along with their individual correlation (black line); VPs are color- and shape-coded as indicated in the legend. The subjects are ordered as a function of their correlation strength, from high (.227) to low (.219). Although observers exhibited idiosyncratic VP-dependent fixation durations, all showed the general pattern, that facial features fixated longer (i.e., VPs) elicited larger neural responses. Note that here the neural face discrimination response magnitude is displayed at the electrode showing the largest effect (i.e., A28).</p></caption>
<graphic xlink:href="402263_fig5.tif"/>
</fig>
<p>A small effect was also found on the central-parietal cluster comprising seven electrodes, with D14 showing the strongest effect (F(1,169)=26.12 &#x00DF;=.13 [.08 .17], <italic>p</italic>=9.64e-07) and C1 exhibiting the smallest effect (F(1,169)=17.78 &#x00DF;=.11 [.06 .17], <italic>p</italic>=4.05e-05) (<xref rid="fig4" ref-type="fig">Fig. 4A</xref>; <xref rid="tbl1" ref-type="table">Table S1</xref>).</p>
</sec>
<sec id="s3b3">
<title>Can <bold>specific</bold> fixation biases account for the observed relationship?</title>
<p>To assess whether subjects exhibiting a particular fixation bias (e.g., for the eyes) would show stronger correlations between fixation and neural biases, we first ranked observers&#x2019; fixation maps based on the magnitude of their individual relationship. As shown in <xref rid="fig6" ref-type="fig">Figure 6A</xref>, subjects showing similar fixation patterns could exhibit relationships of slightly different magnitude (e.g., left eye: S01 and S04), while observers exhibiting different fixation maps could show comparable correlation strengths (e.g., S10 and S13). Additionally, we computed the distance of each observer&#x2019;s fixation map from the average fixation pattern. In this case, each map is treated as a vector and the measure of interest is the cosine distance between each observers&#x2019; map and the average one. This produces a value ranging between 0 and 1 for each subject. The higher the distance the more dissimilar that given subject&#x2019;s pattern is from the average. Finally, we performed a Spearman correlation between this distance and the strength of the relationship between fixation and neural bias, which resulted to be non-significant (r=-.18, <italic>p</italic>=.49) (<xref rid="fig6" ref-type="fig">Figure 6B</xref>).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Fig. 6.</label>
<caption><p>Fixation maps and strength of the fixation-neural-bias relationship. (A) Observers&#x2019; fixation maps sorted as a function of the slope of observers&#x2019; relationship between fixation bias and neural face discrimination response amplitude. The slope is reported for the electrode showing the strongest effect (i.e., A28). (B) The scatterplot illustrates the lack of correlation between: the cosine distance of individuals&#x2019; fixation maps from the average fixation map (y-axis) and strength of the relationship between fixation and neural bias (x-axis). Data show there was not a particular fixation bias more likely to correlate with the neural bias.</p></caption>
<graphic xlink:href="402263_fig6.tif"/>
</fig>
</sec>
</sec>
</sec>
<sec id="s4">
<title>DISCUSSION</title>
<p>This study investigated the relationship between <italic>idiosyncratic</italic> visual sampling strategies for faces and the magnitude of neural face discrimination responses during fixation on different types of facial information. Our data show that visual information sampling is distinct across observers, and these differences are positively correlated with <italic>idiosyncratic</italic> neural responses predominantly at occipito-temporal electrodes. Specifically, the Viewing Positions (VPs) that elicited stronger neural face discrimination responses coincided with the VPs that were more fixated under free-viewing conditions. Altogether, our data show that face processing involves idiosyncratic coupling of <italic>distinct</italic> information sampling strategies and <italic>unique</italic> neural responses to the preferentially sampled facial information.</p>
<p>For many years, the accepted notion in vision research was that face processing elicits a unique and universal cascade of perceptual and cognitive events to process facial identity, with particular importance ascribed to information conveyed by the eye region. For instance, eye movement studies have revealed a bias towards sampling of the eye region (<xref ref-type="bibr" rid="c5">Blais et al., 2008</xref>), the diagnosticity of which has been further documented by psychophysical approaches (e.g., Bubbles) (<xref ref-type="bibr" rid="c13">Gosselin &#x0026; Schyns, 2001</xref>). Electrophysiological studies have also reported increased N170 magnitude during fixation on the eyes, compared to other information (<xref ref-type="bibr" rid="c9">de Lissa et al., 2014</xref>; <xref ref-type="bibr" rid="c25">Nemrodov et al., 2014</xref>). Collectively, these independent findings were taken to support the existence of a fixation and neural preference for the eye-region that is shared across <italic>all</italic> observers.</p>
<p>However, this idea has recently been challenged. For example, findings from eye movement studies emphasize idiosyncrasies in sampling preferences that are highly distinct from the group-average T-shaped pattern (<xref ref-type="bibr" rid="c2">Arizpe et al., 2017</xref>; <xref ref-type="bibr" rid="c21">Mehoudar et al., 2014</xref>), or by the existence of cultural differences (<xref ref-type="bibr" rid="c5">Blais et al., 2008</xref>; <xref ref-type="bibr" rid="c7">Caldara, 2017</xref>). These individual differences are not systematically associated with performance, as &#x201C;mouth lookers&#x201D; (i.e., observers showing preferential fixation on the mouth) could perform similarly to &#x201C;eyes lookers&#x201D;. Equally, two &#x201C;eyes lookers&#x201D; could exhibit very different performance (<xref ref-type="bibr" rid="c28">Peterson &#x0026; Eckstein, 2013</xref>). Nonetheless, each observer&#x2019;s adopted sampling strategy is optimal in the sense that performance is maximal when fixation is enforced on preferably sampled information, and decreases during fixation of other information (<xref ref-type="bibr" rid="c28">Peterson &#x0026; Eckstein, 2013</xref>). These results suggest that individual differences do not reflect random inter-subject variation, but rather subtend functional idiosyncrasies in face processing.</p>
<p>Our results replicate and extend these previous findings, by showing that idiosyncratic visual sampling strategies strikingly mirror individuals&#x2019; patterns of neural face discrimination responses across VPs. Specifically, the facial regions preferentially sampled during natural viewing were those eliciting stronger neural face discrimination responses when fixated. This pattern was present in all observers, with even some of them (n=4) showing a perfect match between the most fixated facial feature and the one eliciting the strongest neural response at the electrode showing the strongest statistical relationship.</p>
<p>The strong and striking relationship between information sampling and neural idiosyncrasies suggests a functionally relevant process. Eye movements feed the neural face system with the diagnostic information in order to optimize information processing. The eyes constantly move to center elements of interest in the fovea, where visual acuity is greatest. This critical functional role, coupled with the relationship reported here between idiosyncratic sampling strategies and the neural face discrimination response pattern thus leads to two main considerations. First, our data show that face identity processing involves a fine-tuned interplay between oculomotor mechanisms and face-sensitive neural network. Second, the diagnosticity associated different types of facial information varies across observers. For a long time, researchers have debated on the nature of face representations, mainly opposing the idea of faces being represented as indivisible wholes (holistic or configural), as opposed to a collection of multiple, distinctively perceivable features (featural). This ongoing debate cannot be settled based on our finding of visual and neural idiosyncrasies. These idiosyncrasies do, however, refute the concept of a <italic>single</italic> face representation format shared across observers.</p>
<p>Our observations raise further important methodological and theoretical questions. The first concerns the traditional approach of standardizing the visual input to allow comparability across observers. This inherently creates a bias as not all observers are comparably tuned to this conventional VP. Additional open questions concern for instance (a) the extent to which the relationship between the visual sampling strategies and neural response patterns is <italic>task</italic>-specific, and (b) the direction of this relationship. Future studies are also necessary to establish whether similar effects can be found for scene, object and word processing. Finally, our approach may offer a promising novel route in clinical settings, if disorders comprising face processing impairments (i.e., prosopagnosia, autism, schizophrenia, etc.) involved an abnormal relationship between fixation patterns and neural responses to faces.</p>
</sec>
<sec id="s5">
<title>CONCLUSION</title>
<p>When processing faces, observers deploy idiosyncratic sampling strategies by preferentially moving their eyes towards specific features. To assess the neural correlates underlying these idiosyncrasies, we recorded eye movements and neural face discrimination responses by means of FPVS. Our data show that idiosyncratic sampling strategies are mirrored by neural preferences, as facial information that is sampled longer during free viewing elicits stronger neural face discrimination responses when fixation is enforced on this feature. These findings show that eye movements and neural responses are finely tuned towards the facial information that is optimal for <italic>individual observers.</italic> The biologically relevant feat of processing facial identity is achieved by the use of idiosyncratic visual and neural representations.</p>
</sec>
</body>
<back>
<ref-list>
<title>REFERENCES</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Ales</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Farzin</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Rossion</surname>, <given-names>B.</given-names></string-name>, &#x0026; <string-name><surname>Norcia</surname>, <given-names>A. M.</given-names></string-name> (<year>2012</year>). <article-title>An objective method for measuring face detection thresholds using the sweep steady-state visual evoked response</article-title>. <source>Journal of Vision</source>, <volume>12</volume>(<issue>10</issue>), <fpage>18</fpage>&#x2013;<lpage>18</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/12.10.18">https://doi.org/10.1167/12.10.18</ext-link></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Arizpe</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Walsh</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Yovel</surname>, <given-names>G.</given-names></string-name>, &#x0026; <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name> (<year>2017</year>). <article-title>The categories, frequencies, and stability of idiosyncratic eye-movement patterns to faces</article-title>. <source>Vision Research</source>, <volume>141</volume>, <fpage>191</fpage>&#x2013;<lpage>203</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2016.10.013">https://doi.org/10.1016/j.visres.2016.10.013</ext-link></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="book"><string-name><surname>Bang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kim</surname>, <given-names>D.</given-names></string-name>, &#x0026; <string-name><surname>Choi</surname>, <given-names>S.</given-names></string-name> (<year>2001</year>). <chapter-title>Asian Face Image Database</chapter-title>. <source>In Lab IM</source>. <publisher-name>Postech</publisher-name>, <publisher-loc>Korea</publisher-loc>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Bentin</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Allison</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Puce</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Perez</surname>, <given-names>E.</given-names></string-name>, &#x0026; <string-name><surname>McCarthy</surname>, <given-names>G.</given-names></string-name> (<year>1996</year>). <article-title>Electrophysiological Studies of Face Perception in Humans</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>8</volume>(<issue>6</issue>), <fpage>551</fpage>&#x2013;<lpage>565</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1162/jocn.1996.8.6.551">https://doi.org/10.1162/jocn.1996.8.6.551</ext-link></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Blais</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Jack</surname>, <given-names>R. E.</given-names></string-name>, <string-name><surname>Scheepers</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Fiset</surname>, <given-names>D.</given-names></string-name>, &#x0026; <string-name><surname>Caldara</surname>, <given-names>R.</given-names></string-name> (<year>2008</year>). <article-title>Culture Shapes How We Look at Faces</article-title>. <source>PLoS ONE</source>, <volume>3</volume>(<issue>8</issue>), <fpage>e3022</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1371/journal.pone.0003022">https://doi.org/10.1371/journal.pone.0003022</ext-link></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Brainard</surname>, <given-names>D. H.</given-names></string-name> (<year>1997</year>). <article-title>The Psychophysics Toolbox</article-title>. <source>Spatial Vision</source>, (<issue>10</issue>), <fpage>433</fpage>&#x2013;<lpage>436</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Caldara</surname>, <given-names>R.</given-names></string-name> (<year>2017</year>). <article-title>Culture Reveals a Flexible System for Face Processing</article-title>. <source>Current Directions in Psychological Science</source>, <volume>26</volume>(<issue>3</issue>), <fpage>249</fpage>&#x2013;<lpage>255</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0963721417710036">https://doi.org/10.1177/0963721417710036</ext-link></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="other"><string-name><surname>Cornelissen</surname>, <given-names>F. W.</given-names></string-name>, <string-name><surname>Peters</surname>, <given-names>E. M.</given-names></string-name>, &#x0026; <string-name><surname>Palmer</surname>, <given-names>J.</given-names></string-name> (<year>2002</year>). <article-title>The Eyelink Toolbox: Eye tracking with MATLAB and the Psychophysics Toolbox</article-title>. <source>Behavior Research Methods, Instruments, and Computers</source>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>de Lissa</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>McArthur</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Hawelka</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Palermo</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Mahajan</surname>, <given-names>Y.</given-names></string-name>, &#x0026; <string-name><surname>Hutzler</surname>, <given-names>F.</given-names></string-name> (<year>2014</year>). <article-title>Fixation location on upright and inverted faces modulates the N170</article-title>. <source>Neuropsychologia</source>, <volume>57</volume>, <fpage>1</fpage>&#x2013;<lpage>11</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuropsychologia.2014.02.006">https://doi.org/10.1016/j.neuropsychologia.2014.02.006</ext-link></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Dzhelyova</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Jacques</surname>, <given-names>C.</given-names></string-name>, &#x0026; <string-name><surname>Rossion</surname>, <given-names>B.</given-names></string-name> (<year>2016</year>). <article-title>At a Single Glance: Fast Periodic Visual Stimulation Uncovers the Spatio-Temporal Dynamics of Brief Facial Expression Changes in the Human Brain</article-title>. <source>Cerebral Cortex</source>, <volume>27</volume>(<issue>8</issue>), <fpage>4106</fpage>&#x2013;<lpage>4123</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhw223">https://doi.org/10.1093/cercor/bhw223</ext-link></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Dzhelyova</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Rossion</surname>, <given-names>B.</given-names></string-name> (<year>2014a</year>). <article-title>Supra-additive contribution of shape and surface information to individual face discrimination as revealed by fast periodic visual stimulation</article-title>. <source>Journal of Vision</source>, <volume>14</volume>, <fpage>15</fpage>&#x2013;<lpage>15</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/14.14.15">https://doi.org/10.1167/14.14.15</ext-link></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Dzhelyova</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Rossion</surname>, <given-names>B.</given-names></string-name> (<year>2014b</year>). <article-title>The effect of parametric stimulus size variation on individual face discrimination indexed by fast periodic visual stimulation</article-title>. <source>BMC Neuroscience</source>, <volume>15</volume>(<issue>1</issue>), <fpage>87</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1186/1471-2202-15-87">https://doi.org/10.1186/1471-2202-15-87</ext-link></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Gosselin</surname>, <given-names>F.</given-names></string-name>, &#x0026; <string-name><surname>Schyns</surname>, <given-names>P. G.</given-names></string-name> (<year>2001</year>). <article-title>Bubbles: a technique to reveal the use of information in recognition tasks</article-title>. <source>Vision Research</source>, <volume>41</volume>(<issue>17</issue>), <fpage>2261</fpage>&#x2013;<lpage>2271</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/S0042-6989(01)00097-9">https://doi.org/10.1016/S0042-6989(01)00097-9</ext-link></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Henderson</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>C. C.</given-names></string-name>, &#x0026; <string-name><surname>Falk</surname>, <given-names>R. J.</given-names></string-name> (<year>2005</year>). <article-title>Eye movements are functional during face learning</article-title>. <source>Memory &#x0026; Cognition</source>, <volume>33</volume>(<issue>1</issue>), <fpage>98</fpage>&#x2013;<lpage>106</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Itier</surname>, <given-names>R. J.</given-names></string-name>, <string-name><surname>Latinus</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Taylor</surname>, <given-names>M. J.</given-names></string-name> (<year>2006</year>). <article-title>Face, eye and object early processing: What is the face specificity?</article-title> <source>NeuroImage</source>, <volume>29</volume>(<issue>2</issue>), <fpage>667</fpage>&#x2013;<lpage>676</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2005.07.041">https://doi.org/10.1016/j.neuroimage.2005.07.041</ext-link></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Kanan</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Bseiso</surname>, <given-names>D. N. F.</given-names></string-name>, <string-name><surname>Ray</surname>, <given-names>N. A.</given-names></string-name>, <string-name><surname>Hsiao</surname>, <given-names>J. H.</given-names></string-name>, &#x0026; <string-name><surname>Cottrell</surname>, <given-names>G. W.</given-names></string-name> (<year>2015</year>). <article-title>Humans have idiosyncratic and task-specific scanpaths for judging faces</article-title>. <source>Vision Research</source>, <volume>108</volume>, <fpage>67</fpage>&#x2013;<lpage>76</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.visres.2015.01.013">https://doi.org/10.1016/j.visres.2015.01.013</ext-link></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Lao</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Miellet</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pernet</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Sokhn</surname>, <given-names>N.</given-names></string-name>, &#x0026; <string-name><surname>Caldara</surname>, <given-names>R.</given-names></string-name> (<year>2017</year>). <article-title>iMap4: An open source toolbox for the statistical fixation mapping of eye movement data with linear mixed modeling</article-title>. <source>Behavior Research Methods</source>, <volume>49</volume>(<issue>2</issue>), <fpage>559</fpage>&#x2013;<lpage>575</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Liu-Shuang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Norcia</surname>, <given-names>A. M.</given-names></string-name>, &#x0026; <string-name><surname>Rossion</surname>, <given-names>B.</given-names></string-name> (<year>2014</year>). <article-title>An objective index of individual face discrimination in the right occipito-temporal cortex by means of fast periodic oddball stimulation</article-title>. <source>Neuropsychologia</source>, <volume>52</volume>, <fpage>57</fpage>&#x2013;<lpage>72</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuropsychologia.2013.10.022">https://doi.org/10.1016/j.neuropsychologia.2013.10.022</ext-link></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Liu-Shuang</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Torfs</surname>, <given-names>K.</given-names></string-name>, &#x0026; <string-name><surname>Rossion</surname>, <given-names>B.</given-names></string-name> (<year>2016</year>). <article-title>An objective electrophysiological marker of face individualisation impairment in acquired prosopagnosia with fast periodic visual stimulation</article-title>. <source>Neuropsychologia</source>, <volume>83</volume>, <fpage>100</fpage>&#x2013;<lpage>113</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuropsychologia.2015.08.023">https://doi.org/10.1016/j.neuropsychologia.2015.08.023</ext-link></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="book"><string-name><surname>Lundqvist</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Flykt</surname>, <given-names>A.</given-names></string-name>, &#x0026; <string-name><surname>&#x00D6;hman</surname>, <given-names>A.</given-names></string-name> (<year>1998</year>). <source>The Karolinska directed emotional faces</source>. <publisher-loc>Stockholm, Sweden</publisher-loc>: <publisher-name>Karolinska Institute. Stockholm. Sweden: Karolinska Institute.: Stockholm. Sweden: Karolinska Institute</publisher-name>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Mehoudar</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Arizpe</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Baker</surname>, <given-names>C. I.</given-names></string-name>, &#x0026; <string-name><surname>Yovel</surname>, <given-names>G.</given-names></string-name> (<year>2014</year>). <article-title>Faces in the eye of the beholder: Unique and stable eye scanning patterns of individual observers</article-title>. <source>Journal of Vision</source>, <volume>14</volume>(<issue>7</issue>), <fpage>6</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/14.7.6">https://doi.org/10.1167/14.7.6</ext-link></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Miellet</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Caldara</surname>, <given-names>R.</given-names></string-name>, &#x0026; <string-name><surname>Schyns</surname>, <given-names>P. G.</given-names></string-name> (<year>2011</year>). <article-title>Local Jekyll and Global Hyde</article-title>. <source>Psychological Science</source>, <volume>22</volume>(<issue>12</issue>), <fpage>1518</fpage>&#x2013;<lpage>1526</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0956797611424290">https://doi.org/10.1177/0956797611424290</ext-link></mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Miellet</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Vizioli</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>He</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Zhou</surname>, <given-names>X.</given-names></string-name>, &#x0026; <string-name><surname>Caldara</surname>, <given-names>R.</given-names></string-name> (<year>2013</year>). <article-title>Mapping Face Recognition Information Use across Cultures</article-title>. <source>Frontiers in Psychology</source>, <volume>4</volume>, <fpage>34</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3389/fpsyg.2013.00034">https://doi.org/10.3389/fpsyg.2013.00034</ext-link></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Mouraux</surname>, <given-names>A.</given-names></string-name>, &#x0026; <string-name><surname>Iannetti</surname>, <given-names>G. D.</given-names></string-name> (<year>2008</year>). <article-title>Across-trial averaging of event-related EEG responses and beyond</article-title>. <source>Magnetic Resonance Imaging</source>, <volume>26</volume>(<issue>7</issue>), <fpage>1041</fpage>&#x2013;<lpage>1054</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.mri.2008.01.011">https://doi.org/10.1016/j.mri.2008.01.011</ext-link></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Nemrodov</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Anderson</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Preston</surname>, <given-names>F. F.</given-names></string-name>, &#x0026; <string-name><surname>Itier</surname>, <given-names>R. J.</given-names></string-name> (<year>2014</year>). <article-title>Early sensitivity for eyes within faces: A new neuronal account of holistic and featural processing</article-title>. <source>NeuroImage</source>, <volume>97</volume>, <fpage>81</fpage>&#x2013;<lpage>94</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2014.04.042">https://doi.org/10.1016/j.neuroimage.2014.04.042</ext-link></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Norcia</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Appelbaum</surname>, <given-names>L. G.</given-names></string-name>, <string-name><surname>Ales</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Cottereau</surname>, <given-names>B. R.</given-names></string-name>, &#x0026; <string-name><surname>Rossion</surname>, <given-names>B.</given-names></string-name> (<year>2015</year>). <article-title>The steady-state visual evoked potential in vision research: A review</article-title>. <source>Journal of Vision</source>, <volume>15</volume>(<issue>6</issue>), <fpage>4</fpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/15.6.4">https://doi.org/10.1167/15.6.4</ext-link></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Nystr&#x00F6;m</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Holmqvist</surname>, <given-names>K.</given-names></string-name> (<year>2010</year>). <article-title>An adaptive algorithm for fixation, saccade, and glissade detection in eyetracking data</article-title>. <source>Behavior Research Methods</source>, <volume>42</volume>(<issue>1</issue>), <fpage>188</fpage>&#x2013;<lpage>204</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.3758/BRM.42.1.188">https://doi.org/10.3758/BRM.42.1.188</ext-link></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Peterson</surname>, <given-names>M. F.</given-names></string-name>, &#x0026; <string-name><surname>Eckstein</surname>, <given-names>M. P.</given-names></string-name> (<year>2013</year>). <article-title>Individual Differences in Eye Movements During Face Identification Reflect Observer-Specific Optimal Points of Fixation</article-title>. <source>Psychological Science</source>, <volume>24</volume>(<issue>7</issue>), <fpage>1216</fpage>&#x2013;<lpage>1225</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0956797612471684">https://doi.org/10.1177/0956797612471684</ext-link></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Rossion</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Torfs</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Jacques</surname>, <given-names>C.</given-names></string-name>, &#x0026; <string-name><surname>Liu-Shuang</surname>, <given-names>J.</given-names></string-name> (<year>2015</year>). <article-title>Fast periodic presentation of natural images reveals a robust face-selective electrophysiological response in the human brain</article-title>. <source>Journal of Vision</source>, <volume>15</volume>(<issue>1</issue>), <fpage>18</fpage>&#x2013;<lpage>18</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/15.1.18">https://doi.org/10.1167/15.1.18</ext-link></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Rousselet</surname>, <given-names>G. A.</given-names></string-name>, <string-name><surname>Ince</surname>, <given-names>R. A. A.</given-names></string-name>, <string-name><surname>van Rijsbergen</surname>, <given-names>N. J.</given-names></string-name>, &#x0026; <string-name><surname>Schyns</surname>, <given-names>P. G.</given-names></string-name> (<year>2014</year>). <article-title>Eye coding mechanisms in early human face event-related potentials</article-title>. <source>Journal of Vision</source>, <volume>14</volume>(<issue>13</issue>), <fpage>7</fpage>&#x2013;<lpage>7</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1167/14.13.7">https://doi.org/10.1167/14.13.7</ext-link></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="other"><string-name><surname>Stacchi</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Ramon</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Lao</surname>, <given-names>J.</given-names></string-name>, &#x0026; <string-name><surname>Caldara</surname>, <given-names>R.</given-names></string-name> (<year>2018</year>). <source>Data from: Neural representations of faces are tuned to eye movements. Dryad Digital Repository</source>. <ext-link ext-link-type="uri" xlink:href="https://datadryad.org/review?doi=DOI:10.5061/dryad.89c5k58">https://datadryad.org/review?doi=DOI:10.5061/dryad.89c5k58</ext-link></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Walker-Smith</surname>, <given-names>G. J.</given-names></string-name>, <string-name><surname>Gale</surname>, <given-names>A. G.</given-names></string-name>, &#x0026; <string-name><surname>Findlay</surname>, <given-names>J. M.</given-names></string-name> (<year>1977</year>). <article-title>Eye Movement Strategies Involved in Face Perception</article-title>. <source>Perception</source>, <volume>6</volume>(<issue>3</issue>), <fpage>313</fpage>&#x2013;<lpage>326</lpage>. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1068/p060313">https://doi.org/10.1068/p060313</ext-link></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="book"><string-name><surname>Yarbus</surname>, <given-names>A. L.</given-names></string-name> (<year>1967</year>). <chapter-title>Eye Movements During Perception of Complex Objects</chapter-title>. <source>In Eye Movements and Vision</source> (pp. <fpage>171</fpage>&#x2013;<lpage>211</lpage>). <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Springer US</publisher-name>.</mixed-citation></ref>
</ref-list>
<sec id="s6">
<title>SUPPLEMENTAL INFORMATION</title>
<fig id="figS1" position="float" fig-type="figure">
<label>Fig. S1.</label>
<caption><p>Fixation maps and neural face discrimination responses of all subjects.</p></caption>
<graphic xlink:href="402263_figS1.tif"/>
</fig>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table S1.</label>
<caption><p>F, beta and p-values for electrodes showing a significant effect.</p></caption>
<graphic xlink:href="402263_tbl1.tif"/>
</table-wrap>
</sec>
</back>
</article>