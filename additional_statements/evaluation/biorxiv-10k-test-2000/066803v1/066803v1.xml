<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/066803</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Scientific Communication and Education</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A statistical definition for reproducibility and replicability</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Patil</surname>
<given-names>Prasad</given-names>
</name>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Peng</surname>
<given-names>Roger D.</given-names>
</name>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Leek</surname>
<given-names>Jeffrey T.</given-names>
</name>
</contrib>
</contrib-group>
<pub-date pub-type="epub"><year>2016</year></pub-date>
<elocation-id>066803</elocation-id>
<history>
<date date-type="received">
<day>29</day>
<month>7</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>29</day>
<month>7</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2016, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2016</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="066803.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Everyone agrees that reproducibility and replicability are fundamental characteristics of scientific studies. These topics are attracting increasing attention, scrutiny, and debate both in the popular press and the scientific literature. But there are no formal statistical definitions for these concepts, which leads to confusion since the same words are used for different concepts by different people in different fields. We provide formal and informal definitions of scientific studies, reproducibility, and replicability that can be used to clarify discussions around these concepts in the scientific and popular press.</p>
</abstract>
<counts>
<page-count count="6"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Main text</title>
<p>Reproducibility and replicability are at the center of heated debates in psychology (<xref ref-type="bibr" rid="c1"><italic>1</italic></xref>) genomics (<xref ref-type="bibr" rid="c2"><italic>2</italic></xref>), climate change (<xref ref-type="bibr" rid="c3"><italic>3</italic></xref>), economics (<xref ref-type="bibr" rid="c4"><italic>4</italic></xref>), and medicine (<xref ref-type="bibr" rid="c5"><italic>5</italic></xref>). The conversation about reproducibility and replicability of science has spilled over into the popular press (<xref ref-type="bibr" rid="c6"><italic>6</italic></xref>, <xref ref-type="bibr" rid="c7"><italic>7</italic></xref>) with major political consequences including new governmental policies and initiatives (<xref ref-type="bibr" rid="c8"><italic>8</italic></xref>) focusing on the reliability of science. These studies have generated responses (<xref ref-type="bibr" rid="c9"><italic>9</italic></xref>), responses to responses (<xref ref-type="bibr" rid="c10"><italic>10</italic></xref>), and ongoing discussion (<xref ref-type="bibr" rid="c11"><italic>11</italic></xref>),(<xref ref-type="bibr" rid="c12"><italic>12</italic></xref>).</p>
<p>Everyone agrees that scientific studies should be reproducible and replicable. The problem is almost no one agrees upon what those terms mean. A major initiative in psychology used the term &#x201C;reproducibility&#x201D; to refer to completely re-doing experiments including data collection (<xref ref-type="bibr" rid="c1"><italic>1</italic></xref>). In cancer biology &#x201C;reproducibility&#x201D; has been used to refer to the re-calculation of results using a fixed set of data and code (<xref ref-type="bibr" rid="c13"><italic>13</italic></xref>). &#x201C;Replication&#x201D; is often used to refer to finding two independent studies that produce a result with similar levels of statistical significance in human genetics(<xref ref-type="bibr" rid="c14"><italic>14</italic></xref>), The same word has been used to refer to re-doing experiments (<xref ref-type="bibr" rid="c15"><italic>15</italic></xref>) and recreating results from fixed data and code(<xref ref-type="bibr" rid="c16"><italic>16</italic></xref>).</p>
<p>These disagreements in terminology seem purely semantic, but they have major scientific and political implications. The recent back-and-forth in the pages of <italic>Science</italic> mentioned above hinged critically on the definition of &#x201C;replication&#x201D; with disagreement between the authors about what those terms meant. The press, government officials, and even late night comedy hosts have pointed out &#x201C;irreproducibility&#x201D; - defined as the inability to re-create statistical results using fixed data and code - as the fundamental problem with the scientific process. But they use this term to encompass all of the more insidious problems of false discoveries, missed discoveries, scientific errors, and scientific misconduct (<xref ref-type="bibr" rid="c17"><italic>17</italic></xref>). Others have suggested conceptual frameworks to help define these terms (<xref ref-type="bibr" rid="c18"><italic>18</italic></xref>) but have stopped short of a statistical model - which is critical for being precise in discussions of often complicated terms.</p>
<p>To address this major difficulty we need a statistical framework for the scientific process. Typically statistical and machine learning models only formally define the outputs of the scientific process with random variables. The outcome is often designated by <bold>Y</bold> and the covariates of interest by <bold>X</bold>. This framework is limited because it does not allow us to model variation in what the scientists in question intended to study, how they performed the experiment, who performed the experiment, what data they collected, what analysis they intended to perform, who analyzed the data, and what analysis was actually performed. We have created a formal statistical model that includes terms for all of these components of the scientific process (see Supplemental Material). The key steps in this process can also be represented using a simple visual model (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><title>A graphic representation of the statistical model for the scientific process <italic>A</italic>.</title>
<p><italic>Reproducibility is defined as re-performing the same analysis with the same code using a different analyst; replicability is defined as re-performing the experiment and collecting new data. <bold>B</bold>. The paper that only 6 out of 53 pre-clinical studies replicated (<xref ref-type="bibr" rid="c5"><italic>5</italic></xref>) only reported a question and a claim, but not the rest of the scientific components of a study. <bold>C</bold>. The disagreement over the Reproducibility Project Psychology (<xref ref-type="bibr" rid="c9"><italic>9</italic></xref>, <xref ref-type="bibr" rid="c19"><italic>19</italic></xref>) is because a replication was not performed since the population changed. <bold>D</bold>. In the case of the controversy over genomic signatures for chemosensitivity (<xref ref-type="bibr" rid="c2"><italic>2</italic></xref>), reproducibility wasn&#x2019;t the main issue - the issue was that the original study did not have correct code and data.</italic></p></caption>
<graphic xlink:href="066803_fig1.tif"/>
</fig>
<p>Here we provide informal definitions for key scientific terms and provide detailed statistical definitions in the Using this modeling framework we provide formal definitions for the following terms (see Supplementary Material for formal statistical definitions and additional definitions):</p>
<sec id="s1a">
<title>A scientific study</title>
<p><italic>consists of document(s) specifying a population, question, hypothesis, experimental design, experimenter, data, analysis plan, analyst, code, parameter estimates, and claims about the parameter estimates.</italic></p>
</sec>
<sec id="s1b">
<title>Publication</title>
<p><italic>Making a public claim on the basis of a scientific study.</italic></p>
</sec>
<sec id="s1c">
<title>Reproducible</title>
<p><italic>Given a population, hypothesis, experimental design, experimenter, data, analysis plan, and code you get the same parameter estimates in a new analysis (<bold><xref ref-type="fig" rid="fig1">Figure 1A</xref></bold>).</italic></p>
</sec>
<sec id="s1d">
<title>Strongly replicable study</title>
<p><italic>Given a population, hypothesis, experimental design, analysis plan, and code you get consistent estimates when you recollect data and perform the analysis using the original code.</italic></p>
</sec>
<sec id="s1e">
<title>Replicable study</title>
<p><italic>Given a population, hypothesis, experimental design, and analysis plan you get consistent estimates when you recollect data and redo the analysis (<bold><xref ref-type="fig" rid="fig1"><xref ref-type="fig" rid="fig1">Figure 1A</xref></xref></bold>).</italic></p>
</sec>
<sec id="s1f">
<title>Strongly replicable claim</title>
<p><italic>Given a population, hypothesis, experimental design, analysis plan, and code, you make an equivalent claim based on the results of the study.</italic></p>
</sec>
<sec id="s1g">
<title>Replicable claim</title>
<p><italic>Given a population, hypothesis, experimental design, and analysis plan, you make an equivalent claim based on the results of the study.</italic></p>
</sec>
<sec id="s1h">
<title>Conceptually replicable</title>
<p><italic>A population and a question relate two hypotheses. A scientific study is performed for the first hypothesis and a claim is made. Then a scientific study is performed for a second hypothesis and a claim is made. The claims from the two studies provide consistent answers to the question.</italic></p>
</sec>
<sec id="s1i">
<title>False discovery</title>
<p><italic>The claim at the conclusion of a scientific study is not equal to the claim you would make if you could observe all data from the population given your hypothesis, experimental design, and analysis plan.</italic></p>
</sec>
<sec id="s1j">
<title>Garden of forking paths</title>
<p><italic>Given a population, hypothesis, experimental design, experimenter, data, analysis plan, and analyst, the code changes given the data you observe.</italic></p>
</sec>
<sec id="s1k">
<title>P-hacking</title>
<p><italic>Given a population, hypothesis, experimental design, experimenter, data, analysis plan, and analyst, the code changes to match a desired statement.</italic></p>
</sec>
<sec id="s1l">
<title>File drawer effect</title>
<p><italic>The probability of publication depends on the claim made at the conclusion of a scientific study.</italic></p>
<p>We can use our statistical framework to resolve arguments and misconceptions around some of the most controversial discussions of reproducibility and replicability. Consider the case of the claim that many of 53 pre-clinical studies were not replicable when performed by scientific teams at a pharmaceutical company (<xref ref-type="bibr" rid="c5"><italic>5</italic></xref>). Under our formal model, the paper describing this replication effort reported a hypothesis - that most studies do not replicate. It also reported a claim - that 47 out of the 53 studies could not be replicated by scientists at the company. However, the population, hypothesis, experimental design, experimenter, data, analysis plan, analysts, code, and estimates are not available (<bold><xref ref-type="fig" rid="fig1">Figure 1B</xref></bold>). This makes it clear that the published report is missing most of the components of a scientific study.</p>
<p>Later, three replication studies [(<xref ref-type="bibr" rid="c20"><italic>20</italic></xref>), (<xref ref-type="bibr" rid="c21"><italic>21</italic></xref>)/(<xref ref-type="bibr" rid="c22"><italic>22</italic></xref>), (<xref ref-type="bibr" rid="c23"><italic>23</italic></xref>)] were reported from the same pharmaceutical company - though it was unclear if they were part of the originally reported 53. It was pointed out that some of the reported studies included experiments with different populations - violating the definition of a replication. A similar issue was at the heart of a disagreement over several of the studies in the Reproducibility Project: Psychology (<xref ref-type="bibr" rid="c1"><italic>1</italic></xref>). In this project, 100 studies were replicated by independent investigators. In one case, a study originally performed in the United States on US college students was evaluated among a group of Italians. It was pointed out that this change in population violates the definition of replication (<xref ref-type="bibr" rid="c9"><italic>9</italic></xref>) - using our framework it is clear the reason is that the population changed (<bold><xref ref-type="fig" rid="fig1">Figure 1C</xref></bold>).</p>
<p>Finally consider one of the earliest and most egregious debates over reproducibility - the case of a predictor of chemosensitivity that ultimately fell apart - leading to a lawsuits, an Institute of Medicine conference and report, and ultimately the end of the lead author&#x2019;s scientific career (<xref ref-type="bibr" rid="c2"><italic>2</italic></xref>). In this case, both the code and the data produced by the original authors were made available; however, they were the wrong code and data. A team from MD Anderson was able to investigate and ultimately produce data and code that reproduced the original results (<bold><xref ref-type="fig" rid="fig1">Figure 1D</xref></bold>). Ultimately, the study was reproducible, which is surprising given the focus on this study being a violation of reproducibility. The problem with the study was not that the data and code could not be produced, it was that these items, when produced, were wrong (<xref ref-type="bibr" rid="c24"><italic>24</italic></xref>).</p>
<p>Our statistical framework can be used to address other issues that are central to the debate over the validity of science including p-hacking(<xref ref-type="bibr" rid="c25"><italic>25</italic></xref>), the garden of forking paths(<xref ref-type="bibr" rid="c26"><italic>26</italic></xref>), conceptual replication, and other scientific and data analytic problems that arise before the tidy data are processed at the end of the experiment. Using a proper and statistically rigorous framework for the scientific process we can help resolve arguments and provide a solid foundation for journal and public policy around these complicated issues.</p>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><given-names>O. S.</given-names> <surname>Collaboration</surname></string-name>, <article-title>Others, An open, large-scale, collaborative effort to estimate the reproducibility of psychological science</article-title>. <source>Perspect. Psychol. Sci.</source> <volume>7</volume>, <fpage>657</fpage>&#x2013;<lpage>660</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><given-names>K. A.</given-names> <surname>Baggerly</surname></string-name>, <string-name><given-names>K. R.</given-names> <surname>Coombes</surname></string-name>, <article-title>DERIVING CHEMOSENSITIVITY FROM CELL LINES: FORENSIC BIOINFORMATICS AND REPRODUCIBLE RESEARCH IN HIGH-THROUGHPUT BIOLOGY</article-title>. <source>Ann. Appl. Stat.</source> <volume>3</volume>, <fpage>1309</fpage>&#x2013;<lpage>1334</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="other"><string-name><given-names>R. E.</given-names> <surname>Benestad</surname></string-name> <etal>et al.</etal>, <article-title>Learning from mistakes in climate research</article-title>. <source>Theor. Appl. Climatol.</source>, <fpage>1</fpage>&#x2013;<lpage>5</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Herndon</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Ash</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Pollin</surname></string-name>, <article-title>Does high public debt consistently stifle economic growth? A critique of Reinhart and Rogoff</article-title>. <source>Cambridge J. Econ</source>. <volume>38</volume>, <fpage>257</fpage>&#x2013;<lpage>279</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><given-names>C. G.</given-names> <surname>Begley</surname></string-name>, <string-name><given-names>L. M.</given-names> <surname>Ellis</surname></string-name>, <article-title>Drug development: Raise standards for preclinical cancer research</article-title>. <source>Nature</source>. <volume>483</volume>, <fpage>531</fpage>&#x2013;<lpage>533</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="other"><string-name><given-names>T.</given-names> <surname>Economist</surname></string-name>, <article-title>Unreliable research</article-title>: <source>Trouble at the lab</source> (<year>2013</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="website"><string-name><given-names>J.</given-names> <surname>Van Bavel</surname></string-name>, <article-title>Why Do So Many Studies Fail to Replicate?</article-title> <source>The New York Times</source> (<year>2016</year>), (available at <ext-link ext-link-type="uri" xlink:href="http://www.nytimes.com/2016/05/29/opinion/sunday/why-do-so-many-studies-fail-to-replicate.html">http://www.nytimes.com/2016/05/29/opinion/sunday/why-do-so-many-studies-fail-to-replicate.html</ext-link>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="website"><collab>Netherlands starts major campaign against research misconduct</collab>, (available at <ext-link ext-link-type="uri" xlink:href="https://www.insidehighered.com/news/2016/06/23/netherlands-starts-major-campaign-against-research-misconduct">https://www.insidehighered.com/news/2016/06/23/netherlands-starts-major-campaign-against-research-misconduct</ext-link>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><given-names>D. T.</given-names> <surname>Gilbert</surname></string-name>, <string-name><given-names>G.</given-names> <surname>King</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Pettigrew</surname></string-name>, <string-name><given-names>T. D.</given-names> <surname>Wilson</surname></string-name>, <article-title>Comment on &#x201C;Estimating the reproducibility of psychological science.&#x201D;</article-title> <source>Science</source>. <volume>351</volume>, <fpage>1037</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><given-names>C. J.</given-names> <surname>Anderson</surname></string-name> <etal>et al.</etal>, <article-title>Response to Comment on &#x201C;Estimating the reproducibility of psychological science.&#x201D;</article-title> <source>Science</source>. <volume>351</volume>, <fpage>1037</fpage>&#x2013;<lpage>1037</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="website"><string-name><given-names>D. T.</given-names> <surname>Gilbert</surname></string-name>, <string-name><given-names>G.</given-names> <surname>King</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Pettigrew</surname></string-name>, <string-name><given-names>T. D.</given-names> <surname>Wilson</surname></string-name>, <article-title>A RESPONSE TO THE REPLY TO OUR TECHNICAL COMMENT ON &#x201C;ESTIMATING THE REPRODUCIBILITY OF PSYCHOLOGICAL SCIENCE&#x201D;</article-title> (<year>2016</year>), (available at <ext-link ext-link-type="uri" xlink:href="http://projects.iq.harvard.edu/files/psychology-replications/files/gkpw&#x005F;response&#x005F;to&#x005F;osc&#x005F;rebutal.pdf">http://projects.iq.harvard.edu/files/psychology-replications/files/gkpw&#x005F;response&#x005F;to&#x005F;osc&#x005F;rebutal.pdf</ext-link>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="website"><string-name><given-names>D. T.</given-names> <surname>Gilbert</surname></string-name>, <string-name><given-names>G.</given-names> <surname>King</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Pettigrew</surname></string-name>, <string-name><given-names>T. D.</given-names> <surname>Wilson</surname></string-name>, <article-title>More on &#x201C;Estimating the Reproducibility of Psychological Science.&#x201D;</article-title> Available at <ext-link ext-link-type="uri" xlink:href="http://projects.iq.harvard.edu/files/psychology-replications/files/gkpw&#x005F;post&#x005F;publication&#x005F;response.pdf">projects.iq.harvard.edu/files/psychology-replications/files/gkpw&#x005F;post&#x005F;publication&#x005F;response.pdf</ext-link> (<year>2016</year>) (available at <ext-link ext-link-type="uri" xlink:href="http://projects.iq.harvard.edu/files/psychology-replications/files/gkpw&#x005F;post&#x005F;publication&#x005F;response.pdf">http://projects.iq.harvard.edu/files/psychology-replications/files/gkpw&#x005F;post&#x005F;publication&#x005F;response.pdf</ext-link>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><given-names>R. D.</given-names> <surname>Peng</surname></string-name>, <article-title>Reproducible research in computational science</article-title>. <source>Science</source>. <volume>334</volume>, <fpage>1226</fpage>&#x2013;<lpage>1227</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><given-names>E.</given-names> <surname>Zeggini</surname></string-name> <etal>et al.</etal>, <article-title>Meta-analysis of genome-wide association data and large-scale replication identifies additional susceptibility loci for type 2 diabetes</article-title>. <source>Nat. Genet</source>. <volume>40</volume>, <fpage>638</fpage>&#x2013;<lpage>645</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Birmaher</surname></string-name> <etal>et al.</etal>, <article-title>Psychometric properties of the Screen for Child Anxiety Related Emotional Disorders (SCARED): a replication study</article-title>. <source>J. Am. Acad. Child Adolesc. Psychiatry</source>. <volume>38</volume>, <fpage>1230</fpage>&#x2013;<lpage>1236</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><given-names>J. P. A.</given-names> <surname>Ioannidis</surname></string-name> <etal>et al.</etal>, <article-title>Repeatability of published microarray gene expression analyses</article-title>. <source>Nat. Genet</source>. <volume>41</volume>, <fpage>149</fpage>&#x2013;<lpage>155</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="other"><string-name><given-names>J. T.</given-names> <surname>Leek</surname></string-name>, <string-name><given-names>L. R.</given-names> <surname>Jager</surname></string-name>, <article-title>Is most published research really false?</article-title> <source>bioRxiv</source> (<year>2016</year>), p. <fpage>050575</fpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><given-names>S. N.</given-names> <surname>Goodman</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Fanelli</surname></string-name>, <string-name><given-names>J. P. A.</given-names> <surname>Ioannidis</surname></string-name>, <article-title>What does research reproducibility mean?</article-title> <source>Sci. Transl. Med.</source> <volume>8</volume>, <fpage>341ps12</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><collab>Open Science Collaboration</collab>, <article-title>Estimating the reproducibility of psychological science</article-title>. <source>Science</source>. <volume>349</volume> (<year>2015</year>), doi:<pub-id pub-id-type="doi">10.1126/science.aac4716</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><given-names>P. E.</given-names> <surname>Cramer</surname></string-name> <etal>et al.</etal>, <article-title>ApoE-directed therapeutics rapidly clear &#x03B2;-amyloid and reverse deficits in AD mouse models</article-title>. <source>Science</source>. <volume>335</volume>, <fpage>1503</fpage>&#x2013;<lpage>1506</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Gardner</surname></string-name> <etal>et al.</etal>, <article-title>G-protein-coupled receptor GPR21 knockout mice display improved glucose tolerance and increased insulin response</article-title>. <source>Biochem. Biophys. Res. Commun</source>. <volume>418</volume>, <fpage>1</fpage>&#x2013;<lpage>5</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><given-names>O.</given-names> <surname>Osborn</surname></string-name> <etal>et al.</etal>, <article-title>G protein-coupled receptor 21 deletion improves insulin sensitivity in diet-induced obese mice</article-title>. <source>J. Clin. Invest</source>. <volume>122</volume>, <fpage>2444</fpage>&#x2013;<lpage>2453</lpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><given-names>B.-H.</given-names> <surname>Lee</surname></string-name> <etal>et al.</etal>, <article-title>Enhancement of proteasome activity by a small-molecule inhibitor of USP14</article-title>. <source>Nature</source>. <volume>467</volume>, <fpage>179</fpage>&#x2013;<lpage>184</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><given-names>J. T.</given-names> <surname>Leek</surname></string-name>, <string-name><given-names>R. D.</given-names> <surname>Peng</surname></string-name>, <article-title>Opinion: Reproducible research can still be wrong: Adopting a prevention approach</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <volume>112</volume>, <fpage>1645</fpage>&#x2013;<lpage>1646</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><given-names>J. P.</given-names> <surname>Simmons</surname></string-name>, <string-name><given-names>L. D.</given-names> <surname>Nelson</surname></string-name>, <string-name><given-names>U.</given-names> <surname>Simonsohn</surname></string-name>, <article-title>False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant</article-title>. <source>Psychol. Sci</source>. <volume>22</volume>, <fpage>1359</fpage>&#x2013;<lpage>1366</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Gelman</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Loken</surname></string-name>, <article-title>The garden of forking paths: Why multiple comparisons can be a problem, even when there is no &#x201C;fishing expedition&#x201D; or &#x201C;p-hacking&#x201D; and the research hypothesis was posited ahead of time</article-title>. <source>Downloaded</source> <month>January</month>. <volume>30</volume>, <fpage>2014</fpage> (<year>2013</year>).</mixed-citation></ref>
</ref-list>
</back>
</article>