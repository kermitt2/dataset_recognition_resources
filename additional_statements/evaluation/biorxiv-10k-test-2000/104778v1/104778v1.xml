<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/104778</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Engaging narratives evoke similar neural activity and lead to similar time perception</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3007-5372</contrib-id>
<name><surname>Cohen</surname><given-names>Samantha</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Henin</surname><given-names>Simon</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Parra</surname><given-names>Lucas C.</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<aff id="a1"><label>1</label><institution>The Graduate Center of the City University of New York</institution></aff>
<aff id="a2"><label>2</label><institution>The City College of the City University of New York</institution></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label>Corresponding Author: Dr. Lucas Parra, Email: <email>parra@ccny.cuny.edu</email>, Phone: (212) 650-7211</corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2017</year>
</pub-date>
<elocation-id>104778</elocation-id>
<history>
<date date-type="received">
<day>31</day>
<month>1</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>31</day>
<month>1</month>
<year>2017</year>
</date>
</history><permissions><copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license></permissions>
<self-uri xlink:href="104778.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>It is said that we lose track of time - that &#x201C;time flies&#x201D; - when we are engrossed in a story. How does engagement with the story cause this distorted perception of time, and what are its neural correlates? People commit both time and attentional resources to an engaging stimulus. For narrative videos, attentional engagement can be represented as the level of similarity between the electroencephalographic responses of different viewers. Here we show that this measure of neural engagement predicted the duration of time that viewers were willing to commit to narrative videos. Contrary to popular wisdom, engagement did not distort the average perception of time duration. Rather, more similar brain responses resulted in a more uniform perception of time across viewers. These findings suggest that by capturing the attention of an audience, narrative videos bring both neural processing and the subjective perception of time into synchrony.</p>
</abstract>
<counts>
<page-count count="16"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Most people are familiar with the experience of becoming completely captivated by a book or movie <sup><xref ref-type="bibr" rid="c1">1</xref></sup>. This phenomenon has been described as &#x201C;engagement&#x201D;, &#x201C;transportation&#x201D;, &#x201C;absorption&#x201D;, or &#x201C;flow&#x201D; <sup><xref ref-type="bibr" rid="c2">2</xref>&#x2013;<xref ref-type="bibr" rid="c4">4</xref></sup>. Some argue that when we are fully absorbed in a narrative, there is a loss of conscious awareness of the external environment and the passage of time <sup><xref ref-type="bibr" rid="c2">2</xref>,<xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c6">6</xref></sup>. There are contradicting theories regarding whether engaging stimuli will elongate or condense perceived time. For instance, if a stimulus is enjoyable, it may evoke greater attention, thus creating a richer perceptual experience that seems longer in retrospect <sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c8">8</xref></sup>. On the other hand, when a stimulus commands attention, it may distract attention away from the passage of time, thus decreasing the perceived time elapsed <sup><xref ref-type="bibr" rid="c9">9</xref></sup>. Further complicating matters, the nature of the temporal distortion is likely dependent on the emotional valence of the stimulus <sup><xref ref-type="bibr" rid="c10">10</xref></sup>.</p>
<p>Here we will explore the relationship between attentional engagement with video narratives and the perceived passage of time on the scale of seconds. Previous research on the neural basis of time perception has examined durations that range from milliseconds to days<sup><xref ref-type="bibr" rid="c11">11</xref></sup>. For stimuli in the order of seconds, previous work mostly concerns controlled stimuli, such as constant tones or images <sup><xref ref-type="bibr" rid="c12">12</xref></sup>. Given the importance of attention to time perception <sup><xref ref-type="bibr" rid="c13">13</xref></sup>, it is possible that time perception may be substantially different during more realistic scenarios, such as engaging narratives <sup><xref ref-type="bibr" rid="c14">14</xref></sup>. Attention is also known to modulate the similarity of electroencephalographic (EEG) evoked responses across viewers during narrative videos <sup><xref ref-type="bibr" rid="c15">15</xref></sup>. In this paper, attentional engagement was assessed from the inter-subject correlation (ISC) of EEG responses <sup><xref ref-type="bibr" rid="c16">16</xref></sup>. For an objective reference of stimulus engagement, we also define a behavioral measure of engagement that is based on the time that online viewers commit to watching the narrative videos. As we will show, this is an objective, value-based metric. Unlike previous measures <sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup>, it is independent of subjective self-report biases and its assessment does not interrupt the processing of the stimulus. Engagement, whether measured neurally, or behaviorally, is thus defined by the commitment to devote a scarce resource to the stimulus. In this case, that resource is either attention or time, and, as predicted, these engagement measures are correlated. Next, we address whether moments of high engagement prolong or shorten the perception of time. Surprisingly, neither the behavioral measure of time commitment, nor the neural measure of attention coincided with the perceived passage of time. Instead, the similarity of brain responses predicted the uniformity of time estimates across viewers. This robust effect was reproduced across two cohorts of viewers. Thus, engagement does not appear to distort perceived time duration, but rather, engagement, inducing a more uniform neural processing of the stimulus, leads to a more uniform assessment of time.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>We define engagement as the commitment to devote a scarce resource, such as attention or time, to a stimulus. Three experiments were performed to establish behavioral and neural measures of engagement, and to relate these assessments to time perception. In the first experiment, behavioral engagement was evaluated from the viewing behavior of large online audiences. In the second experiment, neural engagement was extracted from the EEG responses to the same videos for which behavioral engagement had been measured. In the third experiment, the perception of time was queried during short intervals within the videos.</p>
<sec id="s2a">
<label>1.</label>
<title>Experimental assessment of engagement behavior</title>
<p>Time commitment can be calculated from the stimulus&#x2019;s ability to retain viewers. For a large enough audience, this can be measured from viewership survival, S(t), defined as the fraction of the audience that &#x201C;survives&#x201D; until time, t, in the video (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The rate at which the audience shrinks is the risk of viewership loss, denoted here as <italic>&#x03BB;</italic>(<italic>t</italic>) (<xref ref-type="disp-formula" rid="eqn3">Equation 3</xref> in Methods <xref ref-type="sec" rid="s3">Section 3</xref>, and Supplementary Fig. S1). When the stimulus evokes a high level of engagement, the risk of losing viewers is low. Conversely, when the audience is not engaged, the risk is high. Instantaneous behavioral engagement is formally defined as the inverse of the risk of losing viewers:
<disp-formula id="eqn1"><alternatives><graphic xlink:href="104778_eqn1.gif"/></alternatives></disp-formula></p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Behavioral engagement in &#x201C;experimental&#x201D; cohort mimics &#x201C;real-world&#x201D; behavior.</title>
<p>A: Viewership survival, shown for an example video, is measured as the fraction of the initial number of viewers who are retained over time. &#x201C;Real-world&#x201D; data (darker line) was provided for this study by its content owner, StoryCorps, and represents the viewership (239,511 views) accumulated over several years for this video. &#x201C;Experimental&#x201D; survival (lighter line) was collected in approximately one hour from 1000 viewers recruited online via Amazon&#x2019;s Mechanical Turk (MTurk) platform. Still images from &#x201C;Sundays at Rocco&#x2019;s,&#x201D; a StoryCorps animated short directed by the Rauch Brothers and produced by Lizzie Jacobs and Mike Rauch, reproduced here with permission from StoryCorps. B: Variation in engagement across time correlates between real-world and experimental data (r=0.60, p=2e-7, N=77). Engagement was estimated using a time interval of &#x2206;t=12s for the five videos that were common to both conditions. Dashed line indicates points with equal engagement in both data-sets. C: Experimental engagement correlated with the likelihood that a separate cohort of viewers voluntarily continued to watch the videos when given the option to stop. Dashed line represents the regression line. In both B and C each point represents a time interval, color represents the corresponding time point in the video from A, and engagement is displayed on a log-seconds scale.</p></caption>
<graphic xlink:href="104778_fig1.tif"/>
</fig>
<p>In the present context, where time is the scarce resource being allocated, E(t) is equivalent to the additional time, measured in seconds, that the average viewer is willing to invest in the stimulus (for more detail on interpretation, see Methods <xref ref-type="sec" rid="s3">Section 3</xref> and Discussion).</p>
<p>Raw viewership survival data (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) was collected for two cohorts of viewers. One cohort voluntarily watched the video stimuli online (real-world, 5 videos, approximately 2 million viewers over 2.6 &#x002B;/- 0.8 years, data shared by StoryCorps). The second cohort consisted of a group of subjects who were directed to the videos as part of an experimental paradigm (experimental, 10 videos, N=1000, collected over approximately 1 hour on Amazon&#x2019;s Mechanical Turk platform, MTurk; see Methods). All video stimuli were animated renditions of biographical narratives (161 &#x00B1; 44 s, mean &#x00B1; standard deviation). Five of the videos were viewed by both cohorts. In the real-world, viewers commit time to watch videos despite real-world commitments and time pressures. In contrast, experimental viewers were given an artificial time pressure of 15 minutes to access 27 minutes of video. For the videos common to both groups, real-world viewers were found to be significantly more engaged than those recruited experimentally (t(4)=3.2, p=0.03, paired t-test), Supplementary Fig. S2).</p>
<p>Despite this disparity in overall engagement, the two groups exhibited a similar modulation in engagement over the course of the videos. This relationship was present regardless of the time scale (&#x2206;t) at which engagement was evaluated (in <xref ref-type="disp-formula" rid="eqn3">Equation 3</xref>). The correlation between the experimental and real-world datasets was stable for time intervals, &#x2206;t, ranging from 4 to 21 seconds (r = 0.57 &#x002B;/- 0.06). Here, and in all subsequent analyses, time and engagement were measured on a logarithmic scale following convention in time perception <sup><xref ref-type="bibr" rid="c18">18</xref></sup> and survival analysis literature <sup><xref ref-type="bibr" rid="c19">19</xref></sup>. <xref ref-type="fig" rid="fig1">Figure 1B</xref> displays this relationship for a time interval of &#x2206;t = 12 s. While in the real-world viewership drops by approximately 9,000 viewers in an interval of 12 s, in the experimental cohort only 7 viewers are lost. Therefore, the experimental assessment of engagement is a noisier metric (see Methods, <xref ref-type="sec" rid="s3">Section 3</xref> and compare Supplementary Fig. 1). To validate the interpretation of this measure of engagement as a time commitment, we assessed the willingness to continue to watch the videos in a separate experiment (Results <xref ref-type="sec" rid="s3">Section 3</xref>). Since this additional cohort was being compensated for performing a different task, the decision to continue to watch a video represents both a time and a financial sacrifice. Engagement measured experimentally was correlated with the fraction of viewers that voluntarily elected to continue to watch the videos after completing their time estimation task (r = 0.40, p = 5e-6, N = 121, <xref ref-type="fig" rid="fig1">Figure 1C</xref>). These independent validations indicate that the experimentally derived measure of engagement is a good data-set with which to evaluate a neural measure of engagement.</p>
</sec>
<sec id="s2b">
<label>2.</label>
<title>Neural engagement predicts behavioral engagement</title>
<p>We previously proposed that the similarity of electroencephalographic (EEG) evoked responses across viewers may be a neural marker of engagement <sup><xref ref-type="bibr" rid="c16">16</xref></sup>. The similarity of EEG activity across subjects can be measured as the inter-subject correlation (ISC) of stimulus evoked responses <sup><xref ref-type="bibr" rid="c16">16</xref></sup>. Since ISC is sensitive to attentional state <sup><xref ref-type="bibr" rid="c15">15</xref></sup>, we predicted that there would be a relationship between ISC and the behavioral measure of engagement. To calculate ISC, EEG was recorded from 20 individuals who watched the same 10 videos from above. Components of maximal inter-subject correlation were then extracted from the EEG (see Methods). These components, C1-C3, capture sources of the evoked neural responses that are correlated in time across the entire sample of viewers (corresponding spatial distributions shown in <xref ref-type="fig" rid="fig2">Figure 2A</xref>). As such, each component potentially captures a different aspect of neural processing (e.g. visual, auditory, or supramodal processing, <sup><xref ref-type="bibr" rid="c20">20</xref></sup>). The ISC of each component can be resolved in short time intervals during the stimulus <sup><xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c21">21</xref></sup>, and time-resolved ISC was used to predict time-varying behavioral engagement.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Neural Engagement predicts Behavioral Engagement.</title>
<p>A: Spatial distribution of the three EEG components with maximal inter-subject correlation (ISC; C1 - C3). Color indicates positive (yellow) or negative (blue) correlation between the source of the neural responses and each sensor on the scalp. Component C2, center, contributes most to the relationship between neural and behavioral engagement (see main text). B: Neural engagement <italic>&#x00CA;</italic>(<italic>t</italic>); dashed line) predicts real-world behavioral engagement, <italic>E</italic>(<italic>t</italic>) using the model developed on the experimental behavioral engagement data (r = 0.56, p = 8e-8, N=78, for the 5 testing videos, &#x2206;t=12s). C: Neural engagement correlated with the fraction of a separate cohort of viewers that decided to continue to watch the videos when given the option to stop prematurely. Dashed line represents the regression line (r=0.31, p=6e-4, N=122 intervals from all 10 videos). Each point represents a different time interval in each video colored according to time as in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Both engagement measures are displayed on a log-seconds scale.</p></caption>
<graphic xlink:href="104778_fig2.tif"/>
</fig>
<p>A regression model was first fit to the experimental behavioral engagement data (see Methods Section 7, <xref ref-type="disp-formula" rid="eqn4">Equation 4</xref>). This model&#x2019;s predictive ability was then tested on the real-world data. Goodness of fit was assessed for different time intervals, &#x2206;t, over which both behavioral engagement and ISC were calculated (Supplementary Fig. S3). &#x2206;t=12s was selected as a good compromise between performance and number of samples (i.e. this &#x2206;t had the smallest p-value, p=2e-6 with, R=0.4, N=128). The predictor of engagement behavior, &#x00CA;(<italic>t</italic>), which we refer to as &#x201C;neural engagement&#x201D; can be written as a product of baseline engagement, <italic>E</italic><sub>0</sub>, with a time varying neural factor, <italic>&#x03B3;</italic>(<italic>t</italic>):
<disp-formula id="eqn2"><alternatives><graphic xlink:href="104778_eqn2.gif"/></alternatives></disp-formula></p>
<p>The baseline level of engagement, <italic>E</italic><sub>0</sub>, was estimated to be 212 s, and the overall estimated engagement, averaged in time for all videos, was 307 s. Thus, for the experimental behavioral data, approximately 30&#x0025; of the commitment to watch the stimuli could be accounted for by the temporal variation of the neural predictor variable <italic>&#x03B3;</italic>(<italic>t</italic>) (log-sum of ISC in the largest three components; <xref ref-type="disp-formula" rid="eqn4">Equation 4</xref>). Behavioral engagement was mostly explained by the ISC of the second component (C2 in <xref ref-type="fig" rid="fig2">Figure 2A</xref>), which scales baseline engagement by a factor of 1.5 &#x002B;/- 0.3 (mean and std of <italic>&#x03B3;</italic><sub>2</sub>(<italic>t</italic>) in <xref ref-type="disp-formula" rid="eqn4">Equation 4</xref>, Methods). Components C1 and C3 of the ISC contributed relatively less (1.0 &#x002B;/- 0.005 and 1.1 &#x002B;/- 0.1, mean and std of <italic>&#x03B3;</italic><sub>1</sub>(<italic>t</italic>)and <italic>&#x03B3;</italic><sub>3</sub>(<italic>t</italic>) respectively in <xref ref-type="disp-formula" rid="eqn4">Equation 4</xref>).</p>
<p>To test how well this neural engagement model predicts unseen data, we compared it to the real-world behavioral engagement data and found a significant correlation (r = 0.56, p = 8e-8, N=78 intervals from 5 videos, <xref ref-type="fig" rid="fig2">Figure 2B</xref>). In fact, this correlation was equally strong when training the regression coefficient with the experimental data from the five videos that were not part of the real-world behavioral data (r = 0.58, p = 2e-8, N=78). Thus, the predictive neural engagement model not only generalizes to unseen data, but it also generalizes across different stimuli. As with the behavioral engagement measures, the neural engagement measure, assessed across all videos, also correlated with the voluntary election to continue to watch the videos during the time estimation task (r=0.31, p=6e-4, N=122 intervals from all 10 videos, <xref ref-type="fig" rid="fig2">Figure 2C</xref>). The consistency between the neural and behavioral measures confirms the hypothesis that the similarity of brain responses captures attentional engagement.</p>
</sec>
<sec id="s2c">
<label>3.</label>
<title>Relationship between engagement and time perception</title>
<p>After establishing the validity of both the behavioral and neural measures of engagement, the relationship between stimulus engagement and time perception was assessed. An additional cohort of viewers (recruited from MTurk) provided subjective estimates for the durations of brief periods of time during the videos. These segments corresponded to those for which behavioral and neural engagement were assessed (&#x2206;t=12s, <xref ref-type="fig" rid="fig1">Figures 1</xref> and <xref ref-type="fig" rid="fig2">2</xref>). Each video was shown until an interval of interest, indicated by a visual cue, and after the time interval had elapsed, subjects were asked to report the perceived duration within a range of 8s to 16s (&#x201C;Restricted range&#x201D;, see Methods for implementation details).</p>
<p>Consistent with existing literature <sup><xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup>, the duration of the 12 s intervals was underestimated (11.3 &#x002B;/- 0.03 s), and time intervals later in the videos were perceived as lasting longer than earlier intervals (r = 0.57, p = 2e-12, Supplementary Fig. S4). However, despite prevalent theories that stimulus engagement induces time distortion <sup><xref ref-type="bibr" rid="c17">17</xref></sup>, there was no correlation between the mean estimates of time duration and engagement, measured either behaviorally or neurally (p &#x003E; 0.3). Interestingly, however, both neural and behavioral engagement correlate with the <italic>variability</italic> of time estimates across viewers (<xref ref-type="fig" rid="fig3">Figure 3</xref>, Restricted range, for neural engagement: r = - 0.27, p=2e-3, N = 129, and for real-world behavioral engagement: r = - 0.25, p=0.03, N = 78 intervals from the 5 real-world videos, and experimental behavioral engagement: r = - 0.20, p = 0.03, N = 128 intervals for all 10 videos). Variability is measured as the standard deviation of the time estimates across viewers. Thus, people are not losing track of time, per se, but rather they are tracking time more similarly when they are engrossed in the story.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Engagement predicts the variability of time perception.</title>
<p>Viewers estimated the duration of time intervals within each video. Each point represents a time interval in a video. <bold>A:</bold> The duration of intervals with high neural engagement were perceived less variably across viewers (r = - 0.27, p=2e-3, and r = - 0.23, p = 0.01, N = 129 intervals for all 10 videos, for restricted and expanded range, respectively). <bold>B:</bold> Higher real-world behavioral engagement also corresponded with a reduction in the variability of time perception (r = - 0.25, p=0.03, and r = - 0.25, p = 0.03, N = 78 intervals for the 5 real-world videos, for restricted and expanded range, respectively). All comparisons are made across two independent cohorts who had either a restricted range (blue, 8-16s) or expanded range (red, 4-20s) available for their time duration estimates. All time measures are displayed on a perceptual log-seconds scale.</p></caption>
<graphic xlink:href="104778_fig3.tif"/>
</fig>
<p>To demonstrate the reproducibility of the findings, and because the distribution of time estimates was truncated in this initial experiment (see Supplementary Fig. S5), a second cohort was recruited. This cohort reported duration within an expanded range of 4s to 20s (&#x201C;Expanded range&#x201D;). The variability of time estimates across viewers again correlated with neural engagement (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, Expanded range, r = - 0.23, p = 0.01, N = 129) and with behavioral engagement, measured in the real-world cohort (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, Expanded range, r = - 0.25, p = 0.03, N = 78 intervals from the 5 real-world video). This weaker correlation was not resolved for experimental behavioral engagement (r = - 0.09, p = 0.3, N = 128 intervals from all 10 videos), possibly because the experimental data has a smaller range and is a noisier measure than the real-world assessment (compare Supplementary Fig. 1). Nevertheless, this independent cohort largely confirms our main and novel finding that engagement synchronizes neural activity across brains, thus resulting in a more uniform perception of time across people.</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>It is often said that we lose track of time when absorbed in an engaging narrative <sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c6">6</xref></sup>. Indeed, engagement can potentially either shorten or elongate our subjective perception of time <sup><xref ref-type="bibr" rid="c7">7</xref>&#x2013;<xref ref-type="bibr" rid="c9">9</xref></sup>. Engagement with a narrative may do both depending on its emotional valence <sup><xref ref-type="bibr" rid="c10">10</xref></sup>. The precise neural processing that results in time&#x2019;s distortion during naturalistic narrative stimuli is a matter of ongoing exploration <sup><xref ref-type="bibr" rid="c14">14</xref></sup>. To determine the dependence of time perception on engagement, we first characterized engagement behaviorally in terms of how an audience is retained by narrative videos (<xref ref-type="fig" rid="fig1">Figure 1</xref>). To assess attentional engagement, we measured the inter-subject correlation (ISC) of stimulus-evoked EEG responses to the same videos (following <sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c16">16</xref></sup>). We found that ISC is predictive of the time that viewers were willing to spend with the videos (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Surprisingly, neither behavioral nor attentional engagement correlated with the perceived duration of intervals of time during the videos. Instead, when the videos were more engaging, they were processed in the brain more uniformly (resulting in higher ISC), and the perception of time was more uniform across viewers (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Thus, rather than losing track of time, viewers have a more consistent perception of time when highly engaged in the stimulus, at least for narrative videos on the scale of seconds.</p>
<p>We found that fluctuations in engagement behavior over time are predicted by the neural processing of the stimulus. The time-varying neural predictor, <italic>&#x03B3;</italic>(<italic>t</italic>), modified the baseline level of engagement, <italic>E</italic><sub>0</sub>, to track engagement behavior over time (<xref ref-type="disp-formula" rid="eqn2">Equation 2</xref>). In models that explore the survival of a population, the variable <italic>&#x03B3;</italic> determines the &#x201C;speed&#x201D; at which the population ages, therefore modifying the expected life-span <sup><xref ref-type="bibr" rid="c23">23</xref></sup>. It was therefore possible that while commitment to the stimulus remains constant, the subjective perception of time was variable. In this view, <italic>&#x03B3;</italic>(<italic>t</italic>) would reflect the subjective perception that time &#x201C;flows&#x201D; differently depending on how immersed one is in the narrative <sup><xref ref-type="bibr" rid="c17">17</xref></sup>. Alternatively, behavioral engagement could be interpreted in the context of &#x201C;failure analysis&#x201D; <sup><xref ref-type="bibr" rid="c19">19</xref></sup>, where inverse risk, our definition of engagement behavior, quantifies the mean time to failure. In this case, inverse risk is the average time that it takes to lose a viewer. This idea is consistent with the interpretation of engagement as a time commitment, irrespective of the perception of time duration. The third experiment tested the two alternatives: whether engagement reflects time distortion or time commitment. We found that the perceived passage of time does not correlate with behavioral engagement nor its neural correlate. In contrast, both engagement measures correlated with the willingness to invest more time with the stimulus. Thus, our results are consistent with an interpretation of engagement as a value based decision, and not in terms of a perceptual warping of time. Indeed, time perception appears to have been less &#x201C;warped&#x201D; during moments of high engagement.</p>
<p>Despite extensive research on the neural underpinnings of time perception, no consensus exists on how humans estimate time intervals <sup><xref ref-type="bibr" rid="c24">24</xref></sup>. While it is likely that different mechanisms support the perception of different time scales <sup><xref ref-type="bibr" rid="c11">11</xref></sup>, there is also evidence for shared neural processes that implicate a diverse set of brain regions depending on task specifics <sup><xref ref-type="bibr" rid="c18">18</xref></sup>. The prominent models, such as the pacemaker-accumulator models <sup><xref ref-type="bibr" rid="c25">25</xref></sup>, and the climbing neural activation model <sup><xref ref-type="bibr" rid="c24">24</xref></sup>, posit that there is some input that is integrated across time, and that this integrated signal outputs an estimate of duration. Previous fMRI studies have explored the dependence of such signals on attention <sup><xref ref-type="bibr" rid="c26">26</xref></sup>, emotional content <sup><xref ref-type="bibr" rid="c27">27</xref></sup>, and salience <sup><xref ref-type="bibr" rid="c28">28</xref></sup>. It is likely that all of these features altered time perception during our video stimuli. We found that more similar neural processing across viewers led to a more uniform report of time&#x2019;s passage. This suggests that stimulus processing may ultimately provide (or at least modulate) the input to the time integration circuit, because more uniform stimulus processing (input) led to more uniform time estimates (output) across viewers. It is relatively unique to measure time perception during dynamic natural stimuli. Recently, <xref ref-type="bibr" rid="c14">Lositsky et al. (2016)</xref> measured time perception during auditory narratives and found that changes in stimulus induced brain activity are predictive of retrospective time estimates on the order of minutes. Our results concern prospective time estimates for intervals on the order of seconds. Despite these differences, both studies support the notion that for narrative stimuli, the subjective passage of time is driven by stimulus processing, rather than an internal stimulus-independent &#x201C;clock&#x201D; <sup><xref ref-type="bibr" rid="c29">29</xref></sup>.</p>
<p>It is well established that time perception is affected by attention <sup><xref ref-type="bibr" rid="c13">13</xref></sup>. We therefore leveraged EEG responses evoked by the stimuli to gauge attentional engagement using ISC. The brain activity most predictive of engagement was the second component of the EEG activity correlated across viewers (see <xref ref-type="fig" rid="fig3">Figure 3A</xref> for scalp topography). With a factor of <italic>&#x03B3;</italic><sub>2</sub> = 1.5, this component explains the fluctuation in time commitment from a baseline level of approximately 200s to the average engagement level of approximately 300s. Interestingly, among the three strongest components of the ISC, this component is also the best at discriminating between attentional states <sup><xref ref-type="bibr" rid="c15">15</xref></sup>. This component is also uniquely evoked during narratives with sound, as opposed to those with only visuals, and it may therefore be representative of auditory processing <sup><xref ref-type="bibr" rid="c20">20</xref></sup>, or potentially how strongly the stimulus&#x2019; auditory narrative captures attentional resources.</p>
<p>The concept of &#x201C;engagement&#x201D; is used in a variety of contexts: a client engages a law firm, a couple is engaged to be married, or a student is engaged in the classroom. In all of these scenarios there is a commitment of either time or financial resources. Similarly, in this paper, we define engagement as the commitment of a scarce resource. Unlike self-report assessments <sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup>, this definition is quantifiable in strict numerical terms and can be applied even when subjective reports are impossible <sup><xref ref-type="bibr" rid="c30">30</xref></sup>. Whether the resources engaged are time or attention, a value-based decision is consistently assessed in which the value gained from consuming the narrative is compared to that of possible alternatives <sup><xref ref-type="bibr" rid="c31">31</xref></sup>. Here, both our neural and behavioral measures of engagement predict that viewers can commit time periods that range from seconds to hours (see units on <xref ref-type="fig" rid="fig2">Figure 2B</xref>). In the third experiment, this commitment was literally represented by the decision to spend time with the videos rather than to earn money. This is because the subjects, recruited on MTurk, were forfeiting potential income from the completion of the tasks for which they are paid by continuing to watch the videos. The correlation of both engagement measures with this decision is a direct confirmation that engagement is a value-based decision (<xref ref-type="fig" rid="fig1">Figure 1C</xref> and <xref ref-type="fig" rid="fig2">2C</xref>).</p>
<p>To evaluate engagement behavior, an experimental technique was developed that takes a fraction of the time necessary to acquire comparable real-world data. Our definition of engagement, in terms of devoting a scarce resource, and potentially this technique, could be translated to other media that engage people such as books, music, virtual reality, gaming, or to stimulating activities such as painting or playing sports. Time does not necessarily have to be the resource that is sacrificed. By making a commitment, viewers may be foregoing monetary compensation or social rewards to gain access to entertainment. The worth of engagement may be computed using a currency that can be traded for these and other scarce resources. We predict that if viewers are similarly entrained by the stimulus (or activity), thus eliciting a high level of ISC, they will be immune to extrinsic costs such as the time or money that they are sacrificing for the current moment&#x2019;s enjoyment. Their perception of time, one of the many valuable resources that they are sacrificing, will thus be driven by the stimulus, and consistently so across viewers.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<label>1.</label>
<title>Stimuli</title>
<p>Stimuli were chosen on the basis of their highly emotive content and the availability of online viewership data. We used these same stimuli in a previous EEG study on incidental memory <sup><xref ref-type="bibr" rid="c20">20</xref></sup>. They consisted of 10 different videos (5 from the New York Times&#x0027; Modern Love episodes: &#x201C;Broken Heart Doctor&#x201D;, &#x201C;Don&#x0027;t Let it Snow&#x201D;, &#x201C;Falling in Love at 71&#x201D;, &#x201C;Lost and Found&#x201D;, and &#x201C;The Matchmaker&#x201D;, and 5 from StoryCorps&#x0027; animated shorts: &#x201C;Eyes on the Stars&#x201D;, &#x201C;John and Joe&#x201D;, &#x201C;Marking the Distance&#x201D;, &#x201C;Sundays at Rocco&#x2019;s&#x201D; (depicted in <xref ref-type="fig" rid="fig1">Figure 1A</xref>), and &#x201C;To R.P. Salazar with Love&#x201D;). Stimuli were 161 &#x00B1; 44 s in duration (mean and standard deviation across stimuli) with a total duration of 26 min 48 s.</p>
</sec>
<sec id="s4b">
<label>2.</label>
<title>Behavioral Engagement Data Collection</title>
<p>Real-world viewing behavior was measured using the pool of viewers who found the five StoryCorps videos organically on YouTube, via the StoryCorps website (storycorps.org/animation), or another linked website. Anonymous YouTube Analytics data were provided as aggregated viewership survival data (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, Supplementary Fig. 1) by StoryCorps with permission for analysis and publication. The viewership data captured the behavior of viewers, totaling 2,528,897 across all five videos, amassed since the videos were made available online until the time of data retrieval (2.6 &#x002B;/- 0.9 years).</p>
<p>Behavioral engagement was measured experimentally on an independent set of 1,000 subjects collected on Amazon&#x2019;s Mechanical Turk (MTurk) platform (requester.mturk.com). Participants with IP addresses located in the USA had access to all 10 stimuli in a randomized order for a duration of 15 minutes and were informed that the total duration of all videos exceeded the time allotted. Watching the videos was optional (required a mouse click) and it is therefore possible that a fraction of these participants did not watch any of the videos. It took approximately one hour for this data to be collected for all 1000 subjects. This collection method was selected after pilot testing in three smaller cohorts of 100 subjects each. In the first pilot, participants had the option of immediate remuneration and therefore spent little time with the videos. In the second pilot, participants were not informed that they would be paid. In the third pilot, subjects were informed of their impending payment. The results of this pilot agreed best with the real-world data, and this format was therefore used for the final data collection. None of the pilot data was included in the final analysis reported here.</p>
<p>The YouTube Analytics API was used to extract the fraction of viewers (number of current viewers / total number of initial viewers) who watched each video interval. The resulting curve can be considered the viewership survival, S(t), although it is not a strict survival metric because YouTube allows users to rewind and skip sections and therefore S(t) sometimes increases. Note that the Analytics API divides each video into 100 sampling intervals, regardless of video duration, and was resampled to correspond with the absolute time elapsed, as described below.</p>
</sec>
<sec id="s4c">
<label>3.</label>
<title>Risk of viewership loss</title>
<p>At each time interval, a decision is made regarding whether the current activity provides more reward than another activity. In aggregate, over the population of viewers, this is reflected in the survival function, <italic>S</italic>(<italic>t</italic>), the ratio of people that are still watching, or have survived, until time<italic>t</italic>. A typical example of this from the real-world data is shown in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. The risk, or hazard, of losing viewers, <italic>&#x03BB;</italic>(<italic>t</italic>), can be estimated from <italic>S</italic>(<italic>t</italic>)and is conventionally defined as the relative change of the survival in a time interval <italic>&#x0394;t</italic> <sup><xref ref-type="bibr" rid="c23">23</xref></sup>:
<disp-formula id="eqn3"><alternatives><graphic xlink:href="104778_eqn3.gif"/></alternatives></disp-formula></p>
<p>In a realistic scenario, stimuli are selected, engaged with, and finally abandoned when audience members determines that their limited temporal resources are better spent elsewhere. <xref ref-type="fig" rid="fig2">Figure 2A</xref> shows several real-world examples of how the number of surviving viewers, <italic>S</italic>(<italic>t</italic>),decays over time. The hazard curve, <italic>&#x03BB;</italic>(<italic>t</italic>), derived from the survival function (Supplementary Fig. S1) presents a typical &#x201C;bathtub&#x201D; curve with high risk of viewership loss at the beginning and ending and low risk during the intervening time. In failure analysis inverse risk represents the mean time to failure <sup><xref ref-type="bibr" rid="c19">19</xref></sup>, and for constant risk, the survival curve is exponentially decaying with the inverse risk as its time constant.</p>
<p>Risk of losing the audience, <italic>&#x03BB;</italic>(<italic>t</italic>), was computed according to <xref ref-type="disp-formula" rid="eqn3">Equation (3)</xref> on the 100 samples in the raw data. For visualization purposes, <xref ref-type="fig" rid="fig1">Figure 1A</xref> and Supplementary Fig. S1 show survival, S(t), and risk, <italic>&#x03BB;</italic>(<italic>t</italic>), as a function of absolute time (scaled to seconds). For the purpose of comparing experimental and real-world data, <italic>&#x03BB;</italic>(<italic>t</italic>) was resampled to a time scale of &#x2206;t =12s. Engagement, E(t), derived from <italic>&#x03BB;</italic>(<italic>t</italic>)in <xref ref-type="disp-formula" rid="eqn1">Equation (1)</xref>, is plotted at this scale (Supplementary Fig. S1). This time scale was motivated by the comparison with the neural data. To calculate the average number of viewers lost in a time interval of &#x2206;t =12s, the derivative of S(t) was scaled by the initial number of viewers and then multiplied by 12 seconds.</p>
<p>Time intervals with negative engagement (due to rewinding and skipping on YouTube) are ignored because the logarithm of engagement is used in all analyses. Therefore, at a sampling interval of &#x2206;t =12s, 1 interval is excluded from the experimentally acquired data, and 3 intervals are excluded from the real-world data. This exclusion effects the comparison between the two behavioral engagement measures (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, a reduction from N=81 intervals to N=77), the comparison with the decision to continue watching (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, a reduction from N=122 to N=121), the comparison with neural engagement (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, Results <xref ref-type="sec" rid="s2">Section 2</xref>, for the experimental cohort a reduction from N=129 to N=128, and for the real-world cohort a reduction from N=81 to N=78), and the comparison with the time estimates (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, reductions the same as those for neural engagement).</p>
</sec>
<sec id="s4d">
<label>4.</label>
<title>Perceived Time Data Collection</title>
<p>Perceived time was measured on two additional MTurk cohorts. Each participant watched all 10 stimuli and a pseudo-randomly selected 12 second time interval was denoted by the appearance of a red hourglass in the corner of the video. After the interval had expired, the video was paused, and participants were asked to estimate the duration for which the hourglass had appeared (perceived time). The longest video clips had 19 intervals. To ensure that each of these intervals had at least 20 estimates, 380 subjects were recruited. Across all videos, there were N=129 time intervals, of these 129, N=81 corresponded to intervals in the videos for which real-world behavioral engagement was assessed. In this first cohort, subjects reported perceived duration on a visual analog scale with values ranging from 8 to 16 seconds. A histogram of reported time estimates shows a large number of responses at the boundaries of this range (see Supplementary Fig. S5, restricted range), suggesting that inputs may have been restricted. A second cohort was therefore recruited and given the option to input estimates ranging from 4 to 20 seconds (expanded range). To ensure a robust estimate of the standard deviation of perceived time across subjects, this cohort had 720 participants. After each interval had transpired, participants were also asked a comprehension question related to the content of the story in that interval (same questions as in <sup><xref ref-type="bibr" rid="c20">20</xref></sup>; mean accuracy level 84.0&#x0025; &#x002B;/- 37.7&#x0025; across questions). After the response was recorded, participants were given the option to finish the video. When the interval that was just watched was the last interval in the video, the option to finish the video was not given. As this occurred in 7 cases, this resulted in a reduction from N=129 intervals to N=122 intervals. Prior to the presentation of the 10 experimental stimuli, participants were briefly acquainted with the task on four non-experimental videos. For the first video, subjects are told ahead of time that the interval is 12 seconds in duration; for the next three, interval durations are selected at random to be either 8, 12 or 16 seconds. Without prior knowledge of the duration, subjects are asked to estimate it and are subsequently informed of the correct duration. All data collection was approved by the Institutional Review Board of the City University of New York.</p>
</sec>
<sec id="s4e">
<label>5.</label>
<title>Electroencephalographic Data Collection and Preprocessing</title>
<p>Electroencephalographic (EEG) data were collected for a previous study from a cohort of 20 individuals who watched the same audiovisual stimuli (AV condition, in <xref ref-type="bibr" rid="c20">Cohen &#x0026; Parra, 2016</xref>). For more details regarding participants, EEG data collection, and preprocessing see <xref ref-type="bibr" rid="c20">Cohen &#x0026; Parra, 2016</xref>.</p>
</sec>
<sec id="s4f">
<label>6.</label>
<title>Inter-Subject Correlation</title>
<p>Inter-subject correlation (ISC) is evaluated in the correlated components of the EEG <sup><xref ref-type="bibr" rid="c16">16</xref></sup>. The goal of correlated component analysis in this case is to find linear combinations of electrodes (one could think of them as virtual sensors or &#x201C;sources&#x201D; in the brain) that are maximally correlated between subjects. The ISC calculation implemented here is the same as what has been published previously <sup><xref ref-type="bibr" rid="c15">15</xref>,<xref ref-type="bibr" rid="c20">20</xref></sup>. Following previous research, the top three components of the EEG are extracted and capture maximally correlated responses between subjects <sup><xref ref-type="bibr" rid="c16">16</xref></sup>. To determine the correspondence between behavioral engagement and ISC, ISC was resolved in time, using the same time intervals over which behavioral engagement was measured (see next section). Time resolved ISC in the <italic>i</italic>-th correlated component is here denoted as<italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>).</p>
</sec>
<sec id="s4g">
<label>7.</label>
<title>Comparisons between behavioral and neural engagement</title>
<p>A proportional hazard model was used (<xref ref-type="bibr" rid="c32">Cox, 1972</xref>) to relate engagement behavior and ISC. This resulted in a regression of engagement, <italic>E</italic>(<italic>t</italic>), with a time dependent covariate, <italic>&#x03B3;</italic>(<italic>t</italic>), and a constant baseline engagement, <italic>E</italic><sub>0</sub> (<xref ref-type="disp-formula" rid="eqn2">Equation 2</xref> in Results). Following the traditional form of the proportional hazard model <sup><xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c32">32</xref></sup>, the time dependent covariate, <italic>log &#x03B3;</italic>(<italic>t</italic>), equals the weighted sum of the predictor variables:
<disp-formula id="eqn4"><alternatives><graphic xlink:href="104778_eqn4.gif"/></alternatives></disp-formula></p>
<p>The contribution of ISC in each component, <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>), to the total engagement can thereby be assessed from its corresponding <italic>&#x03B3;</italic><sub><italic>i</italic></sub>(<italic>t</italic>)value. Optimal model parameters <italic>&#x03B2;</italic><sub><italic>i</italic></sub> and <italic>E</italic><sub>0</sub> were found as the best linear fit for the log-engagement data collected from the experimental group. This fit was performed separately for different time intervals (&#x2206;t). Engagement, <italic>E</italic>(<italic>t</italic>), was resampled from the 100 samples in the raw data into various time resolutions (&#x2206;t = 1s, 2s, 3s, &#x2026; and up to 30s) and time-resolved ISC <italic>x</italic><sub><italic>i</italic></sub>(<italic>t</italic>) was also calculated on the matching time intervals. Goodness of fit, R, is shown in Supplementary Fig. S3 as a function of &#x2206;t. For consistency, all subsequent analyses were performed at 12s resolution, which provided the best fit (see Results).</p>
<p>Final parameter estimates were assessed by training on the experimentally measured engagement data and testing on the real-world data without further parameter adaptation. This captures the generalization performance from trained to unseen data, as well as generalization from the experimental procedure to the real-world behavioral engagement data. To capture the generalization between different stimuli, in an additional analysis, training of the model parameters, <italic>&#x03B2;</italic><sub><italic>i</italic></sub>, and <italic>E</italic><sub>0</sub>, was limited to only those videos that were not in the testing set, i.e. training was performed on the experimental data from the 5 New York Times videos and the real-world data from the 5 StoryCorps videos was used for testing.</p>
</sec>
<sec id="s4h">
<label>8.</label>
<title>Relationship between time perception and engagement</title>
<p>Correlation was assessed between both engagement measures (behavioral engagement: <italic>log</italic>(<italic>E</italic>(<italic>t</italic>)) and neural engagement: using the optimal fit for <italic>&#x03B2;</italic><sub><italic>i</italic></sub> at &#x2206;t=12s) and the perceived time of each interval, collected from two independent cohorts of subjects. As expected <sup><xref ref-type="bibr" rid="c13">13</xref></sup>, mean time perception was longer for later intervals in the videos (Supplementary Fig. S4). This drift in mean time perception with wait time does not affect the standard deviation of time estimates because the standard deviation does not depend on the mean (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Linear regressions were used to relate the standard deviation of time estimates to both neural and behavioral measures of engagement.</p>
</sec>
<sec id="s4i">
<label>9.</label>
<title>Statistics</title>
<p>The significance of reported correlation values, r, were computed using conventional parametric statistics which assume independent samples. To rule out that temporal correlations between samples bias these results, p-values for all analyses were also estimated using nonparametric bootstrapping. Specifically, significance levels were computed using phase shuffled data (following <sup><xref ref-type="bibr" rid="c33">33</xref></sup>), which preserve the correlation structure in time but alters the time course of a temporal sequence. Therefore, N=10<sup><xref ref-type="bibr" rid="c6">6</xref></sup> phase shuffled surrogates were produced and correlation coefficient computed for each surrogate. Bootstrap p-values are calculated as the fraction of shuffles with correlation values more extreme than those obtained with the original time sequences. All bootstrap p-values were comparable to the values reported in the main text using parametric statistics (except for the comparison between real-world behavioral engagement and the variance of time estimates, <xref ref-type="fig" rid="fig3">Figure 3C</xref> where p &#x003E; 0.2). In the analysis of the viewer&#x0027;s decision to continue to watch the videos (&#x201C;Fraction continued video&#x201D; in <xref ref-type="fig" rid="fig1">Figure 1C</xref>, and 2C), data from the two time estimation cohorts (Restricted and Expanded) was combined since the decision to continue watching did not vary across the cohorts. All analysis of time and engagement were performed on a log-seconds scale (except for the computation of the mean and standard deviation of the time estimates, <xref ref-type="fig" rid="fig3">Figure 3</xref>). All statistical tests were performed in MATLAB (MathWorks, Natick, MA, USA).</p>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Barthes</surname>, <given-names>R.</given-names></string-name> &#x0026; <string-name><surname>Duisit</surname>, <given-names>L.</given-names></string-name> <article-title>An Introduction to the Structural Analysis of Narrative</article-title>. <source>New Lit. Hist.</source> <volume>6</volume>, <fpage>237</fpage>&#x2013;<lpage>272</lpage> <year>(1975)</year>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Green</surname>, <given-names>M. C.</given-names></string-name> &#x0026; <string-name><surname>Brock</surname>, <given-names>T. C.</given-names></string-name> <article-title>The role of transportation in the persuasiveness of public narratives</article-title>. <source>J. Pers. Soc. Psychol.</source> <volume>79</volume>, <fpage>701</fpage>&#x2013;<lpage>721</lpage> <year>(2000)</year>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal">Tellegen, a &#x0026; <string-name><surname>Atkinson</surname>, <given-names>G.</given-names></string-name> <article-title>Openness to absorbing and self-altering experiences (&#x2018;absorption&#x2019;), a trait related to hypnotic susceptibility</article-title>. <source>J. Abnorm. Psychol.</source> <volume>83</volume>, <fpage>268</fpage>&#x2013;<lpage>277</lpage> <year>(1974)</year>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="book"><string-name><surname>Csikszentmihalyi</surname>, <given-names>M.</given-names></string-name> <source>Flow: The psychology of happiness</source>. (<publisher-name>Harper &#x0026; Row</publisher-name>, <year>1992</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Busselle</surname>, <given-names>R.</given-names></string-name> &#x0026; <string-name><surname>Bilandzic</surname>, <given-names>H.</given-names></string-name> <article-title>Measuring Narrative Engagement</article-title>. <source>Media Psychol.</source> <volume>12</volume>, <fpage>321</fpage>&#x2013;<lpage>347</lpage> <year>(2009)</year>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Green</surname>, <given-names>M. C.</given-names></string-name> <article-title>Transportation into narrative worlds: The role of prior knowledge and perceived realism</article-title>. <source>Discourse Process.</source> <volume>38</volume>, <fpage>247</fpage>&#x2013;<lpage>266</lpage> <year>(2004)</year>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Avni-Badad</surname>, <given-names>D.</given-names></string-name> &#x0026; <string-name><surname>Ritov</surname>, <given-names>I.</given-names></string-name> <article-title>Routine and the perception of time</article-title>. <source>J. Exp. Psychol. Gen.</source> <volume>132</volume>, <fpage>543</fpage>&#x2013;<lpage>50</lpage> <year>(2003)</year>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="book"><string-name><surname>Ornstein</surname>, <given-names>R. E.</given-names></string-name> <source>On the experience of time</source>. (<publisher-name>Penguin</publisher-name>, <year>1969</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="book"><string-name><surname>Zakay</surname>, <given-names>D.</given-names></string-name> in <source>Time and human cognition</source> (eds. <string-name><surname>Levin</surname>, <given-names>I.</given-names></string-name> &#x0026; <string-name><surname>Zakay</surname>, <given-names>D.</given-names></string-name>) (<publisher-name>Elsevier</publisher-name>, <year>1989</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Angrilli</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cherubini</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Pavese</surname>, <given-names>A.</given-names></string-name> &#x0026; <string-name><surname>Manfredini</surname>, <given-names>S.</given-names></string-name> <article-title>The influence of affective factors on time perception</article-title>. <source>Percept. Psychophys.</source> <volume>59</volume>, <fpage>972</fpage>&#x2013;<lpage>982</lpage> <year>(1997)</year>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Buhusi</surname>, <given-names>C.</given-names></string-name> V &#x0026; <string-name><surname>Meck</surname>, <given-names>W. H.</given-names></string-name> <article-title>What makes us tick? Functional and neural mechanisms of interval timing</article-title>. <source>Nat. Rev. Neurosci.</source> <volume>6</volume>, <fpage>755</fpage>&#x2013;<lpage>65</lpage> <year>(2005)</year>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="book"><string-name><surname>Penney</surname>, <given-names>T. B.</given-names></string-name> &#x0026; <string-name><surname>Vaitilingam</surname>, <given-names>L.</given-names></string-name> in <source>Psychology of time</source> (ed. <string-name><surname>Grondin</surname>, <given-names>S.</given-names></string-name>) <fpage>261</fpage>&#x2013;<lpage>294</lpage> (<publisher-name>Emerald Group</publisher-name>, <year>2008</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Grondin</surname>, <given-names>S.</given-names></string-name> <article-title>Timing and time perception: A review of recent behavioral and neuroscience findings and theoretical directions</article-title>. <source>Atten. Percept. Psychophys.</source> <volume>72</volume>, <fpage>561</fpage>&#x2013;<lpage>582</lpage> <year>(2010)</year>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Lositsky</surname>, <given-names>O.</given-names></string-name> <etal>et al.</etal> <article-title>Neural pattern change during encoding of a narrative predicts retrospective duration estimates</article-title>. <source>Elife</source> <volume>5</volume>, <year>(2016)</year>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="other"><string-name><surname>Ki</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kelly</surname>, <given-names>S.</given-names></string-name> &#x0026; <string-name><surname>Parra</surname>, <given-names>L. C.</given-names></string-name> <article-title>Attention strongly modulates reliability of neural responses to naturalistic narrative stimuli</article-title>. <source>J. Neurosci.</source> <year>(2016)</year>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Dmochowski</surname>, <given-names>J. P.</given-names></string-name>, <string-name><surname>Sajda</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Dias</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Parra</surname>, <given-names>L. C.</given-names></string-name> <article-title>Correlated components of ongoing EEG point to emotionally laden attention &#x2013; a possible marker of engagement?</article-title> <source>Front. Hum. Neurosci.</source> <volume>6</volume>, <fpage>1</fpage>&#x2013;<lpage>9</lpage> <year>(2012)</year>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="other"><string-name><surname>Nakamura</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Csikszentmihalyi</surname>, <given-names>M.</given-names></string-name> <article-title>The Concept of Flow Optimal Experience and Its Role in Development</article-title>. <fpage>89</fpage>&#x2013;<lpage>105</lpage> <year>(2002)</year>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Merchant</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Harrington</surname>, <given-names>D. L.</given-names></string-name> &#x0026; <string-name><surname>Meck</surname>, <given-names>W. H.</given-names></string-name> <article-title>Neural Basis of the Perception and Estimation of Time</article-title>. <source>Annu. Rev. Neurosci.</source> <volume>36</volume>, <fpage>313</fpage>&#x2013;<lpage>36</lpage> <year>(2013)</year>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="book"><string-name><surname>Kalbfleisch</surname>, <given-names>J. D.</given-names></string-name> &#x0026; <string-name><surname>Prentice</surname>, <given-names>R. L.</given-names></string-name> <source>The statistical analysis of failure time data</source>. (<publisher-name>John Wiley &#x0026; Sons, Inc</publisher-name>., <year>2002</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="other"><string-name><surname>Cohen</surname>, <given-names>S. S.</given-names></string-name> &#x0026; <string-name><surname>Parra</surname>, <given-names>L. C.</given-names></string-name> <article-title>Memorable audiovisual narratives synchronize sensory and supramodal neural responses</article-title>. <source>eNeuro</source> <year>(2016)</year>. doi:<pub-id pub-id-type="doi">10.1523/ENEURO.0203-16.2016</pub-id></mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Dmochowski</surname>, <given-names>J. P.</given-names></string-name> <etal>et al.</etal> <article-title>Audience preferences are predicted by temporal reliability of neural processing</article-title>. <source>Nat. Commun.</source> <volume>5</volume>, <fpage>1</fpage>&#x2013;<lpage>9</lpage> <year>(2014)</year>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Block</surname>, <given-names>R. A.</given-names></string-name> &#x0026; <string-name><surname>Zakay</surname>, <given-names>D.</given-names></string-name> <article-title>Prospective and retrospective duration judgments: A meta-analytic review</article-title>. <source>Psychon. Bull. Rev.</source> <volume>4</volume>, <fpage>184</fpage>&#x2013;<lpage>197</lpage> <year>(1997)</year>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="other"><string-name><surname>Rodriguez</surname>, <given-names>G.</given-names></string-name> <article-title>Parametric survival models</article-title>. <source>Regres. Model. Strateg.</source> <fpage>1</fpage>&#x2013;<lpage>14</lpage> <year>(2010)</year>. doi:<pub-id pub-id-type="doi">10.1093/biostatistics/kxj028</pub-id></mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Wittmann</surname>, <given-names>M.</given-names></string-name> <article-title>Representation of Duration</article-title>. <source>Nat. Rev. Neurosci.</source> <volume>14</volume>, <fpage>217</fpage>&#x2013;<lpage>223</lpage> <year>(2013)</year>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Matell</surname>, <given-names>M. S.</given-names></string-name> &#x0026; <string-name><surname>Meck</surname>, <given-names>W. H.</given-names></string-name> <article-title>Neuropsychological mechanisms of interval timing behavior</article-title>. <source>BioEssays</source> <volume>22</volume>, <fpage>94</fpage>&#x2013;<lpage>103</lpage> <year>(2000)</year>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Coull</surname>, <given-names>J. T.</given-names></string-name>, <string-name><surname>Vidal</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Nazarian</surname>, <given-names>B.</given-names></string-name> &#x0026; <string-name><surname>Macar</surname>, <given-names>F.</given-names></string-name> <article-title>Functional Anatomy of the Attentional Modulation of Time Estimation</article-title>. <source>Science</source> <volume>303</volume>, <fpage>1506</fpage>&#x2013;<lpage>1508</lpage> <year>(2004)</year>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Dirnberger</surname>, <given-names>G.</given-names></string-name> <etal>et al.</etal> <article-title>Give it time: Neural evidence for distorted time perception and enhanced memory encoding in emotional situations</article-title>. <source>Neuroimage</source> <volume>63</volume>, <fpage>591</fpage>&#x2013;<lpage>599</lpage> <year>(2012)</year>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Wittmann</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>van Wassenhove</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Craig</surname>, a <given-names>D. B.</given-names></string-name> &#x0026; <string-name><surname>Paulus</surname>, <given-names>M. P.</given-names></string-name> <article-title>The neural substrates of subjective time dilation</article-title>. <source>Front. Hum. Neurosci.</source> <volume>4</volume>, <fpage>1</fpage>&#x2013;<lpage>9</lpage> <year>(2010)</year>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Wittmann</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Simmons</surname>, <given-names>A. N.</given-names></string-name>, <string-name><surname>Aron</surname>, <given-names>J. L.</given-names></string-name> &#x0026; <string-name><surname>Paulus</surname>, <given-names>M. P.</given-names></string-name> <article-title>Accumulation of neural activity in the posterior insula encodes the passage of time</article-title>. <source>Neuropsychologia</source> <volume>48</volume>, <fpage>3110</fpage>&#x2013;<lpage>3120</lpage> <year>(2010)</year>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Deaner</surname>, <given-names>R. O.</given-names></string-name>, <string-name><surname>Khera</surname>, <given-names>A. V.</given-names></string-name> &#x0026; <string-name><surname>Platt</surname>, <given-names>M. L.</given-names></string-name> <article-title>Monkeys pay per view: Adaptive valuation of social images by rhesus macaques</article-title>. <source>Curr. Biol.</source> <volume>15</volume>, <fpage>543</fpage>&#x2013;<lpage>548</lpage> <year>(2005)</year>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Rangel</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Camerer</surname>, <given-names>C.</given-names></string-name> &#x0026; <string-name><surname>Montague</surname>, <given-names>P. R.</given-names></string-name> <article-title>A framework for studying the neurobiology of value-based decision making</article-title>. <source>Nat. Rev. Neurosci.</source> <volume>9</volume>, <fpage>545</fpage>&#x2013;<lpage>556</lpage> <year>(2008)</year>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Cox</surname>, <given-names>D. R.</given-names></string-name> <article-title>Regression Models and Life-Tables</article-title>. <source>J. R. Stat. Soc. Ser. B</source> <volume>34</volume>, <fpage>187</fpage>&#x2013;<lpage>220</lpage> <year>(1972)</year>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Prichard</surname>, <given-names>D.</given-names></string-name> &#x0026; <string-name><surname>Theiler</surname>, <given-names>J.</given-names></string-name> <article-title>Generating surrogate data for time series with several simultaneously measured variables</article-title>. <source>Phys. Rev. Lett.</source> <volume>73</volume>, <fpage>951</fpage>&#x2013;<lpage>954</lpage> <year>(1994)</year>.</mixed-citation></ref>
</ref-list>
<ack>
<title>Acknowledgements</title>
<p>We would like to acknowledge StoryCorps for providing us with their YouTube analytics data and their animated interviews as video stimuli. These interviews were recorded by StoryCorps and are provided courtesy of StoryCorps, a national not-for-profit corporation dedicated to preserve and share humanity&#x0027;s stories in order to build connections between people and create a more just and compassionate world. <ext-link ext-link-type="uri" xlink:href="http://www.storycorps.org">www.storycorps.org</ext-link>.</p>
</ack>
</back>
</article>