<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/274035</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Bioinformatics</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Clustering trees: a visualisation for evaluating clusterings at multiple resolutions</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7744-8565</contrib-id>
<name><surname>Zappia</surname><given-names>Luke</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9788-5690</contrib-id>
<name><surname>Oshlack</surname><given-names>Alicia</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Bioinformatics, Murdoch Children&#x2019;s Research Institute</institution></aff>
<aff id="a2"><label>2</label><institution>School of Biosciences, University of Melbourne</institution></aff>
</contrib-group>
<pub-date pub-type="epub"><year>2018</year>
</pub-date>
<elocation-id>274035</elocation-id>
<history>
<date date-type="received">
<day>28</day>
<month>2</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>28</day>
<month>2</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>02</day>
<month>3</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="274035.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract><title>Abstract</title>
<p>Clustering techniques are widely used in the analysis of large data sets to group together samples with similar properties. For example, clustering is often used in the field of single-cell RNA-sequencing in order to identify different cell types present in a tissue sample. There are many algorithms for performing clustering and the results can vary substantially. In particular, the number of groups present in a data set is often unknown and the number of clusters identified by an algorithm can change based on the parameters used. To explore and examine the impact of varying clustering resolution we present clustering trees. This visualisation shows the relationships between clusters at multiple resolutions allowing researchers to see how samples move as the number of clusters increases. In addition, meta-information can be overlaid on the tree to inform the choice of resolution and guide in identification of clusters. We illustrate the uses of clustering trees using two examples, the classical iris dataset and a complex single-cell RNA-sequencing dataset.</p>
</abstract>
<kwd-group kwd-group-type="author"><title>Keywords</title>
<kwd>Clustering</kwd>
<kwd>Visualisation</kwd>
<kwd>scRNA-seq</kwd>
</kwd-group>
<counts>
<page-count count="8"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1"><title>Introduction</title>
<p>Clustering analysis is commonly used to group similar samples across a diverse range of applications. Typically, the goal of clustering is to form groups of samples that are more similar to each other than to samples in other groups. While fuzzy or soft clustering assigns each sample to every cluster with some probability, and hierarchical clustering forms a tree of samples, most methods form hard clusters where each sample is assigned to a single group. This goal can be achieved in a variety of ways, such as by considering the distances between sample (e.g. <italic>k</italic>-means<sup><xref rid="c1" ref-type="bibr">1</xref></sup>&#x2013;<sup><xref rid="c3" ref-type="bibr">3</xref></sup>, PAM<sup><xref rid="c4" ref-type="bibr">4</xref></sup>), areas of density across the dataset (e.g. DBSCAN<sup><xref rid="c5" ref-type="bibr">5</xref></sup>) or relationships to statistical distributions<sup><xref rid="c6" ref-type="bibr">6</xref></sup>.</p>
<p>In many cases the number of groups that should be present in a dataset is not known in advance and deciding the correct number of clusters to use is a significant challenge. For some algorithms, such as <italic>k</italic>-means clustering, the number of clusters must be explicitly provided. Other methods have parameters that, directly or indirectly, control the clustering resolution and therefore the number of clusters produced. While there are methods and statistics (such as the elbow method<sup><xref rid="c7" ref-type="bibr">7</xref></sup> or silhouette plots<sup><xref rid="c8" ref-type="bibr">8</xref></sup>) designed to help analysts decide which clustering resolution to use, they typically produce a single score which only considers a single set of samples or clusters at a time.</p>
<p>An alternative approach would be to consider clusterings at multiple resolutions and examine how samples change groupings as the number of clusters increases. This is the approach taken by the clustering tree visualisation we present here: (i) a dataset is clustered at multiple resolutions producing sets of cluster nodes, (ii) the overlap between clusters at adjacent resolutions is used to build edges, (iii) the resulting graph is presented as a tree. This tree can be used to examine how clusters are related to each other, which clusters are distinct and which are unstable. In the following sections we describe how we construct such a tree and present examples of trees built from a classical clustering dataset and a complex single-cell RNA-sequencing (scRNA-seq) dataset. The figures shown here can be produced in R using our publicly available clustree package.</p>
</sec>
<sec id="s2"><title>Building a clustering tree</title>
<p>To build a clustering tree, we start with a set of clusterings allocating samples to groups at several different resolutions. These could be produced using any hard-clustering algorithm that allows control of the number of clusters in some way. For example, this could be a set of samples clustered using <italic>k</italic>-means with <italic>k</italic> = 1, 2, 3 as shown in <xref rid="fig1" ref-type="fig">Figure 1</xref>. We sort these clusterings so that they are ordered by increasing resolution (<italic>k</italic>), then consider pairs of adjacent clusterings. Each cluster <italic>c</italic><sub><italic>k,i</italic></sub> (where <italic>i</italic> = 1, <italic>&#x2026;, n</italic> and <italic>n</italic> is the number of clusters at resolution <italic>k</italic>) is compared with each cluster <italic>c</italic><sub><italic>k</italic>&#x002B;1,<italic>j</italic></sub> (where <italic>j</italic> = 1, <italic>&#x2026;, m</italic> and <italic>m</italic> is the number of clusters at resolution <italic>k</italic> &#x002B; 1). The overlap between the two clusters is computed as the number of samples that are assigned to both <italic>c</italic><sub><italic>k,i</italic></sub> and <italic>c</italic><sub><italic>k</italic>&#x002B;1,<italic>j</italic></sub>. We next build a graph where each node is a cluster and each edge is an overlap between two clusters.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure"><label>Figure 1:</label>
<caption><p>Illustration of the steps required to build a clustering tree. First a dataset must be clustered at different resolutions. The overlap in samples between clusters at adjacent resolutions is computed and used to calculate the in-proportion for each edge. Finally the edges are filtered and the graph visualised as a tree.</p></caption>
<graphic xlink:href="274035_fig1.tif"/>
</fig>
<p>Many of the edges will be empty, for example in <xref rid="fig1" ref-type="fig">Figure 1</xref> no samples in Cluster A at <italic>k</italic> = 2 end up in Cluster B at <italic>k</italic> = 3. In some datasets there may also be edges that contain few samples. These edges are not informative and result in a cluttered tree. An obvious solution for removing uninformative, low-count edges is to filter them using a threshold on the number of samples they represent. However, in this case the count of samples is not the correct statistic to use because it favours edges at lower resolutions and those connecting larger clusters. Instead we define the in-proportion metric as the ratio between the number of samples on the edge and the number of samples in the cluster it goes towards. This metric shows the importance of the edge to the higher resolution cluster independently of the cluster size. We apply a threshold to the in-proportion in order to remove less informative edges.</p>
<p>The final graph is visualised using a tree layout. This places the cluster nodes in a series of layers where each layer is a different clustering resolution and edges show the transition of samples through those resolutions. Edges are coloured according to the number of samples they represent and the in-proportion metric is used to control the edge transparency, highlighting more important edges. By default, the size of nodes is adjusted according to the number of samples in the cluster and their colour indicates the resolution. The clustree package also includes options for controlling the aesthetics of nodes based on the attributes of samples in the clusters they represent.</p>
</sec>
<sec id="s3"><title>A simple example</title>
<p>To further illustrate how a clustering tree is built, we will work through an example using the classical iris dataset<sup><xref rid="c9" ref-type="bibr">9</xref></sup>. This dataset contains measurements of the sepal length, sepal width, petal length and petal width from 150 iris flowers, 50 from each of three species: <italic>Iris setosa, Iris versicolor</italic> and <italic>Iris virginica</italic>. The iris dataset is commonly used as example for both clustering and classification problems with the <italic>Iris setosa</italic> samples being significantly different to, and linearly separable from, the other samples. We have clustered this dataset using <italic>k</italic>-means clustering with <italic>k</italic> = 1, <italic>&#x2026;</italic>, 5 and produced the clustering tree shown in <xref rid="fig2" ref-type="fig">Figure 2A</xref>.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure"><label>Figure 2:</label>
<caption><p>Clustering trees based on <italic>k</italic>-means clustering of the iris dataset. In A, nodes are coloured according to the value of <italic>k</italic> and sized according to the number of samples they represent. Edges are coloured according to the number of samples (from blue representing few to yellow representing many) and the transparency adjusted according to the in-proportion, with stronger lines showing edges that are more important to the higher resolution cluster. Cluster labels are randomly assigned by the <italic>k</italic>-means algorithm. B shows the same tree with the node colouring changed to show the mean petal length of the samples in each cluster.</p></caption>
<graphic xlink:href="274035_fig2.tif"/>
</fig>
<p>We see that there is one branch of the tree that is clearly distinct (presumably representing <italic>Iris setosa</italic>), remaining unchanged regardless of the number of clusters. On the other side we see the cluster at <italic>k</italic> = 2 cleanly split into two clusters (presumably <italic>Iris versicolor</italic> and <italic>Iris virginica</italic>) at <italic>k</italic> = 3 but as we move to <italic>k</italic> = 4 and <italic>k</italic> = 5 we see clusters being formed from multiple branches with more low proportion edges. This kind of pattern indicates that the data has become over-clustered and we have begun to introduce artificial groupings. In this case we know that <italic>k</italic> = 3 is the correct choice but this is also the value that is suggested by this tree.</p>
<p>We can check our assumption that the distinct branch represents the <italic>Iris setosa</italic> samples and the other two clusters at <italic>k</italic> = 3 are <italic>Iris versicolor</italic> and <italic>Iris virginica</italic> by overlaying some known information about the samples. In <xref rid="fig2" ref-type="fig">Figure 2B</xref> we have coloured the nodes by the mean petal length of the samples they contain. We can now see that clusters in the distinct branch have the shortest petals, with Cluster 1 at <italic>k</italic> = 3 having an intermediate length and Cluster 3 the longest petals. This feature is known to separate the samples into the expected species with <italic>Iris setosa</italic> having the shortest petals on average, <italic>Iris versicolor</italic> an intermediate length and <italic>Iris virginica</italic> the longest.</p>
<p>Although this is a very simple example it still highlights some of the benefits of viewing a clustering tree. We get some indication of the correct clustering resolution by examining the edges and we can overlay known information to assess the quality of the clustering. For example, if we observed that all clusters had the same mean petal length it would suggest that the clustering has not been successful as we know this is an important feature that separates the species. We could potentially learn more by looking at which samples follow low proportion edges or overlaying a series of features to try and understand what causes particular clusters to split.</p>
</sec>
<sec id="s4"><title>Clustering trees for single-cell RNA-seq data</title>
<p>One field that has begun to make heavy use of clustering techniques is the analysis of single-cell RNA- sequencing (scRNA-seq) data. Single-cell RNA-sequencing is a recently developed technology that can measure how genes are expressed in thousands to millions of individual cells11. This technology has been rapidly adopted in fields like developmental biology and immunology where it is valuable to have information from single cells rather than measurements that are averaged across the many different cells in a sample using older RNA sequencing technologies. One of the key uses for scRNA-seq is to discover and interrogate the different cell types present in a sample of a complex tissue. In this situation, clustering is typically used to group similar cells based on their gene expression profiles. Differences in gene expression between groups can then be used to infer the identity or function of those cells<sup><xref rid="c12" ref-type="bibr">12</xref></sup>. The number of cell types in an scRNA-seq dataset can vary depending on factors such as the tissue being studied, its developmental or environmental state and the number of cells captured. Often the number of cells types is not known before the data is generated and some samples can contain dozens of clusters. Therefore, deciding which clustering resolution to use is an important consideration in this application.</p>
<p>As an example of how clustering trees can be used in the scRNA-seq context we consider a commonly used Peripheral Blood Mononuclear Cell (PBMC) dataset. This dataset was originally produced by 10x Genomics and contains 2700 peripheral blood monocuclear cells, representing a range of well-studied immune cell types<sup><xref rid="c13" ref-type="bibr">13</xref></sup>. We have analysed this dataset using the Seurat package<sup><xref rid="c14" ref-type="bibr">14</xref></sup>, a commonly used toolkit for scRNA-seq analysis, following the instructions in their tutorial with the exception of varying the clustering resolution parameter from zero to five (see methods). Seurat uses a graph-based clustering algorithm and the resolution parameter controls the partitioning of this graph, with higher values resulting in more clusters. The clustering trees produced from this analysis are shown in <xref rid="fig3" ref-type="fig">Figure 3</xref>.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure"><label>Figure 3:</label>
<caption><p>Two clustering trees of a dataset of 2700 Peripheral Blood Mononuclear Cells (PBMCs). A) results from clustering using Seurat with resolution parameters from zero to one. At a resolution of 0.1 we see the formation of four main branches, one of which continues to split up to a resolution of 0.5, after which there are only minor changes. B) resolutions from zero to five. At the highest resolutions we begin to see many low in-proportion edges indicating cluster instability. Seurat labels clusters according to their size with Cluster 0 being the largest.</p></caption>
<graphic xlink:href="274035_fig3.tif"/>
</fig>
<p>The clustering tree covering resolutions zero to one in steps of 0.1 (<xref rid="fig3" ref-type="fig">Figure 3A</xref>) shows that four main branches form at a resolution of just 0.1. One of these branches, starting with Cluster 3 at resolution 0.1, remains unchanged while the branch starting with Cluster 2 splits only once at a resolution of 0.4. Most of the branching occurs in the branch starting with Cluster 1 which consistently has sub-branches split off to form new clusters as the resolution increases. There are two regions of stability in this tree; at resolution 0.5-0.6 and resolution 0.7-1.0 where the branch starting at Cluster 0 splits in two.</p>
<p><xref rid="fig3" ref-type="fig">Figure 3B</xref> shows a clustering tree with a greater range of resolutions, from zero to five in steps of 0.5. By looking across this range we can see what happens when the algorithm is forced to produce more clusters than are likely to be truly present in this dataset. As over-clustering occurs we begin to see more low in-proportion edges and new clusters forming from multiple parent clusters. This suggests that those areas of the tree are unstable and that the new clusters being formed are unlikely to represent true groups in the dataset.</p>
<p>Known marker genes are commonly used to identify the cell types that specific clusters correspond to. Overlaying gene expression information onto a clustering tree provides an alternative view that can help to indicate when clusters containing pure cell populations are formed. <xref rid="fig4" ref-type="fig">Figure 4</xref> shows the PBMC clustering tree in <xref rid="fig3" ref-type="fig">Figure 3A</xref> overlaid with the expression of some known marker genes.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure"><label>Figure 4:</label>
<caption><p>Clustering trees of the PBMC dataset coloured according to the expression of known markers. The node colours indicate the average of the log<sub>2</sub> gene counts of samples in each cluster. CD19 (A) identifies B cells, CD14 (B) shows a population of monocytes, CD3D (C) is a marker of T cells and CCR7 (D) shows the split between memory and naive CD4 T cells.</p></caption>
<graphic xlink:href="274035_fig4.tif"/>
</fig>
<p>By adding this extra information, we can quickly identify some of the cell types. CD19 (<xref rid="fig4" ref-type="fig">Figure 4A</xref>) is a marker of B cells and is clearly expressed in the most distinct branch of the tree. CD14 (<xref rid="fig4" ref-type="fig">Figure 4B</xref>) is a marker of a type of monocyte, which becomes more expressed as we follow one of the central branches, allowing us to see which resolution identifies a pure population of these cells. CD3D (<xref rid="fig4" ref-type="fig">Figure 4C</xref>) is a general marker of T cells and is expressed in two separate branches, one which splits into low and high expression of CCR7 (<xref rid="fig4" ref-type="fig">Figure 4D</xref>), separating memory and naive CD4 T cells. By adding expression of known genes to a clustering tree, we can see if more populations can be identified as the clustering resolution is increased and if clusters are consistent with known biology. For most of the Seurat tutorial a resolution of 0.6 is used, but the authors note that by moving to a resolution of 0.8, a split can be achieved between memory and naive CD4 T cells. This is a split that could be anticipated by looking at the clustering tree.</p>
</sec>
<sec id="s5"><title>Discussion and conclusion</title>
<p>Clustering similar samples into groups is a useful technique in many fields, but often analysts are faced with the tricky problem of deciding which clustering resolution to use. Traditional approaches to this problem typically consider a single cluster or sample at a time and may rely on prior knowledge of sample labels. Here we present clustering trees, an alternative visualisation that shows the relationships between clusterings at multiple resolutions.</p>
<p>Clustering trees display how clusters are divided as resolution increases, which clusters are clearly separate and distinct, which are related to each other and how samples change groups as more clusters are produced. Although clustering trees can appear similar to the trees produced from hierarchical clustering there are several important differences. Hierarchical clustering considers the relationships between individual samples and doesn&#x2019;t provide an obvious way to form groups. In contrast, clustering trees are independent of any particular clustering method and show the relationships between distinct groups of samples, any of which could be used for further analysis.</p>
<p>To illustrate the uses of clustering trees we presented two examples, one using the classical iris dataset and a second based on a complex scRNA-seq dataset. Both examples demonstrate how a clustering tree can suggest the correct resolution to use and how overlaying extra information can help to validate those clusters. This is of particular use to scRNA-seq analysis as these datasets are often large, noisy and contain an unknown number of cell types.</p>
<p>Even when the number of clusters to choose is not a problem, clustering trees can be a valuable tool. They provide a compact, information dense, visualisation that can display summarised information across a range of clusters. By modifying the appearance of cluster nodes based on attributes of the samples they represent, clusterings can be evaluated and identities of clusters established. Clustering trees potentially have applications in many fields and in the future could be adapted to be more flexible, such as by accommodating fuzzy clusterings.</p>
</sec>
<sec id="s6"><title>Methods</title>
<p>The clustree software package is built for the R statistical programming language. It relies on the ggraph package (<ext-link ext-link-type="uri" xlink:href="https://github.com/thomasp85/ggraph">https://github.com/thomasp85/ggraph</ext-link>), which is itself built on the ggplot2<sup><xref rid="c15" ref-type="bibr">15</xref></sup> and tidygraph packages (<ext-link ext-link-type="uri" xlink:href="https://github.com/thomasp85/tidygraph">https://github.com/thomasp85/tidygraph</ext-link>). Clustering trees are displayed using the Reingold-Tilford tree layout<sup><xref rid="c16" ref-type="bibr">16</xref></sup> or the Sugiyama layout<sup><xref rid="c17" ref-type="bibr">17</xref></sup>, both available as part of the igraph package<sup><xref rid="c18" ref-type="bibr">18</xref></sup>.</p>
<p>The iris dataset is available as part of R. We clustered this dataset using the &#x201C;kmeans&#x201D; function in the stats package with values of <italic>k</italic> from one to five. Each value of <italic>k</italic> was clustered with a maximum of 100 iterations and with 10 random starting positions. The clustered iris dataset is available as part of the clustree package.</p>
<p>The PBMC dataset was downloaded from the Seurat tutorial page (<ext-link ext-link-type="uri" xlink:href="http://satijalab.org/seurat/pbmc3k_tutorial.html">http://satijalab.org/seurat/pbmc3k_tutorial.html</ext-link>) and this tutorial was followed for most of the analysis. Briefly cells were filtered based on the number of genes they express and the percentage of counts assigned to mitochondrial genes. The data was then log-normalised and 1838 variable genes identified. Potential confounding variables (number of unique molecular identifiers and percentage mitochondrial expression) were regressed from the dataset before performing principal component analysis on the identified variable genes. The first 10 principal components were then used to build a graph which was partitioned into clusters using Louvain modularity optimisation<sup><xref rid="c19" ref-type="bibr">19</xref></sup> with resolution parameters in the range zero to five, in steps of 0.1 between zero and one and then in steps of 0.5.</p>
</sec>
</body>
<back>
<sec id="s7"><title>Declarations</title>
<sec id="s7a"><title>Ethics</title>
<p>Not applicable.</p>
</sec>
<sec id="s7b" sec-type="availability"><title>Availability of data and materials</title>
<p>The clustree package is available from GitHub at <ext-link ext-link-type="uri" xlink:href="https://github.com/lazappi/clustree">https://github.com/lazappi/clustree</ext-link> and the code and datasets used for the analysis in this paper are available from <ext-link ext-link-type="uri" xlink:href="https://github.com/Oshlack/clustree-paper">https://github.com/Oshlack/clustree-paper</ext-link>. The clustered iris dataset is included as part of clustree and the PBMC dataset can be downloaded from the Seurat tutorial page (<ext-link ext-link-type="uri" xlink:href="http://satijalab.org/seurat/pbmc3k_tutorial.html">http://satijalab.org/seurat/pbmc3k_tutorial.html</ext-link>) or the paper GitHub repository.</p>
</sec>
<sec id="s7c" sec-type="COI-statement"><title>Competing interests</title>
<p>The authors declare no competing interests.</p>
</sec>
<sec id="s7d"><title>Funding</title>
<p>Luke Zappia is supported by an Australian Government Research Training Program (RTP) Scholarship. Alicia Oshlack is supported through a National Health and Medical Research Council Career Development Fellowship APP1126157. MCRI is supported by the Victorian Government&#x2019;s Operational Infrastructure Support Program.</p>
</sec>
</sec>
<ack><title>Acknowledgements</title>
<p>Thank you to Marek Cmero for providing comments on a draft of the manuscript.</p>
</ack>
<ref-list><title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Forgy</surname>, <given-names>W. E.</given-names></string-name> <article-title>Cluster analysis of multivariate data: Efficiency versus interpretability of classifications</article-title>. <source>Biometrics</source> <volume>21</volume>, <fpage>768</fpage>&#x2013;<lpage>769</lpage> (<year>1965</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="other"><string-name><surname>Macqueen</surname>, <given-names>J.</given-names></string-name> <article-title>Some methods for classification and analysis of multivariate observations</article-title>. <source>in In 5th berkeley symposium on mathematical statistics and probability</source> (<year>1967</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Lloyd</surname>, <given-names>S.</given-names></string-name> <article-title>Least squares quantization in PCM</article-title>. <source>IEEE Trans. Inf. Theory</source> <volume>28</volume>, <fpage>129</fpage>&#x2013;<lpage>137</lpage> (<year>1982</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="book"><string-name><surname>Kaufman</surname>, <given-names>L.</given-names></string-name> &#x0026; <string-name><surname>Rousseeuw</surname>, <given-names>P. J.</given-names></string-name> <chapter-title>Partitioning around medoids (program PAM)</chapter-title>. <source>in Finding groups in data</source> <fpage>68</fpage>&#x2013;<lpage>125</lpage> (<publisher-name>John Wiley &#x0026; Sons, Inc.</publisher-name>, <year>1990</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="book"><string-name><surname>Ester</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Kriegel</surname>, <given-names>H.-P.</given-names></string-name>, <string-name><surname>Sander</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Xu</surname>, <given-names>X.</given-names></string-name> <chapter-title>A density-based algorithm for discovering clusters a density-based algorithm for discovering clusters in large spatial databases with noise</chapter-title>. <source>in Proceedings of the second international conference on knowledge discovery and data mining</source> <fpage>226</fpage>&#x2013;<lpage>231</lpage> (<publisher-name>AAAI Press</publisher-name>, <year>1996</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Fraley</surname>, <given-names>C.</given-names></string-name> &#x0026; <string-name><surname>Raftery</surname>, <given-names>A. E.</given-names></string-name> <article-title>Model-Based clustering, discriminant analysis, and density estimation</article-title>. <source>J. Am. Stat. Assoc.</source> <volume>97</volume>, <fpage>611</fpage>&#x2013;<lpage>631</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Thorndike</surname>, <given-names>R. L.</given-names></string-name> <article-title>Who belongs in the family?</article-title> <source>Psychometrika</source> <volume>18</volume>, <fpage>267</fpage>&#x2013;<lpage>276</lpage> (<year>1953</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Rousseeuw</surname>, <given-names>P. J.</given-names></string-name> <article-title>Silhouettes: A graphical aid to the interpretation and validation of cluster analysis</article-title>. <source>J. Comput. Appl. Math.</source> <volume>20</volume>, <fpage>53</fpage>&#x2013;<lpage>65</lpage> (<year>1987</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Anderson</surname>, <given-names>E.</given-names></string-name> <article-title>The Irises of the Gaspe Peninsula</article-title>. <source>Bulletin of the American Iris Society</source> <volume>59</volume>, <fpage>2</fpage>&#x2013;<lpage>5</lpage> (<year>1935</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Fisher</surname>, <given-names>R. A.</given-names></string-name> <article-title>The use of multiple measurements in taxonomic problems</article-title>. <source>Ann. Eugen.</source> <volume>7</volume>, <fpage>179</fpage>&#x2013;<lpage>188</lpage> (<year>1936</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Tang</surname>, <given-names>F.</given-names></string-name> <etal>et al.</etal> <article-title>mRNA-Seq whole-transcriptome analysis of a single cell</article-title>. <source>Nat. Methods</source> <volume>6</volume>, <fpage>377</fpage>&#x2013;<lpage>382</lpage> (<year>2009</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Stegle</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Teichmann</surname>, <given-names>S. A.</given-names></string-name> &#x0026; <string-name><surname>Marioni</surname>, <given-names>J. C.</given-names></string-name> <article-title>Computational and analytical challenges in single-cell transcriptomics</article-title>. <source>Nat. Rev. Genet.</source> <volume>16</volume>, <fpage>133</fpage>&#x2013;<lpage>145</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Zheng</surname>, <given-names>G. X. Y.</given-names></string-name> <etal>et al.</etal> <article-title>Massively parallel digital transcriptional profiling of single cells</article-title>. <source>Nat. Commun.</source> <volume>8</volume>, <fpage>14049</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Satija</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Farrell</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Gennert</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Schier</surname>, <given-names>A. F.</given-names></string-name> &#x0026; <string-name><surname>Regev</surname>, <given-names>A.</given-names></string-name> <article-title>Spatial reconstruction of single-cell gene expression data</article-title>. <source>Nat. Biotechnol.</source> <volume>33</volume>, <fpage>495</fpage>&#x2013;<lpage>502</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="book"><string-name><surname>Wickham</surname>, <given-names>H.</given-names></string-name> <source>Ggplot2: Elegant graphics for data analysis</source>. (<publisher-name>Springer</publisher-name> <publisher-loc>New York</publisher-loc>, <year>2010</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Reingold</surname>, <given-names>E. M.</given-names></string-name> &#x0026; <string-name><surname>Tilford</surname>, <given-names>J. S.</given-names></string-name> <article-title>Tidier drawings of trees</article-title>. <source>IEEE Trans. Software Eng.</source> <volume>SE-7</volume>, <fpage>223</fpage>&#x2013;<lpage>228</lpage> (<year>1981</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Sugiyama</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Tagawa</surname>, <given-names>S.</given-names></string-name> &#x0026; <string-name><surname>Toda</surname>, <given-names>M.</given-names></string-name> <article-title>Methods for visual understanding of hierarchical system structures</article-title>. <source>IEEE Trans. Syst. Man Cybern.</source> <volume>11</volume>, <fpage>109</fpage>&#x2013;<lpage>125</lpage> (<year>1981</year>).</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Csardi</surname>, <given-names>G.</given-names></string-name> &#x0026; <string-name><surname>Nepusz</surname>, <given-names>T.</given-names></string-name> <article-title>The igraph software package for complex network research</article-title>. <source>InterJournal, Complex Systems</source> <volume>1695</volume>, <fpage>1</fpage>&#x2013;<lpage>9</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Blondel</surname>, <given-names>V. D.</given-names></string-name>, <string-name><surname>Guillaume</surname>, <given-names>J.-L.</given-names></string-name>, <string-name><surname>Lambiotte</surname>, <given-names>R.</given-names></string-name> &#x0026; <string-name><surname>Lefebvre</surname>, <given-names>E.</given-names></string-name> <article-title>Fast unfolding of communities in large networks</article-title>. <source>J. Stat. Mech.</source> <volume>2008</volume>, <fpage>P10008</fpage> (<year>2008</year>).</mixed-citation></ref>
</ref-list>
</back>
</article>
