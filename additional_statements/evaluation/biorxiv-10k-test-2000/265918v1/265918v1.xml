<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/265918</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Plant Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Crowdsourcing Image Analysis for Plant Phenomics to Generate Ground Truth Data for Machine Learning</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Siegel</surname>
<given-names>Zachary D</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zhou</surname>
<given-names>Naihui</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a9">9</xref>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zarecor</surname>
<given-names>Scott</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lee</surname>
<given-names>Nigel</given-names>
</name>
<xref ref-type="aff" rid="a7">7</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Campbell</surname>
<given-names>Darwin A</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Andorf</surname>
<given-names>Carson M</given-names>
</name>
<xref ref-type="aff" rid="a4">4</xref>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Nettleton</surname>
<given-names>Dan</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a6">6</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lawrence-Dill</surname>
<given-names>Carolyn J</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a8">8</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ganapathysubramanian</surname>
<given-names>Baskar</given-names>
</name>
<xref ref-type="aff" rid="a7">7</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1789-8000</contrib-id>
<name>
<surname>Friedberg</surname>
<given-names>Iddo</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a9">9</xref>
<xref ref-type="corresp" rid="cor1">&#x2020;</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Kelly</surname>
<given-names>Jonathan W</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">&#x2020;</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Psychology, Iowa State University</institution></aff>
<aff id="a2"><label>2</label><institution>Bioinformatics and Computational Biology Program, Iowa State University</institution></aff>
<aff id="a3"><label>3</label><institution>Department of Genetics, Development and Cell Biology, Iowa State University</institution></aff>
<aff id="a4"><label>4</label><institution>Agricultural Research Services, United States Department of Agriculture</institution></aff>
<aff id="a5"><label>5</label><institution>Department of Computer Science, Iowa State University</institution></aff>
<aff id="a6"><label>6</label><institution>Department of Statistics, Iowa State University</institution></aff>
<aff id="a7"><label>7</label><institution>Department of Mechanical Engineering, Iowa State University</institution></aff>
<aff id="a8"><label>8</label><institution>Department of Agronomy, Iowa State University</institution></aff>
<aff id="a9"><label>9</label><institution>Department of Veterinary Microbiology and Preventive Medicine, Iowa State University</institution></aff>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>&#x002A;</label><p>Joint first authors</p></fn>
<corresp id="cor1"><label>&#x2020;</label>Corresponding author</corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2018</year>
</pub-date>
<elocation-id>265918</elocation-id>
<history>
<date date-type="received">
<day>14</day>
<month>2</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>14</day>
<month>2</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>14</day>
<month>2</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="265918.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>The accuracy of machine learning tasks is critically dependent on high quality ground truth data. Therefore, in many cases, producing good ground truth data typically involves trained professionals; however, this can be costly in time, effort, and money. Here we explore the use of crowdsourcing to generate a large number of training data points of good quality. We explore an image analysis task involving the segmentation of corn tassels from images taken in a field setting. We explore the accuracy, speed and other quality metrics when this task is performed by students for academic credit, Amazon MTurk workers, and Master Amazon MTurk workers. We conclude that the Amazon MTurk and Master Mturk workers perform significantly better than the for-credit students, with no significant difference between the two MTurk worker types. The quality of the segmentation produced by Amazon MTurk workers rivals that of an expert worker. We provide best practices to assess the quality of ground truth data, and to compare data quality produced by different sources. We conclude that properly managed crowdsourcing can be used to establish large volumes of viable ground truth data at a low cost and high quality, especially in the context of high throughput plant phenotyping. We also provide several metrics for assessing the quality of the generated datasets.</p>
</abstract>
<counts>
<page-count count="19"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Crop genetics include basic research (what does this gene do?) and efforts to affect agricultural improvement (can I improve this trait?). Geneticists are primarily concerned with the former and plant breeders are concerned with the latter. A major difference in the perspectives between these groups is their interest in learning which genes underlie a trait of interest: whereas geneticists are generally interested in what genes do, breeders can treat the underlying genetics as opaque, selecting for useful traits by tracking molecular markers or via phenotypic selection directly (<xref rid="c1" ref-type="bibr">1</xref>).</p>
<p>Historically, the connections between plant genotype and phenotype were investigated through forward genetics approaches, which involve identifying a trait of interest, then carrying out experiments to identify which gene is responsible for that trait. With the advent of convenient mutagens, molecular genetics, bioinformatics, and high-performance computing, researchers were able to associate genotypes with phenotypes more easily via a reverse genetics approach: mutate genes, sequence them, then look for an associated phenotype.</p>
<p>However, the pursuit of forward genetics approaches is back on the table, given the even more recent availability of inexpensive image data collection and storage coupled with computational image processing and analysis. In addition, the potential for breeders to compute on phenotypes directly is enabled, thus allowing for the scope and scale of breeding gains to be driven by computational power. While high-throughput collection of forward genetic data is now feasible, we must now enable the <italic>analysis</italic> of phenotypic data in a high-throughput way. The first step in such analysis is to identify regions of interest as well as quantitative phenotypic traits from the images collected. Tang <italic>et al</italic> described a model to extract tassel out of one single corn plant photo through color segmentation (<xref rid="c21" ref-type="bibr">21</xref>). However, when images are taken under field conditions, classifying images using the same processing algorithm can yield sub-optimal results. Changes in illumination, perspective, or shading, as well as occlusion, debris, precipitation, and vibration of the imaging payload all result in large fluctuations in image quality and information content. Machine learning (ML) methods have shown exceptional promise in extracting information from such noisy and unstructured image data. Kurtulmu&#x015F; and Kavdir adopted a machine learning classifier, support vector machine (SVM), to identify tassel regions based on the binarization of color images (<xref rid="c10" ref-type="bibr">10</xref>). An increasing number of methods from the field of computer vision are recruited to extract phenotypic traits from field data (<xref rid="c18" ref-type="bibr">18</xref>, <xref rid="c24" ref-type="bibr">24</xref>). For example, fine-grained algorithms have been developed to not only identify tassel regions, but also identify tassel traits such as total tassel number, tassel length, width, etc. (<xref rid="c13" ref-type="bibr">13</xref>, <xref rid="c22" ref-type="bibr">22</xref>)</p>
<p>A necessary requirement for training ML models is the availability of labeled data. Labeled data consist of a large set of representative images with the desired features labeled or highlighted (hence the term labeled data). A large and accurate labeled data set, the <italic>ground truth</italic>, is required for training the algorithm. The focus of this project is the identification of corn tassels, which are complex structures, in field-acquired images (<xref ref-type="fig" rid="fig2">Figure 2</xref>). For this task, the labeling process includes defining a minimum rectangular bounding box around the tassel. While seemingly simple, drawing a bounding box does requires effort to ensure accuracy (<xref rid="c20" ref-type="bibr">20</xref>), and a good deal of time to generate a sufficiently large training set. Preparing such a dataset by a single user can be laborious and time consuming. To ensure accuracy, such a generated set should ideally be proofed by several people, adding more time, labor, and expense to the task.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><p>Overall schema of datasets (boxes) and processes (arrows) that led to the analyses (red).</p></caption>
<graphic xlink:href="265918_fig1.tif"/>
</fig>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><p>Example image used during training to demonstrate correct placement of bounding boxes around tassels</p></caption>
<graphic xlink:href="265918_fig2.tif"/>
</fig>
<p>One solution to the problem is to take a large cohort of untrained individuals to perform the task, and to compile and extract some plurality or majority of their answers as a training set. This approach, also known as crowdsourcing, has been used successfully many times to provide image-based information in diverse fields including astronomy, zoology, computational chemistry, among others (<xref rid="c2" ref-type="bibr">2</xref>, <xref rid="c3" ref-type="bibr">3</xref>, <xref rid="c9" ref-type="bibr">9</xref>, <xref rid="c14" ref-type="bibr">14</xref>).</p>
<p>Crop genetics research has a long history of crowdsourcing large-scale efforts. For geneticists interested in identifying a single individual plant with a particular mutation of interest, large screening fields are grown and groups of student workers are sent into fields to identify phenotypes of interest. Rates of success are often a single instance among thousands of plants. Similarly, plant breeders have used student workers to work in their fields to plant, carry out crosses, de-tassle, etc.</p>
<p>Students participate in experiments to learn about the research process and gain first-hand experience acting as participants. To manage these large university participant pools, cloud based software, such as the Sona system (<ext-link ext-link-type="uri" xlink:href="http://www.sona-systems.com">www.sona-systems.com</ext-link>), are routinely used to schedule experiment appointments and to link to web-based research materials before automatically granting credit to participants. University participant pools provide a unique opportunity for crowdsourcing on a minimal budget because participants are compensated with course credit rather than money.</p>
<p>In addition to students, workers can be recruited through commercial platforms, for instance the Amazon Mechanical Turk (MTurk) platform (<ext-link ext-link-type="uri" xlink:href="https://www.mturk.com/">https://www.mturk.com/</ext-link>). MTurk is one popular venue for crowd-sourcing data due to the large number of available workers and the relative ease with which tasks can be uploaded and payments disbursed. Methods for crowdsourcing data and estimates of quality have been available for years, and several recommendations have emerged from past work. For example, collecting multiple responses per image can account for natural variation and the relative skill of the untrained workers (<xref rid="c19" ref-type="bibr">19</xref>). Furthermore, a majority vote of MTurk workers can label images with similar accuracy to that of experts (<xref rid="c16" ref-type="bibr">16</xref>). Although those studies were limited to labeling categorical features of stock images, other studies have shown success with more complex stimuli. For example, MTurk workers were able to diagnose disease and identify the clinically relevant areas in images of human retinas with accuracy approaching that of medical experts (<xref rid="c14" ref-type="bibr">14</xref>). Amazon&#x2019;s MTurk is a particularly valuable tool for researchers because it provides incentives for high quality work. The offering party has the ability to restrict their task to only workers with a particular work history, or a more general criteria known as &#x2018;Master Turk&#x2019; status. The master title is a status given to workers by Amazon based on a series of criteria that Amazon believes to represent the overall quality of the worker. Amazon does not disclose those criteria.</p>
<p>The time and cost savings of using crowdsourced data are obvious, but crowdsourcing is only a viable solution if the output is sufficiently accurate. The goal of the current project was to test whether crowdsourcing image labels (also called tags) could yield a sufficient positive-data training set for ML from image-based phenotypes in as little as a single day. We focus on corn tassels for this effort (see <xref ref-type="fig" rid="fig3">Figure 3</xref>), but findings are anticipated to extend to other similar tasks in plant phenotyping.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><p>Left: Sample participant-drawn boxes. Right: The Red box is the gold standard box and black is a participant-drawn box</p></caption>
<graphic xlink:href="265918_fig3.tif"/>
</fig>
<p>In this project, we recruited three groups of people for our crowdsourcing tassel identification task, from the two online platforms, Sona and MTurk. The first group was students recruited through Sona (the Course Credit Group). The second group was Master-status Mechanical Turk workers who were paid, (the Master MTurkers group), and third group was non-master Mechanical Turk workers who were paid (the &#x201C;non-Master MTurkers&#x201D; group). The accuracy of the different groups&#x2019; tassel identification was evaluated against an expert-generated gold standard. These crowdsourced labelled images were then used as training data for a &#x201C;bag-of-features&#x201D; machine learning algorithm. The overall scheme of this project is shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p>
<p>We found that under the same fee structure, performance of Master and non-Master MTurkers was not significantly different, with a median performance accuracy (defined in <xref ref-type="sec" rid="s3a">Section 3.1</xref>) of 0.79. The course-credit group performed less well, with a median accuracy of 0.69. There was no significant decline in accuracy in any group over time, though there was a slight improvement in performance time in later images, since all groups showed an increase in speed over time. After training a ML algorithm using the three training sets, the performance was not significantly different, achieving an accuracy of 0.88. We conclude that crowdsourcing via MTurk can be useful for establishing ground truth for complex image analysis tasks in a short amount of time and that MTurkers&#x2019; performance exceeds that of students working for course credit.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Methods</title>
<sec id="s2a">
<label>2.1</label>
<title>Recruiting Participants</title>
<p>The course credit group included 30 participants recruited from the undergraduate psychology participant pool at Iowa State University. Individuals in the course credit group were recruited through the subject pool software Sona (<ext-link ext-link-type="uri" xlink:href="http://www.sona-systems.com">www.sona-systems.com</ext-link>), and were compensated with course credits. The master MTurkers included 65 master-qualified workers recruited through MTurk. The exact qualifications for master status are not published by Amazon, but are known to include work experience and employer ratings of completed work. Master MTurkers were paid &#x0024;8.00 to complete the task and the total cost was &#x0024;572.00. Finally, the non-master MTurkers pool included 66 workers with no qualification restriction, recruited through the Amazon Mechanical Turk website. Due to the nature of Amazon&#x2019;s MTurk system, it is not possible to recruit only participants who are <italic>not</italic> master qualified. However, the purpose of including the non-master MTurkers was to evaluate workers recruited without the additional fee imposed by Amazon for recruitment of Masters MTurkers. Non-master MTurkers were also paid &#x0024;8.00 to complete the task and the total cost was &#x0024;568.00. Note that the costs include Amazon&#x2019;s fees.</p>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Pilot Study</title>
<p>A short cropping task was initially administered to university students and master Turkers as a pilot study to test the viability of this project and task instructions. Each participant was presented with a participant-specific set of 40 images randomly chosen from 393 total images. The accuracy of participant labels helped designate Easy and Hard status for each image. Forty images were classified as &#x201C;easy to crop&#x201D;, and 40 as &#x201C;hard to crop&#x201D;, based on accuracy results of the pilot study. An expert who made gold standard boxes made adjustments to the Easy/Hard classifications based on personal experience. These 80 images were selected for the main study. As opposed to the pilot study, participants in the main study each received the same set of 80 images, with image order randomized separately for each participant. The results of the pilot study indicated that at least 40 images could be processed without evidence of fatigue so the number of images included in the main experiment was increased to 80. The pilot study also indicated, via user feedback, that a compensation rate of &#x0024;8.00 for the set of 80 images was acceptable to the MTurk participants.</p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Gold Standard</title>
<p>We define a <italic>gold standard box</italic> for a given tassel as the box with the smallest area among all bounding boxes that contain the entire tassel, a minimum bounding box. Gold-standard boxes were generated by a trained and experienced researcher. An expert cropped all 80 images then computationally minimized the boxes to be minimum bounding. These images were used to evaluate the labelling performance of crowdsourced workers, and should not be confused with the &#x2018;ground truth&#x2019; which were used to refer the labeled boxes used in training the ML model.</p>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>Materials and Procedure</title>
<p>We randomly selected the images used in this study from a large pool of images obtained as part of an ongoing maize phenomics project. The field images focused on a single row of corn captured by cameras set up as part of the field phenotyping of the maize Nested Association Mapping (<xref rid="c23" ref-type="bibr">23</xref>), using 456 cameras simultaneously, each camera imaging a set of 6 plants. Each camera took an image every 10 minutes during a two week growing period in August 2015 (<xref rid="c12" ref-type="bibr">12</xref>). Some image features varied, for example, due to weather conditions and visibility of corn stalks, but the tassels were always clearly visible. Images were presented through a Java applet linked by a web page hosted by Qualtrics (<ext-link ext-link-type="uri" xlink:href="http://www.qualtrics.com">www.qualtrics.com</ext-link>). After providing Informed Consent, participants viewed a single page with instructions detailing how to identify corn tassels and how to create a minimum bounding box around each tassel. Participants were first shown an example image with the tassels correctly bounded with boxes (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Below the example, participants read instructions on how to create, modify, and delete bounding boxes using the mouse. These instructions explained that an ideal bounding box should contain the entire tassel with as little additional image detail as possible. Additional instructions indicated that overlapping boxes and boxes containing other objects would sometimes be necessary and were acceptable as long as each box accurately encompassed the target tassel. Participants were also instructed to only consider tassels in the closest plant row, ignoring tassels from plants that appear to be more distant. After reading instructions, participants clicked to progress to the actual data collection. No further feedback or training were provided.</p>
<p>For each image, participants created a unique bounding box for each tassel by clicking and dragging the cursor. Participants could subsequently adjust the vertical or horizontal size of any drawn box by clicking and dragging on a box corner, and could adjust the position of any drawn box by clicking and dragging in the box body. Participants were required to place at least one box on each image before moving on to the next image. No upper limit was placed on the number of boxes. Returning to previous images was not allowed. The time required to complete each image was recorded in addition to locations and dimensions of user-drawn boxes.</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Crowdsourcing Accuracy Evaluation</title>
<sec id="s3a">
<label>3.1</label>
<title>Defining Precision and Recall</title>
<p>Consider any given participant-drawn box and gold standard box as in the right panel of <xref ref-type="fig" rid="fig3">Figure 3</xref>. Let <italic>PB</italic> be the area of the participant box, let <italic>GB</italic> be the area of the gold standard box, and let <italic>IB</italic> be the area of the intersection between the participant box and the gold standard box. Precision (<italic>Pr</italic>) is defined as <italic>IB/PB</italic>, and recall (<italic>Rc</italic>) is defined as <italic>IB/GB</italic>. Both <italic>Pr</italic> and <italic>Rc</italic> range from a minimum value of 0 (when the participant box and gold standard box fail to overlap) to a maximum value of 1. As an overall measure of performance for a participant box as an approximation to a gold standard box, we use the harmonic mean of precision and recall given by</p>
<disp-formula id="ueqn1">
<alternatives>
<graphic xlink:href="265918_ueqn1.gif"/>
</alternatives>
</disp-formula>
<p>Each participant box was matched to the gold standard box that maximized <italic>F</italic><sub>1</sub> across all gold standard boxes within the image containing the participant box. In the event that more than one participant box was matched to the same gold standard box, the participant box with the highest <italic>F</italic><sub>1</sub> value was assigned the <italic>Pr</italic>, <italic>Rc</italic>, and <italic>F</italic><sub>1</sub> values for that match, and the other participant boxes matching that same gold standard box were assigned <italic>Pr</italic>, <italic>Rc</italic>, and <italic>F</italic><sub>1</sub> values of zero. In the usual case of a one-to-one matching between participant boxes and gold standard boxes, each participant box was assigned the <italic>Pr</italic>, <italic>Rc</italic>, and <italic>F</italic><sub>1</sub> values associated with its matched gold standard box.</p>
<p>To summarize the performance of a participant on a particular image, <italic>F</italic><sub>1</sub> values across participant-drawn boxes were averaged to obtain a measure referred to as <italic>F<sub>mean</sub></italic>. This provides a dataset with one performance measurement for each combination of participant and image that we use for subsequent statistical analysis.</p>
</sec>
<sec id="s3b">
<label>3.2</label>
<title>Dataset Description</title>
<p>Of the 30 students recruited, 26 completed all 80 images. Of the 65 Master MTurkers recruited, 49 completed all images. Of the 66 non-master MTurkers recruited, 51 completed all images. Data collected from participants who did not complete the survey were not included in subsequent analyses.</p>
<p>As described in <xref ref-type="sec" rid="s3a">Section 3.1</xref>, precision and recall were calculated for each participant-drawn box. Density of precision recall pairs by group based on 61,888 participant-drawn boxes are shown in the heatmap visualization of <xref ref-type="fig" rid="fig4">Figures 4a, 4b</xref> and <xref ref-type="fig" rid="fig4">4c</xref>.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><p>Density of precision, recall and <italic>F<sub>mean</sub></italic> for all three groups</p></caption>
<graphic xlink:href="265918_fig4.tif"/>
</fig>
<p>High value precision-recall pairs are more common than low value precision-recall pairs in all three groups. Perfect recall values were especially common because participants tended to draw boxes that encompassed the minimum bounding box, presumably to ensure that the entire tassel was covered. <xref ref-type="fig" rid="fig4">Figure 4d</xref> shows the distribution of <italic>F<sub>mean</sub></italic> for the three groups.</p>
</sec>
<sec id="s3c">
<label>3.3</label>
<title>Testing for Performance Differences among Groups</title>
<p>We used a linear mixed-effects model analysis to test for performance differences among groups with the <italic>F<sub>mean</sub></italic> value computed for each combination of image and user as the response variable. The model included fixed effects for groups (Master MTurker, non-Master MTurker, course credit), random effects for participants nested within groups, and random effects for images. The <italic>mixed</italic> procedure available in SAS software was used to perform this analysis with the Kenward-Roger method (<xref rid="c8" ref-type="bibr">8</xref>) for computing standard errors and denominator degrees of freedom. The analysis shows significant evidence for differences among groups (<italic>p-</italic>value <italic>&#x003C;</italic> 0.0001). Furthermore, pairwise comparisons between groups (<xref ref-type="table" rid="tbl1">Table 1</xref>) show that both Master and non-Master MTurkers performed significantly better than undergraduate students performing the task for course credit. There was no significant performance difference between Master and non-Master MTurkers.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><p>Parameter estimates from the ANOVA with master MTurk group as baseline.</p></caption>
<graphic xlink:href="265918_tbl1.tif"/>
</table-wrap>
</sec>
<sec id="s3d">
<label>3.4</label>
<title>Time Usage and Fatigue</title>
<p>Participants took a median time of 26.43 seconds to complete an image, with the median time for the Master MTurker group at 30.02 seconds, non-Master MTurkers at 29.40 seconds, and the course credit student group at 16.86 seconds. The course credit group generally spent less time than both MTurker groups. It is worth noting that there is a large variance in time spent on each image, with the longest time for a single image at 15,484.63 seconds, and the shortest being 0.88 seconds. The very long image time was probably due to the participant taking a break after cropping part of the image and then coming back later to finish that image. <xref ref-type="fig" rid="fig5">Figure 5a</xref> shows the histogram of time per question in a log scale.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><p>Density of precision, recall and <italic>F<sub>mean</sub></italic> for all three groups</p></caption>
<graphic xlink:href="265918_fig5.tif"/>
</fig>
<p>There is a general downward trend in the time spent on each image over time. The trend is shown in <xref ref-type="fig" rid="fig5">Figure 5b</xref>, via linear regression on log time with mixed effects. Random effects for user and image were controlled. The trend is statistically significant in all three groups, with similar effect sizes. As a participant complete the next question, his or her average time per question is reduced by about 1&#x0025;, as shown by <xref ref-type="table" rid="tbl2">Table 2</xref>. By looking at the interaction term between participant group and question index, we were able to conclude that the reduced time effect is not significantly different between the Master MTurker and non-Master MTurker group (p&#x003D;0.6003), but is different between the course credit group and Master MTurker group (p&#x003D;0.0431). This difference is weakened in terms of course credit versus non-Master MTurker, with a p-value of 0.1086.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><p>Parameter estimates in linear mixed effects regression of time spent each image</p></caption>
<graphic xlink:href="265918_tbl2.tif"/>
</table-wrap>
<p>We also analyzed the change in accuracy, as measured by <italic>F<sub>mean</sub></italic> as the test progresses. <xref ref-type="fig" rid="fig5">Figure 5c</xref> shows that <italic>F<sub>mean</sub></italic> decreases slightly as the task progresses. The decreases are statistically significant (p <italic>&#x003C;</italic> 0.05) for all three groups. However, the effect sizes (average decrease in <italic>F<sub>mean</sub></italic> per round of image) for both MTurker groups are almost negligible, with Master MTurk group showing a 0.00080 decrease per image and Non-master group showing a 0.00027 decrease. Decrease in <italic>F<sub>mean</sub></italic> for the course credit group is only slightly more noticeable, at 0.00095.</p>
<p>The decreasing <italic>F<sub>mean</sub></italic> trend is statistically significant among the three groups, as is shown by <xref ref-type="table" rid="tbl3">Table 3</xref>. The table is obtained by fitting an interaction term in addition to the fixed effects: Question Ordinal Index and group, as well as the random effects.</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3:</label>
<caption><p>Type 1 Test of Fixed Effects</p></caption>
<graphic xlink:href="265918_tbl3.tif"/>
</table-wrap>
<p>To summarize the effect of image order, there was a subtle decline in <italic>F<sub>mean</sub></italic> and a larger decrease in image completion time as the survey progressed.</p>
<p>Another question of interest was whether image accuracy correlates with image completion time. Indeed, there was a slight increase in accuracy if one spent more time on an image, shown in <xref ref-type="fig" rid="fig5">Figure 5d</xref>. This correlation is statistically significant in all three groups. Again, effect sizes are too small to conclude that spending more time on a single image has a positive effect on accuracy of that image.</p>
<p>In conclusion, all three groups of participants spent less time on each image as the survey progressed, showing familiarity in the task. Although their performance in the task also decreases slightly overtime, the effects were almost negligible. This fatigue effect is most evident in the course credit group. This observation is confirmed by the positive correlation between time spent per question and accuracy.</p>
</sec>
<sec id="s3e">
<label>3.5</label>
<title>Image Difficulty</title>
<p>We obtained the Best Linear Unbiased Predictor (BLUP)(<xref rid="c5" ref-type="bibr">5</xref>) of each image in the above analyses to assess whether each image contributes to increased or decreased accuracy and time. BLUPs can be viewed as estimates corresponding to random effects, in our case, the eighty images. <xref ref-type="fig" rid="fig6">Figure 6</xref> is a scatter plot, with each point representing an image. The x-axis shows the BLUPs with regard to logtime. The higher the BLUP, the more this particular image contributes to increased time spent on each question. Similarly, the y-axis shows the BLUPS with regard to <italic>F<sub>mean</sub></italic>. The higher the BLUP, the more it contributes to increased accuracy. We have also obtained a difficult / easy classification of all eighty images from our expert who manually curated the gold standard boxes, as they are shown by the two different colors on the plot.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Figure 6:</label>
<caption><p>Best Linear Unbiased Predictors for image in analyses for <italic>F<sub>mean</sub></italic> and time in log scale. Color represents image difficulty determined by expert</p></caption>
<graphic xlink:href="265918_fig6.tif"/>
</fig>
<p>It is interesting to observe that longer time spent on a question positively correlates with accuracy. Indeed, the linear regression fit shown as the red line on the plot has an estimate of 0.1003 (<italic>p</italic>&#x003D;0.00136), and an adjusted <italic>R</italic><sup>2</sup> of 0.1127, suggesting weak correlation. It is even more interesting to observe that the images that our expert considered difficult did not take participants longer to complete, nor did they yield significantly lower accuracy. The images are shown to participants in a random order, eliminating the possibility that fatigue contributes to the longer time it takes to complete easy images. Since previous analysis showed that participants tend to spend less time on images shown to them later (<xref ref-type="fig" rid="fig5">Figure 5b</xref>), this may be evidence to suggest an ordering of the images so that more difficult images are shown to the participants first, to take advantage of the fact that participants tend to spend more time on each image in the beginning, to ensure optimal accuracy results.</p>
</sec>
</sec>
<sec id="s4">
<label>4</label>
<title>Machine Learning Accuracy Evaluation</title>
<p>Each of the 126 participants who completed this study labelled a set of 80 images. Each of these sets was used as training set in a bag-of-features (<xref rid="c15" ref-type="bibr">15</xref>) machine learning model. These models were then tested on a new set of labelled images to generate accuracy. Due to algorithmic differences, the accuracy metric in evaluating the ML performance was not comparable with the <italic>F<sub>mean</sub></italic> accuracy. Accuracy was calculated based on the average of the True positive rate and True negative rate for each set of images from the participants. Overall the algorithm achieved an accuracy of 0.8811. For the master and non-master MTurker groups, the average accuracy rates are 0.8851 and 0.8781 respectively. For the course credit group, it was 0.8795. A linear regression was performed to determine whether there is group difference between the machines&#x2019; performance. The F test yielded a p-value of 0.7325, indicating there is no detectable difference between the machine&#x2019;s performance when it comes to different training data.</p>
</sec>
<sec id="s5">
<label>5</label>
<title>Discussion</title>
<p>Machine learning methods have proven useful for processing images for inclusion in various databases. However, these algorithms still require an initial training set created by expert individuals before structures can be automatically extracted from the image and labeled. This project has identified crowdsourcing as a viable method for creating these initial training sets without the time consuming and costly work of an expert. These results indicate that straightforward tasks, such tassel cropping, do not benefit from the extra fee assessed to hire master over non-master MTurkers. Performance between the two groups was not significantly different, and non-master MTurkers can safely be hired without compromising data quality.</p>
<p>The MTurk platform allows for fast collection of data within a day instead of one to two weeks. While MTurk may be one of the most popular crowdsourcing platforms, many universities possess a research participant pool that compensates students with class credit instead of cash for their work. If the image tagging task meets an Institutional Review Board (IRB) approval, students could be tapped to tag images for course credit and further reduce the cost of sourcing data. However, such undergraduate student participant pool performed more poorly than either of the MTurker groups. While it is possible that MTurk workers are simply more conscientious than college students, it is also possible that monetary compensation is a better motivator than course credit. In addition to the direct monetary reward, both groups of MTurkers were also motivated by either working towards or maintaining the &#x201C;master&#x201D; status. Such implicit motivational mechanisms might be useful in setting up a long-term crowdsourcing platform. The distinction in labelling performance between MTurkers and students does not persist when considering the actual outcome of interest: how well the machine learning algorithm identifies corn tassels when supplied with each of the three training sets. Indeed, the accuracy of machine performance is not affected by the quality of the training set provided, which were manually-labelled through crowdsourcing. Therefore, a student participant pool with a non-monetary rewards system provides the opportunity for an alternate model by lowering overall image tagging cost. This would allow additional features to be tagged or a larger number of responses to be sourced with existing funding levels and further database expansion.</p>
<p>Indeed, many non-monetary crowdsourcing projects already exist. For example, the <italic>Backyard Worlds: Planet 9</italic> project hosted by NASA for search of planets and star systems in space (<xref rid="c9" ref-type="bibr">9</xref>), the <italic>Phylo</italic> (<ext-link ext-link-type="uri" xlink:href="http://phylo.cs.mcgill.ca/">http://phylo.cs.mcgill.ca/</ext-link>) game for multiple sequence alignment (<xref rid="c7" ref-type="bibr">7</xref>) and <italic>fold.it</italic> (<ext-link ext-link-type="uri" xlink:href="http://fold.it">http://fold.it</ext-link>) (<xref rid="c3" ref-type="bibr">3</xref>) for protein folding. These projects do not offer monetary rewards but instead attract participants by offering the chance to contribute to real scientific research. This concept has been categorized as citizen science, where nonprofessional scientists participate in crowdsourced research efforts. In addition to the attraction of the subject matter, these projects often have very interactive and entertaining interfaces to quickly engage people&#x2019;s interests and attention, as well as providing extensive demonstrations. Some of them were even designed as games, and competition mechanisms such as rankings provide extra motivation. Another important purpose of such citizen science projects is to educate the public about the subject matter. Given the current climate regarding Genetically Modified Organisms (GMOs), crowdsourcing efforts of crop phenomic and phenotypic research could potentially be a gateway to the better understanding of plant research in the general public. A recent effort has shown that non-experts can be used for accurate image-based plant phenomics annotation tasks (<xref rid="c4" ref-type="bibr">4</xref>). However, the authors have pointed out to the challenge of non-monetary reward in sustaining a large-scale annotation effort.</p>
<p>Phenomics is concerned with the quantitative and qualitative study of phenomes, where all possible traits of a given organism varies in response to genetic mutations and environmental influences (<xref rid="c6" ref-type="bibr">6</xref>). An important field of research in phenomics is the development of high-throughput technology analogous to high-throughput sequencing in genetics and genomic studies, to enable the collection of large-scale data with minimal efforts. A lot of phenotypic traits could be recorded with images, and databases such as BioDIG (<xref rid="c17" ref-type="bibr">17</xref>) makes the connection of such image data with genomic information, providing genetics researchers with tools to examine the relationship between the two types of data directly. Hence, the computation and manipulation of such phenomic image data becomes essential. In plant biology, maize is central for both basic biological research as well as crop production (reviewed in (<xref rid="c11" ref-type="bibr">11</xref>)). As such, phenotypic information derived from ear (female flowers) and tassel (male flowers) are key to both the study of genetics and crop productivity: flowers are where meiosis and fertilization occur as well as the source of grain. To add a new features such as tassel emergence, size, branch number, branch angle and anthesis to the systems such as BioDIG, the specific tassel location and structure should be located, and our solution to this task, is to use crowdsourcing combined with machine learning, to reduce cost and time of such a pipeline, while expanding its utility. Our findings and suggested crowdsourcing methods can be generally applied to other phenomic analysis tasks.</p>
<p>We hope our study will help establish some best practices for researchers in setting up such a crowd-sourcing study. Given the ease and relatively low cost of obtaining data through Amazon&#x2019;s Mechanical Turk, we recommend it over the undergraduate research pool. That being said, student research pools would be a suitable method for obtaining proof of concept or pilot data to support a grant proposal.</p>
</sec>
<sec id="s6" sec-type="funding">
<label>6</label>
<title>Funding</title>
<p>This work was supported primarily by an award from the Iowa State University Presidential Interdisciplinary Research Initiative to support the D3AI (Data-Driven Discovery for Agricultural Innovation) project. For more information, see <ext-link ext-link-type="uri" xlink:href="http://www.d3ai.iastate.edu/">http://www.d3ai.iastate.edu/</ext-link>. Additional support came from the Iowa State University Plant Sciences Institute Faculty Scholars Program and the USDA Agricultural Research Service. IF was funded, in part, by National Science Foundation award ABI 1458359. DN, BG and CJLD gratefully acknowledge Iowa State University&#x2019;s Plant Sciences Institute Scholars program funding.</p>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><given-names>M B</given-names>. <surname>Tierney</surname></string-name> and <string-name><given-names>K H</given-names>. <surname>Lamour</surname></string-name>. <source>An Introduction to Reverse Genetic Tools for Investigating Gene Function</source>. <volume>01</volume> <year>2005</year>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><string-name><given-names>&#x00D6;zg&#x00FC;n</given-names> <surname>Emre Can</surname></string-name>, <string-name><given-names>Neil</given-names> <surname>D&#x2019;Cruze</surname></string-name>, <string-name><given-names>Margaret</given-names> <surname>Balaskas</surname></string-name>, and <string-name><given-names>David</given-names> <surname>W Macdonald</surname></string-name>. <article-title>Scientific crowdsourcing in wildlife research and conservation: Tigers (panthera tigris) as a case study</article-title>. <source>PLoS biology</source>, <volume>15</volume>(<issue>3</issue>):<fpage>e2001001</fpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><given-names>Seth</given-names> <surname>Cooper</surname></string-name>, <string-name><given-names>Firas</given-names> <surname>Khatib</surname></string-name>, <string-name><given-names>Adrien</given-names> <surname>Treuille</surname></string-name>, <string-name><given-names>Janos</given-names> <surname>Barbero</surname></string-name>, <string-name><given-names>Jeehyung</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>Michael</given-names> <surname>Beenen</surname></string-name>, <string-name><given-names>Andrew</given-names> <surname>Leaver-Fay</surname></string-name>, <string-name><given-names>David</given-names> <surname>Baker</surname></string-name>, <string-name><given-names>Zoran</given-names> <surname>Popovi&#x0107;</surname></string-name>, <etal>et al.</etal> <article-title>Predicting protein structures with a multiplayer online game</article-title>. <source>Nature</source>, <volume>466</volume>(<issue>7307</issue>):<fpage>756</fpage>&#x2013;<lpage>760</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names>. <surname>Valerio Giuffrida</surname></string-name>, <string-name><given-names>Feng</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Hanno</given-names> <surname>Scharr</surname></string-name>, and <string-name><given-names>Sotirios A</given-names>. <surname>Tsaftaris</surname></string-name>. <article-title>Citizen crowds and experts: observer variability in image-based plant phenotyping</article-title>. <source>Plant Methods</source>, <volume>14</volume>(<issue>1</issue>):<fpage>12</fpage>, Feb <year>2018</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><string-name><given-names>Charles R</given-names> <surname>Henderson</surname></string-name>. <article-title>Best linear unbiased estimation and prediction under a selection model</article-title>. <source>Biometrics</source>, pages <fpage>423</fpage>&#x2013;<lpage>447</lpage>, <year>1975</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><string-name><given-names>David</given-names> <surname>Houle</surname></string-name>, <string-name><given-names>Diddahally R</given-names> <surname>Govindaraju</surname></string-name>, and <string-name><given-names>Stig</given-names> <surname>Omholt</surname></string-name>. <article-title>Phenomics: the next challenge</article-title>. <source>Nature reviews genetics</source>, <volume>11</volume>(<issue>12</issue>):<fpage>855</fpage>&#x2013;<lpage>866</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><string-name><given-names>Alexander</given-names> <surname>Kawrykow</surname></string-name>, <string-name><given-names>Gary</given-names> <surname>Roumanis</surname></string-name>, <string-name><given-names>Alfred</given-names> <surname>Kam</surname></string-name>, <string-name><given-names>Daniel</given-names> <surname>Kwak</surname></string-name>, <string-name><given-names>Clarence</given-names> <surname>Leung</surname></string-name>, <string-name><given-names>Chu</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>Eleyine</given-names> <surname>Zarour</surname></string-name>, <string-name><given-names>Luis</given-names> <surname>Sarmenta</surname></string-name>, <string-name><given-names>Mathieu</given-names> <surname>Blanchette</surname></string-name>, <string-name><given-names>J&#x00E9;r&#x00F4;me</given-names> <surname>Waldisp&#x00FC;hl</surname></string-name>, <etal>et al.</etal> <article-title>Phylo: a citizen science approach for improving multiple sequence alignment</article-title>. <source>PloS one</source>, <volume>7</volume>(<issue>3</issue>):<fpage>e31362</fpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><string-name><given-names>Michael G</given-names> <surname>Kenward</surname></string-name> and <string-name><given-names>James H</given-names> <surname>Roger</surname></string-name>. <article-title>Small sample inference for fixed effects from restricted maximum likelihood</article-title>. <source>Biometrics</source>, pages <fpage>983</fpage>&#x2013;<lpage>997</lpage>, <year>1997</year>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><given-names>Marc</given-names> <surname>Kuchner</surname></string-name>. <article-title>Backyard worlds: Finding nearby brown dwarfs through citizen science</article-title>. <source>NASA ADAP Proposal</source>, <year>2017</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><string-name><given-names>Ferhat</given-names> <surname>Kurtulmu&#x015F;</surname></string-name> and <string-name><given-names>Ismail</given-names> <surname>Kavdir</surname></string-name>. <article-title>Detecting corn tassels using computer vision and support vector machines</article-title>. <source>Expert Systems with Applications</source>, <volume>41</volume>(<issue>16</issue>):<fpage>7390</fpage>&#x2013;<lpage>7397</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><string-name><given-names>Carolyn J</given-names> <surname>Lawrence</surname></string-name>, <string-name><given-names>Qunfeng</given-names> <surname>Dong</surname></string-name>, <string-name><given-names>Mary L</given-names> <surname>Polacco</surname></string-name>, <string-name><given-names>Trent E</given-names> <surname>Seigfried</surname></string-name>, and <string-name><given-names>Volker</given-names> <surname>Brendel</surname></string-name>. <article-title>Maizegdb, the community database for maize genetics and genomics</article-title>. <source>Nucleic Acids Research</source>, <volume>32</volume>(<issue>suppl 1</issue>):<fpage>D393</fpage>&#x2013;<lpage>D397</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="book"><string-name><given-names>Nigel</given-names> <surname>Lee</surname></string-name>. <chapter-title>High-throughput phenotyping of above and below ground elements of plants using feature detection, extraction and image analysis techniques</chapter-title>. <publisher-name>Master&#x2019;s thesis, Iowa State University</publisher-name>, <year>2016</year>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><string-name><given-names>Hao</given-names> <surname>Lu</surname></string-name>, <string-name><given-names>Zhiguo</given-names> <surname>Cao</surname></string-name>, <string-name><given-names>Yang</given-names> <surname>Xiao</surname></string-name>, <string-name><given-names>Zhiwen</given-names> <surname>Fang</surname></string-name>, <string-name><given-names>Yanjun</given-names> <surname>Zhu</surname></string-name>, and <string-name><given-names>Ke</given-names> <surname>Xian</surname></string-name>. <article-title>Fine-grained maize tassel trait characterization with multi-view representations</article-title>. <source>Computers and Electronics in Agriculture</source>, <volume>118</volume>:<fpage>143</fpage>&#x2013;<lpage>158</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><given-names>Danny</given-names> <surname>Mitry</surname></string-name>, <string-name><given-names>Kris</given-names> <surname>Zutis</surname></string-name>, <string-name><given-names>Baljean</given-names> <surname>Dhillon</surname></string-name>, <string-name><given-names>Tunde</given-names> <surname>Peto</surname></string-name>, <string-name><given-names>Shabina</given-names> <surname>Hayat</surname></string-name>, <string-name><given-names>Kay-Tee</given-names> <surname>Khaw</surname></string-name>, <string-name><given-names>James E</given-names> <surname>Morgan</surname></string-name>, <string-name><given-names>Wendy</given-names> <surname>Moncur</surname></string-name>, <string-name><given-names>Emanuele</given-names> <surname>Trucco</surname></string-name>, and <string-name><given-names>Paul J</given-names> <surname>Foster</surname></string-name>. <article-title>The accuracy and reliability of crowdsource annotations of digital retinal images</article-title>. <source>Translational vision science &#x0026; technology</source>, <volume>5</volume>(<issue>5</issue>):<fpage>6</fpage>&#x2013;<lpage>6</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="book"><string-name><given-names>Eric</given-names> <surname>Nowak</surname></string-name>, <string-name><given-names>Fr&#x00E9;d&#x00E9;ric</given-names> <surname>Jurie</surname></string-name>, and <string-name><given-names>Bill</given-names> <surname>Triggs</surname></string-name>. <chapter-title>Sampling strategies for bag-of-features image classification. In Ale&#x0161; Leonardis, Horst Bischof, and Axel Pinz, editors, <italic>Computer Vision &#x2013; ECCV 2006</italic></chapter-title>, pages <fpage>490</fpage>&#x2013;<lpage>503</lpage>, <publisher-loc>Berlin, Heidelberg</publisher-loc>, <year>2006</year>. <publisher-name>Springer Berlin Heidelberg</publisher-name>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="book"><string-name><given-names>Stefanie</given-names> <surname>Nowak</surname></string-name> and <string-name><given-names>Stefan</given-names> <surname>R&#x00FC;ger</surname></string-name>. <chapter-title>How reliable are annotations via crowdsourcing: a study about inter-annotator agreement for multi-label image annotation. In <italic>Proceedings of the international conference on Multimedia information retrieval</italic></chapter-title>, pages <fpage>557</fpage>&#x2013;<lpage>566</lpage>. <publisher-name>ACM</publisher-name>, <year>2010</year>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><string-name><given-names>Andrew T</given-names> <surname>Oberlin</surname></string-name>, <string-name><given-names>Dominika A</given-names> <surname>Jurkovic</surname></string-name>, <string-name><given-names>Mitchell F</given-names> <surname>Balish</surname></string-name>, and <string-name><given-names>Iddo</given-names> <surname>Friedberg</surname></string-name>. <article-title>Biological database of images and genomes: tools for community annotations linking image and genomic information</article-title>. <source>Database</source>, 2013, <year>2013</year>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><string-name><given-names>Arti</given-names> <surname>Singh</surname></string-name>, <string-name><given-names>Baskar</given-names> <surname>Ganapathysubramanian</surname></string-name>, <string-name><given-names>Asheesh</given-names> <surname>Kumar Singh</surname></string-name>, and <string-name><given-names>Soumik</given-names> <surname>Sarkar</surname></string-name>. <article-title>Machine learning for high-throughput stress phenotyping in plants</article-title>. <source>Trends in plant science</source>, <volume>21</volume>(<issue>2</issue>):<fpage>110</fpage>&#x2013;<lpage>124</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="confproc"><string-name><given-names>Alexander</given-names> <surname>Sorokin</surname></string-name> and <string-name><given-names>David</given-names> <surname>Forsyth</surname></string-name>. <article-title>Utility data annotation with amazon mechanical turk</article-title>. In <source>Computer Vision and Pattern Recognition Workshops, 2008. CVPRW&#x2019;08</source>. <conf-name>IEEE Computer Society Conference on</conf-name>, pages <fpage>1</fpage>&#x2013;<lpage>8</lpage>. IEEE, <year>2008</year>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><string-name><given-names>Hao</given-names> <surname>Su</surname></string-name>, <string-name><given-names>Jia</given-names> <surname>Deng</surname></string-name>, and <string-name><given-names>Li</given-names> <surname>Fei-Fei</surname></string-name>. <source>Crowdsourcing annotations for visual object detection</source>, <volume>volume WS-12-08</volume>, pages <fpage>40</fpage>&#x2013;<lpage>46</lpage>. <year>2012</year>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="book"><string-name><given-names>Wenbing</given-names> <surname>Tang</surname></string-name>, <string-name><given-names>Yane</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Dongxing</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Wei</given-names> <surname>Yang</surname></string-name>, and <string-name><given-names>Minzan</given-names> <surname>Li</surname></string-name>. <chapter-title>Corn tassel detection based on image processing. In <italic>2012 International Workshop on Image Processing and Optical Engineering</italic></chapter-title>, <volume>volume 8335</volume>, page <fpage>83350J</fpage>. <publisher-name>International Society for Optics and Photonics</publisher-name>, <year>2011</year>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><string-name><given-names>Mengni</given-names> <surname>Ye</surname></string-name>, <string-name><given-names>Zhiguo</given-names> <surname>Cao</surname></string-name>, and <string-name><given-names>Zhenghong</given-names> <surname>Yu</surname></string-name>. <source>An image-based approach for automatic detecting tasseling stage of maize using spatio-temporal saliency. In Proc. of SPIE Vol</source>, <volume>volume 8921</volume>, pages <fpage>89210Z</fpage>&#x2013;<lpage>1</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><string-name><given-names>Jianming</given-names> <surname>Yu</surname></string-name>, <string-name><given-names>James B</given-names>. <surname>Holland</surname></string-name>, <string-name><given-names>Michael D</given-names>. <surname>McMullen</surname></string-name>, and <string-name><given-names>Edward S</given-names>. <surname>Buckler</surname></string-name>. <article-title>Genetic design and statistical power of nested association mapping in maize</article-title>. <source>Genetics</source>, <volume>178</volume>(<issue>1</issue>):<fpage>539</fpage>&#x2013;<lpage>551</lpage>, January <year>2008</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><string-name><given-names>Jiaoping</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Hsiang</given-names> <surname>Sing Naik</surname></string-name>, <string-name><given-names>Teshale</given-names> <surname>Assefa</surname></string-name>, <string-name><given-names>Soumik</given-names> <surname>Sarkar</surname></string-name>, <string-name><given-names>RV</given-names> <surname>Chowda Reddy</surname></string-name>, <string-name><given-names>Arti</given-names> <surname>Singh</surname></string-name>, <string-name><given-names>Baskar</given-names> <surname>Ganapathysubramanian</surname></string-name>, and <string-name><given-names>Asheesh K</given-names> <surname>Singh</surname></string-name>. <article-title>Computer vision and machine learning for robust phenotyping in genome-wide studies</article-title>. <source>Scientific Reports</source>, <volume>7</volume>, <year>2017</year>.</mixed-citation></ref>
</ref-list>
</back>
</article>