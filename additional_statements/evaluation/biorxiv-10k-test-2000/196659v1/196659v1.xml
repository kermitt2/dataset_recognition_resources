<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/196659</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Assessment of the need for separate test set and number of medical images necessary for deep learning: a sub-sampling study</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Rokem</surname>
<given-names>Ariel</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wu</surname>
<given-names>Yue</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lee</surname>
<given-names>Aaron</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>The eScience Institute</institution>, University of Washington</aff>
<aff id="a2"><label>2</label><institution>University of Washington</institution>, Department of Ophthalmology</aff>
</contrib-group>
<pub-date pub-type="epub">
<year>2017</year>
</pub-date>
<elocation-id>196659</elocation-id>
<history>
<date date-type="received">
<day>30</day>
<month>9</month>
<year>2017</year>
</date>
<date date-type="rev-recd">
<day>30</day>
<month>9</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>01</day>
<month>10</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="196659.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Deep learning algorithms have tremendous potential utility in the classification of biomedical images. For example, images acquired with retinal optical coherence tomography (OCT) can be used to accurately classify patients with adult macular degeneration (AMD), and distinguish them from healthy control patients. However, previous research has suggested that large amounts of data are required in order to train deep learning algorithms, because of the large number of parameters that need to be fit. Here, we show that a moderate amount of data (data from approximately 1,800 patients) may be enough to reach close-to-maximal performance in the classification of AMD patients from OCT images. These results suggest that deep learning algorithms can be trained on moderate amounts of data, provided that images are relatively homogenous, and the effective number of parameters is sufficiently small. Furthermore, we demonstrate that in this application, cross-validation with a separate test set that is not used in any part of the training does not differ substantially from cross-validation with a validation data-set used to determine the optimal stopping point for training.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Index Terms</title>
<kwd>Opthalmology</kwd>
<kwd>Retina</kwd>
<kwd>Optical Coherence Tomography</kwd>
<kwd>Macula</kwd>
<kwd>Deep learning</kwd>
<kwd>Machine Learning</kwd>
</kwd-group>
<counts>
<page-count count="7"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<label>I.</label>
<title>Introduction</title>
<p>DEEP learning (DL) algorithms [<xref rid="c1" ref-type="bibr">1</xref>] have been tremendously successful at solving a variety of different computational tasks. Although these algorithms were originally developed to perform computer vision tasks that require the identification and classification of natural objects in images [<xref rid="c2" ref-type="bibr">2</xref>], they have been more recently successfully applied in tasks as varied as automated captioning and description of images and videos [<xref rid="c3" ref-type="bibr">3</xref>], [<xref rid="c4" ref-type="bibr">4</xref>], automated transcription of spoken language [<xref rid="c5" ref-type="bibr">5</xref>] automated translation [<xref rid="c6" ref-type="bibr">6</xref>], and in playing games such as Go [<xref rid="c7" ref-type="bibr">7</xref>] and Poker [<xref rid="c8" ref-type="bibr">8</xref>], beating even highly seasoned players in these games.</p>
<p>A key to the success of these algorithms in image processing is that they do not require feature engineering of their front-end filters. Instead, these filters are empirically learned from the data through a process of training. For example, a network is trained to identify natural objects by exposing it to labeled exemplars of images containing the classes to be discriminated. The weights that define the front-end filters, and their pooling in higher levels are automatically adjusted through gradient descent. Progress in the implementation of the computation of the gradients required for this process on graphical processing units (GPUs) has been instrumental in enabling use of these techniques on large and complex data-sets. No less important have been the discovery of network architectures [<xref rid="c9" ref-type="bibr">9</xref>], nonlinearities [<xref rid="c10" ref-type="bibr">10</xref>], regularization procedures [<xref rid="c11" ref-type="bibr">11</xref>] and initialization procedures [<xref rid="c12" ref-type="bibr">12</xref>] that accelerate and improve learning. Taken together, these factors have ushered in an era where training of large DL networks has become practical, and there is wide-spread interest in applying these algorithms to a variety of different tasks.</p>
<sec id="s1a">
<label>A.</label>
<title>Deep Learning in medical imaging</title>
<p>Because DL algorithms were originally developed to perform difficult image processing and classification tasks, one of the compelling avenues for application of DL is in the analysis of data from medical imaging technologies, and the development of computer-assisted diagnostic systems with DL-trained networks at their core. This type of application is rapidly becoming more realistic because of the combination of high-quality biomedical imaging technologies that are becoming common in clinical practice, and the development of large data-sets for the training of these networks. These large data-sets are the result of years of accumulation of electronic medical records (EMR), data-bases that include both image data, as well as expert-generated diagnostic labels. Several recent studies used data from such data-bases in tandem with DL networks to demonstrate the potential for highly accurate automated diagnosis of diseases from images of retina [<xref rid="c13" ref-type="bibr">13</xref>] and of skin [<xref rid="c14" ref-type="bibr">14</xref>].</p>
<p>In previous work, we demonstrated that a DL algorithm is capable of accurate diagnosis of age-related macular degeneration (AMD) from images of the retina acquired with optical coherence tomography (OCT) [<xref rid="c15" ref-type="bibr">15</xref>]. OCT images are taken at a high resolution, and provide information about the three-dimensional structure of the tissue. They routinely collected during clinical ophthalmological visits, and are normally used by clinicians to assess the presence of retinal diseases. They are stored in an EMR, and the combination of the diagnostic labels available in the EMR database, together with the images of retinae from both healthy individuals and AMD patients allowed us to train a DL network with the VGG16 architecture, previously used for object recognition [<xref rid="c16" ref-type="bibr">16</xref>], to distinguish OCT scans from retinae of patients with AMD from healthy retinae with an accuracy of 88.98&#x0025; (ROC AUC of 93.83&#x0025;, peak sensitivity 92.64 &#x0025;, peak specificity 93.69 &#x0025;).</p>
</sec>
<sec id="s1b">
<label>B.</label>
<title>How many samples do you need?</title>
<p>Previous applications of DL in medical imaging, including our own work, relied on large data sets, that are not available for many other technologies, and for diseases that are less common. A major barrier to the wide-spread application of DL algorithms in medical imaging is the assumption that these algorithms only work well when data is extremely abundant, and that supervised learning can progress using accurate labels of each image<sup><xref rid="fn1" ref-type="fn">1</xref></sup>. Indeed, in our previous work using DL for OCT image classification, the network was trained with a data-set of &#x007E;100,000 images. Similarly, other studies have used data-bases with many thousands of patients and up to millions of individual images.</p>
<p>To our knowledge, there is only one previous study asking how many samples are needed in biomedical image classification [<xref rid="c18" ref-type="bibr">18</xref>]. The authors of this study trained a DL network to discriminate between six classes of images (brain, neck, shoulder, etc.) from MRI images. They found that only a few hundred images are required to reach near-perfect accuracy in this task using the GoogLeNet network. However, images of these body parts differ in many respects, and it is not clear that a much simpler algorithm would perform just as well in this classification task.</p>
<p>In the present work we focused on a classification task in which DL algorithms are required to perform more accurately than traditional image processing methods [<xref rid="c19" ref-type="bibr">19</xref>]. We introduce a resampling procedure to test the size of the sample needed in order to train a DL network on a biomedical image classification task, and use this procedure in order to assess the number of samples needed to train a network to accurately discriminate between AMD and healthy retinae from OCT.</p>
</sec>
<sec id="s1c">
<label>C.</label>
<title>Cross-validation and the importance of a separate test set</title>
<p>To avoid over-fitting, and to provide an objective and accurate evaluation of the performance of a classification algorithm, it is common to separate the data into several different sets: a <italic>training set</italic> is used to learn the dependencies between input data and model class labels, and to adjust the parameters of the model. A <italic>validation set</italic> is sometimes used to assess the current state of the model during training. This is done by feeding a sample or samples from the validation set through the algorithm, with a fixed set of parameters, and evaluating the accuracy of the classification with these parameter values, but without using the results to adjust the parameters.</p>
<p>Often, an additional data set is set aside as <italic>test set</italic>. This set is used once the learning has ended, as a single independent estimate of the endpoint of learning. While using an independent data set completely guards against the danger of over-fitting, it also might introduce the danger of variability in the estimate of error, especially with a relatively small size of the test set [<xref rid="c20" ref-type="bibr">20</xref>].</p>
<p>In cross-validation, different parts of the data might serve separately as training and validation data sets [<xref rid="c21" ref-type="bibr">21</xref>]. For example, in k-fold cross-validation training is repeated several times, where in each iteration through the procedure, a portion of <italic>N/k</italic> samples from the data are designated as a validation set, and the remaining data is used for training. After <italic>k</italic> repetitions of this procedure, all the data has been used up as validation data. This means that a full set of errors on the entire data-set has been computed. This procedure is sometimes used for comparative evaluation of different models, and for model selection (by comparing cross-validation errors for two or more models) [<xref rid="c22" ref-type="bibr">22</xref>].</p>
<p>While this procedure is comprehensive, and potentially reduces variability of the estimates, it is also computationally demanding. For this reason, training of DL algorithms usually uses the strategy of training on a sub-set of the data and then using other sub-sets for evaluation and testing. Furthermore, given the large amounts of data that are often available for training and evaluation, the final test step is often omitted in applications of DL. For example, in their highly influential paper on image classification from the ImageNet dataset, Krizhevsky et al. [<xref rid="c23" ref-type="bibr">23</xref>] comment that: &#x201C; In the remainder of this paragraph, we use validation and test error rates interchangeably because in our experience they do not differ by more than 0.1&#x0025;&#x201D;.</p>
<p>The resampling scheme introduced here also presents an opportunity to evaluate whether this statement generalizes well to situations in which data is much less abundant. Therefore, a second aim of the present paper is to assess the use of a separate test in cross-validation of a DL network.</p>
</sec>
</sec>
<sec id="s2">
<label>II.</label>
<title>Methods</title>
<p>This study was approved by the Institutional Review Board of the University of Washington (UW) and adhered to the tenets of the Declaration of Helsinki and the Health Insurance Portability and Accountability Act.</p>
<sec id="s2a">
<label>A.</label>
<title>Optical Coherence Tomography Imaging and Electronic Medical Record Extraction</title>
<p>Macular OCT scans were acquired in the course of clinical care, using a Heidelberg Spectralis OCT scanner (Heidelberg Engineering, Heidelberg, Germany). High-resolution images of the retinal cross-section were obtained using a 61-line raster scan. All of the images from the period 2006 to 2016 were extracted using an automated extraction tool from the instrument imaging database. The images were linked by patient medical record number and dates to the clinical data stored in EPIC. Specifically, all clinical diagnoses and the dates of every clinical encounter, macular laser procedure, and intravitreal injection were extracted from the EPIC Clarity tables.</p>
</sec>
<sec id="s2b">
<label>B.</label>
<title>Patient and Image Selection</title>
<p>A normal patient was defined as having no retinal International Classification of Diseases, 9th Revision (ICD-9) diagnosis and better than 20/30 vision in both eyes during entirety of their recorded clinical history at UW. An AMD patient was defined as having an ICD-9 diagnosis of AMD (codes 362.50, 362.51, and 362.52) by a retina specialist, at least 1 intravitreal injection in either eye, and worse than 20/30 vision in the better-seeing eye. Patients with other macular pathology by ICD-9 code were excluded. These parameters were chosen <italic>a priori</italic> to ensure that macular pathology was most likely present in both eyes in the AMD patients and absent in both eyes in the normal patients. Consecutive images of patients meeting these criteria were included, and no images were excluded due to image quality. Labels from the EMR were then linked to the OCT macular images, and the data were stripped of all protected health identifiers.</p>
<p>As most of the macular pathology is concentrated in the foveal region, the decision was made <italic>a priori</italic> to select the central 11 images from each macular OCT set, and each image was then treated independently, and labeled as either normal or AMD. The images were histogram equalized and the resolution down-sampled to 192 by 124 pixels to accommodate RAM limitations.</p>
</sec>
<sec id="s2c">
<label>C.</label>
<title>Deep Learning Classification Model</title>
<p>A modified version of the VGG16 convolutional neural network [<xref rid="c16" ref-type="bibr">16</xref>] was implemented using Caffe [<xref rid="c24" ref-type="bibr">24</xref>]. This network was originally designed to classify categories in natural images and was adapted here to classify healthy and AMD retinae. Weights were initialized using the Xavier algorithm [<xref rid="c12" ref-type="bibr">12</xref>]. Training was then performed using multiple iterations, each with a batch size of 100 images. ADAM optimization [<xref rid="c25" ref-type="bibr">25</xref>] was used with a starting learning rate of 2 &#x00D7; 10<sup>-7</sup> and the momentum parameters set to 0.9 and 0.99. The loss of the model was recorded at each training iteration, and cross-validation with a separate validation set was conducted every 250 iterations. The training was stopped when the loss of the model decreased and the accuracy of the validation set also decreased (indicating that the model was in the over-fitting regime).</p>
</sec>
<sec id="s2d">
<label>D.</label>
<title>Sub-sampling experiments</title>
<p>At the outset of the experiments, a random subset of 10&#x0025; of the images were segregated at the patient level into a separate <italic>test set</italic> of images. These would be used to test the performance of the DL network at the end of training. The remaining 90&#x0025; of the images were then segregated into 11 replicates of random subsets of 4&#x0025;, 8&#x0025;, 16&#x0025;, 32&#x0025;, 64&#x0025;, and 100&#x0025; of the available images. Within each subset, the images were again subdivided into 75&#x0025; for training and 25&#x0025; for validation. Care was taken to ensure that the validation set and the training set contained images from a mutually exclusive group of patients (ie, no single patient contributed images to both the training and validation sets). The order of images of the training set was then randomized in each replication condition.</p>
<p>Each replication condition was then trained for a total of 75,000 iterations and the maximal validation accuracy was recorded. The weights at the time of the maximal validation accuracy was used to assess the performance of the network against the held-out test set.</p>
</sec>
</sec>
<sec id="s3">
<label>III.</label>
<title>Results</title>
<sec id="s3a">
<label>A.</label>
<title>Data and subsampling</title>
<p>More than 2.6 million optical coherence tomography (OCT) images were extracted from the imaging database and linked to clinical data from the electronica medical records (EMR). A total of 48,312 normal OCT scans and 52,690 AMD scans met the inclusion criteria for use in the training set. At the outset, a test set was set aside comprising of 9,493 images, to be used only for evaluation of the training procedure once it is done.</p>
<p>To test the effect of sample size on accuracy of classification with a DL network, random subsets of 4&#x0025;, 8&#x0025;, 16&#x0025;, 32&#x0025;, 64&#x0025;, and 100&#x0025; were created from the full dataset, and training was conducted using these different subset sizes. The breakdown in the number of images in each subset is described in <xref rid="tbl1" ref-type="table">Table I</xref>.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>TABLE I.</label>
<caption><p>Average Image Counts for Each Subset Condition</p></caption>
<graphic xlink:href="196659_tbl1.tif"/>
</table-wrap>
</sec>
<sec id="s3b">
<label>B.</label>
<title>Learning with different size subsamples</title>
<p>To assess the robustness of the results to the random selection of specific images, random subsamples of each one of these proportions were drawn from the full data-set 11 times. Training of the DL network in each repetition was allowed to progress for 75,000 iterations. Learning curves, recording validation accuracy during training are shown in <xref rid="fig1" ref-type="fig">Figure 1</xref>.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig. 1.</label>
<caption><p>Learning curves for each subset size (percent). In each sub-plot, validation accuracy is plotted against number of training iterations. Each of the repetitions is plotted in light blue, and the average across repetitions is plotted in dark blue. Maximal validation accuracy in each course of training is plotted as a light blue point.</p></caption>
<graphic xlink:href="196659_fig1.tif"/>
</fig>
</sec>
<sec id="s3c">
<label>C.</label>
<title>Test accuracy as a function of subsample size</title>
<p>For each training run, the weights from the training history with the maximal validation accuracy were stored. The model was then assessed with these weights against the held out test set, yielding 11 test accuracy estimates for each proportion, as seen in <xref rid="fig2" ref-type="fig">Figure 2</xref>. As expected, test accuracy increased with sample size, reaching its maximal value at subsamples of 100 &#x0025; (&#x007E;86 &#x0025; accuracy).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig. 2.</label>
<caption><p>Accuracy on held out test set. Weights from the highest validation accuracy during training were used to test accuracy on a held out test set. Dark blue points are the means across 11 repetitions, with dark blue standard deviation error bars. Orange line: a logistic curve model was fit to all of the repetitions and sub-samples. According to this model, 95&#x0025; of maximal accuracy (&#x007E;82 &#x0025; accuracy) can be achieved with 20.84&#x0025; of the data (dashed lines).</p></caption>
<graphic xlink:href="196659_fig2.tif"/>
</fig>
<p>Though accuracy is far from chance even with only 4 &#x0025; of the data (&#x007E;73&#x0025; correct) it does increase precipitously between 4&#x0025; and 64&#x0025; of the data. However, it reaches close-to-maximal accuracy already at a proportion 16 - 32&#x0025; of the total data-set. To quantify the amount of data needed to reach 95&#x0025; of the maximal accuracy, we fit a two-parameter logistic function to the accuracy values across repetitions and sub-sample proportions (orange line in <xref rid="fig2" ref-type="fig">figure 2</xref>), fixing the saturation point of the function to be equal to the mean accuracy at subsamples of 100 &#x0025; of the data. Inverting this function, we find that 95&#x0025; of the maximal accuracy (approximately 82&#x0025; accuracy) can already be achieved with 20&#x0025; of the data (dashed lines in 2).</p>
</sec>
<sec id="s3d">
<label>D.</label>
<title>What explains variability between repetitions?</title>
<p>Variability of test accuracy also diminished substantially with subsample size. Differences in variability in the comparisons across different proportions. These differences in variability could reflect two different factors: the first is the subsample size, and the other is the degree of overlap between different subsamples. For example, for 100 &#x0025; subsamples, variability reflects only the random initial conditions of the network, because all subsamples of 100 &#x0025; are identical. Similarly, the overlap between different subsets in higher proportions is likely to be larger than in smaller proportions. To evaluate the effect of this overlap, we conducted a separate experiment in which a single subset from 4&#x0025; group was used, and training was repeated 11 times using the same set of images, to control for this effect. The learning curves from this protocol are are shown in <xref rid="fig3" ref-type="fig">Figure 3B</xref>, together with the learning curves from random subsamples (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). The learning curves have similar variance as when random subsets are used and the maximal validation accuracy (<xref rid="fig3" ref-type="fig">Figure 3</xref> C) does not differ between these protocols. This indicates that variability in test-set accuracy mostly relates to the size of the subsample, rather than to the amount of overlap between different subsets.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig. 3.</label>
<caption><p>Learning curves of 4&#x0025; subset. <bold>A:</bold> Random 4&#x0025; subsets of the whole dataset chosen in each course of learning. <bold>B:</bold> Replications of the same 4&#x0025; susample were repeated in each course of training. <bold>C:</bold> The average maximal validation accuracies (across the 11 courses of training) are plotted for random (left) and fixed (subsets) of 4&#x0025; each, with standard deviation error bars</p></caption>
<graphic xlink:href="196659_fig3.tif"/>
</fig>
</sec>
<sec id="s3e">
<label>E.</label>
<title>The importance of a separate test set</title>
<p>To assess the importance of the separate test set, we computed the difference between the maximum validation accuracy and test set accuracy (<xref rid="fig4" ref-type="fig">Figure 4</xref>). We find that though variability in this difference decreases with larger sample size, there is no indication that this difference is larger for smaller sample sizes. In addition, there seems to be no overall bias indicating that the accuracy is systematically higher for the validation set, relative to the test set. This indicates that in the training protocol that we used, there was probably only minimal overfitting, or none at all.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig. 4.</label>
<caption><p>Difference between validation and test accuracy. Each light colored point represents a single course of training. The abscissa represents the maximal validation accuracy achieved during this course of training, while the ordinate represents the accuracy of the network in performing the classification on a separate test set, using the same weights that resulted in maximal validation accuracy. Solid colored points each represent the mean across 11 courses of training for each size subset, with standard deviation error bars. Dashed line indicates equality.</p></caption>
<graphic xlink:href="196659_fig4.tif"/>
</fig>
<p>[fig3]</p>
</sec>
</sec>
<sec id="s4">
<label>IV.</label>
<title>Conclusion</title>
<p>The application of DL in biomedical imaging is a promising avenue in current research. Future developments in this field may lead to accurate computer-assisted diagnosis systems with DL networks at their core. However, a major impediment to these developments is the assumption that DL requires very large data-sets that are not available for many types of nascent imaging technologies, or in the case of diseases that are relatively uncommon.</p>
<p>In the present work, we investigated the feasibility of DL with relatively small sample sizes. We focused on a prototype of computer-assisted diagnosis system that can accurately discriminate optical coherence tomography (OCT) images from retinae of patients with age-related macular degeneration AMD, relative to OCT images from the retinae of healthy controls.</p>
<sec id="s4a">
<label>A.</label>
<title>How many images do we need to discriminate AMD from healthy retina?</title>
<p>We found that training a DL network to perform at a high level of accuracy does not require millions of images. Instead, close-to-maximal performance is achieved with as few as approximately 20,000 images. Furthermore, the moderate increase in accuracy from 64&#x0025; to 100&#x0025; of the data suggests that further increase in data size would not result in much higher accuracy. This suggests that &#x007E;87&#x0025; accuracy is as high as possible with these data and this DL algorithm. This limit on performance may be related to data quality; the partial accuracy of the labels in the EMR: these data are heterogeneous and ultimately depends on clinical decisions made by human observers, as well as the limited signal-to-noise ratio of the images in capturing the image features that are diagnostic. However, we do expect further performance improvements to come from more elaborate algorithms that incorporate additional information, or make better use of the information in the images, rather than only from more or better data.</p>
<p>The relatively small number of images required to train a DL network on this classification task is surprising given the VGG16 network has as many as 138M parameters [<xref rid="c9" ref-type="bibr">9</xref>]. Indeed, previous literature using similar networks (e.g. [<xref rid="c2" ref-type="bibr">2</xref>],[<xref rid="c16" ref-type="bibr">16</xref>]) used many millions of training samples to reach high accuracy. The discrepancy between our findings and the previous literature may stem from the differences between the use-case we present here, and the common use-cass for DL in previous literature. The assumption that many items from each class are required and that many millions of separate images are needed to train DL algorithms stems from the object classification literature mentioned in the introduction, but object classification in natural images addresses several challenges that are not typical in the classification of medical images, and particularly clinical images from OCT. Primary to these challenges is the variance in pose and orientation of natural objects within photographic images, which leads to large variance in the appearance of these objects. To capture all the variations of a category (e.g., &#x2018;dog&#x2019;), a DL network would have to be exposed to many thousands of exemplars of this category, generalizing not over all the angles from which this category could be captured, but also all the sub-categories of this category (e.g., &#x2018;malamut&#x2019; or &#x2018;poodle&#x2019;). This variance in input is much more limited in biomedical images, such as OCT. In OCT images, the retina is always oriented in exactly the same direction, with the macula (the center of the retina) usually located in roughly the same part of the image. This reduces the complexity of training substantially, and we hypothesized that it might affect the data requirements for learning on data such as these.</p>
<p>Nevertheless, given the two-alternative classification task performed here, this finding is roughly consistent with the rule of thumb described by [<xref rid="c17" ref-type="bibr">17</xref>](&#x201C; &#x2026; 5,000 labeled exemplars per category&#x2026;&#x201D;). The sub-sampling method introduced here provides a protocol for researchers that are interested in asking whether they have enough data to apply DL to their biomedical image data.</p>
<p>Note that because there are 11 images used in each OCT volume, the &#x007E;20,000 images represent approximately 1800 volumes of data, or 900 patients per group. This number of patients is well within range for many traditional random controlled trials and other clinical studies. This suggests that <italic>de novo</italic> training of DL networks could be integrated into many studies that are testing new imaging technologies, or that are studying less common disorders.</p>
</sec>
<sec id="s4b">
<label>B.</label>
<title>Other strategies</title>
<p>There are currently two major alternative strategies to use for cases where data is limited. Data augmentation synthetically increases the sample size by performing transformations on the data [<xref rid="c26" ref-type="bibr">26</xref>]. This works well as long as the transformations performed to do not destroy the information necessary for classification, but introduce variability against which the DL network should develop tolerance. For clinical imaging data, examples of such transformations might be rigid translations and rotations of the image features.</p>
<p>The other strategy one might use when faced with limited data is transfer learning. This strategy is based on the observation that DL algorithms trained for different image processing tasks often learn very similar first-stage filters [<xref rid="c27" ref-type="bibr">27</xref>]. Therefore, in this approach learning begins with one (larger) data set. This dataset may share only some limited similarity to the datasets that are ultimately of interest, but this phase of learning allows the network to converge on good enough front-end filters. Once learning in this phase has converged, this network is then retrained on the dataset of interest. While this approach is promising, it is not clear what its limitations are, and whether it would work well for specific biomedical image processing tasks.</p>
<p>Both of these approaches are powerful complements to datasets that are not large enough, but even before employing these strategies, practitioners might want to assess whether the amount of data that they already have might be sufficient to accurately learn the classification task at hand.</p>
</sec>
<sec id="s4c">
<label>C.</label>
<title>Do we need a separate test set?</title>
<p>Variance between different courses of training increased substantially with reduced sample sizes. This variance is not due to sampling of different individual items &#x2014; both average accuracy and variance in accuracy between repetitions do not change substantially when the same items are repeatedly used in different subsamples.</p>
<p>Given the limit on test-set accuracy, one might expect that cross-validation on a separate test-set would be crucial, and that results in the test-set might differ substantially from the best performance on a validation set [<xref rid="c28" ref-type="bibr">28</xref>]. Nevertheless, we found no systematic difference in accuracy assessment on a separate test set relative to assessment of accuracy on a validation set that is repeatedly used during training. This finding is consistent with previous anecdotal evidence that has been mentioned in the literature [<xref rid="c23" ref-type="bibr">23</xref>].</p>
<p>Both of these findings may arise from the large number of parameters that are fit through the DL algorithm. The learning procedure may thus converge to different solutions based on initial conditions. When data is small, this may result in divergent solutions, that depend on the randomly generated initial conditions of the network. For larger networks, this implies that overfitting is not induced through repeated use of a validation data-set in accuracy assessments. However, further research would be needed to assess the limits of this conclusion and to merit its broad application.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>ACKNOWLEDGMENT</title>
<p>We would like to acknowledge NVIDIA Corporation for their generous donation of hardware in support of this research.A. Lee and Y. Wu were supported by an unrestricted grant by the Research to Prevent Blindness to the University of Washington. A. Rokem was supported through a grant from the Gordon &#x0026; Betty Moore Foundation and the Alfred P. Sloan Foundation to the University of Washington eScience Institute Data Science Environment.</p>
</ack>
<ref-list>
<title>REFERENCES</title>
<ref id="c1"><label>1</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>LeCun</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name>, and <string-name><given-names>G.</given-names> <surname>Hinton</surname></string-name>, &#x201C; <article-title>Deep learning</article-title>,&#x201D; <source>Nature</source>, vol. <volume>521</volume>, no. <issue>7553</issue>, pp. <fpage>436</fpage>&#x2013;<lpage>444</lpage>, 28 <month>May</month> 2015.</mixed-citation></ref>
<ref id="c2"><label>2</label><mixed-citation publication-type="book"><string-name><given-names>A.</given-names> <surname>Krizhevsky</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Sutskever</surname></string-name>, and <string-name><given-names>G. E.</given-names> <surname>Hinton</surname>,</string-name> &#x201C; <chapter-title>ImageNet classi cation with deep convolutional neural networks</chapter-title>,&#x201D; in <source>Advances in Neural Information Processing Systems 25</source>, <person-group person-group-type="editor"><string-name><given-names>F.</given-names> <surname>Pereira</surname></string-name>, <string-name><given-names>C. J. C.</given-names> <surname>Burges</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Bottou</surname></string-name>, and <string-name><given-names>K. Q.</given-names> <surname>Weinberger</surname></string-name></person-group>, Eds. <publisher-name>Curran Associates, Inc</publisher-name>., <volume>2012</volume>, pp. <fpage>1097</fpage>&#x2013;<lpage>1105</lpage>.</mixed-citation></ref>
<ref id="c3"><label>3</label><mixed-citation publication-type="book"><string-name><given-names>A.</given-names> <surname>Karpathy</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Joulin</surname></string-name>, and <string-name><given-names>F. F. F.</given-names> <surname>Li</surname></string-name>, &#x201C; <chapter-title>Deep fragment embeddings for bidirectional image sentence mapping</chapter-title>,&#x201D; in <source>Advances in Neural Information Processing Systems 27</source>, <person-group person-group-type="editor"><string-name><given-names>Z.</given-names> <surname>Ghahramani</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Welling</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Cortes</surname></string-name>, <string-name><given-names>N. D.</given-names> <surname>Lawrence</surname></string-name>, and <string-name><given-names>K. Q.</given-names> <surname>Weinberger</surname></string-name></person-group>, Eds. <publisher-name>Curran Associates, Inc</publisher-name>., <year>2014</year>, pp. <fpage>1889</fpage>&#x2013;<lpage>1897</lpage>.</mixed-citation></ref>
<ref id="c4"><label>4</label><mixed-citation publication-type="other"><string-name><given-names>J.</given-names> <surname>Donahue</surname></string-name>, <string-name><given-names>L. A.</given-names> <surname>Hendricks</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Rohrbach</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Venugopalan</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Guadarrama</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Saenko</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Darrell</surname></string-name>, <article-title>&#x201C; Long-term recurrent convolutional networks for visual recognition and description,&#x201D;</article-title> 17 Nov. <year>2014</year>.</mixed-citation></ref>
<ref id="c5"><label>5</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Hannun</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Case</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Casper</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Catanzaro</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Diamos</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Elsen</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Prenger</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Satheesh</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Sengupta</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Coates</surname></string-name>, and <string-name><given-names>A. Y.</given-names> <surname>Ng</surname></string-name>, <source>&#x201C; Deep speech: Scaling up end-to-end speech recognition,&#x201D;</source> 17 <month>Dec</month>. 2014.</mixed-citation></ref>
<ref id="c6"><label>6</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Schuster</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>Q. V.</given-names> <surname>Le</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Norouzi</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Macherey</surname></string-name>,<string-name><given-names>M.</given-names> <surname>Krikun</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Cao</surname></string-name>, <string-name><given-names>Q.</given-names> <surname>Gao</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Macherey</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Klingner</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Shah</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Johnson</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Liu</surname></string-name>, <string-name><given-names>&#x0141;.</given-names> <surname>Kaiser</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Gouws</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Kato</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Kudo</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Kazawa</surname></string-name>,<string-name><given-names>K.</given-names> <surname>Stevens</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Kurian</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Patil</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Young</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Smith</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Riesa</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Rudnick</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Vinyals</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Corrado</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Hughes</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Dean</surname></string-name>, <article-title>&#x201C; Google&#x2019;s neural machine translation system</article-title>: <source>Bridging the gap between human and machine translation,&#x201D;</source> <volume>26</volume> <month>Sep</month>. <year>2016</year>.</mixed-citation></ref>
<ref id="c7"><label>7</label><mixed-citation publication-type="journal"><string-name><given-names>D.</given-names> <surname>Silver</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Huang</surname></string-name>, <string-name><given-names>C. J.</given-names> <surname>Maddison</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Guez</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Sifre</surname></string-name>, <string-name><given-names>G.</given-names> <surname>van den Driessche</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Schrittwieser</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Antonoglou</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Panneershelvam</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Lanctot</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Dieleman</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Grewe</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Nham</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Kalchbrenner</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Sutskever</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Lillicrap</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Leach</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Kavukcuoglu</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Graepel</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Hassabis</surname></string-name>, &#x201C; <article-title>Mastering the game of go with deep neural networks and tree search</article-title>,&#x201D; <source>Nature</source>, vol. <volume>529</volume>, no. <issue>7587</issue>, pp. <fpage>484</fpage>&#x2013;<lpage>489</lpage>, 27 Jan. 2016.</mixed-citation></ref>
<ref id="c8"><label>8</label><mixed-citation publication-type="other"><string-name><given-names>J.</given-names> <surname>Hsu</surname></string-name>, &#x201C; <article-title>AI decisively defeats human poker players</article-title>,&#x201D; <ext-link ext-link-type="uri" xlink:href="http://spectrum.ieee.org/automaton/robotics/articial-intelligence/ai-learns-from-mistakes-to-defeat-human-poker-players,">http://spectrum.ieee.org/automaton/robotics/articial-intelligence/ai-learns-from-mistakes-to-defeat-human-poker-players,</ext-link> 31 Jan. <year>2017</year>, accessed: <date-in-citation content-type="access-date">2017-3-28</date-in-citation>.</mixed-citation></ref>
<ref id="c9"><label>9</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Canziani</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Paszke</surname></string-name>, and <string-name><given-names>E.</given-names> <surname>Culurciello</surname></string-name>, <article-title>&#x201C; An analysis of deep neural</article-title> <source>network models for practical applications,&#x201D;</source> 24 May 2016.</mixed-citation></ref>
<ref id="c10"><label>10</label><mixed-citation publication-type="journal"><string-name><given-names>X.</given-names> <surname>Glorot</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Bordes</surname></string-name>, and <string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name>, &#x201C; <article-title>Deep sparse recti er neural networks</article-title>,&#x201D; <source>PMLR</source>, vol. <volume>15</volume>:, pp. <fpage>315</fpage>&#x2013;<lpage>323</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c11"><label>11</label><mixed-citation publication-type="journal"><string-name><given-names>G. E.</given-names> <surname>Hinton</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Srivastava</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Krizhevsky</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Sutskever</surname></string-name>, and <string-name><given-names>R. R.</given-names> <surname>Salakhutdinov</surname></string-name>, <article-title>&#x201C; Improving neural networks by preventing co-adaptation</article-title> <source>of feature detectors,&#x201D;</source> 3 Jul. 2012.</mixed-citation></ref>
<ref id="c12"><label>12</label><mixed-citation publication-type="journal"><string-name><given-names>X.</given-names> <surname>Glorot</surname></string-name> and <string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name>, &#x201C; <article-title>Understanding the dif culty of training deep feedforward neural networks</article-title>,&#x201D; <source>Aistats</source>, vol. <volume>9</volume>, <year>2010</year>.</mixed-citation></ref>
<ref id="c13"><label>13</label><mixed-citation publication-type="journal"><string-name><given-names>V.</given-names> <surname>Gulshan</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Peng</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Coram</surname></string-name>, <string-name><given-names>M. C.</given-names> <surname>Stumpe</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Narayanaswamy</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Venugopalan</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Widner</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Madams</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Cuadros</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Raman</surname></string-name>, <string-name><given-names>P. C.</given-names> <surname>Nelson</surname></string-name>, <string-name><given-names>J. L.</given-names> <surname>Mega</surname></string-name>, and <string-name><given-names>D. R.</given-names> <surname>Webster</surname></string-name>, &#x201C; <article-title>Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</article-title>,&#x201D; <source>JAMA</source>, vol. <volume>316</volume>, no. <issue>22</issue>, pp. <fpage>2402</fpage>&#x2013;<lpage>2410</lpage>, 13 <month>Dec</month>. <year>2016</year>.</mixed-citation></ref>
<ref id="c14"><label>14</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Esteva</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Kuprel</surname></string-name>, <string-name><given-names>R. A.</given-names> <surname>Novoa</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Ko</surname></string-name>, <string-name><given-names>S. M.</given-names> <surname>Swetter</surname></string-name>, <string-name><given-names>H. M.</given-names> <surname>Blau</surname></string-name>, and <string-name><given-names>S.</given-names> <surname>Thrun</surname></string-name>, &#x201C; <article-title>Dermatologist-level classi cation of skin cancer with deep neural networks</article-title>,&#x201D; <source>Nature</source>, vol. <volume>542</volume>, no. <issue>7639</issue>, pp. <fpage>115</fpage>&#x2013;<lpage>118</lpage>, 2 <month>Feb</month>. <year>2017</year>.</mixed-citation></ref>
<ref id="c15"><label>15</label><mixed-citation publication-type="journal"><string-name><given-names>C. S.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>D. M.</given-names> <surname>Baughman</surname></string-name>, and <string-name><given-names>A. Y.</given-names> <surname>Lee</surname></string-name>, &#x201C; <article-title>Deep learning is effective for classifying normal versus age-related macular degeneration oct images</article-title>,&#x201D; <source>Ophthalmology Retina</source>, vol. <volume>1</volume>, no. <issue>4</issue>, pp. <fpage>322</fpage>&#x2013;<lpage>327</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c16"><label>16</label><mixed-citation publication-type="journal"><string-name><given-names>K.</given-names> <surname>Simonyan</surname></string-name> and <string-name><given-names>A.</given-names> <surname>Zisserman</surname></string-name>, &#x201C; <article-title>Very deep convolutional networks for large-scale image recognition</article-title>,&#x201D; <source>CoRR</source>, vol. <volume>abs/1409.1556</volume>, <year>2014</year>. [Online]. Available: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1409.1556">http://arxiv.org/abs/1409.1556</ext-link></mixed-citation></ref>
<ref id="c17"><label>17</label><mixed-citation publication-type="book"><string-name><given-names>I.</given-names> <surname>Goodfellow</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name>, and <string-name><given-names>A.</given-names> <surname>Courville</surname></string-name>, <source>Deep Learning.</source> <publisher-name>MIT Press</publisher-name>, <volume>2016</volume>, <ext-link ext-link-type="uri" xlink:href="http://www.deeplearningbook.org">http://www.deeplearningbook.org</ext-link>.</mixed-citation></ref>
<ref id="c18"><label>18</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Cho</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Shin</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Choy</surname></string-name>, and <string-name><given-names>S.</given-names> <surname>Do</surname></string-name>, &#x201C; <article-title>How much data is needed to train a medical image deep learning system to achieve necessary high accuracy</article-title> &#x201D; <source>arXiv [cs.LG]</source>, <volume>19</volume> <month>Nov</month>. <year>2015</year>.</mixed-citation></ref>
<ref id="c19"><label>19</label><mixed-citation publication-type="journal"><string-name><given-names>G.</given-names> <surname>Lema&#x00CE;tre</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Rastgoo</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Massich</surname></string-name>, <string-name><given-names>C. Y.</given-names> <surname>Cheung</surname></string-name>, <string-name><given-names>T. Y.</given-names> <surname>Wong</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Lamoureux</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Milea</surname></string-name>, <string-name><given-names>F.</given-names> <surname>M&#x00E8;riaudeau</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Sidib&#x00E8;</surname></string-name>, <article-title>&#x201C; Classi cation of SD-OCT volumes using local binary patterns: Experimental validation for DME detection,&#x201C;</article-title> <source>J. Ophthalmol.</source>, vol. <volume>2016</volume>, <fpage>31</fpage> <month>Jul</month>. <year>2016</year>.</mixed-citation></ref>
<ref id="c20"><label>20</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Efron</surname></string-name>, &#x201C; <article-title>Estimating the error rate of a prediction rule: Improvement on Cross-Validation</article-title>,&#x201D; <source>J. Am. Stat. Assoc.</source>, vol. <volume>78</volume>, no. <issue>382</issue>, pp. <fpage>316</fpage>&#x2013;<lpage>331</lpage>, <year>1983</year>.</mixed-citation></ref>
<ref id="c21"><label>21</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Stone</surname></string-name>, &#x201C; <article-title>Cross-Validatory choice and assessment of statistical predictions</article-title>,&#x201D; <source>J. R. Stat. Soc. Series B Stat. Methodol.</source>, vol. <volume>36</volume>, no. <issue>2</issue>, pp. <fpage>111</fpage>&#x2013;<lpage>147</lpage>, <year>1974</year>.</mixed-citation></ref>
<ref id="c22"><label>22</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Stone</surname></string-name>, <article-title>&#x201C; An asymptotic equivalence of choice of model by Cross-Validation and akaike&#x2019;s criterion, &#x201C;</article-title> <source>J. R. Stat. Soc. Series B Stat. Methodol.</source>, vol. <volume>39</volume>, no. <issue>1</issue>, pp. <fpage>44</fpage>&#x2013;<lpage>47</lpage>, <year>1977</year>.</mixed-citation></ref>
<ref id="c23"><label>23</label><mixed-citation publication-type="other"><string-name><given-names>A.</given-names> <surname>Krizhevsky</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Sutskever</surname></string-name>, and <string-name><given-names>G. E.</given-names> <surname>Hinton</surname></string-name>, &#x201C; <article-title>ImageNet classi cation with deep convolutional neural networks</article-title>,&#x201D; in <source>Advances in Neural Information Processing Systems 25</source>, <person-group person-group-type="editor"><string-name><given-names>F.</given-names> <surname>Pereira</surname></string-name>, <string-name><given-names>C. J. C.</given-names> <surname>Burges</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Bottou</surname></string-name>, and <string-name><given-names>K. Q.</given-names> <surname>Weinberger</surname></string-name></person-group>, Eds. <publisher-name>Curran Associates, Inc</publisher-name>., <year>2012</year>, pp. <fpage>1097</fpage>&#x2013;<lpage>1105</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Jia</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Shelhamer</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Donahue</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Karayev</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Long</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Girshick</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Guadarrama</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Darrell</surname></string-name>, &#x201C; <article-title>Caffe: Convolutional architecture for fast feature embedding</article-title>,&#x201D; <source>arXiv preprint arXiv:1408.5093</source>, <year>2014</year>.</mixed-citation></ref>
<ref id="c25"><label>25</label><mixed-citation publication-type="other"><string-name><given-names>D.</given-names> <surname>Kingma</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Ba</surname></string-name>, <article-title>&#x201C; Adam: A method for stochastic optimization,&#x201D; 22</article-title> <month>Dec</month>. <year>2014</year>.</mixed-citation></ref>
<ref id="c26"><label>26</label><mixed-citation publication-type="other"><string-name><given-names>D. C.</given-names> <surname>Cire&#x015F;an</surname></string-name>, <string-name><given-names>U.</given-names> <surname>Meier</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Masci</surname></string-name>, <string-name><given-names>L. M.</given-names> <surname>Gambardella</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Schmidhuber</surname></string-name>, <article-title>&#x201C; High-Performance neural networks for visual object classi cation,&#x201D;</article-title> 1 <month>Feb</month>. <year>2011</year>.</mixed-citation></ref>
<ref id="c27"><label>27</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name> and Others, &#x201C; <article-title>Deep learning of representations for unsupervised and transfer learning</article-title>,&#x201D; <source>ICML Unsupervised and Transfer Learning</source>, vol. <volume>27</volume>, pp. <fpage>17</fpage>&#x2013;<lpage>36</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c28"><label>28</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Bengio</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Hardt</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Recht</surname></string-name>, and <string-name><given-names>O.</given-names> <surname>Vinyals</surname></string-name>, &#x201C; <article-title>Understanding deep learning requires rethinking generalization</article-title>,&#x201D; <source>ICLR 2017</source>, <year>2017</year>.</mixed-citation></ref>
</ref-list>
<fn-group>
<fn id="fn1">
<label><sup>1</sup></label>
<p>&#x201C; As of 2016, a rough rule of thumb is that a supervised deep learning algorithm will generally achieve acceptable performance with around 5,000 labeled examples per category, and will match or exceed human performance when trained with a dataset containing at least 10 million labeled examples.&#x201D; [<xref rid="c17" ref-type="bibr">17</xref>], page 20</p></fn>
</fn-group>
</back>
</article>
