<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/206482</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Evolutionary Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Machine Learning for Population Genetics: A New Paradigm</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5249-4151</contrib-id>
<name><surname>Schrider</surname><given-names>Daniel R.</given-names></name>
<xref ref-type="aff" rid="a1">&#x002A;</xref>
<xref ref-type="aff" rid="a2">&#x2020;</xref>
<xref ref-type="corresp" rid="cor1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4381-4680</contrib-id>
<name><surname>Kern</surname><given-names>Andrew D.</given-names></name>
<xref ref-type="aff" rid="a1">&#x002A;</xref>
<xref ref-type="aff" rid="a2">&#x2020;</xref>
<xref ref-type="corresp" rid="cor1">1</xref>
</contrib>
<aff id="a1"><label>&#x002A;</label><institution>Department of Genetics, Rutgers University</institution>, Piscataway, New Jersey 08554</aff>
<aff id="a2"><label>&#x2020;</label><institution>Human Genetics Institute of New Jersey, Rutgers University</institution>, Piscataway, New Jersey 08554</aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>1</label>Correspondence to: Department of Genetics, Rutgers University, 604 Allison Rd., Piscataway, NJ 08854. E-mail: <email>dan.schrider@rutgers.edu</email>; <email>kern@biology.rutgers.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub"><year>2017</year></pub-date>
<elocation-id>206482</elocation-id>
<history>
<date date-type="received">
<day>19</day>
<month>10</month>
<year>2017</year>
</date>
<date date-type="rev-recd">
<day>19</day>
<month>10</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>20</day>
<month>10</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="206482.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract><title>Abstract</title>
<p>As population genomic datasets grow in size, researchers are faced with the daunting task of making sense of a flood of information. To keep pace with this explosion of data, computational methodologies for population genetic inference are rapidly being developed to best utilize genomic sequence data. In this review we discuss a new paradigm that has emerged in computational population genomics: that of supervised machine learning. We review the fundamentals of machine learning, discuss recent applications of supervised machine learning to population genetics that outperform competing methods, and describe promising future directions in this area. Ultimately, we argue that supervised machine learning is an important and underutilized tool that has considerable potential for the world of evolutionary genomics.</p>
</abstract>
<counts>
<page-count count="19"/>
</counts>
</article-meta>
</front>
<body>
<sec>
<p>Population genetics over the past 50 years has been squarely focused on reconciling molecular genetic data with theoretical models that describe patterns of variation produced by a combination of evolutionary forces. This interplay between empiricism and theory means that many advances in the field have come from the introduction of new stochastic population genetic models, often of increasing complexity, that describe how population parameters (e.g. recombination or mutation rates) might generate specific features of genetic polymorphism (e.g. the site frequency spectrum). The goal, broadly stated, is to formulate a model that describes how nature will produce patterns of variation that we observe. With such a model in hand, all one would have to do would be to estimate its parameters, and in so doing learn everything about the evolution of a given population.</p>
<p>Thus an overwhelming majority of population genetics research has focused on classical statistical estimation from a convenient probabilistic model (i.e. the Wright-Fisher model), or through an approximation to that model (i.e. the coalescent). The central assertion here is that the model sufficiently describes the data such that insights into nature can be made through parameter estimation. This mode of analysis that pervades population genetics is what Leo Breiman [<xref rid="c1" ref-type="bibr">1</xref>] famously referred to as the &#x201C;data modeling culture,&#x201D; wherein independent variables (i.e. the evolutionary and genomic parameters) are fed into a model and the response variables (some aspect of genetic variation) come out the other side. Models are validated in this worldview through the use of goodness-of-fit tests or examination of residuals (a recent modern example can be found in [<xref rid="c2" ref-type="bibr">2</xref>]).</p>
<p>In this review we argue that population genetics as a field might turn to a different mode of analysis, that of the &#x201C;algorithmic modeling culture,&#x201D; or what is now commonly called machine learning (ML). Over the past decade machine learning methods have revolutionized entire fields, including speech recognition [<xref rid="c3" ref-type="bibr">3</xref>], natural language processing [<xref rid="c4" ref-type="bibr">4</xref>], image classification [<xref rid="c5" ref-type="bibr">5</xref>], and bioinformatics [<xref rid="c6" ref-type="bibr">6</xref>, <xref rid="c7" ref-type="bibr">7</xref>]. However, the application of machine learning to problems in population and evolutionary genetics is still in its infancy, save for some pioneering examples [<xref rid="c8" ref-type="bibr">8</xref>&#x2013;<xref rid="c17" ref-type="bibr">17</xref>]. Machine learning approaches have a number of desirable features, and perhaps foremost among them is their potential to be agnostic about the process that creates a given dataset. Machine learning, as a field, aims to optimize predictive accuracy of an algorithm rather than perform parameter estimation of a probabilistic model. What this means in practice is that ML methods can teach us something about nature, even if our models used to describe nature are imprecise. An equally important advantage of the machine learning paradigm is that it enables the efficient use of high-dimensional inputs which act as dependent variables, without specific knowledge of the joint probability distribution of these variables. Inputs that consist of thousands of variables (a.k.a <italic>features</italic> in the ML world) have been used with great success (e.g. [<xref rid="c18" ref-type="bibr">18</xref>, <xref rid="c19" ref-type="bibr">19</xref>]) and increases in the number of features can often yield greater predictive power [<xref rid="c1" ref-type="bibr">1</xref>]. Given the ever-increasing dimensionality of modern genomic data, this is a particularly desirable property of machine learning. In this paper we describe several examples where, through a hybrid of the &#x201C;data modeling&#x201D; and &#x201C;algorithmic modeling&#x201D; paradigms, machine learning methods can leverage highdimensional data to attain far greater predictive power than competing methods. These early successes demonstrate that ML approaches could have the potential to revolutionize the practice of population genetic data analysis.</p>
</sec>
<sec id="s1"><title>An Introduction to Machine Learning</title>
<p>Machine learning is generally divided into two major categories (though hybrid strategies exist): supervised learning [<xref rid="c20" ref-type="bibr">20</xref>] and unsupervised learning [<xref rid="c21" ref-type="bibr">21</xref>]. Unsupervised learning is concerned with uncovering structure within a dataset without prior knowledge of how the data are organized (e.g. identifying clusters). A familiar example of unsupervised learning is principal component analysis (PCA), which in the context of population genetics is used for discovering unknown relatedness relationships among individuals. PCA takes as input a matrix of genotypes (often of very high dimensionality) and then produces a lower dimensional summary that can reveal how genotypes cluster. An excellent example of the application of PCA to population genetics can be found in Novembre <italic>et al.</italic> [<xref rid="c22" ref-type="bibr">22</xref>] where PCA was used to show how relationships among individuals sampled from Europe largely mirrored geography. Supervised learning, on the other hand, relies on prior knowledge about an example dataset in order to make predictions about new data points. Generally, supervised ML is concerned with predicting the value of a response variable, or <italic>label</italic> (either a categorical or continuous value), on the basis of the input variables/features. Supervised learning accomplishes this feat through the use of a <italic>training set</italic> of labeled data examples, whose true response values are known in order to train the predictor (see Box 1 for more detail).</p>
<p>There have been a multitude of important applications of unsupervised machine learning in evolutionary genomics beyond PCA. One popular methodology that has been wildly successful in population and evolutionary genetics is hidden Markov models (HMMs; see [<xref rid="c23" ref-type="bibr">23</xref>]). HMMs are a class of probabilistic graphical model that are well suited to segmenting data that appears as a linear sequence, such a chromosomes. For instance with phylogenetic data, HMMs have been used to uncover differences in evolutionary rates along a chromosome [<xref rid="c24" ref-type="bibr">24</xref>, <xref rid="c25" ref-type="bibr">25</xref>]. Furthermore, HMMs have been used to infer how the phylogeny itself changes across chromosomes due to recombination [<xref rid="c9" ref-type="bibr">9</xref>, <xref rid="c26" ref-type="bibr">26</xref>, <xref rid="c27" ref-type="bibr">27</xref>]. In the context of population genetic data HMMs have been leveraged to detect regions of the genome under positive or negative selection [<xref rid="c10" ref-type="bibr">10</xref>] as well as to localize selective sweeps [<xref rid="c11" ref-type="bibr">11</xref>, <xref rid="c28" ref-type="bibr">28</xref>].</p>
<p>Although unsupervised ML has been deployed widely and effectively throughout the field, to date there has been less attention paid to supervised learning. Here we give a brief overview of the paradigm of supervised ML and highlight recent population genetic studies leveraging these approaches.</p>
<boxed-text id="box1" position="float">
<sec><label>Box 1.</label><title>Supervised Learning in draft form</title>
<p>Supervised ML approaches algorithmically create from a given dataset a function that takes as input a vector and then emits a predicted value for each data point. More formally, these methods learn a function, <italic>f</italic>, that predicts a response variable, <italic>y</italic>, from a <italic>feature vector</italic>, <bold><italic>x</italic></bold>, containing <italic>M</italic> input variables, such that: <italic>f</italic>(<bold><italic>x</italic></bold>) &#x003D; <italic>y</italic>. If <italic>y</italic> is a categorical variable, we refer to the task as a <italic>classification</italic> problem, whereas if <italic>y</italic> is a continuous variable we refer to it as <italic>regression</italic>. In supervised learning, the objective is to optimize <italic>f</italic>: <bold><italic>x</italic></bold> &#x2192; <italic>y</italic> using a &#x201C;training set&#x201D; of labeled data (i.e. whose response values are known). That is, we assume we have a set of training data of length <italic>n</italic> of the form {(<bold><italic>x</italic></bold><sub><bold>1</bold></sub>,<italic>y</italic><sub>1</sub>),&#x2026;, (<bold><italic>x</italic></bold><sub><bold><italic>n</italic></bold></sub>, <italic>y<sub>n</sub></italic>)}, where <bold><italic>x</italic></bold> &#x2208; &#x211D;<sup><italic>M</italic></sup>. A variety of learning algorithms exist which can create functions that can perform either classification or regression, including <italic>support vector machines</italic> (SVMs [<xref rid="c29" ref-type="bibr">29</xref>]), <italic>decision trees</italic> [<xref rid="c30" ref-type="bibr">30</xref>] and <italic>random forests</italic> [<xref rid="c31" ref-type="bibr">31</xref>], <italic>boosting</italic> [<xref rid="c32" ref-type="bibr">32</xref>], and <italic>artificial neural networks</italic> (ANNs [<xref rid="c33" ref-type="bibr">33</xref>]) which in modern form are subsumed under the umbrella of <italic>deep learning</italic> [<xref rid="c34" ref-type="bibr">34</xref>]. These algorithms differ in how they structure and train <italic>f</italic> (see brief descriptions in the Glossary).</p>
<p>To proceed with building <italic>f</italic> we must define a <italic>loss function</italic>, <italic>L</italic>, that indicates how good or bad a given prediction is. A simple choice for a loss function in the context of classification would be the indicator function such that <italic>L</italic>(<italic>f</italic>(<bold><italic>x</italic></bold>), <italic>y</italic>) &#x003D; <bold>1</bold> (<italic>f</italic>(<bold><italic>x</italic></bold>) &#x2260; <italic>y</italic>). For regression one might consider the squared deviation <italic>L</italic>(<italic>f</italic>(<bold><italic>x</italic></bold>), <italic>y</italic>) &#x003D; (<italic>f</italic>(<bold><italic>x</italic></bold>) &#x2212; <italic>y</italic>)<sup>2</sup>. Finally, we define the <italic>risk function</italic>, which is typically the average value of <italic>L</italic> across the training set. &#x201C;Training&#x201D; is the process of minimizing this risk function.</p>
<p>Once training is complete, we must evaluate our performance on an independent test set. This step allows one to assess whether <italic>f</italic> has become sensitive to the general characteristics of the problem at hand, rather than characteristics particular to data examples in the training set (what is known as <italic>overfitting</italic>). For <italic>binary classification</italic> we might characterize the <italic>false positive</italic> and <italic>false negative</italic> rates or related measures such as <italic>precision</italic> and <italic>recall</italic>. A particularly helpful construct in the case of multiclass classification is the <italic>confusion matrix</italic>, which is simply the contingency table of true vs. predicted class labels for each class. For regression, one could use any tool for evaluating model fit (e.g. <italic>R</italic><sup><italic>2</italic></sup>) or examine the distribution of values of one or more loss functions. Residuals can also be checked for evidence of bias in order to anticipate which types of data are likely to produce erroneous predictions.</p>
<fig id="fig1" position="float" fig-type="figure"><label>Figure Box 1.</label>
<caption><p>An example application of supervised machine learning to demographic model selection. We simulated population samples experiencing no population size change (Equilibrium), a recent instantaneous population decline (Contraction), or recent instantaneous expansion (Growth). We then trained a variant of a random forest classifier [<xref rid="c35" ref-type="bibr">35</xref>], which is an ensemble of semi-randomly generated decision trees, to discriminate between these three models on the basis of a feature vector consisting of two population genetic summary statistics [<xref rid="c36" ref-type="bibr">36</xref>, <xref rid="c37" ref-type="bibr">37</xref>]. On the left we show the decision surface: red points represent the Growth scenario, dark blue points represent Equilibrium, and light blue points represent Contraction. The shaded areas in the background show how additional data points would be classified&#x2014;note the nonlinear decision surface separating these three classes. On the right, we show the confusion matrix obtained from measuring classification accuracy on an independent test set. Data were simulated with Hudson&#x2019;s ms [<xref rid="c38" ref-type="bibr">38</xref>], and classification was performed via scikit-learn [<xref rid="c39" ref-type="bibr">39</xref>].</p></caption>
<graphic xlink:href="206482_fig1.tif"/>
</fig>
</sec>
</boxed-text>
</sec>
<sec id="s2"><title>Why use Machine Learning?</title>
<p>Our basic description of supervised ML approaches in Box 1 demonstrates their central rationale: ML focuses on algorithmically constructed models with optimal prediction as their goal rather than parametric data modeling. Furthermore, ML offers several advantages in addition to accurate prediction. Perhaps most important among them is the ability to circumvent using idealized, parametric models of the data when labeled training data can be obtained from empirical observation (an example of this scenario is given in the following section). Indeed in such cases we can use ML to train algorithms to recognize phenomena as they are in nature, rather than how we choose to represent them in a model. Further, in cases where empirically derived training sets are not available, simulation can be used to generate training sets. This ability to use simulation as a stand-in for observed data is key for population genetics applications, where adequately sized datasets with high-confidence labels are currently hard to obtain. While using simulation for training obviates the model agnosticism that is so attractive about ML, discriminative ML models are more robust to model misspecification than traditional data models [<xref rid="c40" ref-type="bibr">40</xref>].</p>
<p>Even when empirical training data cannot feasibly be obtained, there are notable advantages of supervised ML methods. Most importantly, these methods are specifically geared towards using high-dimensional data as input. Typically, classical statistical methods suffer from what has been called the &#x201C;curse of dimensionality&#x201D; whereby high-dimensional data become sparse and thus very difficult to fit models to. On the other hand, most supervised ML methods perform better when the input data has a large number of features, in what is commonly called the &#x201C;blessing of dimensionality&#x201D; (e.g. [<xref rid="c1" ref-type="bibr">1</xref>, <xref rid="c41" ref-type="bibr">41</xref>]). A good example of this comes from the highly cited work of Amit and Geman [<xref rid="c18" ref-type="bibr">18</xref>] on using a random forest-like procedure for handwriting recognition: it took as input a feature vector containing thousands of variables, and proved highly accurate. In a more modern setting, <italic>deep learning</italic> methods have been shown both theoretically and in practice to be able to circumvent the curse of dimensionality in many settings [<xref rid="c42" ref-type="bibr">42</xref>, <xref rid="c43" ref-type="bibr">43</xref>]. This attribute lends significant strength to population genetics analysis: while inferences are traditionally based on a single summary statistic devised for the given task (e.g. [<xref rid="c36" ref-type="bibr">36</xref>, <xref rid="c44" ref-type="bibr">44</xref>&#x2013;<xref rid="c49" ref-type="bibr">49</xref>]), below we describe several recent studies which demonstrate that far greater statistical power can be achieved by simultaneously examining multiple aspects of genetic variation across the genome. Importantly, many ML methods offer direct ways to assess which features of the input are driving inferences, information which can yield insights about the underlying processes [<xref rid="c1" ref-type="bibr">1</xref>].</p>
<p>The last benefit we wish to touch upon is computational efficiency. While training of supervised ML algorithms is computationally costly&#x2014;especially if simulation is used for the training set&#x2014; once an algorithm is trained, prediction from it is exceedingly fast even in situations where a large number of predictions is required (e.g. genome-wide scans). This means that there will be an upfront cost to training (typically hours or days), but genome-wide inference proceeds rapidly thereafter. Moreover, because many ML approaches (e.g. deep learning) have the ability to generalize beyond their input parameters (e.g. [<xref rid="c50" ref-type="bibr">50</xref>]), training sets can be considerably smaller than those used by approaches such as approximate Bayesian computation (ABC [<xref rid="c51" ref-type="bibr">51</xref>]; also see Box 3).</p>
</sec>
<sec id="s3"><title>Supervised ML in population genetics by training on real data: finding purifying selection</title>
<p>When empirically derived training data are available, supervised machine learning can be used to make accurate predictions in data sets that cannot be adequately modeled with a reasonable number of parameters. For instance, a current goal in modern genomics is to be able to predict functional regions of the genome using bioinformatics techniques. While there are numerous sources of information to leverage for this problem, including comparative [<xref rid="c25" ref-type="bibr">25</xref>] and functional genomics [<xref rid="c52" ref-type="bibr">52</xref>], the best manner in which to incorporate population genomic variation to aid in these predictions is a matter of active research. Towards this end we recently used a supervised ML approach to discriminate between genomic regions experiencing purifying selection and those free from selective constraint on the basis of population genomic data alone [<xref rid="c15" ref-type="bibr">15</xref>]. In this study we used a support vector machine (SVM) that employed as input the site frequency spectrum (SFS) from all 1,092 individuals from the Phase I release of 1000 Genomes dataset which consisted of 14 population samples from diverse global locations [<xref rid="c53" ref-type="bibr">53</xref>]. Had we attempted to use all these data simultaneously in a &#x201C;classical&#x201D; population genetics setting we would have been forced to fit a demographic model that described the joint divergence and population size changes of all 14 population samples; a daunting task indeed. While the SFS is well-known to be affected by demography as well as selection [<xref rid="c54" ref-type="bibr">54</xref>], by constructing a training set of regions experiencing purifying selection (inferred from a phylogenetic comparison of non-human mammals) we were able to effectively sidestep the intractable problem of modeling the joint demographic history of the dataset. We were then able to both train and test an SVM on empirical data, achieving &#x007E;88&#x0025; accuracy [<xref rid="c15" ref-type="bibr">15</xref>].</p>
<p>By comparing the predictions from this classifier, which reveal purifying selection occurring in recent evolutionary history, with phylogenetic signatures of more ancient selection, we were able to identify regions showing evidence of functional turnover in the human genome. We showed that these candidate regions were highly enriched in the regulatory domains of genes important for proper central nervous system development. Moreover, another group [<xref rid="c55" ref-type="bibr">55</xref>] recently found that the presence of these candidate regions near a gene was more predictive of human-specific changes of expression in the brain than was the presence of well-known human-accelerated regions (HARs) identified from inter-specific comparisons [<xref rid="c56" ref-type="bibr">56</xref>]. This result lends credence both to our own predictions and more generally to the utility of supervised ML approaches in evolutionary genetics.</p>
</sec>
<sec id="s4"><title>Finding selective sweeps in the genome</title>
<p>The population genetic question that has received the most attention from ML approaches is that of detecting selective sweeps: the signature left by an adaptive mutation that rapidly increases in allele frequency until reaching fixation [<xref rid="c57" ref-type="bibr">57</xref>]. While the classical population genetic strategy for finding sweeps has been to carefully devise test statistics sensitive to selective perturbations [<xref rid="c36" ref-type="bibr">36</xref>, <xref rid="c44" ref-type="bibr">44</xref>&#x2013;<xref rid="c49" ref-type="bibr">49</xref>], in recent years several groups have begun leveraging combinations of statistics through supervised ML to improve inferential power. While each of these methods differ in the exact combination of summary statistics used, their unifying feature is that training sets are generated using coalescent simulations with and without selective sweeps First among these was Pavlidis et al. [<xref rid="c12" ref-type="bibr">12</xref>], who used a SVM to combine Kim and Nielsen&#x2019;s <italic>&#x03C9;</italic> statistic (which measures the spatial pattern of LD expected around a sweep [<xref rid="c47" ref-type="bibr">47</xref>]) with Nielsen et al.&#x2019;s composite-likelihood ratio (a.k.a. CLR, which highlights the spatial skew in the SFS expected around a sweep [<xref rid="c58" ref-type="bibr">58</xref>]). They found that these two statistics in concert had greater power to detect sweeps. Ronen et al. [<xref rid="c14" ref-type="bibr">14</xref>] took the approach of encoding the SFS as the feature vector (i.e. each bin in the SFS is one feature), and then used an SVM to discriminate between selective sweeps and neutrality. Lin et al. [<xref rid="c8" ref-type="bibr">8</xref>] used <italic>boosting</italic> to identify sweeps on the basis of a feature vector containing six different summary statistics each measured across a number of genomic subwindows surrounding the focal window. In a related effort, Pybus et al. [<xref rid="c13" ref-type="bibr">13</xref>] recently used a series of boosting classifiers to detect selective sweeps and classify them according to whether they have reached fixation (complete vs. incomplete) as well as by their timing (recent vs. ancient). Finally, in Schrider and Kern [<xref rid="c16" ref-type="bibr">16</xref>], we describe S/HIC, which uses a variant of a <italic>random forest</italic> [<xref rid="c31" ref-type="bibr">31</xref>] called an Extra-Trees classifier [<xref rid="c35" ref-type="bibr">35</xref>] to detect both classic &#x201C;hard sweeps&#x201D; from <italic>de novo</italic> mutations and &#x201C;soft sweeps&#x201D; resulting from selection on previously segregating variants [<xref rid="c59" ref-type="bibr">59</xref>, <xref rid="c60" ref-type="bibr">60</xref>]. As described in Box 2, S/HIC is able to detect sweeps with high sensitivity and specificity even in the face of nonequilibrium demography, which confounds many other methods. The success of S/HIC and the other efforts listed above demonstrates that an appropriately designed machine learning approach can make rapid advances in performance on difficult problems that have received attention for decades.</p>
<boxed-text id="box2" position="float">
<sec><label>Box 2</label><title>a closer look at S/HIC</title>
<p>We recently introduced a method called S/HIC [<xref rid="c16" ref-type="bibr">16</xref>], which uses a feature vector designed to be not only sensitive to hard and soft sweeps, but also robust to the confounding effects of both linked positive selection (i.e. &#x201C;soft shoulders&#x201D; [<xref rid="c61" ref-type="bibr">61</xref>]) and non-equilibrium demography [<xref rid="c54" ref-type="bibr">54</xref>, <xref rid="c62" ref-type="bibr">62</xref>]. This feature vector included values of 9 different statistics that were each measured in a number of adjacent subwindows (Figure Box 2, below), in a similar vein to Lin et al.&#x2019;s evolBoosting [<xref rid="c8" ref-type="bibr">8</xref>]. What set this feature vector apart is that for each statistic, the value in each subwindow was normalized by dividing by the sum across all subwindows. Thus, the true value of a given statistic in a given subwindow is ignored, while the relative values across the larger window are examined. The reasoning behind this choice is that while demographic events may affect values of population genetic summaries genome-wide (which S/HIC ignores), selective sweeps may result in more dramatic localized skews in these statistics (which S/HIC captures). The results of this design are impressive: S/HIC is able to detect sweeps under challenging demographic scenarios, often with no loss in power even when the demographic history is grossly misspecified during training (e.g. if there is an unknown population bottleneck), a scenario which catastrophically compromises many other methods [<xref rid="c16" ref-type="bibr">16</xref>, <xref rid="c63" ref-type="bibr">63</xref>]. Thus, ML methods&#x2014;especially those with appropriately designed feature vectors&#x2014;can be robust to modeling choices even when training data are simulated.</p>
<p>In Figure Box 2 we illustrate S/HIC&#x2019;s classification strategy and the values included in its feature vector. This figure demonstrates how much additional information S/HIC utilizes in making its predictions in comparison to more traditional population genetic tests, especially those relying on a single statistic. In particular, the S/HIC feature vector not only includes multiple statistics each of which is designed to capture different aspects of genealogies, but also how these statistics vary along the chromosome. In addition to greater robustness to demography as discussed above, incorporating all of this information yields greater discriminatory power, and for this reason such multidimensional methods will be preferable to univariate approaches. We recently applied S/HIC to six human populations with complex demographic histories, where it revealed that soft sweeps appear to account for the majority of recent adaptive events in humans [<xref rid="c64" ref-type="bibr">64</xref>]; the success of this analysis demonstrates the practicality of applying such ML strategies to real data.</p>
<fig id="fig2" position="float" fig-type="figure"><label>Figure Box 2.</label>
<caption><p>A visualization of S/HIC&#x2019;s feature vector and classes. The S/HIC feature vector consists of <italic>&#x03C0;</italic> [<xref rid="c65" ref-type="bibr">65</xref>], <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="206482_inline1.gif"/></alternatives></inline-formula> [<xref ref-type="bibr" rid="c37">37</xref>], <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="206482_inline2.gif"/></alternatives></inline-formula> [<xref ref-type="bibr" rid="c36">36</xref>], the number of distinct haplotypes, average haplotype homozygosity, <italic>H</italic><sub>12</sub> and <italic>H</italic><sub>2</sub>/<italic>H</italic><sub>1</sub> [<xref rid="c66" ref-type="bibr">66</xref>, <xref rid="c67" ref-type="bibr">67</xref>], <italic>Z</italic><sub><italic>nS</italic></sub> [<xref rid="c46" ref-type="bibr">46</xref>], and the maximum value of <italic>&#x03C9;</italic> [<xref rid="c47" ref-type="bibr">47</xref>], The expected values of these statistics are shown for genomic regions containing hard and soft sweeps (as estimated from simulated data). Fay and Wu&#x2019;s <italic>H</italic> [<xref rid="c36" ref-type="bibr">36</xref>] and Tajima&#x2019;s <italic>D</italic> [<xref rid="c48" ref-type="bibr">48</xref>] are also shown, though these may be omitted from the vector as they are redundant with <italic>&#x03C0;</italic>, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="206482_inline3.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="206482_inline4.gif"/></alternatives></inline-formula>. In order to classify a given region, the spatial patterns of these statistics are examined across a genomic window in order to infer whether the center of the window contains a hard selective sweep (blue shaded area on the left, using statistics calculated within the larger blue window), is linked to a hard sweep (purple shaded area and larger window, left), contains a soft sweep (red, on the right), is linked to soft sweep (orange, right), or is evolving neutrally (not shown).</p></caption>
<graphic xlink:href="206482_fig2.tif"/>
</fig>
</sec>
</boxed-text>
<p>The methods listed above have two commonalities: they use machine learning to perform classification on multidimensional input, and they handily outperform more traditional univariate methods. However, these methods also differ from one another substantially in a number of facets: the particular machine learning framework used, the makeup of the feature vector, and the types of sweeps they seek to detect. Thus, the success of these methods underscores not only the power but also remarkable flexibility of supervised ML. By working within the supervised ML paradigm one can effectively tailor a predictor to whatever task is at hand simply by altering the construction of the feature vector and training dataset, and in so doing, make more detailed predictions than is possible using a single statistic.</p>
<p>Unlike the problem of detecting purifying selection, for which a training set may be constructed, we lack an adequate number of selective sweeps whose parameters are known precisely (e.g. the time of the sweep, strength of selection). Thus, the studies described above used simulation to generate training sets. The general idea is to simulate data from one or a number of population genetic models in which parameters are either specified precisely or defined by prior distributions, use those data to train an ML algorithm, and then perform either classification or regression (i.e. parameter estimation). In this context supervised ML allows for likelihood-free inference of population genetic models similar in spirit to ABC. While, like ABC, this approach requires modeling assumptions, it nonetheless offers numerous advantages as described in Box 3 where we contrast ABC with supervised ML</p>
<boxed-text id="box3" position="float">
<sec><label>Box 3</label><title>Comparing Supervised ML and ABC for population genetic inference</title>
<p>Using supervised ML with training data simulated from a specified set of population genetic models is similar in spirit to approximate Bayesian computation (ABC), save for some notable distinctions. ABC begins by simulating a large number of examples whose model parameters are drawn from prior distributions then summarizes these simulations with vectors of population genetic summary statistics. Next, only those simulations most similar to the observed dataset are retained&#x2014;a process known as rejection sampling&#x2014;to approximate the probability distribution for each parameter value given the observed data. ABC is easy to implement, flexible, and has been proven effective in a number of scenarios. However, ABC has some important drawbacks that ML overcomes. Most importantly, when using large feature vectors, ABC is susceptible to the curse of dimensionality [<xref rid="c68" ref-type="bibr">68</xref>]&#x2014;much effort has therefore gone into dimensionality reduction and feature selection for ABC (reviewed in [<xref rid="c69" ref-type="bibr">69</xref>]). While this is so, reducing dimensionality might lead to a loss of information if the remaining summaries are not sufficient statistics of the data. This contrasts with modern ML algorithms, which can benefit from high dimensional data, rather that suffer from them.</p>
<p>A second drawback of ABC is its computational burden. While both ML and ABC require a large number of simulations, ABC does not make efficient use of all of this computation because it typically depends on rejection sampling. Work has been done to retain more of the simulations in ABC, for instance by weighing their influence on parameter estimation according to their similarity to the observed data [<xref rid="c70" ref-type="bibr">70</xref>]. However ML methods naturally use all of the simulations to learn the mapping of data to parameters. Further, deep learning methods have the potential to generalize non-locally [<xref rid="c42" ref-type="bibr">42</xref>], allowing them to make accurate predictions for data quite different from those in the training set. For these reasons, ML may require considerably fewer simulations than ABC. Furthermore, ML methods need not reexamine these simulations in order to perform downstream prediction, unlike ABC, and thus further inference is very fast.</p>
<p>A final difference between ML and ABC is that of interpretability. In the realm of ABC it is not clear which summaries are responsible for a signal. On the other hand many ML methods allow direct measurement of each feature&#x2019;s contribution. Thus, despite their use of algorithmically generated models, ML algorithms are far from black boxes.</p>
</sec>
</boxed-text>
</sec>
<sec id="s5"><title>Inferring Demography and Recombination</title>
<p>Another emerging use of supervised ML in population genetics has been for inference of demographic history and recombination rates. Indeed much attention in the field has been placed on developing methods for the inference of population size histories and patterns of population splitting and migration [<xref rid="c71" ref-type="bibr">71</xref>&#x2013;<xref rid="c75" ref-type="bibr">75</xref>]. ABC methods are among the most popular for inferring demographic histories [<xref rid="c68" ref-type="bibr">68</xref>]. Interestingly, several groups have experimented with augmenting ABC by using ML for selecting the optimal combination of summary statistics [<xref rid="c76" ref-type="bibr">76</xref>] or even generating them [<xref rid="c77" ref-type="bibr">77</xref>]. While this is a promising direction for feature engineering, others have directly used ML to estimate posterior distributions of demographic parameters. For instance, Blum and Francois [<xref rid="c70" ref-type="bibr">70</xref>] used a feed-forward <italic>artificial neural network</italic> (ANN) to learn the mapping of summary statistics onto parameters with excellent results, particularly with respect to computational cost savings.</p>
<p>In addition to demographic parameter estimation, supervised ML has been used recently for demographic model selection (a possibility pointed to by Blum and Francois). For instance, Pudlo et al. [<xref rid="c78" ref-type="bibr">78</xref>] showed that random forests outperform ABC in both accuracy and computational cost when performing demographic model selection, along with greater robustness to the choice of summary statistics included in the input vector. In a recent preprint [<xref rid="c79" ref-type="bibr">79</xref>], we apply Extra-Trees classifiers to a problem of locus-specific demographic model selection: that of identifying regions with gene flow between a pair of closely related species with far greater accuracy than previous methods. Thus in general, ML methods show great promise in demographic estimation and model selection, and may soon be the preferred choice over ABC.</p>
<p>Supervised ML has also been applied to characterize rates and patterns of recombination in the genome. This work has again been done with or without simulation of training data. For instance Adrian et al. [<xref rid="c80" ref-type="bibr">80</xref>] trained a random forest classifier to distinguish among recombination rate classes on the basis of sequence motifs to show that such motifs are predictive of recombination rate in <italic>Drosophila melanogaster</italic>. This work used annotated rates of recombination based on a classical population genetics estimator to define the training set. On the other hand, Haipeng Li&#x2019;s group has developed methodology [<xref rid="c81" ref-type="bibr">81</xref>, <xref rid="c82" ref-type="bibr">82</xref>] that uses <italic>boosting</italic> to infer recombination rate maps from large sample sizes on the basis of simulated training data. Their latest method, FastEPRR, has much greater computational efficiency than and equal accuracy to the widely used LDhat [<xref rid="c83" ref-type="bibr">83</xref>]. Although application of supervised ML methods to this problem has begun only recently, the success of FastEPRR suggests the potential of future gains using these approaches.</p>
</sec>
<sec id="s6"><title>Co-estimation of selection and demography</title>
<p>It is well known that demographic events can mimic the effects of selection [<xref rid="c54" ref-type="bibr">54</xref>] and conversely that selection can confound demographic estimation [<xref rid="c84" ref-type="bibr">84</xref>, <xref rid="c85" ref-type="bibr">85</xref>]. This implies that, although one can attempt to design more robust approaches (e.g. S/HIC, discussed above), the ideal strategy would be to simultaneously make inferences about both of these phenomena. How then can one perform co-estimation of parameters related to multiple evolutionary phenomena? A promising approach that utilizes supervised ML, in this case deep learning, was recently introduced by Sheehan and Song [<xref rid="c17" ref-type="bibr">17</xref>]. They developed a deep neural network, called evoNet, to simultaneously infer population size changes in a three-epoch model and detect selective sweeps. What makes this research particularly important is that Sheehan and Song performed simultaneous classification of loci into selective classes and demographic parameter estimation (based on averages estimated over loci classified as neutral), through the use of a neural network architecture that outputs both categorical and continuous parameters. This inherent flexibility of ML, and deep learning architectures in particular, opens up a whole slew of opportunities for doing population genomic inference in ways that have never before been possible (discussed below).</p>
</sec>
<sec id="s7"><title>Concluding Remarks and Future Directions</title>
<p>The future of population genomic analysis rests in our ability to make sense of large and ever growing datasets. Toward this end, supervised ML techniques represent a new paradigm for analysis, one uniquely suited for making inferences in the context of high-dimensional data produced by an unknown or imprecisely parameterized model. Here we have reviewed a selection of early applications of supervised ML tools to population genomic data. The overwhelming take-home is that supervised ML provides robust, computationally efficient inference for a number of problems that are difficult to gain traction on via classical statistical approaches.</p>
<p>We believe that population genetics is now poised for an explosion in the use of supervised ML approaches. Deep learning in particular, with its incredibly flexible input and output structure, should be an important area of future research, and its earliest application [<xref rid="c17" ref-type="bibr">17</xref>] has yielded the critical ability to co-estimate selection and demography, a central goal of population genetics analysis over the past 15 years. Indeed, deep learning could potentially alter the way that we even think about the nature of our input data itself. For example one flavor of deep learning, convolutional neural networks (CNNs), have made astounding advances in our ability to learn parameters from image data [<xref rid="c86" ref-type="bibr">86</xref>]. Rather than learning on population genetic summary statistics calculated from a multiple sequence alignment (e.g. [<xref rid="c8" ref-type="bibr">8</xref>, <xref rid="c16" ref-type="bibr">16</xref>]), one could instead treat an image of the alignment itself as input. While these data would be extremely high dimensional, the structure of CNNs allows them to implicitly perform dimensionality reduction while capturing salient structures in the input data [<xref rid="c87" ref-type="bibr">87</xref>], allowing for accurate and efficient classification and regression (additional possible future avenues of ML in population genetics are listed in the <bold>Outstanding questions</bold> box). In general, the current explosion in deep learning research promises future improvements in our ability to make evolutionary inferences well beyond current capabilities; the challenge for population geneticists then is to adapt such methods for our own uses.</p>
<boxed-text id="box4" position="float">
<sec><title>Outstanding questions</title>
<p><list list-type="bullet">
<list-item><p>While a few comparisons have shown that ML can outperform ABC, a more thorough assessment of the strengths and limitations of each approach across a variety of problems (e.g. on simulated data) is in order. In what scenarios would either strategy be preferable?</p></list-item>
<list-item><p>Like more traditional methods, ML applications relying on simulated training data must make modeling assumptions. To what extent can ML methods be made more robust to these assumptions (e.g. by appropriately designing the feature vector as done by S/HIC, or through simulating a greater breadth of training examples)?</p></list-item>
<list-item><p>ML methods have the ability to infer the values of multiple parameters simultaneously. How feasible will parameter estimation be in more complex evolutionary models using ML tools such as deep neural networks?</p></list-item>
<list-item><p>As described here, supervised ML relies on summaries of population genetic data as feature vectors, but what summaries are best and can we do better than standard population genetic statistics? The recent rise of convolutional neural networks for image recognition suggests that encoding alignments as images might enable more powerful population genetic inference&#x2014;how best can we encode population genetic data?</p></list-item>
<list-item><p>A type of ANN called generative adversarial networks has been shown to generate data examples that can mimic true data with increasing accuracy. Can such methods be used as a substitute for population genetic simulation, perhaps to generate very large samples and chromosomes that are computationally costly to simulate?</p></list-item>
<list-item><p>Applications of supervised ML to population genetic data can be quite involved, necessitating simulating data, encoding both simulated and real data as feature vectors, training the algorithm, and applying it. Can efforts to create self-contained, efficient, and user-friendly software packages capable of performing this entire workflow streamline this approach and make it more accessible to researchers?</p></list-item>
<list-item><p>While point estimation of population genetic model parameters is important, equally important is establishing credible intervals on our parameter estimates. How can we most effectively use ML for estimating intervals associated with parameter estimates?</p></list-item>
</list></p>
</sec>
</boxed-text>
</sec>
</body><back><ack><title>Acknowledgments</title>
<p>We thank Alexander Xue, Matt Hahn, Parul Johri, Peter Ralph, and Yun Song&#x2019;s group for comments on this manuscript. We also thank Justin Blumenstiel and Lex Flagel for discussions about image classification in population genetics. DRS was supported by NIH award no. K99HG008696. ADK was supported by NIH award no. R01GM117241.</p>
</ack>
<glossary><title>Glossary</title>
<def-list>
<def-item><term>Feature vector</term><def><p>A multidimensional representation of a data point made up of measurements (or features) taken from it (e.g. a vector of population genetic summary statistics measured in a genomic region).</p></def></def-item>
<def-item><term>Training</term><def><p>The process of algorithmically generating from a training set a function that seeks to correctly predict a datum&#x0027;s response variable by examining its feature vector.</p></def></def-item>
<def-item><term>Labeled data</term><def><p>Data examples for which the true response value (or label) is known.</p></def></def-item>
<def-item><term>Training set</term><def><p>A set of labeled examples for use during training.</p></def></def-item>
<def-item><term>Test set</term><def><p>A set of labeled examples for use during testing that is independent of the training set.</p></def></def-item>
<def-item><term>Loss function</term><def><p>A measure of how correctly an example&#x0027;s response variable was predicted.</p></def></def-item>
<def-item><term>Risk function</term><def><p>A measure of aggregated loss across an entire training set (e.g. the expected value of the loss function). We wish to minimize the value of the risk function during training.</p></def></def-item>
<def-item><term>Regression</term><def><p>A machine learning task where the value to be predicted for each example is a continuous number.</p></def></def-item>
<def-item><term>Classification</term><def><p>A machine learning task where the value to be predicted for each example is a categorical label.</p></def></def-item>
<def-item><term>Binary classification</term><def><p>A classification task in which there are two possible class labels, often termed positives and negatives.</p></def></def-item>
<def-item><term>Precision</term><def><p>In binary classification, the fraction of all examples classified as positives that are true positives (i.e. the number of true positives divided by the sum of the number of true positives and number of false positives). Also known as the positive predictive value.</p></def></def-item>
<def-item><term>Recall</term><def><p>In binary classification, the fraction of all positives that are correctly predicted as such (i.e. the number of true positives divided by the sum of the number of true positives and number of false negatives). Also known as sensitivity.</p></def></def-item>
<def-item><term>Confusion matrix</term><def><p>A table for visualizing accuracy in multi-class classification, which is simply the contingency table of the true and predicted classes for each example in a test set (see Figure Box 1 for an example).</p></def></def-item>
<def-item><term>Overfitting</term><def><p>When a model has achieved excellent accuracy in training data set but does not generalize well-&#x002D;i.e. the model has been tuned to precisely recognize patterns of noise in this set that are unlikely to be present in an independent test set. Sometimes referred to as overtraining.</p></def></def-item>
<def-item><term><italic>n</italic>-fold cross validation</term><def><p>When only a small set of labeled data are available, cross validation can be used to measure accuracy. This process partitions the labeled data into <italic>n</italic> non-overlapping equally sized sets, and trains the predictor on the union of <italic>n</italic>-1 of these before testing on the remaining set. This is repeated <italic>n</italic> times, so that each of the <italic>n</italic> sets is used as the test set exactly once, and the average accuracy is recorded.</p></def></def-item>
<def-item><term>Boosting</term><def><p>A class machine learning techniques that seek to iteratively construct a set of predictors, weighing each predictor&#x0027;s influence on the final prediction according to its individual accuracy. Additionally, in most algorithms the new predictor to be added to the set focuses on examples that the current set of predictors has struggled with.</p></def></def-item>
<def-item><term>Support vector machine (SVM)</term><def><p>A machine learning approach that seeks to find the hyperplane that optimally separates two classes of training data. These data are often mapped to highdimensional space using a kernel function. Variations of this approach can be performed to accomplish multi-class classification or regression.</p></def></def-item>
<def-item><term>Decision tree</term><def><p>A hierarchical structure that predicts an example&#x0027;s response variable by examining a feature, and branching to the right subtree if the value of that feature is greater than some threshold, and branching to the left otherwise. At the next level of the tree, another feature is examined. The predicted value is determined by which leaf of the tree is reached at the end of this process.</p></def></def-item>
<def-item><term>Random forest</term><def><p>An ensemble of semi-randomly generated decision trees. An example is run through each tree in the forest, and these trees then vote to determine the predicted value. Random forests can perform both classification and regression.</p></def></def-item>
<def-item><term>Artificial neural network (ANN)</term><def><p>A network of layers of one or more &#x201C;neurons&#x201D; which receive inputs from each neuron in the previous layer, and perform a linear combination on these inputs which is then passed through an activation function. The first layer is the input layer (i.e. the feature vector) and the last layer is the output layer, yielding the predicted responses. Intervening layers are referred to as &#x201C;hidden&#x201D; layers.</p></def></def-item>
<def-item><term>Deep learning</term><def><p>Learning using ANNs or similarly networked algorithmic models that contain multiple &#x0022;hidden&#x0022; layers between the input and output layers.</p></def></def-item>
</def-list>
</glossary>
<ref-list><title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Breiman</surname>, <given-names>L.</given-names></string-name> (<year>2001</year>) <article-title>Statistical modeling: The two cultures (with comments and a rejoinder by the author)</article-title>. from: <source>Statistical science</source> <volume>16</volume>, <fpage>199</fpage>&#x2013;<lpage>231</lpage></mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Elyashiv</surname>, <given-names>E.</given-names></string-name> <etal>et al.</etal> (<year>2016</year>) <article-title>A genomic map of the effects of linked selection in Drosophila</article-title>. from: <source>PLoS Genet.</source> <volume>12</volume>, <fpage>e1006130</fpage></mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Hinton</surname>, <given-names>G.</given-names></string-name> <etal>et al.</etal> (<year>2012</year>) <article-title>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</article-title>. from: <source>IEEE Signal Processing Magazine</source> <volume>29</volume>, <fpage>82</fpage>&#x2013;<lpage>97</lpage></mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Sebastiani</surname>, <given-names>F.</given-names></string-name> (<year>2002</year>) <article-title>Machine learning in automated text categorization</article-title>. from: <source>ACM computing from: surveys (CSUR)</source> <volume>34</volume>, <fpage>1</fpage>&#x2013;<lpage>47</lpage></mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal>, <source>Imagenet classification with deep convolutional neural networks, Advances in neural information processing systems</source>, <year>2012</year>, pp. <fpage>1097</fpage>&#x2013;<lpage>1105</lpage></mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Angermueller</surname>, <given-names>C.</given-names></string-name> <etal>et al.</etal> (<year>2016</year>) <article-title>Deep learning for computational biology</article-title>. from: <source>Mol. Syst. Biol.</source> <volume>12</volume>, <fpage>878</fpage></mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Byvatov</surname>, <given-names>E.</given-names></string-name> and <string-name><surname>Schneider</surname>, <given-names>G.</given-names></string-name> (<year>2003</year>) <article-title>Support vector machine applications in bioinformatics</article-title>. from: <source>Appl. Bioinformatics</source> <volume>2</volume>, <fpage>67</fpage></mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Lin</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal> (<year>2011</year>) <article-title>Distinguishing positive selection from neutral evolution: boosting the performance of summary statistics</article-title>. from: <source>Genetics</source> <volume>187</volume>, <fpage>229</fpage>&#x2013;<lpage>244</lpage></mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Mailund</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal> (<year>2011</year>) <article-title>Estimating divergence time and ancestral effective population size of Bornean and Sumatran orangutan subspecies using a coalescent hidden Markov model</article-title>. from: <source>PLoS Genet.</source> <volume>7</volume>, <fpage>e1001319</fpage></mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Kern</surname>, <given-names>A.D.</given-names></string-name> and <string-name><surname>Haussler</surname>, <given-names>D.</given-names></string-name> (<year>2010</year>) <article-title>A population genetic hidden Markov model for detecting genomic regions under selection</article-title>. from: <source>Mol. Biol. Evol</source>. <volume>27</volume>, <fpage>1673</fpage>&#x2013;<lpage>1685</lpage></mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Boitard</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> (<year>2009</year>) <article-title>Detecting selective sweeps: a new approach based on hidden Markov models</article-title>. from: <source>Genetics</source> <volume>181</volume>, <fpage>1567</fpage>&#x2013;<lpage>1578</lpage></mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Pavlidis</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal> (<year>2010</year>) <article-title>Searching for footprints of positive selection in whole-genome SNP data from nonequilibrium populations</article-title>. from: <source>Genetics</source> <volume>185</volume>, <fpage>907</fpage>&#x2013;<lpage>922</lpage></mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Pybus</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> (<year>2015</year>) <article-title>Hierarchical boosting: a machine-learning framework to detect and classify hard selective sweeps in human populations</article-title>. from: <source>Bioinformatics</source> <volume>31</volume>, <fpage>3946</fpage>&#x2013;<lpage>3952</lpage></mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Ronen</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal> (<year>2013</year>) <article-title>Learning natural selection from the site frequency spectrum</article-title>. from: <source>Genetics</source> <volume>195</volume>, <fpage>181</fpage>&#x2013;<lpage>193</lpage></mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Schrider</surname>, <given-names>D.R.</given-names></string-name> and <string-name><surname>Kern</surname>, <given-names>A.D.</given-names></string-name> (<year>2015</year>) <article-title>Inferring selective constraint from population genomic data suggests recent regulatory turnover in the human brain</article-title>. from: <source>Genome Biol. Evol.</source> <volume>7</volume>, <fpage>3511</fpage>&#x2013;<lpage>3528</lpage></mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Schrider</surname>, <given-names>D.R.</given-names></string-name> and <string-name><surname>Kern</surname>, <given-names>A.D.</given-names></string-name> (<year>2016</year>) <article-title>S/HIC: Robust Identification of Soft and Hard Sweeps Using Machine Learning</article-title>. from: <source>PLoS Genet.</source> <volume>12</volume>, <fpage>e1005928</fpage></mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Sheehan</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Song</surname>, <given-names>Y.S.</given-names></string-name> (<year>2016</year>) <article-title>Deep learning for population genetic inference</article-title>. from: <source>PLoS from: Comput. Biol.</source> <volume>12</volume>, <fpage>e1004845</fpage></mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Amit</surname>, <given-names>Y.</given-names></string-name> and <string-name><surname>Geman</surname>, <given-names>D.</given-names></string-name> (<year>1997</year>) <article-title>Shape quantization and recognition with randomized trees</article-title>. from: <source>Neural Comput.</source> <volume>9</volume>, <fpage>1545</fpage>&#x2013;<lpage>1588</lpage></mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="confproc"><string-name><surname>Chen</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal>, <article-title>Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification</article-title>, <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>, <conf-date>2013</conf-date>, pp. <fpage>3025</fpage>&#x2013;<lpage>3032</lpage></mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Kotsiantis</surname>, <given-names>S.B.</given-names></string-name> <etal>et al.</etal>, <source>Supervised machine learning: A review of classification techniques</source>, <year>2007</year>,</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="book"><string-name><surname>Ghahramani</surname>, <given-names>Z.</given-names></string-name> (<year>2004</year>) <chapter-title>Unsupervised learning</chapter-title>. <source>In Advanced lectures on machine learning</source>, pp. <fpage>72</fpage>&#x2013;<lpage>112</lpage>, <publisher-name>Springer</publisher-name></mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Novembre</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> (<year>2008</year>) <article-title>Genes mirror geography within Europe</article-title>. from: <source>Nature</source> <volume>456</volume>, <fpage>98</fpage></mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="confproc"><string-name><surname>Rabiner</surname>, <given-names>L.R.</given-names></string-name> (<conf-date>1989</conf-date>) <article-title>A tutorial on hidden Markov models and selected applications in speech recognition</article-title>. from: <conf-name>Proceedings of the IEEE</conf-name> <volume>77</volume>, <fpage>257</fpage>&#x2013;<lpage>286</lpage></mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Felsenstein</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Churchill</surname>, <given-names>G.A.</given-names></string-name> (<year>1996</year>) <article-title>A Hidden Markov Model approach to variation among sites in rate of evolution</article-title>. from: <source>Mol. Biol. Evol.</source> <volume>13</volume>, <fpage>93</fpage>&#x2013;<lpage>104</lpage></mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Siepel</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal> (<year>2005</year>) <article-title>Evolutionarily conserved elements in vertebrate, insect, worm, and yeast genomes</article-title>. from: <source>Genome Res.</source> <volume>15</volume>, <fpage>1034</fpage>&#x2013;<lpage>1050</lpage></mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Dutheil</surname>, <given-names>J.Y.</given-names></string-name> <etal>et al.</etal> (<year>2009</year>) <article-title>Ancestral population genomics: the coalescent hidden Markov model approach</article-title>. from: <source>Genetics</source> <volume>183</volume>, <fpage>259</fpage>&#x2013;<lpage>274</lpage></mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Hobolth</surname>, <given-names>A.</given-names></string-name> <etal>et al.</etal> (<year>2007</year>) <article-title>Genomic relationships and speciation times of human, chimpanzee, and gorilla inferred from a coalescent hidden Markov model</article-title>. from: <source>PLoS Genet.</source> <volume>3</volume>, <fpage>e7</fpage></mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Boitard</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> (<year>2012</year>) <article-title>Detecting selective sweeps from pooled next-generation sequencing samples</article-title>. from: <source>Mol. Biol. Evol.</source> <volume>29</volume>, <fpage>2177</fpage>&#x2013;<lpage>2186</lpage></mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Cortes</surname>, <given-names>C.</given-names></string-name> and <string-name><surname>Vapnik</surname>, <given-names>V.</given-names></string-name> (<year>1995</year>) <article-title>Support-vector networks</article-title>. from: <source>Machine Learning</source> <volume>20</volume>, <fpage>273</fpage>&#x2013;<lpage>297</lpage></mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Quinlan</surname>, <given-names>J.R.</given-names></string-name> (<year>1986</year>) <article-title>Induction of decision trees</article-title>. from: <source>Machine Learning</source> <volume>1</volume>, <fpage>81</fpage>&#x2013;<lpage>106</lpage></mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Breiman</surname>, <given-names>L.</given-names></string-name> (<year>2001</year>) <article-title>Random forests</article-title>. from: <source>Machine Learning</source> <volume>45</volume>, <fpage>5</fpage>&#x2013;<lpage>32</lpage></mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Schapire</surname>, <given-names>R.E.</given-names></string-name> (<year>1990</year>) <article-title>The strength of weak learnability</article-title>. from: <source>Machine Learning</source> <volume>5</volume>, <fpage>197</fpage>&#x2013;<lpage>227</lpage></mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="book"><string-name><surname>Bishop</surname>, <given-names>C.M.</given-names></string-name> (<year>1995</year>) <source>Neural networks for pattern recognition</source>, <publisher-name>Oxford university press</publisher-name></mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>LeCun</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal> (<year>2015</year>) <article-title>Deep learning</article-title>. from: <source>Nature</source> <volume>521</volume>, <fpage>436</fpage>&#x2013;<lpage>444</lpage></mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Geurts</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal> (<year>2006</year>) <article-title>Extremely randomized trees</article-title>. from: <source>Machine Learning</source> <volume>63</volume>, <fpage>3</fpage>&#x2013;<lpage>42</lpage></mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Fay</surname>, <given-names>J.C.</given-names></string-name> and <string-name><surname>Wu</surname>, <given-names>C.-I.</given-names></string-name> (<year>2000</year>) <article-title>Hitchhiking under positive Darwinian selection</article-title>. from: <source>Genetics</source> <volume>155</volume>, <fpage>1405</fpage>&#x2013;<lpage>1413</lpage></mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Watterson</surname>, <given-names>G.</given-names></string-name> (<year>1975</year>) <article-title>On the number of segregating sites in genetical models without recombination</article-title>. from: <source>Theor. Popul. Biol.</source> <volume>7</volume>, <fpage>256</fpage>&#x2013;<lpage>276</lpage></mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Hudson</surname>, <given-names>R.R.</given-names></string-name> (<year>2002</year>) <article-title>Generating samples under a Wright-Fisher neutral model of genetic variation</article-title>. from: <source>Bioinformatics</source> <volume>18</volume>, <fpage>337</fpage>&#x2013;<lpage>338</lpage></mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Pedregosa</surname>, <given-names>F.</given-names></string-name> <etal>et al.</etal> (<year>2011</year>) <article-title>Scikit-learn: Machine learning in Python</article-title>. from: <source>Journal of Machine from: Learning Research</source> <volume>12</volume>, <fpage>2825</fpage>&#x2013;<lpage>2830</lpage></mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="confproc"><string-name><surname>Liang</surname>, <given-names>P.</given-names></string-name> and <string-name><surname>Jordan</surname>, <given-names>M.I.</given-names></string-name>, <article-title>An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators</article-title>, <conf-name>Proceedings of the 25th international conference on Machine learning, ACM</conf-name>, <conf-date>2008</conf-date>, pp. <fpage>584</fpage>&#x2013;<lpage>591</lpage></mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="confproc"><string-name><surname>Anderson</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal>, <article-title>The more, the merrier: the blessing of dimensionality for learning large gaussian mixtures</article-title>, <conf-name>Conference on Learning Theory</conf-name>, <conf-date>2014</conf-date>, pp. <fpage>1135</fpage>&#x2013;<lpage>1164</lpage></mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name> and <string-name><surname>LeCun</surname>, <given-names>Y.</given-names></string-name> (<year>2007</year>) <article-title>Scaling learning algorithms towards AI</article-title>. from: <source>Large-scale kernel from: machines</source> <volume>34</volume>, <fpage>1</fpage>&#x2013;<lpage>41</lpage></mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><surname>Poggio</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal> (<year>2017</year>) <article-title>Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review</article-title>. from: <source>International Journal of Automation and Computing</source>, <fpage>1</fpage>&#x2013;<lpage>17</lpage></mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>Fu</surname>, <given-names>Y.-X.</given-names></string-name> and <string-name><surname>Li</surname>, <given-names>W.-H.</given-names></string-name> (<year>1993</year>) <article-title>Statistical tests of neutrality of mutations</article-title>. from: <source>Genetics</source> <volume>133</volume>, <fpage>693</fpage>&#x2013;<lpage>709</lpage></mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>Fu</surname>, <given-names>Y.-X.</given-names></string-name> (<year>1997</year>) <article-title>Statistical tests of neutrality of mutations against population growth, hitchhiking and background selection</article-title>. from: <source>Genetics</source> <volume>147</volume>, <fpage>915</fpage>&#x2013;<lpage>925</lpage></mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><surname>Kelly</surname>, <given-names>J.K.</given-names></string-name> (<year>1997</year>) <article-title>A test of neutrality based on interlocus associations</article-title>. from: <source>Genetics</source> <volume>146</volume>, <fpage>1197</fpage>&#x2013;<lpage>1206</lpage></mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>Kim</surname>, <given-names>Y.</given-names></string-name> and <string-name><surname>Nielsen</surname>, <given-names>R.</given-names></string-name> (<year>2004</year>) <article-title>Linkage disequilibrium as a signature of selective sweeps</article-title>. from: <source>Genetics</source> <volume>167</volume>, <fpage>1513</fpage>&#x2013;<lpage>1524</lpage></mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><surname>Tajima</surname>, <given-names>F.</given-names></string-name> (<year>1989</year>) <article-title>Statistical method for testing the neutral mutation hypothesis by DNA polymorphism</article-title>. from: <source>Genetics</source> <volume>123</volume>, <fpage>585</fpage>&#x2013;<lpage>595</lpage></mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><surname>Voight</surname>, <given-names>B.F.</given-names></string-name> <etal>et al.</etal> (<year>2006</year>) <article-title>A map of recent positive selection in the human genome</article-title>. from: <source>PLoS from: Biol.</source> <volume>4</volume>, <fpage>e72</fpage></mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name> (<year>2009</year>) <article-title>Learning deep architectures for AI</article-title>. from: <source>Foundations and trends<sup>&#x00AE;</sup> in Machine from: Learning</source> <volume>2</volume>, <fpage>1</fpage>&#x2013;<lpage>127</lpage></mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><surname>Beaumont</surname>, <given-names>M.A.</given-names></string-name> <etal>et al.</etal> (<year>2002</year>) <article-title>Approximate Bayesian computation in population genetics</article-title>. from: <source>Genetics</source> <volume>162</volume>, <fpage>2025</fpage>&#x2013;<lpage>2035</lpage></mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><string-name><surname>Dunham</surname>, <given-names>I.</given-names></string-name> <etal>et al.</etal> (<year>2012</year>) <article-title>An integrated encyclopedia of DNA elements in the human genome</article-title>. from: <source>Nature</source> <volume>489</volume>, <fpage>57</fpage>&#x2013;<lpage>74</lpage></mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><string-name><surname>Altshuler</surname>, <given-names>D.M.</given-names></string-name> <etal>et al.</etal> (<year>2012</year>) <article-title>An integrated map of genetic variation from 1,092 human genomes</article-title>. from: <source>Nature</source> <volume>491</volume>, <fpage>56</fpage>&#x2013;<lpage>65</lpage></mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><string-name><surname>Simonsen</surname>, <given-names>K.L.</given-names></string-name> <etal>et al.</etal> (<year>1995</year>) <article-title>Properties of statistical tests of neutrality for DNA polymorphism data</article-title>. from: <source>Genetics</source> <volume>141</volume>, <fpage>413</fpage>&#x2013;<lpage>429</lpage></mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><string-name><surname>Meyer</surname>, <given-names>K.A.</given-names></string-name> <etal>et al.</etal> (<year>2017</year>) <article-title>Differential gene expression in the human brain is associated with conserved, but not accelerated, noncoding sequences</article-title>. from: <source>Mol. Biol. Evol.</source> <volume>34</volume>, <fpage>1217</fpage>&#x2013;<lpage>1229</lpage></mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><string-name><surname>Pollard</surname>, <given-names>K.S.</given-names></string-name> <etal>et al.</etal> (<year>2006</year>) <article-title>Forces shaping the fastest evolving regions in the human genome</article-title>. from: <source>PLoS Genet.</source> <volume>2</volume>, <fpage>e168</fpage></mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><string-name><surname>Maynard Smith</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Haigh</surname>, <given-names>J.</given-names></string-name> (<year>1974</year>) <article-title>The hitch-hiking effect of a favourable gene</article-title>. from: <source>Genet. from: Res.</source> <volume>23</volume>, <fpage>23</fpage>&#x2013;<lpage>35</lpage></mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><string-name><surname>Nielsen</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal> (<year>2005</year>) <article-title>A scan for positively selected genes in the genomes of humans and chimpanzees</article-title>. from: <source>PLoS Biol.</source> <volume>3</volume>, <fpage>e170</fpage></mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><string-name><surname>Hermisson</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Pennings</surname>, <given-names>P.S.</given-names></string-name> (<year>2005</year>) <article-title>Soft sweeps molecular population genetics of adaptation from standing genetic variation</article-title>. from: <source>Genetics</source> <volume>169</volume>, <fpage>2335</fpage>&#x2013;<lpage>2352</lpage></mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><string-name><surname>Orr</surname>, <given-names>H.A.</given-names></string-name> and <string-name><surname>Betancourt</surname>, <given-names>A.J.</given-names></string-name> (<year>2001</year>) <article-title>Haldane&#x2019;s sieve and adaptation from the standing genetic variation</article-title>. from: <source>Genetics</source> <volume>157</volume>, <fpage>875</fpage>&#x2013;<lpage>884</lpage></mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><string-name><surname>Schrider</surname>, <given-names>D.R.</given-names></string-name> <etal>et al.</etal> (<year>2015</year>) <article-title>Soft shoulders ahead: spurious signatures of soft and partial selective sweeps result from linked hard sweeps</article-title>. from: <source>Genetics</source> <volume>200</volume>, <fpage>267</fpage>&#x2013;<lpage>284</lpage></mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="journal"><string-name><surname>Jensen</surname>, <given-names>J.D.</given-names></string-name> <etal>et al.</etal> (<year>2005</year>) <article-title>Distinguishing between selective sweeps and demography using DNA polymorphism data</article-title>. from: <source>Genetics</source> <volume>170</volume>, <fpage>1401</fpage>&#x2013;<lpage>1410</lpage></mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><string-name><surname>Nielsen</surname>, <given-names>R.</given-names></string-name> <etal>et al.</etal> (<year>2005</year>) <article-title>Genomic scans for selective sweeps using SNP data</article-title>. from: <source>Genome Res.</source> <volume>15</volume>, <fpage>1566</fpage>&#x2013;<lpage>1575</lpage></mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="journal"><string-name><surname>Schrider</surname>, <given-names>D.R.</given-names></string-name> and <string-name><surname>Kern</surname>, <given-names>A.D.</given-names></string-name> (<year>2017</year>) <article-title>Soft sweeps are the dominant mode of adaptation in the human genome</article-title>. from: <source>Mol. Biol. Evol.</source> <volume>34</volume>, <fpage>1863</fpage>&#x2013;<lpage>1877</lpage></mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="confproc"><string-name><surname>Nei</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Li</surname>, <given-names>W.-H.</given-names></string-name> (<conf-date>1979</conf-date>) <article-title>Mathematical model for studying genetic variation in terms of restriction endonucleases</article-title>. from: <conf-name>Proceedings of the National Academy of Sciences</conf-name> <volume>76</volume>, <fpage>5269</fpage>&#x2013;<lpage>5273</lpage></mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><string-name><surname>Garud</surname>, <given-names>N.R.</given-names></string-name> <etal>et al.</etal> (<year>2015</year>) <article-title>Recent selective sweeps in North American Drosophila melanogaster show signatures of soft sweeps</article-title>. from: <source>PLoS Genet.</source> <volume>11</volume>, <fpage>e1005004</fpage></mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><string-name><surname>Messer</surname>, <given-names>P.W.</given-names></string-name> and <string-name><surname>Petrov</surname>, <given-names>D.A.</given-names></string-name> (<year>2013</year>) <article-title>Population genomics of rapid adaptation by soft selective sweeps</article-title>. from: <source>Trends in Ecology &#x0026; Evolution</source> <volume>28</volume>, <fpage>659</fpage>&#x2013;<lpage>669</lpage></mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><string-name><surname>Beaumont</surname>, <given-names>M.A.</given-names></string-name> (<year>2010</year>) <article-title>Approximate Bayesian computation in evolution and ecology</article-title>. from: <source>Annual review of ecology, evolution, and systematics</source> <volume>41</volume>, <fpage>379</fpage>&#x2013;<lpage>406</lpage></mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><string-name><surname>Blum</surname>, <given-names>M.G.</given-names></string-name> <etal>et al.</etal> (<year>2013</year>) <article-title>A comparative review of dimension reduction methods in approximate Bayesian computation</article-title>. from: <source>Statistical Science</source> <volume>28</volume>, <fpage>189</fpage>&#x2013;<lpage>208</lpage></mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><string-name><surname>Blum</surname>, <given-names>M.G.</given-names></string-name> and <string-name><surname>Fran&#x00E7;is</surname>, <given-names>O.</given-names></string-name> (<year>2010</year>) <article-title>Non-linear regression models for Approximate Bayesian Computation</article-title>. from: <source>Statistics and Computing</source> <volume>20</volume>, <fpage>63</fpage>&#x2013;<lpage>73</lpage></mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><string-name><surname>Gutenkunst</surname>, <given-names>R.N.</given-names></string-name> <etal>et al.</etal> (<year>2009</year>) <article-title>Inferring the joint demographic history of multiple populations from multidimensional SNP frequency data</article-title>. from: <source>PLoS Genet.</source> <volume>5</volume>, <fpage>e1000695</fpage></mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="confproc"><string-name><surname>Hey</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Nielsen</surname>, <given-names>R.</given-names></string-name> (<conf-date>2007</conf-date>) <article-title>Integration within the Felsenstein equation for improved Markov chain Monte Carlo methods in population genetics</article-title>. from: <conf-name>Proceedings of the National Academy of Sciences</conf-name> <volume>104</volume>, <fpage>2785</fpage>&#x2013;<lpage>2790</lpage></mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><string-name><surname>Li</surname>, <given-names>H.</given-names></string-name> and <string-name><surname>Durbin</surname>, <given-names>R.</given-names></string-name> (<year>2011</year>) <article-title>Inference of human population history from individual whole-genome sequences</article-title>. from: <source>Nature</source> <volume>475</volume>, <fpage>493</fpage>&#x2013;<lpage>496</lpage></mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="journal"><string-name><surname>Liu</surname>, <given-names>X.</given-names></string-name> and <string-name><surname>Fu</surname>, <given-names>Y.-X.</given-names></string-name> (<year>2015</year>) <article-title>Exploring population size changes using SNP frequency spectra</article-title>. from: <source>Nat. Genet.</source> <volume>47</volume>, <fpage>555</fpage>&#x2013;<lpage>559</lpage></mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><string-name><surname>Sheehan</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> (<year>2013</year>) <article-title>Estimating variable effective population sizes from multiple genomes: a sequentially Markov conditional sampling distribution approach</article-title>. from: <source>Genetics</source> <volume>194</volume>, <fpage>647</fpage>&#x2013;<lpage>662</lpage></mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><string-name><surname>Aeschbacher</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> (<year>2012</year>) <article-title>A novel approach for choosing summary statistics in approximate Bayesian computation</article-title>. from: <source>Genetics</source> <volume>192</volume>, <fpage>1027</fpage>&#x2013;<lpage>1047</lpage></mixed-citation></ref>
<ref id="c77"><label>77.</label><mixed-citation publication-type="other"><string-name><surname>Jiang</surname>, <given-names>B.</given-names></string-name> <etal>et al.</etal> (<year>2015</year>) <article-title>Learning summary statistic for approximate Bayesian computation via deep neural network</article-title>. from: <source>arXiv preprint</source> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/arXiv:1510.02175">arXiv:1510.02175</ext-link></mixed-citation></ref>
<ref id="c78"><label>78.</label><mixed-citation publication-type="journal"><string-name><surname>Pudlo</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal> (<year>2016</year>) <article-title>Reliable ABC model choice via random forests</article-title>. from: <source>Bioinformatics</source> <volume>32</volume>, <fpage>859</fpage>&#x2013;<lpage>866</lpage></mixed-citation></ref>
<ref id="c79"><label>79.</label><mixed-citation publication-type="other"><string-name><surname>Schrider</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal> (<year>2017</year>) <article-title>Supervised machine learning reveals introgressed loci in the genomes of from: Drosophila simulans and from: D. sechellia</article-title>. <source>bioRxiv</source>, doi: <pub-id pub-id-type="doi">10.1101/170670</pub-id></mixed-citation></ref>
<ref id="c80"><label>80.</label><mixed-citation publication-type="journal"><string-name><surname>Adrian</surname>, <given-names>A.B.</given-names></string-name> <etal>et al.</etal> (<year>2016</year>) <article-title>Predictive models of recombination rate variation across the Drosophila melanogaster genome</article-title>. from: <source>Genome Biol. Evol.</source> <volume>8</volume>, <fpage>2597</fpage>&#x2013;<lpage>2612</lpage></mixed-citation></ref>
<ref id="c81"><label>81.</label><mixed-citation publication-type="journal"><string-name><surname>Gao</surname>, <given-names>F.</given-names></string-name> <etal>et al.</etal> (<year>2016</year>) <article-title>New software for the fast estimation of population recombination rates (FastEPRR) in the genomic era</article-title>. from: <source>G3: Genes, Genomes, Genetics</source> <volume>6</volume>, <fpage>1563</fpage>&#x2013;<lpage>1571</lpage></mixed-citation></ref>
<ref id="c82"><label>82.</label><mixed-citation publication-type="journal"><string-name><surname>Lin</surname>, <given-names>K.</given-names></string-name> <etal>et al.</etal> (<year>2013</year>) <article-title>A fast estimate for the population recombination rate based on regression</article-title>. from: <source>Genetics</source> <volume>194</volume>, <fpage>473</fpage>&#x2013;<lpage>484</lpage></mixed-citation></ref>
<ref id="c83"><label>83.</label><mixed-citation publication-type="journal"><string-name><surname>McVean</surname>, <given-names>G.A.</given-names></string-name> <etal>et al.</etal> (<year>2004</year>) <article-title>The fine-scale structure of recombination rate variation in the human genome</article-title>. from: <source>Science</source> <volume>304</volume>, <fpage>581</fpage>&#x2013;<lpage>584</lpage></mixed-citation></ref>
<ref id="c84"><label>84.</label><mixed-citation publication-type="journal"><string-name><surname>Ewing</surname>, <given-names>G.B.</given-names></string-name> and <string-name><surname>Jensen</surname>, <given-names>J.D.</given-names></string-name> (<year>2016</year>) <article-title>The consequences of not accounting for background selection in demographic inference</article-title>. from: <source>Mol. Ecol.</source> <volume>25</volume>, <fpage>135</fpage>&#x2013;<lpage>141</lpage></mixed-citation></ref>
<ref id="c85"><label>85.</label><mixed-citation publication-type="journal"><string-name><surname>Schrider</surname>, <given-names>D.R.</given-names></string-name> <etal>et al.</etal> (<year>2016</year>) <article-title>Effects of Linked Selective Sweeps on Demographic Inference and Model Selection</article-title>. from: <source>Genetics</source> <volume>204</volume>, <fpage>1207</fpage>&#x2013;<lpage>1223</lpage></mixed-citation></ref>
<ref id="c86"><label>86.</label><mixed-citation publication-type="other"><string-name><surname>Sermanet</surname>, <given-names>P.</given-names></string-name> <etal>et al.</etal> (<year>2013</year>) <article-title>Overfeat: Integrated recognition, localization and detection using convolutional networks</article-title>. from: <source>arXiv preprint</source> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/arXiv:1312.6229">arXiv:1312.6229</ext-link></mixed-citation></ref>
<ref id="c87"><label>87.</label><mixed-citation publication-type="other"><string-name><surname>Graham</surname>, <given-names>B.</given-names></string-name> (<year>2014</year>) <article-title>Fractional max-pooling</article-title>. from: <source>arXiv preprint</source> <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/arXiv:1412.6071">arXiv:1412.6071</ext-link></mixed-citation></ref>
</ref-list>
</back>
</article>