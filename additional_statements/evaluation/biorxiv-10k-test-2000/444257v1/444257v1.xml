<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/444257</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Individual differences in visual salience vary along semantic dimensions</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1722-7757</contrib-id>
<name>
<surname>de Haas</surname>
<given-names>Benjamin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref></contrib>
<contrib contrib-type="author">
<name>
<surname>Iakovidis</surname>
<given-names>Alexios L.</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3686-1622</contrib-id>
<name>
<surname>Schwarzkopf</surname>
<given-names>D. Samuel</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref></contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5390-0684</contrib-id>
<name>
<surname>Gegenfurtner</surname>
<given-names>Karl R.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Psychology, Justus Liebig Universit&#x00E4;t Giessen</institution>, <country>Germany</country></aff>
<aff id="a2"><label>2</label><institution>Experimental Psychology, University College London</institution>, <country>UK</country></aff>
<aff id="a3"><label>3</label><institution>School of Optometry &#x0026; Vision Science, University of Auckland</institution>, <country>New Zealand</country></aff>
</contrib-group>
<author-notes><corresp id="cor1">Corresponding Author: Benjamin de Haas <email>benjamindehaas@gmail.com</email>, Department of Psychology, Justus Liebig Universit&#x00E4;t Giessen, Germany Otto-Behagel-Str. 10F, 35394 Giessen, Germany, Fon: &#x002B;49-641-9926125</corresp></author-notes>
<pub-date pub-type="epub">
<year>2018</year>
</pub-date>
<elocation-id>444257</elocation-id>
<history>
<date date-type="received">
<day>15</day>
<month>10</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>15</day>
<month>10</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>16</day>
<month>10</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="444257.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<p>What determines where we look? Theories of attentional guidance hold that image features and task demands govern fixation behaviour, while differences between observers are interpreted as &#x2018;noise&#x2019;. Here, we investigated the fixations of &#x003E; 100 human adults freely viewing a large set of complex scenes. We found systematic individual differences in fixation frequencies along six semantic stimulus dimensions. These differences were large (&#x003E; twofold) and highly stable across images and time. Surprisingly, they also held for <italic>first</italic> fixations directed towards each image, which are thought to reflect &#x2018;bottom-up&#x2019; visual salience. The dimensions of individual salience and their covariance pattern replicated across samples from three different countries, suggesting they reflect a fundamental biological space of attention. Our findings show that individual observers have stable salience biases, which can be captured along a canonical set of semantic dimensions. Salience reflects features of the observer as well as the image.</p>
</abstract>
<counts>
<page-count count="35"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Humans constantly move their eyes<sup><xref ref-type="bibr" rid="c1">1</xref></sup>. The foveated nature of the human visual system balances detailed representations with a large field of view. On the retina<sup><xref ref-type="bibr" rid="c2">2</xref></sup> and in visual cortex<sup><xref ref-type="bibr" rid="c3">3</xref></sup>, resources are heavily concentrated towards the central visual field, resulting in the inability to resolve peripheral clutter<sup><xref ref-type="bibr" rid="c4">4</xref></sup> and the need to fixate visual objects of interest. Where we move our eyes determines which objects and details we make out in a scene<sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c6">6</xref></sup>.</p>
<p>Models of attentional guidance aim to predict which parts of an image will attract fixations based on image features<sup><xref ref-type="bibr" rid="c7">7</xref>&#x2013;<xref ref-type="bibr" rid="c10">10</xref></sup> and task demands<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c12">12</xref></sup>. Classic salience models compute image discontinuities of low level attributes, such as luminance, colour and orientation<sup><xref ref-type="bibr" rid="c13">13</xref></sup>. These low-level models are inspired by &#x2018;early&#x2019; visual neurons and their output correlates with neural responses in subcortical<sup><xref ref-type="bibr" rid="c14">14</xref></sup> and cortical<sup><xref ref-type="bibr" rid="c15">15</xref></sup> areas thought to represent neural &#x2018;salience maps&#x2019;. However, while these models work relatively well for impoverished stimuli, human gaze behaviour towards richer scenes can be predicted at least as well by the locations of objects<sup><xref ref-type="bibr" rid="c16">16</xref></sup> and perceived meaning<sup><xref ref-type="bibr" rid="c9">9</xref></sup>. When sematic object properties are taken into account, their weight for gaze prediction far exceeds that of low-level attributes<sup><xref ref-type="bibr" rid="c8">8</xref>,<xref ref-type="bibr" rid="c17">17</xref></sup>. A common thread of low- and high-level salience models is that they interpret salience as a property of the image and treat inter-individual differences as unpredictable<sup><xref ref-type="bibr" rid="c7">7</xref>,<xref ref-type="bibr" rid="c18">18</xref></sup>, often using them as a &#x2018;noise ceiling&#x2019; for model evaluations<sup><xref ref-type="bibr" rid="c18">18</xref></sup>.</p>
<p>However, even the earliest studies of fixation behaviour noted considerable individual differences<sup><xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c20">20</xref></sup>, and basic occulomotor traits vary reliably between observers<sup><xref ref-type="bibr" rid="c21">21</xref>&#x2013;<xref ref-type="bibr" rid="c26">26</xref></sup>. Recent twin-studies revealed that social attention and gaze traces across complex scenes are highly heritable<sup><xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup>. This suggests individual differences in fixation behaviour are not random, but systematic. However, it is largely unclear, <italic>how</italic> individuals differ in their fixation behaviour and what may explain these differences. Can individual fixation behaviour be captured along a limited set of dimensions?</p>
<p>Here, we tested the hypothesis that individual gaze reflects <italic>salience biases</italic> along a limited number of semantic dimensions. We investigated the fixation behaviour of &#x003E; 100 human adults freely viewing 700 complex scenes, containing thousands of semantically annotated objects<sup><xref ref-type="bibr" rid="c8">8</xref></sup>. We quantified salience biases as the <italic>individual</italic> proportion of cumulative fixation time or <italic>first</italic> fixations landing on objects with a given semantic attribute. <italic>First</italic> fixations are thought to reflect &#x2018;automatic&#x2019; or &#x2018;bottom-up&#x2019; salience<sup><xref ref-type="bibr" rid="c29">29</xref></sup> and therefore may reveal individual differences with a deep biological root and under little voluntary control. We tested the reliability of such biases across random subsets of images and re-tests after several weeks. To test the generalizability of salience biases, we replicated their set and covariance pattern across independent samples from three different countries. Finally, we explored whether individual salience biases are related to personality and perception, focussing on the example of face salience and face recognition skills for the latter.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Reliable Salience Biases</title>
<p>We tracked the gaze of healthy human adults freely viewing a broad range of images depicting complex everyday scenes<sup><xref ref-type="bibr" rid="c8">8</xref></sup>. A first sample was tested at University College London, UK (<italic>Lon</italic>; <italic>n</italic> &#x003D; 51), and a replication sample at University of Giessen, Germany (<italic>Gi_1</italic>; n &#x003D; 51). The replication sample was also invited for a re-test after two weeks (<italic>Gi_2</italic>; <italic>n</italic> &#x003D; 48). Additionally we re-analysed a public dataset from Singapore (<italic>Xu et al.</italic> <sup><xref ref-type="bibr" rid="c8">8</xref></sup>; <italic>n</italic> &#x003D; 15).</p>
<p>First, we probed the individual tendency to fixate objects with a given semantic attribute, measuring duration-weighted fixations across a free-viewing period of 3s. We considered a total of twelve semantic properties, which have previously been shown to carry more weight for predicting gaze behaviour (on an aggregate group level) than geometric or pixel-level attributes<sup><xref ref-type="bibr" rid="c8">8</xref></sup>. To test the consistency of individual salience biases across independent sets of images, we probed their reliability across 1000 random (half-)splits of 700 images. We found consistent individual salience biases (<italic>r</italic> &#x003E; .6) for six of the 12 semantic attributes: Neutral <italic>Faces</italic>, <italic>Emotion</italic>al Faces, <italic>Text</italic>, objects being <italic>Touched</italic>, objects with a characteristic <italic>Taste</italic> (i.e. food and beverages) and objects with implied <italic>Motion</italic> (<xref ref-type="fig" rid="fig1">Figure 1</xref>, grey scatter plots).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption>
<title>Consistent individual differences in fixation behaviour along six semantic dimensions.</title>
<p>For each semantic attribute, the grey scatter plot shows individual proportions of cumulative fixation time for the odd <italic>versus</italic> even images in the <italic>Lon</italic> dataset. The green scatter plot shows the corresponding individual proportions of <italic>first</italic> fixations after image onset. For each dimension, two example images are given and overlaid with the fixations from one observer strongly attracted by the corresponding attribute (orange frames) and one observer weakly attracted by it (blue frames). The overlays show the first fixation after image onset as a green circle; any subsequent fixations are shown in purple. The two data points corresponding to the example observers are highlighted in the scatter plot, corresponding to the colour of the respective image frames.</p>
</caption>
<graphic xlink:href="444257_fig1.tif"/>
</fig>
<p>Observers showed up to two-fold differences in the cumulative fixation time attracted by a given semantic attribute and the median consistency of individual differences across image splits for these six dimensions ranged from <italic>r</italic> &#x003D; .64 <italic>P</italic> &#x003C; .001 (<italic>Motion</italic>) to r &#x003D; .94, <italic>P</italic> &#x003C; .001 (<italic>Faces</italic>; <xref ref-type="table" rid="tbl1">Table 1</xref>, left hand side; <italic>P</italic>-Values Bonferroni corrected for 12 consistency correlations).</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption>
<title>Range and median consistency of individual differences in fixation behaviour towards six semantic dimensions.</title>
<p>The left hand side of the table shows data for individual differences in the proportion of cumulative fixation time (across three seconds) spent on objects with the respective semantic attributes. The right hand side shows data on individual differences in the proportion of first fixations after image onset landing on objects with those attributes. For each semantic attribute, the data from different samples (<italic>Lon</italic>, <italic>Gi_1, Gi_2</italic> and <italic>Xu et al</italic>.) is presented in separate rows. The range across participants is given in &#x0025; fixation time or &#x0025; of first fixations and the max/min ratio indicates the relative difference between observers attracted the most or least by a given attribute. Pearson correlations (<italic>r</italic>) indicate the median split-half correlation of individual differences across 1000 random image splits and the corresponding <italic>P</italic>-value (Bonferroni corrected for 12 semantic attributes in the <italic>Lon</italic> sample, as indicated by the &#x002A;). Consistency correlations failing to reach statistical significance are highlighted in red. The final row for each attribute shows the corresponding re-test reliability across several weeks.</p>
</caption>
<graphic xlink:href="444257_tbl1.tif"/>
</table-wrap>
<p>Previous studies have argued that extended viewing behaviour is governed by cognitive factors, while early or first fixations towards an image are governed by &#x2018;bottom-up&#x2019; salience<sup><xref ref-type="bibr" rid="c29">29</xref></sup>. Others however, have found that perceived meaning<sup><xref ref-type="bibr" rid="c9">9</xref></sup> and semantic stimulus properties<sup><xref ref-type="bibr" rid="c8">8</xref></sup> are important predictors of gaze behaviour from the first fixation. We found consistent individual differences also in the proportion of <italic>first</italic> fixations directed towards each attribute. The range of individual differences in the proportion of <italic>first</italic> fixations directed to each of the six attributes was even larger than that for all fixations (up to threefold). Importantly, these inter-observer differences were consistent for all dimensions found for cumulative fixation time except <italic>Motion</italic> (<italic>r</italic> &#x003D; .34, <italic>n</italic>.<italic>s</italic>.), ranging from <italic>r</italic> &#x003D; .57, <italic>P</italic> &#x003C; .001 (<italic>Taste</italic>) to r &#x003D; .88, <italic>P</italic> &#x003C; .001 (<italic>Faces</italic>; <xref ref-type="table" rid="tbl1">Table 1</xref>, right hand side and green scatter plots in <xref ref-type="fig" rid="fig1">Figure 1</xref>; <italic>P</italic>-Values Bonferroni corrected for 12 consistency correlations).</p>
<p>These salience biases proved robust for different splits of images and replicated across datasets from three different countries (<xref ref-type="fig" rid="fig2">Figure 2a</xref>; <xref ref-type="table" rid="tbl1">Table 1</xref>). For the confirmatory <italic>Gi_1</italic> dataset we tested the same number of observers as in the original <italic>Lon</italic> set (&#x003E; 95&#x0025; power to detect consistencies of <italic>r</italic> &#x003E; .5). For cumulative fixation time (grey histograms in <xref ref-type="fig" rid="fig2">Figure 2a</xref>), the six dimensions identified in the <italic>Lon</italic> sample (top row), closely replicated in the <italic>Gi_1</italic> and <italic>Gi_2</italic> samples (middle rows), as well as in a re-analysis of the public <italic>Xu et al</italic>. dataset (bottom row), with consistency correlations ranging from .65 (<italic>Motion</italic> in the <italic>Gi_1</italic> set) to .95 (<italic>Faces</italic> in the <italic>Xu et al.</italic> set; left column of <xref ref-type="table" rid="tbl1">Table 1</xref>). Similar was true for <italic>first</italic> fixations, although the consistency correlation for <italic>Emotion</italic> missed statistical significance in the small <italic>Xu et al.</italic> dataset (green histograms in <xref ref-type="fig" rid="fig2">Figure 2a</xref>; right column of <xref ref-type="table" rid="tbl1">Table 1</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption>
<title>Consistency of results across images, datasets and time.</title>
<p><bold>(a)</bold> Distribution plots of consistency correlations for each of the twelve semantic dimensions tested (as indicated by the labels on the x-axis in b). Results are shown separately for the four observer samples tested (as indicated by the row labels). Gi_1 and Gi_2 refer to the first and second appointment of the Gi sample. The grey left-hand leaf of each distribution plot shows a histogram of split-half correlations for 1000 random splits of the image set, the green right hand leaf shows the corresponding histogram for <italic>first</italic> fixations after image onset. Overlaid dots indicate the median consistency correlation for each distribution. High split-half correlations indicate consistent individual differences in fixation behaviour across images for a given dimension. The dashed red line separates the six attributes found to be consistent dimensions of individual differences in the <italic>Lon</italic> sample from the remaining dimensions. (<bold>b</bold>) Re-test reliability across Gi_1 and Gi_2. The magnitude of re-test correlations for individual dwell time and proportion of first fixations is indicated by grey and green bars, respectively. All correlation and <italic>P</italic>-values can be found in <xref ref-type="table" rid="tbl1">Table 1</xref>.</p>
</caption>
<graphic xlink:href="444257_fig2.tif"/>
</fig>
<p>The individual salience biases we found were consistent across subsets of diverse, complex images. To test whether they reflected stable observer traits, we additionally tested their re-test reliability for the full image set across a period of 6-43 days (average 16 days; the <italic>Gi_1</italic> and <italic>Gi_2</italic> datasets). Salience biases along all six semantic dimensions were highly consistent over time (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). This was true for both, cumulative fixation time (re-test reliabilities ranging from <italic>r</italic> &#x003D; .68, <italic>P</italic> &#x003C; .001 (<italic>Motion</italic>) to r &#x003D; .85, <italic>P</italic> &#x003C; .001 (<italic>Faces</italic>); left column of <xref ref-type="table" rid="tbl1">Table 1</xref>; grey bars in <xref ref-type="fig" rid="fig2">Figure 2b</xref>) and <italic>first</italic> fixations (re-test reliabilities ranging from <italic>r</italic> &#x003D; .62, <italic>P</italic> &#x003C; .001 (<italic>Taste</italic>) to r &#x003D; .89, <italic>P</italic> &#x003C; .001 (<italic>Text</italic>); right column of <xref ref-type="table" rid="tbl1">Table 1</xref>; green bars in <xref ref-type="fig" rid="fig2">Figure 2b</xref>).</p>
<p>Finally, we tested the relationship between individual salience and visual field biases. Different types of objects tend to appear at different parts of the visual field and recent findings suggest that observers are attuned to these contingencies<sup><xref ref-type="bibr" rid="c30">30</xref>&#x2013;<xref ref-type="bibr" rid="c32">32</xref></sup>. The most prominent spatial bias in the scene stimuli we used was a strong tendency for faces to appear in the upper visual field (&#x007E;75&#x0025;). Individual face salience (&#x0025; <italic>first</italic> fixations) was indeed correlated with a general upper visual field bias (as indexed by the median elevation of <italic>first</italic> fixations towards images not containing a face; <italic>r</italic> &#x003D; .49-.78 across samples, all <italic>P</italic> &#x003C; .05). Crucially, however, individual differences in face salience persisted independent of this spatial bias. Individual face salience in the <italic>lower</italic> and upper visual field were highly correlated with each other (<italic>r</italic> &#x003D; .61-.87 across samples, all <italic>P</italic> &#x003C; .05; see Supplementary Methods and <xref ref-type="fig" rid="fig1">Supplementary Fig. 1</xref> for details).</p>
</sec>
<sec id="s2b">
<title>Covariance Structure of Salience Biases</title>
<p>Having established reliable individual differences in fixation behaviour along semantic dimensions, we further explored the space of these biases by quantifying the covariance between them. For this analysis we collapsed neutral and emotional faces into a single <italic>Faces</italic> label, because they are semantically related and corresponding biases were strongly correlated with each other (r &#x003D; .74, <italic>P</italic> &#x003C; .001; <italic>r</italic> &#x003D; .81, <italic>P</italic> &#x003C; .001 for cumulative fixation times and first fixations, respectively). Note that we decided to keep these two dimensions separated for the analyses above because the residuals of fixation times for emotional faces still varied consistently when controlling for neutral faces (r &#x003D; .73, <italic>P</italic> &#x003C; .001), indicating an independent component (however the same was not true for <italic>first</italic> fixations, r &#x003D; .24, <italic>n</italic>.<italic>s</italic>.).</p>
<p>The resulting five dimensions showed a pattern of pairwise correlations that allowed the identification of two clusters (<xref ref-type="fig" rid="fig3">Fig. 3b</xref>, left hand side). This was illustrated by the projection of the pairwise (dis-)similarities onto a two-dimensional space, using metric dimensional scaling (<xref ref-type="fig" rid="fig3">Fig. 3b</xref>, right hand side). <italic>Faces</italic> and <italic>Motion</italic> were positively correlated with each other, but negatively with the remaining three attributes <italic>Text</italic>, <italic>Touched</italic> and <italic>Taste</italic>. Interestingly, <italic>Faces,</italic> the most prominent dimension of individual fixation behaviour, was strongly anti-correlated with <italic>Text</italic> and <italic>Touched</italic>, the second and third most prominent dimensions (<italic>Text</italic>: r &#x003D; -.62, <italic>P</italic> &#x003C; .001 and r &#x003D; -.47, <italic>P</italic> &#x003C; .001 for cumulative fixation times and first fixations, respectively, <xref ref-type="fig" rid="fig3">Fig. 3a</xref>, left hand side; <italic>Touched</italic>: r &#x003D; -.58, <italic>P</italic> &#x003C; .001 and r &#x003D; -.80, <italic>P</italic> &#x003C; .001, <xref ref-type="fig" rid="fig3">Fig. 3a</xref>, right hand side). These findings closely replicated across all four datasets (<xref ref-type="fig" rid="fig4">Fig. 4</xref>). Pair-wise correlations between (z-converted) correlation matrices from different samples ranged from .68 to .95 for cumulative fixation times and from .91 to .98 for first fixations.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption>
<title>Covariance of individual differences along semantic dimensions.</title>
<p>(<bold>a</bold>) Grey scatter plots show the individual proportion of cumulative fixation time (in &#x0025;) for <italic>Faces versus Text</italic> (left hand side) and <italic>Faces versus</italic> objects being <italic>Touched</italic> (right hand side). Green scatter plots show the corresponding data for the individual proportion of <italic>first</italic> fixations after image onset. (<bold>b</bold>) Correlation matrix for individual differences along five semantic dimensions (left hand side; note that the labels for emotional and neutral faces were collapsed for this analysis). Colour indicates pairwise Pearson correlation coefficients as indicated by the bar. <italic>Motion</italic> and <italic>Face</italic> are positively correlated with each other, but negatively correlated with the remaining dimensions, as captured by a two-cluster solution of metric dimensional scaling to two dimensions (right hand side). All data shown are based on individual proportions of fixation time in the <italic>Lon</italic> dataset. The corresponding supplemental Figure S1, shows the consistency of this pattern for the proportion of first fixations and across all four datasets.</p>
</caption>
<graphic xlink:href="444257_fig3.tif"/>
</fig>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption>
<title>Consistency of covariance pattern across datasets.</title>
<p>(<bold>a</bold>) shows covariance patterns for dimensions of individual differences in cumulative fixation durations and (<bold>b</bold>) those for individual differences in the proportion of <italic>first</italic> fixations. Each row shows the results for one dataset as indicated (Lon, Gi_1, Gi_2 and Xu <italic>et al</italic>.). Colours of the left hand correlation matrices indicate pairwise Pearson correlation coefficients between dimensions as indicated by the bar. Please refer to <xref ref-type="fig" rid="fig3">Figure 3</xref> for dimension labels (from top to bottom: taste, text, motion, face; from left to right: motion, text, taste, touched). Note the similarity of covariance patterns between datasets. All pairwise correlations between (Fisher z-transformed) correlation matrices were &#x003E;.68 for cumulative fixation durations and &#x003E;.90 for first fixations. The right hand side scatter plots show the results of metric multidimensional scaling onto two dimensions for cumulative fixation duration (<bold>c</bold>) and first fixations (<bold>d</bold>). This analysis yielded a distinct cluster for <italic>Faces</italic> and <italic>Motion</italic>, as well as a cluster for <italic>Text</italic>, <italic>Touched Taste</italic> for first fixations in all datasets. This structure was very similar for cumulative dwell time, with the exception of <italic>Text</italic> separating from <italic>Touched</italic> and <italic>Taste</italic> in the Gi_1 and Gi_2 data.</p>
</caption>
<graphic xlink:href="444257_fig4.tif"/>
</fig>
</sec>
<sec id="s2c">
<title>Correlates of Salience Biases</title>
<p>To explore potential correlates of individual salience biases, we first explored their relationship with personality variables. Observers in the <italic>Lon</italic> sample completed questionnaires for seven personality dimensions, including the Big Five<sup><xref ref-type="bibr" rid="c33">33</xref></sup>, Sensation Seeking<sup><xref ref-type="bibr" rid="c34">34</xref></sup> and High Sensitivity<sup><xref ref-type="bibr" rid="c35">35</xref></sup> (see Methods for details). We tested all 7&#x00D7;6 pairwise correlations between personality traits and individual salience biases, applying family-wise error correction for multiple comparisons<sup><xref ref-type="bibr" rid="c36">36</xref></sup>. Results showed no significant relationship between personality variables and cumulative fixation times (all <italic>r</italic> &#x003C; .39, <italic>n.s.</italic>; <xref ref-type="fig" rid="fig5">Fig. 5b</xref>, left hand side), or <italic>first</italic> fixations (all <italic>r</italic> &#x003C; .23, <italic>n.s.</italic>; <xref ref-type="fig" rid="fig5">Fig. 5b</xref>, right hand side).</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption>
<title>Correlations between fixation behaviour, face recognition skills and personality.</title>
<p>(<bold>a</bold>) Performance on the Cambridge Face Memory Test (CFMT) correlated significantly with the individual proportion of first fixations landing on faces when freely viewing complex scenes (green, right hand side). This correlation was not significant for cumulative fixation time across the viewing time of three seconds (grey, left hand side). (<bold>b</bold>) Neither cumulative fixation time (grey frame, left hand side), nor the proportion of first fixations (green frame, right hand side) towards any of the six dimensions of individual gaze behaviour correlated significantly with any of the tested dimensions of personality (N: Neuroticism, E: Extraversion, O: Openness, A: Assertiveness, C: Conscientiousness, HS: High Sensitivity, SS: Sensation Seeking). All data from the Gi_1 sample.</p>
</caption>
<graphic xlink:href="444257_fig5.tif"/>
</fig>
<p>Next, we aimed to test the perceptual implications of individual salience biases. We focussed on the most prominent salience bias for <italic>Faces</italic>, as indexed by the individual proportion of <italic>first</italic> fixations landing on faces (which is thought to be an indicator of low-level or bottom-up salience<sup><xref ref-type="bibr" rid="c29">29</xref></sup>). 46 observers from the <italic>Gi</italic> sample took the Cambridge Face Memory Test (CFMT) and we tested the correlation between individual face salience and face recognition skills. CFMT scores and the individual proportion of <italic>first</italic> fixations landing on faces correlated with <italic>r</italic> &#x003D; .41, <italic>P</italic> &#x003C; .005. Interestingly, this correlation did not hold for the individual proportion of total cumulative fixation time landing on faces, which likely represents more voluntary biases in viewing behaviour (<italic>r</italic> &#x003D; .21, <italic>n</italic>.<italic>s</italic>.; <xref ref-type="fig" rid="fig5">Fig. 5a</xref>).</p>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>Individual differences in gaze traces have been documented since the earliest days of eyetracking<sup><xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c20">20</xref></sup>. However, salience models have routinely ignored them and the nature of these differences was unclear. Our findings show that they can be captured by a canonical set of semantic salience biases. These salience biases were highly consistent across hundreds of complex scenes, proved reliable in a re-test after several weeks and persisted independently of correlated visual field biases. This shows that visual salience is not just a factor of the image; individual salience biases are a stable trait of the observer.</p>
<p>Interestingly, not only the set of these biases replicated across independent samples from three different countries, but also their covariance structure. This may partly be driven by environmental and image statistics (for instance, faces are more likely to move than food). But it probably also reflects a deep-rooted neurobiological basis of these biases. This possibility is underscored by earlier studies showing that the visual salience of social stimuli is reduced in individuals with autism spectrum disorder<sup><xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c39">39</xref></sup> and most importantly by recent twin studies, showing that individual differences in gaze traces are heritable<sup><xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c28">28</xref></sup>. The gaze trace dissimilarities investigated in these studies probably are a manifestation of the salience biases we found here. Therefore these salience biases likely have a strong genetic component. Future studies should test this hypothesis directly.</p>
<p>Recent findings in macaque show that face deprived monkeys have a drastically reduced fixation bias towards faces and an increased bias towards hands. These differences in fixation behaviour were accompanied by an underdevelopment of domain specific face patches in the temporal cortex and a relative overdevelopment of hand preferring patches<sup><xref ref-type="bibr" rid="c40">40</xref></sup>. In our study, fixation biases towards the <italic>Touched</italic> label imply strong attentional guidance by hands (see <xref ref-type="fig" rid="fig1">Fig. 1</xref> for examples). It is tempting to speculate that the strong anti-correlation we found between fixation biases towards <italic>Faces</italic> and <italic>Touched</italic> reflects individual differences in developmental exposure and cortical layout. The same could apply to the anti-correlation between <italic>Faces</italic> and <italic>Text</italic>, two stimulus classes which have previously been proposed to compete for cortical territory<sup><xref ref-type="bibr" rid="c41">41</xref></sup>. It is worth noting that most of the reliable dimensions of individual salience biases we found correspond to domain specific patches of the ventral path (as is true for <italic>Faces</italic><sup><xref ref-type="bibr" rid="c42">42</xref>&#x2013;<xref ref-type="bibr" rid="c44">44</xref></sup>, <italic>Text</italic><sup><xref ref-type="bibr" rid="c41">41</xref>,<xref ref-type="bibr" rid="c45">45</xref></sup>, <italic>Motion</italic><sup><xref ref-type="bibr" rid="c46">46</xref></sup>, <italic>Touched</italic><sup><xref ref-type="bibr" rid="c40">40</xref>,<xref ref-type="bibr" rid="c47">47</xref>,<xref ref-type="bibr" rid="c48">48</xref></sup> and maybe <italic>Taste</italic><sup><xref ref-type="bibr" rid="c49">49</xref></sup>). Future studies should directly test the relationship between salience biases and the individual functional architecture of the ventral stream.</p>
<p>Finally, our findings raise important questions about the individual nature of visual perception. Two observers presented with the same image likely end up with a different percept<sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c6">6</xref></sup> and interpretation<sup><xref ref-type="bibr" rid="c50">50</xref></sup> of this image when executing systematically different eye movements. Vision scientists may be chasing a phantom when &#x2018;averaging out&#x2019; individual differences to study the &#x2018;typical observer&#x2019;<sup><xref ref-type="bibr" rid="c51">51</xref>&#x2013;<xref ref-type="bibr" rid="c53">53</xref></sup> and <italic>vice versa</italic> perception may be crucial to understand individual differences in cognitive abilities<sup><xref ref-type="bibr" rid="c54">54</xref></sup>, personality<sup><xref ref-type="bibr" rid="c55">55</xref>,<xref ref-type="bibr" rid="c56">56</xref></sup>, social behaviour<sup><xref ref-type="bibr" rid="c27">27</xref>,<xref ref-type="bibr" rid="c38">38</xref></sup> and development<sup><xref ref-type="bibr" rid="c39">39</xref></sup>.</p>
<p>We only took a first step towards investigating the perceptual and social implications of salience biases here. Individual face salience was significantly correlated with face recognition skills. Interestingly, this was only true when considering the proportion of <italic>first</italic> fixations attracted by faces, which likely represents a &#x2018;bottom-up&#x2019; bias under limited voluntary control. This raises questions about the ontological interplay between face salience and recognition. Small initial differences may grow through mutual reinforcement of face fixations and superior perceptual processing. Future studies should track the development of salience biases and their correlates longitudinally.</p>
<p>We found no significant correlations between salience biases and personality factors captured by standard psychometric questionnaires. Given the large number of tests and control for family-wise error, we cannot rule out the existence of small or medium effects. It is still interesting that salience biases are not strongly related to psychometric personality. The salience biases we found have unusually high magnitudes (up to factor 3) and reliability (up to and greater than .9) for objective psychological traits. They appear to document fundamental differences in visual attention and perception, which are main filters of incoming information about our surroundings. Future studies should explore how salience biases can inform and enrich the science of individual differences.</p>
<p>In summary, we found a small set of semantic dimensions that span a space of individual differences in fixation behaviour. These dimensions replicated across culturally diverse samples and also applied to the first fixations directed towards an image. Visual salience is not just a function of the image, but also a trait of the observer.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>Subjects</title>
<p>The study comprised three original datasets (the <italic>Lon</italic>, <italic>Gi_1</italic> and <italic>Gi_2</italic> samples) and the re-analysis of a public dataset (the <italic>Xu et al</italic>. sample8).</p>
<p>54 healthy adults with normal or corrected-to normal vision participated in the experiment conducted at University College London, UK. The local institutional review board approved the study and participants provided written informed consent. Reimbursement was 25&#x00A3;. Eyetracking failed for three participants due to technical problems, leaving 51 in the <italic>Lon</italic> sample (25 males, 4 left handed, mean age 23 with a standard deviation of 4 years). For two of these participants, data from one block was missing due to a recording failure; the remaining data from these participants were entered into the analysis nonetheless. Control-analysis showed that the results remained virtually unchanged when these data were excluded. 46 of the <italic>Lon</italic> participants additionally completed three personality questionnaires administered online: The IPIP-NEO 120 (NEO)<sup><xref ref-type="bibr" rid="c33">33</xref></sup> to measure the Big Five<sup><xref ref-type="bibr" rid="c57">57</xref></sup>; the Highly Sensitive Person Scale (HSP)<sup><xref ref-type="bibr" rid="c35">35</xref></sup> and the Brief Sensation Seeking Scale (BSSS)<sup><xref ref-type="bibr" rid="c34">34</xref></sup>.</p>
<p>51 healthy adults with normal or corrected-to normal vision participated in the experiment conducted at Justus-Liebig Universit&#x00E4;t Giessen, Germany (the <italic>Gi_1</italic> sample; 11 males, 5 left handed, mean age 24, with a standard deviation of 4 years). For one of these participants, data from one block was missing due to a recording failure; the remaining data from this participants were entered into the analysis nonetheless. Control-analysis showed that results remained virtually unchanged when these data were excluded. The local institutional review board approved the study and participants provided written informed consent. Reimbursement was 30&#x20AC; or course credits.</p>
<p>48 of the <italic>Gi_1</italic> participants returned to the lab for a re-test (the <italic>Gi_2</italic> sample). The period between tests was 16 days on average (minimum 6 days; maximum: 43 days; standard deviation: 7 days). 46 of the <italic>Gi</italic> participants additionally completed an online version of the Cambridge Face Memory Test (CFMT)<sup><xref ref-type="bibr" rid="c58">58</xref></sup>.</p>
<p>The public <italic>Xu et al</italic>. dataset comprises 15 participants with a reported age range of 18-30 years<sup><xref ref-type="bibr" rid="c8">8</xref></sup>.</p>
</sec>
<sec id="s4b">
<title>Stimuli</title>
<p>Participants were presented with 700 natural images depicting a wide variety of complex everyday scenes (<ext-link ext-link-type="uri" xlink:href="http://www-users.cs.umn.edu/&#x007E;qzhao/predicting.html">http://www-users.cs.umn.edu/&#x007E;qzhao/predicting.html</ext-link>). Semantic metadata<sup><xref ref-type="bibr" rid="c8">8</xref></sup> consist of binary pixel maps for 5551 objects in these images and accompanying labels for 12 semantic dimensions. We modified the provided labels to minimize overlap between them in the following way: The (neutral) <italic>Faces</italic> label was removed from all objects with the <italic>Emotion</italic> label (i.e. emotional faces); The <italic>Smell</italic> label was removed from all objects with the <italic>Taste</italic> label; The <italic>Operable</italic> and <italic>Gazed</italic> labels were removed from all objects with the <italic>Touched</italic> label; The <italic>Watchable</italic> label was removed from all objects with the <italic>Text</italic> label. This step allowed to test individual differences in fixation attraction for a given dimension largely independently from the others. Without this step some of the attributes would have been perfectly confounded (for instance, all text is watchable; all emotional faces are faces).</p>
<p>In each experiment, participants viewed seven blocks of 100 images on a screen. Stimulus presentation and data collection was coded in MATLAB Version R2016b (MathWorks, Natick, MA) using Psychtoolbox Version 3.0.12<sup><xref ref-type="bibr" rid="c59">59</xref>,<xref ref-type="bibr" rid="c60">60</xref></sup>. For the <italic>Lon</italic> and <italic>Gi_1</italic> samples the order of images was fixed across all participants and each image was presented for 3s, with a self-paced period of a central fixation dot in between. Participants were simply instructed to &#x2018;look at the images in any way [they] want&#x2019; and initiated the onset of each image with a press of the space bar. The re-test of the Giessen sample (<italic>Gi_2</italic>) followed the same procedure as the first appointment, with the exception of the order of images presented, which was shuffled relative to the first appointment (but again constant across participants).</p>
<p>Participants in the <italic>Lon</italic> sample sat at a distance of &#x007E;65 cm from a screen and saw the stimuli at a resolution of 800 &#x00D7; 600 pixels and a size of 19.2 &#x00D7; 14.4 degrees visual angle. Participants in the <italic>Gi</italic> samples sat with their head in a chinrest, at a distance of 46 cm from the screen and saw the stimuli at a resolution and size of 1000&#x00D7;750 pixels and a size of 37.2 &#x00D7; 27.9 degrees visual angle. The <italic>Xu et al</italic>. sample saw stimuli at a size of 33.7 &#x00D7; 25.3 degrees visual angle. Further details on this sample can be found in the original publication by Xu et al.<sup><xref ref-type="bibr" rid="c8">8</xref></sup>.</p>
</sec>
<sec id="s4c">
<title>Data Collection</title>
<p>The gaze of participants in the <italic>Lon</italic> sample was sampled remotely and binocularly with a Tobii EyeX (Tobii Technologies, Danderyd, Sweden) at a frequency of &#x007E;55 Hz<sup><xref ref-type="bibr" rid="c61">61</xref></sup>. Eyetracking data from the <italic>Gi</italic> samples was collected from the left eye with a tower mounted Eyelink 1000 (SR Research, Ottawa, Canada) at a frequency of 2 kHz (the <italic>Xu et al</italic>. data were collected with a remote Eyelink 1000<sup><xref ref-type="bibr" rid="c8">8</xref></sup>).</p>
<p>At the beginning of each block, participants completed a nine-point calibration and validation procedure, which was repeated if necessary. For the <italic>Gi</italic> samples, fixation data was collected online using the &#x2018;normal&#x2019; setting of the Eyelink parser (saccade velocity and acceleration thresholds of 30 d.v.a./s and 9500 d.v.a./s<sup><xref ref-type="bibr" rid="c2">2</xref></sup>, respectively) and the default drift check procedure in each inter-trial interval. Raw data from the <italic>Lon</italic> sample was converted to fixation data offline (see below).</p>
</sec>
<sec id="s4d">
<title>Data Processing and Analyses</title>
<p>Raw eyetracking data from the <italic>Lon</italic> sample were transformed into fixation data applying a saccade threshold of 30 d.v.a./s and taking the median x and y position of fixation samples as the respective fixation location. Fixations were drift-corrected in a block-wise fashion, centering on the median of fixation locations registered at image onsets during the respective block.</p>
<p>Onset fixations and fixations with a duration below 100ms were disregarded for all datasets (minimum fixation duration following standard recommendations by SR Research and Xu <italic>et al.</italic> <sup><xref ref-type="bibr" rid="c8">8</xref></sup>). Fixations that fell on or within a distance of &#x007E;0.5 d.v.a from a labelled object were assigned the corresponding label. Unlabelled fixations were disregarded for the calculation of cumulative fixation times and the individual proportion of <italic>first</italic> fixations (see below).</p>
<p>In order to quantify the individual tendency to fixate objects bearing a given attribute label, we first calculated the cumulative fixation time for all labelled fixations made by a given observer to a given image set. This allowed us to calculate the proportion of this time spent on a given attribute in a second step.</p>
<p>The <italic>first</italic> fixations analysis considered the proportion of labelled <italic>first</italic> fixations (after image onset) landing on objects with a given attribute for a given observer and image set. Individual proportions of cumulative fixation time and first fixations were expressed in &#x0025;.</p>
</sec>
<sec id="s4e">
<title>Consistency and re-test correlations</title>
<p>To estimate the consistency of individual differences in &#x0025; cumulative fixation time or &#x0025; first fixations along a given attribute dimension, we calculated these measures independently for two random halves of the image for each observer and then correlated individual differences for one half of the images with the other using Pearson correlation coefficients. This procedure was repeated 1000 times. We inspected the frequency histograms of all correlations (<xref ref-type="fig" rid="fig2">Fig. 2</xref>) and considered the median correlation coefficient across random image splits as an indicator of consistency. The accompanying two-sided <italic>P</italic>-values were Bonferroni adjusted for twelve comparisons in the <italic>Lon</italic> sample,</p>
<p>To estimate the re-test reliability of individual differences in &#x0025; cumulative fixation time or &#x0025; first fixations for a given attribute, we correlated the corresponding values for the first session of the <italic>Gi</italic> sample with those of the second.</p>
</sec>
<sec id="s4f">
<title>Covariance patterns between dimensions and correlations with other measures</title>
<p>To investigate the covariance pattern between the dimensions of consistent individual differences in fixation behaviour, we inspected pair-wise Pearson correlation matrices (<xref ref-type="fig" rid="fig3">Fig. 3B</xref> and <xref ref-type="fig" rid="fig4">4</xref>, left hand side). We further used metric multidimensional scaling to project the pair-wise dissimilarities between dimensions (defined as (1-<italic>r</italic>)<sup><xref ref-type="bibr" rid="c2">2</xref></sup>) onto distances in a two-dimensional space (<xref ref-type="fig" rid="fig3">Fig. 3B</xref> and <xref ref-type="fig" rid="fig4">4</xref>, right hand side). To further test the stability of covariance patterns between datasets we calculated pair-wise Pearson correlations between Fisher Z-transformed correlation matrices from different samples.</p>
<p>To explore the relationship between personality and individual gaze behaviour, we computed all pairwise correlations between reliable dimensions of salience biases and personality. Specifically we correlated individual salience biases for <italic>Faces</italic>, <italic>Emotion</italic>, <italic>Text</italic>, <italic>Taste</italic>, <italic>Touched</italic> and <italic>Motion</italic> with the Big Five as determined by the IPIP-NEO 120<sup><xref ref-type="bibr" rid="c33">33</xref></sup> <italic>Neuroticism</italic> (N), <italic>Extraversion</italic> (E), <italic>Openness</italic> (O), <italic>Assertiveness</italic> (A), <italic>Conscientiousness</italic> (C), with high sensitivity (HS) as determined by the Highly Sensitive Person scale<sup><xref ref-type="bibr" rid="c35">35</xref></sup>, and with sensation seeking (SS) as determined by the Brief Sensation Seeking Scale<sup><xref ref-type="bibr" rid="c34">34</xref></sup>. The resulting 6&#x00D7;7 correlation matrix was computed separately for the proportion of cumulative fixation time and for that of <italic>first</italic> fixations; accompanying <italic>p</italic>-values were adjusted for multiple testing using the Holm-Bonferroni correction for family-wise errors<sup><xref ref-type="bibr" rid="c36">36</xref></sup>.</p>
<p>To test the perceptual implications of salience biases, we concentrated of the example of faces. Specifically, we probed a correlation between individual face salience as indicated by the proportion of cumulative dwell time or <italic>first</italic> fixations landing on faces and total scores in the Cambridge Face Memory Test<sup><xref ref-type="bibr" rid="c58">58</xref></sup>. This standard test of individual face recognition skills was administered online to participants from the <italic>Gi</italic> sample, after the first eyetracking session.</p>
</sec>
</sec>
</body>
<back>
<sec sec-type="availability">
<title>Availability of data and code</title>
<p>Anonymised fixation data and code to reproduce the results presented here are freely available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/n5v7t/">https://osf.io/n5v7t/</ext-link></p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Gegenfurtner</surname>, <given-names>K. R.</given-names></string-name> <article-title>The Interaction Between Vision and Eye Movements</article-title>. <source>Perception</source> <fpage>1</fpage>&#x2013;<lpage>25</lpage> (<year>2016</year>). doi:<pub-id pub-id-type="doi">10.1177/0301006616657097</pub-id></mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Curcio</surname>, <given-names>C. A.</given-names></string-name> &#x0026; <string-name><surname>Allen</surname>, <given-names>K. A.</given-names></string-name> <article-title>Topography of ganglion cells in human retina</article-title>. <source>J. Comp. Neurol.</source> <volume>300</volume>, <fpage>5</fpage>&#x2013;<lpage>25</lpage> (<year>1990</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Dougherty</surname>, <given-names>R. F.</given-names></string-name> <etal>et al.</etal> <article-title>Visual field representations and locations of visual areas V1/2/3 in human visual cortex</article-title>. <source>J. Vis.</source> <volume>3</volume>, <fpage>586</fpage>&#x2013;<lpage>98</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Rosenholtz</surname>, <given-names>R.</given-names></string-name> <article-title>Capabilities and Limitations of Peripheral Vision</article-title>. <source>Annu. Rev. Vis. Sci.</source> <fpage>437</fpage>&#x2013;<lpage>459</lpage> (<year>2016</year>). doi:<pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035733</pub-id></mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Henderson</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>C. C.</given-names></string-name>, <string-name><surname>Castelhano</surname>, <given-names>M. S.</given-names></string-name> &#x0026; <string-name><surname>Falk</surname>, <given-names>R. J.</given-names></string-name> <article-title>Eye movements and picture processing during recognition</article-title>. <source>Percept. &#x007B;&#x0026;&#x007D; Psychophys.</source> <volume>65</volume>, <fpage>725</fpage>&#x2013;<lpage>734</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Nelson</surname>, <given-names>W. W.</given-names></string-name> &#x0026; <string-name><surname>Loftus</surname>, <given-names>G. R.</given-names></string-name> <article-title>The functional visual field during picture viewing</article-title>. <source>J. Exp. Psychol. Hum. Learn.</source> <volume>6</volume>, <fpage>391</fpage>&#x2013;<lpage>399</lpage> (<year>1980</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Harel</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Koch</surname>, <given-names>C.</given-names></string-name> &#x0026; <string-name><surname>Perona</surname>, <given-names>P.</given-names></string-name> <article-title>Graph-Based Visual Saliency</article-title>. <source>Proceedings of the 19th International Conference on Neural Information Processing Systems</source> <fpage>545</fpage>&#x2013;<lpage>552</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Xu</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Jiang</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kankanhalli</surname>, <given-names>M. S.</given-names></string-name> &#x0026; <string-name><surname>Zhao</surname>, <given-names>Q.</given-names></string-name> <article-title>Predicting human gaze beyond pixels</article-title>. <source>J. Vis.</source> <volume>14</volume>, (<year>2014</year>).</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Henderson</surname>, <given-names>J. M.</given-names></string-name> &#x0026; <string-name><surname>Hayes</surname>, <given-names>T. R.</given-names></string-name> <article-title>Meaning-based guidance of attention in scenes as revealed by meaning maps</article-title>. <source>Nat. Hum. Behav.</source> <volume>1</volume>, <fpage>743</fpage>&#x2013;<lpage>747</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Einhauser</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Spain</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>Perona</surname>, <given-names>P.</given-names></string-name> <article-title>Objects predict fixations better than early saliency</article-title>. <source>J. Vis.</source> <volume>8</volume>, <fpage>18</fpage>&#x2013;<lpage>18</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Borji</surname>, <given-names>A.</given-names></string-name> &#x0026; <string-name><surname>Itti</surname>, <given-names>L.</given-names></string-name> <article-title>Defending Yarbus: eye movements reveal observers&#x2019; task</article-title>. <source>J. Vis.</source> <volume>14</volume>, <fpage>29</fpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Tatler</surname>, <given-names>B. W.</given-names></string-name>, <string-name><surname>Hayhoe</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Land</surname>, <given-names>M. F.</given-names></string-name> &#x0026; <string-name><surname>Ballard</surname>, <given-names>D. H.</given-names></string-name> <article-title>Eye guidance in natural vision: Reinterpreting salience</article-title>. <source>J. Vis.</source> <volume>11</volume>, <fpage>5</fpage>&#x2013;<lpage>5</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Itti</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Koch</surname>, <given-names>C.</given-names></string-name> &#x0026; <string-name><surname>Niebur</surname>, <given-names>E.</given-names></string-name> <article-title>A model of saliency-based visual attention for rapid scene analysis</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source> <volume>20</volume>, <fpage>1254</fpage>&#x2013;<lpage>1259</lpage> (<year>1998</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>White</surname>, <given-names>B. J.</given-names></string-name> <etal>et al.</etal> <article-title>Superior colliculus neurons encode a visual saliency map during free viewing of natural dynamic video</article-title>. <source>Nat. Commun.</source> <volume>8</volume>, <fpage>14263</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Bogler</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Bode</surname>, <given-names>S.</given-names></string-name> &#x0026; <string-name><surname>Haynes</surname>, <given-names>J.-D.</given-names></string-name> <article-title>Decoding successive computational stages of saliency processing</article-title>. <source>Curr. Biol.</source> <volume>21</volume>, <fpage>1667</fpage>&#x2013;<lpage>71</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Stoll</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Thrun</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Nuthmann</surname>, <given-names>A.</given-names></string-name> &#x0026; <string-name><surname>Einh&#x00E4;user</surname>, <given-names>W.</given-names></string-name> <article-title>Overt attention in natural scenes: objects dominate features</article-title>. <source>Vision Res.</source> <volume>107</volume>, <fpage>36</fpage>&#x2013;<lpage>48</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="book"><string-name><surname>Kummerer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wallis</surname>, <given-names>T. S. A.</given-names></string-name>, <string-name><surname>Gatys</surname>, <given-names>L. A.</given-names></string-name> &#x0026; <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name> <chapter-title>Understanding Low- and High-Level Contributions to Fixation Prediction</chapter-title>. in <source>2017 IEEE International Conference on Computer Vision (ICCV)</source> <fpage>4799</fpage>&#x2013;<lpage>4808</lpage> (<publisher-name>IEEE</publisher-name>, <year>2017</year>). doi:<pub-id pub-id-type="doi">10.1109/ICCV.2017.513</pub-id></mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>K&#x00FC;mmerer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Wallis</surname>, <given-names>T. S. A.</given-names></string-name> &#x0026; <string-name><surname>Bethge</surname>, <given-names>M.</given-names></string-name> <article-title>Information-theoretic model comparison unifies saliency metrics</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A.</source> <volume>112</volume>, <fpage>16054</fpage>&#x2013;<lpage>9</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="book"><string-name><surname>Buswell</surname>, <given-names>G. T.</given-names></string-name>. <source>How people look at pictures: a study of the psychology and perception in art. How people look at pictures: a study of the psychology and perception in art.</source> (<publisher-name>Univ. Chicago Press</publisher-name>, <year>1935</year>).</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="book"><string-name><surname>Yarbus</surname>, <given-names>A. L.</given-names></string-name> in <source>Eye Movements and Vision</source> <fpage>171</fpage>&#x2013;<lpage>211</lpage> (<publisher-name>Springer</publisher-name> <publisher-loc>US</publisher-loc>, <year>1967</year>). doi:<pub-id pub-id-type="doi">10.1007/978-1-4899-5379-7_8</pub-id></mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Andrews</surname>, <given-names>T. J.</given-names></string-name> &#x0026; <string-name><surname>Coppola</surname>, <given-names>D. M.</given-names></string-name> <article-title>Idiosyncratic characteristics of saccadic eye movements when viewing different visual environments</article-title>. <source>Vision Res.</source> <volume>39</volume>, <fpage>2947</fpage>&#x2013;<lpage>53</lpage> (<year>1999</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Castelhano</surname>, <given-names>M. S.</given-names></string-name> &#x0026; <string-name><surname>Henderson</surname>, <given-names>J. M.</given-names></string-name> <article-title>Stable individual differences across images in human saccadic eye movements</article-title>. <source>Can. J. Exp. Psychol.</source> <volume>62</volume>, <fpage>1</fpage>&#x2013;<lpage>14</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Henderson</surname>, <given-names>J. M.</given-names></string-name> &#x0026; <string-name><surname>Luke</surname>, <given-names>S. G.</given-names></string-name> <article-title>Stable individual differences in saccadic eye movements during reading, pseudoreading, scene viewing, and scene search</article-title>. <source>J. Exp. Psychol. Hum. Percept. Perform.</source> <volume>40</volume>, <fpage>1390</fpage>&#x2013;<lpage>400</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Meyh&#x00F6;fer</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Bertsch</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Esser</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>Ettinger</surname>, <given-names>U.</given-names></string-name> <article-title>Variance in saccadic eye movements reflects stable traits</article-title>. <source>Psychophysiology</source> <volume>53</volume>, <fpage>566</fpage>&#x2013;<lpage>578</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Rigas</surname>, <given-names>I.</given-names></string-name> &#x0026; <string-name><surname>Komogortsev</surname>, <given-names>O. V.</given-names></string-name> <article-title>Current research in eye movement biometrics: An analysis based on BioEye 2015 competition</article-title>. <source>Image Vis. Comput.</source> (<year>2016</year>). doi:<pub-id pub-id-type="doi">10.1016/j.imavis.2016.03.014</pub-id></mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Bargary</surname>, <given-names>G.</given-names></string-name> <etal>et al.</etal> <article-title>Individual differences in human eye movements: An oculomotor signature</article-title>. <source>Vision Res.</source> <volume>141</volume>, <fpage>157</fpage>&#x2013;<lpage>169</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Constantino</surname>, <given-names>J. N.</given-names></string-name> <etal>et al.</etal> <article-title>Infant viewing of social scenes is under genetic control and is atypical in autism</article-title>. <source>Nature</source> <volume>547</volume>, <fpage>340</fpage>&#x2013;<lpage>344</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Kennedy</surname>, <given-names>D. P.</given-names></string-name> <etal>et al.</etal> <article-title>Genetic Influence on Eye Movements to Complex Scenes at Short Timescales</article-title>. <source>Curr. Biol.</source> <volume>27</volume>, <fpage>3554</fpage>&#x2013;<lpage>3560.e3</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Anderson</surname>, <given-names>N. C.</given-names></string-name>, <string-name><surname>Ort</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Kruijne</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Meeter</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>Donk</surname>, <given-names>M.</given-names></string-name> <article-title>It depends on <italic>when</italic> you look at it: Salience influences eye movements in natural scene viewing and search early in time</article-title>. <source>J. Vis.</source> <volume>15</volume>, <fpage>9</fpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Kaiser</surname>, <given-names>D.</given-names></string-name> &#x0026; <string-name><surname>Cichy</surname>, <given-names>R. M.</given-names></string-name> <article-title>Typical visual-field locations enhance processing in object-selective channels of human occipital cortex</article-title>. <source>J. Neurophysiol.</source> <volume>120</volume>, <fpage>848</fpage>&#x2013;<lpage>853</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>de Haas</surname>, <given-names>B.</given-names></string-name> <etal>et al.</etal> <article-title>Perception and Processing of Faces in the Human Brain Is Tuned to Typical Feature Locations</article-title>. <source>J. Neurosci.</source> <volume>36</volume>, (<year>2016</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>de Haas</surname>, <given-names>B.</given-names></string-name> &#x0026; <string-name><surname>Schwarzkopf</surname>, <given-names>D. S.</given-names></string-name> <article-title>Feature-location effects in the Thatcher illusion</article-title>. <source>J. Vis.</source> <volume>18</volume>, (<year>2018</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Maples</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Guan</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Carter</surname>, <given-names>N. T.</given-names></string-name> &#x0026; <string-name><surname>Miller</surname>, <given-names>J. D.</given-names></string-name> <article-title>A test of the International Personality Item Pool representation of the Revised NEO Personality Inventory and development of a 120-item IPIP-based measure of the five-factor model</article-title>. <source>Psychol. Assess.</source> <volume>26</volume>, <fpage>1070</fpage>&#x2013;<lpage>84</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Hoyle</surname>, <given-names>R. H.</given-names></string-name>, <string-name><surname>Stephenson</surname>, <given-names>M. T.</given-names></string-name>, <string-name><surname>Palmgreen</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Lorch</surname>, <given-names>E. P.</given-names></string-name> &#x0026; <string-name><surname>Donohew</surname>, <given-names>R. L.</given-names></string-name> <article-title>Reliability and validity of a brief measure of sensation seeking</article-title>. <source>Pers. Individ. Dif.</source> <volume>32</volume>, <fpage>401</fpage>&#x2013;<lpage>414</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Aron</surname>, <given-names>E. N.</given-names></string-name> &#x0026; <string-name><surname>Aron</surname>, <given-names>A.</given-names></string-name> <article-title>Sensory-processing sensitivity and its relation to introversion and emotionality</article-title>. <source>J. Pers. Soc. Psychol.</source> <volume>73</volume>, <fpage>345</fpage>&#x2013;<lpage>368</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Holm</surname>, <given-names>S.</given-names></string-name> <article-title>A simple sequentially rejective multiple test procedure</article-title>. <source>Scand. J. Stat.</source> (<year>1979</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Klin</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Jones</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Schultz</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Volkmar</surname>, <given-names>F.</given-names></string-name> &#x0026; <string-name><surname>Cohen</surname>, <given-names>D.</given-names></string-name> <article-title>Visual fixation patterns during viewing of naturalistic social situations as predictors of social competence in individuals with autism</article-title>. <source>Arch. Gen. Psychiatry</source> <volume>59</volume>, <fpage>809</fpage>&#x2013;<lpage>16</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title>Atypical Visual Saliency in Autism Spectrum Disorder Quantified through Model-Based Eye Tracking</article-title>. <source>Neuron</source> <volume>88</volume>, <fpage>604</fpage>&#x2013;<lpage>16</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Jones</surname>, <given-names>W.</given-names></string-name> &#x0026; <string-name><surname>Klin</surname>, <given-names>A.</given-names></string-name> <article-title>Attention to eyes is present but in decline in 2-6-month-old infants later diagnosed with autism</article-title>. <source>Nature</source> <volume>504</volume>, <fpage>427</fpage>&#x2013;<lpage>31</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Arcaro</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Schade</surname>, <given-names>P. F.</given-names></string-name>, <string-name><surname>Vincent</surname>, <given-names>J. L.</given-names></string-name>, <string-name><surname>Ponce</surname>, <given-names>C. R.</given-names></string-name> &#x0026; <string-name><surname>Livingstone</surname>, <given-names>M. S.</given-names></string-name> <article-title>Seeing faces is necessary for face-domain formation</article-title>. <source>Nat. Neurosci.</source> <volume>20</volume>, <fpage>1404</fpage>&#x2013;<lpage>1412</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title>How learning to read changes the cortical networks for vision and language</article-title>. <source>Science</source> <volume>330</volume>, <fpage>1359</fpage>&#x2013;<lpage>64</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name> &#x0026; <string-name><surname>Yovel</surname>, <given-names>G.</given-names></string-name> <article-title>The fusiform face area: a cortical region specialized for the perception of faces</article-title>. <source>Philos. Trans. R. Soc. Lond. B. Biol. Sci.</source> <volume>361</volume>, <fpage>2109</fpage>&#x2013;<lpage>28</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><surname>Tsao</surname>, <given-names>D. Y.</given-names></string-name>, <string-name><surname>Moeller</surname>, <given-names>S.</given-names></string-name> &#x0026; <string-name><surname>Freiwald</surname>, <given-names>W. A.</given-names></string-name> <article-title>Comparing face patch systems in macaques and humans</article-title>. <source>Proc. Natl. Acad. Sci. U. S. A.</source> <volume>105</volume>, <fpage>19514</fpage>&#x2013;<lpage>9</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name> &#x0026; <string-name><surname>Weiner</surname>, <given-names>K. S.</given-names></string-name> <article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title>. <source>Nat. Rev. Neurosci.</source> <volume>15</volume>, <fpage>536</fpage>&#x2013;<lpage>548</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>McCandliss</surname>, <given-names>B. D.</given-names></string-name>, <string-name><surname>Cohen</surname>, <given-names>L.</given-names></string-name> &#x0026; <string-name><surname>Dehaene</surname>, <given-names>S.</given-names></string-name> <article-title>The visual word form area: expertise for reading in the fusiform gyrus</article-title>. <source>Trends Cogn. Sci.</source> <volume>7</volume>, <fpage>293</fpage>&#x2013;<lpage>299</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><surname>Kourtzi</surname>, <given-names>Z.</given-names></string-name> &#x0026; <string-name><surname>Kanwisher</surname>, <given-names>N.</given-names></string-name> <article-title>Activation in human MT/MST by static images with implied motion</article-title>. <source>J. Cogn. Neurosci.</source> <volume>12</volume>, <fpage>48</fpage>&#x2013;<lpage>55</lpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>Orlov</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Makin</surname>, <given-names>T. R.</given-names></string-name> &#x0026; <string-name><surname>Zohary</surname>, <given-names>E.</given-names></string-name> <article-title>Topographic representation of the human body in the occipitotemporal cortex</article-title>. <source>Neuron</source> <volume>68</volume>, <fpage>586</fpage>&#x2013;<lpage>600</lpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><surname>Weiner</surname>, <given-names>K. S.</given-names></string-name> &#x0026; <string-name><surname>Grill-Spector</surname>, <given-names>K.</given-names></string-name> <article-title>Neural representations of faces and limbs neighbor in human high-level visual cortex: evidence for a new organization principle</article-title>. <source>Psychol. Res.</source> <volume>77</volume>, <fpage>74</fpage>&#x2013;<lpage>97</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><surname>Adamson</surname>, <given-names>K.</given-names></string-name> &#x0026; <string-name><surname>Troiani</surname>, <given-names>V.</given-names></string-name> <article-title>Distinct and overlapping fusiform activation to faces and food</article-title>. <source>Neuroimage</source> <volume>174</volume>, <fpage>393</fpage>&#x2013;<lpage>406</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Bush</surname>, <given-names>J. C.</given-names></string-name>, <string-name><surname>Pantelis</surname>, <given-names>P. C.</given-names></string-name>, <string-name><surname>Morin Duchesne</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Kagemann</surname>, <given-names>S. A.</given-names></string-name> &#x0026; <string-name><surname>Kennedy</surname>, <given-names>D. P.</given-names></string-name> <article-title>Viewing Complex, Dynamic Scenes &#x201C;Through the Eyes&#x201D; of Another Person: The Gaze-Replay Paradigm</article-title>. <source>PLoS One</source> <volume>10</volume>, <fpage>e0134347</fpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><surname>Wilmer</surname>, <given-names>J. B.</given-names></string-name> <article-title>How to use individual differences to isolate functional organization, biology, and utility of visual functions; with illustrative proposals for stereopsis</article-title>. <source>Spat. Vis.</source> <volume>21</volume>, <fpage>561</fpage>&#x2013;<lpage>79</lpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><string-name><surname>Charest</surname>, <given-names>I.</given-names></string-name> &#x0026; <string-name><surname>Kriegeskorte</surname>, <given-names>N.</given-names></string-name> <article-title>The brain of the beholder: honouring individual representational idiosyncrasies</article-title>. <source>Lang. Cogn. Neurosci.</source> <volume>30</volume>, <fpage>367</fpage>&#x2013;<lpage>379</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><string-name><surname>Peterzell</surname>, <given-names>D.</given-names></string-name> <article-title>Discovering Sensory Processes Using Individual Differences: A Review and Factor Analytic Manifesto</article-title>. <source>Electron. Imaging</source> <fpage>1</fpage>&#x2013;<lpage>11</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><string-name><surname>Haldemann</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Stauffer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Troche</surname>, <given-names>S.</given-names></string-name> &#x0026; <string-name><surname>Rammsayer</surname>, <given-names>T.</given-names></string-name> <article-title>Processing Visual Temporal Information and Its Relationship to Psychometric Intelligence</article-title>. <source>J. Individ. Differ.</source> <volume>32</volume>, <fpage>181</fpage>&#x2013;<lpage>188</lpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><string-name><surname>Troche</surname>, <given-names>S. J.</given-names></string-name> &#x0026; <string-name><surname>Rammsayer</surname>, <given-names>T. H.</given-names></string-name> <article-title>Attentional blink and impulsiveness: evidence for higher functional impulsivity in non-blinkers compared to blinkers</article-title>. <source>Cogn. Process.</source> <volume>14</volume>, <fpage>273</fpage>&#x2013;<lpage>81</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><string-name><surname>Wu</surname>, <given-names>D. W.-L.</given-names></string-name>, <string-name><surname>Bischof</surname>, <given-names>W. F.</given-names></string-name>, <string-name><surname>Anderson</surname>, <given-names>N. C.</given-names></string-name>, <string-name><surname>Jakobsen</surname>, <given-names>T.</given-names></string-name> &#x0026; <string-name><surname>Kingstone</surname>, <given-names>A.</given-names></string-name> <article-title>The influence of personality on social attention</article-title>. <source>Pers. Individ. Dif.</source> <volume>60</volume>, <fpage>25</fpage>&#x2013;<lpage>29</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><string-name><surname>Costa</surname>, <given-names>P. T.</given-names></string-name> &#x0026; <string-name><surname>McCrae</surname>, <given-names>R. R.</given-names></string-name> <article-title>Four ways five factors are basic</article-title>. <source>Pers. Individ. Dif.</source> <volume>13</volume>, <fpage>653</fpage>&#x2013;<lpage>665</lpage> (<year>1992</year>).</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="journal"><string-name><surname>Duchaine</surname>, <given-names>B.</given-names></string-name> &#x0026; <string-name><surname>Nakayama</surname>, <given-names>K.</given-names></string-name> <article-title>The Cambridge Face Memory Test: results for neurologically intact individuals and an investigation of its validity using inverted face stimuli and prosopagnosic participants</article-title>. <source>Neuropsychologia</source> <volume>44</volume>, <fpage>576</fpage>&#x2013;<lpage>85</lpage> (<year>2006</year>).</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><string-name><surname>Kleiner</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Brainard</surname>, <given-names>D.</given-names></string-name> &#x0026; <string-name><surname>Pelli</surname>, <given-names>D.</given-names></string-name> <article-title>What&#x2019;s new in Psychtoolbox-3</article-title>. <source>Perception</source> <volume>36</volume>, <fpage>14</fpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="journal"><string-name><surname>Pelli</surname>, <given-names>D. G.</given-names></string-name> <article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title>. <source>Spat. Vis.</source> <volume>10</volume>, <fpage>437</fpage>&#x2013;<lpage>42</lpage> (<year>1997</year>).</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><string-name><surname>Gibaldi</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Vanegas</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bex</surname>, <given-names>P. J.</given-names></string-name> &#x0026; <string-name><surname>Maiello</surname>, <given-names>G.</given-names></string-name> <article-title>Evaluation of the Tobii EyeX Eye tracking controller and Matlab toolkit for research</article-title>. <source>Behav. Res. Methods</source> <volume>49</volume>, <fpage>923</fpage>&#x2013;<lpage>946</lpage> (<year>2017</year>).</mixed-citation></ref>
</ref-list>
<ack>
<title>Acknowledgements</title>
<p>This work was supported by a JUST&#x2019;US fellowship from the University of Giessen (B.d.H), as well as a research fellowship by the Deutsche Forschungsgemeinschaft (HA 7574/1-1; B.d.H.). We would like to thank Xu et al. for publishing their stimuli and dataset, Dr. Pete R. Jones for binding code bridging between the Eye-X C library and MATLAB, as well as Ms Diana Weissleder for help with collecting the <italic>Gi</italic> dataset.</p>
</ack>
<sec id="s5" sec-type="supplementary-material">
<title>Supplementary Information</title>
<sec id="s5a">
<title>Supplementary Methods</title>
<p>To test the relationship between individual salience and visual field biases, we first explored the distribution of objects corresponding to dimensions of individual salience across image space (<xref ref-type="fig" rid="fig1">Supplementary Fig. 1a</xref>; neutral and emotional faces collapsed into <italic>Faces</italic>). The most prominent spatial bias in the scene stimuli was a strong tendency for faces to appear in the upper visual field (&#x007E;75&#x0025; of faces).</p>
<p>Therefore, we tested whether individual face salience (individual &#x0025; of <italic>first</italic> fixations landing on faces) correlated with a general upper visual field bias. We defined the latter as the median elevation of <italic>first</italic> fixations towards images <italic>not</italic> containing a face (relative to central fixation and expressed in &#x0025; image height). This was indeed the case (<italic>r</italic> &#x003D; .49-.78 across samples, all <italic>P</italic> &#x003C; .05; <xref ref-type="fig" rid="fig1">Supplementary Fig. 1b</xref>).</p>
<p>Given a correlation between individual face salience and a general bias towards the upper visual field, we wanted to test whether the salience bias <italic>depended</italic> on the visual field bias. Therefore, we tested whether individual differences in face salience persisted for the lower visual field, correlating the individual percentage of first fixations landing on faces in the upper visual field (UVF) with that landing on faces in the lower visual field (LVF). For this analysis, we defined UVF and LVF faces as those for which the median height of the pixel mask was in the upper and lower image half, respectively. Importantly, individual face salience was stable across the UVF and LVF, showing that it persisted independently of the UVF bias (<italic>r</italic> &#x003D; .61-.87 across samples, all <italic>P</italic> &#x003C; .05; <xref ref-type="fig" rid="fig1">Supplementary Fig. 1c</xref>).</p>
<fig id="figS1" position="float" orientation="portrait" fig-type="figure">
<label>Supplementary Figure 1.</label>
<caption>
<title>Individual salience and visual field biases.</title>
<p><bold>(a)</bold> Distribution of different types of objects in image space across scenes. Notice the upwards bias of Faces. <bold>(b)</bold> Individual face salience significantly correlated with a general upper visual field bias in all four samples (i.e. with the elevation of first fixations towards images without a face; see Suppelemtary Methods above for details). <bold>(c)</bold>. Nevertheless, individual face salience persisted independent of this spatial bias. The individual proportion of first fixations landing in the upper and lower visual field (UVF and LVF) were highly correlated with each other.</p>
</caption>
<graphic xlink:href="444257_figS1.tif"/>
</fig>
</sec>
</sec>
</back>
</article>