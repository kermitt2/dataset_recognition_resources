<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/432229</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The effect of facial expression on contrast sensitivity: a behavioural investigation and extension of Hedger, Garner, &#x0026; Adams (2015)</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9522-8355</contrib-id>
<name>
<surname>Webb</surname>
<given-names>Abigail L. M.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="corresp" rid="cor1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6133-6729</contrib-id>
<name>
<surname>Hibbard</surname>
<given-names>Paul B.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Psychology, University of Essex</institution>, Essex, <country>UK</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><label>&#x002A;</label>Corresponding author</corresp>
<fn fn-type="other"><p>E-mail: <email>awebbc@essex.ac.uk</email> (AW)</p></fn>
</author-notes>
<pub-date pub-type="epub"><year>2018</year></pub-date>
<elocation-id>432229</elocation-id>
<history>
<date date-type="received">
<day>01</day>
<month>10</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>01</day>
<month>10</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>01</day>
<month>10</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="432229.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>It has been argued that rapid visual processing for fearful face expressions is driven by the fact that effective contrast is higher in these faces compared to other expressions, when the contrast sensitivity function is taken into account (Hedger, Garner, &#x0026; Adams, 2015). This proposal has been upheld by data from image analyses, but is yet to be tested at the behavioural level. The present study conducts a traditional contrast sensitivity task for face images of various facial expressions. Findings show that visual contrast thresholds do not differ for different facial expressions We re-conduct analysis of faces&#x2019; effective contrast, using the procedure developed by Hedger, Garner, &#x0026; Adams (2015), and show that higher effective contrast in fearful face expressions relies on face images first being normalised for RMS contrast. When not normalised for RMS contrast, effective contrast in fear expressions is no different, or sometimes even lower, compared to other expressions. These findings are discussed in relation to the implications of contrast normalisation on the salience of face expressions in behavioural and neurophysiological experiments, and also the extent that natural physical differences between facial stimuli are masked during stimulus standardisation and normalisation.</p>
</abstract>
<counts>
<page-count count="32"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Fearful facial expressions are particularly salient to the human visual system, receiving preferential allocation of attentional resources, and inhibiting this attention from relocating to different stimuli<sup>[<xref ref-type="bibr" rid="c1">1</xref>&#x2013;<xref ref-type="bibr" rid="c4">4</xref>]</sup>. This attentional effect is also found when fearful faces appear in peripheral vision<sup>[<xref ref-type="bibr" rid="c5">5</xref>&#x2013;<xref ref-type="bibr" rid="c6">6</xref>]</sup>. When fearful expressions compete with salient noise stimuli for visual awareness, with the face and noise presented to different eyes, they break suppression faster compared to neutral faces<sup>[<xref ref-type="bibr" rid="c7">7</xref>&#x2013;<xref ref-type="bibr" rid="c8">8</xref>]</sup>, and are associated with increased activity in subcortical threat-processing regions even when observers report not having observed a face<sup>[<xref ref-type="bibr" rid="c5">5</xref>&#x2013;<xref ref-type="bibr" rid="c6">6</xref>, <xref ref-type="bibr" rid="c9">9</xref>]</sup>. These findings converge on the notion that the human visual system has evolved specific visual neural mechanisms that enable rapid identification of fearful expressions. This concept is reminiscent of LeDoux&#x2019;s<sup>[<xref ref-type="bibr" rid="c10">10</xref>]</sup> &#x2018;quick and dirty pathway&#x2019; for processing environmental information necessary for successful threat-avoidance.</p>
<p>A visual stimulus might be selectively processed for two reasons: because it is semantically and meaningfully relevant, or because its configuration is somehow congruent with low-level mechanisms in early vision that allow for it to be rapidly and efficiently processed. In terms of the threat bias for fearful faces, this means that fearful faces may be prioritised because of their emotional relevance, or their low-level image properties. The latter, low-level approach has been a particular focus within visual psychophysics, where studies have shown that it is specifically the low spatial frequency information in fearful faces that gives rise to the saliency effects associated with fearful expressions<sup>[<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c11">11</xref>&#x2013;<xref ref-type="bibr" rid="c12">12</xref>]</sup>.. Low frequency components of fear expressions are thought to undergo rapid processing via low-frequency-sensitive subcortical pathways that directly access the amygdala<sup>[<xref ref-type="bibr" rid="c11">11</xref>&#x2013;<xref ref-type="bibr" rid="c12">12</xref>]</sup>. Such findings are interpreted as evidence of visual mechanisms that selectively respond to signals present in fearful faces<sup>[<xref ref-type="bibr" rid="c13">13</xref>&#x2013;<xref ref-type="bibr" rid="c14">14</xref>]</sup>. Hedger, Garner, &#x0026; Adams<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup> propose an equally low-level, but directionally different account, arguing that stimulus properties characteristic of fearful expressions ensure strong responses in the early stages of visual processing<sup>[<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c15">15</xref>]</sup>. Gray<sup>[<xref ref-type="bibr" rid="c7">7</xref>]</sup> and Hedger<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup> make use of this sensory bias hypothesis to explain how perceptual biases for fear expressions may be accounted for by the way in which their physical attributes are well matched to the sensitivity of early visual processing, as opposed to the recruitment of attentional mechanisms that preferentially respond to expressions of fear<sup>[<xref ref-type="bibr" rid="c15">15</xref>, <xref ref-type="bibr" rid="c16">16</xref>]</sup>. Here, a distinction is made between stimulus detection that arises from attentional processes, and that which occurs pre-attentively. Here, Hedger and colleagues<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup> implicate the contrast sensitivity function in the threat bias.</p>
<p>Hedger, Garner and Adams<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup> compared the Fourier amplitude spectra of images of fearful and neutral faces, since both the overall contrast and the spatial frequency content of images are known to modulate stimulus salience, with the human visual system being most efficient at detecting information around 3&#x2013;5 cpd. They assessed the effective contrast of images by multiplying the Fourier amplitude spectra of face stimuli by a standard measure of the contrast sensitivity function, based on the Modelfest data set<sup>[<xref ref-type="bibr" rid="c17">17</xref>]</sup>. This approach quantifies effective contrast as the product of the image amplitude, and the visual system&#x2019;s sensitivity, at each spatial frequency. They found that fearful faces, when matched for RMS contrast, were higher in effective contrast, and therefore better matched for the contrast sensitivity function compared to neutral faces. This finding was accounted for by the higher degree of contrast energy in midrange spatial frequencies, where the contrast sensitivity function peaks, in fearful faces. The effect was found to be consistent across several commonly used face databases including the Karolinska Directed Emotional Faces<sup>[<xref ref-type="bibr" rid="c18">18</xref>]</sup>, Radbound Faces<sup>[<xref ref-type="bibr" rid="c19">19</xref>]</sup>, Ekman and Friesen<sup>[<xref ref-type="bibr" rid="c20">20</xref>]</sup>, Montreal set of facial displays (MSFDE)<sup>[<xref ref-type="bibr" rid="c21">21</xref>]</sup> and NimStim databases<sup>[<xref ref-type="bibr" rid="c22">22</xref>]</sup>. Data from their image analyses support the notion that biases for fearful expressions are driven at least in part by their sensory efficacy.</p>
<p>However, there remain several elements of this approach that are not addressed by Hedger, Garner and Adams<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>. The first relates to the behavioural evidence in support of the efficacy-account. Evidence for a role of preconscious processing of threat-information is provided by the results of experiments using the continuous flash suppression (CFS) paradigm, but does not directly measure expression-related effects on contrast sensitivity. That is, while the more rapid detection of fearful faces in CFS experiments is consistent with their greater effective contrast, we might also expect differences in contrast sensitivity to different facial expressions to be evident more directly. Specifically, if it is the case that the Fourier amplitude spectrum of fearful faces is well matched to the human contrast sensitivity function, we should expect to observe an increase in contrast sensitivity, reflected by decreased contrast thresholds, for fearful faces. The second issue is that prior to the transformation of face images, Hedger and colleagues<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup> normalised face images for their luminance and RMS contrast, such that they were identical on these measures at the physical level. While this is a commonly employed technique in psychophysical studies, performed to reduce contrast-driven differences in stimulus salience, the process of attributing the aggregate physical contrast to all facial stimuli may mask naturally occurring differences in contrast between expressions in a way that could obscure results. Normalising images of natural scenes ensures a degree of consistency between images&#x2019; physical and perceived salience. However, the same may not be true when applied to face images. O&#x2019;Hare &#x0026; Hibbard<sup>[<xref ref-type="bibr" rid="c23">23</xref>]</sup> show inconsistencies between images&#x2019; physical and apparent contrast when there are differences in amplitude spectrum, when these stimuli are matched for RMS content. Given the uncertain effects of normalisation on the physical and perceived salience of facial stimuli, it is reasonable to question the degree to which normalisation influences results from both image analyses and behavioural paradigms. In particular, any consistent differences in RMS contrast across facial expressions would be expected either to increase, or cancel out, differences in sensitivity that can be attributed to differences in effective contrast.</p>
<p>To address these questions, we conducted a replication of the image analyses performed by Hedger, Garner and Adams<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>. We included face stimuli that are physically matched for RMS contrast, but also faces that were physically unmatched, such that they contain natural differences in both physical and apparent contrast. Furthermore, we conducted a traditional contrast sensitivity task in order to psychophysically test predictions from Hedger&#x2019;s image analysis. Here, we employed facial expressions as opposed to sinusoidal grating stimuli to measure expression-related differences in contrast sensitivity. An important feature of this latter study is that it directly addresses the association between face expression and contrast sensitivity at the behavioural level.</p>
</sec>
<sec id="s2">
<title>Materials and Methods</title>
<sec id="s2a">
<title>Participants</title>
<p>Eighteen (15 female, 3 male) participants took part in the study. All participants were informed of the nature of the study and provided written informed consent prior to the study beginning. The University of Essex Ethics Committee approved the employed experimental procedures. All participated in the experiment as part of a credited research module assessment, or in exchange for monetary reward. All participants had normal to corrected vision.</p>
</sec>
<sec id="s2b">
<title>Stimuli and Apparatus</title>
<p>Stimuli were grayscale images of 16 individuals, 8 male and 8 females, taken from the Karolinska Directed Emotional Faces set<sup>[<xref ref-type="bibr" rid="c18">18</xref>]</sup>. Face images included internal features only, and included 4 emotional expressions of neutral, fear, anger and happiness. Though Hedger and colleagues<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup> refer to only fearful and neutral expressions, we included an additional two expressions so as to include positively and negatively-valenced comparisons. All individual faces were presented in their normal, upright form, and in a phase scrambled format. Phase scrambled versions of the face images were used as a control measure, providing versions of faces whose configural content was disrupted but low level statistical properties preserved. An example is shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Phase scrambling was performed using MATLAB fast Fourier transform functions. Contrast thresholds were determined using an adaptive staircase technique (see under Procedure, below). Stimuli were presented using a VIEWPIXX 3D monitor (52cm X 29cm), viewed from a distance of 65 cm. The stimulus size of faces was 5.5 degrees. The screen resolution was 1920&#x00D7;1080 pixels, with a refresh rate of 120Hz and an average luminance of 50 cdm<sup>-2</sup>. Each pixel subtended 1.43 arc min. Stimuli were presented at 10-bit resolution. Participants&#x2019; responses were recorded using the RESPONSEPixx response box. Stimuli were generated and presented using MATLAB and the Psychophysics Tool box extensions<sup>[<xref ref-type="bibr" rid="c24">24</xref>&#x2013;<xref ref-type="bibr" rid="c26">26</xref>]</sup>.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1:</label><caption><p>Image of a fearful face expression extracted from the KDEF database (left), and a fast Fourier transform (FFT) of the same image (right). The same face image is therefore shown in its raw, normal format (left), and in a phase-scrambled format (right). Both images share the same amplitude spectrum (contrast-spatial frequency association), but the random phase structure assigned to the right image distributes this information differently.</p></caption>
<graphic xlink:href="432229_fig1.tif"/>
</fig>
</sec>
<sec id="s2c">
<title>Procedure</title>
<p>Participants were tested individually in a quiet room and informed prior to the experiment that the study was concerned with face perception.</p>
<p>As a 2AFC location task, participants&#x2019; objective was to indicate, using 1 of 2 buttons on a RESPONSEPixx response box whether the target face image appeared to the left or right of centre. The beginning of each trial commenced with the face stimulus on the left or right side of the screen. Participant responses determined the onset of the next trial. The proportion of times that the participant correctly indicated the location of the stimulus was recorded for all face stimuli.</p>
<p>The adaptive staircase method was used to establish the Michelson contrast required for correct detection (75&#x0025; of the time) for each expression stimulus. Here, the starting contrast level for each expressions&#x2019; staircase began at 0.01 Michelson contrast. According to the up-down rule<sup>[<xref ref-type="bibr" rid="c27">27</xref>]</sup>, Michelson contrast was increased by one initial step of 0.005 proceeding 1 incorrect observer response, thus boosting stimulus visibility. Conversely, 3 correct observer responses triggered a decrease in Michelson contrast, initially by 0.005. The overall staircase length was 70 trials, where the initial step size (0.005 Michelson) halved after 17, 35 and 52 trials. 4 experimental blocks were completed, and the 280 trials for each combination of expression and phase scrambling were combined to create a single psychometric function.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<p>The proportions of participants&#x2019; correct responses for each expression, at each contrast level, were used to create a psychometric function. A cumulative Gaussian function was fit to this data using the Palemedes toolbox<sup>[<xref ref-type="bibr" rid="c28">28</xref>]</sup> and used to determine a contrast detection threshold for each expression in its normal and manipulated (scrambled) formats. This 75&#x0025; contrast detection threshold was defined as the contrast required for the participant to correctly identify the location of the face stimulus on 75&#x0025; of trials. These results are plotted in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2:</label><caption><p>Visual contrast thresholds for neutral, fearful, angry and happy facial expressions. Error bars show standard error values. Faces are unfiltered. Fearful face expressions are not associated with lower visual contrast thresholds, contrary to what might be predicted from <xref ref-type="bibr" rid="c15">Hedger et al., (2015)</xref>. Error bars represent &#x00B1;1 standard error of the mean.</p></caption>
<graphic xlink:href="432229_fig2.tif"/>
</fig>
<p>A 4 Emotion (neutral, anger, fear, happy) x 2 Manipulation (normal, scrambled) within subjects ANOVA revealed no significant effects of expression (<italic>F</italic>(3, 51) &#x003D;. 26, <italic>p</italic>&#x003D;.85, &#x03B7;p <sup>2</sup>&#x003D;.01), or manipulation (<italic>F</italic>(1, 17)= .13, <italic>p</italic>&#x003D; .72, &#x03B7;p <sup>2</sup> =.01), and no significant expression x manipulation interaction (<italic>F</italic>(3, 51) &#x003D; 1.20, <italic>p &#x003D;</italic>. 32, &#x03B7;p <sup>2</sup> &#x003D;. 06). Analyses were repeated for contrast thresholds that were calculated using the RMS contrast of face stimuli. A 4 Emotion (neutral, anger, fear, happy) x 2 Manipulation (normal, scrambled) within subjects ANOVA revealed no significant effect of expression (<italic>F</italic>(3, 51) &#x003D;. 42, <italic>p</italic> &#x003D;. 73, &#x03B7;p<sup>2</sup> &#x003D;. 02), or manipulation (<italic>F</italic>(1, 17) &#x003D;. 07, <italic>p &#x003D;</italic>. 79 &#x03B7;p <sup>2</sup> &#x003D;. 004), and no significant expression x manipulation interaction (<italic>F</italic>(3, 51) &#x003D; 1.23, <italic>p</italic> &#x003D;. 31, &#x03B7;p <sup>2</sup> &#x003D;. 07). These findings show that visual contrast thresholds do not vary between face expressions, nor are these findings different according to the two contrast metrics used here (Michelson and RMS). The absence of an expression-related effect on contrast sensitivity provides evidence against Hedger and colleagues&#x2019; <sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup> original claim that fear expressions (compared to neutral faces) exploit the contrast sensitivity function. In an attempt to understand the inconsistency between the present behavioural data, and that generated from image analyses, we conducted the same measure of faces&#x2019; effective contrast as that performed by Hedger, Garner and Adams <sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup> and extended this to include expressions of anger, happiness and disgust, including a condition where all face images had been either normalised for RMS contrast (as was the procedure for Hedger and colleagues<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>) or non-normalised, such that face images were analysed in their raw format, containing possible natural variations in physical contrast.</p>
<sec id="s3a">
<title>Image analyses</title>
<p>Hedger and colleagues<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup> calculated the effective contrast for face images extracted from 5 face databases: Nimstim, KDEF, Radbound, Montreal and Ekman and Friesen face sets. Stimuli were cropped to include internal features only and normalised for RMS contrast prior to analyses.</p>
<p>Here, we calculate effective contrast for the 16 KDEF face images used in our experimental study, referring to the same procedure described by Hedger, Garner and Adams<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>. First, Fourier amplitude spectra were calculated for each face image. From the ModelFest dataset<sup>[<xref ref-type="bibr" rid="c17">17</xref>]</sup>, we extracted visual contrast thresholds for 10 stimulus parameters. These corresponded to Gabor stimuli, ranging from 1.12&#x2013;30 cycles/degree. A smooth curve was fit to the average threshold (over 4 repetitions and all observers in the ModelFest dataset) using a cubic spline. The resulting contrast sensitive distribution was then multiplied by the Fourier amplitude spectrum for each face image to establish each face&#x2019;s effective contrast. <xref ref-type="fig" rid="fig3">Figure 3</xref> shows an example of the procedure for calculating effective contrast for the 16 face images used in the present contrast sensitivity study. To extend our analysis, effective contrast was measured for face images across 4 of the face databases employed by Hedger, Garner and Adams<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>, with the exception of the Ekman &#x0026; Friesen face set<sup>[<xref ref-type="bibr" rid="c20">20</xref>]</sup>.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3:</label><caption><p>(a) The mean amplitude spectrum for each of the five expressions. <italic>(a)</italic> The contrast sensitivity function based on the ModelFest data. (c) The effective contrast, obtained by multiplying the original amplitude function by the contrast sensitivity function. This method for calculating effective contrast replicates that used by Hedger, Garner and Adams<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>.</p></caption>
<graphic xlink:href="432229_fig3.tif"/>
</fig>
<p>As outlined by Hedger and colleagues<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup> the overall estimate of effective contrast for each face image was obtained by summing contrast across spatial frequency after application of the contrast sensitivity model. All face images were analysed in two conditions: after they had been normalised for RMS contrast (according to Hedger and colleagues<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>), and also in their raw form, such that no contrast normalisation had taken place. In the RMS-matched analysis, the RMS contrast of each face was set to be equal to that of the image with the <italic>lowest</italic> contrast in each set. It is for this reason that the RMS-matched stimuli have an overall lower effective contrast. All face images depict forward-facing actors displaying one of 5 expressions (neutral, anger, fear, happy or disgust), cropped to include internal features only. The average effective contrast for each facial expression, compared across the 5 face image samples, including the experimental stimuli for the present contrast sensitivity study, is displayed in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4:</label><caption><p>Effective contrast for neutral faces, and anger, fear, happiness and disgust expressions, measured for raw faces (circle data) and the same faces normalised for RMS contrast (triangle data). Effective contrast measures were performed across 4 samples of face images, including the NimStim (a), KDEF (b), Radbound (c), MSFDE (d), face sets employed by <xref ref-type="bibr" rid="c15">Hedger et al., (2015)</xref>, and for the 16 KDEF faces used in the present contrast sensitivity study (e). Error bars represent &#x00B1;1 standard error of the mean.</p></caption>
<graphic xlink:href="432229_fig4.tif"/>
</fig>
<sec id="s3a1">
<title>42 NimStim face images</title>
<p>Effective contrast for neutral, angry, fearful, happy and disgust NimStim faces are shown in <xref ref-type="fig" rid="fig4">Figure 4 (a)</xref>, and summarised in <xref ref-type="table" rid="tbl1">Table 1 (a)</xref>. Paired comparisons measured the extent that effective contrast associated with fear expressions differed compared to neutral, anger, happy and disgust expression counterparts. When face images were first normalised for RMS contrast, effective contrast belonging to fearful NimStim faces was significantly higher compared to neutral expressions. This finding replicates that observed by Hedger, Garner and Adams<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>, whereby RMS normalised fear expressions were found to be significantly higher in effective contrast compared to neutral expressions. Our data extend this finding to show that RMS normalised fear expressions not only contain significantly higher effective contrast than neutral counterparts, but that their effective contrast is also higher than that for angry, happy and disgust expressions. Alternatively, when effective contrast was measured for the same raw NimStim faces that had <italic>not</italic> been normalised for RMS contrast, fear expressions were significantly higher in effective contrast compared to neutral expressions, but significantly lower in effective contrast compared to angry expressions. The effective contrast for 41 raw NimStim fearful faces did not significantly differ from that belonging to happy and disgust expressions.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label><caption><p>Effective compared between fear and counterpart expressions. Comparisons are made when faces are raw, and thus not normalised for contrast, and also when they are matched for RMS contrast. Measures are performed for 4 databases (a-d), and experimental stimuli used in the present behavioural study (e).</p></caption>
<graphic xlink:href="432229_tbl1a.tif"/>
<graphic xlink:href="432229_tbl1b.tif"/>
<graphic xlink:href="432229_tbl1c.tif"/>
<graphic xlink:href="432229_tbl1d.tif"/>
<graphic xlink:href="432229_tbl1e.tif"/>
</table-wrap>
<p>For 42 raw (not normalised) NimStim faces, RMS contrast was calculated across the 5 face expressions in order to understand how natural differences in physical contrast might influence differences between expressions effective contrast according to whether or not they have been normalised for RMS contrast. Fearful Nimstim faces naturally contained significantly less RMS contrast compared to anger and happy expressions, and did not differ compared to neutral and disgust faces. These data are illustrated in <xref ref-type="fig" rid="fig5">Figure 5</xref>, and summarised in <xref ref-type="table" rid="tbl2">Table 2 (a)</xref>.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5:</label><caption><p>RMS contrast for face expressions <italic>before</italic> faces are subjected to contrast normalisation i.e. when kept in raw format. RMS contrast for 5 expressions is measured across the 5-database face samples used to calculate faces&#x2019; effective contrast. Error bars represent &#x00B1;1 standard error of the mean.</p></caption>
<graphic xlink:href="432229_fig5.tif"/>
</fig>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label><caption><p>Differences between RMS contrast in raw fear expressions and 4 emotion counterparts (neutral, anger,happy, disgust). Fear comparisons are measured across all 4 databases (a-d), and also for the experimental stimuli used in the present contrast sensitivity study (e).</p></caption>
<graphic xlink:href="432229_tbl2a.tif"/>
<graphic xlink:href="432229_tbl2b.tif"/>
<graphic xlink:href="432229_tbl2c.tif"/>
</table-wrap>
</sec>
<sec id="s3a2">
<title>140 KDEF face images</title>
<p>Effective contrast for neutral, angry, fearful, happy and disgust KDEF faces are shown in <xref ref-type="fig" rid="fig4">Figure 4 (b)</xref>, and summarised in <xref ref-type="table" rid="tbl1">Table 1 (b)</xref>. Paired comparisons measured differences in effective contrast between fearful faces and their expression counterparts. For KDEF face images normalised for RMS contrast, fear expressions were significantly higher in effective contrast compared to both angry and happy expressions. Effective contrast did not differ between fear expressions and disgust or neutral faces when normalised for RMS contrast. Alternatively, when effective contrast was measured for the same face images, but in their raw form, fearful KDEF faces were significantly <italic>lower</italic> in effective contrast compared to neutral, angry and disgust expressions, and did not differ compared to happy expressions.</p>
<p>For 140 raw KDEF faces, RMS contrast was calculated across the 5 expressions. Fearful KDEF faces naturally contained significantly less RMS contrast compared to neutral, angry and disgust expressions, and did not differ compared to happy expressions. These data are illustrated in <xref ref-type="fig" rid="fig5">Figure 5</xref>, and summarised in <xref ref-type="table" rid="tbl2">Table 2 (b)</xref>.</p>
</sec>
<sec id="s3a3">
<title>57 Radbound face images</title>
<p>Effective contrast for neutral, angry, fearful, happy and disgust Radbound faces are shown in <xref ref-type="fig" rid="fig4">Figure 4 (c)</xref>, and summarised in <xref ref-type="table" rid="tbl1">Table 1 (c)</xref>. Paired comparisons measured differences in effective contrast between fearful faces and their expression counterparts. For face images normalised for RMS contrast, fear expressions were significantly higher in effective contrast compared to neutral faces, but also compared to all other expressions. These effects were also true when effective contrast was calculated for the same faces in raw form, thus not normalised for RMS contrast. These findings in particular require further discussion, presented in the following section.</p>
<p>For 57 raw Radbound faces, RMS contrast was calculated across the 5 expressions. Fearful Radbound faces naturally contained significantly less RMS contrast compared to all other face expressions. These data are illustrated in <xref ref-type="fig" rid="fig5">Figure 5</xref>, and summarised in <xref ref-type="table" rid="tbl2">Table 2 (c)</xref>.</p>
</sec>
<sec id="s3a4">
<title>7 Montreal (MSFDE) face images</title>
<p>Effective contrast for neutral, angry, fearful, happy and disgust Montreal faces are shown in <xref ref-type="fig" rid="fig4">Figure 4 (d)</xref>, and summarised in <xref ref-type="table" rid="tbl1">Table 1 (d)</xref>. Paired comparisons measured differences in effective contrast between fearful faces and their expression counterparts. When face images were normalised for RMS contrast, effective contrast did not differ for fear faces compared to any other face expressions. Alternatively, when effective contrast was measured for the same faces, but in their raw form, fearful Montreal faces were significantly higher in effective contrast compared to neutral, angry and disgust expressions, and did not differ significantly from happy expressions.</p>
<p>For 7 raw Montreal faces, RMS contrast was calculated across the 5 expressions. RMS contrast for fearful Montreal faces did not differ significantly compared to any other face expression. These data are illustrated in <xref ref-type="fig" rid="fig5">Figure 5</xref>, and summarised in <xref ref-type="table" rid="tbl2">Table 2 (d)</xref>.</p>
</sec>
<sec id="s3a5">
<title>16 KDEF face images (experimental stimuli)</title>
<p>Effective contrasts for the experimental face stimuli used in our contrast sensitivity study are shown in <xref ref-type="fig" rid="fig4">Figure 4 (e)</xref>, and summarised in <xref ref-type="table" rid="tbl1">Table 1 (e)</xref>. Here, effective contrast is measured for experimental face images that were <italic>not</italic> normalised for RMS contrast, and also for versions of the same faces that had been normalised for RMS contrast. Paired comparisons measured differences in effective contrast between fearful faces and their expression counterparts. When faces were normalised for RMS contrast, effective contrast was significantly higher in fear expressions compared to happy expressions, and did not significantly differ compared to neutral, anger or disgust faces. Alternatively, for the same faces that were not normalised for RMS contrast, such that they were analysed in the same format as they were presented to observers, fear expressions were significantly lower in effective contrast compared to both neutral and disgust expressions, and did not differ significantly from angry or happy expressions.</p>
<p>For the 16 raw KDEF faces used in the present contrast sensitivity study, RMS contrast was calculated across the 5 expressions. Fearful faces naturally contained significantly less RMS contrast compared to neutral and disgust faces, more RMS contrast compared to happy faces, and did not differ significantly compared to angry expressions. These data are illustrated in <xref ref-type="fig" rid="fig5">Figure 5</xref>, and summarised in <xref ref-type="table" rid="tbl2">Table 2 (e)</xref>.</p>
<p>Together, data from the present contrast sensitivity study showed that visual contrast thresholds are not influenced by differences between images of facial expressions. Namely, fearful expressions portrayed by face images did not enhance observers&#x2019; contrast sensitivity; as was predicted by findings from Hedger, Garner and Adams<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>. Fearful expressions, according to image analyses by Hedger, Garner and Adams<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup> are higher in effective contrast, and thus well tuned to contrast sensitivity processing. This proposal was driven by data from image analyses measuring differences in effective contrast between fear and neutral face images that had been normalised for RMS contrast. The stimuli used in the present study were raw face images that were not normalised for physical contrast in any way. We replicate measures of effective contrast used by Hedger, Garner and Adams <sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup> to establish the extent that CSF advantages exclusive to fear expressions might be driven by the effect of contrast normalisation on the effective contrast of faces. A general trend across the present image analyses is that greater effective contrast in fear expressions appears to rely on face images having been first normalised for RMS contrast. This was the case for both the experimental stimuli used in the present CSF study, and for the face sets used by Hedger and colleagues<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>.</p>
</sec>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>A widely accepted view in the threat bias literature is that fearful face expressions possess a special status in the human visual system, due to their low level image content<sup>[<xref ref-type="bibr" rid="c1">1</xref>, <xref ref-type="bibr" rid="c3">3</xref>&#x2013;<xref ref-type="bibr" rid="c4">4</xref>, <xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c29">29</xref>]</sup>. Hedger, Garner and Adams<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup> recently showed that visibility, or salience, associated with fear expressions is predicted by their effective contrast content; the extent that the Fourier amplitude of fear expressions, compared to neutral faces, exploits the contrast sensitivity function. In the present study, we conducted a traditional contrast sensitivity task to test whether higher effective contrast purported for fear expressions is associated with lower visual contrast thresholds at the behavioural level. Here, we measured contrast sensitivity for facial stimuli of 5 raw face expressions. No expression-related differences were observed across visual thresholds, as was predicted based on data from Hedger, Garner and Adams<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>.</p>
<p>Specifically, a decrease in visual thresholds for fearful expressions was not observed. Greater effective contrast unique to fear expressions (when compared to neutral faces) was found only for face images that had been normalised for RMS contrast. In order to investigate whether the use of contrast normalisation in Hedger, Garner and Adams&#x2019; <sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup> may have driven effective contrast effects that in its absence were not replicated by our contrast sensitivity study, we repeated calculations of effective contrast using the same procedure employed by Hedger, Garner and Adams<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>. Here, effective contrast was calculated for images of face expressions both when they were normalised for RMS contrast, as was performed by Hedger, Garner and Adams<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>, but also when the same faces had not been normalised for physical contrast. These analyses were performed for the NimStim, KDEF, Montreal (MSFDE) and Radbound face sets used by Hedger and colleagues<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>, and also for the 16 KDEF face images used as the experimental stimuli in the present contrast sensitivity study. Importantly, our findings replicate those of Hedger, Garner and Adams<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>, showing that fear expressions normalised for RMS contrast are significantly higher in effective contrast than neutral counterparts. We extend this finding to show that this is also true when fearful faces are compared to other face expressions. However, when the same faces were analysed in their raw form (i.e. when they were not normalised for physical contrast), this effect of fear diminishes for Nimstim, KDEF, and Montreal face images. These findings indicate that the process of normalising face stimuli significantly increases the effective contrast in fearful face expressions, where naturally these faces tend not to differ in effective contrast compared to other facial expressions, or indeed possess a lower effective contrast. An important finding to discuss here is the absence of this contrast normalisation effect for face images taken from the Radbound face database. Here, we observed that Radbound fear expressions normalised for contrast were significantly higher in effective contrast compared to neutral faces, as well as other expressions; an effect that is consistent with that observed by Hedger, Garner and Adams<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>. However, this effect did not diminish when images were not normalised for contrast, as was observed for all other face samples. Radbound face images were included in the present study on the basis that they were included in the original study by Garner and Adams<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>. Details of the image processing used to create and standardise these actor photographs includes white-balance correction<sup>[<xref ref-type="bibr" rid="c19">19</xref>]</sup>. This process adjusts raw image data in order to remove certain unrealistic and biased appearances, such as those incurred under different lightning conditions<sup>[<xref ref-type="bibr" rid="c30">30</xref>&#x2013;<xref ref-type="bibr" rid="c31">31</xref>]</sup>. It is important to note that database production information for KDEF and NimStim face sets do not refer to any image processing related to white-balance correction, or contrast normalisation<sup>[<xref ref-type="bibr" rid="c32">32</xref>, <xref ref-type="bibr" rid="c22">22</xref>]</sup>. No information about image processing is provided for the Montreal image database<sup>[<xref ref-type="bibr" rid="c21">21</xref>]</sup>. It may therefore be that contrast and luminance information in &#x2018;raw&#x2019; Radbound face images had already been subjected to some degree of normalisation when they were created. In sum, the present study performed a traditional contrast sensitivity task to address the proposal that fearful faces exploit the contrast sensitivity function, and as a result undergoes efficient visual processing<sup>[<xref ref-type="bibr" rid="c15">15</xref>]</sup>. Together, these findings suggest that contrast normalisation &#x2013;a standard procedure in psychophysical studies- significantly influences the physical composition of face stimuli in a way that can be expected to influence their perceived salience under both experimental and neurophysiological conditions.</p>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<p><bold>Data can be made publicly available via Dryad.</bold></p>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Holmes</surname> <given-names>A</given-names></string-name>, <string-name><surname>Green</surname> <given-names>S</given-names></string-name>, <string-name><surname>Vuilleumier</surname> <given-names>P.</given-names></string-name> <article-title>The involvement of distinct visual channels in rapid attention towards fearful facial expressions</article-title>. <source>Cognition &#x0026; Emotion.</source> <year>2005</year> Sep 1;<volume>19</volume>(<issue>6</issue>):<fpage>899</fpage>&#x2013;<lpage>922</lpage>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Yang</surname> <given-names>E</given-names></string-name>, <string-name><surname>Zald</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Blake</surname> <given-names>R.</given-names></string-name> <article-title>Fearful expressions gain preferential access to awareness during continuous flash suppression</article-title>. <source>Emotion.</source> <year>2007</year> Nov;<volume>7</volume>(<issue>4</issue>):<fpage>882</fpage>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Bayle</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Schoendorff</surname> <given-names>B</given-names></string-name>, <string-name><surname>H&#x00E9;naff</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Krolak-Salmon</surname> <given-names>P.</given-names></string-name> <article-title>Emotional facial expression detection in the peripheral visual field</article-title>. <source>PloS one.</source> <year>2011</year> Jun 24;<volume>6</volume>(<issue>6</issue>):<fpage>e21584</fpage>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Carlson</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Reinke</surname> <given-names>KS.</given-names></string-name> <article-title>Masked fearful faces modulate the orienting of covert spatial attention</article-title>. <source>Emotion.</source> <year>2008</year> Aug;<volume>8</volume>(<issue>4</issue>):<fpage>522</fpage>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Williams</surname> <given-names>LM</given-names></string-name>, <string-name><surname>Liddell</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Rathjen</surname> <given-names>J</given-names></string-name>, <string-name><surname>Brown</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Gray</surname> <given-names>J</given-names></string-name>, <string-name><surname>Phillips</surname> <given-names>M</given-names></string-name>, <string-name><surname>Young</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gordon</surname> <given-names>E.</given-names></string-name> <article-title>Mapping the time course of nonconscious and conscious perception of fear: an integration of central and peripheral measures</article-title>. <source>Human brain mapping.</source> <year>2004</year> Feb;<volume>21</volume>(<issue>2</issue>):<fpage>64</fpage>&#x2013;<lpage>74</lpage>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Whalen</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Rauch</surname> <given-names>SL</given-names></string-name>, <string-name><surname>Etcoff</surname> <given-names>NL</given-names></string-name>, <string-name><surname>McInerney</surname> <given-names>SC</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>MB</given-names></string-name>, <string-name><surname>Jenike</surname> <given-names>MA.</given-names></string-name> <article-title>Masked presentations of emotional facial expressions modulate amygdala activity without explicit knowledge</article-title>. <source>Journal of Neuroscience.</source> <year>1998</year> Jan 1;<volume>18</volume>(<issue>1</issue>):<fpage>411</fpage>&#x2013;<lpage>8</lpage>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Gray</surname> <given-names>KL</given-names></string-name>, <string-name><surname>Adams</surname> <given-names>WJ</given-names></string-name>, <string-name><surname>Hedger</surname> <given-names>N</given-names></string-name>, <string-name><surname>Newton</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Garner</surname> <given-names>M.</given-names></string-name> <article-title>Faces and awareness: Low-level, not emotional factors determine perceptual dominance</article-title>. <source>Emotion.</source> <year>2013</year> Jun;<volume>13</volume>(<issue>3</issue>):<fpage>537</fpage>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Stein</surname> <given-names>T</given-names></string-name>, <string-name><surname>Seymour</surname> <given-names>K</given-names></string-name>, <string-name><surname>Hebart</surname> <given-names>MN</given-names></string-name>, <string-name><surname>Sterzer</surname> <given-names>P.</given-names></string-name> <article-title>Rapid fear detection relies on high spatial frequencies</article-title>. <source>Psychological science</source>. <year>2014</year> Feb;<volume>25</volume>(<issue>2</issue>):<fpage>566</fpage>&#x2013;<lpage>74</lpage>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Troiani</surname> <given-names>V</given-names></string-name>, <string-name><surname>Schultz</surname> <given-names>RT.</given-names></string-name> <article-title>Amygdala, pulvinar, and inferior parietal cortex contribute to early processing of faces without awareness</article-title>. <source>Frontiers in human neuroscience.</source> <year>2013</year> Jun 6;<volume>7</volume>:<fpage>241</fpage>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>LeDoux</surname> <given-names>JE</given-names></string-name>, <string-name><surname>Phelps</surname> <given-names>EA.</given-names></string-name> <article-title>Emotional networks in the brain</article-title>. <source>Handbook of emotions.</source> <year>1993</year>;<volume>109</volume>:<fpage>118</fpage>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Bannerman</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Hibbard</surname> <given-names>PB</given-names></string-name>, <string-name><surname>Chalmers</surname> <given-names>K</given-names></string-name>, <string-name><surname>Sahraie</surname> <given-names>A.</given-names></string-name> <article-title>Saccadic latency is modulated by emotional content of spatially filtered face stimuli</article-title>. <source>Emotion.</source> <year>2012</year> Dec;<volume>12</volume>(<issue>6</issue>):<fpage>1384</fpage>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Vuilleumier</surname> <given-names>P</given-names></string-name>, <string-name><surname>Armony</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Driver</surname> <given-names>J</given-names></string-name>, <string-name><surname>Dolan</surname> <given-names>RJ.</given-names></string-name> <article-title>Distinct spatial frequency sensitivities for processing faces and emotional expressions</article-title>. <source>Nature neuroscience.</source> <year>2003</year> Jun;<volume>6</volume>(<issue>6</issue>):<fpage>624</fpage>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>&#x00D6;hman</surname> <given-names>A</given-names></string-name>, <string-name><surname>Mineka</surname> <given-names>S.</given-names></string-name> <article-title>Fears, phobias, and preparedness: toward an evolved module of fear and fear learning</article-title>. <source>Psychological review.</source> <year>2001</year> Jul;<volume>108</volume>(<issue>3</issue>):<fpage>483</fpage>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Vlamings</surname> <given-names>PH</given-names></string-name>, <string-name><surname>Goffaux</surname> <given-names>V</given-names></string-name>, <string-name><surname>Kemner</surname> <given-names>C.</given-names></string-name> <article-title>Is the early modulation of brain activity by fearful facial expressions primarily mediated by coarse low spatial frequency information?</article-title>. <source>Journal of vision.</source> <year>2009</year> May 1;<volume>9</volume>(<issue>5</issue>):<fpage>12</fpage>-.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Hedger</surname> <given-names>N</given-names></string-name>, <string-name><surname>Adams</surname> <given-names>WJ</given-names></string-name>, <string-name><surname>Garner</surname> <given-names>M.</given-names></string-name> <article-title>Fearful faces have a sensory advantage in the competition for awareness</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance.</source> <year>2015</year> Dec;<volume>41</volume>(<issue>6</issue>):<fpage>1748</fpage>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Horstmann</surname> <given-names>G</given-names></string-name>, <string-name><surname>Bauland</surname> <given-names>A.</given-names></string-name> <article-title>Search asymmetries with real faces: Testing the anger-superiority effect</article-title>. <source>Emotion.</source> <year>2006</year> May;<volume>6</volume>(<issue>2</issue>):<fpage>193</fpage>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Watson</surname> <given-names>AB</given-names></string-name>, <string-name><surname>Ahumada</surname> <given-names>AJ.</given-names></string-name> <article-title>A standard model for foveal detection of spatial contrast</article-title>. <source>Journal of vision.</source> <year>2005</year> Oct 1;<volume>5</volume>(<issue>9</issue>):<fpage>6-</fpage>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Lundqvist</surname> <given-names>D</given-names></string-name>, <string-name><surname>Flykt</surname> <given-names>A</given-names></string-name>, <string-name><surname>&#x00D6;hman</surname> <given-names>A.</given-names></string-name> <article-title>The Karolinska directed emotional faces (KDEF)</article-title>. <source>CD ROM from Department of Clinical Neuroscience, Psychology section, Karolinska Institutet</source>. <year>1998</year>;<volume>91</volume>:<fpage>630</fpage>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Langner</surname> <given-names>O</given-names></string-name>, <string-name><surname>Dotsch</surname> <given-names>R</given-names></string-name>, <string-name><surname>Bijlstra</surname> <given-names>G</given-names></string-name>, <string-name><surname>Wigboldus</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Hawk</surname> <given-names>ST</given-names></string-name>, <string-name><surname>Van Knippenberg</surname> <given-names>AD.</given-names></string-name> <article-title>Presentation and validation of the Radboud Faces Database</article-title>. <source>Cognition and emotion.</source> <year>2010</year> Dec 1;<volume>24</volume>(<issue>8</issue>):<fpage>1377</fpage>&#x2013;<lpage>88</lpage>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Ekman</surname> <given-names>P</given-names></string-name>, <string-name><surname>Friesen</surname> <given-names>WV.</given-names></string-name> <article-title>Pictures of facial affect consulting psychologists press</article-title>. <source>Palo Alto, CA.</source> <year>1976</year>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="website"><string-name><surname>Hess</surname>, <given-names>U.</given-names></string-name> <date-in-citation content-type="accessed-date">Accessed June 2018</date-in-citation>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.psychophysiolab.com/accueil.php">http://www.psychophysiolab.com/accueil.php</ext-link></mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Tottenham</surname> <given-names>N</given-names></string-name>, <string-name><surname>Tanaka</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Leon</surname> <given-names>AC</given-names></string-name>, <string-name><surname>McCarry</surname> <given-names>T</given-names></string-name>, <string-name><surname>Nurse</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hare</surname> <given-names>TA</given-names></string-name>, <string-name><surname>Marcus</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Westerlund</surname> <given-names>A</given-names></string-name>, <string-name><surname>Casey</surname> <given-names>BJ</given-names></string-name>, <string-name><surname>Nelson</surname> <given-names>C.</given-names></string-name> <article-title>The NimStim set of facial expressions: judgments from untrained research participants</article-title>. <source>Psychiatry research.</source> <year>2009</year> Aug 15;<volume>168</volume>(<issue>3</issue>):<fpage>242</fpage>&#x2013;<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>O&#x2019;Hare</surname> <given-names>L</given-names></string-name>, <string-name><surname>Hibbard</surname> <given-names>PB.</given-names></string-name> <article-title>Spatial frequency and visual discomfort</article-title>. <source>Vision research.</source> <year>2011</year> Aug 1;<volume>51</volume>(<issue>15</issue>):<fpage>1767</fpage>&#x2013;<lpage>77</lpage>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Brainard</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Vision</surname> <given-names>S.</given-names></string-name> <article-title>The psychophysics toolbox</article-title>. <source>Spatial vision.</source> <year>1997</year> Jan 1;<volume>10</volume>:<fpage>433</fpage>&#x2013;<lpage>6</lpage>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Pelli</surname> <given-names>DG.</given-names></string-name> <article-title>The VideoToolbox software for visual psychophysics: Transforming numbers into movies</article-title>. <source>Spatial vision.</source> <year>1997</year> Jan 1;<volume>10</volume>(<issue>4</issue>):<fpage>437</fpage>&#x2013;<lpage>42</lpage>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Kleiner</surname> <given-names>M</given-names></string-name>, <string-name><surname>Brainard</surname> <given-names>D</given-names></string-name>, <string-name><surname>Pelli</surname> <given-names>D</given-names></string-name>, <string-name><surname>Ingling</surname> <given-names>A</given-names></string-name>, <string-name><surname>Murray</surname> <given-names>R</given-names></string-name>, <string-name><surname>Broussard</surname> <given-names>C.</given-names></string-name> <article-title>What&#x2019;s new in Psychtoolbox-3</article-title>. <source>Perception.</source> <year>2007</year> Aug 27;<volume>36</volume>(<issue>14</issue>):<fpage>1</fpage>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Garc&#x00ED;a-P&#x00E9;rez</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Alcal&#x00E1;-Quintana</surname> <given-names>R</given-names></string-name>, <string-name><surname>Woods</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Peli</surname> <given-names>E.</given-names></string-name> <article-title>Psychometric functions for detection and discrimination with and without flankers Attention</article-title>, <source>Perception, &#x0026; Psychophysics</source>. <year>2011</year> Apr 1;<volume>73</volume>(<issue>3</issue>):<fpage>829</fpage>&#x2013;<lpage>53</lpage>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Prins</surname>, <given-names>N</given-names></string-name> &#x0026; <string-name><surname>Kingdom</surname>, <given-names>F. A. A.</given-names></string-name> (<year>2018</year>) <article-title>Applying the Model-Comparison Approach to Test Specific Research Hypotheses in Psychophysical Research Using the Palamedes Toolbox</article-title>. <source>Frontiers in Psychology</source>, <volume>9</volume>:<fpage>1250</fpage>. <pub-id pub-id-type="doi">doi: 10.3389/fpsyg.2018.01250</pub-id></mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Phelps</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Ling</surname> <given-names>S</given-names></string-name>, <string-name><surname>Carrasco</surname> <given-names>M.</given-names></string-name> <article-title>Emotion facilitates perception and potentiates the perceptual benefits of attention</article-title>. <source>Psychological science.</source> <year>2006</year> Apr;<volume>17</volume>(<issue>4</issue>):<fpage>292</fpage>&#x2013;<lpage>9</lpage>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Yanof</surname> <given-names>A</given-names></string-name>, <string-name><surname>Skow</surname> <given-names>M</given-names></string-name>, <string-name><surname>Tran</surname> <given-names>H</given-names></string-name>, <string-name><surname>Adby</surname> <given-names>A</given-names></string-name>, <article-title>inventors; NXP USA Inc, assignee</article-title>. <source>Digital image processing using white balance and gamma correction</source>. United States patent US 7,236,190. <year>2007</year> Jun 26.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Innocent</surname> <given-names>M</given-names></string-name>, <article-title>inventor; On Semiconductor Trading Ltd, assignee</article-title>. <source>White balance correction using illuminant estimation</source>. United States patent US 8,049,789. <year>2011</year> Nov 1.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="other"><string-name><surname>Lundqvist</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Flykt</surname>, <given-names>A.</given-names></string-name>, &#x0026; <string-name><surname>&#x00D6;hman</surname>, <given-names>A.</given-names></string-name> (<year>1998</year>). <date-in-citation content-type="accessed-date">Accessed June 2018</date-in-citation>. Available from: <ext-link ext-link-type="uri" xlink:href="http://kdef.se/home/aboutKDEF.html">http://kdef.se/home/aboutKDEF.html</ext-link></mixed-citation></ref>
</ref-list>
</back>
</article>