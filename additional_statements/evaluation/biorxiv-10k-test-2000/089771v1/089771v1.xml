<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/089771</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Ecology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Ecological Interactions and the Netflix Problem</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6248-3007</contrib-id>
<name>
<surname>Desjardins-Proulx</surname>
<given-names>Philippe</given-names>
</name>
<xref ref-type="author-notes" rid="n1">0</xref>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4330-1041</contrib-id>
<name>
<surname>Laigle</surname>
<given-names>Idaline</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0735-5184</contrib-id>
<name>
<surname>Poisot</surname>
<given-names>Timoth&#x00E9;e</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4498-7076</contrib-id>
<name>
<surname>Gravel</surname>
<given-names>Dominique</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Universit&#x00E9; de Sherbrooke</institution>, Canada.</aff>
<aff id="a2"><label>2</label><institution>Universit&#x00E9; de Montr&#x00E9;al</institution>, Canada.</aff>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="other"><label>0</label><p>email: <email>philippe.desjardins.proulx@usherbrooke.ca</email></p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2016</year>
</pub-date>
<elocation-id>089771</elocation-id>
<history>
<date date-type="received">
<day>25</day>
<month>11</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>26</day>
<month>11</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2016, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2016</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="089771.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<label>0</label>
<title>Abstract</title>
<p>Species interactions are key components of ecosystems but we generally have an incomplete picture of who-eats-who in a given community. Different techniques have been devised to predict species interactions using theoretical models or abundances. Here, we explore the <italic>K</italic> nearest neighbour approach, with a special emphasis on recommendation, along with other machine learning techniques. Recommenders are algorithms developed for companies like Netflix to predict if a customer would like a product given the preferences of similar customers. These machine learning techniques are well-suited to ecological interactions, since they focus on positive-only data. We also explore how the <italic>K</italic> nearest neighbour approach can be used with both positive and negative information, in which case the goal of the algorithm is to fill missing entries from a matrix (imputation). By removing a prey from a predator, we find that recommenders can guess the missing prey around 50&#x0025; of the times on the first try, with up to 881 possibilities. Traits do not improve significantly the results for the <italic>K</italic> nearest neighbour, although a simple test with a supervised learning approach (random forests) show we can predict interactions with high accuracy using only three traits per species. These techniques are complementary, addressing different scenarios. Recommenders can predict interactions in the absence of traits, using only information about other species' interactions. Supervised learning algorithms such as random forests base their predictions on traits only, they do not exploit other species' interactions. They are useful when traits are known for the species. This result shows that binary interactions can be predicted without regard to the ecological community given only three variables: body mass and two variables for the species' phylogeny. Further work should focus on developing custom similarity measures specialized to ecology to improve the <italic>K</italic>NN algorithms and using richer data to capture indirect relationships between species.</p>
</abstract>
<counts>
<page-count count="15"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>Species form complex networks of interactions and understanding these interactions is a major goal of ecology [<xref ref-type="bibr" rid="c25">25</xref>]. The problem of predicting whether two species will interact has been approached from various perspectives [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c21">21</xref>]. Williams and Martinez [<xref ref-type="bibr" rid="c31">31</xref>] for instance built a simple theoretical model capable of generating binary food webs sharing important features with real food webs [<xref ref-type="bibr" rid="c14">14</xref>], while others have worked to predict interactions from species abundance data [<xref ref-type="bibr" rid="c1">1</xref>]. Being able to predict with high enough accuracy whether two species will interact given simply two sets of attributes, or the preferences of similar species, would be of value to conservation and invasion biology, allowing us to build food webs with partial information about interactions and help us understand cascading effects caused by perturbations. However, the problem is made difficult by the small number of interactions relative to non-interactions and relationships that involve more than two species [<xref ref-type="bibr" rid="c13">13</xref>].</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1:</label>
<caption><p>Summary of the three methods used. The first two use the <italic>K</italic> nearest neighbour algorithm, with the Tanimoto distance measure in the positive-only case (recommendation), and the Euclidean distance with positive and negative values (matrix imputation). The Tanimoto <italic>KNN</italic> makes a recommendation, while the Euclidean <italic>KNN</italic> and random forests predict either an interaction or a non-interaction. The context for the Euclidean <italic>KNN</italic> and random forests are different: the former fills missing entries from a binary interaction matrix while the latter makes a prediction based only on the traits of the predator and its prey.</p></caption>
<graphic xlink:href="089771_tbl1.tif"/>
</table-wrap>
<p>In 2006, Netix offered a prize to anyone who would improve their recommender system by more than 10&#x0025;. It took three years before a team could claim the prize, and the efforts greatly help advanced machine learning methods for recommenders [<xref ref-type="bibr" rid="c23">23</xref>]. Recommender systems try to predict the rating a user would give to an item, recommending them items they would like based on what similar users like [<xref ref-type="bibr" rid="c2">2</xref>]. Ecological interactions can also be described this way: we want to know how much a species would &#x201C;like&#x201D; a prey. Interactions are treated as binary variables, two species interact or they do not, but the same methods could be applied to interaction matrices with preferences. There are two different ways to see the problem of species interactions. In the positive-only case, a species has a set of preys, and we want to predict what other preys they might be interested in. This approach has the benefit of relying only on our most reliable information: positive (preferably observed) interactions. The other approach is to see binary interactions as a matrix filled with interactions (1s) and non-interactions (0s). Here, we want to predict the value of a specific missing entry (is species <italic>x</italic><sub><italic>i</italic></sub> consuming species <italic>x</italic><sub><italic>j</italic></sub>?).</p>
<p>Statistical machine learning algorithms [<xref ref-type="bibr" rid="c23">23</xref>] have proven to be reliable to build effective predictive models for complex data (the &#x201C;unreasonable effectiveness of data&#x201D; [<xref ref-type="bibr" rid="c16">16</xref>]). We will use a simple technique called the <italic>K</italic> nearest neighbour (<italic>KNN</italic>) algorithm both for recommendation (finding good preys to a species with positive-only information) and matrix imputation (filling a specific entry in a matrix with positive and negative interactions). The technique is simple: for a given species, we find the <italic>K</italic> most similar species according to some distance measure, and use these <italic>K</italic> species to base a prediction. For this study, we use a data-set from Digel et al. [<xref ref-type="bibr" rid="c10">10</xref>], which contains 909 species, of which 881 are involved in predator-prey relationships and 871 have at least one prey. The data comes from soil food webs and includes invertebrates, plants, bacteria, and fungi. In total, the data-set has 34 193 interactions. The data was complemented with information on 25 binary attributes (traits) for each species, plus their body mass and information on their phylogeny. We also briefly discuss a supervised learning method, random forests, which is used to predict interactions with only the species&#x2019; traits.</p>
<p>A summary of the three methods used can be found in <xref ref-type="table" rid="tbl1">table 1</xref>. The approaches are not directly comparable. For example, the positive-only <italic>KNN</italic> recommends preys to a species. If we remove a prey from a species, ask the algorithm to recommend a prey, and check whether the prey will come up as the recommendation, there are up to 881 possibilities. On the other hand, the <italic>KNN</italic> algorithm with positive and negative values (matrix imputation) has to decide whether an entry is an interaction or a non-interaction, a 50&#x0025; chance of success by random. These approaches have different uses. Positive-only algorithms are interesting because we are rarely certain that two species do not interact. Also, the <italic>KNN</italic> approach uses information on what similar species do, while random forests only rely on traits.</p>
<p>We show the <italic>KNN</italic> is particularly effective at retrieving missing interactions in the positive-only case, succeeding 50&#x0025; of the times at recommending the right species among 881 possibilities. However, the <italic>KNN</italic> algorithm is significantly less accurate than random forests to predict an interaction with positive and negative data. With few traits, the random forests can achieve high accuracy (&#x2248; 98&#x0025; for both interactions and non-interactions) without any information about other species in the community. Random forests require only three traits to be effective: body mass and two traits based on the species&#x2019; phylogeny.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Method</title>
<sec id="s2a">
<label>2.1</label>
<title>Data</title>
<p>The first data-set was obtained from the study of Digel et al. [<xref ref-type="bibr" rid="c10">10</xref>], who documented the presence and absence of interactions among 882 species from 48 forest soil food webs, details of which are provided in the original publication. 34 193 unique interactions were observed across the 48 food webs, and a total of 215 418 absence of interactions. For matrix imputation, we assume all entries in the 881 by 881 matrix which are not observed interactions are non-interactions. In order to improve representation of interactions involving low trophic levels species that were not identified at the species level in the first data-set, we compiled a second data-set from a review of the literature. We selected all articles involving interactions of terrestrial invertebrate species for a total of 126 studies, across these, a total of 1 439 interactions were recorded between 648 species. Only 88 absences of interactions were found. We selected traits based on to their potential role in consumption interactions (<xref ref-type="table" rid="tbl2">table 2</xref>). For each species or taxa, these traits were documented based on a literature review or from visual assessment of pictures. In addition to these traits, we included two proxies for hard-to-measure traits: feeding guild and taxonomy.</p>
</sec>
</sec>
<sec id="s2b">
<label>2.2</label>
<title><italic>K</italic>-nearest neighbour</title>
<p>Both our recommendation and matrix imputation approaches use the <italic>K</italic>-nearest neighbour (<italic>KNN</italic>) algorithm [<xref ref-type="bibr" rid="c23">23</xref>]. The <italic>KNN</italic> algorithm is an instance-based method, it does not build a general internal model of the data, but instead tries to fill missing entries by a majority vote based on the <italic>K</italic> nearest (i.e. most similar) entries given some distance metrics. In the case of recommendation, there is no concept of &#x201C;missing entry&#x201D;, each species is described by a set of traits and a set of preys, and the algorithm will recommend new preys to the species based on the preys of its <italic>K</italic> nearest neighbours. For example, if <italic>K</italic> = 3, we take the set of preys of the three most similar species to decide which prey to recommend. If species <italic>A</italic> is found twice and <italic>B</italic> once in the set of preys of the most similar species, we will recommend <italic>A</italic> first (assuming, of course, that the species does not already have this prey). See <xref ref-type="table" rid="tbl3">table 3</xref> for a complete example of recommendation. In the &#x201C;Netix&#x201D; problem, this is equivalent to recommend new TV series/movies to a user by searching for the users with the most similar taste and using what they liked as recommendation. It is also possible to tackle the reverse problem: Amazon uses item-based recommendations, in which case we are looking for similar items instead of similar users to base our recommendations [<xref ref-type="bibr" rid="c2">2</xref>].</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2:</label>
<caption><p>The traits used. All traits are binary except for body mass, <italic>Ph</italic><sub>0</sub>, and <italic>Ph</italic><sub>1</sub>. We use taxonomy as a proxy of latent traits following [<xref ref-type="bibr" rid="c22">22</xref>]. To do so, we used the R package <italic>ape</italic> to obtain taxonomic distances between the species, perform classical multidimensional scaling (or principal coordinates analysis) [<xref ref-type="bibr" rid="c8">8</xref>] on taxonomic distances, and use the scores of each species on the first two axes (<italic>Ph</italic><sub>0</sub> and <italic>Ph</italic><sub>1</sub>) as taxonomy-based traits. These three real-valued variables are scaled to be in the [0, 1) range. For the Tanimoto similarity index, these three continuous variables have to be converted to binary features. For each, we create four binary features (<italic>n</italic> = 881/4).</p></caption>
<graphic xlink:href="089771_tbl2.tif"/>
</table-wrap>
<p>For matrix imputation, if we want to know if a species preys on <italic>A</italic>, we look at how many of the <italic>K</italic> most similar species prey on <italic>A</italic> (a ratio that can be interpreted as a probability). In this case, the problem is seen as a matrix with missing entries, so the question is not to recommend new preys to a predator, but whether a specific relationship exists (see <xref ref-type="fig" rid="fig1">figure 1</xref> for a complete example). It is often suggested to avoid picking <italic>K</italic> that are multiple of the number of classes to avoid ties [<xref ref-type="bibr" rid="c28">28</xref>]. Here there are two classes: interaction and non-interactions, so we will only use odd <italic>K</italic>s.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><p><bold>A:</bold> The initial matrix with missing entries, green squares are used for interactions, red for non-interactions, and white for missing values. Rows represent entries and columns features. In our case, we have a square matrix where rows are species and the columns represent their preys. We want to find the value denoted by X. B: The K-nearest neighbours algorithms look at the entries that are the most similar to the entry with the missing value, pick the 3 closest (<italic>K</italic> = 3), and use the values at the columns with the missing entry. <bold>C</bold>: In this case, for the column with the missing entry, the 3 nearest neighbours have 2 non-interactions and 1 interaction, so the algorithm fills the entry with a non-interaction.</p></caption>
<graphic xlink:href="089771_fig1.tif"/>
</fig>
<p>Different distance measures can be used. Here, we will use the Tanimoto coefficient for recommendations and the Euclidean distance for matrix imputation. Choosing the right value for <italic>K</italic> is tricky. Low values give high importance to the most similar entries, while high values provide a larger set of examples. Fortunately, the most computationally intensive task is to compute the distances between all pairs, a step that is independent of <italic>K</italic>. As a consequence, once the distances are computed, we can quickly run the algorithm with different values of <italic>K</italic>.</p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Recommendation</title>
<p>The Tanimoto (or Jaccard) similarity measure is defined as the size of the intersection of two sets divided by their union, or:
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="089771_eqn1.gif"/></alternatives>
</disp-formula>
</p>
<p>Since it is a similarity measure in the [0, 1] range, we can transform it into a distance function with 1 &#x2212; <italic>tanimoto</italic>(<bold>x</bold>, <bold>y</bold>). The distance function will use two types of information: the set of traits of the species (see <xref ref-type="table" rid="tbl2">table 2</xref>) and their set of preys. We define the distance function with traits as:
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="089771_eqn2.gif"/></alternatives>
</disp-formula>
where <italic>w</italic><sub><italic>t</italic></sub> is the weight given to traits, x<sub>t</sub> and y<sub>t</sub> are the sets of traits for species <italic>x</italic> and <italic>y</italic>, and x<sub>i</sub>, y<sub>i</sub> are their sets of preys. Thus, when <italic>w</italic><sub><italic>t</italic></sub> = 0, only interactions are used to compute the distance, and when <italic>w</italic><sub><italic>t</italic></sub> = 1, only traits are used. See <xref ref-type="table" rid="tbl3">table 3</xref> for an example.</p>
<p>The data is the set of preys and binary traits for each species (<xref ref-type="table" rid="tbl2">Table 2</xref>). To test the approach, we randomly remove an interaction for each species and ask the algorithm to recommend up to 10 preys for the species with the missing interaction. We count how many recommendations are required to retrieve the missing interactions and compute the top1, top5, and top10 success rates, which are defined as the probabilities to retrieve the missing interaction with 1, 5, or 10 recommendations. We repeat this process 10 times for each species with at least 2 preys (7200 attempts). We test all odd values of <italic>K</italic> from 1 to 19, and <italic>w</italic><sub><italic>t</italic></sub> = {0, 0.2, 0.4, 0.6, 0.8, 1}. We also divided species in groups according to the number of preys they have to see if it is easier to find the missing interaction for species with fewer preys.</p>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>Matrix imputation</title>
<p>The <italic>KNN</italic> algorithm with Euclidean distance works with both positive and negative entries. In this case, an interaction is represented with a value of 1, while a non-interaction is a represented with 0, in a <italic>n</italic> &#x00D7; <italic>n</italic> matrix (<italic>n</italic> = 881). The goal is to predict the value of a missing entry (<xref ref-type="fig" rid="fig1">Figure 1</xref>). The Euclidean distance is defined as
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="089771_eqn3.gif"/></alternatives>
</disp-formula>
</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3:</label>
<caption><p>
Fictional example to illustrate recommendations with <italic>K</italic> nearest neighbour using the Tanimoto distance measure modified to include species traits. We are trying to recommend a prey to species 0 given that the three most similar species are species 6, 28, and 70. For example, the distance to species 70 would be <italic>w</italic><sub><italic>t</italic></sub>0.5 &#x002B; (1 &#x2212; <italic>w</italic><sub><italic>t</italic></sub>)1/3. To find recommendations, the set of preys found in the <italic>K</italic> = 3 most similar entries is computed, in this case {812 = 2, 70 = 2, 72 = 1}, leading to the list of recommendations [812, 70, 72]. Because they are found most often in the <italic>K</italic> most similar species, candidates 812 and 70 will be suggested before 72. To test this approach, we remove a prey from a species and check whether the algorithm recommend the missing prey. Especially with low <italic>K</italic>, it&#x2019;s possible that no recommendations can be found, for example if the most similar species has the exact same preys.</p></caption>
<graphic xlink:href="089771_tbl3.tif"/>
</table-wrap>
<p>However, we want to give different weights to different aspects of the species, so we compute the distance between two species as:
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="089771_eqn4.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn5">
<alternatives><graphic xlink:href="089771_eqn5.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn6">
<alternatives><graphic xlink:href="089771_eqn6.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn7">
<alternatives><graphic xlink:href="089771_eqn7.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn8">
<alternatives><graphic xlink:href="089771_eqn8.gif"/></alternatives>
</disp-formula>
</p>
<p>Where <italic>w</italic><sub><italic>m</italic></sub>, <italic>w</italic><sup><italic>p</italic></sup>, <italic>w</italic><sub><italic>t</italic></sub>, <italic>w</italic><sub><italic>i</italic></sub> are the weight given to body mass, the two coordinates of a classical multidimensional scaling, binary traits, and interactions, respectively. For simplicity we require that <italic>w</italic><sub><italic>m</italic></sub> &#x002B; <italic>w</italic><sub><italic>p</italic></sub> &#x002B; <italic>w</italic><sub><italic>t</italic></sub> &#x002B; <italic>w</italic><sub><italic>i</italic></sub> = 1.</p>
<p>The data is an 881 &#x00D7; 881 interaction matrix. To test the <italic>KNN</italic> algorithm with the Euclidean distance, we randomly remove a single interaction from the matrix, ask the algorithm to fill the entry, and count how many times the correct value is retrieved. For each set of parameters tested, we repeat this process 50 000 times, and count the number of true positives (tp), true negatives (tn), false positives (fp) and false negatives (fn). The score for predicting interactions (<italic>Score</italic><sub><italic>y</italic></sub>), non-interactions (<italic>Score</italic><sub><italic>&#x00AC;y</italic></sub>) and the accuracy are defined as
<disp-formula id="eqn9">
<alternatives><graphic xlink:href="089771_eqn9.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn10">
<alternatives><graphic xlink:href="089771_eqn10.gif"/></alternatives>
</disp-formula>
<disp-formula id="eqn11">
<alternatives><graphic xlink:href="089771_eqn11.gif"/></alternatives>
</disp-formula>
with 34193 and 741968 being the number of observed interactions and non-interactions in the 881 by 881 matrix. We also use the True Skill Statistics (TSS), defined as
<disp-formula id="eqn12">
<alternatives><graphic xlink:href="089771_eqn12.gif"/></alternatives>
</disp-formula>
</p>
<p>The <italic>TSS</italic> ranges from -1 to 1.</p>
</sec>
<sec id="s2e">
<label>2.5</label>
<title>Supervised learning</title>
<p>We also do a simple test with random forests to see if it is possible to predict interactions in this data-set using only the traits [<xref ref-type="bibr" rid="c6">6</xref>]. In this case, the random forests perform supervised learning: we are trying to predict <italic>y</italic> (interaction) from the vector of traits x by first learning a model on the training set, and testing the learned model on a testing set. We keep 5&#x0025; of the data for testing. We perform grid search to find the optimal parameters for the random forests.</p>
</sec>
<sec id="s2f">
<label>2.6</label>
<title>Code and Data</title>
<p>Since several machine learning algorithms depends on computing distances (or similarities) for all pairs, many data structures have been designed to compute them efficiently from kd-trees discovered more than thirty years ago [<xref ref-type="bibr" rid="c11">11</xref>] to ball trees, metric skip lists, navigating nets [<xref ref-type="bibr" rid="c19">19</xref>], and cover trees [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c19">19</xref>]. We use an exact but naive approach that works well with small data-sets. Since <italic>distance</italic>(<italic>x</italic>, <italic>y</italic>) = 0 if <italic>x</italic> = <italic>y</italic> and <italic>distance</italic>(<italic>x</italic>, <italic>y</italic>) = <italic>distance</italic>(<italic>y</italic>, <italic>x</italic>), our C&#x002B;&#x002B; implementation stores the distances in a lower triangular matrix without the diagonal, yielding <italic>n</italic>(<italic>n</italic> &#x2212; 1)/2 distances to compute. We used Scikit for random forests [<xref ref-type="bibr" rid="c24">24</xref>]. The C&#x002B;&#x002B;11 code for the <italic>KNN</italic> algorithm, Python scripts for random forests, and all data-sets used are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/PhDP/Articles">https://github.com/PhDP/Articles</ext-link> [<xref ref-type="bibr" rid="c9">9</xref>].</p>
</sec>
<sec id="s3">
<label>3</label>
<title>Results</title>
<sec id="s3a">
<label>3.1</label>
<title>Recommendation</title>
<p>While matrix imputation has a 50&#x0025; change of success by random, the Tanimoto <italic>KNN</italic> needs to pick the right prey among up to 881 possibilities. Yet, it succeeds on its first recommendation around 50&#x0025; of the times. When the first recommendation fails, the next 9 recommendations only retrieve the right species around 15&#x0025; of the times so the top5 and top10 success rates are fairly close to the top1 success rate (see <xref ref-type="fig" rid="fig2">figure 2</xref>). The Tanimoto measure is particularly effective for species with fewer preys, achieving more than 80&#x0025; success rate for species with 10 or fewer preys (<xref ref-type="fig" rid="fig3">Figure 3</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><p>After removing a prey from a predator, we ask the KNN algorithms with Tanimoto measure to make 10 recommendations (from best to worst). The figure shows how many recommendations are required to retrieve the missing interaction. Most retrieved interactions are found with the first attempt. This data was generated with <italic>K</italic> = 7 and <italic>w</italic><sub><italic>t</italic></sub> = 0.</p></caption>
<graphic xlink:href="089771_fig2.tif"/>
</fig>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><p>Success on first guess with Tanimoto similarity as a function of the number of prey. The KNN algorithm with Tanimoto similarity is more effective at predicting missing preys when the number of preys is small. This is probably in good part because there are more information available to the algorithm, since 473 species have 10 or fewer preys, 295 have between 10 and 100, 103 species have more than 100 preys.</p></caption>
<graphic xlink:href="089771_fig3.tif"/>
</fig>
<p>The highest first-try success rates (the probability to pick the missing interaction on the first recommendation) are found with <italic>K</italic> = 7 and no weights to traits, and with <italic>K</italic> = 17 and a small weight of 0.2 to traits (<xref ref-type="table" rid="tbl4">Table 4</xref>). Overall, the value of <italic>K</italic> had little effect on predictive ability.</p>
</sec>
<sec id="s3b">
<label>3.2</label>
<title>Matrix imputation</title>
<p>We show our results with <italic>K</italic> Nearest neighbours algorithm in <xref ref-type="table" rid="tbl5">table 5</xref>. The best result is achieved with <italic>K</italic> = 1 but has a TSS of only 0.66 (<xref ref-type="table" rid="tbl5">Table 5</xref>). More than 99&#x0025; of non-interactions are predicted correctly, but only 2=3 of the interactions are predicted correctly (<xref ref-type="table" rid="tbl5">Table 5</xref>).</p>
<table-wrap id="tbl4" orientation="portrait" position="float">
<label>Table 4:</label>
<caption><p>Top1 success rates for the <italic>KNN</italic>/Tanimoto algorithm with various <italic>K</italic> and weights to traits. When <italic>w</italic><sub><italic>t</italic></sub> = 0.0, the algorithm will only use interactions to compute similarity between species. When <italic>w</italic><sub><italic>t</italic></sub> = 1, the algorithm will only consider the species&#x2019; traits (see <xref ref-type="table" rid="tbl2">table 2</xref>). The value is the probability to retrieve the correct missing interaction with the first recommendation. For each entry, <italic>n</italic> = 871 (the number of species minus 10, the number of species with no preys). The best result is achieved with <italic>K</italic> = 17 and <italic>w</italic> = 0.2, although the results for most values of <italic>K</italic> and <italic>w</italic> = [0.0, 0.2] are all fairly close. The success rate increases with <italic>K</italic> when only traits are considered (<italic>w</italic> = 1).</p></caption>
<graphic xlink:href="089771_tbl4.tif"/>
</table-wrap>
<table-wrap id="tbl5" orientation="portrait" position="float">
<label>Table 5:</label>
<caption><p>Matrix imputation with Euclidean distance. This tables uses the weights <italic>w</italic><sub><italic>m</italic></sub> = 1/3, <italic>w</italic><sub><italic>p</italic></sub> = 0, <italic>w</italic><sub><italic>t</italic></sub> = 1/3, <italic>w</italic><sub><italic>i</italic></sub> = 1/3.</p></caption>
<graphic xlink:href="089771_tbl5.tif"/>
</table-wrap>
<p>As for the weights <italic>w</italic><sub><italic>m</italic></sub> (body mass), <italic>w</italic><sub><italic>p</italic></sub> (coordinates from a classical multidimensional scaling of the phylogeny), <italic>w</italic><sub><italic>t</italic></sub> (binary traits), <italic>w</italic><sub><italic>i</italic></sub> (interactions), the optimal values for <italic>w</italic><sub><italic>m</italic></sub> and <italic>w</italic><sub><italic>i</italic></sub> lie between 1/3 and 1/4 and vary a bit with different values of <italic>K</italic>. <italic>w</italic><sub><italic>t</italic></sub> has a small but consistent effect: changing the weight from 0 to 1/3 improves the result by roughly 1&#x0025;, but higher values will start to decrease predictive ability. Positive values of <italic>w</italic><sub><italic>p</italic></sub> always have a negative effect on predictive abilities. With minor variations with <italic>K</italic>, the optimal weights are thus <italic>w</italic><sub><italic>m</italic></sub> &#x2248; 1/3, <italic>w</italic><sub><italic>p</italic></sub> = 0, <italic>w</italic><sub><italic>t</italic></sub> = 1/3, <italic>w</italic><sub><italic>i</italic></sub> &#x2248; 1/3. The optimal weight to <italic>w</italic><sub><italic>i</italic></sub> increases with <italic>K</italic>.</p>
</sec>
<sec id="s3c">
<label>3.3</label>
<title>Supervised learning</title>
<p>Random forests predict correctly 99.55&#x0025; of the non-interactions and 96.81&#x0025; of the interactions, for a TSS of 0.96. Much of this accuracy is due to the three real-valued traits (body mass, <italic>Ph</italic><sub>0</sub>, <italic>Ph</italic><sub>1</sub>). Without them, too many entries have the same feature vector x, making it impossible for the algorithm to classify them correctly. Removing the binary traits has little effect on the model. With only body mass, <italic>Ph</italic><sub>0</sub>, <italic>Ph</italic><sub>1</sub>, the TSS of the random forests is 0.94.</p>
</sec>
</sec>
<sec id="s4">
<label>4</label>
<title>Discussion</title>
<p>We applied different machine learning techniques to the problem of predicting binary species interactions. Recommendation is arguably a better fit for binary species interactions, since it is essentially the same problem commercial recommenders such as Netflix face: given that a user like item <italic>i</italic>, what is the best way to select other items the user would like. In this case, users are species, and the items are their preys, but the problem is the same. In both cases, we can have solid positive evidence (observed or implied interactions), but rarely have proofs of non-interactions. The approach yields strong results, with a top1 success rate above 50&#x0025; in a food web with up to 881 possibilities. The approach could be used, for example, to reconstruct entire food webs using global database of interactions [<xref ref-type="bibr" rid="c26">26</xref>]. The method&#x2019;s effectiveness rely on nestedness: how much species cluster around the same set of preys in a food web [<xref ref-type="bibr" rid="c15">15</xref>]. Thus, it should be less effective in food webs with more unique predators.</p>
<p>The <italic>KNN</italic> algorithm falls into the realm of unsupervised learning, where the goal is to find patterns in data [<xref ref-type="bibr" rid="c23">23</xref>]. The other class of machine learning algorithms, supervised learning, have the clearer goal of predicting a value <italic>y</italic> from a vector of features <bold>x</bold>. For example, in supervised learning, we would try to predict an interaction <italic>y</italic> from the vector of traits <bold>x</bold>, while our unsupervised approach allow us to fill entries from an incomplete matrix regardless of what the entry is (interaction or trait). For matrix imputation, the <italic>KNN</italic> yields less than impressive results, but our random forests test on the same data-set achieves a TSS of 0.96 using the binary traits, body mass, and the coordinates of the classical multidimensional scaling. A random forest can build effective predictive models by creating complex rules based on the traits, while the <italic>KNN</italic> algorithm relies on a simplistic distance metrics. However, the <italic>KNN</italic> approach has some advantages over supervised learning, namely the capacity to fill any entries from an interaction matrix and use the information from other species&#x2019; interactions. The solution is likely to <italic>learn</italic> distance metrics [<xref ref-type="bibr" rid="c4">4</xref>] instead of using a fixed formula. This would allow complex rules while maintaining the <italic>KNN</italic>&#x2019;s ability to fill arbitrary matrices.</p>
<p>Learning distance metrics is a promising avenue to improve our results. Much efforts on the Netflix prize focused on improving similarity measures [<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c18">18</xref>], and custom similarity metrics can be used to improve unsupervised classification algorithms [<xref ref-type="bibr" rid="c4">4</xref>]. Learning distance metrics from data is a common way to improve methods based on a nearest neighbour search [<xref ref-type="bibr" rid="c33">33</xref>, <xref ref-type="bibr" rid="c4">4</xref>], allowing the measure itself to be optimized. We only used the <italic>K</italic> nearest neighbour algorithm for unsupervised learning, but several other algorithms can be used to solve the &#x201C;Netflix problem&#x201D;. For example: techniques based on linear programming, such as recent exact methods for matrix completion based on convex optimization [<xref ref-type="bibr" rid="c7">7</xref>] or low-rank matrix factorization. The latter method reduces a matrix to a multiplication between two smaller matrices, which can be used both to predict missing entries and to compress large matrices into small, more manageable matrices [<xref ref-type="bibr" rid="c30">30</xref>]. Given enough data, deep learning methods such as deep Boltzmann machines could also be used [<xref ref-type="bibr" rid="c34">34</xref>]. Deep learning revolutionized machine learning with neural networks made of layers capable of learning increasingly detailed representations of complex data [<xref ref-type="bibr" rid="c17">17</xref>]. Many of the most spectacular successes of machine learning use deep learning [<xref ref-type="bibr" rid="c20">20</xref>]. However, learning several neural layers to form a deep networks would require larger data-sets.</p>
<p>The low sensibility to <italic>K</italic> in recommendations compared to imputation is interesting. This is cause by the fact that, as <italic>K</italic> grows, the set of species includes more and more unrelated species with widely different set of preys. For imputation, adding more species with different preys means it is likely to misclassify an interactions as a non-interaction. However, if we increase <italic>K</italic> from <italic>k</italic> to <italic>k</italic> &#x002B; &#x03B4; for a recommendation, the species in <italic>&#x03B4;</italic> range are not only less similar, but they are less likely to share preys among themselves. Since recommendations are based on how many times a prey is found in the <italic>K</italic> nearest species, the species in the &#x03B4; range are unlikely to have as much weight as the first <italic>k</italic> species.</p>
<p>Our results have two limitations. It is possible that our food web was exceptionally simple, and that a different food web would behave differently, especially if it has lower nestedness. The success of the <italic>KNN</italic> algorithms depends on local structure: how much can we learn from similar species. If each species has a unique set of preys, the <italic>KNN</italic> will struggle more. Also, a deeper issue is that real food webs are not binary structures. Species, populations, and individuals have different densities, prey more strongly on some resources than others, and have preferences. In a binary matrix, we can predict if two species will interaction while completely ignoring the rest of the network, but real food webs involve complex indirect relationships [<xref ref-type="bibr" rid="c32">32</xref>]. It is unclear how much we can learn about ecosystems and species interactions from binary matrices, and our results show that binary interactions are mostly independent of the community, since we are able to effectively predict if two species interactions given only three traits. Species interactions are better represented with a weighted hypergraph [<xref ref-type="bibr" rid="c12">12</xref>], which are well-suited to model relations with an arbitrary number of participants, where the hyperedge would allow for complex indirect relationships to be included. Understanding these hypergraphs is outside the scope of the <italic>KNN</italic> algorithm but could be understood with modern techniques such as Markov logic [<xref ref-type="bibr" rid="c27">27</xref>].</p>
<p>Recommendation (<italic>KNN</italic> algorithm with Tanimoto distance) and supervised learning (random forests) are complementary techniques. Supervised learning is more useful when we have traits and no information about interactions, but it is useless without the traits. On the other hand, the recommender performs well without traits but requires at least partial information about interactions, although it might be possible to use the interactions from different food webs. Matrix imputation might provide the best of the both worlds, allowing us to use both traits and species interactions, but the distance metrics we used performed poorly and we suggest more research could be done on developing better distance metrics for ecological interactions, or learning these metrics from data.</p>
</sec>
</body>
<back>
<ack>
<label>5</label>
<title>Acknowledgements</title>
<p>PDP has been funded by an Alexander Graham Bell Graduate Scholarship from the National Sciences and Engineering Research Council of Canada, an Azure for Research award from Microsoft, and benefited from the Hardware Donation Program from NVIDIA. We tank the team of Digel for documenting the food webs. DG is funded by the Canada Research Chair program and NSERC Discovery grant. TP is funded by an NSERC Discovery grant and an FQRNT Nouveau Chercheur grants.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><given-names>A</given-names> <surname>Aderhold</surname></string-name>, <string-name><given-names>D</given-names> <surname>Husmeier</surname></string-name>, <string-name><given-names>JL</given-names> <surname>Lennon</surname></string-name>, <string-name><given-names>CM</given-names> <surname>Beale</surname></string-name>, and <string-name><given-names>VA</given-names> <surname>Smith</surname></string-name>. <article-title>Hierarchical bayesian models in ecology: Reconstructing species interaction networks from non-homogeneous species abundance data</article-title>. <source>Ecological Informatics</source>, <volume>11</volume>:<fpage>55</fpage>&#x2013;<lpage>64</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="book"><string-name><given-names>CC</given-names> <surname>Aggarwal</surname></string-name>. <source>Recommender Systems</source>. <publisher-name>Springer</publisher-name>, <year>2016</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><given-names>I</given-names> <surname>Bartomeus</surname></string-name>, <string-name><given-names>D</given-names> <surname>Gravel</surname></string-name>, <string-name><given-names>J</given-names> <surname>Tylianakis</surname></string-name>, <string-name><given-names>M</given-names> <surname>Aizen</surname></string-name>, <string-name><given-names>I</given-names> <surname>Dickie</surname></string-name>, and <string-name><given-names>M</given-names> <surname>Bernard-Verdier</surname></string-name>. <article-title>A common framework for identifying linkage rules across different types of interactions</article-title>. <source>Functional Ecology</source>, <year>2016</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><string-name><given-names>A</given-names> <surname>Bellet</surname></string-name>, <string-name><given-names>A</given-names> <surname>Habrard</surname></string-name>, and <string-name><given-names>M</given-names> <surname>Sebban</surname></string-name>. <article-title>Metric Learning</article-title>. <source>Morgan &#x0026; Claypool</source>, <year>2015</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="confproc"><string-name><given-names>A</given-names> <surname>Beygelzimer</surname></string-name>, <string-name><given-names>S</given-names> <surname>Kakade</surname></string-name>, and <string-name><given-names>J</given-names> <surname>Langford</surname></string-name>. <chapter-title>Cover trees for nearest neighbor</chapter-title>. In <conf-name>Proceedings of the 23nd International Conference on Machine Learning</conf-name>, <year>2006</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><string-name><given-names>L</given-names> <surname>Breiman</surname></string-name>. <article-title>Random forests</article-title>. <source>Machine Learning</source>, <volume>45</volume>(<issue>1</issue>):<fpage>5</fpage>&#x2013;<lpage>32</lpage>, <year>2001</year>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><string-name><given-names>EJ</given-names> <surname>Cand&#x00E8;es</surname></string-name> and <string-name><given-names>B</given-names> <surname>Recht</surname></string-name>. <article-title>Exact matrix completion via convex optimization</article-title>. <source>Foundations of Computational mathematics</source>, <volume>9</volume>(<issue>6</issue>):<fpage>717</fpage>&#x2013;<lpage>772</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="book"><string-name><given-names>TF</given-names> <surname>Cox</surname></string-name> and <string-name><given-names>MAA</given-names> <surname>Cox</surname></string-name>. <source>Multidimensional Scaling</source>. <publisher-name>Chapman and Hall</publisher-name>, <year>2001</year>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="website"><string-name><given-names>P</given-names> <surname>Desjardins-Proulx</surname></string-name>. <ext-link ext-link-type="uri" xlink:href="http://github.com/phdp/articles">github.com/phdp/articles</ext-link>. <ext-link ext-link-type="uri" xlink:href="http://doi.org/10.5281/zenodo">http://doi.org/10.5281/zenodo</ext-link>. <fpage>161602</fpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><string-name><given-names>C</given-names> <surname>Digel</surname></string-name>, <string-name><given-names>A</given-names> <surname>Curtsdotter</surname></string-name>, <string-name><given-names>J</given-names> <surname>Riede</surname></string-name>, <string-name><given-names>B</given-names> <surname>Klarner</surname></string-name>, and <string-name><given-names>U</given-names> <surname>Brose</surname></string-name>. <article-title>Unravelling the complex structure of forest soil food webs: higher omnivory and more trophic levels</article-title>. <source>In Oikos</source>, volume <volume>123</volume>, pages <fpage>1157</fpage>&#x2013;<lpage>1172</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><string-name><given-names>JH</given-names> <surname>Friedman</surname></string-name>, <string-name><given-names>JL</given-names> <surname>Bentley</surname></string-name>, and <string-name><given-names>RA</given-names> <surname>Finkel</surname></string-name>. <article-title>An algorithm for finding best matches in logarithmic expected time</article-title>. <source>Transactions on Mathematical Software</source>, <volume>3</volume>(<issue>3</issue>):<fpage>209</fpage>&#x2013;<lpage>226</lpage>, <year>1977</year>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><string-name><given-names>J</given-names> <surname>Gao</surname></string-name>, <string-name><given-names>Q</given-names> <surname>Zhao</surname></string-name>, <string-name><given-names>W</given-names> <surname>Ren</surname></string-name>, <string-name><given-names>A</given-names> <surname>Swami</surname></string-name>, <string-name><given-names>R</given-names> <surname>Ramanathan</surname></string-name>, and <string-name><given-names>A</given-names> <surname>Bar-Noy</surname></string-name>. <article-title>Dynamic shortest path algorithms for hypergraphs</article-title>. <source>Modeling and Optimization in Mobile, Ad Hoc and Wireless Networks</source>, pages <fpage>238</fpage>&#x2013;<lpage>245</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><string-name><given-names>AJ</given-names> <surname>Golubski</surname></string-name>, <string-name><given-names>EE</given-names> <surname>Westlund</surname></string-name>, <string-name><given-names>J</given-names> <surname>Vandermeer</surname></string-name>, and <string-name><given-names>M</given-names> <surname>Pascual</surname></string-name>. <article-title>Ecological networks over the edge: Hypergraph trait-mediated indirect interaction (tmii) structure</article-title>. <source>Trends in Ecology and Evolution</source>, <volume>31</volume>(<issue>5</issue>):<fpage>1083</fpage>&#x2013;<lpage>1090</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><given-names>D</given-names> <surname>Gravel</surname></string-name>, <string-name><given-names>T</given-names> <surname>Poisot</surname></string-name>, <string-name><given-names>C</given-names> <surname>Albouy</surname></string-name>, <string-name><given-names>L</given-names> <surname>Velez</surname></string-name>, and <string-name><given-names>D</given-names> <surname>Mouillot</surname></string-name>. <article-title>Inferring food web structure from predator-prey body size relationships</article-title>. <source>Methods in Ecology and Evolution</source>, <volume>4</volume>(<issue>11</issue>):<fpage>1083</fpage>&#x2013;<lpage>1090</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><string-name><given-names>PR</given-names> <surname>Guimaraes</surname></string-name> and <string-name><given-names>P</given-names> <surname>Guimaraes</surname></string-name>. <article-title>Improving the analyses of nestedness for large sets of matrices</article-title>. <source>Environmental Modelling and Software</source>, <volume>21</volume>(<issue>10</issue>):<fpage>1512</fpage>&#x2013;<lpage>1513</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><string-name><given-names>A</given-names> <surname>Halevy</surname></string-name>, <string-name><given-names>P</given-names> <surname>Norvig</surname></string-name>, and <string-name><given-names>F</given-names> <surname>Pereira</surname></string-name>. <article-title>The unreasonable effectiveness of data</article-title>. <source>IEEE Intelligent Systems</source>, <volume>24</volume>:<fpage>8</fpage>&#x2013;<lpage>12</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><string-name><given-names>GE</given-names> <surname>Hinton</surname></string-name>, <string-name><given-names>S</given-names> <surname>Osindero</surname></string-name>, and <string-name><given-names>YW</given-names> <surname>Teh</surname></string-name>. <article-title>A fast learning algorithm for deep belief nets</article-title>. <source>Neural computation</source>, <volume>18</volume>(<issue>7</issue>):<fpage>1527</fpage>&#x2013;<lpage>1554</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><string-name><given-names>T</given-names> <surname>Hong</surname></string-name> and <string-name><given-names>D</given-names> <surname>Tsamis</surname></string-name>. <source>Use of KNN for the Netflix Prize</source>. <year>2006</year>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="confproc"><string-name><given-names>M</given-names> <surname>Izbicki</surname></string-name> and <string-name><given-names>CR</given-names> <surname>Shelton</surname></string-name>. <chapter-title>Faster cover trees</chapter-title>. In <conf-name>Proceedings of the 32nd International Conference on Machine Learning</conf-name>, <year>2015</year>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><string-name><given-names>V</given-names> <surname>Mnih</surname></string-name>, <string-name><given-names>K</given-names> <surname>Kavukcuoglu</surname></string-name>, <string-name><given-names>D</given-names> <surname>Silver</surname></string-name>, <string-name><given-names>A</given-names> <surname>Graves</surname></string-name>, <string-name><given-names>I</given-names> <surname>Antonoglou</surname></string-name>, <string-name><given-names>D</given-names> <surname>Wierstra</surname></string-name>, and <string-name><given-names>M</given-names> <surname>Riedmiller</surname></string-name>. <article-title>Playing atari with deep reinforcement learning</article-title>. <source>arXiv</source>, <year>2013</year>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><string-name><given-names>I</given-names> <surname>Morales-Castilla</surname></string-name>, <string-name><given-names>MG</given-names> <surname>Matias</surname></string-name>, <string-name><given-names>D</given-names> <surname>Gravel</surname></string-name>, and <string-name><given-names>MB.</given-names> <surname>Ara&#x00FA;joemail</surname></string-name>. <article-title>Inferring biotic interactions from proxies</article-title>. <source>Ecological Informatics</source>, <volume>30</volume>(<issue>6</issue>):<fpage>347</fpage>&#x2013;<lpage>356</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><string-name><given-names>N</given-names> <surname>Mouquet</surname></string-name>, <string-name><given-names>V</given-names> <surname>Devictor</surname></string-name>, <string-name><given-names>CN</given-names> <surname>Meynard</surname></string-name>, <string-name><given-names>F</given-names> <surname>Munoz</surname></string-name>, <string-name><given-names>LF</given-names> <surname>Bersier</surname></string-name>, <string-name><given-names>J</given-names> <surname>Chave</surname></string-name>, <string-name><given-names>P</given-names> <surname>Couteron</surname></string-name>, <string-name><given-names>A</given-names> <surname>Dalecky</surname></string-name>, <string-name><given-names>C</given-names> <surname>Fontaine</surname></string-name>, and <string-name><given-names>D</given-names> <surname>Gravel</surname></string-name>. <article-title>Ecophylogenetics: advances and perspectives</article-title>. <source>Biological reviews</source>, <volume>87</volume>(<issue>4</issue>):<fpage>769</fpage>&#x2013;<lpage>785</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="book"><string-name><given-names>KP</given-names> <surname>Murphy</surname></string-name>. <source>Machine Learning: A Probabilistic Perspective</source>. <publisher-name>The MIT Press</publisher-name>, <year>2012</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><string-name><given-names>F.</given-names> <surname>Pedregosa</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Varoquaux</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Gramfort</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Michel</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Thirion</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Grisel</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Blondel</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Prettenhofer</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Weiss</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Dubourg</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Vanderplas</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Passos</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Cournapeau</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Brucher</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Perrot</surname></string-name>, and <string-name><given-names>E.</given-names> <surname>Duchesnay</surname></string-name>. <article-title>Scikit-learn: Machine learning in Python</article-title>. <source>Journal of Machine Learning Research</source>, <volume>12</volume>:<fpage>2825</fpage>&#x2013;<lpage>2830</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="book"><string-name><given-names>SL</given-names> <surname>Pimm</surname></string-name>. <source>Food Webs</source>. <publisher-name>Springer</publisher-name>, <year>1982</year>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><string-name><given-names>JH</given-names> <surname>Poelen</surname></string-name>, <string-name><given-names>JD</given-names> <surname>Simons</surname></string-name>, and <string-name><given-names>CJ</given-names> <surname>Mungall</surname></string-name>. <article-title>Global biotic interactions: An open infrastructure to share and analyze species-interaction datasets</article-title>. <source>Ecological Informatics</source>, <volume>24</volume>:<fpage>148</fpage>&#x2013;<lpage>159</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><string-name><given-names>M</given-names> <surname>Richardson</surname></string-name> and <string-name><given-names>P</given-names> <surname>Domingos</surname></string-name>. <article-title>Markov logic networks</article-title>. <source>Machine Learning</source>, <volume>62</volume>(<issue>1-2</issue>):<fpage>107</fpage>&#x2013;<lpage>136</lpage>, <year>2006</year>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="book"><string-name><given-names>S</given-names> <surname>Theodoridis</surname></string-name>. <source>Machine Learning: A Bayesian and Optimization Perspective</source>. <publisher-name>Academic Press</publisher-name>, <year>2015</year>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><string-name><given-names>A</given-names> <surname>Toscher</surname></string-name> and <string-name><given-names>M</given-names> <surname>Jahrer</surname></string-name>. <source>The BigChaos solution to the Netflix prize</source>. <year>2008</year>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><string-name><given-names>RJ</given-names> <surname>Vanderbei</surname></string-name>. <source>Linear programming: Foundations and extensions</source>. <year>2013</year>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><string-name><given-names>RJ</given-names> W<surname>illiams</surname></string-name> and <string-name><given-names>ND</given-names> <surname>Martinez</surname></string-name>. <article-title>Simple rules yield complex food webs</article-title>. <source>Nature</source>, <volume>404</volume>:<fpage>180</fpage>&#x2013;<lpage>183</lpage>, <year>2000</year>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><string-name><given-names>JT</given-names> <surname>Wootton</surname></string-name>. <article-title>The nature and consequences of indirect effects in ecological communities</article-title>. <source>Annual Review of Ecology and Systematics</source>, pages <fpage>443</fpage>&#x2013;<lpage>466</lpage>, <year>1994</year>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><string-name><given-names>EP</given-names> <surname>Xing</surname></string-name>, <string-name><given-names>AY</given-names> <surname>Ng</surname></string-name>, <string-name><given-names>MI</given-names> <surname>Jordan</surname></string-name>, and <string-name><given-names>S</given-names> <surname>Russell</surname></string-name>. <article-title>Distance metric learning with application to clustering with side-information</article-title>. <source>Advances in neural information processing systems</source>, <volume>15</volume>:<fpage>505</fpage>&#x2013;<lpage>512</lpage>, <year>2003</year>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><string-name><given-names>J</given-names> <surname>Zhang</surname></string-name>. <article-title>Deep transfer learning via restricted boltzmann machine for document classification</article-title>. In <source>ICMLA: Machine Learning and Applications</source>, volume <volume>1</volume>, pages <fpage>323</fpage>&#x2013;<lpage>326</lpage>, <year>2011</year>.</mixed-citation></ref>
</ref-list>
</back>
</article>