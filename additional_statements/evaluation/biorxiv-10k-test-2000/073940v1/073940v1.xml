<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/073940</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Preserved position information in high-level visual cortex with large receptive fields</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Majima</surname>
<given-names>Kei</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Sukhanov</surname>
<given-names>Paul</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Horikawa</surname>
<given-names>Tomoyasu</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9300-8268</contrib-id>
<name>
<surname>Kamitani</surname>
<given-names>Yukiyasu</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Graduate School of Informatics, Kyoto University, Sakyo-ku</institution>, Kyoto 606-8501, <country>Japan</country></aff>
<aff id="a2"><label>2</label><institution>ATR Computational Neuroscience Laboratories</institution>, Kyoto 619-0288, <country>Japan</country></aff>
<aff id="a3"><label>3</label><institution>Nara Institute of Science and Technology, Ikoma</institution>, Nara 630-0192, <country>Japan</country></aff>
</contrib-group>
<author-notes>
<corresp id="cor1"><institution>Corresponding author: Yukiyasu Kamitani, Ph.D. Graduate School of Informatics, Kyoto University, Yoshida-honmachi, Sakyo-ku</institution>, Kyoto 606-8501, <country>Japan</country> Phone: &#x002B;81-75-753-9133, Fax: &#x002B;81-75-753-3145 E-mail: <email>kamitani@i.kyoto-u.ac.jp</email></corresp>
<fn><p>The authors declare no competing financial interests.</p></fn>
<fn><p><bold>Author Contributions:</bold> YK and PS designed the study. PS and TH performed experiments. KM and PS performed analysis. KM, TH, and YK wrote the manuscript.</p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2016</year>
</pub-date>
<elocation-id>073940</elocation-id>
<history>
<date date-type="received">
<day>06</day>
<month>9</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>07</day>
<month>9</month>
<year>2016</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2016, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2016</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="073940.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Neurons in high-level visual areas respond to more complex visual features with broader receptive fields (RFs) compared to those in low-level visual areas. Thus, high-level visual areas are generally considered to carry less information regarding the position of seen objects in the visual field. However, larger RFs may not imply loss of position information at the population level. Here, we evaluated how accurately the position of a seen object could be predicted (decoded) from activity patterns in each of six representative visual areas with different RF sizes (V1-V4, LOC, and FFA). We collected fMRI responses while subjects viewed a ball randomly moving in a two-dimensional field. To estimate population RF sizes of individual fMRI voxels, RF models were fitted for individual voxels in each brain area. The voxels in higher visual areas showed larger estimated RFs than those in lower visual areas. Then, the ball's position in a separate session was predicted by maximum likelihood estimation using the RF models of individual voxels. We also tested a model-free multivoxel regression (support vector regression, SVR) to predict the position. We found that regardless of the difference in RF size, all visual areas showed similar prediction accuracies, especially on the horizontal dimension. The results suggest that precise position information is available in population activity of higher visual cortex, and that it may be used in later neural processing for recognition and behavior.</p>
<sec>
<title>Significance statement</title>
<p>High-level ventral visual areas are thought to achieve position invariance with larger receptive fields at the cost of the loss of precise position information. However, larger receptive fields may not imply loss of position information at the population level. Here, multivoxel fMRI decoding reveals that high-level visual areas are predictive of an object&#x2019;s position with similar accuracies to low-level visual areas, preserving the information potentially available for later processing.</p></sec>
</abstract>
<counts>
<page-count count="22"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Along the ventral visual cortical pathway, neurons in higher-level areas respond to more complex visual features with broader receptive fields (RFs). This is thought to serve to represent objects regardless of the position in the visual field. Because of this receptive field property, position information is often assumed to be lost in these areas (<xref ref-type="bibr" rid="c9">Ito et al., 1995;</xref> <xref ref-type="bibr" rid="c13">Logothetis and Sheinberg, 1996</xref>; <xref ref-type="bibr" rid="c17">Tanaka, 1996</xref>). However, the loss of position information in single neurons does not necessarily imply the loss of position information at the population level. Theoretical studies have suggested that if the RFs of model neurons are uniformly distributed in the 2D visual field, the Fisher information about the position of a stimulus is not degraded by an increase in RF size (<xref ref-type="bibr" rid="c18">Zhang and Sejnowski, 1999</xref>; <xref ref-type="bibr" rid="c7">Eurich and Wilke, 2000</xref>). As the Fisher information provides the theoretical lower bound of the estimation/decoding error, position information may not be lost even in visual areas with large RFs, such as the lateral occipital complex (LOC) and fusiform face area (FFA). While several recent fMRI studies demonstrated successful classification of the position (e.g. left vs. right, upper vs. lower) of a presented object from ventral visual areas (<xref ref-type="bibr" rid="c15">Schwarzlose et al., 2008</xref>; <xref ref-type="bibr" rid="c2">Carlson et al., 2011</xref>; <xref ref-type="bibr" rid="c8">Golomb and Kanwisher, 2011</xref>), the relationship between RF size and decoded position information across visual areas has not been quantitatively examined.</p>
<p>Here, we estimated RF sizes for fMRI voxels and evaluated how accurately the position of a seen object was predicted (decoded) from activity patterns in each of six representative visual areas (V1&#x2013;V4, LOC, and FFA). In our experiments, we collected fMRI responses while subjects viewed a ball randomly moving in a two-dimensional field (<xref rid="fig1" ref-type="fig">Figure 1</xref>; a ball with a diameter of 1.6&#x00B0; presented within a 7.6&#x00B0; &#x00D7; 7.6&#x00B0; square field). The subjects were instructed to fix their eyes to the fixation point and keep track of the ball in their mind. fMRI activity was collected at a 3 &#x00D7; 3 &#x00D7; 3 mm resolution, and the signals from voxels in areas V1&#x2013;V4, LOC and FFA were analyzed (see Materials and methods). To estimate RF sizes, RF models were fitted for individual voxels in each brain area (<xref ref-type="bibr" rid="c5">Dumoulin and Wandell, 2008</xref>). In the decoding analysis, the ball position was predicted either by maximum likelihood estimation using the RF models of individual voxels or by support vector regression (SVR; <xref ref-type="bibr" rid="c4">Drucker et al., 1997</xref>; <xref ref-type="bibr" rid="c3">Chang and Lin, 2011</xref>) with multivoxel patterns as inputs (<xref rid="fig1" ref-type="fig">Figure 1</xref>; see Materials and methods). While the maximum likelihood method provides straightforward interpretation given accurate RF models, SVR is expected to perform model-free information retrieval from fMRI data.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Visual stimulus and analysis overview.A white-and-black checkered sphere was display on a screen with aflickering rate of 6 Hz.notations show the size of sphere and the size of the field where the sphere could move.the position of the center of the sphere was predicted from measured brain activity.prediction was performed based on maximum likelihood estimation using estimated receptive field models or the support vector regression algorithm.</p></caption>
<graphic xlink:href="073940_fig1.tif"/>
</fig>
</sec>
<sec id="s2">
<title>Materials and methods</title>
<sec id="s2a">
<title>Subjects</title>
<p>Five healthy subjects (one female and four males, aged between 23 and 31) with normal or corrected-to-normal vision participated in our experiments. This sample size was chosen based on previous fMRI studies with similar experimental designs (<xref ref-type="bibr" rid="c5">Dumoulin and Wandell, 2008</xref>; <xref ref-type="bibr" rid="c1">Amano et al., 2009</xref>). We obtained written informed consent from all subjects prior to their participation in the experiments, and the Ethics Committee at the [authours&#x2019; institute] approved the study protocol.</p>
</sec>
<sec id="s2b">
<title>Position tracking experiment</title>
<p>The stimulus was created with Psychtoolbox-3 (<ext-link ext-link-type="uri" xlink:href="http://psychtoolbox.org/">http://psychtoolbox.org/</ext-link>)(RRID: SCR_002881) and the associated openGL for Psychtoolbox extension. The stimulus was projected onto a display in the fMRI scanner and viewed through a mirror attached to the headcoil. We conducted three scanning sessions (runs) for each subject. In each run, an initial rest period of 32 s was followed by four blocks of stimulus presentation, which each lasted for 240 s. The stimulus presentation blocks were separated by 12-s rest periods. An extra 12-s rest period was added to the end of each run (1,040 s total for each run). During each of the rest periods, a circular fixation point (0.25&#x00B0; diameter) was displayed on the center of the display and subjects were instructed to attend to this point. During stimulus presentation, in addition to the fixation point, a white-and-black checkered sphere with a diameter of 1.6&#x00B0; was displayed with a flickering rate of 6 Hz (<xref rid="fig1" ref-type="fig">Figure 1</xref>).</p>
<p>The sphere was programmed to move in a random orbit produced by the following process. For each frame, the position of the center of the sphere was updated by
<disp-formula id="ueqn1">
<alternatives><graphic xlink:href="073940_ueqn1.gif"/></alternatives></disp-formula>
where <bold>s</bold>(<italic>t</italic>) is the position at frame <italic>t</italic> (i.e. <bold>s</bold>(<italic>t</italic>) = (<italic>s</italic><sub><italic>x</italic></sub>(<italic>t</italic>), <italic>s</italic><sub><italic>y</italic></sub>(<italic>t</italic>))) and <bold>p</bold>(<italic>t</italic>) is the vector indicating the direction of the movement from frame <italic>t</italic> to (<italic>t</italic> &#x002B; 1), which imitates momentum. The constant <italic>c</italic>, which is a parameter that controls the speed, was set to 0.008 in this study. The vector<bold>p</bold>(<italic>t</italic>) was updated by
<disp-formula id="ueqn2">
<alternatives><graphic xlink:href="073940_ueqn2.gif"/></alternatives></disp-formula>
where <bold>&#x03B5;</bold> is a random vector sampled from a two dimensional Gaussian distribution <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="073940_inline1.gif"/></alternatives></inline-formula> for every frame. &#x03C3; was set to 0.1 in this study. The movement of the sphere center was limited within a 6.0&#x00B0; &#x00D7; 6.0&#x00B0; square field (the stimulus spanned a 7.6&#x00B0; &#x00D7; 7.6&#x00B0; square field. If <bold>s</bold>(<italic>t</italic> &#x002B; 1) was not in the allowed region in terms of horizontal or vertical position, the first or second element of <bold>p</bold>(<italic>t</italic>) was multiplied by &#x2212;1 before the position was updated. This procedure ensures that the sphere is bound to the edge of the allowed region. The frame rate of stimulus presentation was 60 Hz.</p>
</sec>
<sec id="s2c">
<title>Retinotopy experiment</title>
<p>The retinotopy experiments were conducted according to the conventional protocol (<xref ref-type="bibr" rid="c6">Engel et al., 1994</xref>; <xref ref-type="bibr" rid="c16">Sereno et al., 1995</xref>). We used a rotating wedge and an expanding ring covered in a flickering checkerboard. The data were used to delineate the borders between visual cortical areas, and to identify the retinotopic map (V1&#x2013;V4) on the flattened cortical surfaces of individual subjects.</p>
</sec>
<sec id="s2d">
<title>Localizer experiment</title>
<p>The functional localizer experiments were conducted to identify the lateral occipital complex (LOC)(<xref ref-type="bibr" rid="c12">Kourtzi and Kanwisher, 2000</xref>) and fusiform face area (FFA)(<xref ref-type="bibr" rid="c10">Kanwisher et al., 1997</xref>) for each individual subject. The localizer experiments comprised four to eight runs, and each run contained 16 stimulus blocks. In the experiments, intact or scrambled images (12&#x00B0; &#x00D7; 12&#x00B0;) belonging to face, object, house, and scene categories were presented around the center of the screen. Stimuli from each of the eight stimulus types (four categories &#x00D7; two conditions) were presented twice per run. Each stimulus block consisted of a 15-s intact or scrambled stimulus presentation. The intact and scrambled stimulus blocks were presented successively (the order of the intact and scrambled stimulus blocks was random), followed by a 15-s rest period where a uniform gray background was displayed. Extra 33-s and 6-s rest periods were presented before and after each run, respectively. In each stimulus block, 20 different images of the same stimulus type were presented for 0.3 s, separated by 0.4-second-long blank intervals.</p>
</sec>
<sec id="s2e">
<title>MRI acquisition</title>
<p>We collected fMRI data using a 3.0-Tesla Siemens MAGNETOM Trio a Tim scanner located at [the institute where the experiments were conducted]. An interleaved T2&#x002A;-weighted gradient-EPI scan was performed to acquire functional images of the entire occipital lobe (position tracking experiment and retinotopy experiment: TR, 2,000 ms; TE, 30 ms; flip angle, 80 deg; FOV, 192 &#x00D7; 192 mm; voxel size, 3 &#x00D7; 3 &#x00D7; 3 mm; slice gap, 0 mm; number of slices, 30; localizer experiment: TR, 3,000 ms; TE, 30 ms; flip angle, 80 deg; FOV, 192 &#x00D7; 192 mm; voxel size, 3 &#x00D7; 3 &#x00D7; 3 mm; slice gap, 0 mm; number of slices, 50). T2-weighted turbo spin echo images were scanned to acquire high-resolution anatomical images of the same slices used for the EPI (position tracking experiment and retinotopy experiment: TR, 6,000 ms; TE, 57 ms; flip angle, 160 deg; FOV, 192 &#x00D7; 192 mm; voxel size, 0.75 &#x00D7; 0.75 &#x00D7; 3.0 mm; localizer experiment: TR, 7,020 ms; TE, 69 ms; flip angle, 160 deg; FOV, 192 &#x00D7; 192 mm; voxel size,0.75 &#x00D7; 0.75 &#x00D7; 3.0 mm). T1-weighted magnetization-prepared rapid acquisition gradient-echo (MP-RAGE) fine-structural images of the entire head were also acquired (TR, 2,250 ms; TE,3.06 ms; TI, 900 ms; flip angle, 9 deg, FOV, 256 &#x00D7; 256 mm; voxel size, 1.0 &#x00D7; 1.0 &#x00D7; 1.0 mm).</p>
</sec>
<sec id="s2f">
<title>MRI data preprocessing</title>
<p>The first 8-s scans (position tracking experiment and retinotopy experiment) or 9-s scans (localizer experiment) of each run were discarded to avoid MRI scanner instability. We then subjected the acquired fMRI data to three-dimensional motion correction with SPM5 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</ext-link>). Those data were then coregistered to the within-session high-resolution anatomical images of the same slices used for EPI and subsequently to the whole-head high-resolution anatomical images. The coregistered data were then re-interpolated as 3 &#x00D7; 3 &#x00D7; 3 mm voxels.</p>
<p>For the data from the position tracking experiment, the signal amplitudes from individual voxels were linearly detrended in each run and shifted by 4 s (two fMRI volumes) to compensate for hemodynamic delay.</p>
</sec>
<sec id="s2g">
<title>Region of interest (ROI) selection</title>
<p>V1, V2, V3, and V4 were identified using the data from the retinotopy experiments (<xref ref-type="bibr" rid="c6">Engel et al., 1994</xref>; <xref ref-type="bibr" rid="c16">Sereno et al., 1995</xref>). The lateral occipital complex (LOC) and fusiform face area (FFA) were identified using the data from the functional localizer experiments (<xref ref-type="bibr" rid="c10">Kanwisher et al., 1997</xref>; <xref ref-type="bibr" rid="c12">Kourtzi and Kanwisher, 2000</xref>). The data from the retinotopy experiment were transformed into Talairach space and the visual cortical borders were delineated on the flattened cortical surfaces using BrainVoyager QX (<ext-link ext-link-type="uri" xlink:href="http://www.brainvoyager.com">http://www.brainvoyager.com</ext-link>)(RRID: SCR_013057). The coordinates of voxels around the gray-white matter boundary in V1&#x2013;V4 were identified and transformed back into the original coordinates of the EPI images. The localizer experiment data were analyzed using SPM5. The voxels showing significantly higher activation in response to intact object or face images compared with that for scrambled images (<italic>t</italic>-test, uncorrected <italic>p</italic> &#x003C; 0.05 or 0.01) were identified, and defined as the LOC and FFA, respectively.</p>
</sec>
<sec id="s2h">
<title>Population receptive field model fitting</title>
<p>To estimate the receptive field, we fitted a population receptive field model to voxel amplitudes from each voxel in the visual cortex. We used fMRI data from the position tracking experiment in the analysis. Our model was based on a two-dimensional Gaussian receptive field and the noise on voxel amplitudes was assumed to be Gaussian (<xref ref-type="bibr" rid="c5">Dumoulin and Wandell, 2008</xref>). Mathematically, this assumption was expressed by
<disp-formula id="ueqn3">
<alternatives><graphic xlink:href="073940_ueqn3.gif"/></alternatives></disp-formula>
and
<disp-formula id="ueqn4">
<alternatives><graphic xlink:href="073940_ueqn4.gif"/></alternatives></disp-formula></p>
<p><inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="073940_inline2.gif"/></alternatives></inline-formula> and <italic>r</italic>(<italic>t</italic>) are the fitted and observed voxel amplitudes for the <italic>t</italic>-th fMRI volume. <italic>C</italic><sub>0</sub>,<italic>C</italic><sub>1</sub>, <italic>&#x03BC;</italic><sub><italic>x</italic></sub>, <italic>&#x03BC;</italic><sub><italic>y</italic></sub>, <italic>&#x03C3;</italic>, and <italic>&#x03C3;<sub>noise</sub></italic> are constants to be estimated. <italic>I</italic>(<italic>x</italic>, <italic>y</italic>, <italic>t</italic>) is the binary image function whose output is one if the visual stimulus is present at location(<italic>x</italic>, <italic>y</italic>) at the time of the<italic>t</italic>-th fMRI volume measurement, and zero otherwise.</p>
<p>The six parameters were fitted by maximum likelihood estimation, which was done by maximizing</p>
<disp-formula id="ueqn5">
<alternatives><graphic xlink:href="073940_ueqn5.gif"/></alternatives></disp-formula>
<p><italic>T</italic> is the number of the fMRI volumes used for model fitting, and we used 960 volumes from two experimental runs. <italic>p</italic>(<italic>r</italic>(<italic>t</italic>)|<italic>C</italic><sub>0</sub>, <italic>C</italic><sub>1</sub>, <italic>&#x03BC;</italic><sub><italic>x</italic></sub>, <italic>&#x03BC;</italic><sub><italic>y</italic></sub>, <italic>&#x03C3;</italic>, <italic>&#x03C3;<sub>noise</sub></italic>) is the probability density function of <italic>r</italic>(<italic>t</italic>) given the six parameters. The maximization was conducted using a tool implemented in MATLAB (fminsearch.m from the optimization toolbox). To avoid local solutions, initial values in the optimization were searched on a regular grid. As per previous studies (<xref ref-type="bibr" rid="c5">Dumoulin and Wandell, 2008</xref>; <xref ref-type="bibr" rid="c11">Kay et al., 2008</xref>; <xref ref-type="bibr" rid="c14">Nishimoto et al., 2011</xref>), only well-fitted voxels were used in the analysis. First, we eliminated the voxels whose estimated RF centers were outside the field the stimulus sphere could span (7.6&#x00B0; &#x00D7; 7.6&#x00B0;). Then we calculated the correlation coefficients between the real and fitted amplitudes to evaluate the fitness. The voxels with <italic>r</italic> &#x003E; 0.2 were used.</p>
<p>The estimated models were also used in the decoding analysis. To separate data for decoding analysis and for RF model fitting, we performed a cross-validation procedure. In our experiments, each subject participated in the position tracking experiment that consisted of three experimental runs. Two runs were used for fitting receptive field models and the rest run was used as test data in the decoding analysis. The test run was shifted such that all runs were treated as test data once (leave-one-run-out cross-validation).</p>
</sec>
<sec id="s2i">
<title>Decoding analysis</title>
<p>We used the RF model or support vector regression (SVR)(<xref ref-type="bibr" rid="c4">Drucker et al., 1997</xref>; <xref ref-type="bibr" rid="c3">Chang and Lin, 2011</xref>) to predict the position of the stimulus from fMRI responses. In the prediction with the RF models, we calculated the stimulus position with the highest likelihood as the predicted position for each fMRI volume. Thus, the predicted position with the RF models was
<disp-formula id="ueqn6">
<alternatives><graphic xlink:href="073940_ueqn6.gif"/></alternatives></disp-formula>
where
<disp-formula id="ueqn7">
<alternatives><graphic xlink:href="073940_ueqn7.gif"/></alternatives></disp-formula>
and
<disp-formula id="ueqn8">
<alternatives><graphic xlink:href="073940_ueqn8.gif"/></alternatives></disp-formula></p>
<p>Here, <italic>s<sub>x</sub></italic> and <italic>s</italic><sub><italic>y</italic></sub> are the parameters that indicate the position of the stimulus center in the model, and <italic>r</italic><sub>n</sub> is the voxel amplitude of the <italic>n</italic>-th voxel in a given fMRI response. <italic>p</italic>(<italic>r<sub>n</sub></italic>|<italic>s</italic><sub><italic>x</italic></sub>,<italic>s<sub>y</sub></italic>) is the probability density function of <italic>r<sub>n</sub></italic> given that the stimulus center is at <italic>s</italic><sub><italic>x</italic></sub>, <italic>s</italic><sub><italic>y</italic></sub>). We assumed the Gaussian noise on different voxels to be independent, and the voxels in each visual area were combined by taking the product of their probability density functions.Here, <italic>C</italic><sub>0(<italic>n</italic>)</sub>, <italic>C</italic><sub>1(<italic>n</italic>)</sub>, <italic>&#x03BC;</italic><sub><italic>x</italic>(<italic>n</italic>)</sub>, <italic>&#x03BC;</italic><sub><italic>y</italic>(<italic>n</italic>)</sub>, <italic>&#x03C3;</italic><sub>(<italic>n</italic>)</sub>, and <italic>&#x03C3;</italic><sub><italic>noise</italic>(<italic>n</italic>)</sub> are the RF model parameters for the <italic>n</italic>-th voxel. <italic>I</italic>(<italic>x</italic>, <italic>y</italic>; <italic>s</italic><sub><italic>x</italic></sub>, <italic>s</italic><sub><italic>y</italic></sub>) is the binary image function when the stimulus is centered on (<italic>s</italic><sub><italic>x</italic></sub>, <italic>s</italic><sub><italic>y</italic></sub>), thus the value of <italic>I</italic>(<italic>x</italic>, <italic>y</italic>; <italic>s</italic><sub><italic>x</italic></sub>, <italic>s</italic><sub><italic>y</italic></sub>) is one if the distance between (<italic>x</italic>, <italic>y</italic>) and (<italic>s</italic><sub><italic>x</italic></sub>, <italic>s</italic><sub><italic>y</italic></sub>) is less than the stimulus radius (0.8&#x00B0;), and zero otherwise.</p>
<p>For practical reasons, for each fMRI volume, we calculated the likelihood for each of 60 &#x00D7; 60 positions in the visual field and the position with the highest likelihood was treated as the predicted position.</p>
<p>In the prediction with SVR, the predicted position is given by
<disp-formula id="ueqn9">
<alternatives><graphic xlink:href="073940_ueqn9.gif"/></alternatives></disp-formula>
where
<disp-formula id="ueqn10">
<alternatives><graphic xlink:href="073940_ueqn10.gif"/></alternatives></disp-formula>
<bold>w</bold><sub><italic>x</italic></sub> and <bold>w</bold><sub><italic>y</italic></sub> are weight vectors, <italic>b</italic><sub><italic>x</italic></sub> and <italic>b</italic><sub><italic>y</italic></sub> are biases, and &#x03A6;(<bold>r</bold>) is a vector function that satisfies
<disp-formula id="ueqn11">
<alternatives><graphic xlink:href="073940_ueqn11.gif"/></alternatives></disp-formula></p>
<p>The models were trained by minimizing the cost function of the SVR algorithm with training data, and the model training and prediction were performed without explicitly calculating the weight vectors by using the kernel trick (<xref ref-type="bibr" rid="c4">Drucker et al., 1997</xref>; <xref ref-type="bibr" rid="c3">Chang and Lin, 2011</xref>) (RRID: SCR_010243).</p>
<p>We generated predicted positions for 1,440 fMRI volumes in three runs, and calculated the correlation coefficient between the true and predicted positions in the horizontal or vertical axes as the prediction accuracy.</p>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<p>First, we fitted an RF model to the response of each voxel (<xref ref-type="bibr" rid="c5">Dumoulin and Wandell, 2008</xref>). Our model consists of a two-dimensional Gaussian receptive field with the parameters of the mean (x, y positions) and the standard deviation (RF size). Gaussian noise is assumed in the response amplitude. To evaluate the fitness, we calculated the correlation coefficients between the real and fitted amplitudes. As per in previous studies, only well-fitted voxels with <italic>r</italic> &#x003E; 0.2 were used in the analysis (<xref ref-type="bibr" rid="c5">Dumoulin and Wandell, 2008</xref>; <xref ref-type="bibr" rid="c11">Kay et al., 2008</xref>; <xref ref-type="bibr" rid="c14">Nishimoto et al., 2011</xref>) (<italic>n</italic> = 192&#x00B1;52, 237&#x00B1;47, 254&#x00B1;78, 135&#x00B1;84, 155&#x00B1;91, and 164&#x00B1;108 for V1&#x2013;V4, LOC, and FFA, mean &#x00B1; S.D. across subjects and sessions). Estimated RF sizes tended to be larger for voxels in the higher visual cortex (<xref rid="fig2" ref-type="fig">Figure 2A</xref>), consistent with previous studies (<xref ref-type="bibr" rid="c5">Dumoulin and Wandell, 2008</xref>; <xref ref-type="bibr" rid="c1">Amano et al., 2009</xref>).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Sizes of estimated receptive fields and decoding accuracy. <bold>(A)</bold> Mean receptive field size for each visual area. We evaluated the receptive field size of each voxel using the parameter sigma of the fitted Gaussian receptive field. Colored lines show the mean across voxels for individual subjects. Black line shows the mean across subjects. <bold>(B)</bold> Examples of true and predicted trajectories of the ball position. The predicted trajectories were produced by maximum likelihood estimation using the receptive field models. <bold>(C)</bold> Decoding accuracy. The ball position was predicted from brain activity by maximum likelihood estimation with the RF models (left) and SVR (right). The accuracy was evaluated using the correlation coefficient between the true and predicted trajectories. The calculations were performed separately for the horizontal (black line) and vertical (gray line) positions. Error bars show the 95&#x0025; confidence intervals across subjects.</p></caption>
<graphic xlink:href="073940_fig2.tif"/>
</fig>
<p>Using the models described above, we conducted a decoding analysis to evaluate the amount of position information in each visual area. We estimated the 2D-coordinates of the ball position by taking the position with the highest likelihood for a given fMRI activity pattern. To quantify the prediction accuracy, we calculated the correlation coefficient between the true and predicted coordinates for each of the horizontal and vertical axes. Model fitting and position prediction were performed on fMRI data from separate runs via a cross-validation procedure (leave-one-run-out cross-validation).</p>
<p>The ball position was well predicted from the brain activity in all brain areas tested (<xref rid="fig2" ref-type="fig">Figure 2B,C left;</xref> Movie 1): the correlation coefficients between the true and predicted positions (mean across subjects; horizontal/vertical coordinates) were 0.75/0.73 for V1, 0.74/0.74 for V2, 0.77/0.75 for V3, 0.63/0.62 for V4, 0.66/0.35 for LOC, and 0.66/0.40 for FFA (95&#x0025; CIs:[0.54, 0.87]/[0.48, 0.87], [0.59, 0.84]/[0.53, 0.86], [0.50, 0.90]/[0.43, 0.90], [0.17, 0.86]/[0.21,0.85], [0.47, 0.79]/[0.04, 0.60], and [0.50, 0.77]/[0.05, 0.66], respectively). Notably, the two higher visual areas with large RFs showed effective position decoding. All areas showed similar predictive performance for the horizontal position (<xref rid="fig2" ref-type="fig">Figure 2C</xref> left, black line). However, the decoding accuracy showed a decline in the LOC and FFA (<xref rid="fig2" ref-type="fig">Figure 2C</xref> left, gray line). This anisotropy can also be seen by plotting the decoding accuracies for pairs of opposing directions (<xref rid="fig3" ref-type="fig">Figure 3</xref>). The anisotropy in the LOC and FFA is consistent with the classification results in a previous fMRI study (<xref ref-type="bibr" rid="c2">Carlson et al., 2011</xref>), although the previous study did not test it for the lower visual cortex.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Decoding accuracy along eight directions. The true positions of the stimuli and positions predicted with RF models were projected in each of eight directions. Then, we calculated the correlation coefficient between the true and predicted coordinates in each direction and plotted these values on a polar plot. The same values were plotted for symmetrical directions. The center and the outer edge of the polar plots correspond to the correlation values of zero and one, respectively.</p></caption>
<graphic xlink:href="073940_fig3.tif"/>
</fig>
<p>The brain areas compared here contained different numbers of voxels. So, to confirm that this result was not due to the difference in the number of voxels used for prediction, we conducted the same decoding analysis with 20 randomly selected voxels within each brain area. We obtained similar comparison results (<xref rid="fig4" ref-type="fig">Figure 4</xref>). We also obtained a similar pattern of decoding performance with SVR (<xref rid="fig2" ref-type="fig">Figure 2C</xref>, right), indicating that this tendency is independent of the decoding method.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Decoding accuracy after matching the numbers of voxels. The format is the same as in <xref rid="fig2" ref-type="fig">Figure 2C</xref>. We performed decoding analysis with RF models on brain activity from 20 randomly selected voxels in each visual area. Decoding accuracies were first averaged across 100 instances of random voxel selection in individual subjects, and then averaged across subjects. Error bars show the 95&#x0025; confidence interval across subjects. After matching the numbers of voxels, we observed a similar tendency as in <xref rid="fig2" ref-type="fig">Figure 2C</xref>. This indicates that the tendency across visual areas was not caused by the difference in the number of voxels.</p></caption>
<graphic xlink:href="073940_fig4.tif"/>
</fig>
<p>To find out factors that could affect the anisotropy, we examined the distribution of the RF centers of individual voxels in each area (<xref rid="fig5" ref-type="fig">Figure 5A,B</xref>. In LOC and FFA, the vertical positions of the RFs were narrowly distributed compared with V1&#x2013;V4, while the horizontal positions of the RFs were distributed with similar degrees for all visual areas. This suggests that the lower decoding accuracies of LOC and FFA for the vertical direction could be attributable to the narrow distribution of the RFs along this direction, which is a factor not related to RF size.</p>
<table-wrap id="utbl1" orientation="portrait" position="float">
<graphic xlink:href="073940_utbl1.tif"/>
</table-wrap>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>In the present study, to investigate the relationship between the size of RFs and retrievable position information, we estimated RF sizes for fMRI voxels and evaluated how accurately the position of a seen object was predicted from activity patterns in each of six representative visual areas. We found that even with larger RF sizes, the position of the stimulus was predicted from activity patterns in high-level visual areas with similar accuracies to low-level visual areas especially for the horizontal position (<xref rid="fig2" ref-type="fig">Figure 2</xref>).</p>
<p>In the comparison of the decoding accuracy between the horizontal and vertical positions, the decoding accuracies for activity in the LOC and FFA regarding the vertical position were lower than those for the horizontal position, and this anisotropy was not observed for the lower visual areas (<xref rid="fig2" ref-type="fig">Figure 2B,C</xref> and <xref rid="fig3" ref-type="fig">Figure 3</xref>). Although a previous fMRI study came to a similar conclusion on the anisotropy in the LOC and FFA (<xref ref-type="bibr" rid="c2">Carlson et al., 2011</xref>), our study have compared lower to higher visual areas along the ventral cortical hierarchy using quantitative models. Furthermore, we demonstrated that these lower decoding accuracies are accompanied by a narrow spatial distribution of RFs for the corresponding direction (<xref rid="fig5" ref-type="fig">Figure5</xref>), which may be a cause of the horizontal-vertical asymmetry in decoding accuracy. Further investigation of such collective properties of RFs would be useful for characterizing the mechanism and function of each brain region in representing position information.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Spatial distribution of estimated receptive fields. <bold>(A)</bold> Examples of the distribution of estimated receptive field centers. Each circle shows the position of the receptive field center of a single voxel. We plotted the positions for the voxels in V1 and FFA from subject S3. <bold>(B)</bold> Standard deviation of the positions of receptive field centers. Error bars show the 95&#x0025; confidence intervals across subjects.</p></caption>
<graphic xlink:href="073940_fig5.tif"/>
</fig>
<p>Taken together, our findings provide experimental evidence that large RFs do not imply the loss of position information at the population revel. Regions in the higher visual cortex, such as LOC and FFA, appear to encode as much position information as the lower visual cortex, especially in the horizontal dimension, regardless of RF size. While our results demonstrate the availability of rich position information in higher visual cortex, it remains to be seen whether and how such information is used in later neural processing for recognition and behavior.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>The authors would like to thank Mitsuaki Tsukamoto, Makoto Takemiya, and Keiji Harada for helpful comments on the manuscript. This work was supported by Strategic International Cooperative Program (JST/AMED), ImPACT (Cabinet Office, Japan), and MEXT/JSPS KAKENHI grant no. 15H05920, 15H05710.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Amano</surname> <given-names>K</given-names></string-name>, <string-name><surname>Wandell</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Dumoulin</surname> <given-names>SO</given-names></string-name> (<year>2009</year>) <article-title>Visual field maps, population receptive field sizes, and visual field coverage in the human MT&#x002B; complex</article-title>. <source>J Neurophysiol</source> <volume>102</volume>: <fpage>2704</fpage>&#x2013;<lpage>2718</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Carlson</surname> <given-names>T</given-names></string-name>, <string-name><surname>Hogendoorn</surname> <given-names>H</given-names></string-name>, <string-name><surname>Fonteijn</surname> <given-names>H</given-names></string-name>, <string-name><surname>Verstraten</surname> <given-names>FAJ</given-names></string-name> (<year>2011</year>) <article-title>Spatial coding and invariance in object-selective cortex</article-title>. <source>Cortex</source> <volume>47</volume>: <fpage>14</fpage>&#x2013;<lpage>22</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Chang</surname> <given-names>C-C</given-names></string-name>, <string-name><surname>Lin</surname> <given-names>C-J</given-names></string-name> (<year>2011</year>) <article-title>LIBSVM: A library for support vector machines</article-title>. <source>ACM Trans IntellSyst Technol</source> <volume>2</volume>: <fpage>1</fpage>&#x2013;<lpage>27</lpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Drucker</surname> <given-names>H</given-names></string-name>, <string-name><surname>Burges</surname> <given-names>CJC</given-names></string-name>, <string-name><surname>Kaufman</surname> <given-names>L</given-names></string-name>, <string-name><surname>Smola</surname> <given-names>A</given-names></string-name>, <string-name><surname>Vapnik</surname> <given-names>V</given-names></string-name> (<year>1997</year>) <article-title>Support vector regression machines</article-title>. <source>Adv Neural Inf Process Syst</source> <volume>9</volume>: <fpage>155</fpage>&#x2013;<lpage>161</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Dumoulin</surname> <given-names>SO</given-names></string-name>, <string-name><surname>Wandell</surname> <given-names>BA</given-names></string-name> (<year>2008</year>) <article-title>Population receptive field estimates in human visual cortex</article-title>.<source>Neuroimage</source> <volume>39</volume>: <fpage>647</fpage>&#x2013;<lpage>660</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Engel</surname> <given-names>SA</given-names></string-name>, <string-name><surname>Rumelhart</surname> <given-names>DE</given-names></string-name>, <string-name><surname>Wandell</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>AT</given-names></string-name>, <string-name><surname>Glover</surname> <given-names>GH</given-names></string-name>, <string-name><surname>Chichilnisky</surname> <given-names>EJ</given-names></string-name>, <string-name><surname>Shadlen</surname> <given-names>MN</given-names></string-name> (<year>1994</year>) <article-title>fMRI of human visual cortex</article-title>. <source>Nature</source> <volume>369</volume>:<fpage>525</fpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Eurich</surname> <given-names>CW</given-names></string-name>, <string-name><surname>Wilke</surname> <given-names>SD</given-names></string-name> (<year>2000</year>) <article-title>Multidimensional encoding strategy of spiking neurons</article-title>. <source>Neural Comput</source> <volume>12</volume>: <fpage>1519</fpage>&#x2013;<lpage>1529</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="other"><string-name><surname>Golomb</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name> (<year>2011</year>) <article-title>Higher Level Visual Cortex Represents Retinotopic, Not Spatiotopic, Object Location</article-title>. <source>Cereb Cortex</source>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Ito</surname> <given-names>M</given-names></string-name>, <string-name><surname>Tamura</surname> <given-names>H</given-names></string-name>, <string-name><surname>Fujita</surname> <given-names>I</given-names></string-name>, <string-name><surname>Tanaka</surname> <given-names>K</given-names></string-name> (<year>1995</year>) <article-title>Size and position invariance of neuronal responses in monkey inferotemporal cortex</article-title>. <source>J Neurophysiol</source> <volume>73</volume>: <fpage>218</fpage>&#x2013;<lpage>226</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name>, <string-name><surname>McDermott</surname> <given-names>J</given-names></string-name>, <string-name><surname>Chun</surname> <given-names>MM</given-names></string-name> (<year>1997</year>) <string-name><surname>The</surname> <given-names>fusiform face area: a module in human extrastriate cortex specialized for face perception</given-names></string-name>. <source>J Neurosci</source> <volume>17</volume>: <fpage>4302</fpage>&#x2013;<lpage>4311</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Kay</surname> <given-names>KN</given-names></string-name>, <string-name><surname>Naselaris</surname> <given-names>T</given-names></string-name>, <string-name><surname>Prenger</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name> (<year>2008</year>) <string-name><surname>Identifying</surname> <given-names>natural images from human brain activity</given-names></string-name>. <source>Nature</source> <volume>452</volume>: <fpage>352</fpage>&#x2013;<lpage>355</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Kourtzi</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name> (<year>2000</year>) <article-title>Cortical regions involved in perceiving object shape</article-title>. <source>JNeurosci</source> <volume>20</volume>: <fpage>3310</fpage>&#x2013;<lpage>3318</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Logothetis</surname> <given-names>NK</given-names></string-name>, <string-name><surname>Sheinberg</surname> <given-names>DL</given-names></string-name> (<year>1996</year>) <article-title>Visual object recognition</article-title>. <source>Annu Rev Neurosci</source> <volume>19</volume>: <fpage>577</fpage>&#x2013;<lpage>621</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Nishimoto</surname> <given-names>S</given-names></string-name>, <string-name><surname>Vu</surname> <given-names>AT</given-names></string-name>, <string-name><surname>Naselaris</surname> <given-names>T</given-names></string-name>, <string-name><surname>Benjamini</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Yu</surname> <given-names>B</given-names></string-name>, <string-name><surname>Gallant</surname> <given-names>JL</given-names></string-name> (<year>2011</year>) <article-title>Reconstructing visual experiences from brain activity evoked by natural movies</article-title>. <source>Curr Biol</source> <volume>21</volume>: <fpage>1641</fpage>&#x2013;<lpage>1646</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Schwarzlose</surname> <given-names>RF</given-names></string-name>, <string-name><surname>Swisher</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Dang</surname> <given-names>S</given-names></string-name>, <string-name><surname>Kanwisher</surname> <given-names>N</given-names></string-name> (<year>2008</year>) <article-title>The distribution of category and location information across object-selective regions in human visual cortex</article-title>. <source>Proc Natl Acad Sci U S A</source> <volume>105</volume>: <fpage>4447</fpage>&#x2013;<lpage>4452</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Sereno</surname> <given-names>MI</given-names></string-name>, <string-name><surname>Dale</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Reppas</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Kwong</surname> <given-names>KK</given-names></string-name>, <string-name><surname>Belliveau</surname> <given-names>JW</given-names></string-name>, <string-name><surname>Brady</surname> <given-names>TJ</given-names></string-name>, <string-name><surname>Rosen</surname> <given-names>BR</given-names></string-name>, <string-name><surname>Tootell</surname> <given-names>RB</given-names></string-name> (<year>1995</year>) <article-title>Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging</article-title>. <source>Science</source> <volume>268</volume>: <fpage>889</fpage>&#x2013;<lpage>893</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Tanaka</surname> <given-names>K</given-names></string-name> (<year>1996</year>) <article-title>Inferotemporal cortex and object vision</article-title>. <source>Annu Rev Neurosci</source> <volume>19</volume>: <fpage>109</fpage>&#x2013;<lpage>139</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname> <given-names>K</given-names></string-name>, <string-name><surname>Sejnowski</surname> <given-names>TJ</given-names></string-name> (<year>1999</year>) <article-title>Neuronal tuning: To sharpen or broaden?</article-title> <source>Neural Comput</source> <volume>11</volume>: <fpage>75</fpage>&#x2013;<lpage>84</lpage>.</mixed-citation></ref>
</ref-list>
<sec id="s5">
<title>Movie legend</title>
<sec id="s5a">
<title>Movie 1</title>
<p>Examples of true and predicted ball positions. The predicted positions were produced by maximum likelihood estimation using the RF models.</p>
</sec>
</sec>
</back>
</article>