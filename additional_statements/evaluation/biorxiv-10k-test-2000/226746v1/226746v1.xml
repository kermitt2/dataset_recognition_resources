<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/226746</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A clustering neural network model of insect olfaction</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Pehlevan</surname><given-names>Cengiz</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Genkin</surname><given-names>Alexander</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Chklovskii</surname><given-names>Dmitri B.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Center for Computational Biology, Flatiron Institute</institution>, New York, NY</aff>
<aff id="a2"><label>2</label><institution>NYU Langone Medical Center</institution>, New York, NY</aff>
</contrib-group>
<author-notes>
<fn id="n1"><p><email>cpehlevan@flatironinstitute.org</email>, <email>alexander.genkin@gmail.com</email>, <email>mitya@flatironinstitute.org</email></p></fn>
</author-notes>
<pub-date pub-type="epub"><year>2017</year></pub-date>
<elocation-id>226746</elocation-id>
<history>
<date date-type="received">
<day>30</day>
<month>11</month>
<year>2017</year>
</date>
<date date-type="rev-recd">
<day>30</day>
<month>11</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>30</day>
<month>11</month>
<year>2017</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="226746.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract><title>Abstract</title>
<p>A key step in insect olfaction is the transformation of a dense representation of odors in a small population of neurons - projection neurons (PNs) of the antennal lobe - into a sparse representation in a much larger population of neurons - Kenyon cells (KCs) of the mushroom body. What computational purpose does this transformation serve? We propose that the PN-KC network implements an online clustering algorithm which we derive from the <italic>k</italic>-means cost function. The vector of PN-KC synaptic weights converging onto a given KC represents the corresponding cluster centroid. KC activities represent attribution indices, i.e. the degree to which a given odor presentation is attributed to each cluster. Remarkably, such clustering view of the PN-KC circuit naturally accounts for several of its salient features. First, attribution indices are nonnegative thus rationalizing rectification in KCs. Second, the constraint on the total sum of attribution indices for each presentation is enforced by a Lagrange multiplier identified with the activity of a single inhibitory interneuron reciprocally connected with KCs. Third, the soft-clustering version of our algorithm reproduces observed sparsity and overcompleteness of the KC representation which may optimize supervised classification downstream.</p>
</abstract>
<counts>
<page-count count="8"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1"><label>I.</label><title>Introduction</title>
<p>In the quest to understand neural computation, olfaction presents an attractive target. Thanks to recent technological developments, there is a wealth of experimental data on olfactory processing. Stereotypy of early olfactory processing between vertebrates and invertebrates suggests the existence of universal principles. Yet, a comprehensive algorithmic theory of olfaction remains elusive.</p>
<p>Here, we attempt to understand the computational task solved by one key olfactory circuit in invertebrates [<xref ref-type="bibr" rid="c1">1</xref>], <xref ref-type="fig" rid="fig1">Figure 1</xref>, top: antennal lobe (AL) projection neurons (PNs) synapsing onto a much larger population of Kenyon cells (KCs) in the mushroom body (MB) which in turn are reciprocally inhibited by a single giant interneuron (GI, which is called GGN in the locust [<xref ref-type="bibr" rid="c2">2</xref>], [<xref ref-type="bibr" rid="c3">3</xref>] and APL in <italic>Drosophila</italic> [<xref ref-type="bibr" rid="c4">4</xref>], [<xref ref-type="bibr" rid="c5">5</xref>]). The PN-KC-GI circuit transforms a dense PN activity vector into a sparse representation in KCs [<xref ref-type="bibr" rid="c6">6</xref>], [<xref ref-type="bibr" rid="c7">7</xref>], [<xref ref-type="bibr" rid="c8">8</xref>]. The lack of a teaching signal suggests that this transformation is learned in an unsupervised setting. In contrast, learning the weights of KC synapses onto MB output neurons (MBONs) is thought to rely on reinforcement [<xref ref-type="bibr" rid="c9">9</xref>].</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure"><label>Fig. 1.</label>
<caption><p>Top: Schematic of the insect olfactory system. Odors are transduced in the olfactory receptor neurons (ORNs). Each projection neurons (PN) receives inputs from ORNs of the same class in the AL glomerulus. Then, PNs (numbering &#x2248; 800 in the locust and &#x2248; 150 in <italic>Drosophila)</italic> synapse onto Kenyon cells (KCs) (numbering &#x2248; 50,000 in the locust [<xref ref-type="bibr" rid="c37">37</xref>] and &#x2248; 2,000 in <italic>Drosophila</italic> [<xref ref-type="bibr" rid="c9">9</xref>]) of the mushroom body (MB). A single giant interneuron (GI) provides reciprocal inhibition to KCs [<xref ref-type="bibr" rid="c4">4</xref>], [<xref ref-type="bibr" rid="c5">5</xref>]. KCs project onto a smaller number (34 in <italic>Drosophila</italic> [<xref ref-type="bibr" rid="c9">9</xref>]) of MB output neurons (MBONs). The dashed box delineates the circuit modeled in this paper. Bottom: A biologically plausible clustering network whose architecture is remarkably similar to the insect MB (P-principal neuron, I-interneuron).</p></caption>
<graphic xlink:href="226746_fig1.tif"/>
</fig>
</sec>
<sec id="s2"><label>II.</label><title>Related work</title>
<p>Many existing publications proposed mechanistic models of the PN-KC-GI circuit and attempted to explain known structural and functional features. In particular, they aimed to explain the sparseness of the KC activity [<xref ref-type="bibr" rid="c10">10</xref>], [<xref ref-type="bibr" rid="c2">2</xref>], [<xref ref-type="bibr" rid="c11">11</xref>], [<xref ref-type="bibr" rid="c12">12</xref>], [<xref ref-type="bibr" rid="c13">13</xref>] and studied how sparseness and other features affect olfactory learning and generalization at the output of KCs [<xref ref-type="bibr" rid="c14">14</xref>], [<xref ref-type="bibr" rid="c15">15</xref>], [<xref ref-type="bibr" rid="c16">16</xref>], [<xref ref-type="bibr" rid="c17">17</xref>]. A recent work proposed that KC representations solve a similarity search problem by representing similar odors with similar KC responses [<xref ref-type="bibr" rid="c18">18</xref>], following the similarity matching principle [<xref ref-type="bibr" rid="c19">19</xref>].</p>
<p>However, these papers did not attempt to derive the observed network structure and function from a principled computational objective. Such derivation is the goal of the present paper.</p>
<sec id="s2a"><label>A.</label><title>Demixing network models</title>
<p>Previous attempts to derive network structure and function assumed that the computational task of olfactory processing, in general [<xref ref-type="bibr" rid="c20">20</xref>], [<xref ref-type="bibr" rid="c21">21</xref>], [<xref ref-type="bibr" rid="c22">22</xref>], and the PN-KC-GI circuit, in particular [<xref ref-type="bibr" rid="c23">23</xref>], [<xref ref-type="bibr" rid="c24">24</xref>], [<xref ref-type="bibr" rid="c25">25</xref>], is compressive sensing or sparse autoencoding. In this view, odors are sparse vectors in the space of concentrations of all possible molecules. ORNs project these vectors onto the vectors of receptor affinities resulting in (normalized) vectors of AL PN activity. Then, the PN-KC projection recovers the concentration of each constituent molecule in the activity of a corresponding KC.</p>
<p>However, modeling olfaction with a compressive sensing or autoencoding computational objective has three problems. First, experimentally measured KC responses seem to contradict the demixing theory. Even though many KCs often respond selectively to a single component in an odor mixture [<xref ref-type="bibr" rid="c21">21</xref>], some KCs respond to specific mixtures but not to their components by themselves [<xref ref-type="bibr" rid="c21">21</xref>].</p>
<p>Second, circuits proposed for computing sparse representations contradict the PN-KC-GI circuit architecture. They require one of the following: an all-to-all KC connectivity [<xref ref-type="bibr" rid="c26">26</xref>], [<xref ref-type="bibr" rid="c27">27</xref>], [<xref ref-type="bibr" rid="c28">28</xref>], [<xref ref-type="bibr" rid="c29">29</xref>], many inhibitory interneurons [<xref ref-type="bibr" rid="c30">30</xref>], [<xref ref-type="bibr" rid="c31">31</xref>], [<xref ref-type="bibr" rid="c32">32</xref>], [<xref ref-type="bibr" rid="c33">33</xref>], [<xref ref-type="bibr" rid="c34">34</xref>], [<xref ref-type="bibr" rid="c35">35</xref>], or strictly feedforward architecture [<xref ref-type="bibr" rid="c23">23</xref>], [<xref ref-type="bibr" rid="c24">24</xref>], [<xref ref-type="bibr" rid="c20">20</xref>], all three inconsistent with the single inhibitory interneuron in the MB.</p>
<p>Third, demixing models often interpret nonnegative neural activity in KCs (spiking neurons cannot have negative firing rates) as a manifestation of the physical nonnegativity of molecular concentrations (counts of molecules cannot be negative) [<xref ref-type="bibr" rid="c36">36</xref>]. However, such explanation does not generalize to most other neurons in the brain whose activity is nevertheless nonnegative. Hence, there must be another reason.</p>
</sec>
<sec id="s2b"><label>B.</label><title>Classic online k-means algorithm and neural networks</title>
<p>In this paper, we explore the hypothesis that the computational task of the PN-KC-GI circuit is to cluster streaming olfactory stimuli. In this subsection, we review a classic online clustering algorithm and clustering neural networks.</p>
<p>The goal of clustering is to segregate data into <italic>k</italic> classes. A popular clustering algorithm, <italic>k</italic>-means, [<xref ref-type="bibr" rid="c38">38</xref>], [<xref ref-type="bibr" rid="c39">39</xref>] achieves this goal by minimizing the sum of squared distances between data points, &#x007B;x<sub>1</sub>,&#x2026;, x<sub><italic>T</italic></sub>&#x007D;, and cluster centroids, &#x007B;<bold>w</bold><sub>1</sub>,&#x2026;, <bold>w</bold><sub><italic>k</italic></sub>&#x007D;, specified by the attribution indices <italic>y<sub>i,t</sub>, i</italic> &#x2208; [1, <italic>k</italic>], <italic>t</italic> &#x2208; [1, <italic>T</italic>]:
<disp-formula id="eqn1"><alternatives><graphic xlink:href="226746_eqn1.gif"/></alternatives></disp-formula></p>
<p>Here and below, vectors are lowercase boldface and matrices capital boldface.</p>
<p>Importantly, a biologically plausible algorithm must minimize <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref> in the online (or streaming) setting. Indeed, sensory organs stream data to neural networks sequentially, one datum at a time, and the network has to compute the corresponding attribution indices on the fly without seeing the full dataset or storing any significant fraction of past inputs in memory.</p>
<p>The classic online <italic>k</italic>-means algorithm [<xref ref-type="bibr" rid="c38">38</xref>] performs alternating minimization of <xref ref-type="disp-formula" rid="eqn1">Eq. 1</xref> by, first, assigning current datum, x<sub><italic>t</italic></sub>, to the cluster with the closest centroid, w<sub><italic>c</italic></sub>, i.e. &#x201C;winner-take-all&#x201D; (WTA), and, second, updating the number of data points in the winning cluster, <italic>n<sub>c</sub></italic>, and the centroid of that cluster:</p>
<statement><label>Algorithm 1</label><title>Classic online algorithm for <italic>k</italic>-means clustering</title>
<p><list list-type="order">
<list-item><p>Initialize <bold>w</bold><sub>1</sub>, and <bold>n</bold> &#x003D; <bold>1</bold></p></list-item>
<list-item><p>Repeat for each <italic>t</italic> &#x003D; 1,&#x2026;</p></list-item>
<list-item><p>Set
<disp-formula id="eqn2"><alternatives><graphic xlink:href="226746_eqn2.gif"/></alternatives></disp-formula></p></list-item>
<list-item><p>Update
<disp-formula id="eqn3"><alternatives><graphic xlink:href="226746_eqn3.gif"/></alternatives></disp-formula></p></list-item>
</list></p>
</statement>
<p>The online <italic>k</italic>-means algorithm is often given a neural interpretation [<xref ref-type="bibr" rid="c40">40</xref>], where <italic>y<sub>i,t</sub></italic> is the activity of the <italic>i</italic><sup>th</sup> output neuron (KC) in response to input x<sub><italic>t</italic></sub> and w<sub><italic>i</italic></sub> is the synaptic weight vector impinging onto that neuron. Yet, such interpretation is difficult to reconcile with known biology. First, computing squared distances in <xref ref-type="disp-formula" rid="eqn2">Eq. 2</xref> seems to require the availability of x<sub><italic>t</italic></sub> and w<sub><italic>i</italic></sub> at the same location in a neuron. Contrary to that, while <bold>x</bold><sub><italic>t</italic></sub> is available only to the upstream neurons (PNs) and their synapses, the KC soma sees only <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="226746_inline1.gif"/></alternatives></inline-formula>. This difficulty could be circumvented by expanding the square and dropping the term, ||x<sub><italic>t</italic></sub>||<sup>2</sup>, constant among all KCs. Then, the argument becomes <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="226746_inline2.gif"/></alternatives></inline-formula> which could be computed in each KC&#x2019;s soma. Second, and more serious, difficulty is to actually find the minimum, (2), over all KCs. To compare <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="226746_inline3.gif"/></alternatives></inline-formula> among KCs, the KCs would have to output that quantity, contradicting the assumption that they output <italic>y<sub>i,t</sub></italic>.</p>
<p>Whereas neural implementations of clustering have been proposed before (reviewed in [<xref ref-type="bibr" rid="c40">40</xref>]), they either lack biological plausibility or are not derived from a principled objective. The central idea behind clustering neural network is competitive learning where neurons compete to assign a datum to their associated cluster by WTA dynamics. The &#x201C;winner&#x201D; neuron&#x2019;s synaptic weights, encoding that cluster&#x2019;s centroid, are updated using a Hebbian rule. Examples of such algorithms include the classic <italic>k</italic>-means algorithm (see Section 2.3) [<xref ref-type="bibr" rid="c38">38</xref>], [<xref ref-type="bibr" rid="c39">39</xref>], the self-organizing maps (SOM) [<xref ref-type="bibr" rid="c41">41</xref>], the neural gas [<xref ref-type="bibr" rid="c42">42</xref>], the adaptive resonance theory (ART) [<xref ref-type="bibr" rid="c43">43</xref>], and the nonnegative similarity matching (NSM) network [<xref ref-type="bibr" rid="c28">28</xref>]. A probabilistic interpretation of WTA networks with Hebbian learning is given in [<xref ref-type="bibr" rid="c44">44</xref>].</p>
</sec>
</sec>
<sec id="s3"><label>III.</label><title>Our contributions</title>
<p>This paper makes four main contributions: i) Starting from the classic <italic>k</italic>-means objective function, we derive an online algorithm that can be implemented by activity dynamics and synaptic plasticity rules in a biologically plausible neural network. In contrast, ART, SOM, the neural gas, and the classic <italic>k</italic>-means algorithms all assume that WTA operation is implemented in some biologically plausible way without specifying the network architecture or dynamics [<xref ref-type="bibr" rid="c45">45</xref>]. Hebbian plasticity, but not WTA dynamics, has been related to a clustering cost function in [<xref ref-type="bibr" rid="c46">46</xref>]. ii) The derived clustering network bears a striking resemblance to the architecture and dynamics of the PN-KC-GI circuit. Specifically, it accounts for the rectification in KCs as their activity represents nonnegative attribution indices and the existence of a single interneuron as its activity represents the Lagrange multiplier arising from the norm constraint (1). iii) In contrast with existing demixing models, we interpret activity of a KC as the presence of an olfactory object, which may be a complex odor made of many components or a simple odor of a single component, consistent with experimental findings [<xref ref-type="bibr" rid="c21">21</xref>]. iv) We extend our model to the soft-clustering scenario in which there could be multiple &#x201C;winner&#x201D; neurons. Such sparse, overcomplete representation is both reminiscent of the insect mushroom body and may be optimal for learning supervised classification downstream.</p>
</sec>
<sec id="s4"><label>IV.</label><title>A Neural Online <italic>k</italic>-means Algorithm</title>
<p>In this Section, we present a new online <italic>k</italic>-means algorithm that overcomes both biological plausibility difficulties mentioned above.</p>
<sec id="s4a"><label>A.</label><title>ADerivation of a neurally inspired online algorithm</title>
<p>We start by relaxing the constraint, <italic>y<sub>it</sub></italic> &#x2208; &#x007B;0,1&#x007D;, in (1):
<disp-formula id="eqn4"><alternatives><graphic xlink:href="226746_eqn4.gif"/></alternatives></disp-formula></p>
<p>To see that, despite relaxation, the optimum of <xref ref-type="disp-formula" rid="eqn4">Eq. 4</xref> has the property <italic>y<sub>it</sub></italic> &#x2208; &#x007B;0,1&#x007D;, note that, for any value of w<sub><italic>i</italic></sub>s, the problem separates by data points. Then, for each <italic>t</italic>, we get:
<disp-formula id="eqn5"><alternatives><graphic xlink:href="226746_eqn5.gif"/></alternatives></disp-formula></p>
<p>The solution of this problem is obviously, <italic>y<sub>i&#x002A;,t</sub></italic> &#x003D; <italic>&#x03B4;<sub>i,i&#x002A;</sub></italic> where <italic>i</italic>&#x002A; &#x003D; arg min<sub><italic>i</italic></sub>||x<sub><italic>t</italic></sub> &#x2013; w<sub><italic>i</italic></sub>||<sup>2</sup>, and <italic>&#x03B4;<sub>i,i&#x002A;</sub></italic> is the Kronecker delta, i.e., the same as for (1).</p>
<p>Next, we eliminate cluster centroids, w<italic><sub>i</sub></italic>s, from the objective. By taking a derivative of <xref ref-type="disp-formula" rid="eqn4">Eq. 4</xref> with respect to wi and setting it to zero we find that the optimal cluster centroid is given by the center of mass, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="226746_inline4.gif"/></alternatives></inline-formula>. After substituting this expression into <xref ref-type="disp-formula" rid="eqn4">Eq. 4</xref>, some algebra, and using the Lagrangian form for the constraint, we obtain:
<disp-formula id="eqn6"><alternatives><graphic xlink:href="226746_eqn6.gif"/></alternatives></disp-formula></p>
<p>To derive an online algorithm, we reason that future inputs have not been seen and past outputs and Lagrange multipliers cannot be modified. Therefore, at each time step, <italic>T</italic>, we optimize <xref ref-type="disp-formula" rid="eqn6">Eq. 6</xref> only with respect to the latest outputs <italic>y<sub>T</sub></italic>: &#x003D; [<italic>y</italic><sub>1</sub>,<italic>T</italic>,&#x2026;, <italic>y<sub>k</sub>,T</italic>]<sup>&#x22A4;</sup> and the Lagrange multiplier, <italic>z<sub>T</sub></italic>. Specifically, we solve:
<disp-formula id="eqn7"><alternatives><graphic xlink:href="226746_eqn7.gif"/></alternatives></disp-formula>
where we introduced the factor <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="226746_inline5.gif"/></alternatives></inline-formula> inside the parenthesis, which will be crucial for deriving an algorithm asymptotically equivalent to <italic>k</italic>-means (see <xref ref-type="app" rid="app1">Appendix I</xref>). Next, we drop i) the negligible current values from the sums in the denominators by invoking the large-<italic>T</italic> limit, ii) <italic>y<sub>i,T</sub></italic> or <italic>z<sub>i,T</sub></italic> independent terms, and iii) the negligible terms <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="226746_inline6.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="226746_inline7.gif"/></alternatives></inline-formula> by again invoking the large-<italic>T</italic> limit. Finally, we arrive at:
<disp-formula id="eqn8"><alternatives><graphic xlink:href="226746_eqn8.gif"/></alternatives></disp-formula>
where
<disp-formula id="eqn9"><alternatives><graphic xlink:href="226746_eqn9.gif"/></alternatives></disp-formula></p>
<p>To guarantee invertibility of <bold>N</bold><sub><italic>T</italic></sub> we initialize it with an identity matrix. We use a Matlab-inspired notation where, for any matrix <bold>M</bold>, diag(<bold>M</bold>) is a vector equal to <bold>M</bold>&#x2019;s main diagonal and, for any vector <bold>v</bold>, diag(<bold>v</bold>) is a diagonal matrix with <bold>v</bold> on the main diagonal.</p>
<p>For each datum, our neurally-inspired Algorithm 2 performs two steps: i) Optimizes (8), ii) Recursively updates <bold>N</bold><sub><italic>T</italic></sub>, <bold>W</bold><sub><italic>T</italic></sub> and <bold><italic>&#x03B8;</italic></bold><sub><italic>T</italic></sub> (<xref ref-type="disp-formula" rid="eqn11">Eq. 11</xref> below) using optimal <bold>y</bold><sub><italic>T</italic></sub> and <italic>z<sub>T</sub></italic>. Because the optimal <italic>y<sub>i,T</sub></italic> &#x2208; &#x007B;0,1&#x007D;, diag(<bold>N</bold><sub><italic>T</italic></sub>) &#x003D; <bold>n</bold><sub><italic>T</italic></sub> counts data points assigned to clusters until time, <italic>T</italic>.
<statement><label>Algorithm 2</label><title>Neural algorithm for <italic>k</italic>-means clustering</title>
<p><list list-type="order">
<list-item><p>Initialize <bold>W</bold><sub>1</sub>, <bold><italic>&#x03B8;</italic></bold><sub>1</sub> and <bold>n</bold><sub>1</sub> &#x003D; 1.</p></list-item>
<list-item><p>Repeat for each <italic>T</italic> &#x003D; 1,&#x2026;</p></list-item>
<list-item><p>Set
<disp-formula id="eqn10"><alternatives><graphic xlink:href="226746_eqn10.gif"/></alternatives></disp-formula>
where a<sub><italic>T</italic></sub> &#x003D; &#x2212;<bold>W</bold><sub><italic>Tx<sub>T</sub></italic></sub> &#x002B; <bold><italic>&#x03B8;</italic></bold><sub><italic>T</italic></sub>.</p></list-item>
<list-item><p>Update
<disp-formula id="eqn11"><alternatives><graphic xlink:href="226746_eqn11.gif"/></alternatives></disp-formula></p></list-item>
</list></p>
</statement></p>
<p>Step 3 of Algorithm 2, i.e. Eq.(8), is the Lagrange form (with <italic>z<sub>T</sub></italic> as the Lagrange multiplier) of the following optimization problem:
<disp-formula id="eqn12"><alternatives><graphic xlink:href="226746_eqn12.gif"/></alternatives></disp-formula></p>
<p>As Eq.(12) is a linear program on the simplex, it is optimized at the vertex with the smallest inner product value. Let <italic>c</italic> &#x003D; arg min<sub><italic>i</italic></sub> <italic>a<sub>i,T</sub></italic>, then <italic>y<sub>i,T</sub></italic> &#x003D; <italic>&#x03B4;<sub>i,c</sub></italic>, recovering the WTA rule.</p>
<p>To find the value of <italic>z<sub>T</sub></italic>, we write the unconstrained Lagrangian: <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="226746_inline8.gif"/></alternatives></inline-formula>. From complementary slackness, &#x03BB;<sub><italic>c</italic></sub> &#x003D; 0 and, from stationarity, a<sub><italic>T</italic></sub> &#x002B; <italic>z<sub>T</sub></italic> <bold>1</bold> &#x002B; <bold>&#x03BB;</bold> &#x003D; 0 [<xref ref-type="bibr" rid="c47">47</xref>]. Therefore, <italic>z<sub>T</sub></italic> &#x003D; &#x2212;<italic>a<sub>c,T</sub></italic> and we arrive at the following simplification:
<disp-formula id="eqn13"><alternatives><graphic xlink:href="226746_eqn13.gif"/></alternatives></disp-formula></p>
<p>Comparing Algorithm 2 with classic <italic>k</italic>-means (Algorithm 1) we note several similarities. Cluster centroids (rows of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="226746_inline9.gif"/></alternatives></inline-formula>) are centers of mass of the data points assigned to the corresponding clusters. Their update, (11), is identical to that in the classic online <italic>k</italic>-means algorithm (3). Both algorithms employ WTA dynamics.</p>
<p>Algorithm 2 differs from Algorithm 1 in that the &#x201C;winner&#x201D; is not determined by the Euclidean distance from cluster centroid to data point but by solving a linear program <xref ref-type="disp-formula" rid="eqn12">Eq. 12</xref>. Yet, despite this difference, Algorithm 2 is asymptotically equivalent to Algorithm 1 because <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="226746_inline10.gif"/></alternatives></inline-formula> (see <xref ref-type="app" rid="app1">Appendix I</xref>). This solves the first difficulty with implementing <italic>k</italic>-means using neural networks.</p>
<p>Whereas Algorithm 2 with (13) gets us closer to a biological implementation by dropping the requirement for the availability of the terms ||<bold>x</bold><sub><italic>t</italic></sub>||<sup>2</sup> and ||<bold>w</bold><sub><italic>i</italic></sub>||<sup>2</sup> at each output neuron&#x2019;s soma, it is still not clear how to implement the min operation biologically. The utility of Algorithm 2 with (13), which can be run on the CPU architecture, is due to its speed.</p>
</sec>
<sec id="s4b"><label>B.</label><title>A neural circuit implementation</title>
<p>In order to implement the online algorithm 2 by a biologically plausible neural network, we solve the augmented Lagrangian of the linear program in <xref ref-type="disp-formula" rid="eqn12">Eq. 12</xref> by primal-dual descent-ascent algorithm [<xref ref-type="bibr" rid="c47">47</xref>] (<italic>&#x03C1;</italic> is the augmented Lagrangian coefficient and <italic>&#x03B7;</italic> is the step size.):
<disp-formula id="eqn14"><alternatives><graphic xlink:href="226746_eqn14.gif"/></alternatives></disp-formula></p>
<p>Using the dynamics in (14), Algorithm 2 has a biologically plausible neural network interpretation, <xref ref-type="fig" rid="fig1">Fig. 1</xref>, bottom. Each principal neuron (KC) receives input projected onto a synaptic weight vector (rows of <bold>W</bold>) representing the corresponding cluster centroid and the output activity of that neuron, <italic>y<sub>i,T</sub></italic>, represents the corresponding cluster attribution index. <italic>&#x03B8;<sub>i</sub></italic>s are neural firing thresholds. The activity of a single inhibitory interneuron (GI) reciprocally connected with principal neurons represents the Lagrange multiplier, <italic>z<sub>T</sub></italic>. The <bold>W</bold> update in <xref ref-type="disp-formula" rid="eqn11">Eq. 11</xref> is a local Hebbian synaptic plasticity rule. The update of threshold, <bold><italic>&#x03B8;</italic></bold>, is carried out by homeostatic plasticity. Finally, the augmented Lagrangian term may be implemented by the interaction via the common extracellular space shared by multiple principal neurons (KCs) [<xref ref-type="bibr" rid="c48">48</xref>], [<xref ref-type="bibr" rid="c49">49</xref>].</p>
</sec>
<sec id="s4c"><label>C.</label><title>Numerical experiments</title>
<p>To evaluate the neural Algorithm 2 we compared it with the classic Algorithm 1 on the famous Fisher&#x2019;s Iris dataset [<xref ref-type="bibr" rid="c50">50</xref>]. This dataset contains 150 samples equally representing three species of Iris: setosa, virginica, and versicolor. Each sample has four measurements: sepal length, sepal width, petals length, petal width (all in centimeters). We initialized the algorithms with three random points from a normal distribution with the mean and standard deviations of the whole data set, with independent coordinates. To emulate online behavior, the samples were permuted randomly. The same initial points and permutation were used for both algorithms.</p>
<p>The two algorithms perform similarly according to two indicators. <xref ref-type="table" rid="tbl1">Table I</xref> presents confusion matrices and Rand index values for the classic and neural algorithms, mean and standard deviation over 10 repetitions. <xref ref-type="fig" rid="fig2">Fig. 2</xref>, top, shows traces of cluster centroids evolution along the online execution of the two algorithms.</p>
<table-wrap id="tbl1" orientation="portrait" position="float"><label>TABLE 1</label>
<caption><p>Confusion matrices on Fisher&#x2019;s Iris dataset.</p></caption>
<graphic xlink:href="226746_tbl1.tif"/>
</table-wrap>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure"><label>Fig. 2.</label>
<caption><p>Performance of the neural Algorithm 2 on Fisher&#x2019;s Iris data. Top: Trajectories of the three cluster centroids traced during the execution of the classic Algorithm 1 (blue) and the neural Algorithm 2 (red) initialized identically (solid black circles). In one case the traces almost coincide, in two other cases they deviate but end up in the similar positions. Shown is a projection of a 4D space onto 2D. Bottom: Neural dynamics during the execution of step 3 with <xref ref-type="disp-formula" rid="eqn14">Eq. 14</xref>, Algorithm 2, for one datum.</p></caption>
<graphic xlink:href="226746_fig2.tif"/>
</fig>
<p><xref ref-type="fig" rid="fig2">Fig. 2</xref>, bottom, gives an example of internal dynamics of the neural Algorithm 2, step 3 with <xref ref-type="disp-formula" rid="eqn14">Eq. 14</xref>, for one datum.</p>
</sec>
</sec>
<sec id="s5"><label>V.</label><title>A Neural Soft <italic>k</italic>-means Algorithm</title>
<p>Whereas our model explains the existence of a single inhibitory interneuron, it does not account for the observed activity of KCs. Indeed, in <italic>k</italic>-means clustering only one attribution index is non-zero for every datum implying that only one KC is active for each odor presentation. In contrast, experiments suggest that about 5&#x2013;10&#x0025; of KCs are active in the same time window [<xref ref-type="bibr" rid="c8">8</xref>].</p>
<sec id="s5a"><label>A.</label><title>Soft-clustering online algorithm and neural network</title>
<p>Our framework can explain this observation if, instead of hard clustering, we consider soft clustering where a datum may have multiple non-zero attribution indices. One may think that this problem can be solved by using the fuzzy <italic>k</italic>-means algorithm [<xref ref-type="bibr" rid="c52">52</xref>], where the cost function of <xref ref-type="disp-formula" rid="eqn4">Eq. 4</xref> is generalized to
<disp-formula id="eqn15"><alternatives><graphic xlink:href="226746_eqn15.gif"/></alternatives></disp-formula>
where <italic>m</italic> &#x2265; 1 (<italic>m</italic> &#x003D; 1 limit recovers the hard <italic>k</italic>-means cost). However, fuzzy <italic>k</italic>-means does not produce sparse output, see <xref ref-type="app" rid="app2">Appendix II</xref>, in contradiction with experimental observations [<xref ref-type="bibr" rid="c6">6</xref>], [<xref ref-type="bibr" rid="c7">7</xref>], [<xref ref-type="bibr" rid="c8">8</xref>].</p>
<p>Therefore, we propose an alternative formulation which yields soft and sparse clustering and can be related to a semidefinite program (SDP) previously considered for clustering [<xref ref-type="bibr" rid="c53">53</xref>], [<xref ref-type="bibr" rid="c54">54</xref>] and manifold learning [<xref ref-type="bibr" rid="c55">55</xref>]. To this end, we add to the <italic>k</italic>-means cost function an <italic>l</italic><sub>2</sub>-norm penalty:
<disp-formula id="eqn16"><alternatives><graphic xlink:href="226746_eqn16.gif"/></alternatives></disp-formula>
where the additional term in the objective is the only difference from (4), and <italic>&#x03B1;</italic> &#x003E; 0 is a penalty parameter.</p>
<p>Following the steps in <xref ref-type="sec" rid="s4a">Section IV-A</xref>, we arrive at the online optimization problem for every time step:
<disp-formula id="eqn17"><alternatives><graphic xlink:href="226746_eqn17.gif"/></alternatives></disp-formula></p>
<p>Solving optimization problem 17 and updating variables <bold>W</bold>, <bold><italic>&#x03B8;</italic></bold> and <bold>N</bold> recursively, we arrive at the online neural Algorithm 3.
<statement><label>Algorithm 3</label><title>Neural algorithm for soft <italic>k</italic>-means clustering</title>
<p><list list-type="order">
<list-item><p>Initialize <bold>W</bold><sub>1</sub>, <bold><italic>&#x03B8;</italic></bold><sub>1</sub> and <bold>n</bold><sub>1</sub> &#x003D; 1 (<bold>N</bold><sub><italic>T</italic></sub> &#x003D; diag(<bold>n</bold><sub><italic>T</italic></sub>)).</p></list-item>
<list-item><p>Repeat for each <italic>T</italic> &#x003D; 1,&#x2026;</p></list-item>
<list-item><p>Set
<disp-formula id="eqn18"><alternatives><graphic xlink:href="226746_eqn18.gif"/></alternatives></disp-formula>
where a<sub><italic>T</italic></sub> &#x003D; &#x2212;<bold>W</bold><sub><italic>Tx<sub>T</sub></italic></sub> &#x002B; <bold><italic>&#x03B8;</italic></bold><sub><italic>T</italic></sub>.</p></list-item>
<list-item><p>Update
<disp-formula id="eqn19"><alternatives><graphic xlink:href="226746_eqn19.gif"/></alternatives></disp-formula></p></list-item>
</list></p>
</statement></p>
<p>Unlike <xref ref-type="disp-formula" rid="eqn8">Eq. 8</xref>, <xref ref-type="disp-formula" rid="eqn17">Eq. 17</xref> is the Lagrangian form of a quadratic problem with linear constraints (with <italic>z<sub>T</sub></italic> being the Lagrange multiplier):
<disp-formula id="eqn20"><alternatives><graphic xlink:href="226746_eqn20.gif"/></alternatives></disp-formula></p>
<p>Step 3 of Algorithm 3 can be carried out by any quadratic program solver. To see why optimal solutions to (20) are sparse, note that for every <italic>y<sub>i,T</sub></italic> &#x003E; 0 the Lagrange multiplier for the non-negativity constraint is zero, so that the stationary condition takes the form:
<disp-formula id="eqn21"><alternatives><graphic xlink:href="226746_eqn21.gif"/></alternatives></disp-formula>
from which we have: <italic>y<sub>i,T</sub></italic> &#x003D; (&#x2212;<italic>z<sub>T</sub></italic> &#x2013; <italic>a<sub>i,T</sub></italic>)<italic>n<sub>i,T</sub></italic>/2<italic>&#x03B1;T</italic>. If all <italic>y<sub>i,T</sub></italic> &#x003E; 0, then using the equality constraint we can define: <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="226746_inline11.gif"/></alternatives></inline-formula>. Using positivity of <italic>y<sub>i,T</sub></italic> we have <italic>z<sub>T</sub></italic> &#x003C; &#x2212;<italic>a<sub>i,T</sub></italic>, or
<disp-formula id="eqn22"><alternatives><graphic xlink:href="226746_eqn22.gif"/></alternatives></disp-formula></p>
<p>This inequality contains input variables only and cannot always be guaranteed to hold, so, in those cases, <italic>y<sub>i,T</sub></italic> &#x003D; 0 for some <italic>i</italic>.</p>
<p>In a biologically plausible neural circuit, the quadratic program, <xref ref-type="disp-formula" rid="eqn20">Eq. 20</xref>, in the augmented Lagrangian [<xref ref-type="bibr" rid="c47">47</xref>] form can be solved using primal-dual descent-ascent algorithm (<italic>&#x03C1;</italic> - the augmented Lagrangian coefficient and <italic>&#x03B7;</italic> - the step size):
<disp-formula id="eqn23"><alternatives><graphic xlink:href="226746_eqn23.gif"/></alternatives></disp-formula></p>
<p>The only difference with the hard-clustering case, Eq.(14), is that the principal neurons are now leaky integrators with adaptive time constant, <italic>&#x03B1;T</italic>/<italic>n<sub>i,T</sub></italic>.</p>
</sec>
<sec id="s5b"><label>B.</label><title>Numerical experiments</title>
<p>We tested the utility of the representation learned by Algorithm 3 on the MNIST handwritten digits dataset by running a supervised classification algorithm on the vectors of attribution indices. The experiment was performed on a small-scale dataset (5,000 images were used in unsupervised training, last 1,000 images from unsupervised were used in supervised training, and another 1,000 images were used in testing) and on a large-scale dataset (60,000, 10,000, and 10,000 images, correspondingly). The results were averaged over 10 experiments with different random initializations.</p>
<p>After cluster centroids were learned by Algorithm 3 we classified the vectors of attribution indices using two versions of supervised learning that yielded similar results. The first was multiclass linear SVM, <xref ref-type="fig" rid="fig3">Figure 3</xref>. The second - directly connected each cluster to the most fitting target class (results not shown). Specifically, we defined <italic>T<sub>d</sub></italic> as the set of all <italic>t</italic> values where the true digit is <italic>d</italic>. Then for each cluster i we assigned the best matching digit label by taking arg max<sub><italic>d</italic></sub> &#x03A3;<sub><italic>t</italic>&#x2208;<italic>Td</italic></sub> <italic>y<sub>it</sub></italic>. Then, we classified each image according to the digit which had the maximum sum of attribution indices.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure"><label>Fig. 3.</label>
<caption><p>Performance of the soft clustering Algorithm 3 on the MNIST hand-written digits data set for different <italic>&#x03B1;</italic> and number of clusters (special case, <italic>&#x03B1;</italic> &#x003D; 0, corresponds to the hard-clustering Algorithm 2). Top: Accuracy of supervised digit classification based on attribution indices. Bottom: Sparseness of the attribution indices (average fraction of non-zero indices per datum).</p></caption>
<graphic xlink:href="226746_fig3.tif"/>
</fig>
<p>We found that larger numbers of KCs led to better performance, <xref ref-type="fig" rid="fig3">Figure 3</xref>, top. Also, <xref ref-type="fig" rid="fig3">Figure 3</xref>, top, shows that nonzero values of the penalty parameter, <italic>&#x03B1;</italic>, resulted in better performance on small-scale (left) but not large-scale (right) datasets. <xref ref-type="fig" rid="fig3">Figure 3</xref>, bottom, shows that greater values of the penalty parameter, <italic>&#x03B1;</italic>, resulted in more active channels. This is consistent with viewing <italic>&#x03B1;</italic> as a regularization coefficient.</p>
</sec>
</sec>
<sec id="s6"><label>VI.</label><title>Discussion</title>
<p>The proposed clustering neural network has a striking resemblance to the PN-KC-GI circuit in the insect olfactory system, <xref ref-type="fig" rid="fig1">Figure 1</xref>. We reproduced the general architecture including the divergence in the PN-KC circuit and, for the first time, derived the existence of a single inhibitory interneuron from a computational objective. The circuit dynamics in the model requires rectification in KCs, Eqs. (<xref ref-type="disp-formula" rid="eqn14">14</xref>,<xref ref-type="disp-formula" rid="eqn23">23</xref>), which occurs naturally in spiking neurons. The model does not require rectification in the inhibitory interneuron, (14,23), in agreement with the lack of spiking in giant interneurons [<xref ref-type="bibr" rid="c2">2</xref>].</p>
<p>According to our model, PN-KC synapses should obey Hebbian plasticity rules, Eqs. (<xref ref-type="disp-formula" rid="eqn11">11</xref>,<xref ref-type="disp-formula" rid="eqn19">19</xref>). KC-GI reciprocal synapses do not require plasticity, Eqs. (<xref ref-type="disp-formula" rid="eqn11">11</xref>,<xref ref-type="disp-formula" rid="eqn19">19</xref>). We predict interactions among KCs, Eqs. (<xref ref-type="disp-formula" rid="eqn14">14</xref>,<xref ref-type="disp-formula" rid="eqn23">23</xref>), that could be electrically mediated via shared extracellular space [<xref ref-type="bibr" rid="c48">48</xref>], [<xref ref-type="bibr" rid="c49">49</xref>].</p>
<p>In contrast to demixing models in which KCs signal the presence of constituent molecules in a complex odor, our model suggests that KCs represent olfactory objects which could be complex or simple odors. Our model predicts the existence of KCs selective to odor mixtures but not to the components alone. While preliminary evidence of such KC responses exists [<xref ref-type="bibr" rid="c21">21</xref>], they need to be investigated further.</p>
<p>Our clustering network has an advantage compared to existing random projection models [<xref ref-type="bibr" rid="c13">13</xref>], [<xref ref-type="bibr" rid="c17">17</xref>], [<xref ref-type="bibr" rid="c15">15</xref>], [<xref ref-type="bibr" rid="c18">18</xref>] in that it requires fewer KCs to represent olfactory stimuli. To span the PN activity vector space, random projections would require the number of KCs exponential in the number of dimensions. In contrast, our network is capable of reducing the required number of KCs by learning to represent only those parts of the PN activity space that are populated by data. In addition to minimizing volume [<xref ref-type="bibr" rid="c56">56</xref>] and energy [<xref ref-type="bibr" rid="c57">57</xref>] costs, reducing the number of KCs facilitates supervised learning.</p>
<p>In our model, nonnegativity of KC activity arises from the natural property of attribution indices (a datum cannot belong to a cluster with a negative attribution index). In contrast to the physical nonnegativity explanation of demixing models applicable only to neurons that code for molecular concentrations such as KCs, our interpretation can be applied, in addition, to most other spiking neurons encountered throughout the brain. Therefore, our interpretation is more general.</p>
</sec>
</body><back><app-group>
<app id="app1"><label>Appendix I</label><title>Neural clustering algorithm is asymptotically Equivalent to Classic <italic>k</italic>-means</title>
<p>Here, we demonstrate that Algorithm 2 asymptotically approaches classic <italic>k</italic>-means. To see this, let us focus on one cluster <italic>i</italic> and denote <italic>t</italic><sub>1</sub>, <italic>t</italic><sub>2</sub>,&#x2026;the sequence of all time indices such that <italic>y<sub>i,t<sub>j</sub></sub></italic> &#x003D; 1. From <xref ref-type="disp-formula" rid="eqn9">Eq. 9</xref> we have:
<disp-formula id="eqn24"><alternatives><graphic xlink:href="226746_eqn24.gif"/></alternatives></disp-formula>
where we substituted the value of Lagrange multiplier as derived in the main text. Also, we denoted by <bold>w</bold><italic><sub>i,t<sub>j</sub></sub></italic> the centroid of the <italic>i</italic>-th cluster, noting that the <italic>i</italic>-th row of matrix <bold>W</bold><italic><sub>t<sub>j</sub></sub></italic> is double that vector. From the recursive relation between <italic>a<sub>i,t<sub>j</sub></sub></italic> and <italic>&#x03B8;<sub>i,t<sub>j</sub></sub></italic> we can infer:
<disp-formula id="eqn25"><alternatives><graphic xlink:href="226746_eqn25.gif"/></alternatives></disp-formula></p>
<p>As the matrix, <bold>W</bold>, stabilizes with time, we make an approximation and replace the running cluster centroids with the latest value, and then take the expectation:
<disp-formula id="eqn26"><alternatives><graphic xlink:href="226746_eqn26.gif"/></alternatives></disp-formula>
where for the last step we recall that vectors <bold>x</bold><sub><italic>th</italic></sub> are all and only those that constitute <italic>i</italic>-th cluster. Then
<disp-formula id="eqn27"><alternatives><graphic xlink:href="226746_eqn27.gif"/></alternatives></disp-formula></p>
<p>Therefore, asymptotically, the linear program solution of WTA in Algorithm 2 assigns each datum to the nearest cluster, just like <italic>k</italic>-means would, without requiring neurons to output Euclidean distances. Since the last term in (27) does not depend on i, the optimum in (6) only depends on distances between data points and, therefore, just like <italic>k</italic>-means, exhibits translational invariance (approximately), i.e. attribution indices depend only on the distance between data points, not on their absolute coordinates. In experiments, we observe the approximation to work well, so that <italic>&#x03B8;<sub>i,t<sub>j</sub></sub></italic> quickly approaches and, then, follows <bold>w</bold><italic><sub>i,t<sub>j</sub></sub></italic> closely.</p>
</app>
<app id="app2"><label>Appendix II</label><title>Fuzzy <italic>k</italic>-means produces dense output</title>
<p>To see why fuzzy <italic>k</italic>-means does not produce sparse output, write the Lagrangian for seeking <italic>y<sub>i,t</sub></italic> under fixed <bold>w</bold><sub><italic>i</italic></sub>:
<disp-formula id="eqn28"><alternatives><graphic xlink:href="226746_eqn28.gif"/></alternatives></disp-formula></p>
<p>We write minus in front of &#x03BB; because, since the objective is increasing, the values of <italic>y<sub>i,t</sub></italic> need to be constrained only from below. Stationarity condition gives:
<disp-formula id="eqn29"><alternatives><graphic xlink:href="226746_eqn29.gif"/></alternatives></disp-formula></p>
<p>If <italic>y<sub>k,t</sub></italic> &#x003D; 0, then stationarity becomes &#x2212;&#x03BB; &#x003D; <italic>&#x03BC;<sub>k</sub></italic>, and due to non-negativity of both variables: <italic>&#x03BC;<sub>k</sub></italic> &#x003D; &#x03BB; &#x003D; 0. Then for all <italic>i</italic> &#x2260; <italic>k</italic> stationarity becomes <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="226746_inline12.gif"/></alternatives></inline-formula>, which, using complementary slackness, leads to all <italic>y<sub>i,t</sub></italic> &#x003D; 0 in violation of constraint unless for some <italic>j</italic> it happens that ||<bold>x</bold><sub><italic>t</italic></sub> &#x2013; <bold>w</bold><sub><italic>j</italic></sub>|| &#x003D; 0. More generally, using some other than degree function <italic>f</italic>(<italic>y<sub>i,t</sub></italic>) in the objective, a necessary condition for sparsity is <italic>f</italic>&#x2032;(0) &#x003E; 0.</p>
</app>
<app id="app3"><label>Appendix III</label><title>Relation of <italic>l</italic><sub>2</sub>-Regularized <italic>k</italic>-means to an SDP for Clustering and Manifold Learning</title>
<p>Here we show that the SDP used for clustering [<xref ref-type="bibr" rid="c53">53</xref>], [<xref ref-type="bibr" rid="c54">54</xref>] and manifold learning [<xref ref-type="bibr" rid="c55">55</xref>] is a convex relaxation of the <italic>l</italic><sub>2</sub>-regularized <italic>k</italic>-means cost function (16).</p>
<p>To see this connection, we rewrite the optimization problem (16) in terms of new variables. Plugging in for optimal <bold>w</bold><sub><italic>i</italic></sub> &#x003D; &#x03A3;<sub><italic>t</italic></sub> <italic>y<sub>i,t</sub></italic> <bold>x</bold><sub><italic>t</italic></sub>/&#x03A3;<sub><italic>t</italic></sub> <italic>y<sub>i,t</sub></italic>, and defining new variables
<disp-formula id="eqn30"><alternatives><graphic xlink:href="226746_eqn30.gif"/></alternatives></disp-formula>
the cost (16) becomes &#x2013;Tr (<bold>DQ</bold>)&#x002B;<italic>&#x03B1;T</italic> Tr <bold>Q</bold>. By construction, <italic>Q</italic>1 &#x003D; 1 and <bold>Q</bold> &#x2265; 0. Therefore, we can relax the objective (16) to an SDP, where optimization is with respect to the positive semidefinite matrix <bold>Q</bold>:
<disp-formula id="eqn31"><alternatives><graphic xlink:href="226746_eqn31.gif"/></alternatives></disp-formula></p>
<p>The SDP of [<xref ref-type="bibr" rid="c53">53</xref>], [<xref ref-type="bibr" rid="c54">54</xref>], [<xref ref-type="bibr" rid="c55">55</xref>] is
<disp-formula id="eqn32"><alternatives><graphic xlink:href="226746_eqn32.gif"/></alternatives></disp-formula>
from which (31) can be recovered by replacing the last constraint by a penalty term in the cost.</p>
</app>
</app-group>
<ack><title>Acknowledgements</title>
<p>We thank Anirvan Sengupta, Mariano Tepper and Evgeniy Bauman for helpful discussions.</p>
</ack>
<ref-list><title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><given-names>N. Y.</given-names> <surname>Masse</surname></string-name>, <string-name><given-names>G. C.</given-names> <surname>Turner</surname></string-name>, and <string-name><given-names>G. S.</given-names> <surname>Jefferis</surname></string-name>, &#x201C;<article-title>Olfactory information processing in drosophila</article-title>,&#x201D; <source>Current Biology</source>, vol. <volume>19</volume>, no. <issue>16</issue>, pp. <fpage>R700</fpage>&#x2013;<lpage>R713</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Papadopoulou</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Cassenaer</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Nowotny</surname></string-name>, and <string-name><given-names>G.</given-names> <surname>Laurent</surname></string-name>, &#x201C;<article-title>Normalization for sparse encoding of odors by a wide-field interneuron</article-title>,&#x201D; <source>Science</source>, vol. <volume>332</volume>, no. <issue>6030</issue>, pp. <fpage>721</fpage>&#x2013;<lpage>725</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Leitch</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Laurent</surname></string-name> <etal>et al.</etal>, &#x201C;<article-title>Gabaergic synapses in the antennal lobe and mushroom body of the locust olfactory system</article-title>,&#x201D; <source>J Comp Neurol</source>, vol. <volume>372</volume>, no. <issue>4</issue>, pp. <fpage>487</fpage>&#x2013;<lpage>514</lpage>, <year>1996</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><string-name><given-names>X.</given-names> <surname>Liu</surname></string-name> and <string-name><given-names>R. L.</given-names> <surname>Davis</surname></string-name>, &#x201C;<article-title>The gabaergic anterior paired lateral neuron suppresses and is suppressed by olfactory learning</article-title>,&#x201D; <source>Nat Neurosci</source>, vol. <volume>12</volume>, no. <issue>1</issue>, pp. <fpage>53</fpage>&#x2013;<lpage>59</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><string-name><given-names>A. C.</given-names> <surname>Lin</surname></string-name>, <string-name><given-names>A. M.</given-names> <surname>Bygrave</surname></string-name>, <string-name><given-names>A.</given-names> <surname>De Calignon</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Lee</surname></string-name>, and <string-name><given-names>G.</given-names> <surname>Miesenb&#x00F6;ck</surname></string-name>, &#x201C;<article-title>Sparse, decorrelated odor coding in the mushroom body enhances learned odor discrimination</article-title>,&#x201D; <source>Nat Neurosci</source>, vol. <volume>17</volume>(<issue>4</issue>), p. <fpage>559</fpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Perez-Orive</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Mazor</surname></string-name>, <string-name><given-names>G. C.</given-names> <surname>Turner</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Cassenaer</surname></string-name>, <string-name><given-names>R. I.</given-names> <surname>Wilson</surname></string-name>, and <string-name><given-names>G.</given-names> <surname>Laurent</surname></string-name>, &#x201C;<article-title>Oscillations and sparsening of odor representations in the mushroom body</article-title>,&#x201D; <source>Science</source>, vol. <volume>297</volume>, no. <issue>5580</issue>, pp. <fpage>359</fpage>&#x2013;<lpage>365</lpage>, <year>2002</year>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="journal"><string-name><given-names>G. C.</given-names> <surname>Turner</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Bazhenov</surname></string-name>, and <string-name><given-names>G.</given-names> <surname>Laurent</surname></string-name>, &#x201C;<article-title>Olfactory representations by drosophila mushroom body neurons</article-title>,&#x201D; <source>J Neurophysiol</source>, vol. <volume>99</volume>, no. <issue>2</issue>, pp. <fpage>734</fpage>&#x2013;<lpage>746</lpage>, <year>2008</year>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><string-name><given-names>K. S.</given-names> <surname>Honegger</surname></string-name>, <string-name><given-names>R. A.</given-names> <surname>Campbell</surname></string-name>, and <string-name><given-names>G. C.</given-names> <surname>Turner</surname></string-name>, &#x201C;<article-title>Cellular-resolution population imaging reveals robust sparse coding in the drosophila mushroom body</article-title>,&#x201D; <source>Journal of Neuroscience</source>, vol. <volume>31</volume>, no. <issue>33</issue>, pp. <fpage>11 772</fpage>&#x2013;<lpage>11 785</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Aso</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Sitaraman</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Ichinose</surname></string-name>, <string-name><given-names>K. R.</given-names> <surname>Kaun</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Vogt</surname></string-name>, <string-name><given-names>G.</given-names> <surname>BelliartGu&#x00E9;rin</surname></string-name>, <string-name><given-names>P.-Y.</given-names> <surname>Pla&#x00E7;ais</surname></string-name>, <string-name><given-names>A. A.</given-names> <surname>Robie</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Yamagata</surname></string-name> <etal>et al.</etal>, &#x201C;<article-title>Mushroom body output neurons encode valence and guide memory-based action selection in drosophila</article-title>,&#x201D; <source>Elife</source>, vol. <volume>3</volume>, p. <fpage>e04580</fpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><string-name><given-names>H.</given-names> <surname>Demmer</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Kloppenburg</surname></string-name>, &#x201C;<article-title>Intrinsic membrane properties and inhibitory synaptic input of kenyon cells as mechanisms for sparse coding?</article-title>&#x201D; <source>J Neurophysiol</source>, vol. <volume>102</volume>, no. <issue>3</issue>, pp. <fpage>1538</fpage>&#x2013;<lpage>1550</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><string-name><given-names>F.</given-names> <surname>Farkhooi</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Froese</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Muller</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Menzel</surname></string-name>, and <string-name><given-names>M. P.</given-names> <surname>Nawrot</surname></string-name>, &#x201C;<article-title>Cellular adaptation facilitates sparse and reliable coding in sensory pathways</article-title>,&#x201D; <source>PLoS Comput Biol</source>, vol. <volume>9</volume>, no. <issue>10</issue>, p. <fpage>e1003251</fpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Kee</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Sanda</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Gupta</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Stopfer</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Bazhenov</surname></string-name>, &#x201C;<article-title>Feed-forward versus feedback inhibition in a basic olfactory circuit</article-title>,&#x201D; <source>PLoS Comput Biol</source>, vol. <volume>11</volume>, no. <issue>10</issue>, p. <fpage>e1004531</fpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Luo</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Axel</surname></string-name>, and <string-name><given-names>L.</given-names> <surname>Abbott</surname></string-name>, &#x201C;<article-title>Generating sparse and selective third-order responses in the olfactory system of the fly</article-title>,&#x201D; <source>PNAS</source>, vol. <volume>107</volume>, no. <issue>23</issue>, pp. <fpage>10 713</fpage>&#x2013;<lpage>10 718</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Wessnitzer</surname></string-name>, <string-name><given-names>J. M.</given-names> <surname>Young</surname></string-name>, <string-name><given-names>J. D.</given-names> <surname>Armstrong</surname></string-name>, and <string-name><given-names>B.</given-names> <surname>Webb</surname></string-name>, &#x201C;<article-title>A model of non-elemental olfactory learning in drosophila</article-title>,&#x201D; <source>J Comput Neurosci</source>, vol. <volume>32</volume>, no. <issue>2</issue>, pp. <fpage>197</fpage>&#x2013;<lpage>212</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Litwin-Kumar</surname></string-name>, <string-name><given-names>K. D.</given-names> <surname>Harris</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Axel</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Sompolinsky</surname></string-name>, and <string-name><given-names>L.</given-names> <surname>Abbott</surname></string-name>, &#x201C;<article-title>Optimal degrees of synaptic connectivity</article-title>,&#x201D; <source>Neuron</source>, vol. <volume>93</volume>, no. <issue>5</issue>, pp. <fpage>1153</fpage>&#x2013;<lpage>1164</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><string-name><given-names>F.</given-names> <surname>Peng</surname></string-name> and <string-name><given-names>L.</given-names> <surname>Chittka</surname></string-name>, &#x201C;<article-title>A simple computational model of the bee mushroom body can explain seemingly complex forms of olfactory learning and memory</article-title>,&#x201D;<source>Curr Biol</source>, <year>2016</year>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><string-name><given-names>K.</given-names> <surname>Krishnamurthy</surname></string-name>, <string-name><given-names>A. M.</given-names> <surname>Hermundstad</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Mora</surname></string-name>, <string-name><given-names>A. M.</given-names> <surname>Walczak</surname></string-name>, and <string-name><given-names>V.</given-names> <surname>Balasubramanian</surname></string-name>, &#x201C;<article-title>Disorder and the neural representation of complex odors: smelling in the real world</article-title>,&#x201D; <source>bioRxiv</source>, p. <fpage>160382</fpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Dasgupta</surname></string-name>, <string-name><given-names>C. F.</given-names> <surname>Stevens</surname></string-name>, and <string-name><given-names>S.</given-names> <surname>Navlakha</surname></string-name>, &#x201C;<article-title>A neural algorithm for a fundamental computing problem</article-title>,&#x201D; <source>Science</source>, vol. <volume>358</volume>, no. <issue>6364</issue>, pp. <fpage>793</fpage>&#x2013;<lpage>796</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Pehlevan</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Hu</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Chklovskii</surname></string-name>, &#x201C;<article-title>A hebbian/anti-hebbian neural network for linear subspace learning: A derivation from multidimensional scaling of streaming data</article-title>,&#x201D; <source>Neural Comput</source>, vol. <volume>27</volume>, pp. <fpage>1461</fpage>&#x2013;<lpage>1495</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><string-name><given-names>A. J.</given-names> <surname>Bell</surname></string-name> and <string-name><given-names>T. J.</given-names> <surname>Sejnowski</surname></string-name>, &#x201C;<article-title>An information-maximization approach to blind separation and blind deconvolution</article-title>,&#x201D; <source>Neural Comput</source>, vol. <volume>7</volume>, no. <issue>6</issue>, pp. <fpage>1129</fpage>&#x2013;<lpage>1159</lpage>, <year>1995</year>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><string-name><given-names>K.</given-names> <surname>Shen</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Tootoonian</surname></string-name>, and <string-name><given-names>G.</given-names> <surname>Laurent</surname></string-name>, &#x201C;<article-title>Encoding of mixtures in a simple olfactory system</article-title>,&#x201D; <source>Neuron</source>, vol. <volume>80</volume>, no. <issue>5</issue>, pp. <fpage>1246</fpage>&#x2013;<lpage>1262</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Grabska-Barwi&#x0144;ska</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Barthelm&#x00E9;</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Beck</surname></string-name>, <string-name><given-names>Z. F.</given-names> <surname>Mainen</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Pouget</surname></string-name>, and <string-name><given-names>P. E.</given-names> <surname>Latham</surname></string-name>, &#x201C;<article-title>A probabilistic approach to demixing odors</article-title>,&#x201D;<source>Nat Neurosci</source>, <year>2016</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Tootoonian</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Lengyel</surname></string-name>, &#x201C;<article-title>A dual algorithm for olfactory computation in the locust brain</article-title>,&#x201D; in <source>NIPS</source>, <year>2014</year>, pp. <fpage>2276</fpage>&#x2013;<lpage>2284</lpage>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Zhang</surname></string-name> and <string-name><given-names>T. O.</given-names> <surname>Sharpee</surname></string-name>, &#x201C;<article-title>A robust feedforward model of the olfactory system</article-title>,&#x201D;<source>PLoS Comput Biol</source>, vol. <volume>12</volume>, no. <issue>4</issue>, p. <fpage>e1004850</fpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Stevens</surname></string-name>, &#x201C;<article-title>What the fiy&#x2019;s nose tells the fiy&#x2019;s brain</article-title>,&#x201D; <source>PNAS</source>, vol. <volume>112</volume>, no. <issue>30</issue>, pp. <fpage>9460</fpage>&#x2013;<lpage>9465</lpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><string-name><given-names>P.</given-names> <surname>F&#x00F6;ldiak</surname></string-name>, &#x201C;<article-title>Forming sparse representations by local anti-hebbian learning</article-title>,&#x201D; <source>Biol Cyb</source>, vol. <volume>64</volume>, no. <issue>2</issue>, pp. <fpage>165</fpage>&#x2013;<lpage>170</lpage>, <year>1990</year>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Olshausen</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Field</surname></string-name>, &#x201C;<article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title>,&#x201D; <source>Nature</source>, vol. <volume>381</volume>, no. <issue>6583</issue>, pp. <fpage>607</fpage>&#x2013;<lpage>609</lpage>, <year>1996</year>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Pehlevan</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Chklovskii</surname></string-name>, &#x201C;<article-title>A hebbian/anti-hebbian network derived from online non-negative matrix factorization can cluster and discover sparse features</article-title>,&#x201D; in <source>ACSSC</source>. IEEE, <year>2014</year>, pp. <fpage>769</fpage>&#x2013;<lpage>775</lpage>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Hu</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Pehlevan</surname></string-name>, and <string-name><given-names>D. B.</given-names> <surname>Chklovskii</surname></string-name>, &#x201C;<article-title>A hebbian/anti-hebbian network for online sparse dictionary learning derived from symmetric matrix factorization</article-title>,&#x201D; in <source>ACSSC</source>. IEEE, <year>2014</year>, pp. <fpage>613</fpage>&#x2013;<lpage>619</lpage>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Olshausen</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Field</surname></string-name>, &#x201C;<article-title>Sparse coding with an overcomplete basis set: A strategy employed by v1?</article-title>&#x201D; <source>Vision Res</source>, vol. <volume>37</volume>, no. <issue>23</issue>, pp. <fpage>3311</fpage>&#x2013;<lpage>3325</lpage>, <year>1997</year>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Zylberberg</surname></string-name>, <string-name><given-names>J. T.</given-names> <surname>Murphy</surname></string-name>, and <string-name><given-names>M. R.</given-names> <surname>DeWeese</surname></string-name>, &#x201C;<article-title>A sparse coding model with synaptically local plasticity and spiking neurons can account for the diverse shapes of v1&#x2026;</article-title>&#x201D; <source>PLoS Comp Biol</source>, vol. <volume>7</volume>, no. <issue>10</issue>, p. <fpage>e1002250</fpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Koulakov</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Rinberg</surname></string-name>, &#x201C;<article-title>Sparse incomplete representations: a potential role of olfactory granule cells</article-title>,&#x201D; <source>Neuron</source>, vol. <volume>72</volume>, no. <issue>1</issue>, pp. <fpage>124</fpage>&#x2013;<lpage>136</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Druckmann</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Hu</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Chklovskii</surname></string-name>, &#x201C;<article-title>A mechanistic model of early sensory processing based on subtracting sparse representations</article-title>,&#x201D; in <source>NIPS</source>, <year>2012</year>, pp. <fpage>1979</fpage>&#x2013;<lpage>1987</lpage>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Zhu</surname></string-name> and <string-name><given-names>C.</given-names> <surname>Rozell</surname></string-name>, &#x201C;<article-title>Modeling inhibitory interneurons in efficient sensory coding models</article-title>,&#x201D;<source>PLoS Comput Biol</source>, vol. <volume>11</volume>, no. <issue>7</issue>, p. <fpage>e1004353</fpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Pehlevan</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Chklovskii</surname></string-name>, &#x201C;<article-title>A normative theory of adaptive dimensionality reduction in neural networks</article-title>,&#x201D; in <source>NIPS</source>, <year>2015</year>, pp. <fpage>2260</fpage>&#x2013;<lpage>2268</lpage>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="journal"><string-name><given-names>D.</given-names> <surname>Kepple</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Giaffar</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Rinberg</surname></string-name>, and <string-name><given-names>A.</given-names> <surname>Koulakov</surname></string-name>, &#x201C;<article-title>Deconstructing odorant identity via primacy in dual networks</article-title>,&#x201D; <source>arXiv preprint arXiv:1609.02202</source>, <year>2016</year>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Masson</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Mustaparta</surname></string-name> <etal>et al.</etal>, &#x201C;<article-title>Chemical information processing in the olfactory system of insects</article-title>.&#x201D; <source>Physiol Rev</source>, vol. <volume>70</volume>, no. <issue>1</issue>, pp. <fpage>199</fpage>&#x2013;<lpage>245</lpage>, <year>1990</year>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>MacQueen</surname></string-name> <etal>et al.</etal>, &#x201C;<article-title>Some methods for classification and analysis of multivariate observations</article-title>,&#x201D; in <source>Proc of the 5th Berkeley symposium on mathematical statistics and probability</source>, <year>1967</year>, pp. <fpage>281</fpage>&#x2013;<lpage>297</lpage>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Lloyd</surname></string-name>, &#x201C;<article-title>Least squares quantization in pcm</article-title>,&#x201D; <source>IEEE Trans Inf Theory</source>, vol. <volume>28</volume>, no. <issue>2</issue>, pp. <fpage>129</fpage>&#x2013;<lpage>137</lpage>, <year>1982</year>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="journal"><string-name><given-names>K.-L.</given-names> <surname>Du</surname></string-name>, &#x201C;<article-title>Clustering: A neural network approach</article-title>,&#x201D; <source>Neural networks</source>, vol. <volume>23</volume>, no. <issue>1</issue>, pp. <fpage>89</fpage>&#x2013;<lpage>107</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Kohonen</surname></string-name>, &#x201C;<article-title>Self-organized formation of topologically correct feature maps</article-title>,&#x201D; <source>Biol Cyb</source>, vol. <volume>43</volume>, no. <issue>1</issue>, pp. <fpage>59</fpage>&#x2013;<lpage>69</lpage>, <year>1982</year>.</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="other"><string-name><given-names>T.</given-names> <surname>Martinetz</surname></string-name> and <string-name><given-names>K.</given-names> <surname>Schulten</surname></string-name>, &#x201C;<article-title>A &#x201C;neural-gas&#x201D; network learns topologies</article-title>,&#x201D; <year>1991</year>.</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="journal"><string-name><given-names>G. A.</given-names> <surname>Carpenter</surname></string-name> and <string-name><given-names>S.</given-names> <surname>Grossberg</surname></string-name>, &#x201C;<article-title>A massively parallel architecture for a self-organizing neural pattern recognition machine</article-title>,&#x201D; <source>Comput Vision Graph</source>, vol. <volume>37</volume>, no. <issue>1</issue>, pp. <fpage>54</fpage>&#x2013;<lpage>115</lpage>, <year>1987</year>.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Keck</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Savin</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>L&#x00FC;cke</surname></string-name>, &#x201C;<article-title>Feedforward inhibition and synaptic scaling&#x2013;two sides of the same coin?</article-title>&#x201D; <source>PLoS computational biology</source>, vol. <volume>8</volume>, no. <issue>3</issue>, p. <fpage>e1002432</fpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Yuille</surname></string-name> and <string-name><given-names>D.</given-names> <surname>Geiger</surname></string-name>, &#x201C;<article-title>Winner-take-all networks</article-title>,&#x201D; <source>The handbook of brain theory and neural networks</source>, pp. <fpage>1228</fpage>&#x2013;<lpage>1231</lpage>, <year>2003</year>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="journal"><string-name><given-names>H.</given-names> <surname>Ritter</surname></string-name> and <string-name><given-names>K.</given-names> <surname>Schulten</surname></string-name>, &#x201C;<article-title>Kohonen&#x2019;s self-organizing maps: exploring their computational capabilities</article-title>,&#x201D; in <source>Proc IEEE Int Conf Neur Net</source>, vol. <volume>1</volume>, <year>1988</year>, pp. <fpage>109</fpage>&#x2013;<lpage>116</lpage>.</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="book"><string-name><given-names>S.</given-names> <surname>Boyd</surname></string-name> and <string-name><given-names>L.</given-names> <surname>Vandenberghe</surname></string-name>, <chapter-title>Convex optimization</chapter-title>. <publisher-name>Cambridge university press</publisher-name>, <year>2004</year>.</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Weckstr&#x00F6;m</surname></string-name> and <string-name><given-names>S.</given-names> <surname>Laughlin</surname></string-name>, &#x201C;<article-title>Extracellular potentials modify the transfer of information at photoreceptor output synapses in the blowfiy compound eye</article-title>,&#x201D; <source>Journal of Neuroscience</source>, vol. <volume>30</volume>, no. <issue>28</issue>, pp. <fpage>9557</fpage>&#x2013;<lpage>9566</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="journal"><string-name><given-names>W. B.</given-names> <surname>Thoreson</surname></string-name> and <string-name><given-names>S. C.</given-names> <surname>Mangel</surname></string-name>, &#x201C;<article-title>Lateral interactions in the outer retina</article-title>,&#x201D; <source>Progress in retinal and eye research</source>, vol. <volume>31</volume>, no. <issue>5</issue>, pp. <fpage>407</fpage>&#x2013;<lpage>441</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="journal"><string-name><given-names>R. A.</given-names> <surname>Fisher</surname></string-name>, &#x201C;<article-title>The use of multiple measurements in taxonomic problems</article-title>,&#x201D; <source>Annals of Eugenics</source>, vol. <volume>7</volume>, no. <issue>2</issue>, pp. <fpage>179</fpage>&#x2013;<lpage>188</lpage>, <year>1936</year>. [Online]. Available: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1111/j.1469-1809.1936.tb02137.x">http://dx.doi.org/10.1111/j.1469&#x2013;1809.1936.tb02137.x</ext-link></mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><string-name><given-names>W. M.</given-names> <surname>Rand</surname></string-name>, &#x201C;<article-title>Objective criteria for the evaluation of clustering methods</article-title>,&#x201D; <source>J Am Stat Assoc</source>, vol. <volume>336</volume>, p. <fpage>846</fpage>&#x2013;<lpage>850</lpage>, <year>1971</year>.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="journal"><string-name><given-names>J. C.</given-names> <surname>Bezdek</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Ehrlich</surname></string-name>, and <string-name><given-names>W.</given-names> <surname>Full</surname></string-name>, &#x201C;<article-title>Fcm: The fuzzy c-means clustering algorithm</article-title>,&#x201D; <source>Comput Geosci</source>, vol. <volume>10</volume>(<issue>2&#x2013;3</issue>), pp. <fpage>191</fpage>&#x2013;<lpage>203</lpage>, <year>1984</year>.</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Kulis</surname></string-name>, <string-name><given-names>A. C.</given-names> <surname>Surendran</surname></string-name>, and <string-name><given-names>J. C.</given-names> <surname>Platt</surname></string-name>, &#x201C;<article-title>Fast low-rank semidefinite programming for embedding and clustering</article-title>,&#x201D; in <source>International Conference on Artificial Intelligence and Statistics</source>, <year>2007</year>, pp. <fpage>235</fpage>&#x2013;<lpage>242</lpage>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="journal"><string-name><given-names>P.</given-names> <surname>Awasthi</surname></string-name>, <string-name><given-names>A. S.</given-names> <surname>Bandeira</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Charikar</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Krishnaswamy</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Villar</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Ward</surname></string-name>, &#x201C;<article-title>Relax, no need to round: Integrality of clustering formulations,&#x201D; in Proceedings of the 2015 Conference on Innovations in Theoretical Computer Science</article-title>. <source>ACM</source>, <year>2015</year>, pp. <fpage>191</fpage>&#x2013;<lpage>200</lpage>.</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Tepper</surname></string-name>, <string-name><given-names>A. M.</given-names> <surname>Sengupta</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Chklovskii</surname></string-name>, &#x201C;<article-title>The surprising secret identity of the semidefinite relaxation of k-means: manifold learning</article-title>,&#x201D; <source>arXiv preprint arXiv:1706.06028</source>, <year>2017</year>.</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Rivera-Alba</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Peng</surname></string-name>, <string-name><given-names>G. G.</given-names> <surname>de Polavieja</surname></string-name>, and <string-name><given-names>D. B.</given-names> <surname>Chklovskii</surname></string-name>, &#x201C;<article-title>Wiring economy can account for cell body placement across species and brain areas</article-title>,&#x201D; <source>Current Biology</source>, vol. <volume>24</volume>(<issue>3</issue>), pp. <fpage>R109</fpage>&#x2013;<lpage>R110</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="journal"><string-name><given-names>S. B.</given-names> <surname>Laughlin</surname></string-name>, &#x201C;<article-title>Energy as a constraint on the coding and processing of sensory information</article-title>,&#x201D; <source>Current opinion in neurobiology</source>, vol. <volume>11</volume>, no. <issue>4</issue>, pp. <fpage>475</fpage>&#x2013;<lpage>480</lpage>, <year>2001</year>.</mixed-citation></ref>
</ref-list>
</back>
</article>