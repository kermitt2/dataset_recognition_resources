<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en"><front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/105510</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Effective synaptic interactions in subsampled nonlinear networks with strong coupling</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Brinkman</surname>
<given-names>Braden A. W.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Rieke</surname>
<given-names>Fred</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a4">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Shea-Brown</surname>
<given-names>Eric</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Buice</surname>
<given-names>Michael</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a5">5</xref>
</contrib>
<aff id="a1">
<label>1</label><institution>Department of Applied Mathematics,University of Washington, Seattle</institution>, WA, 98195, <country>USA</country>
</aff>
<aff id="a2">
<label>2</label><institution>Department of Physiology and Biophysics, University of Washington</institution>, Seattle, WA, 98195, <country>USA</country>
</aff>
<aff id="a3">
<label>3</label><institution>Graduate Program in Neuroscience, University of Washington</institution>, Seattle, WA, 98195, <country>USA</country>
</aff>
<aff id="a4">
<label>4</label><institution>Howard Hughes Medical Institute, University of Washington</institution>, Seattle, WA, 98195, <country>USA</country>
</aff>
<aff id="a5">
<label>5</label><institution>Allen Institute for Brain Science, Seattle</institution>, WA, 98109, <country>USA</country>
</aff>
</contrib-group>
<pub-date pub-type="epub">
<year>2017</year>
</pub-date>
<elocation-id>105510</elocation-id>
<history>
<date date-type="received">
<day>02</day>
<month>2</month>
<year>2017</year>
</date>
<date date-type="accepted">
<day>03</day>
<month>2</month>
<year>2017</year>
</date>
</history><permissions><copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license></permissions>
<self-uri xlink:href="105510.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract><p>A major obstacle to understanding neural coding and computation is the fact that experimental recordings typically sample only a small fraction of the neurons in a circuit. Measured neural properties are skewed by interactions between recorded neurons and the &#x201C;hidden&#x201D; portion of the network. To properly interpret neural data, we thus need a better understanding of the relationships between measured effective neural properties and the true underlying physiological properties. Here, we focus on how the effective spatiotemporal dynamics of the synaptic interactions between neurons are reshaped by coupling to unobserved neurons. We find that the effective interactions from a pre-synaptic neuron <italic>r</italic>&#x2032; to a post-synaptic neuron <italic>r</italic> can be decomposed into a sum of the true interaction from <italic>r</italic>&#x2032; to <italic>r</italic> plus corrections from every directed path from <italic>r</italic>&#x2032; to <italic>r</italic> through unobserved neurons. Importantly, the resulting formula reveals when the hidden units have&#x2014;or do not have&#x2014;major effects on reshaping the interactions among observed neurons. As a prominent example, we derive a formula for strong impact of hidden units in random networks with connection weights that scale with 1/&#x221A;<italic>N</italic>, where <italic>N</italic> is the network size&#x2014;precisely the scaling observed in recent experiments.</p></abstract>
<counts>
<page-count count="30"/>
</counts>
</article-meta>
</front>
<body>
<p>Establishing relationships between a network&#x2019;s architecture and its function is a fundamental problem in neuroscience and network science in general. Not only is the architecture of a neural circuit intimately related to its function, but pathologies in wiring between neurons are believed to underlie diseases conditions [<xref ref-type="bibr" rid="c1">1</xref>&#x2013;<xref ref-type="bibr" rid="c15">15</xref>].</p>
<p>A major obstacle to uncovering structure-function relationships is the fact most experiments can only directly observe small fractions of an active network. The state-of-the-art methods for determining connections between neurons in living networks is to infer them by fitting statistical models to neural spiking data [<xref ref-type="bibr" rid="c16">16</xref>&#x2013;<xref ref-type="bibr" rid="c24">24</xref>]. However, the fact that we cannot observe all neurons in a network means that the statistically inferred connections are at best &#x201C;effective&#x201D; connections, representing some dynamical relationship between the activity of nodes but not necessarily a true physical connection [<xref ref-type="bibr" rid="c20">20</xref>,<xref ref-type="bibr" rid="c25">25</xref>&#x2013;<xref ref-type="bibr" rid="c30">30</xref>]. The exact relationship between the effective and true connections is in general unknown, making it difficult to extrapolate from the statistics of effective connections back to the statistics of the true connections. Establishing this relationship thus has immediate importance for interpreting experimental measurements of synaptic interactions. It also informs fundamental a question in network computation: how hidden units shape the dynamics of a subset of nodes.</p>
<p>To provide a foundation upon which we can begin to understand the relationship between true network connectivity and the effective connections between neurons, we analyze a probabilistic model of network activity in which all properties are known, and derive a novel approximation to the effective model for the network of sub-sampled observed neurons. This makes explicit how the synaptic interactions between neurons are modified by unobserved neurons in the network, and under what conditions these modifications are, and are not, significant. As an important example, we study a sparse, random (Erdo&#x02DD;s-R&#x00B4;eyni) network with N cells and with synaptic weights that scale as <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline1.gif"/></alternatives></inline-formula> [<xref ref-type="bibr" rid="c31">31</xref>&#x2013;<xref ref-type="bibr" rid="c34">34</xref>], as has been recently observed experimentally [<xref ref-type="bibr" rid="c35">35</xref>], and show how unobserved neurons significantly reshape the effective synaptic interactions away from the ground-truth interactions. This is not the case with more &#x201C;classical&#x201D; 1/<italic>N</italic> scaling. Hence, depending on the nature of the statistics of synaptic weights and how they scale with network size, hidden paths can have a major impact on the observed interactions between neurons, impacting both the local network computation and interpretation of experimental data.</p>
<p><italic>Model</italic>&#x2014;We model the full network of <italic>N</italic> neurons as a nonlinear Hawkes process [<xref ref-type="bibr" rid="c36">36</xref>]. This is commonly known as a &#x201C;Generalized linear (point process) model&#x201D; in neuroscience, and is broadly used to fit neural activity data [<xref ref-type="bibr" rid="c16">16</xref>&#x2013;<xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c21">21</xref>&#x2013;<xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c37">37</xref>]. Here we use it as a generative model for network activity, as it is a can be directly linked to common spiking models such as integrate and fire systems driven by noisy inputs [<xref ref-type="bibr" rid="c38">38</xref>,<xref ref-type="bibr" rid="c39">39</xref>]. Similar studies of the impact of hidden neurons have been performed for the kinetic Ising model, another non-equilibrium model that has been applied to neural activity [<xref ref-type="bibr" rid="c40">40</xref>&#x2013;<xref ref-type="bibr" rid="c42">42</xref>]. However, the versions of the kinetic Ising model studied thus far have a Markovian time-dependence, whereas the nonlinear Hawkes model we consider here may have arbitrary history dependence, important for correctly capturing the statistics of network dynamics.</p>
<p>To derive an approximate model for an observed subset of the network, we partition the network into two sets: recorded neurons (labeled by indices <italic>r</italic>) and hidden neurons (labeled by indices <italic>h</italic>). Each recorded neuron has an instantaneous firing rate <italic>&#x03BB;<sub>r</sub></italic> (<italic>t</italic>) such that the probability that the neuron fires within a small time window [<italic>t, t&#x002B;dt</italic>] is <italic>&#x03BB;<sub>r</sub> (t)dt</italic>. The instantaneous firing rate in our model is <disp-formula id="eqn1"><alternatives><graphic xlink:href="105510_eqn1.gif"/></alternatives></disp-formula> where <italic>&#x03BB;</italic><sub>0</sub> is a characteristic firing rate, <italic>&#x03D5;</italic>(<italic>x</italic>) is a nonnegative, continuous function, <italic>&#x00B5;<sub>r</sub></italic> is a tonic drive that sets the baseline firing rate of the neuron, and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline2.gif"/></alternatives></inline-formula> is the convolution of the spike filter <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline3.gif"/></alternatives></inline-formula> with spikes <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline4.gif"/></alternatives></inline-formula> <italic>from</italic> pre-synaptic neuron <italic>j</italic> to post-synaptic neuron <italic>i</italic>. In this work we will take the tonic drive to be constant in time, and focus on the steady-state network activity in response to this drive. We consider spike filters of the form <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline5.gif"/></alternatives></inline-formula>, where the temporal waveforms g<italic><sub>j</sub></italic>(t) are normalized such that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline6.gif"/></alternatives></inline-formula> for all neurons <italic>j</italic>. Because of this normalization, the weight <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline7.gif"/></alternatives></inline-formula> carries units of time. Self-couplings <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline7_1.gif"/></alternatives></inline-formula> need not be interpreted as actual autapses, but rather account for the influence of a neuron&#x2019;s spiking history on its own firing rate (e.g., <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline7_2.gif"/></alternatives></inline-formula> suppresses firing, like a refractory period, while <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline7_3.gif"/></alternatives></inline-formula> promotes bursting). The firing rates for the hidden neurons follow the same expression with indices <italic>h</italic> and <italic>r</italic> interchanged.</p>
<p>We seek to describe the dynamics of the recorded neurons entirely in terms of their own set of spiking histories, eliminating the dependence on the activity of the hidden neurons. This demands averaging out the activity of the hidden neurons; in practice this is intractable to perform exactly [<xref ref-type="bibr" rid="c43">43</xref>&#x2013;<xref ref-type="bibr" rid="c45">45</xref>]. Here, we use a mean field approximation and assume that the input from the hidden neurons can be approximated by its mean <italic>conditioned on the activity of the recorded neurons</italic>. This yields the following formula for the instantaneous firing rates of the recorded neurons:
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn1.gif"/></alternatives></disp-formula></p>
<p>Here, the effective baselines <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline8.gif"/></alternatives></inline-formula> simply modulated by the net input to the neuron, so we do not focus on them here. The effective coupling filters are given in the frequency domain by
<disp-formula id="eqn2"><alternatives><graphic xlink:href="105510_eqn2.gif"/></alternatives></disp-formula></p>
<p>Here, the &#x03BD;<italic><sub>h</sub></italic> are the steady-state mean firing rates of the hidden neurons and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline9.gif"/></alternatives></inline-formula> is the linear response function of the hidden network to perturbations in the <italic>input</italic> [<xref ref-type="bibr" rid="c46">46</xref>]. Both &#x03BD;<italic><sub>h</sub></italic> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline9a.gif"/></alternatives></inline-formula> are calculated <italic>in the absence of the recorded neurons</italic>, using a mean field theory approximation. The complete derivation is given in the Supplementary Information (SI).</p>
<p>In deriving these results, we have neglected both fluctuations around the mean input from the hidden neurons, as well as higher order filtering of the recorded neuron spikes. These results hold for any choice of network parameters for which the mean field steady state of the hidden network exists. For details on the justification of these approximations, see the SI.</p>
<p>The effective coupling filters are what we would&#x2014;in principle&#x2014;measure experimentally if we observe only a subset of a network. In practice, inferring these network properties from data is an extremely nontrivial task [<xref ref-type="bibr" rid="c16">16</xref>&#x2013;<xref ref-type="bibr" rid="c24">24</xref>,<xref ref-type="bibr" rid="c40">40</xref>,<xref ref-type="bibr" rid="c41">41</xref>], and details of the fitting procedure could potentially further skew the inferred coupling filters. We will put aside these complications here, and assume we have access to an inference procedure that allows us to measure <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline10.gif"/></alternatives></inline-formula> without error, so that we may focus on their properties and relationship to the ground-truth coupling filters.</p>
<p><italic>Structure of effective coupling filters</italic>&#x2014;The ground-truth coupling filters <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline11.gif"/></alternatives></inline-formula> are modified by a correction term <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline12.gif"/></alternatives></inline-formula>. The linear response function <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline13.gif"/></alternatives></inline-formula> admits a series representation in terms of paths through the network that neuron <italic>r&#x2032;</italic> can send a signal to neuron <italic>r through hidden neurons only</italic>. This is shown schematically in <xref ref-type="fig" rid="fig1">Fig. 1.</xref> The effect of self-coupling from a neuron back to itself can be absorbed into the contribution of each hidden node in a path.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>FIG. 1.</label>
<caption><p>The hidden unit problem: <bold>A</bold>: The functional connection between two neurons can be decomposed into contributions from all paths from one neuron to the other through &#x201C;hidden&#x201D; intermediary neurons within the same circuit. In this schematic, neurons 1 and 2 are observed, while 3 and 4 are hidden. The effective connection from 1 to 2 comprises a direct connection plus contributions from paths signals can travel through neurons 3 and 4 before arriving at neuron 2. <bold>B</bold>: Leftmost, the effective connection from neuron 1 to 2. subsequent plots decompose this connection into contributions from each path in <bold>A</bold>. Only <italic>directed</italic> connections from neuron 1 to 2 through hidden units contribute to the effective connection. For instance, although neuron 2 makes a connection to neuron 3, this does not generate an effective connection from 2 to 1 because neither neurons 3 nor 4 make a connection back to neuron 1, and hence there is no path that neuron 2 can send a signal to neuron 1.</p></caption>
<graphic xlink:href="105510_fig1.tif"/></fig>
<p>We may write down a set of Feynmanesque rules for explicitly calculating terms in this series. First, we define the gain, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline14.gif"/></alternatives></inline-formula>. The contribution of each term can then be written down using the following rules: <italic>i</italic>) for the edge connecting recorded neuron <italic>r&#x2032;</italic> to a hidden neuron <italic>h<sub>i</sub></italic>, assign a factor <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline15.gif"/></alternatives></inline-formula> for each node corresponding to a hidden neuron <italic>h<sub>i</sub></italic>, assign a factor <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline16.gif"/></alternatives></inline-formula> for each edge connecting hidden neurons <italic>h<sub>i</sub></italic> &#x2260; <italic>h<sub>j</sub></italic>, assign a factor <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline17.gif"/></alternatives></inline-formula>; and <italic>iv</italic>) for the edge connecting hidden neuron <italic>h<sub>j</sub></italic> to recorded neuron <italic>r</italic>, assign a factor <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline18.gif"/></alternatives></inline-formula>. All factors for each path are multiplied together, and all paths are then summed over.</p>
<p>In practice, the linear response matrix <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline19.gif"/></alternatives></inline-formula> can be calculated directly by numerical matrix inversion and multiplied with <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline20.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline21.gif"/></alternatives></inline-formula> to form the correction to <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline22.gif"/></alternatives></inline-formula>, then inverse Fourier transformed to return to the time domain. However, the utility of the path-length series is the intuitive understanding of the origin of contributions to the effective coupling filters and our ability to analytically analyze the strength of contributions from each path [<xref ref-type="bibr" rid="c47">47</xref>]. This is reminiscent of recent works expanding correlations functions of linear models of network spiking in terms of network &#x201C;motifs&#x201D; [<xref ref-type="bibr" rid="c48">48</xref>&#x2013;<xref ref-type="bibr" rid="c50">50</xref>]. One immediate insight the path decomposition offers is that unconnected neurons only develop effective interaction between one other if there is a path that one neuron can send a signal to the other.</p>
<p><italic>Strongly coupled networks</italic>&#x2014;We can now investigate under what conditions hidden paths significantly skew measured neural interactions. The synaptic weights <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline7a.gif"/></alternatives></inline-formula> (for <italic>i</italic> &#x2260; <italic>j</italic>) generally must scale with the size of the network in order for network activity to be stable. The more inputs a neuron receives, the weaker the incoming synaptic connection weights must be so as to not drive the neuron to constantly fire. While a glance at <xref ref-type="disp-formula" rid="eqn1">Eq. (1)</xref> suggests we might expect that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline24.gif"/></alternatives></inline-formula> synaptic inputs to a neuron demands <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline25.gif"/></alternatives></inline-formula> for the inputs to a neuron to be <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline65.gif"/></alternatives></inline-formula>, this will in fact be too weak. A balance of positive and negative independent couplings reduces the typical magnitude, requiring only <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline27.gif"/></alternatives></inline-formula> for network stability. Hence, we term these two cases &#x201C;weak&#x201D; coupling <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline28.gif"/></alternatives></inline-formula>)) and &#x201C;strong&#x201D; coupling <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline29.gif"/></alternatives></inline-formula>). Previous work has studied the hidden-neuron problem in the weak coupling limit [<xref ref-type="bibr" rid="c51">51</xref>&#x2013;<xref ref-type="bibr" rid="c54">54</xref>]; here we study the strong coupling limit. The <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline1a.gif"/></alternatives></inline-formula> scaling of the strong coupling limit has been theoretically predicted to be important for network computations in balanced networks [<xref ref-type="bibr" rid="c31">31</xref>&#x2013;<xref ref-type="bibr" rid="c34">34</xref>]. Moreover, this scaling has recently been observed experimentally in cultured neural tissue [<xref ref-type="bibr" rid="c35">35</xref>], indicating that it may have intrinsic physiological importance.</p>
<p>We consider the case, ubiquitous in neural modeling, of an Erd&#x0151;s-R&#x00E9;yni (ER) network of N neurons, with connection sparsity <italic>p</italic> (only 100<italic>p</italic>&#x0025; of connections are non-zero). The baselines of the neurons are taken to be equal for all neurons, <italic>&#x00B5;<sub>i</sub></italic> = <italic>&#x00B5;</italic><sub>0</sub>, for which there will exist a time-independent steady state. We choose an exponential nonlinearity, <italic>&#x03D5;</italic>(<italic>x</italic>) = <italic>e<sup>x</sup></italic>. This is the &#x201C;canonical&#x201D; choice of nonlinearity used in applications of this model [<xref ref-type="bibr" rid="c16">16</xref>&#x2013;<xref ref-type="bibr" rid="c18">18</xref>,<xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c55">55</xref>]. We will further assume exp(<italic>&#x00B5;</italic><sub>0</sub>) &#x226A; 1, so that we may use this as a small control parameter. For <italic>i</italic> &#x2260; <italic>j</italic>, the non-zero synaptic weights between neurons <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline31.gif"/></alternatives></inline-formula> independently drawn from a normal distribution with zero mean and standard deviation <italic>J<sub>0</sub></italic>/(<italic>pN</italic>)<sup>2<italic>a</italic></sup>, where <italic>J</italic><sub>0</sub> controls the overall strength of the weights and <italic>a</italic> = 1 or 1/2, corresponding to &#x201C;weak&#x201D; and &#x201C;strong&#x201D; coupling, respectively, for this choice of synaptic weight statistics. For simplicity we take <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline32.gif"/></alternatives></inline-formula>, for all neurons <italic>i</italic>. To simplify the analytic analysis, we do not impose Dale&#x2019;s Principle, which requires a neuron&#x2019;s outgoing connections to all be the same sign. We have checked the case of Dale&#x2019;s Principle numerically, and find similar results to the ER network [<xref ref-type="bibr" rid="c55">55</xref>]. Numerical values of all parameters are given in Table SI in the SI.</p>
<p>Consider an example case of a network of <italic>N</italic> = 1000 neurons and <italic>N<sub>rec</sub></italic> = 3 recorded neurons. The resulting effective coupling filters (solid blue curves) for these three neurons are plotted against their true ground-truth coupling filters in. Although the true network architecture is sparse, the resulting effective couplings are not sparse&#x2014;a major qualitative difference in the apparent network architectures. While for many neuron pairs corrections to true non-zero filters were slight, for others, the effective coupling filters deviate significantly away from the true ones, indicating that contributions from paths through the hidden network are comparable to the direct connections.</p>
<p>This result demonstrates that the effective coupling filters can deviate significantly from the direct coupling, but what is the typical deviation&#x003F; To assess this, we study the statistics of the integrated coupling strengths <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline33.gif"/></alternatives></inline-formula>. The expected value of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline34.gif"/></alternatives></inline-formula> is zero, so we compute the standard deviation of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline35.gif"/></alternatives></inline-formula> labeled <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline36.gif"/></alternatives></inline-formula>. Because the self-coupling weights are <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline37.gif"/></alternatives></inline-formula>, we consider only the standard deviations for <italic>r</italic> &#x2260; <italic>r&#x2032;</italic> pairs. When the hidden network is large, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline38.gif"/></alternatives></inline-formula> becomes increasingly Gaussian, and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline41.gif"/></alternatives></inline-formula> represents the width of this distribution. We first estimate these standard deviations by numerically computing <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline42.gif"/></alternatives></inline-formula> as we scale up the synaptic amplitude <italic>J</italic><sub>0</sub> and observe progressively smaller fractions of neurons <italic>f</italic>.</p>
<p>The numerical results are shown as solid curves in <xref ref-type="fig" rid="fig3">Fig. (3)</xref>, for both strong coupling and weak coupling. There are two striking results. First, deviations are nearly negligible <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline43.gif"/></alternatives></inline-formula>) for 1/<italic>N</italic> scaling of connections. Thus, for large Erd&#x0151;s-R&#x00E9;yni networks with synapses that scale with the system size, vast numbers of hidden neurons combine to have negligible effect on effective couplings. This is in marked contrast to the case when coupling is strong <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline44.gif"/></alternatives></inline-formula> scaling), when hidden neurons have a pronounced impact <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline45.gif"/></alternatives></inline-formula>. This is particularly the case when <italic>f</italic> &#x226A; 1&#x2014;as in almost every experiment in neuroscience, where the hidden neurons out-number observed ones by orders of magnitude&#x2014;or when <italic>J</italic><sub>0</sub> &#x2272; 1.0, when typical deviations become half the magnitude of the true couplings themselves (upper blue line). For <italic>J</italic><sub>0</sub> &#x2273; 1.0, the network activity is unstable for an exponential nonlinearity.</p>
<p>To gain insight into these numerical results, we use our path series expansion to calculate the standard deviation <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline36a.gif"/></alternatives></inline-formula>, normalized by <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline37a.gif"/></alternatives></inline-formula>, up to contributions from paths up to length-3. We find
<disp-formula id="eqn3"><alternatives><graphic xlink:href="105510_eqn3.gif"/></alternatives></disp-formula>
corresponding to the black dashed curves in <xref ref-type="fig" rid="fig3">Fig. 3.</xref> <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref> is a truncation of a series in powers of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline46.gif"/></alternatives></inline-formula>, where <italic>f</italic> = <italic>N<sub>rec</sub>/N</italic> is the fraction of recorded neurons. The most important feature of this series is the fact that it only depends on the <italic>fraction</italic> of recorded neurons <italic>f</italic>, not the absolute number, <italic>N</italic>. Contributions from long paths remain finite, even as <italic>N</italic> &#x2192; &#x221E;. Importantly, this is not the case for weak 1/<italic>N</italic> coupling, in which the series is in powers of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline47.gif"/></alternatives></inline-formula>, so that contributions from long paths are negligible in large networks <italic>N</italic> &#x226B; 1. (See [<xref ref-type="bibr" rid="c55">55</xref>] for derivation and results for <italic>N</italic> = 100.) Deviations of <xref ref-type="disp-formula" rid="eqn3">Eq. (3)</xref> from the numerical solutions in <xref ref-type="fig" rid="fig3">Fig. 3</xref> indicate that contributions from truncated terms are not negligible when <italic>f</italic> &#x226A; 1. As these terms correspond to paths of length-4 or more, this shows that long chains through the network contribute significantly to shaping effective interactions.</p>
<p><italic>Discussion</italic> &#x2014; We have derived a quantitative relationship between &#x201C;ground-truth&#x201D; synaptic interactions and the effective interactions that unobserved neurons generate between subsets of observed neurons. This provides the tools to determine the conditions under which unobserved neurons substantially reshape observed neural dynamics. For the commonly assumed Erd&#x0151;s-R&#x00E9;yni case with independent synaptic weights scaling with <italic>N</italic><sup>&#x2212;1/2</sup> such hidden units yield qualitative changes to the magnitude and dynamics neural interactions. Together with theoretical and experimental evidence for this scaling in cortex [<xref ref-type="bibr" rid="c31">31</xref>&#x2013;<xref ref-type="bibr" rid="c35">35</xref>], this suggests that neural coupling filters inferred from cortical activity data may differ markedly from the true connectivity.</p>
<p>Our work offers a way to probe the architecture of the unobserved network through measured <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline48.gif"/></alternatives></inline-formula>. While inverting <xref ref-type="disp-formula" rid="eqn2">Eq. (2)</xref> to obtain a unique set of true couplings is generally an ill-posed problem, future work could use the path length expansion combined with physiological constraints to make inferences about the patterns of true connectivities consistent with the measured effective ones. Intriguingly, this same approach may yield insight into the set of circuits that can perform specific computations that are expressed in terms of effective interactions among a subset of cells. This can be particularly useful when the interactions required cor a computation violate Dale&#x2019;s principle or have architectures inconsistent with anatomical measurements [56,57]. If these regarded as effective interactions, however, then we may be able to use <xref ref-type="disp-formula" rid="eqn2">Eq. (2)</xref> to generate patterns of biologically plausible &#x201C;hidden&#x201D; circuit architectures consistent with the effec-tive interactions of the designed circuit.</p>
<p>Similar strategies may even be employed by the nervous system itself. For instance, why do many principal neurons&#x2014;those which project from one circuit to another&#x2014;not make direct reciprocal connections to one another, instead being linked by intermediary neurons&#x003F; One hypothesis is that direct synaptic interactions <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline52_1.gif"/></alternatives></inline-formula> are limited by physiological constraints, and &#x201C;hidden&#x201D; interneurons are necessary to reshape the spatiotemporal dynamics of neural interactions into forms that can perform required computations, such as by mixing excitatory and inhibitory interactions to generate a richer range of possible synaptic calculations (cf. <xref ref-type="fig" rid="fig1">Fig. 1).</xref> The explicit link we draw between effective interactions and their origins in biological circuitry is thus a new lens for interpreting emerging data on neural connectivity.</p></body>
<back>
<ack><p><italic>Acknowledgments</italic>&#x2014;We thank Gabe Ocker for providing the GLM network simulation code that we modified to perform the full network simulations in this work, Tyler Kekona for work on an early version of a related project, and Ben Lansdell and Christof Koch for helpful feedback. Support provided by the Sackler Scholar Program in Integrative Biophysics (BAWB), CRCNS grant DMS-1208027 (ESB, FR), NIH grant EY11850 and HHMI (FR). This work was partially based on work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216 (MB). MB and ESB wish to thank the Allen Institute for Brain Science founders, Paul G. Allen and Jody Allen, for their vision, encouragement, and support.</p></ack>
<ref-list><title>References</title>
<ref id="c1">
<label>[1]</label><mixed-citation publication-type="journal"><string-name><given-names>D. S.</given-names> <surname>Bassett</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Bullmore</surname></string-name>, <string-name><given-names>B. A.</given-names> <surname>Verchinski</surname></string-name>, <string-name><given-names>V. S.</given-names> <surname>Mattay</surname></string-name>, <string-name><given-names>D. R.</given-names> <surname>Weinberger</surname></string-name>, and <string-name><given-names>A.</given-names> <surname>Meyer-Lindenberg</surname></string-name>, <source>Journal of Neuroscience</source> <volume>28</volume>, <fpage>9239</fpage> (<year>2008</year>), <ext-link ext-link-type="orcid" xlink:href="http://www.jneurosci.org/content/30/50/16876.full.pdf">http://www.jneurosci.org/content/28/37/9239.full.pdf.</ext-link></mixed-citation></ref>
<ref id="c2">
<label>[2]</label><mixed-citation publication-type="journal"><string-name><given-names>M. A.</given-names> <surname>Kramer</surname></string-name>, <string-name><given-names>E. D.</given-names> <surname>Kolaczyk</surname></string-name>, and <string-name><given-names>H. E.</given-names> <surname>Kirsch</surname></string-name>, <source>Epilepsy Research</source> <volume>79</volume>, <fpage>173</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c3">
<label>[3]</label><mixed-citation publication-type="journal"><string-name><given-names>K.</given-names> <surname>Supekar</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Menon</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Rubin</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Musen</surname></string-name>, and <string-name><given-names>M. D.</given-names> <surname>Greicius</surname></string-name>, <source>PLOS Computational Biology</source> <volume>4</volume>, <fpage>1</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c4">
<label>[4]</label><mixed-citation publication-type="journal"><string-name><given-names>C.-Y.</given-names> <surname>Lo</surname></string-name>, <string-name><given-names>P.-N.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>K.-H.</given-names> <surname>Chou</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>He</surname></string-name>, and <string-name><given-names>C.-P.</given-names> <surname>Lin</surname></string-name>, <source>Journal of Neuroscience</source> <volume>30</volume>, <fpage>16876</fpage> (<year>2010</year>), <ext-link ext-link-type="orcid" xlink:href="http://www.jneurosci.org/content/30/50/16876.full.pdf">http://www.jneurosci.org/content/30/50/16876.full.pdf</ext-link>.</mixed-citation></ref>
<ref id="c5">
<label>[5]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Chavez</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Valencia</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Navarro</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Latora</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Martinerie</surname></string-name>, <source>Phys. Rev. Lett</source>. <volume>104</volume>, <fpage>118701</fpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c6">
<label>[6]</label><mixed-citation publication-type="journal"><string-name><given-names>L.</given-names> <surname>Douw</surname></string-name>, <string-name><given-names>M.</given-names> <surname>de Groot</surname></string-name>, <string-name><given-names>E.</given-names> <surname>van Dellen</surname></string-name>, <string-name><given-names>J. J.</given-names> <surname>Heimans</surname></string-name>, <string-name><given-names>H. E.</given-names> <surname>Ronner</surname></string-name>, <string-name><given-names>C. J.</given-names> <surname>Stam</surname></string-name>, and <string-name><given-names>J. C.</given-names> <surname>Reijneveld</surname></string-name>, <source>PLOS ONE</source> <volume>5</volume>, <fpage>1</fpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c7">
<label>[7]</label><mixed-citation publication-type="journal"><string-name><given-names>E.</given-names> <surname>van Diessen</surname></string-name>, <string-name><given-names>J. I.</given-names> <surname>Hanemaaijer</surname></string-name>, <string-name><given-names>W. M.</given-names> <surname>Otte</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Zelmann</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Jacobs</surname></string-name>, <string-name><given-names>F. E.</given-names> <surname>Jansen</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Dubeau</surname></string-name>, <string-name><given-names>C. J.</given-names> <surname>Stam</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Gotman</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Zijlmans</surname></string-name>, <source>NeuroImage</source> <volume>82</volume>, <fpage>564</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c8">
<label>[8]</label><mixed-citation publication-type="journal"><string-name><given-names>Y. D.</given-names> <surname>Reijmer</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Leemans</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Caeyenberghs</surname></string-name>, <string-name><given-names>S. M.</given-names> <surname>Heringa</surname></string-name>, <string-name><given-names>H. L.</given-names> <surname>Koek</surname></string-name>, <string-name><given-names>G. J.</given-names> <surname>Biessels</surname></string-name>, and <article-title>O. behalf of the Utrecht Vascular Cognitive Impairment Study Group</article-title>, <source>Neurology</source> <volume>80</volume>, <fpage>1370</fpage> (<year>2013</year>), <ext-link ext-link-type="orcid" xlink:href="http://www.neurology.org/content/80/15/1370.full.pdf&#x002B;html">http://www.neurology.org/content/80/15/1370.full.pdf&#x002B;html</ext-link>.</mixed-citation></ref>
<ref id="c9">
<label>[9]</label><mixed-citation publication-type="journal"><string-name><given-names>E. H.</given-names> <surname>Seo</surname></string-name>, <string-name><given-names>D. Y.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>J.-M.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>J.-S.</given-names> <surname>Park</surname></string-name>, <string-name><given-names>B. K.</given-names> <surname>Sohn</surname></string-name>, <string-name><given-names>D. S.</given-names> <surname>Lee</surname></string-name>, <string-name><given-names>Y. M.</given-names> <surname>Choe</surname></string-name>, and <string-name><given-names>J. I.</given-names> <surname>Woo</surname></string-name>, <source>PLOS ONE</source> <volume>8</volume>, <fpage>1</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c10">
<label>[10]</label><mixed-citation publication-type="journal"><string-name><given-names>C. J.</given-names> <surname>Stam</surname></string-name>, <source>Nat Rev Neurosci</source> <volume>15</volume>, <fpage>683</fpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c11">
<label>[11]</label><mixed-citation publication-type="journal"><string-name><given-names>D. E.</given-names> <surname>Warren</surname></string-name>, <string-name><given-names>J. D.</given-names> <surname>Power</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Bruss</surname></string-name>, <string-name><given-names>N. L.</given-names> <surname>Denburg</surname></string-name>, <string-name><given-names>E. J.</given-names> <surname>Waldron</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Sun</surname></string-name>, <string-name><given-names>S. E.</given-names> <surname>Petersen</surname></string-name>, and <string-name><given-names>D.</given-names> <surname>Tranel</surname></string-name>, <source>Proceedings of the National Academy of Sciences</source> <volume>111</volume>, <fpage>14247</fpage> (<year>2014</year>), <ext-link ext-link-type="orcid" xlink:href="http://www.pnas.org/content/111/39/14247.full.pdf">http://www.pnas.org/content/111/39/14247.full.pdf</ext-link>.</mixed-citation></ref>
<ref id="c12">
<label>[12]</label><mixed-citation publication-type="journal"><string-name><given-names>K. T. E.</given-names> <surname>Olde Dubbelink</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Hillebrand</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Stoffers</surname></string-name>, <string-name><given-names>J. B.</given-names> <surname>Deijen</surname></string-name>, <string-name><given-names>J. W. R.</given-names> <surname>Twisk</surname></string-name>, <string-name><given-names>C. J.</given-names> <surname>Stam</surname></string-name>, and <string-name><given-names>H. W.</given-names> <surname>Berendse</surname></string-name>, <source>Brain</source> <volume>137</volume>, <fpage>197</fpage> (<year>2014</year>), <ext-link ext-link-type="orcid" xlink:href="http://brain.oxfordjournals.org/content/137/1/197.full.pdf">http://brain.oxfordjournals.org/content/137/1/197.full.pdf</ext-link>.</mixed-citation></ref>
<ref id="c13">
<label>[13]</label><mixed-citation publication-type="journal"><string-name><given-names>B. C.</given-names> <surname>Bernhardt</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Bonilha</surname></string-name>, and <string-name><given-names>D. W.</given-names> <surname>Gross</surname></string-name>, <source>Epilepsy &#x0026; Behavior</source> <volume>50</volume>, <fpage>162</fpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c14">
<label>[14]</label><mixed-citation publication-type="journal"><string-name><given-names>J. D.</given-names> <surname>Medaglia</surname></string-name> and <string-name><given-names>D. S.</given-names> <surname>Bassett</surname></string-name>, <article-title>ArXiv e-prints</article-title> (<year>2017</year>), <source>arXiv</source>:<pub-id pub-id-type="arxiv">1701.01101</pub-id> <label>[q-bio.NC]</label>.</mixed-citation></ref>
<ref id="c15">
<label>[15]</label><mixed-citation publication-type="journal"><string-name><given-names>C. J.</given-names> <surname>Honey</surname></string-name>, <string-name><given-names>J.-P.</given-names> <surname>Thivierge</surname></string-name>, and <string-name><given-names>O.</given-names> <surname>Sporns</surname></string-name>, <source>NeuroImage</source> <volume>52</volume>, <fpage>766</fpage>(<year>2010</year>), <article-title>computational Models of the Brain</article-title>.</mixed-citation></ref>
<ref id="c16">
<label>[16]</label><mixed-citation publication-type="book"><string-name><given-names>E</given-names>. <surname>Simoncelli</surname></string-name>, <string-name><given-names>L</given-names>. <surname>Paninski</surname></string-name>, <string-name><given-names>J</given-names>. <surname>Pillow</surname></string-name>, and <string-name><given-names>O</given-names>. <surname>Schwartz</surname></string-name>, <source>in The Cognitive Neurosciences</source>, edited by M. Gazzaniga (<publisher-name>MIT Press</publisher-name>, <year>2004</year>) <edition>3rd</edition> ed., pp. <fpage>327</fpage>&#x2013;<lpage>338</lpage>.</mixed-citation></ref>
<ref id="c17">
<label>[17]</label><mixed-citation publication-type="journal"><string-name><given-names>L.</given-names> <surname>Paninski</surname></string-name>, <source>Network: Computation in Neural Systems</source> <volume>15</volume>, <fpage>243</fpage> (<year>2004</year>).</mixed-citation></ref>
<ref id="c18">
<label>[18]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Pillow</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Paninski</surname></string-name>, <string-name><given-names>V. J.</given-names> <surname>Uzzell</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Simoncelli</surname></string-name>, and <string-name><given-names>E. J.</given-names> <surname>Chichilnisky</surname></string-name>, <source>The Journal of neuroscience : the official journal of the Society for Neuroscience</source> <volume>25</volume>, <fpage>11003</fpage> (<year>2005</year>).</mixed-citation></ref>
<ref id="c19">
<label>[19]</label><mixed-citation publication-type="journal"><string-name><given-names>J. E.</given-names> <surname>Kulkarni</surname></string-name> and <string-name><given-names>L.</given-names> <surname>Paninski</surname></string-name>, <source>Network: Computation in Neural Systems</source> <volume>18</volume>, <fpage>375</fpage> (<year>2007</year>), pMID: <pub-id pub-id-type="pmid">17943613</pub-id>, <ext-link ext-link-type="orcid" xlink:href="http://dx.doi.org/10.1080/09548980701625173">http://dx.doi.org/10.1080/09548980701625173</ext-link>.</mixed-citation></ref>
<ref id="c20">
<label>[20]</label><mixed-citation publication-type="book"><string-name><given-names>J. W.</given-names> <surname>Pillow</surname></string-name> and <string-name><given-names>P. E.</given-names> <surname>Latham</surname></string-name>, <source>in Advances in Neural Information Processing Systems 20</source>, edited by <string-name><given-names>J</given-names>. <surname>Platt</surname></string-name>, <string-name><given-names>D</given-names>. <surname>Koller</surname></string-name>, <string-name><given-names>Y</given-names>. <surname>Singer</surname></string-name>, and <string-name><given-names>S</given-names>. <surname>Roweis</surname></string-name> (<publisher-name>MIT Press</publisher-name>, <publisher-loc>Cambridge, MA</publisher-loc>, <year>2007</year>) pp. <fpage>1161</fpage>&#x2013;<lpage>1168</lpage>.</mixed-citation></ref>
<ref id="c21">
<label>[21]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Pillow</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Shlens</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Paninski</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Sher</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Litke</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Chichilnisky</surname></string-name>, and <string-name><given-names>E.</given-names> <surname>Simoncelli</surname></string-name>, <source>Nature</source> <volume>454</volume>, <fpage>995</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c22">
<label>[22]</label><mixed-citation publication-type="journal"><string-name><given-names>G.</given-names> <surname>Field</surname></string-name>, <string-name><given-names>J. L.</given-names> <surname>Gauthier</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Sher</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Greschner</surname></string-name>, <string-name><given-names>T. a.</given-names> <surname>Machado</surname></string-name>, <string-name><given-names>L. H.</given-names> <surname>Jepson</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Shlens</surname></string-name>, <string-name><given-names>D. E.</given-names> <surname>Gunning</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Mathieson</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Dabrowski</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Paninski</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Litke</surname></string-name>, and <string-name><given-names>E. J.</given-names> <surname>Chichilnisky</surname></string-name>, <source>Nature</source> <volume>467</volume>, <fpage>673</fpage> (<year>2010</year>).</mixed-citation></ref>
<ref id="c23">
<label>[23]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Vidne</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Ahmadian</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Shlens</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Pillow</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Kulkarni</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Litke</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Chichilnisky</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Simoncelli</surname></string-name>, and <string-name><given-names>L.</given-names> <surname>Paninski</surname></string-name>, <source>J Comput Neurosci</source> <volume>33</volume>, <fpage>97</fpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c24">
<label>[24]</label><mixed-citation publication-type="other"><string-name><given-names>L.</given-names> <surname>Paninski</surname></string-name>, <volume>6536</volume> (<year>2015</year>), <pub-id pub-id-type="doi">10.1088/0954-898X</pub-id>.</mixed-citation></ref>
<ref id="c25">
<label>[25]</label><mixed-citation publication-type="journal"><string-name><given-names>R.</given-names> <surname>Dahlhaus</surname></string-name>, <source>Metrika</source> <volume>51</volume>, <fpage>157</fpage> (<year>2000</year>).</mixed-citation></ref>
<ref id="c26">
<label>[26]</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Eichler</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Dahlhaus</surname></string-name>, and <string-name><given-names>J</given-names>. <surname>Sandk&#x00FC;hler</surname></string-name>, <source>Biological Cybernetics</source> <volume>89</volume>, <fpage>289</fpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c27">
<label>[27]</label><mixed-citation publication-type="journal"><string-name><given-names>I. H.</given-names> <surname>Stevenson</surname></string-name>, <string-name><given-names>J. M.</given-names> <surname>Rebesco</surname></string-name>, <string-name><given-names>L. E.</given-names> <surname>Miller</surname></string-name>, and <string-name><given-names>K. P.</given-names> <surname>Krding</surname></string-name>, <source>Current Opinion in Neurobiology</source> <volume>18</volume>, <fpage>582</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c28">
<label>[28]</label><mixed-citation publication-type="journal"><string-name><given-names>R.</given-names> <surname>Li&#x00E9;geois</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Mishra</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Zorzi</surname></string-name>, and <string-name><given-names>R.</given-names> <surname>Sepulchre</surname></string-name>, in <source>2015 54th IEEE Conference on Decision and Control (CDC)</source> (<year>2015</year>) pp. <fpage>3965</fpage>&#x2013;<lpage>3970</lpage>.</mixed-citation></ref>
<ref id="c29">
<label>[29]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Peters</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Bhlmann</surname></string-name>, and <string-name><given-names>N</given-names>. <surname>Meinshausen</surname></string-name>, <article-title>Journal of the Royal Statistical Society: Series B</article-title> (<source>Statistical Methodology</source>) <volume>78</volume>, <fpage>947</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c30">
<label>[30]</label><mixed-citation publication-type="journal"><string-name><given-names>N. J.</given-names> <surname>Foti</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Nadkarni</surname></string-name>, <string-name><given-names>A. K.</given-names> <surname>Lee</surname></string-name>, and <string-name><given-names>E. B</given-names>. <surname>Fox</surname></string-name>, in <source>2nd KDD Workshop on Mining and Learning from Time Series</source> (<year>2016</year>).</mixed-citation></ref>
<ref id="c31">
<label>[31]</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>van Vreeswijk</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Sompolinsky</surname></string-name>, <source>Science</source> <volume>274</volume>, <fpage>1724</fpage> (<year>1996</year>), <ext-link ext-link-type="orcid" xlink:href="http://science.sciencemag.org/content/274/5293/1724.full.pdf">http://science.sciencemag.org/content/274/5293/1724.full.pdf</ext-link>.</mixed-citation></ref>
<ref id="c32">
<label>[32]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Litwin-Kumar</surname></string-name> and <string-name><given-names>B.</given-names> <surname>Doiron</surname></string-name>, <source>Nat Neurosci</source> <volume>15</volume>, <fpage>1498</fpage> (<year>2012</year>).</mixed-citation></ref>
<ref id="c33">
<label>[33]</label><mixed-citation publication-type="journal"><string-name><given-names>R.</given-names> <surname>Rosenbaum</surname></string-name> and <string-name><given-names>B.</given-names> <surname>Doiron</surname></string-name>, <source>Phys. Rev. X</source> <volume>4</volume>, <fpage>021039</fpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c34">
<label>[34]</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Deneve</surname></string-name> and <string-name><given-names>C. K.</given-names> <surname>Machens</surname></string-name>, <source>Nat Neurosci</source> <volume>19</volume>, <fpage>375</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c35">
<label>[35]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Barral</surname></string-name> and <string-name><given-names>A. D</given-names> <surname>Reyes</surname></string-name>, <source>Nat Neurosci</source> <volume>19</volume>, <fpage>1690</fpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c36">
<label>[36]</label><mixed-citation publication-type="journal"><string-name><given-names>G.</given-names> <surname>Koch Ocker</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Josi&#x0107;</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Shea-Brown</surname></string-name>, and <string-name><given-names>M. A.</given-names> <surname>Buice</surname></string-name>, <source>ArXiv e-prints</source> (<year>2016</year>), arXiv:<pub-id pub-id-type="arxiv">1610.03828</pub-id> <label>[q-bio.NC]</label>.</mixed-citation></ref>
<ref id="c37">
<label>[37]</label><mixed-citation publication-type="journal"><string-name><given-names>E. S.</given-names> <surname>Chornoboy</surname></string-name>, <string-name><given-names>L. P.</given-names> <surname>Schramm</surname></string-name>, and <string-name><given-names>A. F.</given-names> <surname>Karr</surname></string-name>, <source>Biological Cybernetics</source> <volume>59</volume>, <fpage>265</fpage> (<year>1988</year>).</mixed-citation></ref>
<ref id="c38">
<label>[38]</label><mixed-citation publication-type="book"><string-name><given-names>W.</given-names> <surname>Gerstner</surname></string-name>, <string-name><given-names>W. M.</given-names> <surname>Kistler</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Naud</surname></string-name>, and <string-name><given-names>L.</given-names> <surname>Paninski</surname></string-name>, <source>Neuronal Dynamics: From single neurons to networks and models of cognition</source> (<publisher-name>Cambridge University Press</publisher-name>, <publisher-loc>Cambridge, U.K</publisher-loc>., <year>2014</year>).</mixed-citation></ref>
<ref id="c39">
<label>[39]</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Ostojic</surname></string-name> and <string-name><given-names>N.</given-names> <surname>Brunel</surname></string-name>, <source>PLOS Computational Biology</source> <volume>7</volume>, <fpage>1</fpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c40">
<label>[40]</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Dunn</surname></string-name> and <string-name><given-names>Y.</given-names> <surname>Roudi</surname></string-name>, <source>Phys. Rev. E</source> <volume>87</volume>, <fpage>022127</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c41">
<label>[41]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Tyrcha</surname></string-name> and <string-name><given-names>J.</given-names> <surname>Hertz</surname></string-name>, <source>Mathematical Biosciences and Engineering</source> <volume>11</volume>, <fpage>149</fpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c42">
<label>[42]</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Dunn</surname></string-name> and <string-name><given-names>C.</given-names> <surname>Battistin</surname></string-name>, <source>ArXiv e-prints</source> (<year>2016</year>), arXiv:<pub-id pub-id-type="arxiv">1612.06185</pub-id> [cond-mat.dis-nn].</mixed-citation></ref>
<ref id="c43">
<label>[43]</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Bravi</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Sollich</surname></string-name>, <source>ArXiv e-prints</source> (<year>2016</year>), arXiv:<pub-id pub-id-type="arxiv">1611.09849</pub-id> [physics.chem-ph].</mixed-citation></ref>
<ref id="c44">
<label>[44]</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Bravi</surname></string-name> and <string-name><given-names>P</given-names>. <surname>Sollich</surname></string-name>, <source>ArXiv e-prints</source> (<year>2016</year>), arXiv:<pub-id pub-id-type="arxiv">1612.01976</pub-id> [cond-mat.dis-nn].</mixed-citation></ref>
<ref id="c45">
<label>[45]</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Bravi</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Opper</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Sollich</surname></string-name>, <source>Phys. Rev. E</source> <volume>95</volume>, <fpage>012122</fpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c46">
<label>[46]</label><mixed-citation publication-type="other">This differs from the linear response function to perturbations in the <italic>output</italic>, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline209.gif"/></alternatives></inline-formula>. However, the two are related by <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline210.gif"/></alternatives></inline-formula> where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline211.gif"/></alternatives></inline-formula> is the gain.</mixed-citation></ref>
<ref id="c47">
<label>[47]</label><mixed-citation publication-type="other">The series expansion is also more practical in the general case in which the baselines <italic>&#x03BC;<sub>h</sub></italic>(<italic>t</italic>) are time-dependent and we cannot solve the problem in the frequency domain.</mixed-citation></ref>
<ref id="c48">
<label>[48]</label><mixed-citation publication-type="journal"><string-name><given-names>V.</given-names> <surname>Pernice</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Staude</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Cardanobile</surname></string-name>, and <string-name><given-names>S.</given-names> <surname>Rotter</surname></string-name>, <source>PLOS Computational Biology</source> <volume>7</volume>, <fpage>1</fpage> (<year>2011</year>).</mixed-citation></ref>
<ref id="c49">
<label>[49]</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Hu</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Trousdale</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Josi</surname></string-name>, and <string-name><given-names>E.</given-names> <surname>Shea-Brown</surname></string-name>, <source>Journal of Statistical Mechanics: Theory and Experiment</source> <volume>2013</volume>, <fpage>P03012</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c50">
<label>[50]</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Hu</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Trousdale</surname></string-name>, <string-name><given-names>K</given-names>. <surname>Josi&#x0107;</surname></string-name>, and <string-name><given-names>E</given-names>. <surname>Shea-Brown</surname></string-name>, <source>Phys. Rev. E</source> <volume>89</volume>, <fpage>032802</fpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c51">
<label>[51]</label><mixed-citation publication-type="journal"><string-name><given-names>D. Q.</given-names> <surname>Nykamp</surname></string-name>, <source>SIAM Journal on Applied Mathematics</source> <volume>65</volume>, <fpage>2005</fpage> (<year>2005</year>), <ext-link ext-link-type="orcid" xlink:href="http://dx.doi.org/10.1137/S0036139903437072">http://dx.doi.org/10.1137/S0036139903437072</ext-link>.</mixed-citation></ref>
<ref id="c52">
<label>[52]</label><mixed-citation publication-type="journal"><string-name><given-names>D. Q.</given-names> <surname>Nykamp</surname></string-name>, <source>Mathematical Biosciences</source> <volume>205</volume>, <fpage>204</fpage> (<year>2007</year>).</mixed-citation></ref>
<ref id="c53">
<label>[53]</label><mixed-citation publication-type="journal"><string-name><given-names>D. Q.</given-names> <surname>Nykamp</surname></string-name>, <source>SIAM Journal on Applied Mathematics</source> <volume>68</volume>, <fpage>354</fpage> (<year>2007</year>), <ext-link ext-link-type="orcid" xlink:href="http://dx.doi.org/10.1137/070683350">http://dx.doi.org/10.1137/070683350</ext-link>.</mixed-citation></ref>
<ref id="c54">
<label>[54]</label><mixed-citation publication-type="journal"><string-name><given-names>D. Q.</given-names> <surname>Nykamp</surname></string-name>, <source>Phys. Rev. E</source> <volume>78</volume>, <fpage>021902</fpage> (<year>2008</year>).</mixed-citation></ref>
<ref id="c55">
<label>[55]</label><mixed-citation publication-type="other">
<label>[55]</label>See Supplementary Information.</mixed-citation></ref>
<ref id="c56">
<label>[56]</label><mixed-citation publication-type="journal"><string-name><given-names>P. D.</given-names> <surname>King</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Zylberberg</surname></string-name>, and <string-name><given-names>M. R.</given-names> <surname>DeWeese</surname></string-name>, <source>Journal of Neuroscience</source> <volume>33</volume>, <fpage>5475</fpage> (<year>2013</year>), <ext-link ext-link-type="orcid" xlink:href="http://www.jneurosci.org/content/33/13/5475.full.pdf">http://www.jneurosci.org/content/33/13/5475.full.pdf</ext-link>.</mixed-citation></ref>
<ref id="c57">
<label>[57]</label><mixed-citation publication-type="journal"><string-name><given-names>H. F.</given-names> <surname>Song</surname></string-name>, <string-name><given-names>G. R.</given-names> <surname>Yang</surname></string-name>, and <string-name><given-names>X.-J.</given-names> <surname>Wang</surname></string-name>, <source>PLOS Computational Biology</source> <volume>12</volume>, <fpage>1</fpage> (<year>2016</year>).</mixed-citation></ref></ref-list>
<sec id="s1" sec-type="Supplementary-material">
<title>SUPPLEMENTARY INFORMATION</title>
<sec id="s1a">
<title>A. Model definition: nonlinear Hawkes model (Generalized linear point process model)</title>
<p>The firing rate of a neuron <italic>i</italic> in the full network is given by
<disp-formula id="eqnS1"><alternatives><graphic xlink:href="105510_eqnS1.gif"/></alternatives></disp-formula>
where <italic>&#x03BB;</italic><sub>0</sub> is a characteristic rate, <italic>&#x03D5;</italic>(<italic>x</italic>) &#x2265; 0 is a nonlinear function, <italic>&#x00B5;<sub>i</sub></italic> (potentially a function of some external stimulus <italic>&#x03B8;</italic>) is a time-independent tonic drive that sets the baseline firing rate of the neuron in the absence of input from other neurons, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline55_1.gif"/></alternatives></inline-formula> is a coupling filter that filters spikes <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline55_2.gif"/></alternatives></inline-formula> fired by presynaptic neuron <italic>j</italic> at time <italic>t<sub>i</sub></italic>, incident on post-synaptic neuron <italic>i</italic>, and <italic>&#x03B5;<sub>i</sub></italic>(<italic>t</italic>) is an external input current. We will take <italic>&#x03B5;<sub>i</sub></italic>(<italic>t</italic>) = 0 for simplicity in this work, focusing on the spontaneous activity of the network. We need not attach a mechanistic interpretation to these filters, but a convenient interpretation is that the GLM model is like a soft-threshold integrate-and-fire network model, such that we can interpret the spike filtering as coming from some synaptic dynamics [<xref ref-type="bibr" rid="c1">1</xref>,<xref ref-type="bibr" rid="c2">2</xref>]. As such, we take the actual rate to be
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn2.gif"/></alternatives></disp-formula>
where <italic>s<sub>j</sub></italic>(<italic>t</italic>) is the synaptic activity of pre-synaptic neuron <italic>j</italic> at time <italic>t</italic>. In general,
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn3.gif"/></alternatives></disp-formula>
where <italic>g<sub>j</sub></italic>(<italic>t</italic>) is the temporal waveform of the post-synaptic currents evoked by pre-synaptic neuron <italic>j</italic>, normalized to integrate to 1.</p>
<p>This interpretation of the model is useful for simulating the network activity, and we use such a method for our simulations verifying our analytic calculations (see &#x201C;Simulations of network activity&#x201D;). Otherwise, we will simply use the form of the model as written in <xref ref-type="disp-formula" rid="eqnS1">Eq. (S.1)</xref>.</p>
</sec>
<sec id="s1b">
<title>B. Model network architectures</title>
<p>Our main result, <xref ref-type="disp-formula" rid="eqn2">Eq. (2)</xref>, is valid for general network architectures with arbitrary weighted synaptic connections, so long as the hidden subset of the network has stable dynamics when the recorded neurons are removed. An example for which our method must be modified would be a network in which all or the majority of the hidden neurons are excitatory, as the hidden network is unlikely to be stable when the recorded neurons are disconnected. Similarly, we find that synaptic weight distributions with undefined moments will generally cause the network activity to be unstable. For example, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline56.gif"/></alternatives></inline-formula> drawn from a Cauchy distribution generally yield unstable network dynamics unless the weights are scaled inversely with a large power of the network size <italic>N</italic>.</p>
<p>To specify a concrete network architecture to study, we choose a directed Erd&#x0151;s-R&#x00E9;yni random network with sparsity <italic>p</italic>&#x2014;i.e., each directed connection between neurons is assigned independently with probability <italic>p</italic> that the weight is non-zero. The weights of the non-zero connections are then drawn from a separate distribution chosen to have mean 0 and variance <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline57.gif"/></alternatives></inline-formula>. The choice of exponent <italic>a</italic> determines whether the coupling is weak (<italic>a</italic> = 1) or strong (<italic>a</italic> = 1/2). In most of our analytical results we only need the mean and variances of the weights, so we do not need to specify the exact distribution. In simulations, we use a normal distribution, but have also checked (symmetrized) lognormal distributions, which yield similar results (not shown). The reason for scaling the weights as 1/(<italic>pN</italic>)<italic><sup>a</sup></italic>, as opposed to just 1/<italic>N<sup>a</sup></italic>, is that the mean incoming degree of connections is <italic>p</italic>(<italic>N</italic> &#x2212; 1) &#x2248; <italic>pN</italic> for large networks; this scaling thus controls for the typical magnitude of incoming spikes.</p>
<p>For strongly coupled networks, the combined effect of sparsity and synaptic weight distribution yields an overall standard deviation of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline58.gif"/></alternatives></inline-formula>. Because the sparsity parameter <italic>p</italic> cancels out, it does not matter if we consider <italic>p</italic> to be fixed or <italic>k</italic><sub>0</sub> = <italic>pN</italic> to be fixed&#x2014;both cases are equivalent. However, this is not the case if we scale <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline59.gif"/></alternatives></inline-formula> by 1/<italic>k</italic><sub>0</sub>, as the overall standard deviation would then be <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline60.gif"/></alternatives></inline-formula>, which only corresponds to the weak-coupling limit if <italic>p</italic> is fixed. If <italic>k</italic><sub>0</sub> is fixed, the standard deviation would scale as <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline61.gif"/></alternatives></inline-formula>.</p>
<p>It is worth noting that the determination of &#x201C;weak&#x201D; versus &#x201C;strong&#x201D; coupling depends not only on the power of <italic>N</italic> with which synaptic weights scale, but also on the network architecture and correlation structure of the weights <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline62.gif"/></alternatives></inline-formula>. For example, for an all-to-all connected matrix with symmetric rank-1 synaptic weights of the form <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline63.gif"/></alternatives></inline-formula>, where the <italic>&#x03B6;<sub>i</sub></italic> are independently distributed normal random variates, the standard deviation of <italic>each &#x03B6;</italic> must scale as <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline64.gif"/></alternatives></inline-formula> in order for hidden paths to generate <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline65a.gif"/></alternatives></inline-formula> contributions to effective interactions, such that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline66.gif"/></alternatives></inline-formula> scales as 1/<italic>N</italic> but the coupling is still strong.</p>
<p>Real neurons appear to split into two broad separate classes, &#x201C;excitatory&#x201D; and &#x201C;inhibitory,&#x201D; a dichotomy know as Dale&#x2019;s principle. Neurons in a network that obeys this principle will have coupling filters <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline67.gif"/></alternatives></inline-formula> that are strictly positive for excitatory neurons and strictly negative for inhibitory neurons. We do not impose this restriction on the model used to generate the results presented in the main text, but have tested the consequence of imposing Dale&#x2019;s principle on the network. We find similar results, shown in Fig. (S1) below (compare to Fig. (3) in main text). The trends are the same as in networks that do not obey Dale&#x2019;s principle, with the resulting ratios being slightly reduced. Because our analysis requires calculation of the mean field firing rates of the hidden network in absence of the recorded neurons, random sampling of the network may, by chance, yield hidden networks with an imbalance of excitatory neurons, for which the mean field firing rates of the hidden network may diverge for our choice of exponential nonlinearity. This is the origin of the relatively larger error bars in Fig. (S1): less random samplings for which the hidden network was stable were available to perform the computation. Choosing a nonlinearity that saturates, such as <italic>&#x03D5;</italic>(<italic>x</italic>) = <italic>c</italic>/(1 &#x002B; exp(&#x2212;<italic>x</italic>)), prevents the mean-field firing rates from diverging, yielding stable network activity.</p>
<p>Finally, Erd&#x0151;s-R&#x00E9;yni networks are relatively easy to analyze analytically, and are ubiquitous in many influential computational and theoretical studies, real world networks typically have more structure. As preliminary tests of our results in networks with more realistic features, we have also simulated networks with Watts-Strogatz network architectures. A Watts-Strogatz network is generated by starting with a <italic>K</italic>-nearest neighbor network (such that fraction of non-zero connections each neuron makes is <italic>p</italic> = <italic>K</italic>/(<italic>N</italic> &#x2212; 1)) and rewiring a fraction <italic>&#x03B2;</italic> of those connections. The limit <italic>&#x03B2;</italic> = 0 remains a <italic>K</italic>-nearest neighbor network, while <italic>&#x03B2;</italic> &#x2192; 1 yields an Erd&#x0151;s-R&#x00E9;yni network. We have simulated networks on Watts-Strogatz architectures, finding similar results as for the Erd&#x0151;s-R&#x00E9;yni network, as seen in Fig. (S2). We generated the adjacency matrices of the Watts-Strogatz networks using code available in [<xref ref-type="bibr" rid="c3">3</xref>].</p>
<fig id="figS1" position="float" fig-type="figure">
<label>FIG. S1</label>
<caption><p>Same as Fig. (3) in main text, but for a Erd&#x0151;s-R&#x00E9;yni network architecture with Dale&#x2019;s principle imposed; i.e., all connections neuron <italic>j</italic> makes are positive if the neuron is excitatory (<inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline212.gif"/></alternatives></inline-formula> &#x003E; 0) or negative if the neuron is inhibitory (<inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline213.gif"/></alternatives></inline-formula> &#x003C; 0). The fraction of excitatory to inhibitory neurons is chosen to be 50&#x0025; on average. The resulting ratios are smaller than the corresponding ratios for Erd&#x0151;s-R&#x00e9;yni networks in which we do not impose Dale&#x2019;s principle, but the trends otherwise hold. The greater uncertainties on estimates are because random sampling of the full network may generate hidden networks with an unstable mean field theory (see main text for explanation). Theoretical estimates of the relative standard deviations were not calculated. Parameter values are given in Table SI.</p></caption>
<graphic xlink:href="105510_figS1.tif"/></fig>
<p>Parameter values used to generate our networks are given in Table SI.</p>
<fig id="figS2" position="float" fig-type="figure">
<label>FIG. S2</label>
<caption><p>Same as Fig. (3) in main text (Erd&#x0151;s-R&#x00E9;yni network, Dale&#x2019;s principle not imposed), but for a Watts-Strogatz network architecture. The resulting curves are only slightly smaller than for an Erd&#x0151;s-R&#x00E9;yni network. Theoretical estimates of the relative standard deviations were not calculated. Parameter values are given in Table SI.</p></caption>
<graphic xlink:href="105510_figS2.tif"/>
</fig>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>TABLE SI.</label>
<caption><p>Network connectivity parameter values.</p></caption>
<graphic xlink:href="105510_tblS1.tif"/></table-wrap>
</sec>
<sec id="s1c">
<title>C. Choice of nonlinearity <italic>&#x03D5;</italic>(<italic>x</italic>)</title>
<p>The nonlinear function <italic>&#x03D5;</italic>(<italic>x</italic>) sets the instantaneous firing rate for the neurons in our model. The &#x201C;canonical&#x201D; choice of nonlinearity for our network model is an exponential, <italic>&#x03D5;</italic>(<italic>x</italic>) = exp(<italic>x</italic>) [<xref ref-type="bibr" rid="c4">4</xref>&#x2013;<xref ref-type="bibr" rid="c7">7</xref>]. The exponential has particularly nice theoretical properties, but is also convenient for fitting this model to data, as the log-likelihood function of the model will be convex for the exponential (and some similar families of nonlinearity).</p>
<p>The fact that the exponential is unbounded is necessary to <italic>guarantee</italic> that a neuron spikes given enough input. A bounded nonlinearity imposes a maximum instantaneous firing rate, such that it is possible that the instantaneous rate saturates but does not guarantee the neuron will spike. The downside of an unbounded nonlinearity is that it is possible for the average firing rates to diverge, and the network never reaches a steady state. For example, in a purely excitatory network (all <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline68.gif"/></alternatives></inline-formula>) with an exponential nonlinearity, neural firing will run away without a sufficiently strong self-refractory coupling to suppress the firing rate. This will not occur with a bounded nonlinearity, as excitation can only drive neurons to fire at some maximum but finite rate.</p>
<p>This is a problem in simulations of networks obeying Dale&#x2019;s principle. For the exponential nonlinearity, the mean field theory for the hidden network occasionally does not exist due to an imbalance of excitatory and inhibitory neurons caused by our random recorded of neurons. However, the Dale&#x2019;s law network is stable if the nonlinearity is bounded. We demonstrate this below in Figs. S3 and S4, comparing simulations of Erd&#x0151;s-R&#x00E9;yni networks with and without Dale&#x2019;s principle for a sigmoidal nonlinearity <italic>&#x03D5;</italic>(<italic>x</italic>) = 2/(1 &#x002B; <italic>e<sup>&#x2212;x</sup></italic>).</p>
<fig id="figS3" position="float" fig-type="figure">
<label>FIG. S3</label>
<caption><p>Same as Fig. (3) in main text, but for a sigmoidal nonlinearity <italic>&#x03D5;</italic>(<italic>x</italic>) = 2/(1 &#x002B; <italic>e<sup>&#x2212;x</sup></italic>).</p></caption>
<graphic xlink:href="105510_figS3.tif"/>
</fig>
<fig id="figS4" position="float" fig-type="figure">
<label>FIG. S4</label>
<caption><p>Same as Fig. (S1) above, but for a sigmoidal nonlinearity <italic>&#x03D5;</italic>(<italic>x</italic>) = 2/(1 &#x002B; <italic>e<sup>&#x2212;x</sup></italic>). Because the sigmoid is bounded the mean field solution cannot diverge, yielding better results.</p></caption>
<graphic xlink:href="105510_figS4.tif"/>
</fig>
</sec>
<sec id="s1d">
<title>D. Derivation of effective baselines and coupling filters</title>
<p>To study how hidden neuron affects the inferred properties of recorded neurons, we partition the network into &#x201C;recorded&#x201D; neurons, labeled by indices <italic>r</italic> (with sub-or superscripts to differentiate different recorded neurons, e.g., <italic>r</italic> and <italic>r</italic>&#x2032; or <italic>r</italic><sub>1</sub> and <italic>r</italic><sub>2</sub>) and &#x201C;hidden&#x201D; neurons labeled by indices <italic>h</italic> (with sub-or superscripts). The rates of these two groups are thus
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn4.gif"/></alternatives></disp-formula></p>
<p>Because the set hidden neurons is generally much larger than the set of recorded neurons, we expect that we can approximate the input to the recorded neurons with the mean input, conditioned on the activity of the recorded neurons. That is, we can split the hidden input to the recorded neurons up into two terms, the mean plus fluctuations around the mean:
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn5.gif"/></alternatives></disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline69.gif"/></alternatives></inline-formula> denotes the mean activity of the hidden neurons conditioned on the activity of the recorded units, and <italic>&#x03BE;<sub>r</sub></italic>(<italic>t</italic>) are fluctuations around this mean, i.e., <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline70.gif"/></alternatives></inline-formula>). Note that <italic>&#x03BE;<sub>r</sub></italic>(<italic>t</italic>) is also conditional on the activity of the recorded units.</p>
<p>We can calculate the cross-correlation of the fluctuations,
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn6.gif"/></alternatives></disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline71.gif"/></alternatives></inline-formula> is the cross-correlation between hidden neurons <italic>h</italic><sub>1</sub> and <italic>h</italic><sub>2</sub>. If the autocorrelation of the fluctuations (<italic>r</italic> = <italic>r</italic>&#x2032;) is small compared to the mean input to the recorded neurons <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline72.gif"/></alternatives></inline-formula>, or if <italic>J<sub>r,h</sub></italic> is small, then we may neglect these fluctuations and focus only on the effects that the mean input has on the recorded sub-network. At the level of the mean field theory approximation we make in this work, the spike-train correlations are zero. One can calculate corrections to mean field theory (see below), but we leave investigation of the properties of these fluctuations for the focus of future studies. This will not affect our overall analysis, as non-negligible noise will not alter the form of the effective couplings between neurons, which are deterministic.</p>
<p>In order to calculate how hidden input shapes the activity of recorded neurons, we need to calculate the mean <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline72_1.gif"/></alternatives></inline-formula>. This mean input is difficult to calculate in general, especially when conditioned on the activity of the recorded neurons. In principle, the mean can be calculated as</p>
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn7.gif"/></alternatives></disp-formula>
<p>This is not a tractable calculation. Taylor series expanding the exponential reveals that the mean will depend on <italic>all</italic> higher cumulants of the hidden unit spike trains, which cannot in general be calculated explicitly. Instead, we again appeal to the fact that in a large, sufficiently connected network, we expect fluctuations to be small, as long as the network is not near a critical point. In this case, we may make a mean field approximation, which amounts to solving the self-consistent equation
<disp-formula id="eqnS2"><alternatives><graphic xlink:href="105510_eqnS2.gif"/></alternatives></disp-formula>
In general, this equation must be solved numerically. Unfortunately, the conditional dependence on the activity of the recorded neurons presents a problem, as in principle we must solve this equation for <italic>all possible patterns of recorded unit activity</italic>. Instead, we note that the mean hidden neuron firing rate is a <italic>functional</italic> of the filtered recorded input <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline73.gif"/></alternatives></inline-formula>, so we can expand it as a functional Taylor series (sometimes known as a Volterra series) around some reference filtered activity <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline74.gif"/></alternatives></inline-formula>
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn8.gif"/></alternatives></disp-formula></p>
<p>Within our mean field approximation, the Taylor coefficients are simply the response functions of the network &#x2014; i.e., the zeroth order coefficient is the mean firing rate of the neurons in the reference state <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline75.gif"/></alternatives></inline-formula>, the first order coefficient is the linear response function of the network, the second order coefficient is a nonlinear response function, and so on.</p>
<p>There are two natural choices for the reference state <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline76.gif"/></alternatives></inline-formula>. The first is simply the state of zero recorded unit activity, while the second is the mean activity of the recorded neurons. The zero-activity case conforms to the choice of GLM models used in practice, as one can view the linear filtering of spikes as the first order term in an expansion of a more general nonlinear filter. Choosing the mean activity as the reference state may be more appropriate if the recorded neurons have high firing rates, but requires adjusting the form of the GLM model so that firing rates are modulated by filtering deviations of spikes from the mean firing rate, rather than filtering the spikes themselves. In the main text, we focus on the zero-activity reference state. We present the formulation for the mean field reference state at the end of the SI.</p>
<p>For the zero-activity reference state <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline76a.gif"/></alternatives></inline-formula> = 0, the conditional mean is
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn9.gif"/></alternatives></disp-formula></p>
<p>The mean inputs <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline76_1.gif"/></alternatives></inline-formula> are the mean field approximations to the firing rates of the hidden neurons in the absence of the recorded neurons. Defining <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline76_2.gif"/></alternatives></inline-formula>, these firing rates are given by
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn10.gif"/></alternatives></disp-formula>
in writing this equation we have used the fact that the steady-state mean field firing rates will be time-independent, and hence the convolution <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline77.gif"/></alternatives></inline-formula>, where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline78.gif"/></alternatives></inline-formula>. The mean field equations for the <italic>&#x03BD;<sub>h</sub></italic> are a system of transcendental equations that in general cannot be solved exactly. In practice we will solve the equations numerically, but we can develop a series expansion for the solutions (see below).</p>
<p>The next term in the series expansion is the linear response function of the hidden unit network, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline79.gif"/></alternatives></inline-formula> given by</p>
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn11.gif"/></alternatives></disp-formula>
<p>The &#x201C;gain&#x201D; <italic>&#x03B3;<sub>h</sub></italic> is defined by
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn12.gif"/></alternatives></disp-formula>
where <italic>&#x03D5;&#x2032;</italic>(<italic>x</italic>) is the derivative of the nonlinearity with respect to its argument.</p>
<p>We may solve for &#x0393;<sub><italic>h,h</italic>&#x2032;</sub> (<italic>t</italic> &#x2013; <italic>t</italic>&#x2032;) by first converting to the frequency domain and performing a matrix inverse:
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn13.gif"/></alternatives></disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline80.gif"/></alternatives></inline-formula> is the output-linear response function, computed by performing a matrix inverse.</p>
<p>If <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline81.gif"/></alternatives></inline-formula> for some matrix norm &#x007C;&#x007C; &#x00B7; &#x007C;&#x007C;, then the matrix <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline82.gif"/></alternatives></inline-formula> admits a convergent series expansion,
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn14.gif"/></alternatives></disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline83.gif"/></alternatives></inline-formula> is a matrix product and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline84.gif"/></alternatives></inline-formula>. We can write an element of the matrix product out as
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn15.gif"/></alternatives></disp-formula>
inserting <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline85.gif"/></alternatives></inline-formula> yields</p>
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn16.gif"/></alternatives></disp-formula>
<p>It is this expression that we interpret in terms of summing over paths through network of hidden neurons that join two observed neurons: the <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline86.gif"/></alternatives></inline-formula> are contributed by edges from neuron <italic>h<sub>j</sub></italic> to <italic>h<sub>i</sub></italic>, and the <italic>&#x03B3;<sub>h<sub>i</sub></sub></italic> are contributed by the nodes. In this expansion, we allow edges from one neuron back to itself, meaning we include paths in which signals loop back around to the same neuron arbitrarily many times before the signal is propagated further. Such loops can be easily factored, contributing a factor <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline87.gif"/></alternatives></inline-formula>. We can thus remove the need to consider self-loops in our rules for calculating effective coupling filters by assigning a factor <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline87_1.gif"/></alternatives></inline-formula> to each node, rather than just <italic>&#x03B3;<sub>h</sub></italic>.</p>
<p>This result can be derived directly. Let us decompose the matrix <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline88.gif"/></alternatives></inline-formula> in a diagonal and off-diagonal piece, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline89.gif"/></alternatives></inline-formula>. Then,
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn17.gif"/></alternatives></disp-formula></p>
<p>We assumed that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline90.gif"/></alternatives></inline-formula> is invertible, which requires that there is no element for which <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline91.gif"/></alternatives></inline-formula> for all <italic>&#x03C9;</italic>. Assuming this is the case, the inverse of the matrix is trivial to calculate, as it is diagonal:
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn18.gif"/></alternatives></disp-formula></p>
<p>Hence, inserting the contribution from the factor <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline92.gif"/></alternatives></inline-formula> that we pulled out, and the factor <italic>&#x03BD;<sub>h&#x2032;</sub></italic> that left-multiplies <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline93.gif"/></alternatives></inline-formula> to give <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline94.gif"/></alternatives></inline-formula>, we have
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn19.gif"/></alternatives></disp-formula></p>
<p>This is the same as our previous expression, with <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline95.gif"/></alternatives></inline-formula>) and restricting the sum over hidden units such that self-loops are removed (<italic>h<sub>i</sub></italic> &#x2260; <italic>h<sub>i&#x002B;1</sub></italic>), proving the result described in the main text. We note again that this puts restrictions on the allowed size of self-interactions, as the zeros of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline96.gif"/></alternatives></inline-formula> must be in the upper-half plane of the complex <italic>&#x03C9;</italic> plane in order for the filters to be causal and physically meaningful. The complete expression for the correction term <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline97.gif"/></alternatives></inline-formula> is thus
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn20.gif"/></alternatives></disp-formula></p>
<p>This is the exact mathematical expression underlying the rules given in the main text.</p>
<p>We have now determined the zero and first order Taylor series coefficients in our expansion of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline98.gif"/></alternatives></inline-formula>. If these are the dominant terms, i.e., if we may neglect higher order terms in this expansion, we may approximate the instantaneous firing rates of the recorded neurons by
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn21.gif"/></alternatives></disp-formula>
where
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn22.gif"/></alternatives></disp-formula>
are the effective baselines of the recorded neurons and
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn23.gif"/></alternatives></disp-formula>
are the effective coupling filters in the frequency domain, as given in the main text. In addition to neglecting the higher order spike filtering terms here, we have also neglected fluctuations around the mean input from the hidden network. These fluctuations are zero within our mean field approximation, but we could in principle calculate corrections to the mean field predictions using the results of [<xref ref-type="bibr" rid="c8">8</xref>]. Below, we use these methods to calculate the tree-level corrections to the correlation functions of the spikes and estimate the size of these fluctuations.</p>
</sec>
<sec id="s1e">
<title>E. Specific choices of network properties used to generate <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref></title>
<p>To generate the results in <xref ref-type="fig" rid="fig2">Fig. 2</xref> in the main text, we choose the coupling filters to be <italic>J<sub>i,j</sub></italic> (<italic>t</italic>) = <italic>J<sub>i,j</sub> &#x03B1;</italic><sup>2</sup><italic>te<sup>&#x2212;&#x03B1;t</sup></italic>, which has Fourier transform
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn24.gif"/></alternatives></disp-formula>
using the Fourier convention
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn25.gif"/></alternatives></disp-formula></p>
<fig id="fig2" position="float" fig-type="figure">
<label>FIG. 2.</label>
<caption><p>Effective coupling filters <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline39.gif"/></alternatives></inline-formula> (solid blue) versus true coupling filters (dashed red) for <italic>N<sub>rec</sub></italic> = 3 recorded neurons in a network of <italic>N</italic> = 1000 total neurons. The simulated network has an Erd&#x0151;s-R&#x00E9;yni connectivity with sparsity <italic>p</italic> = 0.2 and normally distributed non-zero weights with zero mean and standard deviation <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline40.gif"/></alternatives></inline-formula> (strong coupling). All self-couplings are set to zero (indicated by all true coupling filters being zero in plots along grid diagonal).</p></caption>
<graphic xlink:href="105510_fig2.tif"/>
</fig>
<fig id="fig3" position="float" fig-type="figure">
<label>FIG. 3.</label>
<caption><p>Solid lines:Numerical estimates of the standard deviation of the difference between effective coupling weights <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline49.gif"/></alternatives></inline-formula> and true coupling weights <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline34a.gif"/></alternatives></inline-formula> between neurons <italic>r</italic> &#x2260; <italic>r&#x2032;</italic>, normalized by the standard deviation of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline34b.gif"/></alternatives></inline-formula>. These estimates account for all paths through hidden neurons. For weak 1/<italic>N</italic> coupling (red), the ratio of standard deviations is <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline50.gif"/></alternatives></inline-formula>. For strong <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline51.gif"/></alternatives></inline-formula> coupling (blue) the ratio is <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline52.gif"/></alternatives></inline-formula> and grows in strength as the fraction of recorded neurons <italic>N<sub>rec</sub></italic>/<italic>N</italic> decreases or the typical synaptic strength <italic>J</italic><sub>0</sub> increases. Dashed black lines show theoretical estimates of the relative standard deviations accounting only for hidden paths of length-3 connecting recorded neurons. Deviations from the length-3 prediction at small <italic>f</italic> and large <italic>J</italic><sub>0</sub> indicate that contributions from circuit paths involving many hidden neurons are significant in these regimes. Parameter values are given in Table SI in the Supplementary Information.</p></caption>
<graphic xlink:href="105510_fig3.tif"/>
</fig>
<p>The weight matrix <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline112.gif"/></alternatives></inline-formula> is generated as described in &#x201C;Model network architectures,&#x201D; choosing <italic>J</italic><sub>0</sub> = 1.0. We partition this network up into recorded and hidden subsets. For a network of <italic>N</italic> neurons, we choose neurons 1 to <italic>N</italic><sub>rec</sub> to be recorded, and the remainder to be hidden, hence we define (using an index notation starting at 1; indices should be subtracted by 1 for 0-based index counting)
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn26.gif"/></alternatives></disp-formula>
and
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn27.gif"/></alternatives></disp-formula></p>
<p>We numerically calculate the linear response matrix <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline99.gif"/></alternatives></inline-formula> by evaluating
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn28.gif"/></alternatives></disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline100.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline101.gif"/></alternatives></inline-formula> is an <italic>N</italic><sub>hid</sub> &#x00D7; <italic>N</italic><sub>hid</sub> diagonal matrix with elements <italic>&#x03B3;<sub>h</sub></italic>.</p>
<p>The effective coupling filter in the frequency domain can then be evaluated pointwise at a desired set of frequencies <italic>&#x03C9;</italic> by matrix multiplication,
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn29.gif"/></alternatives></disp-formula></p>
<p>We then return to the time domain by inverse Fourier transforming the result, achieved by treating <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline102.gif"/></alternatives></inline-formula> as an <italic>N</italic><sub>rec</sub> &#x00D7; <italic>N</italic><sub>rec</sub> &#x00D7; <italic>N</italic><sub>freq</sub> array (where <italic>N</italic><sub>freq</sub> the number of frequencies at which we evaluate the effective coupling) and multiplying along the frequency dimension by an <italic>N</italic><sub>freq</sub> &#x00D7; <italic>N</italic><sub>time</sub> matrix <bold>E</bold> with elements <italic>E<sub>&#x03C9;,t</sub></italic> = exp(<italic>i&#x03C9;t</italic>)&#x2206;<italic>t</italic>/(2<italic>&#x03C0;</italic>), for <italic>N</italic><sub>time</sub> sufficiently small time bins of size <italic>&#x03B4;t</italic> = 0.1/<italic>&#x03B1;</italic>, for <italic>&#x03B1;</italic> = 10, as listed in Table SII.</p>
<p>To generate <xref ref-type="fig" rid="fig3">Fig. 3</xref>, we focus on the zero-frequency component of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline103.gif"/></alternatives></inline-formula>, which is also equal to the time integral of <bold>J</bold><sup>eff</sup>(<italic>t</italic>). As in the main text, we label this elements of this component <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline104.gif"/></alternatives></inline-formula>, which is equal to
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn30.gif"/></alternatives></disp-formula></p>
<p>We do not need to simulate the full network to study the statistics of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline105.gif"/></alternatives></inline-formula>. We only need to generate samples of the matrix <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline106.gif"/></alternatives></inline-formula> and evaluate <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline107.gif"/></alternatives></inline-formula>. This is where the choice of an Erd&#x0151;s-R&#x00E9;yni network that is not restricted to obey Dale&#x2019;s principle becomes convenient. Because the weights <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline108.gif"/></alternatives></inline-formula> are <italic>i.i.d.</italic> and the sign of the weight is random, population averages will be equivalent to expected values. i.e., the sample mean
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn31.gif"/></alternatives></disp-formula>
and sample variance
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn32.gif"/></alternatives></disp-formula>
will tend to the expected values <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline109.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline110.gif"/></alternatives></inline-formula> for large networks. We have explicitly removed the diagonal elements from these averages because these elements will have slightly different statistics from the off-diagonal elements due to the fact that all ground-truth self-couplings are set to zero, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline111.gif"/></alternatives></inline-formula>. This allows us to compare the population variance, plotted in <xref ref-type="fig" rid="fig3">Fig. 3</xref> (after normalization by the population variance of the true off-diagonal weights), to the expected variance calculated analytically below.</p>
<p>The error bars in <xref ref-type="fig" rid="fig3">Fig. 3</xref> are generated by first drawing a single sample of true weights <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline112a.gif"/></alternatives></inline-formula>, and then taking 100 random subsets of <italic>N</italic><sub>rec</sub> = &#x007B;10, 110, 210, 310, 410, 510, 610, 710, 810, 910, 999&#x007D; recorded neurons. For this analysis, random subsets were generated by permuting the indices of the full weight matrix <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline112b.gif"/></alternatives></inline-formula> and taking the last <italic>N</italic><sub>rec</sub> neurons to be recorded. For each random subset of the network we calculate the population statistics. The standard error of, for example, the population variance <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline113.gif"/></alternatives></inline-formula> across subsets gives an estimate of the error. However, this estimate is conditioned on the full network structure, for which we only have a single sample so far. To average over the effects of global network architecture, we draw a total of 10 network architecture samples, and average a second time over these samples to obtain our final estimates of the population variance of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline114.gif"/></alternatives></inline-formula>. We note that for an Erd&#x0151;s-R&#x00E9;yni network, this second stage of averaging is probabilistically unnecessary: for a large enough network random subsets of a single large network are statistically identical to random subsets drawn from several samples of full Erd&#x0151;s-R&#x00E9;yni networks. However, this will not be true for networks with more structure, such as Watts-Strogatz networks or networks that obey Dale&#x2019;s principle, and this second stage of averaging over the global network architecture is necessary in these cases.</p>
</sec>
<sec id="s1f">
<title>F. Derivation of a series approximation for the mean field firing rates (exponential nonlinearity)</title>
<p>The mean field firing rates for the hidden neurons are given by
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn33.gif"/></alternatives></disp-formula>
where we focus specifically on the case of exponential nonlinearity <italic>&#x03D5;</italic>(<italic>x</italic>) = exp(<italic>x</italic>). For this choice of nonlinearity, <italic>&#x03B3;<sub>h</sub></italic> = <italic>&#x03BD;<sub>h</sub></italic>, so our series for the mean field firing rates is also automatically a series for the gains.</p>
<p>This system of transcendental equations generally cannot be solved analytically. However, for small exp(<italic>&#x00B5;<sub>h</sub></italic>) <italic>&#x226A;</italic> 1 we can derive, recursively, a series expansion for the firing rates. We first consider the case of <italic>&#x00B5;<sub>h</sub></italic> = <italic>&#x00B5;</italic><sub>0</sub> for all hidden neurons <italic>h</italic>. Let &#x03F5; = exp(<italic>&#x00B5;</italic><sub>0</sub>). We may then write
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn34.gif"/></alternatives></disp-formula></p>
<p>Plugging this into the mean field equation,
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn35.gif"/></alternatives></disp-formula>
Thus
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn36.gif"/></alternatives></disp-formula></p>
<p>For <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline115.gif"/></alternatives></inline-formula>, it follows immediately that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline116.gif"/></alternatives></inline-formula>. Then, for <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline117.gif"/></alternatives></inline-formula>, the sum in <italic>m</italic> truncates at <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline118.gif"/></alternatives></inline-formula> is zero for <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline119.gif"/></alternatives></inline-formula>, as all indices are positive). Thus,
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn37.gif"/></alternatives></disp-formula>
For <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline120.gif"/></alternatives></inline-formula>
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn38.gif"/></alternatives></disp-formula></p>
<p>For <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline121.gif"/></alternatives></inline-formula>,
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn39.gif"/></alternatives></disp-formula></p>
<p>With this we have calculated the firing rates to <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline122.gif"/></alternatives></inline-formula>.</p>
<p>The analysis can be straightforwardly extended to the case of heterogeneous <italic>&#x00B5;<sub>h</sub></italic>, though it becomes more tedious to compute terms in the (now multivariate) series. Assuming &#x03F5;<sub><italic>h</italic></sub> &#x2261; exp(<italic>&#x00B5;h</italic>) &#x226A; 1 for all <italic>h</italic>, to <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline123.gif"/></alternatives></inline-formula> we find
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn40.gif"/></alternatives></disp-formula></p>
</sec>
<sec id="s1g">
<title>G. Variance of the effective coupling to second order in <italic>N</italic><sub>rec</sub>/<italic>N</italic> &#x0026; fourth order in <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline124.gif"/></alternatives></inline-formula> (exponential nonlinearity)</title>
<p>To estimate the strength of the hidden paths, we would like to calculate the variance of the effective coupling <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline105a.gif"/></alternatives></inline-formula> and compare its strength to the variance of the direct couplings <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline125.gif"/></alternatives></inline-formula>, where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline126.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline127.gif"/></alternatives></inline-formula>, as in the main text.</p>
<p>We assume that the synaptic weights <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline128.gif"/></alternatives></inline-formula> are independently and identically distributed with zero mean and variance <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline129.gif"/></alternatives></inline-formula> for <italic>i</italic> &#x2260; <italic>j</italic>, where <italic>a</italic> = 1 corresponds to weak coupling and <italic>a</italic> = 1/2 corresponds to strong coupling. We assume no self-couplings, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline130.gif"/></alternatives></inline-formula> for all neurons <italic>i</italic>. The overall factor of <italic>p</italic> in <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline131.gif"/></alternatives></inline-formula> comes from the sparsity of the network. For example, for normally distributed non-zero weights with variance <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline132.gif"/></alternatives></inline-formula>, the total probability for every connection in the network is
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn41.gif"/></alternatives></disp-formula></p>
<p>Because the <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline133.gif"/></alternatives></inline-formula> are <italic>i.i.d.</italic>, the mean of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline134.gif"/></alternatives></inline-formula> is zero:
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn42.gif"/></alternatives></disp-formula>
where we used the fact that &#x0393;<italic><sub>h,h&#x2032;</sub></italic> depends only on the hidden neuron couplings <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline135.gif"/></alternatives></inline-formula>, which are independent of the couplings to the recorded neurons, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline136.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline137.gif"/></alternatives></inline-formula>.</p>
<p>The variance of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline138.gif"/></alternatives></inline-formula> is thus equal to the mean of its square, for <italic>r</italic> &#x2260; <italic>r&#x2032;</italic>,
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn43.gif"/></alternatives></disp-formula></p>
<p>In this derivation, we used the fact that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline139.gif"/></alternatives></inline-formula> due to the fact that the synaptic weights are uncorrelated. We now need to compute <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline140.gif"/></alternatives></inline-formula>. This is intractable in general, so we will resort to calculating this in a series expansion in powers of &#x2261; exp(<italic>&#x00B5;</italic><sub>0</sub>) for the exponential nonlinearity model. Our result will also turn out to be an expansion in powers of <italic>J</italic><sub>0</sub> and 1 &#x2212; <italic>f</italic> &#x2261; <italic>N</italic><sub>hid</sub>/<italic>N</italic>. Because we are explicitly using the exponential nonlinearity, <italic>&#x03B3;<sub>h</sub></italic> = <italic>&#x03BD;<sub>h</sub></italic>, so we do not need to derive a series for <italic>&#x03B3;<sub>h</sub></italic> in powers of in order to perform this calculation.</p>
<p>The lowest order approximation is obtained by the approximation <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline141.gif"/></alternatives></inline-formula> yielding
<disp-formula id="eqnS3"><alternatives><graphic xlink:href="105510_eqnS3.gif"/></alternatives></disp-formula></p>
<p>This result varies linearly with <italic>f</italic>, while numerical evaluation of the variance shows obvious curvature for <italic>f</italic> &#x226A; 1 and <italic>J</italic><sub>0</sub> &#x2272; 1, so we need to go to higher order. This becomes tedious very quickly, so we will only work to <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline142.gif"/></alternatives></inline-formula> (it turns out <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline143.gif"/></alternatives></inline-formula> corrections vanish).</p>
<p>We calculate <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline144.gif"/></alternatives></inline-formula> using a recursive strategy, rather than the formal series solution. That is, we begin with the expression
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn44.gif"/></alternatives></disp-formula>
and plug it into itself until we obtain an expression to a desired order in. In doing so, we note that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline145.gif"/></alternatives></inline-formula>, so we will first work to fourth order in <italic>&#x03BD;<sub>h</sub></italic>, and then plug in the series for <italic>&#x03BD;<sub>h</sub></italic> in powers of <italic>&#x03B5;</italic>.</p>
<p>We begin with
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn45.gif"/></alternatives></disp-formula></p>
<p>The third order term <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline146.gif"/></alternatives></inline-formula> vanished because we assume no self-couplings. We have obtained <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline147.gif"/></alternatives></inline-formula> to fourth order in <italic>&#x03BD;<sub>h</sub></italic>; now we need to plug in the series expression for <italic>&#x03BD;<sub>h</sub></italic> to obtain the series in powers of <italic>&#x03BB;</italic><sub>0</sub><italic>&#x03B5;</italic>. We will do this order by order in <italic>&#x03BD;<sub>h</sub></italic>. The easiest terms are the fourth order terms, as</p>
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn46.gif"/></alternatives></disp-formula>
<p>The second order term is
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn47.gif"/></alternatives></disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline148.gif"/></alternatives></inline-formula>. We need the average <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline149.gif"/></alternatives></inline-formula>. The third-order term will vanish upon averaging, and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline150.gif"/></alternatives></inline-formula>; the first term vanished because matching indices it requires <italic>h</italic> = <italic>h</italic><sub>1</sub> and and <italic>h</italic><sub>1</sub> = <italic>h</italic><sub>2</sub>, giving <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline151.gif"/></alternatives></inline-formula> because there are no self-couplings. We thus obtain</p>
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn48.gif"/></alternatives></disp-formula>
<p>We thus arrive at
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn49.gif"/></alternatives></disp-formula>
the factor 1 &#x2212; <italic>&#x03B4;</italic><sub><italic>h,h&#x2032;</italic></sub> on the last term on the second line accounts for the fact that it does not contribute when <italic>h</italic> = <italic>h&#x2032;</italic>, as <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline152.gif"/></alternatives></inline-formula>. Putting everything together,
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn50.gif"/></alternatives></disp-formula></p>
<p>For weak coupling, this tends to 1 in the <italic>N</italic> &#x226B; 1 limit. For strong coupling, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline153_1.gif"/></alternatives></inline-formula>, and hence
<disp-formula id="eqnS4"><alternatives><graphic xlink:href="105510_eqnS4.gif"/></alternatives></disp-formula>
where we have used little-<italic>o</italic> notation to denote that there are higher order terms dominated by (<italic>&#x03BB;</italic><sub>0</sub><italic>J</italic><sub>0</sub>&#x03F5;)<sub>4</sub>(1 &#x2212; <italic>f</italic>)<sub>2</sub>. With this expression, we have improved on our approximation of the relative variance of the effective coupling to the true coupling; however, the neglected higher order terms still become significant as <italic>f</italic> &#x2192; 0 and <italic>J</italic><sub>0</sub> &#x2192; 1, indicating that hidden paths have a significant impact when synaptic strengths are moderately strong and only a small fraction of the neurons have been observed.</p>
<p>Because the synaptic weights <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline108a.gif"/></alternatives></inline-formula> are independent, we may rewrite <xref ref-type="disp-formula" rid="eqnS4">Eq. (S.4)</xref> as
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn51.gif"/></alternatives></disp-formula>
or, in terms of the ratio of standard deviations,
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn52.gif"/></alternatives></disp-formula>
where we used the approximation <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline154.gif"/></alternatives></inline-formula> for <italic>x</italic> small.</p>
<p>In the main text, we plotted results for <italic>N</italic> = 1000 total neurons (<xref ref-type="fig" rid="fig3">Fig. 3</xref>). For strongly coupled networks, the results should only depend on the fraction of observed neurons, <italic>f</italic> = <italic>N</italic>rec/<italic>N</italic>, while for weak coupling the results do depend on the absolute number <italic>N</italic>. To demonstrate these, in <xref ref-type="fig" rid="figS5">Fig. S5</xref> we remake <xref ref-type="fig" rid="fig3">Fig. 3</xref> for <italic>N</italic> = 100 neurons. We see that the strongly coupled results have not been altered, whereas the weakly coupled results yield stronger deviations (as the deviations are <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline155.gif"/></alternatives></inline-formula>).</p>
<fig id="figS5" position="float" fig-type="figure">
<label>FIG. S5</label>
<caption><p>Same as <xref ref-type="fig" rid="fig3">Fig. 3</xref>, but for <italic>N</italic> = 100 neurons and <italic>N</italic><sub>rec</sub> = &#x007B;1, 11, 21, 31, 41, 51, 61, 71, 81, 91, 99&#x007D; recorded neurons. Because we plot the relative deviations of the coupling strength against the fraction of observed neurons, the curves for the strongly coupled case are the same as for <italic>N</italic> = 1000, as expected, while the weakly coupled case yields stronger deviations.</p></caption>
<graphic xlink:href="105510_figS5.tif"/>
</fig>
</sec>
<sec id="s1h">
<title>H. Estimating the error from neglecting higher order spike filtering (exponential nonlinearity)</title>
<p>In the main text we calculate corrections to the baseline and linear spike filters, neglecting higher-order spike filtering and fluctuations around the mean input to the recorded neurons. We would like to know when these approximations are valid. We will do so within mean field theory (meaning the noise fluctuations contribute zero error as they do not contribute to the mean field approximation). Specifically, we will assume that the quadratic spike filtering term is small, and calculate the corresponding correction to our mean field approximation of the firing rates when this term is completely neglected. If we take as our approximation of the recorded neuron firing instantaneous firing rates
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn53.gif"/></alternatives></disp-formula>
then the mean field approximation of the firing rates is
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn54.gif"/></alternatives></disp-formula>
where we have used the fact that the average firing rates are independent of time, and replaced <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline156.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline157.gif"/></alternatives></inline-formula> with their time integrals, denoted by <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline158.gif"/></alternatives></inline-formula>. The parameter <italic>b</italic> is just a book-keeping parameter.</p>
<p>To calculate the lowest order correction to the linear filtering approximation (<italic>b</italic> &#x2192; 0), we write <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline159.gif"/></alternatives></inline-formula>, treating <italic>b</italic> formally as a small parameter. The linear firing rate <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline208.gif"/></alternatives></inline-formula> is given by
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn55.gif"/></alternatives></disp-formula></p>
<p>For the quadratically-modified firing rates, keeping terms only to linear order in <italic>b</italic>,
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn56.gif"/></alternatives></disp-formula></p>
<p>Collecting on <italic>b</italic> and rearranging,
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn57.gif"/></alternatives></disp-formula></p>
<p>Because <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline160.gif"/></alternatives></inline-formula>, the expansion parameters we have been using, the lowest order approximation for <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline161.gif"/></alternatives></inline-formula> is
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn58.gif"/></alternatives></disp-formula></p>
<p>The coefficient <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline162.gif"/></alternatives></inline-formula> is the amplitude of the quadratic spike filter. The expression, which we calculate later in the SI, is
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn59.gif"/></alternatives></disp-formula>
and hence <disp-formula><alternatives><graphic xlink:href="105510_Ueqn60.gif"/></alternatives></disp-formula></p>
<p>To lowest order the error term <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline163.gif"/></alternatives></inline-formula> is
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn61.gif"/></alternatives></disp-formula></p>
<p>For <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline23.gif"/></alternatives></inline-formula> <italic>i.i.d.</italic>, the population average should converge to the expected value, which is zero because the <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline23a.gif"/></alternatives></inline-formula> have mean zero. We can calculate the root-mean-squared-error by looking at the variance:
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn62.gif"/></alternatives></disp-formula></p>
<p>In principle, we should take care to separate out the <italic>r</italic><sub>1</sub> &#x2260; <italic>r</italic><sub>2</sub> and <italic>r</italic><sub>1</sub> = <italic>r</italic><sub>2</sub> terms from the sum, as the latter will contribute a factor <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline23b.gif"/></alternatives></inline-formula> which we have not specified yet (though one could calculate for specific choices, such as the normal distribution that we use in practice). However, for both <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline165.gif"/></alternatives></inline-formula> will scale as <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline166.gif"/></alternatives></inline-formula>, so we will neglect constant factors and simply use this scaling to arrive at the result
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn63.gif"/></alternatives></disp-formula></p>
<p>If we take <italic>N &#x2192; &#x221E;</italic> with <italic>N</italic><sub>rec</sub> = <italic>f N</italic> and <italic>N</italic><sub>hid</sub> = (1 <italic>&#x2212; f</italic>)<italic>N</italic> for <italic>f</italic> fixed, the root mean squared error (RMSE) scales as
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn64.gif"/></alternatives></disp-formula></p>
<p>For <italic>a</italic> = 1 (weak coupling), the error falls off quite quickly as <italic>N</italic><sup>3/2</sup>, while it is independent of <italic>N</italic> for <italic>a</italic> = 1/2 (strong coupling). However, the error does still scale with the fraction of observed neurons, as <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline167.gif"/></alternatives></inline-formula>. This demonstrates that the typical error that arises from neglecting the nonlinear filtering is small both when most neurons have been observed (<italic>f</italic> &#x2272; 1) and when very few neurons have been observed (<italic>f</italic> &#x2273; 0). While it may at first seem surprising that the error is small when very few neurons have been observed, the result does make intuitive sense: when a very small fraction of the network is observed, we can treat the unobserved portion of the network as a &#x201C;reservoir&#x201D; or &#x201C;bath.&#x201D; Feedback from the observed neurons into the reservoir has a comparatively small effect, so we can get away with neglecting feedback between the observed and unobserved partitions of the network. However, when the number of observed neurons is comparable to the number of unobserved neurons, neither can be treated as a reservoir, and feedback between the two partitions is substantial. Neglecting the higher order spike filter terms may be inaccurate in this case. We check this numerically below.</p>
</sec>
<sec id="s1i">
<title>I. Validating the mean field approximation and linear conditional rate approximation via direct simulations of network activity (exponential nonlinearity)</title>
<p>The results presented in the main text are based on analytical calculations or numerical analyses using analytically derived formulas. For example, the statistics of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline168.gif"/></alternatives></inline-formula> are calculated based on our expression <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline169.gif"/></alternatives></inline-formula> can be calculated by solving the matrix equation
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn65.gif"/></alternatives></disp-formula></p>
<p>Generating these results does not require a simulating the full network, so we check here that our approximations indeed agree with the results of full network simulations.</p>
<p>We check of validity of two main results: 1) that mean field theory is an accurate approximation for the parameters we consider, and 2) that our truncation of the conditional hidden firing rates <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline170.gif"/></alternatives></inline-formula> at linear order in <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline171.gif"/></alternatives></inline-formula> is valid.</p>
<p>We first discuss some basic details of the simulation. The simulation code we use is a modification of the code used in [<xref ref-type="bibr" rid="c8">8</xref>], written by Gabe Ocker; refer to this paper for full details of the simulation.</p>
<p>The main changes we made are considering exponential nonlinearities and synaptic weights drawn from normal or lognormal distributions.</p>
<p>As in [<xref ref-type="bibr" rid="c8">8</xref>], we choose the coupling filters to follow an alpha function
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn66.gif"/></alternatives></disp-formula></p>
<p>The Heaviside step function &#x0398;(<italic>t</italic>) enforces causality of the filter, using the convention &#x0398;(0) = 0. All neurons have the same time constant 1/<italic>&#x03B1;</italic>.</p>
<p>To efficiently simulate this network the code computes the synaptic variable <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline172.gif"/></alternatives></inline-formula> not by direct convolution but by solving the inhomogeneous system of differential equations, setting <italic>x</italic>(<italic>t</italic>) = <italic>s</italic>(<italic>t</italic>) and <italic>y</italic>(<italic>t</italic>) = <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline207.gif"/></alternatives></inline-formula>
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn67.gif"/></alternatives></disp-formula></p>
<p>The instantaneous firing rates of the neurons can in this way be quickly computed in time steps of a specified size &#x2206;<italic>t</italic>. The number of spikes <italic>n<sub>i</sub></italic> that neuron <italic>i</italic> fires in the <italic>t</italic><sup>th</sup> time bin is drawn from a Poisson distribution with probability <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline173.gif"/></alternatives></inline-formula>. An initial transient period of spiking before the network achieves a steady state is discarded.</p>
<p>The parameters we use in our simulations of the full network are given in Table SII.</p>
<table-wrap id="tblS2" orientation="portrait" position="float"><caption><p>TABLE SII.Network activity simulation parameter values.</p></caption>
<graphic xlink:href="105510_tblS2.tif"/></table-wrap>
<sec id="s2j1">
<label>1.</label>
<title>Verifying the mean field approximation</title>
<p>To confirm that the mean field approximation is valid, we seek to compare the empirically measured spike rates measured from simulations of the network activity to the calculated mean field rates. The empirical rates are measured as
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn68.gif"/></alternatives></disp-formula>
calculated after discarding the initial transient period of firing, for any neuron <italic>i</italic> (recorded or hidden). The steady-state mean field firing rates are the solutions of the transcendental equation
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn69.gif"/></alternatives></disp-formula></p>
<p>The only difference between this equation and the equation for <italic>&#x03BD;<sub>h</sub></italic> is that the neuron indices are not restricted to hidden units. i.e., the <italic>&#x03BD;<sub>h</sub></italic> are the mean field rates for the hidden neurons <italic>only</italic> (recorded neurons removed entirely), whereas the <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline174.gif"/></alternatives></inline-formula> are the mean field rates for the entire network. If the mean field approximation is valid, the empirical rates should be approximately equal to the mean field rates, so a scatter plot of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline175.gif"/></alternatives></inline-formula> should roughly lie along the identity line. We test this for a network in the strong coupling limit <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline176.gif"/></alternatives></inline-formula> for four values of <italic>J</italic><sub>0</sub>, <italic>J</italic><sub>0</sub> = 0.25, 0.5, 0.75, and 1.0. We expect <italic>J</italic><sub>0</sub> = 1.0 to be close to the stability threshold of the model based on a linearized analysis [<xref ref-type="bibr" rid="c9">9</xref>,<xref ref-type="bibr" rid="c10">10</xref>]; i.e., for <italic>J</italic><sub>0</sub> &#x2273; 1.0 there may not be a steady state, so this may be where we expect the mean field approximation to break down. As seen in <xref ref-type="fig" rid="figS6">Fig. S6</xref>, the mean field approximation appears to hold well even up to <italic>J</italic><sub>0</sub> = 1.0, though there are some slight deviations for neurons with large rates.</p>
<fig id="figS6" position="float" fig-type="figure">
<label>FIG. S6</label>
<caption><p>Empirical estimates of average neuron firing rates from simulations plotted against mean firing rates predicted by mean field theory. The fact that the data lies along the identity line demonstrates validity of MFT up to <italic>J</italic><sub>0</sub> = 1.0.</p></caption>
<graphic xlink:href="105510_figS6.tif"/>
</fig>
<sec id="s2j1a">
<label>1.</label>
<title>Verifying the linearized conditional mean approximation</title>
<p>Having verified that the mean field approximation is valid, we now seek to check our linearized approximation of the firing rates of the hidden neurons <italic>conditioned on the activity of the recorded neurons</italic>, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline189.gif"/></alternatives></inline-formula>. That is, we calculated above that
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn70.gif"/></alternatives></disp-formula>
the &#x2026; correspond to higher order spike filtering terms that we have neglected in our analyses, assuming them to be small. In an earlier calculation above, we estimated that the error incurred by neglecting higher order spike filtering is of the order <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline177.gif"/></alternatives></inline-formula>, but we would like to confirm the negligibility of the higher order coupling through simulations.</p>
<p>To do so, we compare the empirical firing rates of the designated &#x201C;hidden&#x201D; neurons obtained from simulations of the full network models with the approximation of the firing rates of the hidden neurons conditioned on the recorded neurons using the linear expansion, averaged over recorded neuron activity to give
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn71.gif"/></alternatives></disp-formula>
where as usual the zero-frequency component of the linear response function <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline178.gif"/></alternatives></inline-formula> of the hidden neurons is calculated in the absence of recorded neurons.</p>
<p>If we plot a scatter plot of this against the empirical estimates of the hidden neurons, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline179.gif"/></alternatives></inline-formula>, the data points will lie along the identity line if our approximation is valid. If the data deviates from the identity line, it indicates that the neglected higher-order filtering terms contribute substantially to the firing rates of the neurons. It is possible that the zeroth order rate approximation, <italic>&#x03BD;<sub>h</sub></italic>, would be sufficient to describe the data, so we compare the empirical rates to both <italic>&#x03BD;<sub>h</sub></italic> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline180.gif"/></alternatives></inline-formula>.</p>
<p>As in the mean field approximation test, we focused on a strongly coupled network with <italic>J</italic><sub>0</sub> = 0.25, 0.5, 0.75, and 1.0. Because our analytical estimate of the error suggests small error for small fractions of recorded neurons and larger error when <italic>N</italic><sub>rec</sub> &#x223C;<italic>N</italic><sub>hid</sub>, we check both <italic>N</italic><sub>rec</sub> = 100 neurons out of <italic>N</italic> = 1000 neurons (<italic>f</italic> = 0.1) in <xref ref-type="fig" rid="figS7">Fig. S7</xref> and <italic>N</italic><sub>rec</sub> = 500 neurons out of <italic>N</italic> = 1000 (<italic>f</italic> = 0.5) in <xref ref-type="fig" rid="figS8">Fig. S8</xref>.</p>
<p>For each value of <italic>J<sub>0</sub></italic>, we present two plots: the empirical rates versus the mean field rates <italic>&#x03BD;<sub>h</sub></italic> in the absence of recorded neurons (the zeroth order approximation; Figs. S7 and S8, top row), and the empirical rates versus the linear approximation <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline181.gif"/></alternatives></inline-formula> (the first order approximation; Figs. S7 and S8, bottom row). We find that in both cases the data is centered around the identity line, but the spread of data grows with <italic>J</italic><sub>0</sub> for the zeroth order approximation, while it is quite tight for the first order approximation up to <italic>J</italic><sub>0</sub> = 1.0, validating our neglect of the higher order spike filtering terms. We also confirm that <italic>N</italic><sub>rec</sub> = 500 offers worse agreement than <italic>N</italic><sub>rec</sub> = 100, though the agreement between <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline182.gif"/></alternatives></inline-formula> is still not too bad.</p>
</sec>
</sec>
</sec>
<sec id="s1j">
<title>J. Second order nonlinear response function</title>
<p>Higher order terms in the series expansion represent nonlinear response functions. We do not focus on these terms in this work, assuming instead that we can truncate this series expansion at linear order. We will, however, estimate the error incurred by this truncation. To do so will we need the next order term, so we now go to second order. Rather than differentiate our formal solution for the linear response, we differentiate the implicit form, yielding
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn72.gif"/></alternatives></disp-formula></p>
<fig id="figS7" position="float" fig-type="figure">
<label>FIG. S7</label>
<caption><p><bold>Top row:</bold> scatter plot comparing <italic>&#x03BD;<sub>h</sub></italic>, the mean field firing rates of the hidden neurons in the absence of recorded neurons, to empirically estimated firing rates in simulations of the full network, for four different values of typical synaptic strength, <italic>J</italic><sub>0</sub> = 0.25, 0.5, 0.75, and 1.0. The data lie along the identity line, demonstrating a strong correlation between <italic>&#x03BD;<sub>h</sub></italic> and the empirical data. However, the spread of data around the identity line indicates that deviations of the mean firing rates away from <italic>&#x03BD;<sub>h</sub></italic>, caused by coupling to the recorded neurons, is significant. <bold>Bottom row:</bold> Comparison of the first order approximation of the firing rates of hidden neurons, which accounts for the effects of recorded neurons, to the empirical rates. The data lie tightly along the identity with very little dispersion, demonstrating that higher order spike filtering is unnecessary even up to <italic>J</italic><sub>0</sub> = 1.0, for <italic>N</italic><sub>rec</sub> = 100.</p></caption>
<graphic xlink:href="105510_figS7.tif"/>
</fig>
<p>Rearranging,
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn73.gif"/></alternatives></disp-formula></p>
<p>Inverting the operator on the left hand side yields the input linear response function (when combined with the factor of <italic>&#x03B3;<sub>h</sub></italic> on the right hand side), giving the solution
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn74.gif"/></alternatives></disp-formula></p>
<p>Because <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline183.gif"/></alternatives></inline-formula> is proportional to <italic>&#x03B3;<sub>h</sub></italic>, the second order nonlinear response function is also proportional to <italic>&#x03B3;<sub>h</sub></italic>.</p>
<p>The effective quadratic spike filtering that enters in the instantaneous rate of the recorded neurons is thus
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn75.gif"/></alternatives></disp-formula>
where we define the quadratic spike filter to be
<disp-formula id="eqnS5"><alternatives><graphic xlink:href="105510_eqnS5.gif"/></alternatives></disp-formula></p>
<fig id="figS8" position="float" fig-type="figure">
<label>FIG. S8</label>
<caption><p>Same as <xref ref-type="fig" rid="figS7">Fig. S7</xref> but for <italic>N</italic><sub>rec</sub> = 500 recorded neurons out of a total of <italic>N</italic> = 1000. Demonstrates validity of linear approximation (neglecting higher order spike filtering) up to <italic>J</italic><sub>0</sub> = 1.0, for <italic>N</italic><sub>rec</sub> = 500. The zeroth order approximation (top row) is quite poor, indicating the necessity of accounting for feedback from the recorded neurons. This first order approximation (bottom row) lies tightly along the identity line, indicating that even when the recorded and hidden populations are of comparable size, higher order spike filtering may not be significant. However, there appears to be some deviation for <italic>J</italic><sub>0</sub> = 1.0, indicating that accounting for higher order spike filtering may be beneficial in this parameter regime.</p></caption>
<graphic xlink:href="105510_figS8.tif"/>
</fig>
<p>We would like to estimate the typical size of this term to leading order so that we may estimate the error we make by neglecting it. For the exponential nonlinearity, <italic>&#x03B3;<sub>h</sub></italic> = <italic>&#x03BD;<sub>h</sub></italic> and to leading order in <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline184.gif"/></alternatives></inline-formula>, and hence</p>
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn76.gif"/></alternatives></disp-formula>
<p>This is the result we used in our earlier calculation estimating the error we make in the mean firing rates of the recorded neurons by neglecting higher order spike filtering.</p>
</sec>
<sec id="s1k">
<title>K. Tree-level calculation of the effective noise correlations</title>
<p>In our approximation of the model for the recorded neurons, we also neglected fluctuations from the mean input around the hidden neuron input. We should therefore check how strong this noise is. At the level of a mean-field approximation we may neglect it, so we will need to go to a tree-level approximation to calculate it.</p>
<p>The noise is defined by</p>
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn77.gif"/></alternatives></disp-formula>
<p>It has zero mean (by construction), conditioned on the activity of the recorded units &#x2014; i.e., the &#x201C;noise&#x201D; receives feedback from the recorded neurons. We can evaluate the cross-correlation function of this noise, conditioned on the recorded unit activity. This is given by
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn78.gif"/></alternatives></disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline185.gif"/></alternatives></inline-formula> is the cross-correlation function of the spikes (the superscript <italic>c</italic> denotes &#x2018;cumulant&#x2019; or &#x2018;connected&#x2019; correlation function to distinguish it from the moments without the superscript). At the level of mean field theory <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline186.gif"/></alternatives></inline-formula>, and thus the cross-correlation function is zero. We can go beyond mean field theory and calculate the tree-level contribution to the correlation functions using the field theory diagrammatic techniques of [<xref ref-type="bibr" rid="c8">8</xref>]. We will do so for the reference state of zero-recorded unit activity, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline190.gif"/></alternatives></inline-formula>, as we expect this to be the leading order contribution to the correlation function. As we are interested primarily in the typical magnitude of the noise compared to the interaction terms, we will work only to leading order in &#x03F5; = exp(<italic>&#x00B5;</italic><sub>0</sub>) for the exponential nonlinearity network. We find
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn79.gif"/></alternatives></disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline187.gif"/></alternatives></inline-formula> is the linear response to perturbations to the <italic>output</italic> of a neuron&#x2019;s rate. It is related to <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline188.gif"/></alternatives></inline-formula>, where <italic>&#x03B3;<sub>h</sub></italic> = <italic>&#x03BD;<sub>h</sub></italic> for <italic>&#x03D5;</italic>(<italic>x</italic>) = <italic>e<sup>x</sup></italic>. The overall noise cross-correlation function is then approximately
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn80.gif"/></alternatives></disp-formula></p>
<p>If <italic>r</italic> &#x2260; <italic>r&#x2032;</italic>, the expected noise cross-correlation, averaged over the synaptic weights <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline23c.gif"/></alternatives></inline-formula>, is zero. If <italic>r</italic> = <italic>r&#x2032;</italic>, the expected value is non-zero. The expected noise auto-correlation function is then</p>
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn81.gif"/></alternatives></disp-formula>
<p>For the specific case of <italic>g</italic>(<italic>t</italic>) = <italic>&#x03B1;</italic><sup>2</sup><italic>te&#x2212;&#x03B1;t</italic>&#x0398;(<italic>t</italic>), we have</p>
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn82.gif"/></alternatives></disp-formula>
<p>For weak coupling (<italic>a</italic> = 1), the expected autocorrelation function falls off with network size as 1/<italic>N</italic>, while for strong coupling (<italic>a</italic> = 1/2), it scales with the fraction of observed neurons <italic>f</italic>, but is independent of the absolute network size. The overall <italic>&#x03BB;</italic><sub>0</sub>&#x03F5; scaling puts the magnitude of the autocorrelation function on par with contributions from hidden paths through a single hidden neuron that contributes a factor of <italic>&#x03BB;</italic><sub>0</sub>&#x03F5; to the correction to the coupling filters. Based on our results shown in <xref ref-type="fig" rid="fig3">Fig. 3</xref>, which suggest that contributions from long paths through hidden neurons are significant when the fraction of neurons <italic>f</italic> is small and <italic>J</italic><sub>0</sub> &#x2272; 1, we expect that network noise will also be significant in these regimes. This will not modify the results presented in the main paper, however. It simply means that this noise should be retained in the rate of our approximate model,
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn83.gif"/></alternatives></disp-formula></p>
<p>Lastly, we note that here we only calculated the leading order contribution to the noise correlation functions around the reference state of no recorded unit activity. Much as we expanded <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline189a.gif"/></alternatives></inline-formula> in a functional Taylor series around <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline190a.gif"/></alternatives></inline-formula>, we could do so for the correlation functions as well to study how these effective noise correlations depend on the activity of the recorded neurons. We leave such a calculation for future investigations.</p>
</sec>
<sec id="s1l">
<title>L. Full mean-field reference state</title>
<p>For most of our analyses, we have been expanding the conditional firing rates of the hidden neurons around a reference state of zero activity of the recorded neurons. The quantities <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline191.gif"/></alternatives></inline-formula>, and so on, are thus calculated using a network in which the recorded neurons have been removed. We have demonstrated that this approximation is valid for the networks considered in this paper. However, this approximation may break down in networks in which the recorded spike at high rates. In this case, we may need another reference state to expand the conditional rates around. A natural choice of reference state <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline192.gif"/></alternatives></inline-formula> in this case would be the mean firing rates of the neurons. We will set up this expansion here.</p>
<p>The mean firing rates of the neurons are intractable to calculate exactly, so we will estimate them by the mean field rates, an approximation that we expect to be accurate in the high-firing rate regime.</p>
<p>The mean field equations for the full network are</p>
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn84.gif"/></alternatives></disp-formula>
<p>We can then expand <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline193.gif"/></alternatives></inline-formula>, truncating at linear order to obtain
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn85.gif"/></alternatives></disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline194.gif"/></alternatives></inline-formula> is the input linear response of the <italic>full network</italic>, including the recorded neurons. We can then approximate the instantaneous firing rates of the recorded neurons by
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn86.gif"/></alternatives></disp-formula>
note that we introduced <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline195.gif"/></alternatives></inline-formula> so that we could write the instantaneous firing not as a function of filtered spike trains but as a functioned of filtered deviations from the mean firing rate. Importantly, although it looks like only the baseline is different from the zero-activity reference state case but the coupling is the same, the linear response function <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline196.gif"/></alternatives></inline-formula> is <italic>not</italic> the same as the zero-reference state case, and hence the correction to the coupling is slightly different. The solutions look similar, but the linear response functions now incorporate the effects of the recorded units as well. In particular, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline197.gif"/></alternatives></inline-formula> satisfies the equation
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn87.gif"/></alternatives></disp-formula>
where<inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline198.gif"/></alternatives></inline-formula> is the gain of neuron <italic>i</italic> accounting for the entire network,</p>
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn88.gif"/></alternatives></disp-formula>
<p>Thus, in Fourier space
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn89.gif"/></alternatives></disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline199.gif"/></alternatives></inline-formula> is an <italic>N &#x00D7; N</italic> matrix &#x2013; i.e., it contains the couplings and firing rates of <italic>all</italic> neurons, recorded and hidden. Hence, while this looks formally similar to the result we obtained in the zero activity state, the inclusion of recorded neurons modifies our rules for calculating contributions to the effective coupling filters. In particular, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline200.gif"/></alternatives></inline-formula> involves contributions from paths through both hidden and recorded neurons, unlike the zero-activity reference case, which involved contributions only from paths through hidden neurons. The reason for this, of course, is that the reference state depends on the entire network, not just the hidden neurons. The difference between the cases matters only matter at higher orders in our expansion &#x2014; paths of length <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline201.gif"/></alternatives></inline-formula> or greater. We can see this by writing out the first few terms in the path length expansion of the effective coupling,
<disp-formula><alternatives><graphic xlink:href="105510_Ueqn90.gif"/></alternatives></disp-formula>
for conciseness, we have assumed zero-self coupling <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline202.gif"/></alternatives></inline-formula>, but this could be restored by setting <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline203.gif"/></alternatives></inline-formula> <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline204.gif"/></alternatives></inline-formula></p>
<p>We see that the first few terms of the expansion are the same as the zero-activity reference case, with the exception that the <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline205.gif"/></alternatives></inline-formula> are the gains for the entire network, not just the hidden network absent the recorded neurons. It is only the <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="105510_inline206.gif"/></alternatives></inline-formula> term at which contributions to the linear response functions involving paths through any neuron <italic>j</italic>, recorded or hidden, start to appear. Because we typically expect the amplitude of these terms to be small, we anticipate expanding around the mean field reference state will yield similar results to the expansion around the zero-activity reference state presented in the main paper.</p>
</sec>
</sec>
<ref-list>
<ref id="c61">
<label>[1]</label><mixed-citation publication-type="book"><string-name><given-names>W.</given-names> <surname>Gerstner</surname></string-name>, <string-name><given-names>W. M.</given-names> <surname>Kistler</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Naud</surname></string-name>, and <string-name><given-names>L.</given-names> <surname>Paninski</surname></string-name>, <source>Neuronal Dynamics: From single neurons to networks and models of cognition</source> (<publisher-name>Cambridge University Press</publisher-name>, <publisher-loc>Cambridge, U.K</publisher-loc>, <year>2014</year>)</mixed-citation></ref>
<ref id="c62">
<label>[2]</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Ostojic</surname></string-name> and <string-name><given-names>N.</given-names> <surname>Brunel</surname></string-name>, <source>PLOS Computational Biology</source> <volume>7</volume>, <fpage>1</fpage> <year>(2011)</year>.</mixed-citation></ref>
<ref id="c63">
<label>[3]</label><mixed-citation publication-type="journal"><string-name><given-names>H. F.</given-names> <surname>Song</surname></string-name> and <string-name><given-names>X.-J.</given-names> <surname>Wang</surname></string-name>, <source>Phys. Rev. E</source> <volume>90</volume>, <fpage>062801</fpage> <year>(2014)</year>.</mixed-citation></ref>
<ref id="c64">
<label>[4]</label><mixed-citation publication-type="book"><string-name><given-names>E.</given-names> <surname>Simoncelli</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Paninski</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Pillow</surname></string-name>, and <string-name><given-names>O.</given-names> <surname>Schwartz</surname></string-name>, <source>in The Cognitive Neurosciences</source>, edited by M. Gazzaniga (<publisher-name>MIT Press</publisher-name>, <year>2004</year>) <edition>3rd</edition> ed., pp. <fpage>327</fpage>&#x2013;<lpage>338</lpage>.</mixed-citation></ref>
<ref id="c65">
<label>[5]</label><mixed-citation publication-type="journal"><string-name><given-names>L.</given-names> <surname>Paninski</surname></string-name>, <source>Network: Computation in Neural Systems</source> <volume>15</volume>, <fpage>243</fpage> <year>(2004)</year>.</mixed-citation></ref>
<ref id="c66">
<label>[6]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Pillow</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Paninski</surname></string-name>, <string-name><given-names>V. J.</given-names> <surname>Uzzell</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Simoncelli</surname></string-name>, and <string-name><given-names>E. J.</given-names> <surname>Chichilnisky</surname></string-name>, <source>The Journal of neuroscience : the official journal of the Society for Neuroscience</source> <volume>25</volume>, <fpage>11003</fpage> <year>(2005)</year>.</mixed-citation></ref>
<ref id="c67">
<label>[7]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Pillow</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Shlens</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Paninski</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Sher</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Litke</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Chichilnisky</surname></string-name>, and <string-name><given-names>E.</given-names> <surname>Simoncelli</surname></string-name>, <source>Nature</source> <volume>454</volume>, <fpage>995</fpage> <year>(2008)</year>.</mixed-citation></ref>
<ref id="c68">
<label>[8]</label><mixed-citation publication-type="journal"><string-name><given-names>G.</given-names> <surname>Koch Ocker</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Josi&#x0107;</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Shea-Brown</surname></string-name>, and <string-name><given-names>M. A.</given-names> <surname>Buice</surname></string-name>, <source>ArXiv e-prints</source> (<year>2016</year>), arXiv:<pub-id pub-id-type="arxiv">1610.03828</pub-id> [q-bio.NC].</mixed-citation></ref>
<ref id="c69">
<label>[9]</label><mixed-citation publication-type="journal"><string-name><given-names>A. G.</given-names> <surname>Hawkes</surname></string-name>, <source>Biometrika</source> <volume>58</volume>, <fpage>83</fpage> <year>(1971)</year>.</mixed-citation></ref>
<ref id="c70">
<label>[10]</label><mixed-citation publication-type="journal"><string-name><given-names>P.</given-names> <surname>Br&#x00E9;maud</surname></string-name> and <string-name><given-names>L</given-names>. <surname>Massouli&#x00E9;</surname></string-name>, <source>Ann. Probab</source>. <volume>24</volume>, <fpage>1563</fpage> <year>(1996)</year>.</mixed-citation></ref></ref-list></back></article>