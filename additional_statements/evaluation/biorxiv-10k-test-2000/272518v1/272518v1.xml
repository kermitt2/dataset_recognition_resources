<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/272518</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>End-to-end deep image reconstruction from human brain activity</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Shen</surname>
<given-names>Guohua</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Dwivedi</surname>
<given-names>Kshitij</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Majima</surname>
<given-names>Kei</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Horikawa</surname>
<given-names>Tomoyasu</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9300-8268</contrib-id>
<name>
<surname>Kamitani</surname>
<given-names>Yukiyasu</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Computational Neuroscience Laboratories</institution>, ATR, Kyoto, <country>Japan</country></aff>
<aff id="a2"><label>2</label><institution>Graduate School of Informatics, Kyoto University</institution>, Kyoto, <country>Japan</country></aff>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>&#x002A;</label><p>These authors contributed equally to this work</p></fn>
<corresp id="cor1">Correspondence: Yukiyasu Kamitani: <email>kamitani@i.kyoto-u.ac.jp</email></corresp>
</author-notes>
<pub-date pub-type="epub"><year>2018</year></pub-date>
<elocation-id>272518</elocation-id>
<history>
<date date-type="received">
<day>27</day>
<month>2</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>27</day>
<month>2</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>27</day>
<month>2</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="272518.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract><title>Abstract</title>
<p>Deep neural networks (DNNs) have recently been applied successfully to brain decoding and image reconstruction from functional magnetic resonance imaging (fMRI) activity. However, direct training of a DNN with fMRI data is often avoided because the size of available data is thought to be insufficient to train a complex network with numerous parameters. Instead, a pre-trained DNN has served as a proxy for hierarchical visual representations, and fMRI data were used to decode individual DNN features of a stimulus image using a simple linear model, which were then passed to a reconstruction module. Here, we present our attempt to directly train a DNN model with fMRI data and the corresponding stimulus images to build an end-to-end reconstruction model. We trained a generative adversarial network with an additional loss term defined in a high-level feature space (feature loss) using up to 6,000 training data points (natural images and the fMRI responses). The trained deep generator network was tested on an independent dataset, directly producing a reconstructed image given an fMRI pattern as the input. The reconstructions obtained from the proposed method showed resemblance with both natural and artificial test stimuli. The accuracy increased as a function of the training data size, though not outperforming the decoded feature-based method with the available data size. Ablation analyses indicated that the feature loss played a critical role to achieve accurate reconstruction. Our results suggest a potential for the end-to-end framework to learn a direct mapping between brain activity and perception given even larger datasets.</p></abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>brain decoding</kwd>
<kwd>visual image reconstruction</kwd>
<kwd>functional magnetic resonance imaging</kwd>
<kwd>deep neural networks</kwd>
</kwd-group>
<counts>
<page-count count="24"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<label>1.</label>
<title>Introduction</title>
<p>Decoding visual contents from brain activity using deep neural networks (DNN) has recently shown promising results. DNNs have been applied on functional magnetic resonance imaging (fMRI) data to reconstruct the perceived image (<xref rid="c5" ref-type="bibr">G&#x00FC;&#x00E7;l&#x00FC;t&#x00FC;rk et al., 2017</xref>; <xref rid="c18" ref-type="bibr">Shen et al., 2017</xref>; <xref rid="c6" ref-type="bibr">Han et al., 2017</xref>; <xref rid="c17" ref-type="bibr">Seeliger et al., 2017</xref>) or to identify the object category (<xref rid="c8" ref-type="bibr">Horikawa and Kamitani, 2017</xref>). However, the above methods avoid directly training a DNN model on the fMRI data due to smaller dataset sizes in fMRI studies. To solve the dataset size issue, the feature representation from a DNN pretrained on a large scale image dataset was used as a proxy for the neural representations of the human visual system. Hence, these methods involve two independent steps, 1) decoding DNN features from fMRI activity and 2) reconstruction or identification using the decoded DNN features.</p>
<p>In image generation studies in computer vision, a DNN can be trained using an end-to-end approach to directly generate the image from a modality correlated with the images, e.g., captions (<xref rid="c12" ref-type="bibr">Mansimov et al., 2015</xref>) and DNN features (<xref rid="c2" ref-type="bibr">Dosovitskiy and Brox 2016a</xref>, <xref rid="c3" ref-type="bibr">2016b</xref>). In the end-to-end approach, the DNN model generates the image directly from the correlated modality. However, the dataset sizes used to train such models are usually larger as compared to the dataset sizes in fMRI studies. For instance, <xref rid="c12" ref-type="bibr">Mansimov et al. (2015)</xref> trained a caption-to-image model on Microsoft COCO dataset that consists of 82,783 images, each annotated with at least 5 captions. <xref rid="c2" ref-type="bibr">Dosovitskiy and Brox (2016a)</xref> trained a DNN model on ImageNet training dataset (over 1.2 million images) to reconstruct images from corresponding DNN features. On the other hand, the largest fMRI dataset used for reconstruction in <xref rid="c18" ref-type="bibr">Shen et al. (2017)</xref> consists of only 6,000 training samples. Thus, training a DNN to reconstruct images directly from fMRI data is often avoided and considered infeasible due to the smaller dataset size.</p>
<p>In this study, we sought to evaluate the potential of the end-to-end approach to obtain a direct mapping from fMRI activity to stimulus space given a limited training dataset. Training a DNN in an end-to-end manner implies that the input to the DNN is the fMRI activity and output of the DNN is the reconstruction of the perceived stimulus. If we can successfully perform reconstruction using the end-to-end approach, then we can avoid feature decoding step used in earlier studies and reconstruct directly from the fMRI activity.</p>
<p>For designing an end-to-end DNN model to reconstruct from fMRI data, we transformed the fMRI data into a 1-dimensional vector and therefore, the reconstruction model must transform 1-dimensional fMRI data to a 3-dimension image in RGB color space. The neural network architectures that transform 1-dimensional image features to the original image are thus well-suited for this purpose. The fMRI data from the visual cortex can also be considered as a representation of the perceived image, and thus can be used as an input in such architectures for the end-to-end training.</p>
<p><xref rid="c2" ref-type="bibr">Dosovitskiy and Brox (2016a)</xref> proposed a DNN based method to generate the original image from the corresponding features by optimizing in image space. The loss in image space only is usually insufficient to obtain a good reconstruction since it generates a reconstruction that is an average of all the possible reconstructions with the same distance in image space and hence the reconstructed images are blurred. The feature loss in high dimensional space, also called perceptual loss, constrains the reconstruction to be perceptually similar to the original image. Adversarial loss (<xref rid="c4" ref-type="bibr">Goodfellow et al., 2014</xref>) constrains the distribution of the reconstructed images to be close to the distribution of natural images. In a subsequent study, <xref rid="c3" ref-type="bibr">Dosovitskiy and Brox (2016b)</xref>, showed that reconstruction from features could be improved by introducing feature and adversarial loss terms. Hence, we adopted the approach of <xref rid="c3" ref-type="bibr">Dosovitskiy and Brox (2016b)</xref> to reconstruct the perceived stimuli directly from the fMRI activity. We modified their model to take input directly from the fMRI activity and trained the model from scratch on the dataset from <xref rid="c18" ref-type="bibr">Shen et al. (2017)</xref>.</p>
<p>In this study, we first demonstrate that we can obtain reconstructions resembling the original stimulus images from the model trained on this dataset. We further explore the generalizability of the proposed method on artificial shapes and alphabetical letters. To understand the effect of training dataset size on reconstruction quality, we compare the reconstruction results as the training dataset size gradually increased from 120 samples to 6,000 samples. Finally, to investigate the effects of the different loss functions used in the reconstruction method, we perform an ablation study by removing one loss function at a time and performing a subjective and objective comparison of the reconstruction results.</p>
</sec>
<sec id="s2">
<label>2.</label>
<title>Materials and Methods</title>
<p>In this section, we briefly describe the method we used for our experiments and the details of the dataset. For more details regarding the image reconstruction method please refer to <xref rid="c3" ref-type="bibr">Dosovitskiy and Brox (2016b)</xref>, and for details regarding dataset, please refer to <xref rid="c18" ref-type="bibr">Shen et al. (2017)</xref>.</p>
<sec id="s2a">
<label>2.1.</label>
<title>Problem Statement</title>
<p>Let <bold>X</bold> &#x2208; &#x211D;<sup><italic>w</italic>&#x00D7;<italic>h</italic>&#x00D7;3</sup> be the stimulus image displayed in the perception experiment where <italic>w</italic> and <italic>h</italic> are width and height of the stimulus image respectively and 3 denotes the number of channels (RGB) of the color image. Let <bold>f</bold> &#x2208; &#x211D;<sup><italic>N</italic></sup> be the corresponding preprocessed fMRI vector for the brain activity recorded during the perception and <italic>N</italic> is the number of voxels in the visual cortex (VC). We are interested in obtaining a reconstruction of the stimulus from fMRI vector <bold>f</bold>.</p>
<p>To solve this problem, we use a DNN <bold>G</bold><sub>&#x03B8;</sub> with parameters <bold>&#x03B8;</bold> which performs non-linear operations on <bold>f</bold> to obtain a plausible reconstruction <bold>G<sub>&#x03B8;</sub></bold>(<bold>f</bold>) of the stimulus image.</p>
</sec>
<sec id="s2b">
<label>2.2.</label>
<title>Image reconstruction model</title>
<p>We modified the DNN model proposed by <xref rid="c3" ref-type="bibr">Dosovitskiy and Brox (2016b)</xref> to reconstruct stimulus images from fMRI data.</p>
<p>For the fMRI vector <bold>f</bold> corresponding to the stimulus image <bold>X</bold>, the model is trained to generate a plausible reconstruction image <bold>G<sub>&#x03B8;</sub></bold>(<bold>f</bold>) from <bold>f</bold>. The network architecture (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) consists of three convolutional neural networks: a generator <bold>G<sub>&#x03B8;</sub></bold> which transforms the fMRI vector <bold>f</bold> to <bold>G<sub>&#x03B8;</sub></bold>(<bold>f</bold>), a discriminator <bold>D<sub>&#x03A6;</sub></bold> which discriminates the reconstructed image <bold>G<sub>&#x03B8;</sub></bold>(<bold>f</bold>) from the natural image <bold>X</bold>, and a comparator <bold>C</bold> which performs the comparison between the reconstructed image <bold>G<sub>&#x03B8;</sub></bold>(<bold>f</bold>) and the original stimulus image <bold>X</bold> in the feature space.</p>
<fig id="fig1" position="float" fig-type="figure" orientation="portrait">
<label>Figure 1</label>
<caption><title>Schematics of our reconstruction approach.</title>
<p><bold>(A) Model training.</bold> We use an adversarial training strategy adopted from <xref rid="c3" ref-type="bibr">Dosovitskiy and Brox (2016b)</xref>, which consists of 3 DNNs: a generator, a comparator, and a discriminator. The training images are presented to a human subject, while brain activity is measured by fMRI. The fMRI activity is used as an input to the generator. The generator is trained to reconstruct the images from the fMRI activity to be as similar to the presented training images in both pixel and feature space. The adversarial loss constrains the generator to generate reconstructed images that fool the discriminator to classify them as the true training images. The discriminator is trained to distinguish between the reconstructed image and the true training image. The comparator is a pre-trained DNN, which was trained to recognize the object in natural images. Both the reconstructed and true training images are used as an input to the comparator, which compares the image similarity in feature space. <bold>(B) Model test.</bold> In the test phase, the images are reconstructed by providing the fMRI activity of the test image as the input to the generator.</p></caption>
<graphic xlink:href="272518_fig1.tif"/>
</fig>
<p>The input to the generator is the fMRI vector <bold>f</bold> from VC and the output is the reconstructed image <bold>G<sub>&#x03B8;</sub></bold>(<bold>f</bold>). The generator consists of three fully connected layers followed by six upconvolution layers to generate the final reconstruction image <bold>G<sub>&#x03B8;</sub></bold>(<bold>f</bold>).</p>
<p>The comparator network <bold>C</bold> is Caffenet trained on Imagenet dataset for the image classification task. The Caffenet model is a replication of Alexnet (<xref rid="c11" ref-type="bibr">Krizhevsky et. al. 2012</xref>) model with the order of pooling and normalization layers switched and without relighting data-augmentation during training. The network consists of 5 convolutional and 3 fully connected layers. We used the last convolutional layer of the comparator to compare the reconstructed image and the original stimulus image in feature space. The parameters of the comparator were not updated during the training of the reconstruction model.</p>
<p>The discriminator <bold>D<sub>&#x03A6;</sub></bold> consists of five convolutional layers followed by an average pooling layer and two fully-connected layers. The output layer of the discriminator is a 2-way softmax and the network is trained to discriminate the original stimulus image from the reconstructed image. The purpose of the discriminator is to learn to differentiate between original stimulus images and images reconstructed by the generator. The generator is trained concurrently to optimize an adversarial loss function to fool the discriminator into classifying the reconstructed image as the real stimulus image. The adversarial loss forces the generator to generate more realistic images closer to image distribution of the training data.</p>
<p>The architectures of the discriminator and comparator networks were the same as in <xref rid="c3" ref-type="bibr">Dosovitskiy and Brox (2016b)</xref>. The generator was modified to take its input from the fMRI data as opposed to DNN features in <xref rid="c3" ref-type="bibr">Dosovitskiy and Brox (2016b)</xref>.</p>
<p>Let <bold>X</bold><sub><italic>i</italic></sub> denote the <italic>i<sup>th</sup></italic> stimulus image in the dataset, <bold>f</bold><sub><italic>i</italic></sub> denote the corresponding fMRI data for the <italic>i<sup>th</sup></italic> image and <bold>G<sub>&#x03B8;</sub></bold>(<bold>f</bold><sub><italic>i</italic></sub>) denote the reconstruction output of the generator. The parameters <bold>&#x03B8;</bold> of generator <bold>G<sub>&#x03B8;</sub></bold> are updated to minimize the weighted sum of three loss terms for a minibatch using stochastic gradient descent: loss in image space <italic>L</italic><sub>img</sub>, feature loss <italic>L</italic><sub>feat</sub>, adversarial loss <italic>L</italic><sub>adv</sub>:
<disp-formula id="ueqn1"><alternatives><graphic xlink:href="272518_ueqn1.gif"/></alternatives></disp-formula>
where
<disp-formula id="ueqn2"><alternatives><graphic xlink:href="272518_ueqn2.gif"/></alternatives></disp-formula>
and <italic>&#x03BB;</italic><sub>img</sub>, <italic>&#x03BB;</italic><sub>feat</sub>, and <italic>&#x03BB;</italic><sub>adv</sub> denote the weights of the loss in image space <italic>L</italic><sub>img</sub>, feature loss <italic>L</italic><sub>feat</sub>, and adversarial loss <italic>L</italic><sub>adv</sub>, respectively.</p>
<p>The discriminator is trained concurrently with the generator to minimize <italic>L</italic><sub>discr</sub>
<disp-formula id="ueqn3"><alternatives><graphic xlink:href="272518_ueqn3.gif"/></alternatives></disp-formula></p>
<p>The parameters of the comparator <bold>C</bold> are fixed throughout the training since it is used just for the comparison in feature space and therefore no update was required.</p>
<p>We performed the training using Caffe framework (<xref rid="c9" ref-type="bibr">Jia et al., 2014</xref>) and modified the implementation of the model provided by <xref rid="c3" ref-type="bibr">Dosovitskiy and Brox (2016b)</xref>. The weights of generator and discriminator were initialized using MSRA (<xref rid="c7" ref-type="bibr">He et al., 2015</xref>) initialization. The comparator weights were initialized by Caffenet weights trained on Imagenet classification. We used Adam solver (<xref rid="c10" ref-type="bibr">Kingma and Ba, 2015</xref>) with momentum <italic>&#x03B2;</italic><sub>1</sub> &#x003D; 0.9, <italic>&#x03B2;</italic><sub>2</sub> &#x003D; 0.999 and initial learning rate 0.0002 for optimization. We used a batch size of 64 and trained for 500,000 mini-batch iterations in all the experiments. Following the similar training procedure as <xref rid="c3" ref-type="bibr">Dosovitskiy and Brox (2016b)</xref>, we temporarily stopped updating the discriminator if the ratio of <italic>L</italic><sub>discr</sub> and <italic>L</italic><sub>adv</sub> was below 0.1. This was done to prevent the discriminator from overfitting. The weights of the individual loss functions <italic>&#x03BB;</italic><sub>img</sub>, <italic>&#x03BB;</italic><sub>feat</sub>, and <italic>&#x03BB;</italic><sub>adv</sub> were set to <italic>&#x03BB;</italic><sub>img</sub> &#x003D; 2<italic>e</italic> &#x2212; 6, <italic>&#x03BB;</italic><sub>feat</sub> &#x003D; 0.01, and <italic>&#x03BB;</italic><sub>adv</sub> &#x003D; 100.</p>
<p>We applied image jittering during the training for data augmentation and to take into account the eye movement of the subjects during image presentation experiment. Generally, for a typical subject, the size of eye movement is about 1 degree viewing angle. The viewing angle for the presented images is 12 degrees. All the training images are resized to 248 &#x00D7; 248 before the training. During the training, we randomly cropped a 227 &#x00D7; 227 window from each training image as the target image for each iteration to ensure that the largest jittering size was 12 &#x002A; (248 &#x2212; 227)/227&#x007E;1 degrees.</p>
<p>For dataset size analysis we trained the reconstruction model with variable number of training samples for 100 epochs with a batch size of 60. The rest of the hyperparameters were same as the previous analysis.</p>
</sec>
<sec id="s2c">
<label>2.3.</label>
<title>Dataset from (<xref rid="c18" ref-type="bibr">Shen et al., 2017</xref>)</title>
<p>We used an fMRI dataset from a previous reconstruction study (<xref rid="c18" ref-type="bibr">Shen et al., 2017</xref>). This dataset was used to reconstruct stimulus images from visual features of a deep convolutional neural network decoded from the brain.</p>
<p>The stimulus images in the dataset were categorized into four types: training-natural images, test-natural images, artificial shapes and alphabetical letters. The natural images used for the experiment were selected from 200 representative object categories in the ImageNet (<xref rid="c1" ref-type="bibr">Deng et al., 2009</xref>; 2011, fall release) dataset. The training-natural image dataset consisted of 1,200 images from 150 object categories and test-natural image dataset consisted of 50 images from 50 object categories. There was no overlap between the categories used in the training and test datasets. The artificial shapes consisted of 40 images obtained by combining 8 colors and 5 shapes. The alphabetical letters consisted of 10 images of different letters from the English alphabet in black color.</p>
<p>The image presentation experiments consisted of four distinct types of sessions corresponding to the four categories of stimulus images described above. In one set of the training-natural image session, a total of 1,200 images was presented only once. This set of training session was repeated five times. In the test-natural image session, the artificial-shape session, and the alphabetical letter session, 50, 40, and 10 images were presented 20, 20, and 12 times each, respectively. The presentation order of the images was randomized across runs.</p>
<p>The fMRI data obtained during the image presentation experiment were preprocessed for motion correction followed by co-registration to the within-session high-resolution anatomical images of the same slices and subsequently to T-1 anatomical images. The coregistered data were then re-interpolated as 2 &#x00D7; 2 &#x00D7; 2 mm voxels.</p>
<p>The fMRI data samples were created by first regressing out nuisance parameters, including a linear trend, and temporal components proportional to six motion parameters calculated by the SPM5 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</ext-link>) motion correction procedure, from each voxel amplitude for each run. After that, voxel amplitudes were normalized relative to the mean amplitude of the initial 24-s rest period of each run, and were despiked to reduce extreme values (beyond &#x00B1;3SD for each run). The voxel amplitudes were then averaged within each 8-s (training natural image-sessions), 12-s (test natural-image, artificial-shapes, and alphabetical-letter sessions) stimulus block (four or six volumes), after shifting the data by 4 s (two volumes) to compensate for hemodynamic delays.</p>
<p>The voxels used for the reconstruction were selected from the visual cortex (VC), which consisted of lower visual areas: V1, V2, V3, and V4 and higher visual areas: lateral occipital complex (LOC), fusiform face area (FFA), and parahippocampal place area (PPA). V1, V2, V3, and V4 were identified using a retinotopy experiments and LOC, FFA, and PPA were identified using a functional localizer experiments (<xref rid="c18" ref-type="bibr">Shen et al., 2017</xref>).</p>
<p>The fMRI data was further normalized before using it as an input to the reconstruction model. The mean <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="272518_inline1.gif"/></alternatives></inline-formula> and standard deviation <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="272518_inline2.gif"/></alternatives></inline-formula> of amplitude a<sub>v<sub>i</sub></sub> of each voxel v<sub>i</sub> in VC were estimated across the training data. Then for both training and test data, the normalized amplitudes <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="272518_inline3.gif"/></alternatives></inline-formula> of voxel v<sub>i</sub> &#x2208; <italic>VC</italic> for each sample were estimated as follows:
<disp-formula id="ueqn4"><alternatives><graphic xlink:href="272518_ueqn4.gif"/></alternatives></disp-formula></p>
<p>To compensate the statistical difference between the training and test fMRI data (we performed trial-averaging for the test fMRI data while we considered each trial as an individual sample for the training fMRI data), we rescaled the test fMRI data by a factor of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="272518_inline4.gif"/></alternatives></inline-formula> where <italic>n</italic> is number of trials averaged, before we use the test fMRI data as the input to the generator.</p>
<p>For dataset size analysis, initially a fixed number of training images and the corresponding fMRI activity from five trials were selected for training. As the dataset size was increased more training images with fMRI activity were subsequently added. The training dataset size was increased gradually from (5 &#x00D7; 24)120 to (5 &#x00D7; 1200) 6,000 training samples.</p>
</sec>
<sec id="s2d">
<label>2.4.</label>
<title>Evaluation</title>
<p>We evaluated the quality of reconstruction using both objective and subjective assessment methods. For both assessment methods, we performed a pairwise similarity comparison, where one reconstructed image was compared with two possible candidate images, one was the original stimulus image from which the reconstruction was obtained and the other was randomly selected from the test dataset of the same image type.</p>
<p>For the subjective assessment, we conducted a behavioral experiment similar to <xref rid="c18" ref-type="bibr">Shen et al. (2017)</xref>. In the experiment, another group of 13 raters (6 females and 7 males, aged between 19 and 48 years) were presented a reconstructed image with two possible candidate images and were asked to select the candidate image which appears more similar to the reconstructed image.</p>
<p>For the objective assessment, we compared the pixel-wise correlation coefficients of the reconstructed image with the two candidate images and selected the candidate image with the higher correlation coefficient.</p>
<p>For both assessments, we calculated the percentage of trials in which the original stimulus image was selected and used it as the quality measure. The trial for each reconstructed image was conducted with all the pairs of the images from same type.</p>
<p>We conducted another behavioral experiment to study the effect of different loss terms in the proposed approach. 3 raters (1 female and 2 males, aged between 30 and 37 years) from a different group were presented one original stimulus image and two reconstructed images generated from different combination of loss terms. The raters were asked to judge which one of the reconstructions showed higher resemblance with the original stimulus image. This pairwise comparison was conducted for 8 pairs of combinations of loss terms for all the stimulus image in test dataset. We use the winning percentage as the quantitative measure to compare reconstructions obtained from different combinations of loss terms. The winning percentage is the percentage of trials in which the reconstruction from one combination was judged better than the other. For computing the winning percentage from pixel-wise correlation coefficients, the reconstruction with higher correlation coefficient was selected. For more details about the design of both the behavioral experiments, please refer to <xref rid="c18" ref-type="bibr">Shen et al. (2017)</xref>.</p>
</sec>
</sec>
<sec id="s3">
<label>3.</label>
<title>Results</title>
<sec id="s3a">
<label>3.1.</label>
<title>Image reconstruction</title>
<p>We trained the reconstruction model on the training session samples of the fMRI dataset from <xref rid="c18" ref-type="bibr">Shen et al. (2017)</xref> which consisted of perception fMRI data corresponding to 1,200 natural images. In the training session, each stimulus image was presented to the subject 5 times. We treated each stimulus presentation as a separate training sample for the reconstruction model. Therefore, the training dataset we used, consisted of 6,000 (5 &#x00D7; 1200) samples.</p>
<p>We evaluated the reconstruction quality on three test datasets: natural images, artificial shapes and alphabetical letters. For generating reconstructions, fMRI samples corresponding to the same image (20 samples in the test-natural image session, 20 samples in the artificial shapes session, and 12 samples in the alphabetical letters session) were averaged across trials to increase the signal to noise ratio, averaged fMRI samples were used as input to the trained generator. <xref ref-type="fig" rid="fig2">Figure 2A</xref> shows some example images from the test natural images and their corresponding reconstructions from three different subjects obtained using our model. The reconstructions from all three subjects closely resemble the natural &#x002D;image stimuli in shape. The color, however, is not preserved in some of the reconstructions. The reconstruction results from our model show that despite utilizing a small dataset, it was possible to train a model from scratch that could reconstruct visually similar images from fMRI data with high accuracy (<xref ref-type="fig" rid="fig2">Figure 2B</xref>; 78.1&#x0025; accuracy by pixel-wise spatial correlation, 95.7&#x0025; by human judgment).</p>
<fig id="fig2" position="float" fig-type="figure" orientation="portrait">
<label>Figure 2</label>
<caption><title>Results of reconstruction of natural images.</title>
<p><bold>(A)</bold> Presented and reconstructed natural images are shown here. The presented images (in black frames) are shown in the top row. Three corresponding reconstructed images (in gray frames) from each of the three subjects are shown underneath. <bold>(B)</bold> Reconstruction accuracy of natural images in terms of percentage of correct pair-wise classification based on both pixel correlation and human judgment (error bars, 95&#x0025; confidence interval (CI) across samples; three subjects pooled; chance level, 50&#x0025;).</p></caption>
<graphic xlink:href="272518_fig2.tif"/>
</fig>
<p>Further, we evaluated the generalization of the method using artificial shapes similar to <xref rid="c18" ref-type="bibr">Shen et al. (2017)</xref>. We demonstrate that using the proposed approach, artificial shapes (<xref ref-type="fig" rid="fig3">Figure 3A</xref>) can be reconstructed with high accuracy (<xref ref-type="fig" rid="fig3">Figure 3B</xref>. 69.3&#x0025; by pixel-wise spatial correlation, 92.7&#x0025; by human judgment) even though the model was trained on natural images.</p>
<fig id="fig3" position="float" fig-type="figure" orientation="portrait">
<label>Figure 3</label>
<caption><title>Reconstruction of artificial shapes and alphabetical letters.</title>
<p><bold>(A)</bold> Reconstruction of artificial shapes. The original stimulus images (in black frames) are shown in the top row. Three corresponding reconstructed images (in gray frames) from each of the three subjects are shown underneath. <bold>(B)</bold> Reconstruction accuracy of artificial shapes. <bold>(C)</bold> Reconstruction accuracy of both shape and color. <bold>(D)</bold> Reconstruction of alphabetical letters. <bold>(E)</bold> Reconstruction accuracy of alphabetical letters. For <bold>(B)</bold>, <bold>(C)</bold> and <bold>(E)</bold>, reconstruction accuracy is assessed in terms of percentage of correct pair-wise classification based on both pixel-wise correlation and human judgment (error bars, 95&#x0025; CI across samples; three subjects pooled; chance level, 50&#x0025;).</p></caption>
<graphic xlink:href="272518_fig3.tif"/>
</fig>
<p>From the artificial shape reconstruction results, we observed that the shape of the stimulus is well preserved in the reconstructions. However, the color in the reconstructions is preserved only for the red-colored shapes, while the reconstructions of the other-colored shapes do not show resemblance in terms of color. To compare the reconstruction quality in terms of shape and color, we performed comparison across the reconstructed images of same shapes and colors. The quantitative results from <xref ref-type="fig" rid="fig3">Figure 3C</xref> (shape: 76.5&#x0025; by pixel-wise spatial correlation, 95.0&#x0025; by human judgment, color: 56.7&#x0025; by pixel-wise spatial correlation, 75.6&#x0025; by human judgment) suggest that reconstructed images show more resemblance in terms of shape as compared to color.</p>
<p>We further show the generalizability of our approach by showing highly accurate reconstructions of the alphabetical letters images (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). The alphabetical letters reconstruction accuracy was 95.9&#x0025; according to pixel-wise spatial correlation, and 96.4&#x0025; according to human judgment.</p>
<p>We compared the reconstruction accuracy of the proposed method with <xref rid="c18" ref-type="bibr">Shen et al. (2017)</xref> values to analyze the difference between the two methods. We observed that on spatial correlation metric (natural images: ours 78.1&#x0025;, <xref rid="c18" ref-type="bibr">Shen et al. (2017)</xref> 76.1&#x0025;) our method outperformed <xref rid="c18" ref-type="bibr">Shen et al. (2017)</xref> but on human judgment metric the results from <xref rid="c18" ref-type="bibr">Shen et al. (2017)</xref> were better compared to our method (natural images: ours 95.7&#x0025;, <xref rid="c18" ref-type="bibr">Shen et al. (2017)</xref> 99.1&#x0025;). In the method from <xref rid="c18" ref-type="bibr">Shen et al. (2017)</xref>, they use a natural image prior that causes their reconstructions to look more natural and to outperform our method in terms of human judgment. We tried to introduce a natural &#x002D;image prior through using a discriminator but the reconstructions did not appear as natural as compared to the results of <xref rid="c18" ref-type="bibr">Shen et al. (2017)</xref>. However, in this work, our focus is not to propose a better reconstruction method, but to evaluate the potential of an end-to-end method to learn direct mapping from fMRI data to visual image.</p>
</sec>
<sec id="s3b">
<label>3.2.</label>
<title>Effect of dataset size</title>
<p>The results of the previous analyses show that it is possible to train the model with only 6,000 training samples from scratch. Therefore, we sought to investigate the effect of dataset size on the reconstruction quality. Furthermore, we attempted to check the possibility of improving the reconstruction quality by using more training samples.</p>
<p>We performed our analysis with an increasing training dataset from 120 to 6,000. We trained the model with 120, 300, 600, 1,500, 3,000, and 6,000 training samples and showed a qualitative comparison through the reconstructions (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) and quantitatively through the pixel correlation and human judgment scores (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Through visual inspection of the reconstruction results in <xref ref-type="fig" rid="fig4">Figure 4A</xref>, we could observe that reconstruction quality improves on increasing the number of training samples. Pixel-wise correlation and human judgment scores (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) also exhibit an increasing trend with increasing the number of training samples. The accuracy score improvement that follows with increasing number of training samples suggests that although we can obtain highly accurate reconstructions with only 6,000 training samples, there is still room for improvement, and better reconstruction quality could possibly be achieved given a larger dataset is available.</p>
<fig id="fig4" position="float" fig-type="figure" orientation="portrait">
<label>Figure 4</label>
<caption><title>Effect of training dataset size.</title>
<p><bold>(A)</bold> Reconstruction from brain activity (Subject 1) using reconstruction models trained with different training dataset sizes. The presented images (in black frames) are shown in the first column. The corresponding reconstructed images (in gray frames) are shown to the right of each presented image (from left to right, the number of training samples increases). <bold>(B)</bold> Reconstruction accuracy in terms of percentage of correct pair-wise classification based on both pixel correlation and human judgment (error bars, 95&#x0025; CI across samples; single subject (Subject 1), chance level, 50&#x0025;).</p></caption>
<graphic xlink:href="272518_fig4.tif"/>
</fig>
</sec>
<sec id="s3c">
<label>3.3.</label>
<title>Effect of loss functions: Ablation study</title>
<p>We performed an ablation study to understand the effect of different loss functions used in training the reconstruction model. We removed one loss function at a time and compared the reconstructions with those obtained using all three loss functions. <xref ref-type="fig" rid="fig5">Figure 5A</xref> shows the comparison results of the reconstructions obtained after removing one loss function at a time. On visually assessing the reconstruction results from <xref ref-type="fig" rid="fig5">Figure 5A</xref>, the reconstructions obtained from the model with all three loss terms show the best resemblance to the original stimulus images.</p>
<fig id="fig5" position="float" fig-type="figure" orientation="portrait">
<label>Figure 5</label>
<caption><title>Ablation study of loss terms.</title>
<p><bold>(A)</bold> Reconstruction from brain activity (Subject 1) using the reconstruction model with some components of loss removed. The presented images (in black frames) are shown in the first column. The corresponding reconstructed images (in gray frames) obtained with different models are shown to the right of each presented image (from right to left, the model is: full reconstruction model (<italic>Full</italic>), with image loss removed (&#x2212;<italic>L</italic><sub>img</sub>), with feature loss removed (&#x2212;<italic>L</italic><sub>feat</sub>), and with adversarial loss removed (&#x2212;<italic>L</italic><sub>adv</sub>). <bold>(B)</bold> Reconstruction accuracy in terms of winning percentage of pair-wise classification based on both pixel correlation and human judgment (error bars, 95&#x0025; CI across samples; single subject (Subject 1) chance level, 50&#x0025;).</p></caption>
<graphic xlink:href="272518_fig5.tif"/>
</fig>
<p>We observed that the difference in the human judgment and pixel correlation scores was not significant for the ablation study. So, to quantitatively compare the reconstruction quality for the ablation study we used the winning percentage as our criteria for comparison. The difference in winning percentage between the model optimized with all three loss terms and the model optimized with one loss term removed indicates the importance of the corresponding loss term. From <xref ref-type="fig" rid="fig5">Figure 5B</xref>, we can observe that the model trained with all three loss terms showed the highest winning percentage followed by the model where the loss in the image space is removed. The results demonstrate that the model trained with all three loss terms was preferred by the human raters as compared to the other models. Removing loss in the image space shows a similar drop for both of the winning percentage analyses (pixel correlation 21.3&#x0025;, human judgment 28.0&#x0025;) but the difference is not as pronounced as the other two loss functions. Removing feature loss shows the highest drop in the winning percentage for both spatial correlation (26.0&#x0025;) and human judgment (37.2&#x0025;). This demonstrates the importance of optimization in high dimensional feature space as it not only enhances the spatial details, but also makes the reconstruction more perceptually similar to its corresponding original stimulus image for human raters. Removing adversarial loss does not show much difference for spatial correlation criterion (6.0&#x0025;), but, in the case of human judgment criterion, the difference is high (37.2&#x0025;). This suggests that optimizing the adversarial loss forces the reconstruction to appear closer to natural image distribution.</p>
</sec>
</sec>
<sec id="s4">
<label>4.</label>
<title>Discussion</title>
<p>We have demonstrated that it is possible to learn a direct mapping function from fMRI activity in the visual cortex to the stimulus observed during perception. We showed this by performing an end-to-end training of a DNN model which reconstructs perceived stimuli from fMRI data. The reconstruction results on natural images obtained from the trained model show strong resemblance to the perceived stimuli in shape and in some cases color as well. Although trained only on natural images, the model generates accurate reconstructions of artificial shapes and alphabetical letters, thus showing good generalization comparable to <xref rid="c18" ref-type="bibr">Shen et al. (2017)</xref>. We also demonstrated that the reconstruction quality improves as we increase the number of training samples and thus we believe that with more training samples we may be able to further improve the reconstruction accuracy.</p>
<p>We performed an ablation study by removing one loss function at a time to understand the importance of each loss term used for training the proposed model. The results showed that the model trained with all three loss terms achieved the best performance in terms of winning percentage. The removal of loss in image space resulted in similar change in the winning percentage calculated from behavioral experiments and spatial correlation scores. The removal of feature loss showed a significant drop in the winning percentage for both human ratings and spatial correlation, though the drop in human ratings was more pronounced. This implies that optimization in feature space enhances both spatial and perceptual similarity of the reconstructed image with the original stimulus. The removal of adversarial loss showed no significant drop in terms of spatial correlation but the drop in human rating result is quite high. This suggests that the addition of adversarial loss in the optimization process constrains the reconstructed image to be closer to training image distribution, leading to a significant difference in human ratings.</p>
<p>Earlier studies on decoding the stimulus in pixel space either search for a match in the exemplar set (<xref rid="c14" ref-type="bibr">Naselaris et al., 2009</xref>; <xref rid="c15" ref-type="bibr">Nishimoto et al., 2011</xref>) or try to obtain the exact reconstruction of the stimulus (<xref rid="c13" ref-type="bibr">Miyawaki et al., 2008</xref>; <xref rid="c16" ref-type="bibr">Wen et al. 2017</xref>; <xref rid="c5" ref-type="bibr">G&#x00FC;&#x00E7;l&#x00FC;t&#x00FC;rk et al. 2017</xref>; <xref rid="c18" ref-type="bibr">Shen et al. 2017</xref>; <xref rid="c6" ref-type="bibr">Han et al. 2017</xref>; <xref rid="c17" ref-type="bibr">Seeliger et al. 2017</xref>). In the exemplar matching methods, the visualization is limited to the samples in the exemplar set and hence these methods cannot be generalized to stimuli that are not included in the exemplar set. The reconstruction methods, however, are more robust to generalization to a new stimulus domain. Recent work from <xref rid="c18" ref-type="bibr">Shen et al. (2017)</xref> extended the reconstruction approach by capitalizing on multiple layers of DNN features, which were predicted from brain activity. They show that the decoders trained on only natural images can be successfully used to obtain reconstructions of artificial shapes and alphabetical letters.</p>
<p>DNN based reconstruction methods (<xref rid="c5" ref-type="bibr">G&#x00FC;&#x00E7;l&#x00FC;t&#x00FC;rk et al. 2017</xref>; <xref rid="c18" ref-type="bibr">Shen et al. 2017</xref>; <xref rid="c6" ref-type="bibr">Han et al. 2017</xref>, <xref rid="c17" ref-type="bibr">Seeliger et al. 2017</xref>) avoided directly training a DNN model for reconstruction. Instead, they used decoded features as an intermediate representation of the fMRI activity that was used as the input to a reconstruction module. This method is effective since the decoded features can easily be plugged into known image reconstruction/generation methods. It is also thought to be efficient given the lack of large-scale diverse fMRI datasets as compared to computer vision datasets used for end-to-end training of vision tasks. This makes it difficult to learn a direct mapping from brain activity to stimulus space without overfitting to the training dataset. Thus, learning this direct mapping from limited training samples was the main motivation of this work.</p>
<p>A potential advantage of the direct mapping is that it is free from constraints imposed by the pre-trained DNN and the features derived from a large scale image dataset. Even though the decoded features are correlated with the original image features, in <xref rid="c8" ref-type="bibr">Horikawa and Kamitani (2017)</xref> the maximum correlation coefficient on average was less than 0.5. So, we do not believe that information in the decoded features is the maximum visual information that can be extracted from the brain. Therefore, if enough training samples are available, a direct mapping may help in preventing this information loss.</p>
<p>In the present study, we demonstrated that it is possible to skip the intermediate step of feature decoding by an end-to-end approach, which allows us to learn a direct mapping from fMRI data to the perceived stimulus. Although the reconstructions obtained using the proposed method were highly accurate with good generalizability, our method could not outperform the method using decoded features on the available dataset. However, the results from the dataset size analysis suggest that the reconstruction quality could possibly be improved by increasing the size of the training dataset.</p>
</sec>
</body>
<back>
<sec id="s5">
<title>Conflict of Interest</title>
<p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
</sec>
<sec id="s6">
<title>Author contributions</title>
<p>Y.K. directed the study. G.S., K.D., and K.M. developed the reconstruction methods, G.S. and T.H. performed the experiments and analyses. K.D. and Y.K. wrote the paper.</p>
</sec>
<sec id="s7" sec-type="funding">
<title>Funding</title>
<p>This research was supported by grants from the New Energy and Industrial Technology Development Organization (NEDO), JSPS KAKENHI Grant number JP15H05710, JP15H05920, JP26870935, and ImPACT Program of Council for Science, Technology and Innovation (Cabinet Office, Government of Japan).</p>
</sec>
<ack>
<title>Acknowledgements</title>
<p>The authors thank Mohamed Abdelhack for valuable comments on the manuscript and Mitsuaki Tsukamoto for the help in computational environmental setting.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="confproc"><string-name><surname>Deng</surname>, <given-names>Jia</given-names></string-name>, <etal>et al.</etal> &#x201C;<article-title>Imagenet: A large-scale hierarchical image database</article-title>.&#x201D; <conf-name>Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference</conf-name> on. <conf-sponsor>IEEE</conf-sponsor>, <year>2009</year>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="confproc"><string-name><surname>Dosovitskiy</surname>, <given-names>Alexey</given-names></string-name>, and <string-name><surname>Thomas</surname> <given-names>Brox</given-names></string-name>. &#x201C;<article-title>Inverting visual representations with convolutional networks</article-title>.&#x201D; <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>. <year>2016</year>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Dosovitskiy</surname>, <given-names>Alexey</given-names></string-name>, and <string-name><surname>Thomas</surname> <given-names>Brox</given-names></string-name>. &#x201C;<article-title>Generating images with perceptual similarity metrics based on deep networks</article-title>.&#x201D; <source>Advances in Neural Information Processing Systems</source>. <year>2016</year>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Goodfellow</surname>, <given-names>Ian</given-names></string-name>, <etal>et al.</etal> &#x201C;<article-title>Generative adversarial nets</article-title>.&#x201D; <source>Advances in neural information processing systems</source>. <year>2014</year>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="other"><string-name><surname>G&#x00FC;&#x00E7;l&#x00FC;t&#x00FC;rk</surname>, <given-names>Ya&#x011F;mur</given-names></string-name>, <etal>et al.</etal> &#x201C;<article-title>Deep adversarial neural decoding</article-title>.&#x201D; <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1705.07109</pub-id> (<year>2017</year>).</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="other"><string-name><surname>Han</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Wen</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Shi</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>K.H.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name> and <string-name><surname>Liu</surname>, <given-names>Z.</given-names></string-name>, <year>2017</year>. <article-title>Variational autoencoder: An unsupervised model for modeling and decoding fMRI activity in visual cortex</article-title>. <source>bioRxiv</source>, p.<fpage>214247</fpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="confproc"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Sun</surname>, <given-names>J.</given-names></string-name>, <year>2015</year>. <article-title>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</article-title>. In <conf-name>Proceedings of the IEEE international conference on computer vision</conf-name> (pp. <fpage>1026</fpage>&#x2013;<lpage>1034</lpage>).</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Horikawa</surname>, <given-names>T.</given-names></string-name>, &#x0026; <string-name><surname>Kamitani</surname>, <given-names>Y.</given-names></string-name> (<year>2017</year>). <article-title>Generic decoding of seen and imagined objects using hierarchical visual features</article-title>. <source>Nature communications</source>, <fpage>8</fpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="confproc"><string-name><surname>Jia</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Shelhamer</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Donahue</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Karayev</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Long</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Girshick</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Guadarrama</surname>, <given-names>S.</given-names></string-name> and <string-name><surname>Darrell</surname>, <given-names>T.</given-names></string-name>, <year>2014 November</year>,. <article-title>Caffe: Convolutional architecture for fast feature embedding</article-title>. In <conf-name>Proceedings of the 22nd ACM international conference on Multimedia</conf-name> (pp. <fpage>675</fpage>&#x2013;<lpage>678</lpage>). <publisher-name>ACM</publisher-name>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="other"><string-name><surname>Kingma</surname>, <given-names>D.</given-names></string-name> and <string-name><surname>Ba</surname>, <given-names>J.</given-names></string-name>, <year>2014</year>. <article-title>Adam: A method for stochastic optimization</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1412.6980</pub-id></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sutskever</surname>, <given-names>I.</given-names></string-name> and <string-name><surname>Hinton</surname>, <given-names>G.E.</given-names></string-name>, <year>2012</year>. <article-title>Imagenet classification with deep convolutional neural networks</article-title>. <source>In Advances in neural information processing systems</source> (pp. <fpage>1097</fpage>&#x2013;<lpage>1105</lpage>).</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="other"><string-name><surname>Mansimov</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Parisotto</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Ba</surname>, <given-names>J.L.</given-names></string-name> and <string-name><surname>Salakhutdinov</surname>, <given-names>R.</given-names></string-name>, <year>2015</year>. <article-title>Generating images from captions with attention</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1511.02793</pub-id>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Miyawaki</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Uchida</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Yamashita</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Sato</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Morito</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Tanabe</surname>, <given-names>H. C.</given-names></string-name>, &#x2026; &#x0026; <string-name><surname>Kamitani</surname>, <given-names>Y.</given-names></string-name> (<year>2008</year>). <article-title>Visual image reconstruction from human brain activity using a combination of multiscale local image decoders</article-title>. <source>Neuron</source>, <volume>60</volume>(<issue>5</issue>), <fpage>915</fpage>&#x2013;<lpage>929</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Naselaris</surname>, <given-names>Thomas</given-names></string-name>, <etal>et al.</etal> &#x201C;<article-title>Bayesian reconstruction of natural images from human brain activity</article-title>.&#x201D; <source>Neuron</source> <volume>63</volume>.<issue>6</issue> (<year>2009</year>): <fpage>902</fpage>&#x2013;<lpage>915</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Nishimoto</surname>, <given-names>Shinji</given-names></string-name>, <etal>et al.</etal> &#x201C;<article-title>Reconstructing visual experiences from brain activity evoked by natural movies</article-title>.&#x201D; <source>Current Biology</source> <volume>21</volume>.<issue>19</issue> (<year>2011</year>): <fpage>1641</fpage>&#x2013;<lpage>1646</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="other"><string-name><surname>Wen</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Shi</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Lu</surname>, <given-names>K.H.</given-names></string-name> and <string-name><surname>Liu</surname>, <given-names>Z.</given-names></string-name>, <year>2016</year>. <article-title>Neural Encoding and Decoding with Deep Learning for Dynamic Natural Vision</article-title>. <source>arXiv preprint</source> arXiv:<pub-id pub-id-type="arxiv">1608.03425</pub-id>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="other"><string-name><surname>Seeliger</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>G&#x00FC;&#x00E7;l&#x00FC;</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Ambrogioni</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>G&#x00FC;&#x00E7;l&#x00FC;t&#x00FC;rk</surname>, <given-names>Y.</given-names></string-name> and <string-name><surname>van Gerven</surname>, <given-names>M.A.J.</given-names></string-name>, <year>2017</year>. <article-title>Generative adversarial networks for reconstructing natural images from brain activity</article-title>. <source>bioRxiv</source>, p.<fpage>226688</fpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="other"><string-name><surname>Shen</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Horikawa</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Majima</surname>, <given-names>K.</given-names></string-name> and <string-name><surname>Kamitani</surname>, <given-names>Y.</given-names></string-name>, <year>2017</year>. <article-title>Deep image reconstruction from human brain activity</article-title>. <source>bioRxiv</source>, p.<fpage>240317</fpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>