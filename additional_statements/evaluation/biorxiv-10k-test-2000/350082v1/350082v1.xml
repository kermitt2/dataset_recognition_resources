<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/350082</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The Role of Sensory Uncertainty in Simple Perceptual Organization</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0350-6467</contrib-id>
<name>
<surname>Zhou</surname>
<given-names>Yanli</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">&#x262F;</xref>
<xref ref-type="author-notes" rid="n3">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Acerbi</surname>
<given-names>Luigi</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">&#x262F;</xref>
<xref ref-type="author-notes" rid="n2">&#x00A4;</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9835-9083</contrib-id>
<name>
<surname>Ma</surname>
<given-names>Wei Ji</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Center for Neural Science, New York University</institution>, New York, NY, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution>Department of Psychology, New York University</institution>, New York, NY, <country>USA</country></aff>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>&#x262F;</label><p>These authors contributed equally to this work.</p></fn>
<fn id="n2" fn-type="present-address"><label>&#x00A4;</label><p>Current address: D&#x00E9;partement des neurosciences fondamentales, Universit&#x00E9; de Gen&#x00E8;ve, CMU, 1 rue Michel-Servet, 1206 Gen&#x00E8;ve, Switzerland.</p></fn>
<fn id="n3"><label>&#x002A;</label><p><email>yanlizhou@nyu.edu</email></p></fn>
</author-notes>
<pub-date pub-type="epub"><year>2018</year></pub-date>
<elocation-id>350082</elocation-id>
<history>
<date date-type="received">
<day>18</day>
<month>6</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>18</day>
<month>6</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>18</day>
<month>6</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="350082.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Perceptual organization is the process of grouping scene elements into whole entities, for example line segments into contours. Uncertainty in grouping arises from scene ambiguity and sensory noise. Some classic Gestalt principles of perceptual organization have been re-framed in terms of Bayesian models, whereby the observer computes the probability that the whole entity is present. Previous studies that proposed a Bayesian interpretation of perceptual organization, however, have ignored sensory uncertainty, despite the fact that accounting for the current level of uncertainty is the main signature of Bayesian decision making. Crucially, trial-by-trial manipulation of sensory uncertainty is necessary to test whether humans perform optimal Bayesian inference in perceptual organization, as opposed to using some non-Bayesian heuristic. We distinguish between these hypotheses in an elementary form of perceptual organization, namely judging whether two line segments separated by an occluder are collinear. We manipulate sensory uncertainty by varying retinal eccentricity. A Bayes-optimal observer would take the level of sensory uncertainty into account &#x2013; in a very specific way - in deciding whether a measured offset between the line segments is due to non-collinearity or to sensory noise. We find that people deviate slightly but systematically from Bayesian optimality, while still performing &#x201C;probabilistic computation&#x201D; in the sense that they take into account sensory uncertainty via a heuristic rule. Our work contributes to an understanding of the role of sensory uncertainty in higher-order perception.</p>
<sec>
<title>Author summary</title>
<p>Our percept of the world is governed not only by the sensory information we have access to, but also by the way we interpret this information. When presented with a visual scene, our visual system undergoes a process of grouping visual elements together to form coherent entities so that we can interpret the scene more readily and meaningfully. For example, when looking at a pile of autumn leaves, one can still perceive and identify a whole leaf even when it is partially covered by another leaf. While Gestalt psychologists have long described perceptual organization with a set of qualitative laws, recent studies offered a statistically-optimal &#x2013; Bayesian, in statistical jargon &#x2013; interpretation of this process, whereby the observer chooses the scene configuration with the highest probability given the available sensory inputs. However, these studies drew their conclusions without considering a key actor in this kind of statistically-optimal computations, that is the role of sensory uncertainty. One can easily imagine that our decision on whether two contours belong to the same leaf or different leaves is likely going to change when we move from viewing the pile of leaves at a great distance (high sensory uncertainty), to viewing very closely (low sensory uncertainty). Our study examines whether and how people incorporate uncertainty into perceptual organization, by varying sensory uncertainty from trial to trial in a simple perceptual organization task. We found that people indeed take into account sensory uncertainty, however in a way that subtly deviates from optimal behavior.</p>
</sec>
</abstract>
<counts>
<page-count count="26"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Perceptual organization is the process whereby the brain integrates primitive elements of a visual scene into whole entities. Typically, the same scene could afford different interpretations because of ambiguity and perceptual noise. How the brain singles out one interpretation has long been described to follow a set of qualitative principles defined in Gestalt psychology. For example, contour integration, a form of perceptual organization that consists of the perceptual grouping of distinct line elements into a single continuous contour, is often described by the Gestalt principles of &#x201C;good continuation&#x201D; and &#x201C;proximity&#x201D;. They state that humans extrapolate reasonable object boundaries by grouping local contours consistent with a smooth global structure [<xref rid="c1" ref-type="bibr">1</xref>].</p>
<p>While Gestalt principles represent a useful catalogue of well-established perceptual phenomena, they lack a theoretical basis, cannot make quantitative predictions, and are agnostic with respect to uncertainty arising from sensory noise. This not only limits understanding at the psychological level, it is also problematic within a broader agenda of quantitatively linking neural activity in different brain areas to behavior. For example, neural investigations of the perception of illusory contours, a perceptual organization phenomenon in which the observer perceives object contours when they are not physically present, have largely remained at a qualitative level. An alternative approach that does not suffer from these shortcomings uses the framework of Bayesian inference, whereby the observer computes the probabilities of possible world states given sensory observations using Bayes&#x2019; rule [<xref rid="c2" ref-type="bibr">2</xref>]. In the realm of perceptual organization, Bayesian models stipulate that the observer computes the probabilities of different hypotheses about which elements belong to the same object (e.g., [<xref rid="c3" ref-type="bibr">3</xref>&#x2013;<xref rid="c6" ref-type="bibr">6</xref>]). For the example of contour integration, such hypotheses would be that line elements belong to the same contour and that they belong to different contours.</p>
<p>A fully Bayesian approach to perceptual organization would provide a normative way for dealing both with high-level uncertainty arising from ambiguity in the latent structure of the scene, and with low-level (sensory) uncertainty arising from noise in measuring primitive elements of the scene. Crucially, however, previous studies in perceptual organization have not examined whether the decision rule adapts flexibly as function of sensory uncertainty. Such adaptation is a form of <italic>probabilistic computation</italic> and is one of the basic signatures of Bayesian optimality [<xref rid="c7" ref-type="bibr">7</xref>]. This question is fundamental to understanding whether, how, and to which extent the brain represents and computes with probability distributions [<xref rid="c8" ref-type="bibr">8</xref>]. A trial-by-trial manipulation of sensory uncertainty is a necessary test of probabilistic computation, because otherwise Bayesian inference would be indistinguishable from an observer using an inflexible, uncertainty-independent mapping [<xref rid="c9" ref-type="bibr">9</xref>,<xref rid="c10" ref-type="bibr">10</xref>]. Indeed, manipulation of sensory uncertainty has been a successful approach for studying probabilistic computation in low-level perception, such as in multisensory cue combination [<xref rid="c11" ref-type="bibr">11</xref>,<xref rid="c12" ref-type="bibr">12</xref>] and in integration of sensory measurements with prior expectations [<xref rid="c13" ref-type="bibr">13</xref>,<xref rid="c14" ref-type="bibr">14</xref>]. Moreover, tasks with varying uncertainty have yielded insights into the neural representation of uncertainty [<xref rid="c15" ref-type="bibr">15</xref>,<xref rid="c16" ref-type="bibr">16</xref>].</p>
<p>In the current study, we investigate the effect of varying sensory uncertainty on an elementary form of perceptual organization. Specifically, we manipulate sensory uncertainty unpredictably on a trial-to-trial basis by changing stimulus retinal eccentricity in a simple collinearity judgment task. Our experimental manipulation allows for a stronger test of the hypothesis that perceptual grouping is Bayes-optimal, at least for the atomic case of collinearity judgment of two line segments. However, this is not enough; proper comparison of Bayesian models against plausible alternatives is critical in establishing the theoretical standing of the Bayesian approach [<xref rid="c17" ref-type="bibr">17</xref>&#x2013;<xref rid="c19" ref-type="bibr">19</xref>]. As an alternative to performing the complex, hierarchical computations characteristic of optimal Bayesian inference, the brain might draw instead on simpler non-Bayesian decision rules and even non-probabilistic heuristics [<xref rid="c20" ref-type="bibr">20</xref>,<xref rid="c21" ref-type="bibr">21</xref>] such as grouping scene elements based on some simple, learned rule. In contour integration, line elements would belong to the same contour if they were close enough in space and orientation, independently of other properties of the scene. Therefore, we rigorously compare the Bayes-optimal decision rule against alternative non-Bayesian ones, both probabilistic and non-probabilistic. While we find compelling evidence of probabilistic computation, a probabilistic, non-Bayesian heuristic model outperforms the Bayes-optimal model, suggesting a form of sub-optimality in the decision-making process. Our study paves the way for a combined understanding of how different sources of uncertainty affect perceptual organization.</p>
</sec>
<sec id="s2">
<title>Results</title>
<p>Subjects (<italic>n</italic> &#x003D; 8) performed a <italic>collinearity judgment</italic> task (<xref ref-type="fig" rid="fig1">Fig 1A</xref>). On each trial, the participant was presented with a vertical occluder and the stimulus consisted of two horizontal lines of equal length on each side of the occluder. At stimulus offset, the participant reported whether the two lines were collinear or not via a single key press. To avoid the learning of a fixed mapping, we withheld correctness feedback. In different blocks in the same sessions, participants also completed a <italic>height judgment task</italic> (<xref ref-type="fig" rid="fig1">Fig 1B</xref>), with the purpose of providing us with an independent estimate of the participants&#x2019; sensory noise. In both tasks, sensory uncertainty was manipulated by varying retinal eccentricity on a trial to trial basis (<xref ref-type="fig" rid="fig1">Fig 1D</xref>). We investigated whether people took into account their sensory noise <italic>&#x03C3;</italic><sub><italic>x</italic></sub>(<italic>y</italic>), which varied with eccentricity level <italic>y</italic>, when deciding collinearity.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig 1.</label>
<caption><title>Tasks and generative model.</title>
<p>A: Collinearity judgment task. After stimulus offset, participants reported if the line segments belonged to the same line or different lines. B: Height judgment task. Participants reported whether the left line segment was higher or the right line segment was higher. C: Generative model of the collinearity judgment task. Trial type <italic>C</italic> &#x003D; 1 when the two lines segments are collinear, and <italic>C</italic> &#x003D; 0 when line segments are non-collinear. On a given trial, the stimulus pair <italic>y</italic><sub><italic>L</italic></sub>,<italic>y</italic><sub><italic>R</italic></sub> randomly appeared around one of four eccentricity levels (<italic>y</italic> &#x003D; 0,4.8, 9.6, andl6.8), measured by degrees of visual angle (dva). For all models, the observer&#x2019;s measurements <italic>x</italic><sub><italic>L</italic></sub>,<italic>x</italic><sub><italic>R</italic></sub> are assumed to follow a Gaussian distribution centered on the true stimulus <italic>y</italic><sub><italic>L</italic></sub>,<italic>y</italic><sub><italic>R</italic></sub> respectively, with standard deviation <italic>&#x03C3;</italic><sub><italic>x</italic></sub>(<italic>y</italic>) dependent on eccentricity level <italic>y</italic>. D: Possible eccentricity levels (in dva). E: Stimulus distribution for collinearity judgment task. When <italic>C</italic> &#x003D; 1, the vertical position of the left line segment <italic>y</italic><sub><italic>L</italic></sub> is drawn from a Gaussian distribution centered at <italic>y</italic> with fixed standard deviation. The vertical position of the right segment <italic>y</italic><sub><italic>R</italic></sub> is then made equal to <italic>y</italic><sub><italic>L</italic></sub>. When <italic>C</italic> &#x003D; 0, <italic>y</italic><sub><italic>L</italic></sub> and <italic>y</italic><sub><italic>R</italic></sub> are independently drawn from the same Gaussian.</p></caption>
<graphic xlink:href="350082_fig1.tif"/>
</fig>
<p>We found a main effect of vertical offset on the proportion of collinearity reports (two-way repeated-measures ANOVA with Greenhouse-Geisser correction; <italic>F</italic><sub>(3.69,114)</sub> &#x003D; 101, <italic>&#x03F5;</italic> &#x003D; 0.461, <italic>p</italic> &#x003C; 0.001, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="350082_inline1.gif"/></alternatives></inline-formula>) and a main effect of eccentricity (<italic>F</italic><sub>(2.38,169)</sub> &#x003D; 51.2, <italic>&#x03F5;</italic> &#x003D; 0.794, <italic>p</italic> &#x003C; 0.001, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="350082_inline2.gif"/></alternatives></inline-formula>), suggesting that the experimental manipulations were effective (<xref ref-type="fig" rid="fig2">Fig 2A,B</xref>). We also found a significant interaction between offset and eccentricity (<italic>F</italic><sub>(4.38,30.7)</sub> &#x003D; 7.88, <italic>&#x03f5;</italic> &#x003D; 0.183, <italic>p</italic> &#x003C; 0.001, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="350082_inline3.gif"/></alternatives></inline-formula>), which is evident in the psychometric curves across subjects (<xref ref-type="fig" rid="fig2">Fig 2C</xref>).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig 2.</label>
<caption><title>Collinearity judgement task data.</title>
<p>A: Accuracy as a function of retinal eccentricity level (chance probability &#x003D; 0.5). B: Proportion of reporting &#x201C;collinear&#x201D; as a function of vertical offset between the two line segments. C: Proportion of reporting &#x201C;collinear&#x201D; as a function of vertical offset of the two line segments at each eccentricity level. Error bars indicate Mean &#x00B1; 1 SEM across 8 subjects.</p></caption>
<graphic xlink:href="350082_fig2.tif"/>
</fig>
<p>We did not find significant effects of learning across sessions (see S1 Appendix), so in our analyses for each subject we pooled data from all sessions.</p>
</sec>
<sec id="s3">
<title>Models</title>
<p>We describe here three main observer models which correspond to different assumptions with respect to when the observer reports &#x201C;collinear&#x201D;, that is three different forms of decision boundaries (<xref ref-type="fig" rid="fig3">Fig 3</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig 3.</label>
<caption><title>Decision boundaries for fixed-criterion (Fixed), Bayesian (Bayes) and linear heuristic (Lin) models (left to right).</title>
<p>The probability of reporting &#x201C;collinear&#x201D; given stimulus and eccentricity condition is equal to the probability that the observer&#x2019;s measurements of vertical positions of left and right line segments fall within the boundary defined by the model.</p></caption>
<graphic xlink:href="350082_fig3.tif"/>
</fig>
<p>We first consider the behavior of a Bayes-optimal observer (&#x201C;Bayes&#x201D;) who utilizes the probability distributions defined in the generative model (<xref ref-type="fig" rid="fig1">Fig 1C</xref>) to make decisions that maximize the probability of being correct, given the available sensory measurements. In particular, the Bayes-optimal observer accounts for uncertainty when deciding whether a measured offset between the line segments is due to non-collinearity or to sensory noise by choosing the category (<italic>C</italic> &#x003D; 1 or <italic>C</italic> &#x003D; 0) with the highest posterior probability <italic>p</italic>(<italic>C</italic>&#x007C;<italic>x</italic><sub><italic>L</italic></sub>,<italic>x</italic><sub><italic>R</italic></sub>), where <italic>x</italic><sub><italic>L</italic></sub>,<italic>x</italic><sub><italic>R</italic></sub> are measurements of the two line segments on a particular trial. This strategy translates into reporting &#x201C;collinear&#x201D; when <italic>x</italic><sub><italic>L</italic></sub>,<italic>x</italic><sub><italic>R</italic></sub> fall within the optimal decision boundary, which is a function of (a) both measurements &#x2013; not simply their difference &#x2013;, (b) sensory noise (that is, eccentricity) in the trial, and (c) the prior belief about the proportion of collinear trials <italic>p</italic>(<italic>C</italic> &#x003D; 1) (<xref ref-type="fig" rid="fig3">Fig 3B</xref>). Note that a strictly optimal observer would have a prior that matches the experimental distribution, <italic>p</italic>(<italic>C</italic> &#x003D; 1) &#x003D; 0.5. Here we relaxed the assumption and allowed <italic>p</italic>(<italic>C</italic> &#x003D; 1) to be a free parameter. The Bayes-optimal model assumes that the observer knows the noise level associated with the current trial (see Model variants for a relaxation of this assumption).</p>
<p>To investigate whether people apply instead a learned stimulus mapping that is uncertainty independent, we tested a fixed-criterion model (&#x201C;Fixed&#x201D;) in which the observer responds that two line segments are collinear whenever the measured offset &#x007C;<italic>x</italic><sub><italic>L</italic></sub> &#x2212; <italic>x</italic><sub><italic>R</italic></sub>&#x007C; is less than a fixed distance <italic>&#x03BA;</italic> (a free parameter of the model). This corresponds to an eccentricity-invariant decision boundary (<xref ref-type="fig" rid="fig3">Fig 3A</xref>).</p>
<p>Finally, we also considered an instance of probabilistic, non-Bayesian computation via a heuristic model (&#x201C;Lin&#x201D;) in which the observer takes stimulus uncertainty into account in a simple, linear way: the observer responds &#x201C;collinear&#x201D; whenever the measured offset &#x007C;<italic>x</italic><sub><italic>L</italic></sub> &#x2212; <italic>x</italic><sub><italic>R</italic></sub>&#x007C; is less than an uncertainty-dependent criterion,
<disp-formula id="eqn1"><alternatives><graphic xlink:href="350082_eqn1.gif"/></alternatives>
</disp-formula>
where <italic>&#x03BA;</italic><sub>0</sub> and <italic>&#x03BA;</italic><sub>1</sub> are free parameters of the model (<xref ref-type="fig" rid="fig3">Fig 3C</xref>).</p>
</sec>
<sec id="s4">
<title>Model comparison</title>
<p>To fully account for parameter uncertainty, we used Markov Chain Monte Carlo (MCMC) to sample the posterior distributions of the parameters for each model and individual subject. To estimate goodness of fit (that is, predictive accuracy) while taking into account model complexity, we compared models using the leave-one-out cross-validation score (LOO), estimated on a subject-by-subject basis directly from the MCMC posterior samples via Pareto smoothed importance sampling [<xref rid="c22" ref-type="bibr">22</xref>]. Higher LOO scores correspond to better predictive accuracy and, thus, better models.</p>
<p>We found that the fixed-criterion model fits the worst (LOO<sub>Bayes</sub> &#x2212; LOO<sub>Fixed</sub> &#x003D; 25.6 &#x00B1; 13.6, LOO<sub>Lin</sub> &#x2212; LOO<sub>Fixed</sub> &#x003D; 69.3 &#x00B1; 16.5; Mean &#x00B1; SEM across subjects), while also yielding the poorest qualitative fits to the behavioral data (<xref ref-type="fig" rid="fig4">Fig 4A</xref>). This result suggests that participants used not only their measurements but also sensory uncertainty from trial to trial, thus providing first evidence for probabilistic computation in collinearity judgment. Moreover, we find that the linear heuristic model performs better than the Bayesian model (LOO<sub>Lin</sub> &#x2014; LOO<sub>Bayes</sub> &#x003D; 43.7 &#x00B1; 13.3), suggestive of a suboptimal way of taking uncertainty into account.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig 4.</label>
<caption><title>Model fits and model comparison for fixed-criterion (Fixed), Bayes-optimal (Bayes) and linear heuristic (Lin) models (from left to right).</title>
<p>A: Model fits to proportion of responding collinear as a function of vertical offset of the two line segments. Error bars indicate Mean &#x00B1; 1 SEM across subjects. Shaded regions indicates Mean &#x00B1; 1 SEM of fits for each model, with each model on a separate row. B: Model comparison via leave-one-out cross-validation score (LOO). Bars indicate individual subjects&#x2019; LOO scores for every model, relative to the fixed-criterion model. A positive value indicates that the model in the corresponding row had a better LOO score than the fixed-criterion model. Shaded regions indicate Mean &#x00B1; 1 SEM in LOO differences across subjects. The Lin model won the model comparison, whereas Fixed was the worst model.</p></caption>
<graphic xlink:href="350082_fig4.tif"/>
</fig>
<p>To allow for model heterogeneity across subjects, we also combined model evidence from different subjects using a hierarchical Bayesian approach that treats the model as a random variable to accommodate between-subject random effects [<xref rid="c23" ref-type="bibr">23</xref>]. This method allowed us to compute the expected posterior frequency for each model, that is the probability that a randomly chosen subject belongs to a particular model in the comparison. This analysis confirmed our previous model comparison ordering, with the Fixed model having the lowest expected frequency (0.11 &#x00B1; 0.09), Bayes the second highest (0.18 &#x00B1; 0.11) and Lin by far the highest (0.71 &#x00B1; 0.13). We also calculated the protected exceedance probability [<xref rid="c24" ref-type="bibr">24</xref>], that is the probability that a particular model is the most frequent model in the set, above and beyond chance. We found consistent results - namely the Fixed model has the lowest protected exceedance probability (0.048), followed by Bayes (0.062), and Lin (0.89).</p>
</sec>
<sec id="s5">
<title>Validation of noise parameters</title>
<p>In all analyses so far, the observer&#x2019;s sensory noise levels at each eccentricity level <italic>&#x03C3;</italic><sub><italic>x</italic></sub>(<italic>y</italic>) were individually fitted as free parameters (four noise parameters, one per eccentricity level). To obtain an independent estimate of the subjects&#x2019; noise, and thus verify if the noise parameters estimated from the collinearity task data truly capture subjects&#x2019; sensory noise, we introduced in the same sessions an independent Vernier discrimination task (height judgment task) [<xref rid="c25" ref-type="bibr">25</xref>,<xref rid="c26" ref-type="bibr">26</xref>]. In this task, participants judged whether the right line segment was displaced above or below the left line segment (<xref ref-type="fig" rid="fig1">Fig 1B</xref> and <xref ref-type="fig" rid="fig5">Fig 5A</xref>). Importantly, the observer&#x2019;s optimal decision rule in this task is based solely on the sign of the observer&#x2019;s measured offset between the line segments, and does not depend on sensory noise (that is, respond &#x201C;right segment higher&#x201D; whenever <italic>x</italic><sub><italic>R</italic></sub> &#x003E; <italic>x</italic><sub><italic>L</italic></sub>). Moreover, trials in this task matched the stimulus statistics used in non-collinear trials of the collinearity judgment task. Therefore, the height judgment task afforded an independent set of estimates of subjects&#x2019; noise levels.</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Fig 5.</label>
<caption><title>Height judgment task results.</title>
<p>A: Height judgment task data. Proportion of reporting &#x201C;right line segment higher&#x201D; is plotted as a function of vertical offset between line segments. Error bars indicate Mean &#x00B1; 1 SEM across subjects. B: Noise parameters estimated from the best-fitting model, linear heuristic (Lin), on collinearity judgment task vs. noise parameters estimated from the height judgment task, in dva. Each dot corresponds to a subject&#x2019;s estimated noise parameters for a given eccentricity level. C: Models&#x2019; fits to collinearity judgment task data when noise parameters estimated from the height judgment task were imported into the models. Shaded regions indicate Mean &#x00B1; 1 SEM of fits. See <xref ref-type="fig" rid="fig4">Fig 4A</xref> for comparison. D: Model comparison on collinearity judgment task data via LOO, constrained by importing noise parameters from the height judgment task. Results are consistent with the model comparison ordering we found in the original unconstrained fits, with free noise parameters (see <xref ref-type="fig" rid="fig4">Fig 4B</xref> for comparison).</p></caption>
<graphic xlink:href="350082_fig5.tif"/>
</fig>
<p>Repeated-measures ANOVA indicated a main effect of the vertical offset between the two line segments on the proportion of reports &#x201C;right higher&#x201D; (<italic>F</italic><sub>(2.58,80.1)</sub> &#x003D; 320, <italic>&#x03F5;</italic> &#x003D; 0.323, <italic>p</italic> &#x003C; 0.001, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="350082_inline4.gif"/></alternatives></inline-formula>), no main effect of eccentricity (<italic>F</italic><sub>(1.99,141)</sub> &#x003D; 0.300, <italic>&#x03F5;</italic> &#x003D; 0.662, <italic>p</italic> &#x003D; 0.740, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="350082_inline5.gif"/></alternatives></inline-formula>), and an interaction between eccentricity and offset (<italic>F</italic><sub>(4.67,32.7</sub>) &#x003D; 8.75, <italic>&#x03F5;</italic> &#x003D; 0.195, <italic>p</italic> &#x003C; 0.001, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="350082_inline6.gif"/></alternatives></inline-formula>). These findings confirm that, as expected, participants in the height judgement task took into account the offset, and their performance was also affected simultaneously by offset and eccentricity (that is, sensory noise).</p>
<p>We found that sensory noise parameters estimated from the best model (Lin) in the collinearity task were well correlated &#x2013; across subjects and eccentricities &#x2013; with those estimated from the height judgment task (<italic>r</italic> &#x003D; 0.87) (<xref ref-type="fig" rid="fig5">Fig 5B</xref>), indicating that the model is correctly capturing subjects&#x2019; noise characteristics in collinearity judgment.</p>
<p>We next examined whether the model comparison between Bayes, Fixed, and Lin could be constrained using the parameter estimates obtained from the height judgment task, and whether such a constrained comparison would alter our findings. For each subject and each eccentricity level, we imported the maximum-a-posteriori noise parameter of that subject at that eccentricity level, as estimated from the height judgment task, into the model for the collinearity task. This left the Bayes, Fixed, and Lin models with only 2, 2, and 3 parameters, respectively, which we estimated via MCMC as previously described. The fits of the constrained models were comparable to those of their unconstrained counterparts (compare <xref ref-type="fig" rid="fig5">Fig 5C</xref> to 4A). The quantitative comparison of the constrained models was also consistent with that of the unconstrained models (compare <xref ref-type="fig" rid="fig5">Fig 5D</xref> to 4B): LOO<sub>Bayes</sub> &#x2212; LOO<sub>Fixed</sub> &#x003D; 93.5 &#x00B1; 26.7, LOO<sub>Lin</sub> &#x2212; LOO<sub>Fixed</sub> &#x003D; 142.4 &#x00B1; 28.3. Overall, this analysis shows that our models correctly captured subjects&#x2019; noise features, and that our conclusions are not merely due to excessive flexibility of our models, as we obtain the same results with models with very few free parameters.</p>
</sec>
<sec id="s6">
<title>Suboptimality analysis</title>
<p>In the previous sections we have found that the Lin model wins the model comparison against the Bayes-optimal model, suggestive of suboptimal behavior among participants. Here we closely examine the degree of suboptimality in terms of the loss of accuracy in the collinearity task with respect to Bayesian optimal behavior.</p>
<p>In order to assess the accuracy that an observer with a given set of noise parameters could achieve, had they performed Bayes-optimally, we proceeded as follows. For each subject, we generated a simulated dataset from the Bayes-optimal model using the maximum-a-posteriori noise parameters <italic>&#x03C3;</italic><sub><italic>x</italic></sub>(<italic>y</italic>) estimated from both the collinearity judgment task and the height judgment task. We used both estimates to ensure that our results did not depend on a specific way of estimating noise parameters. For this analysis, we assumed optimal parameters, that is <italic>p</italic><sub>Common</sub> &#x003D; 0.5 and no lapse (&#x03BB; &#x003D; 0).</p>
<p>We found a significant difference between observed accuracy and estimated optimal accuracy based on collinearity judgment noise, as shown in <xref ref-type="fig" rid="fig6">Fig 6</xref> (two-way repeated-measures ANOVA with Greenhouse-Geisser correction; <italic>F</italic><sub>(1.00,7.00)</sub> &#x003D; 37.8, <italic>&#x03F5;</italic> &#x003D; 1.00, <italic>p</italic> &#x003C; 0.001, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="350082_inline7.gif"/></alternatives></inline-formula>). There is a significant main effect of eccentricity (<italic>F</italic><sub>(2.03,14.2)</sub> &#x003D; 128, <italic>&#x03F5;</italic> &#x003D; 0.675, <italic>p</italic> &#x003C; 0.001, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="350082_inline8.gif"/></alternatives></inline-formula>), which is expected from the experimental manipulations. We also found no significant interaction between optimality condition and eccentricity (<italic>F</italic><sub>(2.22,15.5)</sub> &#x003D; 2.31, <italic>&#x03F5;</italic> &#x003D; 0.738, <italic>p</italic> &#x003D; 0.106, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="350082_inline9.gif"/></alternatives></inline-formula>). Analogously, for height judgement noise parameters, there is also a significant difference between observed accuracy and estimated optimal accuracy (<italic>F</italic><sub>(1.00,7.00)</sub> &#x003D; 7.45, <italic>&#x03F5;</italic> &#x003D; 1.00, <italic>p</italic> &#x003D; 0.029, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="350082_inline10.gif"/></alternatives></inline-formula>), a significant main effect of eccentricity (<italic>F</italic><sub>(1.85,13.0)</sub> &#x003D; 54.4, <italic>&#x03F5;</italic> &#x003D; 0.618, <italic>p</italic> &#x003C; 0.001, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="350082_inline11.gif"/></alternatives></inline-formula>), and no significant interaction between optimality condition and eccentricity (<italic>F</italic><sub>(2.16,15.1)</sub> &#x003D; 3.46, <italic>&#x03F5;</italic> &#x003D; 0.720, <italic>p</italic> &#x003D; 0.055, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="350082_inline12.gif"/></alternatives></inline-formula>). These results confirm the results of the model comparison in that there is a statistically significant difference between our subjects&#x2019; performance and optimal behavior.</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Fig 6.</label>
<caption><title>Suboptimality analysis.</title>
<p>Black line: Observed accuracy across four eccentricity levels (chance probability &#x003D; 0.5). Error bars indicate Mean &#x00B1; 1 SEM across subjects. Green line: Estimated accuracy if subjects perform Bayes-optimally, with noise parameters obtained via the collinearity judgement task. Blue line: Estimated accuracy with noise parameters obtained via the height judgment task. Performance was slightly suboptimal across participants.</p></caption>
<graphic xlink:href="350082_fig6.tif"/>
</fig>
<p>However, a <italic>statistically</italic> significant difference does not necessarily imply a substantial difference in terms of performance, as previous studies have shown that participants can be &#x201C;optimally lazy&#x201D; by deviating from optimal performance in a way that has minimal impact on overall expected score in a task [<xref rid="c27" ref-type="bibr">27</xref>]. We quantified our subjects&#x2019; performance in terms of <italic>efficiency</italic>, that is the proportion of correct responses with respect to optimal behavior. Our subjects exhibited an overall efficiency of 0.953 &#x00B1; 0.007 (based on collinearity judgment noise), or 0.959 &#x00B1; 0.015 (based on height judgement noise), which suggests that our subjects were only slightly suboptimal (see Discussion).</p>
</sec>
<sec id="s7">
<title>Model variants</title>
<p>We consider here several alternative observer models that relax some key assumptions we made when constructing our main observers, to verify whether our findings still hold.</p>
<sec id="s7a">
<title>Mismatch of noise parameters</title>
<p>So far, we have assumed that observers utilize directly their noise parameters <italic>&#x03C3;</italic><sub><italic>x</italic></sub>(<italic>y</italic>) when computing the decision rule. Here we propose a variant of the Bayes-optimal model, &#x201C;Mismatch&#x201D;, in which the observer instead uses a set of <italic>assumed</italic> noise parameters that may deviate from the true standard deviations of their measurement distributions [<xref rid="c28" ref-type="bibr">28</xref>]. This model is identical to the Bayes-optimal model except that all four <italic>&#x03C3;</italic><sub><italic>x</italic></sub>(<italic>y</italic>) are substituted with <italic>&#x03C3;</italic><sub><italic>x</italic>,assumed</sub>(<italic>y</italic>), the assumed noise parameters, in the calculation of the decision variables. To limit model complexity, we chose for the assumed noise parameters a parametric form which is a linear function of the true noise parameters <italic>&#x03C3;</italic><sub><italic>x</italic></sub>(<italic>y</italic>). To avoid issues of lack of parameter identifiability [<xref rid="c29" ref-type="bibr">29</xref>], for the Mismatch model we also fixed <italic>p</italic><sub>common</sub> &#x003D; 0.5. Thus, the Mismatch model has the same number of free parameters as Lin, and one more than Bayes.</p>
<p>After relaxing the Bayes-optimal model to allow for assumed noise parameters, we found that the Mismatch model fits better than the original Bayes model (LOO<sub>Mismatch</sub> &#x2212; LOO<sub>Bayes</sub> &#x003D; 29.8 &#x00B1; 13.0), and, thus, better than the Fixed model as well, which was already the worst in the comparison (LOO<sub>Mismatch</sub> &#x2212; LOO<sub>Fixed</sub> &#x003D; 55.5 &#x00B1; 14.0). However, we found that the Lin model is still the best-fitting model (LOO<sub>Mismatch</sub> &#x2212; LOO<sub>Lin</sub> &#x003D; &#x2212;13.9 &#x00B1; 5.4). All combined, these results suggest that a degree of suboptimality in the observers might have arisen from a lack of knowledge of their own noise characteristics [<xref rid="c28" ref-type="bibr">28</xref>], but such mismatch is not enough to entirely explain the observed pattern of behavior.</p>
</sec>
<sec id="s7b">
<title>Trial dependency</title>
<p>We also tested for potential influence of stimulus uncertainty from previous trials (&#x201C;History&#x201D; model) on the response of the current trial. Specifically, for the History model we extended the formula of the decision boundary of the Lin model to be a linear function of the noise parameters of the current trial, as before, plus the noise associated with up to four previous trials, that is <italic>&#x03C3;</italic><sub><italic>x</italic></sub>(<italic>y</italic><sub><italic>t</italic></sub>), <italic>&#x03C3;</italic><sub><italic>x</italic></sub>(<italic>y</italic><sub><italic>t</italic>&#x2212;1</sub>)), <italic>&#x03C3;</italic><sub><italic>x</italic></sub>(<italic>y</italic><sub><italic>t</italic>&#x2212;2</sub>), <italic>&#x03C3;</italic><sub><italic>x</italic></sub>(<italic>y</italic><sub><italic>t</italic>&#x2212;3</sub>), <italic>&#x03C3;</italic><sub><italic>x</italic></sub>(<italic>y</italic><sub><italic>t</italic>&#x2212;4</sub>), respectively, each one with a separate weight.</p>
<p>We found no evidence of trial dependency, for the History model fits about as well or even slightly worse than Lin (LOO<sub>History</sub> &#x2212; LOO<sub>Lin</sub> &#x003D; &#x2212;2.4 &#x00B1; 0.24). In particular, we also found that the maximum-a-posteriori weights associated with <italic>&#x03C3;</italic><sub><italic>x</italic></sub>(<italic>y</italic><sub><italic>t</italic>&#x2212;1</sub>) to <italic>&#x03C3;</italic><sub><italic>x</italic></sub>(<italic>y</italic><sub><italic>t</italic>&#x2212;4</sub>) were all not significantly different from zero across participants (respectively, <italic>t</italic><sub>(7)</sub> &#x003D; 1.45, <italic>p</italic> &#x003D; 0.19; <italic>t</italic><sub>(7)</sub> &#x003D; 0.0754, <italic>p</italic> &#x003D; 0.94; <italic>t</italic><sub>(7)</sub> &#x003D; &#x2212;1.18, <italic>p</italic> &#x003D; 0.28; <italic>t</italic><sub>(7)</sub> &#x003D; &#x2212;1.27, <italic>p</italic> &#x003D; 0.24). These results show that sensory uncertainty from previous trials had no effect on the observers&#x2019; decision in the current trial.</p>
</sec>
<sec id="s7c">
<title>Nonparametric examination</title>
<p>In the Lin model (and variants thereof), so far we assumed a linear parametric relationship between the decision boundary and the noise level <italic>&#x03C3;</italic><sub><italic>x</italic></sub>(<italic>y</italic>), as per <xref ref-type="disp-formula" rid="eqn4">Eq 1</xref>.</p>
<p>Here we loosened this constraint and fitted the decision boundary for each eccentricity level as an individual parameter. Due to its flexible nature, we consider this &#x201C;Nonparametric&#x201D; model merely as a descriptive model, which we expect to explain the data very well. We use the Nonparametric model as a means to provide an upper-bound on the LOO score for each individual, so as to have an absolute metric to evaluate the performances of other models (in a spirit similarly to estimating the entropy of the data, that is an estimate of the intrinsic variability of the data which represents an upper bound on the performance of any model [<xref rid="c30" ref-type="bibr">30</xref>]). As expected, given the large amount of flexibility, the Nonparametric model fits better than Lin (LOO<sub>Nonparametric</sub> &#x2212; LOO<sub>Lin</sub> &#x003D; 14.6 &#x00B1; 6.5), but we note that the difference in LOO is substantially less than the difference between Lin and Bayes (43.7 &#x00B1; 13.3), or Lin and Fixed (69.3 &#x00B1; 16.5), suggesting that Lin is capturing subjects&#x2019; behavior quite well, close to a full nonparametric description of the data.</p>
<p>We can also use the Nonparametric model to examine how close the parametric estimates of decision boundary from Lin, our best model so far, are to those obtained nonparametrically. We observed that the average decision boundary across 8 subjects, as a function of eccentricity, was consistent with the average nonparametric estimates of the decision boundary at every eccentricity level (<xref ref-type="fig" rid="fig7">Fig 7A,B</xref>). This agreement means that the decision boundaries adopted by observers in the task were, indeed, approximately linear in the eccentricity levels, as assumed by the linear heuristic model.</p>
<fig id="fig7" position="float" fig-type="figure">
<label>Fig 7.</label>
<caption><title>Nonparametric model.</title>
<p>A: Decision boundary estimates of linear heuristic model (Lin) vs. decision boundary estimates of the Nonparametric model at different eccentricity levels (Mean &#x00B1; 1 SEM). B: Decision boundary at every eccentricity level fitted non-parametrically vs. Decision boundary at every eccentricity level fitted from the Lin model. Even when allowed to vary freely (&#x201C;non-parametrically&#x201D;), the decision boundaries are approximately linear in eccentricity, as per the Lin model.</p></caption>
<graphic xlink:href="350082_fig7.tif"/>
</fig>
</sec>
</sec>
<sec id="s8">
<title>Discussion</title>
<p>To study how people group together elements of a visual scene, we designed a behavioral experiment in which participants were asked to judge whether two line segments partially occluded belonged to the same line. Using computational observer models to describe the obtained data, we found that people utilize sensory uncertainty when making collinearity judgements, however in a slightly suboptimal way. Crucially, our results are robust to changes in model assumptions, such as noise model mismatch, history effects, and different decision boundaries, and we independently validated our parameter estimates in a different task. With trial-by-trial manipulation of eccentricity in a collinearity judgment task, our study presents the first rigorous examination of the role of sensory uncertainty for probabilistic computations in perceptual organization.</p>
<p>The present study is linked to the broader effort to study hierarchical Bayesian inference in perception, whereby the observer is required to marginalize over stimulus values (here, line offset) to build a posterior over latent, discrete causal scenarios (here, same line of different lines). Such framework was adopted and tested in a variety of domains such as cue combination [<xref rid="c31" ref-type="bibr">31</xref>], change detection [<xref rid="c32" ref-type="bibr">32</xref>], perception of sameness [<xref rid="c33" ref-type="bibr">33</xref>], and causal inference [<xref rid="c34" ref-type="bibr">34</xref>]. In particular, our models share the same formal structure of models of causal inference in multisensory perception [<xref rid="c34" ref-type="bibr">34</xref>,<xref rid="c35" ref-type="bibr">35</xref>]. In such tasks, the observer receives sensory measurements of possibly discrepant cues from distinct sensory modalities (e.g., vision and hearing), and has to infer whether the cues originated from the same source (<italic>C</italic> &#x003D; 1) or from different sources (<italic>C</italic> &#x003D; 0) &#x2013; leading to, respectively, cue integration and cue segregation. Previous work has shown that Bayesian causal inference models provide a good qualitative description of human performance in multisensory perception with discrepant cues, but quantitative comparison hints at deviations from exact Bayesian behavior [<xref rid="c30" ref-type="bibr">30</xref>], not unlike what we find here. Our study differs from previous work in that here we focus on an atomic form of perceptual organization.</p>
<p>While in our study we closely examined the effect of varying sensory uncertainty, our task did not strictly introduce ambiguity, a integral element of Gestalt perception. Ambiguity translates to overlapping stimulus distributions, and ambiguous trials are only found in the collinear category of our task. With the presence of ambiguity, an observer will not be able to achieve perfect performance even when sensory noise is completely absent. Shapes defined by illusory contours such as variants of the Kanizsa triangle were previously used to study representations of illusory contours in the cortical areas of the brain in functional imaging [<xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c37">37</xref>], rendering them potential candidates for stimuli that can incorporate both ambiguity and sensory uncertainty.</p>
<p>Nevertheless, by studying the role of sensory uncertainty alone, our study presents a more careful account of Bayesian inference in perceptual organization. In particular, we compared the Bayesian observer model against other observer models that each describe an alternative plausible decision strategy. We were able to distinguish a fixed stimulus mapping that mimics Bayesian inference from probabilistic computation, which requires the observer to flexibly adjust their decision boundary according to sensory uncertainty. Despite evidence for probabilistic computations, we found that data was better explained by a non-Bayesian heuristic model.</p>
<p>A possible explanation for subjects&#x2019; heuristic strategy, which differed slightly but systematically from optimal performance, might be that they had received insufficient training. While we found no evidence of learning across sessions, it is possible that participants would have learnt to perform optimally had they received correctness feedback on the task, possibly with greater incentives to motivate their learning. The main purpose of our experiment was to explore the role of sensory uncertainty - thus, we limited the amount of training trials with performance feedback on purpose, to prevent the possible learning of a fixed mapping of stimulus to collinearity condition that is independent of sensory uncertainty. The tradeoff between providing sufficient training trials and avoiding learning of fixed mapping makes it difficult to test behaviorally the hypothesis that sub-optimality stems from insufficient training. A possible alternative avenue for exploring the effect of task learning could be through training an artificial neural network on the same psychophysical task, and examining how performance evolves as a function of training epochs, and whether this mimics human behavior. For example, a hierarchical, probabilistic and stochastic neural network such as Deep Boltzmann Machine is a desirable candidate as it can learn to generate sensory data in an unsupervised fashion, a procedure that provides a plausible account for visual cortical processing [<xref rid="c38" ref-type="bibr">38</xref>,<xref rid="c39" ref-type="bibr">39</xref>]. Notably, such stochastic hierarchical generative model was used to show that visual numerosity &#x2013; a higher-order feature - can be invariantly encoded in the deepest hidden layer of the neural network [<xref rid="c40" ref-type="bibr">40</xref>], and could analogously give rise to illusory contours neurons as found in monkeys [<xref rid="c38" ref-type="bibr">38</xref>].</p>
<p>Our finding that elementary perceptual organization is probabilistic &#x2013; albeit slightly suboptimal - leads naturally to a fundamental open question in neuroscience, that is whether and how the visual system performs (or approximates) probabilistic inference in the presence of complex, naturalistic stimuli. There is a trade-off between stimulus complexity and modeling tractability in that we experimenters do not normally have access to the generative model of a complex visual scene, preventing the deployment of powerful statistical tools from ideal-observer analysis such as those used in the current work. However, for example, a recent theoretical paper introduced a flexible, parametric model of overlapping and occluded geometric shapes that resemble the pattern of a bed of leaves (&#x201C;dead leaves&#x201D; [<xref rid="c41" ref-type="bibr">41</xref>]). Our rigorous model comparison approach, combined with such complex psychophysical stimuli, provides a viable direction for future studies interested in further exploring the probabilistic nature of perceptual organization.</p>
</sec>
<sec id="s9">
<title>Materials and Methods</title>
<sec id="s9a">
<title>Subjects</title>
<p>8 subjects (6 female), aged 20-30, participated in the experiment. Subjects received &#x0024;10 for each of four 1-hour sessions, plus a completion bonus of &#x0024;10. The Institutional Review Board at New York University approved the experimental procedures (protocol &#x0023;IRB-FY2016-599: &#x201C;Visual perception, attention, and memory&#x201D;) and all subjects gave written informed consent.</p>
</sec>
<sec id="s9b">
<title>Apparatus and stimuli</title>
<p>The stimuli were shown on a 60 Hz 9.7-inch 2048-by-1536 pixel display. The display (LG LP097QX1-SPA2) was the same as that used in the 2013 iPad Air (Apple). The screen was secured to an arm with height adjusted to each subject&#x2019;s eye level. A chin rest was horizontally aligned with the center of the screen. The distance between the eyes and the display was 27.5 cm. To minimize potential biases caused by external visual cues, we added a large black panel surrounding the display. The display was connected to a Windows desktop PC using the Psychophysics Toolbox extensions [<xref rid="c42" ref-type="bibr">42</xref>,<xref rid="c43" ref-type="bibr">43</xref>] for MATLAB (MathWorks, Natick, MA).</p>
<p>On each trial, a dark gray occluder (23 cd/m<sup>2</sup>) with a width of 5.6 degrees of visual angle (dva) was displayed against a light gray background (50 cd/m<sup>2</sup>). A white (159 cd/m<sup>2</sup>) fixation dot 0.24 dva in diameter was shown in the lower central part of the occluder; this dot corresponded to a retinal eccentricity of 0 dva. The stimuli consisted of two horizontal white line segments on both sides of the occluder. The line segments were all 5.6 dva in width and 0.16 dva in height. The vertical &#x201C;base position&#x201D; <italic>y</italic> of a pair of line segments had one of four levels of retinal eccentricity (0, 4.8, 9.6, and 16.8 dva).</p>
</sec>
<sec id="s9c">
<title>Trial procedure</title>
<p>Subjects completed two tasks, which we call <italic>collinearity judgment</italic> task and <italic>height judgment</italic> task. On each trial in the collinearity judgment task (<xref ref-type="fig" rid="fig1">Fig 1A</xref>), the occluder and fixation dot were displayed for 850 ms, followed by the stimulus for 100 ms. On a &#x201C;non-collinear&#x201D; trial, the vertical positions of the two line segments were independently drawn from a normal distribution centered at one of the four &#x201C;base&#x201D; eccentricity levels (0, 4.8, 9.6, or 16.8 dva), with a standard deviation of 0.48 dva (<xref ref-type="fig" rid="fig1">Fig 1E</xref>); on a &#x201C;collinear&#x201D; trial, we drew the vertical position of the line segment on one side and matched the line segment on the other side. In each session, 50&#x0025; of the trials were &#x201C;collinear&#x201D; and 50&#x0025; were &#x201C;non-collinear&#x201D;, randomly interleaved. At stimulus offset, the fixation dot turned green to prompt the subject to indicate whether the two line segments were collinear. The participant pressed one of 8 keys, corresponding to 8 choice-confidence combinations, ranging from high-confident collinear to high-confident non-collinear. Response time was not constrained. No performance feedback was given at the end of the trial.</p>
<p>Height judgment task trials followed the same procedure (<xref ref-type="fig" rid="fig1">Fig 1B</xref>), except that the subject was asked to report which of the two line segments was highest (&#x201C;left&#x201D; or &#x201C;right&#x201D;). We generated the line segments in the same fashion as in the &#x201C;non-collinear&#x201D; condition of the collinearity judgment task. Audio feedback was given after each response to indicate whether the choice was correct.</p>
<p>For the analyses described in this paper, we only considered choice data (&#x201C;collinear/non-collinear&#x201D;, &#x201C;left/right&#x201D;), leaving analysis of confidence reports to future work.</p>
</sec>
<sec id="s8d">
<title>Experiment procedure</title>
<p>During each session, subjects completed one height judgment task block, followed by three collinearity judgment task blocks, and finished with another height judgment task block. Each height judgment task block consisted of 60 trials, and each collinearity judgment task block consisted of 200 trials.</p>
<p>A demonstration of the experimental procedure was given to each subject at the beginning of the first session. Participants were informed that there were an equal number of left/right trials in the height judgment task as well as an equal number of collinear/non-collinear trials in the collinearity judgment task. To familiarize subjects with the stimulus distribution and to check for understanding of the tasks, participants completed 16 practice trials at the beginning of each session. Stimulus presentation time was longer on practice trials (500 ms), and audio correctness feedback was given at the end of each practice trial. We did not analyze the responses on the practice trials.</p>
</sec>
<sec id="s8e">
<title>Data analysis</title>
<p>In order to visualize psychometric curves with enough trials, we binned the offset values between the left and right line segments into the following intervals: (&#x2212;&#x221E;, &#x2212;3.31], (&#x2212;3.31,&#x2212;2.08], (&#x2212;2.08,&#x2212;1.17], (&#x2212; 1.17,&#x2212;0.38], (&#x2212;0.38,0.38], (0.38,1.17], (1.17,2.08], (2.08,3.31], (3.31,&#x221E;), in dva. These values were chosen to include a comparable number of trials per interval, based on the quantiles of the Gaussian distribution of the offset used in the experiment. For the collinearity judgment task, we computed the proportion of trials in which subjects reported &#x201C;collinear&#x201D; at each offset bin and retinal eccentricity level. For the height judgment task, we computed the proportion of trials in which subjects reported &#x201C;right higher&#x201D; at each offset bin and retinal eccentricity level.</p>
<p>Repeated-measures ANOVA with offset bin and eccentricity level as within-subjects factors were performed separately on the proportion of reporting &#x201C;collinear&#x201D; in the collinearity judgment task and the proportion of reporting &#x201C;right higher&#x201D; in the height judgment task. We applied Greenhouse-Geisser correction of the degrees of freedom in order to account for deviations from sphericity [<xref rid="c44" ref-type="bibr">44</xref>], and report effect sizes as partial eta squared, denoted with <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="350082_inline13.gif"/></alternatives></inline-formula>.</p>
<p>For all analyses the criterion for statistical significance was <italic>p</italic> &#x003C; .05, and we report uncorrected <italic>p</italic>-values. Unless specified otherwise, summary statistics are reported in the text as mean &#x00B1; SEM between subjects. Note that we used the summary statistics described in this section only for visualization and to perform simple descriptive statistics; all models were fitted to raw trial data as described next.</p>
</sec>
<sec id="s8f">
<title>Model fitting</title>
<p>For each model and subject, the noise parameters <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="350082_inline14.gif"/></alternatives></inline-formula> for <italic>y</italic> &#x003D; 0,4.8, 9.6 and 16.8 dva were fitted as individual parameters.</p>
<p>We calculated the log likelihood of each individual dataset for a given model with parameter vector <bold><italic>&#x03B8;</italic></bold> by summing the log probability of trial <italic>i</italic> over all N trials,
<disp-formula id="eqn2"><alternatives><graphic xlink:href="350082_eqn2.gif"/></alternatives>
</disp-formula>
where the response probability <italic>p</italic><sub><italic><bold>&#x03B8;</bold></italic>,model</sub><inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="350082_inline15.gif"/></alternatives></inline-formula> is defined in S1 Appendix.</p>
<p>We fitted the models by drawing samples from the unnormalized log posterior distribution of the parameters <italic>p</italic>(<bold><italic>&#x03B8;</italic></bold>&#x007C;data) using Markov Chain Monte Carlo (parallel slice sampling [<xref rid="c30" ref-type="bibr">30</xref>,<xref rid="c45" ref-type="bibr">45</xref>]) for each subject. The posterior distribution of the parameters is proportional to the sum of data likelihood (<xref ref-type="disp-formula" rid="eqn3">Eq 3</xref>) and a factorized prior over the each parameter <italic>j</italic>,
<disp-formula id="eqn3"><alternatives><graphic xlink:href="350082_eqn3.gif"/></alternatives>
</disp-formula></p>
<p>We used log-transformed coordinates for scale parameters (e.g., noise), and for all
parameters we assumed a uniform non-informative prior (uniform in log space for scale parameters) [<xref rid="c46" ref-type="bibr">46</xref>], within reasonably large bounds. Three parallel chains were ran with starting point set at maximum likelihood point estimates of the parameters, evaluated with Bayesian Adaptive Direct Search [<xref rid="c47" ref-type="bibr">47</xref>], to ensure that the chains were initialized within a high posterior density region.</p>
<p>After running all chains, we computed Gelman and Rubin&#x2019;s potential scale reduction statistic <italic>R</italic> for all parameters to check for convergence [<xref rid="c48" ref-type="bibr">48</xref>]. An <italic>R</italic> value that diverges from 1 indicates convergence problems, whereas a value close to 1 suggests convergence of the chains. The average difference between <italic>R</italic> value and 1 across all parameters, subjects and models is 1.16 &#x00D7; 10<sup>&#x2212;4</sup>, and all <italic>R</italic> values fall within (0.99,1.003], suggesting good convergence. To verify compatibility between different runs, we also visually inspected the posteriors from different chains. We merged samples from all chains for each model and subject in further analyses.</p>
<p>To visualize model fits (or posterior predictions) in <xref ref-type="fig" rid="fig4">Fig 4</xref> and 5, we computed the posterior mean model prediction for each subject based on 60 independent samples from the posterior (equally spaced in the sampled chains). We then plotted average and standard deviation across subjects.</p>
</sec>
<sec id="s8g">
<title>Model comparison</title>
<p>To estimate the predictive accuracy of the models while taking into account model complexity, we compared models using the leave-one-out cross-validation score (LOO). Leave-one-out cross-validation is a model evaluation technique in which all but one trial of a dataset is used as training set to make prediction on the left-out trial using the fitted model. This process is repeated until all trials have been iterated through. For the purpose of computational efficiency, we estimate the leave-one-out cross validation score via Pareto smoothed importance sampling [<xref rid="c22" ref-type="bibr">22</xref>], which uses samples from the posterior distribution of the parameters <bold><italic>&#x03B8;</italic></bold>,
<disp-formula id="eqn4"><alternatives><graphic xlink:href="350082_eqn4.gif"/></alternatives>
</disp-formula>
where <bold><italic>&#x03B8;</italic></bold><sup><italic>k</italic></sup> is the k-th posterior sample for the corresponding model and <italic>w</italic><sub><italic>i,k</italic></sub> is the importance weight of trial <italic>i</italic> for sample <italic>k</italic>.</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We thank Andra Mihali, Will Adler, Maija Honig and Zahy Bnaya for useful discussions of earlier versions of this manuscript. This work has utilized the NYU IT High Performance Computing resources and services.</p>
</ack>
<sec>
<p><bold>S1 Appendix.</bold> Supplemental methods. Analysis of learning; model specification; model recovery analysis.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="book"><string-name><surname>Wertheimer</surname> <given-names>M</given-names></string-name>. <collab>Gestalt theory</collab>. In <person-group person-group-type="editor"><string-name><given-names>W. D.</given-names> <surname>Ellis</surname></string-name></person-group> (Ed.). In: <chapter-title>A source book of Gestalt psychology</chapter-title>. <publisher-name>Kegan Paul Trench, Trubner &#x0026; Company</publisher-name>; <year>1938</year>. p. <fpage>1</fpage>&#x2013;<lpage>11</lpage>. Available from: <monospace><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1037&#x0025;2F11496-001">https://doi.org/10.1037&#x0025;2F11496-001</ext-link></monospace>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="book"><string-name><surname>Knill</surname> <given-names>DC</given-names></string-name>, <string-name><surname>Richards</surname> <given-names>W</given-names></string-name>. <chapter-title>Perception as Bayesian Inference</chapter-title>. <person-group person-group-type="editor"><string-name><surname>Knill</surname> <given-names>DC</given-names></string-name>, <string-name><surname>Richards</surname> <given-names>W</given-names></string-name></person-group>, editors. <publisher-name>Cambridge University Press</publisher-name>; <year>1996</year>. Available from: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.10177.2Fcbo9780511984037">https://doi.org/10.10177.2Fcbo9780511984037</ext-link>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Feldman</surname> <given-names>J</given-names></string-name>. <article-title>Bayesian contour integration</article-title>. <source>Perception &#x0026; Psychophysics</source>. <year>2001</year>;<volume>63</volume>(<issue>7</issue>):<fpage>1171</fpage>&#x2013;<lpage>1182</lpage>. doi:<pub-id pub-id-type="doi">10.3758/bf03194532</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Elder</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Goldberg</surname> <given-names>RM</given-names></string-name>. <article-title>Ecological statistics of Gestalt laws for the perceptual organization of contours</article-title>. <source>Journal of Vision</source>. <year>2002</year>;<volume>2</volume>(<issue>4</issue>):<fpage>5</fpage>. doi:<pub-id pub-id-type="doi">10.1167/2.4.5</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><surname>Geisler</surname> <given-names>WS</given-names></string-name>, <string-name><surname>Perry</surname> <given-names>JS</given-names></string-name>. <article-title>Contour statistics in natural images: Grouping across occlusions</article-title>. <source>Visual Neuroscience</source>. <year>2009</year>;<volume>26</volume>(<issue>01</issue>):<fpage>109</fpage>. doi:<pub-id pub-id-type="doi">10.1017/s0952523808080875</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Froyen</surname> <given-names>V</given-names></string-name>, <string-name><surname>Kogo</surname> <given-names>N</given-names></string-name>, <string-name><surname>Singh</surname> <given-names>M</given-names></string-name>, <string-name><surname>Feldman</surname> <given-names>J</given-names></string-name>. <article-title>Modal and amodal shape completion</article-title>. <source>Journal of Vision</source>. <year>2015</year>;<volume>15</volume>(<issue>12</issue>):<fpage>321</fpage>. doi:<pub-id pub-id-type="doi">10.1167/15.12.321</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Ma</surname> <given-names>WJ</given-names></string-name>. <article-title>Organizing probabilistic models of perception</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2012</year>;<volume>16</volume>(<issue>10</issue>):<fpage>511</fpage>&#x2013;<lpage>518</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.tics.2012.08.010</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Pouget</surname> <given-names>A</given-names></string-name>, <string-name><surname>Beck</surname> <given-names>JM</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>WJ</given-names></string-name>, <string-name><surname>Latham</surname> <given-names>PE</given-names></string-name>. <article-title>Probabilistic brains: knowns and unknowns</article-title>. <source>Nature Neuroscience</source>. <year>2013</year>;<volume>16</volume>(<issue>9</issue>):<fpage>1170</fpage>&#x2013;<lpage>1178</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.3495</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Maloney</surname> <given-names>LT</given-names></string-name>, <string-name><surname>Mamassian</surname> <given-names>P</given-names></string-name>. <article-title>Bayesian decision theory as a model of human visual perception: Testing Bayesian transfer</article-title>. <source>Visual Neuroscience</source>. <year>2009</year>;<volume>26</volume>(<issue>01</issue>):<fpage>147</fpage>. doi:<pub-id pub-id-type="doi">10.1017/s0952523808080905</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Ma</surname> <given-names>WJ</given-names></string-name>, <string-name><surname>Jazayeri</surname> <given-names>M</given-names></string-name>. <article-title>Neural Coding of Uncertainty and Probability</article-title>. <source>Annual Review of Neuroscience</source>. <year>2014</year>;<volume>37</volume>(<issue>1</issue>):<fpage>205</fpage>&#x2013;<lpage>220</lpage>. doi:<pub-id pub-id-type="doi">10.1146/annurev-neuro-071013-014017</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Ernst</surname> <given-names>MO</given-names></string-name>, <string-name><surname>Banks</surname> <given-names>MS</given-names></string-name>. <article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title>. <source>Nature</source>. <year>2002</year>;<volume>415</volume>(<issue>6870</issue>):<fpage>429</fpage>&#x2013;<lpage>433</lpage>. doi:<pub-id pub-id-type="doi">10.1038/415429a</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Alais</surname> <given-names>D</given-names></string-name>, <string-name><surname>Burr</surname> <given-names>D</given-names></string-name>. <article-title>The Ventriloquist Effect Results from Near-Optimal Bimodal Integration</article-title>. <source>Current Biology</source>. <year>2004</year>;<volume>14</volume>(<issue>3</issue>):<fpage>257</fpage>&#x2013;<lpage>262</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2004.01.029</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Stocker</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name>. <article-title>Noise characteristics and prior expectations in human visual speed perception</article-title>. <source>Nature Neuroscience</source>. <year>2006</year>;<volume>9</volume>(<issue>4</issue>):<fpage>578</fpage>&#x2013;<lpage>585</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn1669</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Girshick</surname> <given-names>AR</given-names></string-name>, <string-name><surname>Landy</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Simoncelli</surname> <given-names>EP</given-names></string-name>. <article-title>Cardinal rules: visual orientation perception reflects knowledge of environmental statistics</article-title>. <source>Nature Neuroscience</source>. <year>2011</year>;<volume>14</volume>(<issue>7</issue>):<fpage>926</fpage>&#x2013;<lpage>932</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.2831</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Fetsch</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Pouget</surname> <given-names>A</given-names></string-name>, <string-name><surname>DeAngelis</surname> <given-names>GC</given-names></string-name>, <string-name><surname>Angelaki</surname> <given-names>DE</given-names></string-name>. <article-title>Neural correlates of reliability-based cue weighting during multisensory integration</article-title>. <source>Nature Neuroscience</source>. <year>2011</year>;<volume>15</volume>(<issue>1</issue>):<fpage>146</fpage>&#x2013;<lpage>154</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.2983</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Rohe</surname> <given-names>T</given-names></string-name>, <string-name><surname>Noppeney</surname> <given-names>U</given-names></string-name>. <article-title>Cortical Hierarchies Perform Bayesian Causal Inference in Multisensory Perception</article-title>. <source>PLOS Biology</source>. <year>2015</year>;<volume>13</volume>(<issue>2</issue>):<fpage>e1002073</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pbio.1002073</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Jones</surname> <given-names>M</given-names></string-name>, <string-name><surname>Love</surname> <given-names>BC</given-names></string-name>. <article-title>Bayesian Fundamentalism or Enlightenment? On the explanatory status and theoretical contributions of Bayesian models of cognition</article-title>. <source>Behavioral and Brain Sciences</source>. <year>2011</year>;<volume>34</volume>(<issue>04</issue>):<fpage>169</fpage>&#x2013;<lpage>188</lpage>. doi:<pub-id pub-id-type="doi">10.1017/s0140525&#x00D7;10003134</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Bowers</surname> <given-names>JS</given-names></string-name>, <string-name><surname>Davis</surname> <given-names>CJ</given-names></string-name>. <article-title>More varieties of Bayesian theories but no enlightenment</article-title>. <source>Behavioral and Brain Sciences</source>. <year>2011</year>;<volume>34</volume>(<issue>04</issue>):<fpage>193</fpage>&#x2013;<lpage>194</lpage>. doi:<pub-id pub-id-type="doi">10.1017/s0140525&#x00D7;11000227</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Palminteri</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wyart</surname> <given-names>V</given-names></string-name>, <string-name><surname>Koechlin</surname> <given-names>E</given-names></string-name>. <article-title>The Importance of Falsification in Computational Cognitive Modeling</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2017</year>;<volume>21</volume>(<issue>6</issue>):<fpage>425</fpage>&#x2013;<lpage>433</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.tics.2017.03.011</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Simon</surname> <given-names>HA</given-names></string-name>. <article-title>Rational choice and the structure of the environment</article-title>. <source>Psychological Review</source>. <year>1956</year>;<volume>63</volume>(<issue>2</issue>):<fpage>129</fpage>&#x2013;<lpage>138</lpage>. doi:<pub-id pub-id-type="doi">10.1037/h0042769</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Gigerenzer</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gaissmaier</surname> <given-names>W</given-names></string-name>. <article-title>Heuristic Decision Making</article-title>. <source>Annual Review of Psychology</source>. <year>2011</year>;<volume>62</volume>(<issue>1</issue>):<fpage>451</fpage>&#x2013;<lpage>482</lpage>. doi:<pub-id pub-id-type="doi">10.1146/annurev-psych-120709-145346</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Vehtari</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gelman</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gabry</surname> <given-names>J</given-names></string-name>. <article-title>Erratum to: Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC</article-title>. <source>Statistics and Computing</source>. <year>2016</year>;<volume>27</volume>(<issue>5</issue>):<fpage>1433</fpage>&#x2013;<lpage>1433</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s11222-016-9709-3</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Stephan</surname> <given-names>K</given-names></string-name>, <string-name><surname>Penny</surname> <given-names>W</given-names></string-name>, <string-name><surname>Daunizeau</surname> <given-names>J</given-names></string-name>, <string-name><surname>Moran</surname> <given-names>R</given-names></string-name>, <string-name><surname>Friston</surname> <given-names>K</given-names></string-name>. <article-title>Bayesian Model Selection for Group Studies</article-title>. <source>NeuroImage</source>. <year>2009</year>;<volume>47</volume>:<fpage>S167</fpage>. doi:<pub-id pub-id-type="doi">10.1016/s1053-8119(09)71793-8</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Rigoux</surname> <given-names>L</given-names></string-name>, <string-name><surname>Stephan</surname> <given-names>KE</given-names></string-name>, <string-name><surname>Friston</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Daunizeau</surname> <given-names>J</given-names></string-name>. <article-title>Bayesian model selection for group studies &#x2014; Revisited</article-title>. <source>NeuroImage</source>. <year>2014</year>;<volume>84</volume>:<fpage>971</fpage>&#x2013;<lpage>985</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.065</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="book"><string-name><surname>Westheimer</surname> <given-names>G</given-names></string-name>. <collab>Visual Hyperacuity</collab>. In: <chapter-title>Progress in Sensory Physiology</chapter-title>. <publisher-name>Springer Berlin Heidelberg</publisher-name>; <year>1981</year>. p. <fpage>1</fpage>&#x2013;<lpage>30</lpage>. Available from: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.10077.2F978-3-642-66744-2_1">https://doi.org/10.10077.2F978-3-642-66744-2_1</ext-link>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Harris</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Fahle</surname> <given-names>M</given-names></string-name>. <article-title>The detection and discrimination of spatial offsets</article-title>. <source>Vision Research</source>. <year>1995</year>;<volume>35</volume>(<issue>1</issue>):<fpage>51</fpage>&#x2013;<lpage>58</lpage>. doi:<pub-id pub-id-type="doi">10.1016/0042-6989(94)e0082-v</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Acerbi</surname> <given-names>L</given-names></string-name>, <string-name><surname>Vijayakumar</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wolpert</surname> <given-names>DM</given-names></string-name>. <article-title>Target Uncertainty Mediates Sensorimotor Error Correction</article-title>. <source>PloS one</source>. <year>2017</year>;<volume>12</volume>(<issue>1</issue>):<fpage>e0170466</fpage>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Acerbi</surname> <given-names>L</given-names></string-name>, <string-name><surname>Vijayakumar</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wolpert</surname> <given-names>DM</given-names></string-name>. <article-title>On the Origins of Suboptimality in Human Probabilistic Inference</article-title>. <source>PLoS Computational Biology</source>. <year>2014</year>;<volume>10</volume>(<issue>6</issue>):<fpage>e1003661</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1003661</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Acerbi</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>W</given-names></string-name>, <string-name><surname>Vijayakumar</surname> <given-names>S</given-names></string-name>. <article-title>A framework for testing identifiability of Bayesian models of perception</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2014</year>;.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Acerbi</surname> <given-names>L</given-names></string-name>, <string-name><surname>Dokka</surname> <given-names>K</given-names></string-name>, <string-name><surname>Angelaki</surname> <given-names>DE</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>WJ</given-names></string-name>. <source>Bayesian Comparison of Explicit and Implicit Causal Inference Strategies in Multisensory Heading Perception</source>. <year>2017</year>;doi:<pub-id pub-id-type="doi">10.1101/150052</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="book"><string-name><surname>Landy</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Banks</surname> <given-names>MS</given-names></string-name>, <string-name><surname>Knill</surname> <given-names>DC</given-names></string-name>. <chapter-title>Sensory Cue Integration</chapter-title>. <person-group person-group-type="editor"><string-name><surname>Trommersh&#x00E4;user</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kording</surname> <given-names>K</given-names></string-name>, <string-name><surname>Landy</surname> <given-names>MS</given-names></string-name></person-group>, editors. <publisher-name>Oxford University Press</publisher-name>; <year>2011</year>. Available from: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.109372Facprof73Aoso72F9780195387247.001.0001">https://doi.org/10.109372Facprof73Aoso72F9780195387247.001.0001</ext-link>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Keshvari</surname> <given-names>S</given-names></string-name>, <string-name><surname>van den Berg</surname> <given-names>R</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>WJ</given-names></string-name>. <article-title>Probabilistic Computation in Human Perception under Variability in Encoding Precision</article-title>. <source>PLoS ONE</source>. <year>2012</year>;<volume>7</volume>(<issue>6</issue>):<fpage>e40216</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0040216</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="confproc"><string-name><surname>van den Berg</surname> <given-names>R</given-names></string-name>, <string-name><surname>Vogel</surname> <given-names>M</given-names></string-name>, <string-name><surname>Josic</surname> <given-names>K</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>WJ</given-names></string-name>. <article-title>Optimal inference of sameness</article-title>. <conf-name>Proceedings of the National Academy of Sciences</conf-name>. <year>2012</year>;<volume>109</volume>(<issue>8</issue>):<fpage>3178</fpage>&#x2013;<lpage>3183</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1108790109</pub-id>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>K&#x00F6;rding</surname> <given-names>KP</given-names></string-name>, <string-name><surname>Beierholm</surname> <given-names>U</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>WJ</given-names></string-name>, <string-name><surname>Quartz</surname> <given-names>S</given-names></string-name>, <string-name><surname>Tenenbaum</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Shams</surname> <given-names>L</given-names></string-name>. <article-title>Causal Inference in Multisensory Perception</article-title>. <source>PLoS ONE</source>. <year>2007</year>;<volume>2</volume>(<issue>9</issue>):<fpage>e943</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0000943</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Shams</surname> <given-names>L</given-names></string-name>, <string-name><surname>Beierholm</surname> <given-names>UR</given-names></string-name>. <article-title>Causal inference in perception</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2010</year>;<volume>14</volume>(<issue>9</issue>):<fpage>425</fpage>&#x2013;<lpage>432</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.tics.2010.07.001</pub-id>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="confproc"><string-name><surname>Hirsch</surname> <given-names>J</given-names></string-name>, <string-name><surname>DeLaPaz</surname> <given-names>RL</given-names></string-name>, <string-name><surname>Relkin</surname> <given-names>NR</given-names></string-name>, <string-name><surname>Victor</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kim</surname> <given-names>K</given-names></string-name>, <string-name><surname>Li</surname> <given-names>T</given-names></string-name>, <etal>et al.</etal> <article-title>Illusory contours activate specific regions in human visual cortex: evidence from functional magnetic resonance imaging</article-title>. <conf-name>Proceedings of the National Academy of Sciences</conf-name>. <year>1995</year>;<volume>92</volume>(<issue>14</issue>):<fpage>6469</fpage>&#x2013;<lpage>6473</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.92.14.6469</pub-id>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Mendola</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Dale</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Fischl</surname> <given-names>B</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>AK</given-names></string-name>, <string-name><surname>Tootell</surname> <given-names>RB</given-names></string-name>. <article-title>The representation of illusory and real contours in human cortical visual areas revealed by functional magnetic resonance imaging</article-title>. <source>Journal of Neuroscience</source>. <year>1999</year>;<volume>19</volume>(<issue>19</issue>):<fpage>8560</fpage>&#x2013;<lpage>8572</lpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Lee</surname> <given-names>TS</given-names></string-name>, <string-name><surname>Mumford</surname> <given-names>D</given-names></string-name>. <article-title>Hierarchical Bayesian inference in the visual cortex</article-title>. <source>Journal of the Optical Society of America A</source>. <year>2003</year>;<volume>20</volume>(<issue>7</issue>):<fpage>1434</fpage>. doi:<pub-id pub-id-type="doi">10.1364/josaa.20.001434</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Fiser</surname> <given-names>J</given-names></string-name>, <string-name><surname>Berkes</surname> <given-names>P</given-names></string-name>, <string-name><surname>Orb&#x00E1;n</surname> <given-names>G</given-names></string-name>, <string-name><surname>Lengyel</surname> <given-names>M</given-names></string-name>. <article-title>Statistically optimal perception and learning: from behavior to neural representations</article-title>. <source>Trends in Cognitive Sciences</source>. <year>2010</year>;<volume>14</volume>(<issue>3</issue>):<fpage>119</fpage>&#x2013;<lpage>130</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.tics.2010.01.003</pub-id>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Stoianov</surname> <given-names>I</given-names></string-name>, <string-name><surname>Zorzi</surname> <given-names>M</given-names></string-name>. <article-title>Emergence of a &#x2018;visual number sense&#x2019; in hierarchical generative models</article-title>. <source>Nature Neuroscience</source>. <year>2012</year>;<volume>15</volume>(<issue>2</issue>):<fpage>194</fpage>&#x2013;<lpage>196</lpage>. doi:<pub-id pub-id-type="doi">10.1038/nn.2996</pub-id>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Pitkow</surname> <given-names>X</given-names></string-name>. <article-title>Exact feature probabilities in images with occlusion</article-title>. <source>Journal of vision</source>. <year>2010</year>;<volume>10</volume>(<issue>14</issue>):<fpage>42</fpage>&#x2013;<lpage>42</lpage>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Brainard</surname> <given-names>DH</given-names></string-name>. <article-title>The Psychophysics Toolbox</article-title>. <source>Spatial Vision</source>. <year>1997</year>;<volume>10</volume>(<issue>4</issue>):<fpage>433</fpage>&#x2013;<lpage>436</lpage>. doi:<pub-id pub-id-type="doi">10.1163/156856897&#x00D7;00357</pub-id>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><surname>Pelli</surname> <given-names>DG</given-names></string-name>. <article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title>. <source>Spatial Vision</source>. <year>1997</year>;<volume>10</volume>(<issue>4</issue>):<fpage>437</fpage>&#x2013;<lpage>442</lpage>. doi:<pub-id pub-id-type="doi">10.1163/156856897&#x00D7;00366</pub-id>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>Greenhouse</surname> <given-names>SW</given-names></string-name>, <string-name><surname>Geisser</surname> <given-names>S</given-names></string-name>. <article-title>On methods in the analysis of profile data</article-title>. <source>Psychometrika</source>. <year>1959</year>;<volume>24</volume>(<issue>2</issue>):<fpage>95</fpage>&#x2013;<lpage>112</lpage>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>Neal</surname> <given-names>RM</given-names></string-name>. <article-title>Slice sampling</article-title>. <source>Annals of statistics</source>. <year>2003</year>; p. <fpage>705</fpage>&#x2013;<lpage>741</lpage>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="book"><string-name><surname>Jaynes</surname> <given-names>ET</given-names></string-name>. <chapter-title>Probability theory: the logic of science</chapter-title>. <publisher-name>Cambridge university press</publisher-name>; <year>2003</year>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>Acerbi</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ma</surname> <given-names>WJ</given-names></string-name>. <article-title>Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2017</year>;<volume>30</volume>:<fpage>1834</fpage>&#x2013;<lpage>1844</lpage>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="book"><string-name><surname>Gelman</surname> <given-names>A</given-names></string-name>, <string-name><surname>Carlin</surname> <given-names>JB</given-names></string-name>, <string-name><surname>Stern</surname> <given-names>HS</given-names></string-name>, <string-name><surname>Dunson</surname> <given-names>DB</given-names></string-name>, <string-name><surname>Vehtari</surname> <given-names>A</given-names></string-name>, <string-name><surname>Rubin</surname> <given-names>DB</given-names></string-name>. <chapter-title>Bayesian Data Analysis</chapter-title> <edition>Third Edition</edition>. <publisher-name>Chapman and Hall/CRC</publisher-name>; <year>2013</year>. Available from: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1201&#x0025;2Fb16018">https://doi.org/10.1201&#x0025;2Fb16018</ext-link>.</mixed-citation></ref>
</ref-list>
</back>
</article>