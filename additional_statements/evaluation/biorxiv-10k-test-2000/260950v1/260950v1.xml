<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/260950</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The interplay of synaptic plasticity and scaling enables the self-organized allocation of multiple memory representations</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Auth</surname>
<given-names>Johannes Maria</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">&#x262F;</xref>
<xref ref-type="author-notes" rid="n2">&#x00A4;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Nachstedt</surname>
<given-names>Timo</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n1">&#x262F;</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Tetzlaff</surname>
<given-names>Christian</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="author-notes" rid="n3">&#x002A;</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Third Institute of Physics, Georg-August-Universit&#x00E4;t G&#x00F6;ttingen</institution>, <country>Germany</country></aff>
<aff id="a2"><label>2</label><institution>Bernstein Center for Computational Neuroscience, Georg-August-Universit&#x00E4;t G&#x00F6;ttingen</institution>, <country>Germany</country></aff>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>&#x262F;</label><p>These authors contributed equally to this work.</p></fn>
<fn id="n2" fn-type="present-address"><label>&#x00A4;</label><p>Current Address: Department of Neurobiology, The Weizmann Institute of Science, Rehovot, Israel</p></fn>
<fn id="n3"><label>&#x002A;</label><p><email>tetzlaff@phys.uni-goettingen.de</email></p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2018</year>
</pub-date>
<elocation-id>260950</elocation-id>
<history>
<date date-type="received">
<day>06</day>
<month>2</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>06</day>
<month>2</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>06</day>
<month>2</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="260950.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>It is commonly assumed that memories about experienced stimuli are represented in the brain by groups of highly interconnected neurons called Hebbian cell assemblies. This requires allocating and storing information in the neural circuitry, which happens through synaptic weight adaptation. It remains, however, largely unknown how memory allocation and storage can be achieved and coordinated to allow for a faithful representation of multiple memories without disruptive interference between them. In this theoretical study, we show that the interplay between conventional Hebbian synaptic plasticity and homeostatic synaptic scaling organizes synaptic weight adaptations such that, on the one hand, a new stimulus forms a new memory while, on the other hand, different stimuli are assigned to distinct Hebbian cell assemblies. We show that the resulting dynamics can reproduce experimental in-vivo data focusing on how other neuronal and synaptic factors, such as neuronal excitability and network connectivity, influence memory formation. Thus, the here presented model suggests that a few fundamental synaptic mechanisms may suffice to implement memory allocation and storage in neural circuitry.</p>
<sec>
<title>Author summary</title>
<p>The survival in a changing environment requires the reliable learning, storage, and organization of relevant stimuli in the neuronal circuit. This theoretical work addresses the important issue of how a neuronal circuit coordinates the learning-related synaptic adaptations to properly assign new information to groups of neurons and to form long-lasting memory representations of multiple stimuli. We show that this requires only three synaptic properties &#x2013; homosynaptic potentiation paired with heterosynaptic depression both driven by the postsynaptic activity level. These properties naturally arise from the generic interplay between conventional Hebbian synaptic plasticity and homeostatic synaptic scaling. Therefore, this study shows that the complex processes of memory allocation and storage can be attributed to ubiquitous synaptic mechanisms in the neural circuitry.</p>
</sec>
</abstract>
<counts>
<page-count count="16"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Learning and memorizing information about the surrounding environment is a vital function of neural circuits of living beings. For this, a neural circuit has to accurately form and organize internal representations of the different pieces of information received.</p>
<p>The Hebbian hypothesis [<xref ref-type="bibr" rid="c1">1</xref>&#x2013;<xref ref-type="bibr" rid="c5">5</xref>] states that, when a neural circuit receives a new piece of information, the corresponding stimulus activates a group of neurons and, via activity-dependent synaptic plasticity [<xref ref-type="bibr" rid="c6">6</xref>&#x2013;<xref ref-type="bibr" rid="c9">9</xref>], adapts the weights of related synapses. This process leads to the formation of a strongly interconnected group of neurons or Hebbian cell assembly (CA), which serves as an internal memory representation of the stimulus [<xref ref-type="bibr" rid="c3">3</xref>&#x2013;<xref ref-type="bibr" rid="c5">5</xref>]. The recall of the memory translates into the activation of the respective CA. In order to recognize similar pieces of information, similar stimuli also have to be able to activate the corresponding CA. This is enabled, on the one hand, by strong recurrent interconnections between CA-neurons resulting in pattern completion [<xref ref-type="bibr" rid="c4">4</xref>,<xref ref-type="bibr" rid="c10">10</xref>] and, on the other hand, by an accurate adjustment of the synapses connecting stimuli with CAs (memory allocation). Up to now, experimental and theoretical studies mainly focussed on revealing either the formation of CAs [<xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c11">11</xref>-<xref ref-type="bibr" rid="c19">19</xref>] or the allocation of stimuli to neurons [<xref ref-type="bibr" rid="c20">20</xref>&#x2013;<xref ref-type="bibr" rid="c28">28</xref>]. By contrast, it is still unknown how a neural circuit coordinates in a self-organized manner the reliable allocation of stimuli to neurons with the proper formation of a memory representation.</p>
<p>In this theoretical study, we show in a network model that conventional Hebbian synaptic plasticity [<xref ref-type="bibr" rid="c3">3</xref>, <xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c30">30</xref>] together with the slower, homeostatic processes of synaptic scaling [<xref ref-type="bibr" rid="c31">31</xref>&#x2013;<xref ref-type="bibr" rid="c34">34</xref>] leads to the self-organized coordination of weight changes at feed-forward and recurrent synapses enabling the accurate formation and allocation of CAs. The model reproduces <italic>in-vivo</italic> experimental data and provides testable predictions. Furthermore, the analysis of a population model, capturing the main features of the network dynamics, enables us to determine three generic properties of synaptic adaptations required to form and allocate memory representations in a reliable manner. These properties are (i) homosynaptic potentiation, (ii) heterosynaptic depression, and (iii) the down-regulation of synaptic weight changes by the postsynaptic activity level.</p>
</sec>
<sec id="s2">
<title>Materials and methods</title>
<sec id="s2a">
<title>Numerical simulations</title>
<p>The three neuronal populations in the numerical model consist of 936 excitatory neurons (36 in input area, 900 in memory area) and a single inhibitory unit describing a population of inhibitory neurons which are connected to the excitatory neurons in an all-to-all manner. All neurons are described by a rate-coded leaky integrator model. The memory area is arranged as a quadratic neural grid of 30&#x00D7;30 units. Each neuron within the grid receives excitatory connections from four randomly chosen input neurons. In addition, it is recurrently connected to its circular neighborhood of radius four (measured in neuronal units; for visualization see SI text) and to the global inhibitory unit.</p>
</sec>
<sec id="s2b">
<title>Neuron model</title>
<p>The three neuronal populations considered, i.e. the input area, the memory area, as well as a global inhibitory unit are described by differential equations of a rate-coded leaky integrate neuron model. The membrane potential <inline-formula><alternatives><inline-graphic xlink:href="260950_inline1.gif"/></alternatives></inline-formula> of each neuron <italic>i</italic> in the memory area is described as follows:
<disp-formula id="eqn1"><alternatives><graphic xlink:href="260950_eqn1.gif"/></alternatives></disp-formula>
with the membrane time constant <italic>&#x03C4;</italic>, membrane resistance <italic>R</italic>, number of neurons in the memory area <italic>N</italic><sup>M</sup>, number of neurons in the input area <italic>N</italic><sup>I</sup>, and input rate <italic>I</italic><sub><italic>k</italic></sub>. The membrane potential is converted into a firing rate <italic>F</italic><sub>i</sub> by a sigmoidal transfer function with maximal firing rate <italic>&#x03B1;</italic>, steepness <italic>&#x03B2;</italic> and inflexion point &#x220A;:
<disp-formula id="eqn2"><alternatives><graphic xlink:href="260950_eqn2.gif"/></alternatives></disp-formula>
</p>
<p>The global inhibitory unit is also modeled as a rate-coded leaky integrator receiving inputs from all neurons of the memory area. Its membrane potential <italic>u</italic><sub>inh</sub> follows the differential <xref ref-type="disp-formula" rid="eqn3">equation 3</xref> with inhibitory membrane time scale <italic>&#x03C4;</italic><sub>inh</sub> and resistance <italic>R</italic><sub>inh</sub>. The potential is converted into a firing rate <italic>F</italic><sub>inh</sub> by a sigmoidal transfer function (<xref ref-type="disp-formula" rid="eqn4">equation 4</xref>).
<disp-formula id="eqn3"><alternatives><graphic xlink:href="260950_eqn3.gif"/></alternatives></disp-formula>
<disp-formula id="eqn4"><alternatives><graphic xlink:href="260950_eqn4.gif"/></alternatives></disp-formula>
</p>
<p>As the neurons in the input area form the stimuli, their output activation is set manually. Thus no further description is needed.</p>
</sec>
<sec id="s2c">
<title>Synaptic plasticity</title>
<p>The synaptic weight changes in the excitatory feed-forward (<xref ref-type="disp-formula" rid="eqn5">equation 5</xref>) and recurrent connections (<xref ref-type="disp-formula" rid="eqn6">equation 6</xref>) are determined by the combined learning rule of conventional Hebbian synaptic plasticity (first term) and synaptic scaling (second term) with time constants &#x03BC;, <italic>k</italic><sup>ff</sup>, <italic>k</italic><sup>rec</sup>, and target firing rate <italic>F<sup>T</sup></italic>. The differential equation for the synaptic weight of a feed-forward connection <inline-formula><alternatives><inline-graphic xlink:href="260950_inline2.gif"/></alternatives></inline-formula> from input neuron <italic>k</italic> &#x220A; {0, &#x2026;, <italic>N</italic><sup>l</sup>} to memory neuron <italic>i</italic> &#x220A; {0, &#x2026;, <italic>N</italic><sup><italic>M</italic></sup>} is:
<disp-formula id="eqn5"><alternatives><graphic xlink:href="260950_eqn5.gif"/></alternatives></disp-formula>
</p>
<p>The dynamics of the synaptic weight of a recurrent connection <inline-formula><alternatives><inline-graphic xlink:href="260950_inline3.gif"/></alternatives></inline-formula> from memory neuron <italic>j</italic> &#x220A; {0, &#x2026;, <italic>N</italic><sup><italic>M</italic></sup>} to memory neuron <italic>i</italic> &#x220A; {0, &#x2026;, <italic>N</italic><sup><italic>M</italic></sup>} is determined by:
<disp-formula id="eqn6"><alternatives><graphic xlink:href="260950_eqn6.gif"/></alternatives></disp-formula>
</p>
<p><inline-formula><alternatives><inline-graphic xlink:href="260950_inline4.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="260950_inline5.gif"/></alternatives></inline-formula> are the entries in the feed-forward or recurrent connectivity matrices, respectively. Being either of value 1 (existing connection) or 0 (non-existing connection), they maintain the system's connectivity structure, i.e. prevent the generation of connections.</p>
<p>All other connections are considered to be non-plastic.</p>
</sec>
<sec id="s2d">
<title>Coding framework</title>
<p>The simulation codes were written in Python 3.5. The model system was updated by using the Euler method at time steps of 5 ms.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Model Parameters: Variable, Descriptions and used Values</title>
</caption>
<graphic xlink:href="260950_tbl1.tif"/>
</table-wrap>
</sec>
<sec id="s2e">
<title>Simulation of self-org. formation of two CAs</title>
<p>The system undergoes two learning phases being presented two completely dissimilar stimulus patterns <italic>A</italic> (first phase) and <italic>B</italic> (second phase). For stimulus pattern <italic>A</italic> half of the input neurons are set to be active at 130 <italic>Hz</italic> whereas the other half remains inactive at 0 <italic>Hz</italic>. Vice versa for stimulus pattern <italic>B</italic>. During the respective learning phase the respective pattern is presented 10 times for 5 sec with a 1 sec pause in between. Both learning phases are embraced by test phases in which plasticity is shut off and both patterns are presented for 0.5 sec each to apply measures on the existing memory structures.</p>
</sec>
<sec id="s2f">
<title>Comparison to experimental data</title>
<p>The manipulation of the neuronal excitability has been done by adapting the value for e in the transfer function of the neuron model, i.e. shifting its inflexion point to lower (increased excitability) or higher (decreased excitability) values. Similar to the methods used in experiments [<xref ref-type="bibr" rid="c36">36</xref>], we manipulated a subpopulation of neurons within a randomly chosen circular area in the memory area (about 10&#x0025; of the network). The relative recruitment factor is the relation of recruitment probabilities for manipulated and control neurons averaged over 100 repetitions.</p>
</sec>
</sec>
<sec id="s3">
<title>Population model</title>
<sec id="s3a">
<title>We consider two non-overlapping populations of <italic>N</italic> excitatory neurons and one inhibitory unit</title>
<p>The state of every population <italic>i</italic> &#x220A; {1, 2} is determined by its mean membrane potential <inline-formula><alternatives><inline-graphic xlink:href="260950_inline6.gif"/></alternatives></inline-formula>, its mean recurrent synaptic weight <inline-formula><alternatives><inline-graphic xlink:href="260950_inline7.gif"/></alternatives></inline-formula> between neurons of the population, and the mean weight <inline-formula><alternatives><inline-graphic xlink:href="260950_inline8.gif"/></alternatives></inline-formula> of feed-forward synapses projecting signals from the currently active input onto the population. We assume that the two populations interact solely through the inhibitory unit whose state is given by its membrane potential <italic>u</italic><sub>inh</sub>. Thus, the dynamics of the model is described by a set of seven differential equations (see following section). To obtain its equilibria, we analytically derive the nullclines <inline-formula><alternatives><inline-graphic xlink:href="260950_inline9.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="260950_inline10.gif"/></alternatives></inline-formula> and numerically determine their intersections. The stability of an equilibrium is obtained from the sign of the eigenvalues of the system's Jacobi Matrix.</p>
<p>For analyzing in which regimes population 1 and population 2 are assigned to an input stimulus as a function of the initial synaptic weights (<xref ref-type="fig" rid="fig2">Fig 2 E</xref> and <xref ref-type="fig" rid="fig3">Fig 3</xref>), we initialize the system with the given combination of feed-forward and recurrent average synaptic weights and <inline-formula><alternatives><inline-graphic xlink:href="260950_inline11.gif"/></alternatives></inline-formula>, simulate it for 100 <italic>sec</italic>, and assess which of the two populations is active. For further details and parameter values see <italic>SI</italic> Text.</p>
</sec>
<sec id="s3b">
<title>Model Definition</title>
<p>The two excitatory populations in the population model are described by their mean membrane potentials <inline-formula><alternatives><inline-graphic xlink:href="260950_inline12.gif"/></alternatives></inline-formula>, <italic>i</italic> &#x220A; {1, 2}, <italic>k</italic> &#x220A; {A, B}:
<disp-formula id="eqn7"><alternatives><graphic xlink:href="260950_eqn7.gif"/></alternatives></disp-formula>
</p>
<p>Here, the time scale <italic>T</italic>, the resistance <italic>R</italic> and the synaptic weight <italic>w</italic><sub>i,inh</sub> have the same value as in the network simulations. The average number <inline-formula><alternatives><inline-graphic xlink:href="260950_inline13.gif"/></alternatives></inline-formula> of incoming recurrent connections from neurons within the population as well as the number of feed-forward synapses transmitting signals from active inputs to every neuron (<inline-formula><alternatives><inline-graphic xlink:href="260950_inline14.gif"/></alternatives></inline-formula>) are taken from simulations (<inline-formula><alternatives><inline-graphic xlink:href="260950_inline15.gif"/></alternatives></inline-formula>, Figure S5 C; <italic>n</italic><sup>ff</sup> = 2.3, Figure S5 B).</p>
<p>The membrane potential of the inhibitory population is given by
<disp-formula id="eqn8"><alternatives><graphic xlink:href="260950_eqn8.gif"/></alternatives></disp-formula>
with <italic>T</italic><sub>inh</sub>, <italic>R</italic><sub>inh</sub> and <italic>w</italic><sub>inh, 1</sub> = w<sub>inh, 2</sub> corresponding to the respective values in the network simulations. The number <italic>N</italic> of neurons per population is adjusted to the <italic>CAs</italic> in the network simulation and chosen as <italic>N</italic> = 120 (Figure S5 A).</p>
<p>The transfer function of the neurons within the population is the same as for individual neurons:
<disp-formula id="eqn9"><alternatives><graphic xlink:href="260950_eqn9.gif"/></alternatives></disp-formula>
</p>
<p>The synaptic weight changes of recurrent and feed-forward synapses follow the interplay of conventional Hebbian synaptic plasticity and synaptic scaling (<italic>i</italic> &#x220A; {1, 2}):
<disp-formula id="eqn10"><alternatives><graphic xlink:href="260950_eqn10.gif"/></alternatives></disp-formula>
<disp-formula id="eqn11"><alternatives><graphic xlink:href="260950_eqn11.gif"/></alternatives></disp-formula>
</p>
</sec>
</sec>
<sec id="s4">
<title>Results</title>
<sec id="s4a">
<title>Formation and allocation of one memory representation</title>
<p>We consider a recurrently connected excitatory population of neurons (memory area with an all-to-all connected inhibitory population to regulate the global activity level; <xref ref-type="fig" rid="fig1">Fig 1 A</xref>) with each neuron receiving inputs from several randomly assigned neurons of a second excitatory population (input area). All synapses between the excitatory neurons (red; feed-forward and recurrent) are adapted by conventional Hebbian synaptic plasticity together with homeostatic synaptic scaling (see Methods; [<xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c33">33</xref>,<xref ref-type="bibr" rid="c35">35</xref>]). A stimulus is represented by the activation of a stimulus-specific subset of neurons in the input area forming a stimulus pattern (<xref ref-type="fig" rid="fig1">Fig 1</xref> B).</p>
<p>Throughout this study, we consider initially a learning phase during which stimulus pattern <italic>A</italic> (magenta; <xref ref-type="fig" rid="fig1">Fig 1 B, C</xref>) is repeatedly presented to a blank neural circuit (no encoded CA) to investigate the basic CA formation and allocation dynamics. After the successful formation of a CA (blue; <xref ref-type="fig" rid="fig1">Fig 1 B</xref>), in a second learning phase, stimulus pattern <italic>B</italic> (green) is repeatedly presented to analyze the influence of the encoded CA on the formation and allocation of a further CA (orange). In addition, we consider three test phases (<xref ref-type="fig" rid="fig1">Fig 1 C</xref>), during which synaptic dynamics are frozen, to enable the investigation of the resulting response dynamics of the circuit according to the different stimulus patterns.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig 1.</label>
<caption><title>The interaction of conventional Hebbian synaptic plasticity and synaptic scaling enables the stimulus-dependent formation and allocation of memory representations in a neuronal network model.</title> <p>A: The network model consists of an excitatory population of neurons (memory area) receiving inputs from a second excitatory (input area) and one inhibitory population (global inhibition). All synapses in between excitatory neurons (red) are plastic resulting to (B:) the stimulus-specific (<italic>A:</italic> magenta; <italic>B</italic>: green) allocation and formation of cell assemblies (CA1: blue; CA2: orange) shown here schematically. C: Stimulation protocol used throughout this study. During the test phases (T0, T1, T2), synaptic dynamics are stopped to analyze the changes induced by the learning phases. D: The two learning phases yield a reduction of the average shortest path length (ASPL) between stimulus-activated neurons in the memory area. E: In addition, the average strength of the intra-population synapses is increased such that pop. 1 and pop. 2 become independent CAs. F, G: Furthermore, the average synaptic strength of the feed-forward connections from the stimulus-neurons to the formed CAs are adapted specifically such that pop. 1 represents stimulus <italic>A</italic> (F) and pop. 2 represents stimulus <italic>B</italic> (G). H: The response overlap (RVO) between both CAs depends non-linearly on the disparity between the stimulus patterns. Data presented are mean values with standard deviation over 100 repetitions (H) or rather 1000 repetitions (D-G).</p>
</caption>
<graphic xlink:href="260950_fig1.tif"/>
</fig>
<p>The first presentation of a new stimulus to the network triggers the activation of a widely distributed pattern of neurons (see SI Text) being not directly connected with each other (high average shortest path length (ASPL); <xref ref-type="fig" rid="fig1">Fig 1 D</xref>, test 0). By contrast, if the stimulus is repeatedly presented in a given time interval (here stimulus <italic>A</italic>), the dynamics of the network reshapes the pattern of activated neurons such that the final pattern consists of a group of neighboring, interconnected neurons (decrease in ASPL; magenta, test 1). As shown in our previous studies [<xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c35">35</xref>], the combination of synaptic plasticity and scaling together with a repeated activation of an interconnected group of neurons yields an average strengthening of the interconnecting synapses without significantly altering other synapses (blue; <xref ref-type="fig" rid="fig1">Fig 1 E</xref>, test 1). The decreased ASPL and the locally strengthened synaptic weights point to the stimulus-dependent formation of a CA during the first learning phase.</p>
<p>However, do the self-organizing dynamics also link specifically the stimulus-neurons with the CA-neurons? The repeated presentation of stimulus pattern <italic>A</italic> yields an on average strengthening of synapses projecting from the stimulus-neurons to the whole memory area (<xref ref-type="fig" rid="fig1">Fig 1 F</xref>, test 1). Essentially, the synapses from the stimulus-neurons to the CA-neurons (blue) have a significantly stronger increase in synaptic weights than the controls (gray). This implies a proper assignment of the stimulus to the newly formed CA during the learning phase resulting in a higher chance of activating the CA-neurons when the stimulus is presented later again.</p>
<p>These results reveal that the interaction of synaptic plasticity and scaling self-organizes for a wide parameter regime (see SI Text) synaptic changes at recurrent and feed-forward connections to form and allocate a memory representation in a blank neuronal network.</p>
</sec>
<sec id="s4b">
<title>Formation and allocation of a second memory representation</title>
<p>Clearly the presence of a memory representation can alter the self-organizing dynamics shown before, which could impede the proper formation of representations of further stimuli. For instance, the existence of a CA in the neuronal network could bias the adaptations of the feed-forward synapses such that a new stimulus is also assigned to this CA. This would imply that the neural circuit is unable to discriminate between the originally CA-associated stimulus and the new stimulus. Thus, to investigate the influence of prior learning, we repeatedly present a second stimulus B (second learning phase) after the proper formation of the CA associated to stimulus <italic>A</italic>.</p>
<p>Similar to the first learning phase, the repeated presentation of stimulus pattern <italic>B</italic> (here, patterns <italic>A</italic> and <italic>B</italic> have a stimulus disparity of 1 indicating no overlap between stimulus patterns; see SI Text) yields the activation of a group of strongly interconnected neurons (decreased ASPL; green, <xref ref-type="fig" rid="fig1">Fig 1 D</xref>, test 2) and a strengthening of the corresponding recurrent synaptic weights (orange; <xref ref-type="fig" rid="fig1">Fig 1 E</xref>, test 2). Thus, the stimulus-dependent formation of a new CA is not impeded by the existence of another CA. Furthermore, both CAs are distinguishable as they do not share any neuron in the memory area (<xref ref-type="fig" rid="fig1">Fig 1 H</xref>, disparity equals 1). This depends on the disparity between stimuli; for quite dissimilar stimuli (disparity &#x2273; 0.5) both CAs are separated, for more similar stimuli (0.3 &#x2272; disparity &#x2272; 0.5) the system undergoes a state transition, and for quite similar stimuli (disparity &#x2272; 0.3) both stimuli activate basically the same group of neurons. Note that the latter demonstrates that the network does correctly assign noisy versions of a learned stimulus pattern (pattern completion [<xref ref-type="bibr" rid="c10">10</xref>]) instead of forming a new CA, while the first case illustrates that the network performs pattern separation [<xref ref-type="bibr" rid="c10">10</xref>]. This indicates a correct assignment of the stimuli to the corresponding CAs, such that also in the presence of another CA the weight changes of synapses between stimulus pattern and newly formed CA are adapted accordingly (orange; <xref ref-type="fig" rid="fig1">Fig 1 G</xref>, test 2). Thus, the self-organizing dynamics yields the formation and allocation of a new CA during the second learning phase. Note that the synaptic weights of the initially encoded CA are not altered significantly during this phase (compare blue bars at test 1 with at test 2 in <xref ref-type="fig" rid="fig1">Fig 1 E-G</xref>). But, although stimulus <italic>A</italic> is not present, the second learning phase leads to a weakening of synapses projecting from stimulus A neurons to the newly formed CA considerably below control (orange; <xref ref-type="fig" rid="fig1">Fig 1 F</xref>, test 2). Similarly, during the first learning phase the synaptic weights between stimulus <italic>B</italic> neurons and the first cell assembly are also weakened (blue; <xref ref-type="fig" rid="fig1">Fig 1 G</xref>, test 1). Apparently, this weakening of synapses from the other, non-assigned stimulus to a CA reduces the chance of spurious activations. Thus, the self-organized dynamics resulting from the interaction of synaptic plasticity and scaling enables the proper formation and allocation of several memory representations without significant interferences between them.</p>
</sec>
<sec id="s4c">
<title>Generic properties of synaptic adaptations enabling the formation and allocation of memory representations</title>
<p>In order to obtain a more detailed understanding of the self-organizing processes of synaptic plasticity and scaling underlying the reliable formation and allocation of memory representations, we have to reduce the complexity of the model to enable analytical investigations. For this, we assume that the different involved neuronal populations in the input (stimulus <italic>A</italic> and <italic>B</italic> neurons) and memory area (populations 1 and 2 becoming CAs in the memory area) are by themselves homogeneous allowing the consideration of average population dynamics (<xref ref-type="fig" rid="fig2">Fig 2 A</xref>). This maps the main features of the complex network dynamics to a 7-dimensional model (see Materials and Methods and SI Text).</p>
<p>First, we investigate the formation of a memory representation in a blank neural circuit. The solutions of the nullclines of the dynamics in the space of the average recurrent synaptic weights of the two neuronal groups (<inline-formula><alternatives><inline-graphic xlink:href="260950_inline16.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xlink:href="260950_inline17.gif"/></alternatives></inline-formula>; <xref ref-type="fig" rid="fig2">Fig 2 B</xref>, left) show that the dynamics during the first learning phase is dominated by three equilibrium states: one is repulsive (red, 7) and two are attractive (green, 2 and 3; the equilibria 4, 8, 9, 10 do not influence the here discussed dynamics). The two attractive states represent that one of the two neuronal populations becomes a strongly interconnected CA (e.g., pop. 1 in state 2), while the other remains in a weakly interconnected state. The repulsive state lies on the identity line (<inline-formula><alternatives><inline-graphic xlink:href="260950_inline18.gif"/></alternatives></inline-formula>) having the same distance to both attractive states. The resulting mirror symmetry in the phase space implies that the dynamics on the one side of the identity line (reaching the attractive state lying on this side) equals the dynamics on the other side. Therefore, given that before learning both populations are similarly weak interconnected, small variations in the initial condition of the network are sufficient to determine which group of neurons will become a CA during learning (see black trace for an example). Note that this symmetry-dependent formation of a CA is quite robust against variations in parameters, as the strength of the stimulus (<inline-formula><alternatives><inline-graphic xlink:href="260950_inline19.gif"/></alternatives></inline-formula>), which agrees with results from the more detailed network model discussed before (purple dots; <xref ref-type="fig" rid="fig2">Fig 2 C</xref>).</p>
<p>In parallel to the development of the recurrent synaptic weights, the synaptic strengths of the feed-forward connections change to assure proper memory allocation. Thus, we analyze the activity-dependency of the interaction of synaptic plasticity and scaling and obtain the change of the feed-forward synaptic weights expected during the learning phase given different activity conditions of the input and memory populations (<xref ref-type="fig" rid="fig2">Fig 2 D</xref>). In general, the combination of both activity levels determines whether the weight of a feed-forward synapse is potentiated (red), depressed (blue), or not adapted (white). If both activities are on a quite high level, synapses are potentiated (case II; homosynaptic potentiation). If the presynaptic activity (input) is on a low level and the postsynaptic activity (population) is on a high level, feed-forward synapses are depressed (case I; heterosynaptic depression). However, if the postsynaptic activity is low, synaptic changes are negligible regardless of the level of presynaptic activity (cases III and IV).</p>
<p>The different parts of recurrent and feed-forward synaptic dynamics described before together lead to the formation and allocation of a CA as described in the following. During the first learning phase (stimulus <italic>A</italic>), a small variation in initial conditions breaks the symmetry (white dashed line; <xref ref-type="fig" rid="fig2">Fig 2 E</xref>) such that the system is, for instance, in the basin of attraction that population 1 will represent stimulus <italic>A</italic> (blue area). This leads to the strengthening of recurrent synapses within population 1 forming a CA (increase of <inline-formula><alternatives><inline-graphic xlink:href="260950_inline20.gif"/></alternatives></inline-formula>; <xref ref-type="fig" rid="fig2">Fig 2 B, E</xref>). In parallel, the synaptic strengthening induces an increase of the activity level of the population (state 2 in <xref ref-type="fig" rid="fig2">Fig 2 B</xref>, right) yielding, together with the active stimulus <italic>A</italic>, an average increase of the corresponding feed-forward synapses (<inline-formula><alternatives><inline-graphic xlink:href="260950_inline21.gif"/></alternatives></inline-formula>; case II). These synaptic changes push the system further away from the symmetry condition (white arrows; <xref ref-type="fig" rid="fig2">Fig 2 E</xref>, left) implying a more stable memory representation. Note that changing the strength of synapses connecting stimulus <italic>A</italic> with population 2 (<inline-formula><alternatives><inline-graphic xlink:href="260950_inline22.gif"/></alternatives></inline-formula>) could result to a shift of the symmetry condition (indicated by black arrows) counteracting the stabilization process. However, this effect is circumvented by the system as the second population has a low activity level and therefore corresponding feed-forward synapses are not adapted (case IV).</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig 2.</label>
<caption><title>Population model of network dynamics enables analytical derivation of the underlying synaptic dynamics.</title> <p>A: Schema of the population model of the averaged network dynamics (bars indicate the average over all neurons in population s &#x2208; {<italic>A</italic>, <italic>B</italic>, 1, 2}). <inline-formula><alternatives><inline-graphic xlink:href="260950_inline23.gif"/></alternatives></inline-formula>: stimulus amplitude; <inline-formula><alternatives><inline-graphic xlink:href="260950_inline24.gif"/></alternatives></inline-formula>: population activity; <inline-formula><alternatives><inline-graphic xlink:href="260950_inline25.gif"/></alternatives></inline-formula>: strength of feed-forward synapses between populations s and p; wp<sup>ec</sup>: strength of recurrent synapses within population p. B: The intersections of the population nullclines projected into weight (left) and activity (right) space reveal equilibria (attractive: green; repulsive: red) indicating the formation of a CA (green markers 2 and 3), if the system deviates from the identity line. Numbers correspond to numbers of equilibria in C. &#x03B1; is the maximal neuronal activity. C: The bifurcation diagram of the network indicates that CAs are formed for a wide variety of input amplitudes (<inline-formula><alternatives><inline-graphic xlink:href="260950_inline26.gif"/></alternatives></inline-formula>). The dashed line illustrates the value used in B (see insets). Solutions of the full network (purple dots) and population model match (deviation for large values of Ia, gray area; details see SI Text). D: The dynamics of feed-forward synaptic strengths depends on the input amplitude and the population activity subdivided into four different cases (I-IV). E: These cases (I-IV) together with the strengthening of recurrent synapses (CA1) yield the self-organized formation and allocation of CAs. Namely, during the first learning phase, synaptic changes drive the system (white dot) into regimes where either population 1 (blue) or population 2 (orange) will represent the corresponding stimulus (left: stimulus <italic>A</italic>; right: stimulus <italic>B</italic>).</p>
</caption>
<graphic xlink:href="260950_fig2.tif"/>
</fig>
<p>The formation of a CA acts as a variation or perturbation of the initial condition breaking the symmetry for the second learning phase (stimulus <italic>B</italic>; <xref ref-type="fig" rid="fig2">Fig 2 E</xref>, right). The formation of the first CA pushes the system into the blue area (CA1-arrow). This indicates that, during learning stimulus <italic>B</italic>, population 1 would also represent stimulus <italic>B</italic> impeding the discrimination ability of the network between stimulus <italic>A</italic> and <italic>B</italic>. However, during the first learning phase, as stimulus <italic>B</italic> is inactive, synapses projecting from stimulus <italic>B</italic> neurons to population 1 neurons are depressed (case I) and the system switches into the orange area. This area indicates that, during learning stimulus <italic>B</italic>, population 2 would represent the stimulus. Please note that this switch can be impeded by adapting the connections from stimulus <italic>B</italic> to population 2 (<inline-formula><alternatives><inline-graphic xlink:href="260950_inline27.gif"/></alternatives></inline-formula>) shifting the symmetry condition. But, similar to before, this effect is circumvented by the system as population 2 is basically inactive (case III). Thus, after the first learning phase, the synaptic dynamics regulated by the combination of Hebbian plasticity and synaptic scaling drives the system into an intermediate state, which implies that the system will definitely reach the desired state during learning the second stimulus (<xref ref-type="fig" rid="fig3">Fig 3</xref>). This intermediate state can only be reached if synaptic adaptations comprise three properties implied by the four cases I-IV: (i) homosynaptic potentiation (case I), (ii) heterosynaptic depression (case II), and (iii) the down-regulation of synaptic weight changes by the postsynaptic activity level (cases III and IV).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig 3.</label>
<caption><title>Summary of the synaptic changes underlying the formation and allocation of memory &#x201C;representations.</title> <p>The interaction of synaptic plasticity and scaling brings a blank network (A; test 0) during the first learning phase (B) to an intermediate state (C; test 1). From this intermediate state, the second learning phase (D) yields the system into the end state (E; test 2), in which each stimulus is allocated to one CA (A to pop. 1 and B to pop. 2). A, C, E: Thickness of lines indicates average synaptic strength. B, D: Similar to panels in <xref ref-type="fig" rid="fig2">Fig 2 E</xref>. Black indicates regimes in which both populations would represent the corresponding stimulus.</p>
</caption>
<graphic xlink:href="260950_fig3.tif"/>
</fig>
</sec>
<sec id="s4d">
<title>Experimental Data</title>
<p>In addition to the described three properties, the here-proposed model implies the existence of a symmetry condition underlying the formation and allocation of memory representations. Small variations in the initial condition of the system suffice to break this symmetry. These variations could be, aside from noise, enforced experimentally by adapting neuronal parameters in a local group of neurons. Amongst others, several experiments [<xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c37">37</xref>] indicate that the probability of a group of neurons to become part of a newly formed memory representation can be influenced by changing their excitability pharmacologically (e.g., by varying the CREB concentration). We reproduced such manipulations in the model by adapting the neuronal excitability of a group of neurons accordingly and analyzing the data similar to experiments (see Methods). Thus, we investigated the probability of a single neuron to become part of a CA averaged over the whole manipulated group of neurons (relative recruitment factor) and compared the results to experimental findings (<xref ref-type="fig" rid="fig4">Fig 4 A</xref>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Fig 4.</label>
<caption><title>The model of synaptic plasticity and scaling matches experimental <italic>in-vivo</italic> data and provides experimentally verifiable predictions.</title> <p>A: Artificially modifying the excitability of a subset of neurons alters the probability of these neurons to become part of a memory representation (normalized to control). Experimental data are taken from [<xref ref-type="bibr" rid="c36">36</xref>]. Data presented are mean values with standard error of the mean. B: The alteration of the excitability of one group of neurons (here pop. 1) compared to others yields a shift of the network's equilibria (here shown schematically; details see SI Text) compared to its initial state (dots) inducing a bias towards one population. A, B: The model analysis yields the prediction (instance iv) that this bias can be counterbalanced by additionally decreasing the average synaptic weight of the manipulated population before learning (here by a factor of 0.1), which shifts the initial state of the network back to the symmetry condition (red).</p>
</caption>
<graphic xlink:href="260950_fig4.tif"/>
</fig>
<p>On the one hand, if the excitability of a group of neurons is artificially increased briefly before learning, the probability of these neurons to become part of the memory representation is significantly enhanced. On the other hand, if the excitability is decreased, the neurons are less likely to become part of the representation. Considering the theoretical results shown before (<xref ref-type="fig" rid="fig2">Fig 2 B</xref>), this phenomenon can be explained as follows: the manipulation of the excitability in one population of neurons changes the distance between the repulsive equilibrium state (red; <xref ref-type="fig" rid="fig4">Fig 4 B</xref>) to the two attractive states (green). Thus, an increased (decreased) excitability yields a larger (smaller) distance between the repulsive state and the attractive state related to the manipulated population. This larger (smaller) distance implies a changed basin of attraction of the manipulated population enhancing the chance that the initial condition of the network (black dots) lies within this basin. This implies an increase (decrease) of the probability to become a CA as depicted by the variation of the experimentally measured single neuron probability. In addition, the theoretical analysis yields the prediction that the measured effects will be altered by manipulating other parameters. For instance, if the synaptic weight of the population with increased excitability is on average decreased before stimulus presentation (e.g., by PORCN; [<xref ref-type="bibr" rid="c38">38</xref>]), the network's initial condition is shifted such that the CREB-induced influence on the relative recruitment factor is counterbalanced (<xref ref-type="fig" rid="fig4">Fig 4 A</xref>, instance iv). Thus, the match between model and <italic>in-vivo</italic> experimental data supports the here-proposed ansatz that the combination of Hebbian synaptic plasticity and synaptic scaling yields the self-organized allocation and formation of memory representations.</p>
</sec>
<sec id="s4e">
<title>Discussion</title>
<p>Our here-presented theoretical study indicates that the formation as well as the allocation of memory representations in neuronal networks depend on the self-organized coordination of synaptic changes at feed-forward and recurrent synapses. We predict that the combined dynamics of conventional Hebbian synaptic plasticity and synaptic scaling could be sufficient for yielding this self-organized coordination as it implies three generic properties: (i) homosynaptic potentiation, (ii) heterosynaptic depression, and (iii) the down-regulation of synaptic weight changes by the postsynaptic activity level.</p>
<p>Previous theoretical studies show that the properties (i) and (ii) are required in recurrent neuronal networks to dynamically form memory representations [<xref ref-type="bibr" rid="c16">16</xref>&#x2013;<xref ref-type="bibr" rid="c18">18</xref>]. However, these studies do not consider the feed-forward synaptic dynamics. On the other hand, studies analyzing feed-forward dynamics, such as the self-organization of cortical maps [<xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c23">23</xref>,<xref ref-type="bibr" rid="c25">25</xref>], also indicate the importance of homosynaptic potentiation and heterosynaptic depression. However, these studies do not consider the recurrent synaptic dynamics. Only by considering both feed-forward <italic>and</italic> recurrent synaptic dynamics, we revealed the requirement of property (iii) that a low level of postsynaptic activity curtails the synaptic changes which is also supported by experimental evidence [<xref ref-type="bibr" rid="c39">39</xref>, <xref ref-type="bibr" rid="c40">40</xref>].</p>
<p>Note that, here, property (iii) is realized by both mechanisms: Hebbian synaptic plasticity as well as synaptic scaling. By contrast, property (i) is implemented by Hebbian synaptic plasticity only and property (ii) is realized by synaptic scaling only. This indicates that synaptic scaling could have an essential role in the allocation and formation of multiple memory representations beyond the widely assumed stabilization of neural network dynamics [<xref ref-type="bibr" rid="c29">29</xref>, <xref ref-type="bibr" rid="c32">32</xref>, <xref ref-type="bibr" rid="c33">33</xref>, <xref ref-type="bibr" rid="c41">41</xref>, <xref ref-type="bibr" rid="c42">42</xref>].</p>
<p>Similar to previous studies [<xref ref-type="bibr" rid="c16">16</xref>,<xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c43">43</xref>], we consider here an abstract model to describe the neuronal and synaptic dynamics of the network. Despite the abstract level of description, the model matches experimental <italic>in-vivo</italic> data of memory allocation. Other theoretical models match similar experimental data [<xref ref-type="bibr" rid="c28">28</xref>, <xref ref-type="bibr" rid="c44">44</xref>]; however, these models are of greater biological detail including more dynamic processes (e.g., short-term plasticity). However, only by considering an abstract model, we have been able to derive analytical expressions such that we could find the requirement of the three generic properties yielding the proper formation and allocation of memories. Remarkably, the synaptic plasticity processes considered in the detailed models [<xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c44">44</xref>,<xref ref-type="bibr" rid="c45">45</xref>] also imply the three generic properties (i-iii) supporting our findings.</p>
<p>The here shown results indicate that the combined dynamics of Hebbian synaptic plasticity and synaptic scaling are sufficient to determine the basic dynamics underlying the allocation and formation of memory representations. Interestingly, for several different stimuli the combined dynamics always yields the formation of separated memory representations. In particular the process of heterosynaptic depression impedes the formation of overlaps between several memory representations. However, amongst others, experimental results indicate that memory representations can overlap [<xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c46">46</xref>,<xref ref-type="bibr" rid="c47">47</xref>] and, in addition, theoretical studies show that overlaps increase the storage capacity of a neuronal network [<xref ref-type="bibr" rid="c14">14</xref>] and can support memory recall [<xref ref-type="bibr" rid="c48">48</xref>]. To partially counterbalance the effect of heterosynaptic depression to enable the formation of overlaps, further time-dependent processes are required. For instance, the CREB-induced enhancement of neuronal excitability biases the neuronal and synaptic dynamics such that the respective subgroup of neurons is more likely to be involved in the formation of a memory representation (<xref ref-type="fig" rid="fig4">Fig 4</xref>; [<xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c44">44</xref>]). Furthermore, the dynamics of CREB seem to be time-dependent [<xref ref-type="bibr" rid="c28">28</xref>,<xref ref-type="bibr" rid="c36">36</xref>,<xref ref-type="bibr" rid="c37">37</xref>]. Therefore, the enhancement of CREB can counterbalance heterosynaptic depression for a given period of time and, by this, could enable the formation of overlaps. We expect that the impact of such time-dependent processes on the dynamics of memories can be integrated into the here-proposed model to analyze the detailed formation of overlaps between memory representations.</p>
</sec></sec>
<sec id="s5">
<title>Conclusion</title>
<p>In summary, the here-shown theoretical results indicate that the complex dynamics of allocation and formation of multiple memory representations could result from the interplay between Hebbian synaptic plasticity and synaptic scaling. This interplay reliably coordinates in a self-organized manner synaptic changes at feed-forward as well as recurrent synapses as it entails three generic properties. Furthermore, the here-derived theoretical model enables the integration of further neuronal and synaptic processes to assess a general understanding of the dynamics underlying the self-organized allocation and formation of multiple memory representations. For more information, see SI text.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We thank Florentin W&#x00F6;rg&#x00F6;tter for fruitful comments. The research was funded by the H2020-FETPROACT project Plan4Act (#732266) [JMA, CT], by the Federal Ministry of Education and Research Germany (#01GQ1005A, #01GQ1005B) [TN, CT], and by the International Max Planck Research School for Physics of Biological and Complex Systems by stipends of the country of Lower Saxony with funds from the initiative &#x201C;Nieders&#x00E4;chsisches Vorab&#x201D; and of the University of G&#x00F6;ttingen [TN].</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><given-names>W.</given-names> <surname>James</surname></string-name>. <article-title>The principles of psychology</article-title>. <source>New York: Henry Holt and Company</source>, <year>1890</year>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Konorski</surname></string-name>. <article-title>Conditioned reflexes and neuron organization</article-title>. <source>Cambridge: Cambridge University Press</source>, <year>1948</year>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><given-names>D. O.</given-names> <surname>Hebb</surname></string-name>. <article-title>The Organization of Behaviour</article-title>. <source>Wiley, New York</source>, <year>1949</year>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><given-names>G.</given-names> <surname>Palm</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Knoblauch</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Hauser</surname></string-name>, and <string-name><given-names>A.</given-names> <surname>Sch&#x00FC;z</surname></string-name>. <article-title>Cell assemblies in the cerebral cortex</article-title>. <source>Biol. Cybern</source>., <volume>108</volume>:<fpage>559</fpage>&#x2013;<lpage>572</lpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Holtmaat</surname></string-name> and <string-name><given-names>P.</given-names> <surname>Caroni</surname></string-name>. <article-title>Functional and structural underpinnings of neuronal assembly formation in learning</article-title>. <source>Nat. Neurosci</source>., <volume>19</volume>:<fpage>1553</fpage>&#x2013;<lpage>1562</lpage>, <month>December</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><given-names>T. V. P.</given-names> <surname>Bliss</surname></string-name> and <string-name><given-names>T.</given-names> <surname>Lomo</surname></string-name>. <article-title>Long-lasting potentiation of synaptic transmission in the dentate area of the anaesthetized rabbit following stimulation of the perforant path</article-title>. <source>J. Physiol</source>., <volume>232</volume>:<fpage>331</fpage>&#x2013;<lpage>356</lpage>, <year>1973</year>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><given-names>W. B.</given-names> <surname>Levy</surname></string-name> and <string-name><given-names>O.</given-names> <surname>Steward</surname></string-name>. <article-title>Temporal contiguity requirements for long-term associative potentiation/depression in the hippocampus</article-title>. <source>Neuroscience</source>, <volume>8</volume>(<issue>4</issue>):<fpage>791</fpage>&#x2013;<lpage>797</lpage>, <year>1983</year>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><given-names>S. J.</given-names> <surname>Martin</surname></string-name>, <string-name><given-names>P. D.</given-names> <surname>Grimwood</surname></string-name>, and <string-name><given-names>R. G. M.</given-names> <surname>Morris</surname></string-name>. <article-title>Synaptic plasticity and memory: an evaluation of the hypothesis</article-title>. <source>Annu. Rev. Neurosci</source>., <volume>23</volume>:<fpage>649</fpage>&#x2013;<lpage>711</lpage>, <year>2000</year>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><given-names>R. C.</given-names> <surname>Malenka</surname></string-name> and <string-name><given-names>M. F.</given-names> <surname>Bear</surname></string-name>. <article-title>LTP and LTD: an embarrassment of riches</article-title>. <source>Neuron</source>, <volume>44</volume>:<fpage>5</fpage>&#x2013;<lpage>21</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><given-names>M. R.</given-names> <surname>Hunsaker</surname></string-name> and <string-name><given-names>R. P.</given-names> <surname>Kesner</surname></string-name>. <article-title>The operation of pattern separation and pattern completion processes associated with different attributes or domains of memory</article-title>. <source>Neurosci. Biobehav. Rev</source>., <volume>37</volume>:<fpage>36</fpage>&#x2013;<lpage>58</lpage>, <month>January</month> <year>2013</year>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><given-names>J. J.</given-names> <surname>Hopfield</surname></string-name>. <article-title>Neural networks and physical systems with emergent collective computational abilities</article-title>. <source>Proceedings of the National Academy of Sciences</source>, <volume>79</volume>(<issue>8</issue>):<fpage>2554</fpage>&#x2013;<lpage>2558</lpage>, <month>apr</month> <year>1982</year>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><given-names>J. J.</given-names> <surname>Hopfield</surname></string-name>. <article-title>Neurons with graded response have collective computational properties like those of two-state neurons</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>81</volume>:<fpage>3088</fpage>&#x2013;<lpage>3092</lpage>, <year>1984</year>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><given-names>D. J.</given-names> <surname>Amit</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Gutfreund</surname></string-name>, and <string-name><given-names>H.</given-names> <surname>Sompolinsky</surname></string-name>. <article-title>Storing infinite numbers of patterns in a spin-glas model of neural networks</article-title>. <source>Phys. Rev. Lett</source>., <volume>55</volume>(<issue>14</issue>):<fpage>1530</fpage>&#x2013;<lpage>1533</lpage>, <year>1985</year>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Tsodyks</surname></string-name> and <string-name><given-names>M.</given-names> <surname>Feigelman</surname></string-name>. <article-title>Enhanced storage capacity in neural networks with low level of activity</article-title>. <source>Europhys. Lett</source>., <volume>6</volume>(<issue>2</issue>):<fpage>101</fpage>&#x2013;<lpage>105</lpage>, <year>1988</year>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><given-names>D. J.</given-names> <surname>Amit</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Brunel</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Tsodyks</surname></string-name>. <article-title>Correlations of cortical Hebbian reverberations: theory versus experiment</article-title>. <source>J. Neurosci</source>., <volume>14</volume>(<issue>11</issue>):<fpage>6435</fpage>&#x2013;<lpage>6445</lpage>, <year>1994</year>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Tetzlaff</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Kolodziejski</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Timme</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Tsodyks</surname></string-name>, and <string-name><given-names>F.</given-names> <surname>W&#x00F6;rg&#x00F6;tter</surname></string-name>. <article-title>Synaptic scaling enables dynamically distinct short- and long-term memory formation</article-title>. <source>PLoS Comput. Biol</source>., <volume>9</volume>(<issue>10</issue>):<fpage>e1003307</fpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Litwin-Kumar</surname></string-name> and <string-name><given-names>B.</given-names> <surname>Doiron</surname></string-name>. <article-title>Formation and maintaince of neuronal assemblies through synaptic plasticity</article-title>. <source>Nat. Commun</source>., <volume>5</volume>:<fpage>5319</fpage>, <year>2014</year>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><given-names>F.</given-names> <surname>Zenke</surname></string-name>, <string-name><given-names>E. J.</given-names> <surname>Agnes</surname></string-name>, and <string-name><given-names>W.</given-names> <surname>Gerstner</surname></string-name>. <article-title>Diverse synaptic plasticity mechanisms orchestrated to form and retrieve memories in spiking neural networks</article-title>. <source>Nat. Commun</source>., <volume>6</volume>:<fpage>6922</fpage>, <month>apr</month> <year>2015</year>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><given-names>N.</given-names> <surname>Brunel</surname></string-name>. <article-title>Is cortical connectivity optimized for storing information?</article-title> <source>Nat. Neurosci</source>., <volume>19</volume>:<fpage>749</fpage>&#x2013;<lpage>755</lpage>, <month>May</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><given-names>D. J.</given-names> <surname>Willshaw</surname></string-name>, <string-name><given-names>O. P.</given-names> <surname>Buneman</surname></string-name>, and <string-name><given-names>H. C.</given-names> <surname>Longuet-Higgins</surname></string-name>. <article-title>Non-holographic associative memory</article-title>. <source>Nature</source>, <volume>222</volume>:<fpage>960</fpage>&#x2013;<lpage>962</lpage>, <year>1969</year>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Kohonen</surname></string-name>. <article-title>Self-organized formation of topologically correct feature maps</article-title>. <source>Biol. Cybern</source>., <volume>43</volume>(<issue>1</issue>):<fpage>59</fpage>&#x2013;<lpage>69</lpage>, <year>1982</year>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><given-names>D. M.</given-names> <surname>Adelsberger-Mangan</surname></string-name> and <string-name><given-names>W. B.</given-names> <surname>Levy</surname></string-name>. <article-title>Information maintenance and statistical dependence reduction in simple neural networks</article-title>. <source>Biol. Cybern</source>., <volume>67</volume>:<fpage>469</fpage>&#x2013;<lpage>477</lpage>, <year>1992</year>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><given-names>T. J.</given-names> <surname>Sullivan</surname></string-name> and <string-name><given-names>V. R.</given-names> <surname>de Sa</surname></string-name>. <article-title>Homeostatic synaptic scaling in self-organizing maps</article-title>. <source>Neural Networks</source>, <volume>19</volume>(<issue>6-7</issue>):<fpage>734</fpage>&#x2013;<lpage>743</lpage>, <month>jul</month> <year>2006</year>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Knoblauch</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Palm</surname></string-name>, and <string-name><given-names>F. T.</given-names> <surname>Sommer</surname></string-name>. <article-title>Memory capacities for synaptic and structural plasticity</article-title>. <source>Neural Comput</source>., <volume>22</volume>(<issue>2</issue>):<fpage>289</fpage>&#x2013;<lpage>341</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><given-names>J.-L.R.</given-names> <surname>Stevens</surname></string-name> <string-name><given-names>J. S.</given-names> <surname>Law</surname></string-name> <string-name><given-names>J.</given-names> <surname>Antolik</surname></string-name>, and <string-name><given-names>J. A.</given-names> <surname>Bednar</surname></string-name>. <article-title>Mechanisms for stable, robust, and adaptive development of orientation maps in the primary visual cortex</article-title>. <source>J. Neurosci</source>., <volume>33</volume>(<issue>40</issue>):<fpage>15747</fpage>&#x2013;<lpage>15766</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Babadi</surname></string-name> and <string-name><given-names>H.</given-names> <surname>Sompolinsky</surname></string-name>. <article-title>Sparseness and expansion in sensory representations</article-title>. <source>Neuron</source>, <volume>83</volume>(<issue>5</issue>):<fpage>1213</fpage>&#x2013;<lpage>1226</lpage>, <month>sep</month> <year>2014</year>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Rogerson</surname></string-name>, <string-name><given-names>D. J.</given-names> <surname>Cai</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Frank</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Sano</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Shobe</surname></string-name>, <string-name><given-names>M. F.</given-names> <surname>Lopez-Aranda</surname></string-name>, and <string-name><given-names>A. J.</given-names> <surname>Silva</surname></string-name>. <article-title>Synaptic tagging during memory allocation</article-title>. <source>Nat. Rev. Neurosci</source>., <volume>15</volume>(<issue>3</issue>):<fpage>157</fpage>&#x2013;<lpage>169</lpage>, <month>feb</month> <year>2014</year>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><given-names>G.</given-names> <surname>Kastellakis</surname></string-name>, <string-name><given-names>A. J.</given-names> <surname>Silva</surname></string-name>, and <string-name><given-names>P.</given-names> <surname>Poirazi</surname></string-name>. <article-title>Linking memories across time via neuronal and dendritic overlaps in model neurons with active dendrites</article-title>. <source>Cell Reports</source>, <volume>17</volume>(<issue>6</issue>):<fpage>1491</fpage>&#x2013;<lpage>1504</lpage>, <month>nov</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><given-names>L. F.</given-names> <surname>Abbott</surname></string-name> and <string-name><given-names>S. B.</given-names> <surname>Nelson</surname></string-name>. <article-title>Synaptic plasticity: Taming the beast</article-title>. <source>Nat. Neurosci. (Suppl.)</source>, <volume>3</volume>:<fpage>1178</fpage>&#x2013;<lpage>1183</lpage>, <year>2000</year>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><given-names>W.</given-names> <surname>Gerstner</surname></string-name> and <string-name><given-names>W. M.</given-names> <surname>Kistler</surname></string-name>. <article-title>Mathematical formulations of Hebbian learning</article-title>. <source>Biol. Cybern</source>., <volume>87</volume>:<fpage>404</fpage>&#x2013;<lpage>415</lpage>, <year>2002</year>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><given-names>G. G.</given-names> <surname>Turrigiano</surname></string-name>, <string-name><given-names>K. R.</given-names> <surname>Leslie</surname></string-name>, <string-name><given-names>N. S.</given-names> <surname>Desai</surname></string-name>, <string-name><given-names>L. C.</given-names> <surname>Rutherford</surname></string-name>, and <string-name><given-names>S. B.</given-names> <surname>Nelson</surname></string-name>. <article-title>Activity-dependent scaling of quantal amplitude in neocortical neurons</article-title>. <source>Nature</source>, <volume>391</volume>:<fpage>892</fpage>&#x2013;<lpage>896</lpage>, <year>1998</year>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><given-names>G. G.</given-names> <surname>Turrigiano</surname></string-name> and <string-name><given-names>S. B.</given-names> <surname>Nelson</surname></string-name>. <article-title>Homeostatic plasticity in the developing nervous system</article-title>. <source>Nat. Rev. Neurosci</source>., <volume>5</volume>:<fpage>97</fpage>&#x2013;<lpage>107</lpage>, <month>Feb</month> <year>2004</year>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Tetzlaff</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Kolodziejski</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Timme</surname></string-name>, and <string-name><given-names>F.</given-names> <surname>W&#x00F6;rg&#x00F6;tter</surname></string-name>. <article-title>Synaptic scaling in combination with many generic plasticity mechanisms stabilizes circuit connectivity</article-title>. <source>Front. Comput. Neurosci</source>., <volume>5</volume>:<fpage>47</fpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><given-names>K. B.</given-names> <surname>Hengen</surname></string-name>, <string-name><given-names>M. E.</given-names> <surname>Lambo</surname></string-name>, <string-name><given-names>S. D.</given-names> <surname>Van</surname></string-name> <article-title>Hooser, D. B. Katz, and G. G. Turrigiano. Firing rate homeostasis in visual cortex of freely behaving rodents</article-title>. <source>Neuron</source>, <volume>80</volume>:<fpage>335</fpage>&#x2013;<lpage>342</lpage>, <year>2013</year>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Nachstedt</surname></string-name> and <string-name><given-names>C.</given-names> <surname>Tetzlaff</surname></string-name>. <article-title>Working memory requires a combination of transient and attractor-dominated dynamics to process unreliably timed inputs</article-title>. <source>Sci. Rep</source>., <volume>7</volume>:<fpage>2473</fpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><given-names>A. P.</given-names> <surname>Yiu</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Mercaldo</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Yan</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Richards</surname></string-name>, <string-name><given-names>A. J.</given-names> <surname>Rashid</surname></string-name>, <string-name><given-names>H.-L.</given-names> <surname>Hsiang</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Pressey</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Mahadevan</surname></string-name>, <string-name><given-names>M. M.</given-names> <surname>Tran</surname></string-name>, <string-name><given-names>S. A.</given-names> <surname>Kushner</surname></string-name>, <string-name><given-names>M. A.</given-names> <surname>Woodin</surname></string-name>, <string-name><given-names>P. W.</given-names> <surname>Frankland</surname></string-name>, and <string-name><given-names>S. A.</given-names> <surname>Josselyn</surname></string-name>. <article-title>Neurons are recruited to a memory trace based on relative neuronal excitability immediately before training</article-title>. <source>Neuron</source>, <volume>83</volume>(<issue>3</issue>):<fpage>722</fpage>&#x2013;<lpage>735</lpage>, <month>aug</month> <year>2014</year>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><given-names>P. W.</given-names> <surname>Frankland</surname></string-name> and <string-name><given-names>S. A.</given-names> <surname>Josselyn</surname></string-name>. <article-title>Memory allocation</article-title>. <source>Neuropsychopharmacology</source>, <volume>40</volume>(<issue>40</issue>):<fpage>243</fpage>&#x2013;<lpage>243</lpage>, <month>jan</month> <year>2015</year>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><given-names>N.</given-names> <surname>Erlenhardt</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Yu</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Abiraman</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Yamasaki</surname></string-name>, <string-name><given-names>J. I.</given-names> <surname>Wadiche</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Tomita</surname></string-name>, and <string-name><given-names>D. S.</given-names> <surname>Bredt</surname></string-name>. <article-title>Porcupine controls hippocampal ampar levels, composition, and synaptic transmission</article-title>. <source>Cell Rep</source>., <volume>14</volume>:<fpage>782</fpage>&#x2013;<lpage>794</lpage>, <month>February</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><given-names>P. J.</given-names> <surname>Sj&#x00F6;str&#x00F6;m</surname></string-name>, <string-name><given-names>G. G.</given-names> <surname>Turrigiano</surname></string-name>, and <string-name><given-names>S. B.</given-names> <surname>Nelson</surname></string-name>. <article-title>Rate, timing, and cooperativity jointly determine cortical synaptic plasticity</article-title>. <source>Neuron</source>, <volume>32</volume>:<fpage>1149</fpage>&#x2013;<lpage>1164</lpage>, <year>2001</year>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Graupner</surname></string-name> and <string-name><given-names>N.</given-names> <surname>Brunel</surname></string-name>. <article-title>Mechanisms of induction and maintenance of spike-timing dependent plasticity in biophysical synapse models</article-title>. <source>Front. Comput. Neurosci</source>., <volume>4</volume>:<fpage>23</fpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><given-names>G. G.</given-names> <surname>Turrigiano</surname></string-name>. <article-title>The dialectic of Hebb and homeostasis</article-title>. <source>Philos. Trans. R. Soc. Lond. B. Biol. Sci</source>., <volume>372</volume>, <month>March</month> <year>2017</year>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><given-names>F.</given-names> <surname>Zenke</surname></string-name> and <string-name><given-names>W.</given-names> <surname>Gerstner</surname></string-name>. <article-title>Hebbian plasticity requires compensatory processes on multiple timescales</article-title>. <source>Philos. Trans. R. Soc. Lond. B. Biol. Sci</source>., <volume>372</volume>, <month>March</month> <year>2017</year>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Tetzlaff</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Dasgupta</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Kulvicius</surname></string-name>, and <string-name><given-names>F.</given-names> <surname>W&#x00F6;rg&#x00F6;tter</surname></string-name>. <article-title>The use of Hebbian cell assemblies for nonlinear computation</article-title>. <source>Sci. Rep</source>., <volume>5</volume>:<fpage>12866</fpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><given-names>D.</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Par&#x00E9;</surname></string-name>, and <string-name><given-names>S. S.</given-names> <surname>Nair</surname></string-name>. <article-title>Assignment of model amygdala neurons to the fear memory trace depends on competitive synaptic interactions</article-title>. <source>J. Neurosci</source>., <volume>33</volume>:<fpage>14354</fpage>&#x2013;<lpage>14358</lpage>, <month>September</month> <year>2013</year>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><given-names>D.</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Par&#x00E9;</surname></string-name>, and <string-name><given-names>S. S.</given-names> <surname>Nair</surname></string-name>. <article-title>Mechanisms contributing to the induction and storage of pavlovian fear memories in the lateral amygdala</article-title>. <source>Learn. Mem</source>., <volume>20</volume>:<fpage>421</fpage>&#x2013;<lpage>430</lpage>, <month>July</month> <year>2013</year>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><given-names>D. J.</given-names> <surname>Cai</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Aharoni</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Shuman</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Shobe</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Biane</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Song</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Wei</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Veshkini</surname></string-name>, <string-name><given-names>M.</given-names> <surname>La-Vu</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Lou</surname></string-name>, <string-name><given-names>S. E.</given-names> <surname>Flores</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Kim</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Sano</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Zhou</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Baumgaertel</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Lavi</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Kamata</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Tuszynski</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Mayford</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Golshani</surname></string-name>, and <string-name><given-names>A. J.</given-names> <surname>Silva</surname></string-name>. <article-title>A shared neural ensemble links distinct contextual memories encoded close in time</article-title>. <source>Nature</source>, <volume>534</volume>(<issue>7605</issue>):<fpage>115</fpage>&#x2013;<lpage>118</lpage>, <month>may</month> <year>2016</year>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Yokose</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Okubo-Suzuki</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Nomoto</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Ohkawa</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Nishizono</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Suzuki</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Matsuo</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Tsujimura</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Takahashi</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Nagase</surname></string-name>, <string-name><given-names>A. M.</given-names> <surname>Watabe</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Sasahara</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Kato</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Inokuchi</surname></string-name>. <article-title>Overlapping memory trace indispensable for linking, but not recalling, individual memories</article-title>. <source>Science</source>, <volume>355</volume>:<fpage>398</fpage>&#x2013;<lpage>403</lpage>, <month>January</month> <year>2017</year>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Recanatesi</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Katkov</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Romani</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Tsodyks</surname></string-name>. <article-title>Neural network model of memory retrieval</article-title>. <source>Front. Comp. Neurosci</source>., <volume>9</volume>:<fpage>149</fpage>, <year>2015</year>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="journal"><string-name><given-names>M.</given-names> <surname>Rubinov</surname></string-name> and <string-name><given-names>O.</given-names> <surname>Sporns</surname></string-name>. <article-title>Complex network measures of brain connectivity: uses and interpretations</article-title>. <source>Neuroimage</source>, <volume>52</volume>:<fpage>1059</fpage>&#x2013;<lpage>1069</lpage>, <year>2010</year>.</mixed-citation></ref>
</ref-list>
</back>
</article>