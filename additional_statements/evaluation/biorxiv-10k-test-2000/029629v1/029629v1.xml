<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/029629</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Scientific Communication</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Relative Citation Ratio (RCR): A new metric that uses citation rates to measure influence at the article level</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7657-552X</contrib-id>
<name><surname>Hutchins</surname><given-names>B. Ian</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7657-552X</contrib-id>
<name><surname>Yuan</surname><given-names>Xin</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7657-552X</contrib-id>
<name><surname>Anderson</surname><given-names>James M.</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7657-552X</contrib-id>
<name><surname>Santangelo</surname><given-names>George M.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Office of Portfolio Analysis</institution></aff>
<aff id="a2"><label>2</label><institution>Division of Program Coordination, Planning, and Strategic Initiatives, National Institutes of Health</institution>, Bethesda, MD</aff>
</contrib-group>
<pub-date pub-type="epub"><year>2015</year></pub-date>
<elocation-id>029629</elocation-id>
<history>
<date date-type="received">
<day>21</day>
<month>10</month>
<year>2015</year>
</date>
<date date-type="accepted">
<day>22</day>
<month>10</month>
<year>2015</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2015, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2015</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="029629.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Despite their recognized limitations, bibliometric assessments of scientific productivity have been widely adopted. We describe here an improved method that makes novel use of the co-citation network of each article to field-normalize the number of citations it has received. The resulting Relative Citation Ratio is article-level and field-independent, and provides an alternative to the invalid practice of using Journal Impact Factors to identify influential papers. To illustrate one application of our method, we analyzed 88,835 articles published between 2003 and 2010, and found that the National Institutes of Health awardees who authored those papers occupy relatively stable positions of influence across all disciplines. We demonstrate that the values generated by this method strongly correlate with the opinions of subject matter experts in biomedical research, and suggest that the same approach should be generally applicable to articles published in all areas of science. A beta version of <italic>iCite</italic>, our web tool for calculating Relative Citation Ratios of articles listed in PubMed, is available at <ext-link ext-link-type="uri" xlink:href="https://icite.od.nih.gov">https://icite.od.nih.gov</ext-link>.</p>
</abstract>
<counts>
<page-count count="32"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1"><title>Introduction</title>
<p>In the current highly competitive pursuit of research positions and funding support (<xref ref-type="bibr" rid="c12">Couzin-Frankel, 2013</xref>), faculty hiring committees and grant review panels must make difficult predictions about the likelihood of future scientific success. Traditionally, these judgments have largely depended on recommendations by peers, informal interactions, and other subjective criteria. In recent years, decision-makers have increasingly turned to numerical approaches such as counting first or corresponding author publications, using the impact factor of the journals in which those publications appear, and computing Hirsch or H-index (<xref ref-type="bibr" rid="c21">Hirsch, 2005</xref>). The widespread adoption of these metrics, and the recognition that they are inadequate (<xref ref-type="bibr" rid="c39">Seglen, 1997</xref>; <xref ref-type="bibr" rid="c2">Anon, 2005</xref>, <xref ref-type="bibr" rid="c3">Anon, 2013</xref>), highlight the ongoing need for alternative methods that can provide effectively normalized and reliable data-driven input to administrative decision-making, both as a means of sorting through large pools of qualified candidates, and as a way to help combat implicit bias. A return to purely subjective evaluation with its attendant risk of partiality is neither desirable nor practical, and the use of metrics that are of limited value in decision-making is widespread and growing (<xref ref-type="bibr" rid="c33">Pulverer, 2013</xref>). The need for useful metrics is particularly pressing for funding agencies making policy decisions based upon the evaluation of large portfolios that often encompass diverse areas of science.</p>
<p>Though each of the above mentioned methods of quantitation has strengths, accompanying weaknesses limit their utility. Counting first or corresponding author publications does on some level reflect the extent of a scientist&#x2019;s contribution to their field, but it has the unavoidable effect of privileging quantity over quality, and may undervalue collaborative science (<xref ref-type="bibr" rid="c42">Stallings et al., 2013</xref>). Journal impact factor (JIF) was for a time seen as a valuable indicator of scientific quality because it serves as a convenient, and not wholly inaccurate, proxy for expert opinion (<xref ref-type="bibr" rid="c18">Garfield, 2006</xref>). However, its blanket use also camouflages large differences in the influence of individual papers. This is because impact factor is calculated as the average number of times articles published over a two-year period in a given journal are cited; in reality, citations follow a log-normal rather than a Gaussian distribution (<xref ref-type="bibr" rid="c32">Price, 1976</xref>; <xref ref-type="bibr" rid="c50">Wang et al., 2013</xref>). Moreover, since practitioners in disparate fields have differential access to high-profile publication venues, impact factor is of limited use in multidisciplinary science-of-science analyses. Despite these serious flaws, JIF continues to have a large effect on funding and hiring decisions (<xref ref-type="bibr" rid="c2">Anon, 2005</xref>; <xref ref-type="bibr" rid="c23">Johnston, 2013</xref>; <xref ref-type="bibr" rid="c28">Misteli, 2013</xref>). H-index, which attempts to assess the cumulative impact of the work done by an individual scientist, disadvantages early career stage investigators; it also undervalues some fields of research by failing to normalize raw citation counts (<xref ref-type="bibr" rid="c33">Pulverer, 2013</xref>).</p>
<p>Alternative models for quantifying scientific accomplishment have been proposed but have not been widely adopted, perhaps because they are overly complicated to calculate and/or are difficult to interpret (<xref ref-type="bibr" rid="c7">Bollen et al., 2009</xref>; <xref ref-type="bibr" rid="c48">Waltman et al., 2011a</xref>). Some have dramatically improved our theoretical understanding of citation dynamics (<xref ref-type="bibr" rid="c47">Walker et al., 2007</xref>; <xref ref-type="bibr" rid="c35">Radicchi et al., 2008</xref>; <xref ref-type="bibr" rid="c43">Stringer et al., 2010</xref>; <xref ref-type="bibr" rid="c50">Wang et al., 2013</xref>). However, to combine a further technical advance with a high likelihood of widespread adoption by varied stakeholders, including scientists, administrators and funding agencies, several practical challenges must be overcome. Citation metrics must be article-level, field-normalized in a way that is scalable from small to large portfolios without introducing significant bias at any level, benchmarked to peer performance in order to be interpretable, and correlated with expert opinion. In addition, metrics should be freely accessible and calculated in a transparent way. Many efforts have been made to fulfill one or more of these requirements, including citation normalization to journals or journal categories (<xref ref-type="bibr" rid="c30">Moed et al., 1985</xref>; <xref ref-type="bibr" rid="c52">Zitt and Small, 2008</xref>; <xref ref-type="bibr" rid="c31">Opthof and Leydesdorff, 2010</xref>; <xref ref-type="bibr" rid="c45">van Raan et al., 2010</xref>; <xref ref-type="bibr" rid="c48">Waltman et al., 2011a</xref>, <xref ref-type="bibr" rid="c49">2011b</xref>; <xref ref-type="bibr" rid="c8">Bornmann and Leydesdorff, 2013</xref>), citation percentiles (<xref ref-type="bibr" rid="c8">Bornmann and Leydesdorff, 2013</xref>; <xref ref-type="bibr" rid="c9">Bornmann and Marx, 2013</xref>), eigenvector normalization (<xref ref-type="bibr" rid="c4">Bergstrom and West, 2008</xref>; <xref ref-type="bibr" rid="c5">Bergstrom et al., 2008</xref>) or source-normalization (<xref ref-type="bibr" rid="c52">Zitt and Small, 2008</xref>; <xref ref-type="bibr" rid="c29">Moed, 2010</xref>) including the Mean Normalized Citation Score (<xref ref-type="bibr" rid="c48">Waltman et al., 2011a</xref>) and Source-Normalized Impact per Paper metrics (<xref ref-type="bibr" rid="c29">Moed, 2010</xref>). While all are improvements on Impact Factor, none meet all of the criteria listed above. Furthermore, these existing approaches are often unhelpful to decision-makers because they aggregate works from researchers across disparate geographical regions and institutional types. For example, current methods do not provide a way for a primarily undergraduate institutions to compare their portfolios against other teaching-focused institutions, nor do they allow developing nations to compare their research to that done in other developing nations (<xref ref-type="bibr" rid="c13">Crous, 2014</xref>). Incorporating a customizable benchmark as an integral part of an ideal citation metric would enable such an apples to apples comparison and facilitate downstream decision making activity.</p>
<p>We report here the development and validation of the Relative Citation Ratio (RCR) metric, which meets all of the above criteria and is based upon the novel idea of using the co-citation network of each article to field- and time-normalize by calculating the expected citation rate from the aggregate citation behavior of a topically linked cohort. An average citation rate is computed for the network, benchmarked to peer performance, and used as the RCR denominator; as is true of other bibliometrics, article citation rate (ACR) is used as the numerator. We use the RCR metric here to determine the extent to which National Institutes of Health (NIH) awardees maintain high or low levels of influence on their respective fields of research.</p>
</sec>
<sec id="s2"><title>Results</title>
<sec id="s2a"><title>Co-citation networks represent an article&#x2019;s area of influence</title>
<p>Choosing to cite is the long-standing way in which one scholar acknowledges the relevance of another&#x2019;s work. Before now, however, the utility of citations as a metric for quantifying influence has been limited, primarily because it is difficult to compare the value of one citation to another; different fields have different citation behaviors and are composed of widely varying numbers of potential citers (<xref ref-type="bibr" rid="c22">Jeong et al., 2003</xref>; <xref ref-type="bibr" rid="c34">Radicchi and Castellano, 2012</xref>). An effective citation-based evaluative tool must also take into account the length of time a paper has been available to potential citers, since a recently published article has had less time to accumulate citations than an older one. Finally, fair comparison is complicated by the fact that an author&#x2019;s choice of which work to cite is not random; a widely known paper is more likely to be referenced than an obscure one of equal relevance. This is because the accrual of citations follows a power law or log-normal pattern, in accordance with a process called preferential attachment (<xref ref-type="bibr" rid="c22">Jeong et al., 2003</xref>; <xref ref-type="bibr" rid="c16">Eom and Fortunato, 2011</xref>; <xref ref-type="bibr" rid="c50">Wang et al., 2013</xref>). Functionally this means that, each time a paper is cited, it is <italic>a priori</italic> more likely to be cited again.</p>
<p>An accurate citation-based measure of influence must address all of these issues, but we reasoned that the key to developing such a metric would be the careful identification of a comparison group, i.e., a cluster of interrelated papers against which the citation performance of an article of interest, or reference article (RA), could be evaluated. Using a network of papers linked to that RA through citations occurred to us as a promising possibility (<bold><xref ref-type="fig" rid="fig1">Figure 1</xref></bold>). There are <italic>a priori</italic> three types of article-linked citation networks (<xref ref-type="bibr" rid="c41">Small, 1973</xref>). A citing network is the collection of papers citing the RA (<bold><xref ref-type="fig" rid="fig1">Figure 1a</xref></bold>, top row), a co-citation network is defined as the other papers appearing in the reference lists alongside the RA (<bold><xref ref-type="fig" rid="fig1">Figure 1a</xref></bold>, middle row), and a cited network is the collection of papers in the reference list of the RA (<bold><xref ref-type="fig" rid="fig1">Figure 1a</xref></bold>, bottom row).</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Properties of co-citation networks. (<bold><italic>a</italic></bold>) Schematic of a co-citation network. The Reference Article (RA) (red, middle row) cites previous papers from the literature (orange, bottom row); subsequent papers cite the RA (blue, top row). The co-citation network is the set of papers that appear alongside the article in the subsequent citing papers (green, middle row). The Field Citation Rate is calculated as the mean of the latter articles&#x2019; journal citation rates. (<bold><italic>b</italic></bold>) Growth of co-citation networks over time. Three RAs published in 2006 (red dots) were cited 5 (top row), 9 (middle row), or 31 times (bottom row) by 2011. Three intervals were chosen to illustrate the growth of the corresponding co-citation networks: 2006-2007, 2006-2009, and 2006-2011 (the first, second, and third columns, respectively). Each article in one of the three co-citation networks is shown as a separate green dot; the edges (connections between dots) indicates their presence together in the same reference list. (<bold><italic>c</italic></bold>) Cluster algorithm-based content analysis of the 215 papers in the co-citation network of a sample reference article (RA; panel <italic>b</italic>, bottom network series) identified a changing pattern of relevance to different sub-disciplines over time. This RA described the identification of new peptides of possible clinical utility due to their similarity to known conotoxins. Papers in the co-citation network of this RA focused on: (1) &#x03B1;-conotoxin mechanisms of action; (2) structure and evolution of conotoxins; (3) cyclotide biochemistry; (4) conotoxin phylogenetics; and (5) identification and synthesis of lantibiotics. (<bold><italic>d</italic></bold>) Growth of an article&#x2019;s co-citation network is proportional to the number of times it has been cited. Each point is the average network size of 1000 randomly chosen papers with between 1 and 100 citations (error bars represent the standard error of the mean). Each paper is only counted once, even if it is co-cited with the article of interest multiple times. An average of 17.8 new papers is added to the co-citation network for each additional citation. This suggests substantial duplication of articles within a co-citation network, since on average 32.4 papers (median of 30) are referenced in each citing article.</p></caption>
<graphic xlink:href="029629_fig1.tif"/>
</fig>
<p>All three types of networks would be expected to accurately reflect the interdisciplinary nature of modern biomedical research and the expert opinion of publishing scientists, who are themselves the best judges of what constitutes a field. Unlike cited networks, citing and co-citation networks can grow over time, allowing for the dynamic evaluation of an article&#x2019;s influence; they can also indicate whether or not an article gains relevance to additional disciplines (<bold><xref ref-type="fig" rid="fig1">Figure 1b, c</xref></bold>). An important difference between citing and co-citation networks, however, is size. Papers in the biomedical sciences have a median of 30 articles in their reference list in this dataset, so each citation event can be expected to add multiple papers to an article&#x2019;s co-citation network (<bold><xref ref-type="fig" rid="fig1">Figure 1d</xref></bold>), but only one to its citing network. The latter are therefore highly vulnerable to finite number effects; in other words, for an article of interest with few citations, small changes in the citing network would have a disproportionate effect on how that article&#x2019;s field was defined. We therefore chose to pursue co-citation networks as a way to describe an individual paper&#x2019;s field.</p></sec>
<sec id="s2b"><title>Calculating the Relative Citation Ratio</title>
<p>Having chosen our comparison group, the next step was to decide how to calculate the values that numerically represent the co-citation network of each RA. The most obvious choice, averaging the citation rates of articles in the co-citation network, would also be highly vulnerable to finite number effects. We therefore chose to average the citation rates of the journals represented by the collection of articles in each co-citation network. If a journal was represented twice, its journal citation rate (JCR) was added twice when calculating the average JCR. For reasons of algorithmic parsimony we used the JCRs for the year each article in the co-citation network was published; a different choice at this step would be expected to have little if any effect, since almost all JCRs are quite stable over time (<bold>Supplemental <xref ref-type="fig" rid="figS1">Figure 1</xref>; Supplemental <xref ref-type="table" rid="tblS1">Table 1</xref></bold>). Since a co-citation network can be reasonably thought to correspond with an RA&#x2019;s area of science, the average of all JCRs in a given network can be redefined as that RA&#x2019;s field citation rate (FCR).</p>
<p>Using this method <bold>(<xref ref-type="fig" rid="fig2">Figure 2a-c</xref>; Supplemental <xref ref-type="fig" rid="figS2">Figure 2</xref>; Supplemental Equations 1 and 2</bold>), we calculated FCRs for 35,837 papers published in 2009 by NIH grant recipients, specifically those who received R01 awards, the standard mechanism used by NIH to fund investigator-initiated research. We also calculated what the FCR would be if it were instead based on citing or cited networks. It is generally accepted that, whereas practitioners in the same field exhibit at least some variation in citation behavior, much broader variation exists among authors in different fields. The more closely a method of field definition approaches maximal separation of between-field and within-field citation behaviors, the lower its expected variance in citations per year (CPY). FCRs based on co-citation networks exhibited lower variance than those based on cited or citing networks (<xref ref-type="table" rid="tbl1">Table 1</xref>). Interestingly, a larger analysis of the 88,835 papers published by investigators with continuous R01 funding between 2003 and 2010 shows that FCRs also display less variance than either ACRs (<italic>p</italic> &#x003C; 10<sup>&#x2212;4</sup>, F-test for unequal variance) or JIFs (<italic>p</italic> &#x003C; 10<sup>&#x2212;4</sup>, F-test for unequal variance, <bold><xref ref-type="fig" rid="fig2">Figure 2d</xref>, <xref ref-type="table" rid="tbl1">Table 1</xref></bold>), confirming that co-citation networks are better at defining an article&#x2019;s field than its journal of publication.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><p>Variance of Field Citation Rates and Expected Citation Rates using different levels of the citation network for calculations (based on 35,837 R01-funded papers published in 2009).</p></caption>
<graphic xlink:href="029629_tbl1.tif"/>
</table-wrap>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Algorithm for calculating the Relative Citation Ratio. (<bold><italic>a</italic></bold>) Article Citation Rate (ACR) is calculated as the total citations divided by the number of years excluding the calendar year of publication (<bold>Supplemental Equation 1</bold>), when few, if any, citations accrue (<bold>Supplemental <xref ref-type="fig" rid="figS2">Figure 2</xref></bold>). (<bold><italic>b</italic></bold>) Generate an expectation for article citation rates based on a preselected benchmark group, by regressing the ACR of the benchmark papers onto their FCRs (<bold>Supplemental Equations 3, 4</bold>), one regression each publication year. The graphed examples were sampled from a random distribution for illustrative purposes. (<bold><italic>c</italic></bold>) The coefficients from each year&#x2019;s regression equation transforms the Field Citation Rates of papers published in the same year into Expected Citation Rates (<bold>Supplemental Equation 5</bold>). Each paper&#x2019;s RCR is its ACR/ECR ratio. A portfolio&#x2019;s RCR is simply the average of the individual articles&#x2019; RCRs (<bold>Supplemental Equation 6</bold>). (<bold><italic>d</italic></bold>) Box-and whisker plots of 88,835 NIH-funded papers (published between 2003 and 2010), summarizing their Article Citation Rate, Journal Impact Factor (matched to the article&#x2019;s year of publication), and Field Citation Rate. Boxes show the 25th-75th percentiles with a line at the median; whiskers extend to the 10th and 90th percentiles.</p></caption>
<graphic xlink:href="029629_fig2.tif"/>
</fig>
<p>Having established the co-citation network as a means of determining an FCR for each RA, our next step was to calculate ACR/FCR ratios. Since both ACR and FCR are measured in CPY, this generates a rateless, timeless metric that can be used to assess the relative influence of any two RAs. However, it does not measure these values against any broader context. For example, if two RAs have ACR/FCR ratios of 0.7 and 2.1, this represents a three-fold difference in influence, but it is unclear which of those values would be closer to the overall mean or median for a large collection of papers. One additional step is therefore needed to adjust the raw ACR/FCR ratios so that, for any given FCR, the mean RCR equals 1.0. Any selected cohort of RAs can be used as a standard for anchoring expectations, i.e. as a customized benchmark (Supplemental Equations 3-6). We selected R01-funded papers as our benchmark set; for any given year, regression of the ACR and FCR values of R01-funded papers yields the equation describing, for the FCR of a given RA published in that year, the expected citation rate (<bold><xref ref-type="fig" rid="fig2">Figure 2b</xref> and Supplemental <xref ref-type="table" rid="tblS2">Table 2</xref></bold>). Inserting the ACR as the numerator and FCR of that RA into the regression equation as the denominator is the final step in calculating its RCR value, which incorporates the normalization both to its field of research, and to the citation performance of its peers <bold>(<xref ref-type="fig" rid="fig2">Figure 2b, c</xref> and Supplemental Information</bold>).</p>
<p>For analyses where it is important that article RCRs sum to the number of papers for accounting purposes, ordinary least squares (OLS) linear regression of ACR on FCR will benchmark articles such that the mean RCR is equal to 1.0. However, the median article RCR will be lower if there is a skewed distribution. For comparison to the &#x201C;average&#x201D; article, quantile regression will yield a median RCR equal to 1.0. OLS regression benchmarking may be more suitable for large-scale analyses conducted by universities or funding agencies, while the quantile regression benchmarking approach might be more suitable for web tools enabling search and exploration at the article or investigator level. In the following analyses, we used OLS regression such that the mean RCR for benchmark articles is equal to 1.0.</p></sec>
<sec id="s2c"><title>Expert validation of RCR as a measure of influence</title>
<p>For the work presented here, we chose as a benchmark the full set of 311,497 RAs published from 2002 through 2012 by NIH-R01 awardees. To measure the degree of correspondence between our method and expert opinion, we compared RCRs generated by benchmarking ACR/FCR values against this standard to three independent sets of post-publication evaluations by subject matter experts (details in <bold>Supplemental Information</bold>). We compared RCR with expert rankings for 2193 articles published in 2009 and evaluated by Faculty of 1000 members (<bold><xref ref-type="fig" rid="fig3">Figure 3a</xref></bold>), as well as rankings of 430 Howard Hughes Medical Institute- or NIH-funded articles published between 2005 and 2011 and evaluated in a study conducted by the Science and Technology Policy Institute (STPI, <bold><xref ref-type="fig" rid="fig3">Figure 3b</xref></bold>), and finally, 290 articles published in 2009 by extramurally funded NIH investigators and evaluated by NIH intramural investigators in a study of our own design (<bold><xref ref-type="fig" rid="fig3">Figure 3c</xref>; Supplemental <xref ref-type="fig" rid="figS5">Figs. 5</xref>-<xref ref-type="fig" rid="figS7">7</xref></bold>). All three approaches demonstrate that RCR values are well correlated with reviewers&#x2019; judgments. We asked experts in the latter study to provide, in addition to an overall score, scores for several independent sub-criteria: likely impact of the research, importance of the question being addressed, robustness of the study, appropriateness of the methods, and human health relevance. Random forest analysis indicated that their scores for likely impact were weighted most heavily in determining their overall evaluation (<bold>Supplemental <xref ref-type="fig" rid="figS6">Figure 6</xref></bold>).</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption><p>Relative Citation Ratios correspond with expert reviewer scores. (<bold><italic>a-c</italic></bold>) Bubble plots of reviewer scores vs. RCR for three different datasets. Articles are binned by reviewer score; bubble area is proportionate to the number of articles in that bin. (<bold><italic>a</italic></bold>) F1000 scores for 2193 R01-funded papers published in 2009. Faculty reviewers rated the articles on a scale of one to three (&#x201C;Good&#x201D;, &#x201C;Very Good&#x201D;, and &#x201C;Exceptional&#x201D;, respectively); those scores were summed into a composite F1000 score for each article (Supplemental <xref ref-type="fig" rid="figS3">Figure 3</xref>). (<bold><italic>b</italic></bold>) Reviewer scores of 430 HHMI and NIH-funded papers collected by the Science and Technology Policy Institute. (<bold><italic>c</italic></bold>) Scores of 290 R01-funded articles reviewed by experts from the NIH Intramural Research Program. Black line, linear regression.</p></caption>
<graphic xlink:href="029629_fig3.tif"/>
</fig>
<p>In addition to correlating with expert opinion, RCR is ranking invariant, which is considered to be a desirable property of bibliometric indicators (<xref ref-type="bibr" rid="c37">Rousseau and Leydesdorff, 2011</xref>; <xref ref-type="bibr" rid="c19">Gl&#x00E4;nzel and Moed, 2012</xref>). In short, an indicator is ranking invariant when it is used to place two groups of articles in hierarchical order, and the relative positions in that order do not change when uncited articles are added to each group. The RCR metric is ranking invariant when the same number of uncited articles is added to two groups of equal size (<bold>Supplemental Equations 7-9</bold>). RCR is also ranking invariant when the same proportion of uncited articles is added to two groups of unequal size (<bold>Supplemental Equations 10-11</bold>). This demonstrates that the RCR method can be used effectively and safely in evaluating the relative influence of large groups of publications.</p></sec>
<sec id="s2d"><title>Quantifying how past influence predicts future performance</title>
<p>We next undertook a large case study of all 88,835 articles published by NIH investigators who maintained continuous R01 funding from fiscal year (FY) 2003 through FY2010 to ask how the RCR of publications from individual investigators changed over this eight year interval. Each of these investigators had succeeded at least once in renewing one or more of their projects through the NIH competitive peer review process. In aggregate, the RCR values for these articles are well-matched to a log-normal distribution; in contrast, as noted previously by others, the distribution of impact factors of the journals in which they were published is non-normal (<xref ref-type="bibr" rid="c26">Mansilla et al., 2007</xref>; <xref ref-type="bibr" rid="c15">Egghe, 2009</xref>) (<bold><xref ref-type="fig" rid="fig4">Figure 4a, b</xref></bold>). Sorting into quintiles based on JIF demonstrates that, though journals with the highest impact factors have the highest median RCR, influential publications can be found in virtually all journals (<bold><xref ref-type="fig" rid="fig4">Figure 4c, d</xref></bold>). Focusing on a dozen representative journals with a wide range of JIFs further substantiates the finding that influential science appears in many venues, and reveals noteworthy departures from the correlation between JIF and median RCR (see Supplemental Information). For example, NIH-funded articles in both <italic>Organic Letters</italic> (JIF &#x003D; 4.7) and the <italic>Journal of the Acoustical Society of America</italic> (JIF &#x003D; 1.6) have a higher median RCR than those in <italic>Nucleic Acids Research</italic> (JIF &#x003D; 7.1; <bold><xref ref-type="fig" rid="fig4">Figure 4e</xref></bold>).</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption><p>Properties of Relative Citation Ratios at the article and investigator level. (<bold><italic>a, b</italic></bold>) Frequency distribution of article-level RCRs (<bold><italic>a</italic></bold>) and Journal Impact Factors (<bold><italic>b</italic></bold>), from 88,835 papers (authored by 3089 R01-funded PIs) for which co-citation networks were generated. Article RCRs are well-fit by a log-normal distribution (R<sup>2</sup> &#x003D; 0.99), and Journal Impact Factors less so (R<sup>2</sup> &#x003D; 0.79). (<bold><italic>c</italic></bold>) Box-and-whisker plots summarizing Journal Impact Factors for the same papers, binned by Impact Factor quintile (line, median; box, 25th&#x2013;75th percentiles; whiskers, 10th to 90th percentiles). (<bold><italic>d</italic></bold>) RCR for the same papers using the same bins by Journal Impact Factor quintile (same scale as <italic>c</italic>). Although the median RCR for each bin generally corresponds to the Impact Factor quintile, there is a wide range of article RCRs in each category. (<bold><italic>e</italic></bold>) Box-and-whisker plots summarizing RCRs of these same papers published in selected journals. In each journal, there are papers with article RCRs surpassing the median RCR of the highest Impact Factor journals (left three). The Impact Factor of each journal is shown above. (<bold><italic>f, g</italic></bold>) Frequency distribution of investigator-level RCRs (<bold><italic>f</italic></bold>) and Journal Impact Factors (<bold><italic>g</italic></bold>), representing the mean values for papers authored by each of 3089 R01-funded PIs. Dashed line in (<bold><italic>f</italic></bold>), mode of RCR for PIs.</p></caption>
<graphic xlink:href="029629_fig4.tif"/>
</fig>
<p>As part of this case study we also calculated the average RCR and average JIF for papers published by each of the 3089 NIH R01 principal investigators (PIs) represented in the dataset of 88,835 articles. In aggregate, the average RCR and JIF values for NIH R01 PIs exhibited log-normal distributions (<bold><xref ref-type="fig" rid="fig4">Figure 4f, g</xref></bold>) with substantially different hierarchical ordering (<bold>Supplemental <xref ref-type="fig" rid="figS8">Figure 8</xref></bold>). This raised a further question concerning PIs with RCR values near the mode of the log-normal distribution (dashed line in <bold><xref ref-type="fig" rid="fig4">Figure 4f</xref></bold>): as measured by the ability to publish work that influences their respective fields, to what extent does their performance fluctuate? We addressed this question by dividing the eight year window (FY2003 through FY2010) in half. Average RCRs in the first time period (FY2003 through FY2006) were sorted into quintiles, and the percentage of PIs in the second time period (FY2007 through FY2010) that remained in the same quintile, or moved to a higher or lower quintile, was calculated. The position of PIs in these quintiles proved to be relatively immobile; 53&#x0025; of PIs in the top quintile remained at the top, and 53&#x0025; of those in the bottom quintile remained at the bottom (<bold><xref ref-type="fig" rid="fig5">Figure 5a</xref></bold>). For each PI we also calculated a weighted RCR (the number of articles multiplied by their average RCR); comparing on this basis yielded almost identical results (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). It is worth noting that average FCRs for investigators were extremely stable from one 4-year period to the next (Pearson <italic>r</italic> &#x003D; 0.92, <bold><xref ref-type="table" rid="tbl2">Table 2</xref></bold>), Since FCRs are the quantitative representation of co-citation networks, this further suggests that each co-citation network is successfully capturing the corresponding investigator&#x2019;s field of research.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption><p>Scientific mobility of investigators&#x2019; influence relative to their field. Color intensity is proportional to the percentage of PIs in each quintile. (<bold><italic>a</italic></bold>) 3089 investigators who were continuously funded by at least one R01 were ranked by their articles&#x2019; average RCR in each time window, and split into quintiles. From left to right, investigators starting in different quintiles were tracked to see their rank in the next 4-year period. (<bold><italic>b</italic></bold>) The same analysis, but the number of published articles was multiplied by their average RCR to calculate an influence-weighted article count. PIs were ranked by this aggregate score and split into quintiles.</p></caption>
<graphic xlink:href="029629_fig5.tif"/>
</fig>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><p>Summary of investigator-level bibliometric measures and their stability from one 4-year period to the next (PIs with 5 or more articles in each period, except for article count).</p></caption>
<graphic xlink:href="029629_tbl2.tif"/>
</table-wrap></sec>
</sec>
<sec id="s3"><title>Discussion</title>
<p>The relationship between scientists and JIFs has been likened to the prisoner&#x2019;s dilemma from game theory: because grant reviewers use JIFs in their evaluations, investigators must continue to weigh this in their decision-making or risk being out-competed by their peers on this basis (<xref ref-type="bibr" rid="c10">Casadevall and Fang, 2014</xref>; <xref ref-type="bibr" rid="c40">Shaw, 2014</xref>). A groundswell of support for the San Francisco Declaration on Research Assessment (<ext-link ext-link-type="uri" xlink:href="http://www.ascb.org/dora">http://www.ascb.org/dora</ext-link>) has not yet been sufficient to break this cycle (<xref ref-type="bibr" rid="c1">Alberts, 2013</xref>; <xref ref-type="bibr" rid="c6">Bertuzzi and Drubin, 2013</xref>; <xref ref-type="bibr" rid="c38">Schekman and Patterson, 2013</xref>; <xref ref-type="bibr" rid="c44">Suhrbier and Poland, 2013</xref>; <xref ref-type="bibr" rid="c10">Casadevall and Fang, 2014</xref>; <xref ref-type="bibr" rid="c40">Shaw, 2014</xref>). Continued use of the Journal Impact Factor as an evaluation metric will fail to credit researchers for publishing highly influential work. Articles in high-profile journals have average RCRs of approximately 3. However, high-Impact-Factor journals (JIF &#x2265; 28) only account for 11&#x0025; of papers that have an RCR of 3 or above. Using Impact Factors to credit influential work means overlooking 89&#x0025; of similarly influential papers published in less prestigious venues.</p>
<p>Bibliometrics like JIF and H-index are attractive because citations are affirmations of the spread of knowledge amongst publishing scientists, and are important indicators of the influence of a particular set of ideas. These and other prior attempts to describe a normalized citation metric have resulted in imperfect systems for the comparison of diverse scholarly works (Supplemental Information and Supplemental Figure 17), either because they measure only the average performance of a group of papers (<xref ref-type="bibr" rid="c46">Vinkler, 2003</xref>), or because the article of interest is measured against a control group that includes widely varying areas of science (<xref ref-type="bibr" rid="c48">Waltman et al., 2011a</xref>; <xref ref-type="bibr" rid="c34">Radicchi and Castellano, 2012</xref>; <xref ref-type="bibr" rid="c25">Leydesdorff and Bornmann, 2015</xref>). An example of the latter is citation percentiling, which the Leiden manifesto (<xref ref-type="bibr" rid="c20">Hicks et al., 2015</xref>) recently recommended as best practice in bibliometrics. The RCR method is an improvement over the use of citation percentiling alone, since masking the skewed distribution of citations and article influence, while statistically convenient, can disadvantage portfolios of high-risk, high-reward research that would be expected to have a small proportion of highly influential articles (<xref ref-type="bibr" rid="c36">Rand and Pfeiffer, 2009</xref>).</p>
<p>Though tracking the productivity of individual scientists with bibliometrics has been controversial, it is difficult to contradict the assertion that uncited articles (RCR &#x003D; 0) have little if any influence on their respective fields, or that the best-cited articles (RCR &#x003E; 20) are impressively influential. We have not determined whether smaller differences, for example those with average or slightly above-average RCRs (e.g. 1.0 versus 1.2), reliably reflect differential levels of influence. Further, citation-based metrics can never fully capture all of the relevant information about an article, such as the underlying value of a study or the importance of making progress in solving the problem being addressed. The RCR metric is also not designed to be an indicator of long-term impact, and citation metrics are not appropriate for applied research that is intended to target a narrow audience of non-academic engineers or clinicians. However, as citation rates mark the breadth and speed of the diffusion of knowledge among publishing scholars, these quantitative metrics can effectively supplement subject matter expertise in the evaluation of research groups seeking to make new discoveries and widely disseminate their findings.</p>
<p>Bibliometric methods also have the potential to track patterns of scientific productivity over time, which may help answer important questions about how science progresses. In particular, co-citation networks can be used to characterize the relationship between scientific topics (including interdisciplinarity), emerging areas, and social interactions. For example, is the membership of an influential group of investigators in a given field or group of fields stable over time, or is it dynamic, and why? Our data demonstrate the existence of an established hierarchy of influence within the exclusive cohort of NIH R01 recipients who remained continuously funded over an eight-year time frame. This may mean that investigators tend to ask and answer questions of similar interest to their fields. Additionally or alternatively, stable differences in investigators&#x2019; status, such as scientific pedigree, institutional resources, and/or peer networks, may be significant drivers of persistently higher or lower RCR values. Future statistical analyses may therefore reveal parameters that contribute to scholarly influence. To the extent that scientific (im)mobility is a product of uneven opportunities afforded to investigators, there may be practical ways in which funding agencies can make policy changes that increase mobility and seed breakthroughs more widely.</p>
<p>There is increasing interest from the public in the outcomes of research. It is therefore becoming necessary to demonstrate outcomes at all levels of funding bodies&#x2019; research portfolios, beyond the reporting of success stories that can be quickly and succinctly communicated. For this reason, quantitative metrics are likely to become more prominent in research evaluation, especially in large-scale program and policy evaluations. Questions about how to advance science most effectively within the constraints of limited funding require that we apply scientific approaches to determine how science is funded (<xref ref-type="bibr" rid="c14">Danthi et al., 2014</xref>; <xref ref-type="bibr" rid="c24">Kaltman et al., 2014</xref>; <xref ref-type="bibr" rid="c27">Mervis, 2014</xref>). Since quantitative analysis will likely play an increasingly prominent role going forward, it is critical that the scientific community accept only approaches and metrics that are demonstrably valid, vetted, and transparent, and insist on their use only in a broader context that includes interpretation by subject matter experts.</p>
<p>Recent work has improved our theoretical understanding of citation dynamics (<xref ref-type="bibr" rid="c35">Radicchi et al., 2008</xref>; <xref ref-type="bibr" rid="c43">Stringer et al., 2010</xref>; <xref ref-type="bibr" rid="c50">Wang et al., 2013</xref>). However, citation counts are not the primary interest of funding bodies, but rather progress in solving scientific challenges. The NIH particularly values work that ultimately culminates in advances to human health, a process that has historically taken decades (<xref ref-type="bibr" rid="c11">Contopoulos-Ioannidis et al., 2008</xref>). Here, too, metrics have facilitated quantitation of the diffusion of knowledge from basic research toward human health studies, by examining the <italic>type</italic> rather than the <italic>count</italic> of citing articles (<xref ref-type="bibr" rid="c51">Weber, 2013</xref>). Insights into how to accelerate this process will probably come from quantitative analysis. To credit the impact of research that may currently be underappreciated, comprehensive evaluation of funding outputs will need to incorporate metrics that can capture many other outputs, outcomes, and impact, such as the value of innovation, clinical outcomes, new software, patents, and economic activity. As such, the metric described here should not be viewed as a tool to be used as a primary criterion in funding decisions, but as one of several metrics that can provide assistance to decision-makers at funding agencies or in other situations in which quantitation can be used judiciously to supplement, not substitute for, expert opinion.</p>
</sec>
<sec id="s4"><title>Materials and Methods</title>
<sec id="s4a"><title>Citation data</title>
<p>The Thomson Reuters Web of Science citation dataset from 2002-2012 was used for all citation analyses. Because of our primary interest in biomedical research, we limited our analysis to those journals in which NIH R01-funded researchers published during this time. For assigning a journal citation rate to a published article, we used the 2-year synchronous journal citation rate (<xref ref-type="bibr" rid="c17">Garfield, 1972</xref>; <xref ref-type="bibr" rid="c37">Rousseau and Leydesdorff, 2011</xref>) for its journal in the year of its publication. Publications from the final year of our dataset (2012) were not included in analyses because they did not have time to accrue enough citations from which to draw meaningful conclusions, but references from these papers to earlier ones were included in citation counts.</p></sec>
<sec id="s4b"><title>Grant and Principal Investigator data</title>
<p>Grant data was downloaded from the NIH RePORTER database. Grant-to-publication linkages were first derived from the NIH SPIRES database, and the data were cleaned to address false-positives and - negatives. Grant and publication linkages to Principal Investigators were established using Person Profile IDs from the NIH IMPAC-II database. To generate a list of continuously funded investigators, only those Person Profile IDs with active R01 support in each of Fiscal Years 2003-2010 were included.</p></sec>
<sec id="s4c"><title>Calculations and data visualization</title>
<p>Co-citation networks were generated in Python (Python Software Foundation, Beaverton, OR). This was accomplished on a paper-by-paper basis by assembling the list of articles citing the article of interest, and then assembling a list of each paper that those cited. This list of co-cited papers was de-duplicated at this point. Example code for generating co-citation networks and calculating Field Citation Rates is available on GitHub (<ext-link ext-link-type="uri" xlink:href="http://github.com/NIHOPA">http://github.com/NIHOPA</ext-link>). Further calculations were handled in R (R Foundation for Statistical Computing, Vienna, Austria). Visualizations were generated in Prism 6 (GraphPad, La Jolla, CA), SigmaPlot (Systat Software, San Jose, CA), or Excel 2010 (Microsoft, Redmond, WA).</p>
<p>When comparing citations rates to other metrics (e.g. post-publication review scores), citation rates were log-transformed due to their highly skewed distribution, unless these other scores were similarly skewed (i.e. Faculty of 1000 review scores). For this process, article RCRs of zero were converted to the first power of 10 lower than the lowest positive number in the dataset (generally 10<sup>&#x2212;2</sup>). In the analysis of Principal Investigator RCRs, no investigators had an average RCR of zero.</p></sec>
</sec>
</body>
<back>
<ack><title>Acknowledgements</title>
<p>We thank Francis Collins, Larry Tabak, Kristine Willis, Mike Lauer, Jon Lorsch, Stefano Bertuzzi, Steve Leicht, Stefan Maas, Riq Parra, Dashun Wang, Patricia Forcinito, Carole Christian, Adam Apostoli, Aviva Litovitz and Paula Fearon for their thoughtful comments on the manuscript, Michael Gottesman for help organizing post-publication peer review, and Jason Palmer, Fai Chan, Rob Harriman, Kirk Baker and Kevin Small for help with data processing and software development for <italic>iCite.</italic></p>
</ack>
<ref-list><title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Alberts</surname> <given-names>B</given-names></string-name> (<year>2013</year>) <article-title>Impact factor distortions</article-title>. <source>Science</source> <volume>340</volume>:<fpage>787</fpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencemag.org/content/340/6134/787.full">http://www.sciencemag.org/content/340/6134/787.full</ext-link></mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><person-group person-group-type="author"><anonymous>Anon</anonymous></person-group> (<year>2005</year>) <article-title>Not-so-deep impact</article-title>. <source>Nature</source> <volume>435</volume>:<fpage>1003</fpage>&#x2013;<lpage>1004</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/4351003b">http://dx.doi.org/10.1038/4351003b</ext-link></mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><anonymous>Anon</anonymous></person-group> (<year>2013</year>) <article-title>The maze of impact metrics</article-title>. <source>Nature</source> <volume>502</volume>:<fpage>271</fpage>&#x2013;<lpage>271</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.nature.com/news/the-maze-of-impact-metrics-1.13952">http://www.nature.com/news/the-maze-of-impact-metrics-1.13952</ext-link></mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Bergstrom</surname> <given-names>CT</given-names></string-name>, <string-name><surname>West</surname> <given-names>JD</given-names></string-name> (<year>2008</year>) <article-title>Assessing citations with the Eigenfactor metrics</article-title>. <source>Neurology</source> <volume>71</volume>:<fpage>1850</fpage>&#x2013;<lpage>1851</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/19047558">http://www.ncbi.nlm.nih.gov/pubmed/19047558</ext-link></mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Bergstrom</surname> <given-names>CT</given-names></string-name>, <string-name><surname>West</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Wiseman</surname> <given-names>MA</given-names></string-name> (<year>2008</year>) <article-title>The Eigenfactor metrics</article-title>. <source>J Neurosci</source> <volume>28</volume>:<fpage>11433</fpage>&#x2013;<lpage>11434</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/18987179">http://www.ncbi.nlm.nih.gov/pubmed/18987179</ext-link></mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Bertuzzi</surname> <given-names>S</given-names></string-name>, <string-name><surname>Drubin</surname> <given-names>DG</given-names></string-name> (<year>2013</year>) <article-title>No shortcuts for research assessment</article-title>. <source>Mol Biol Cell</source> <volume>24</volume>:<fpage>1505</fpage>&#x2013;<lpage>1506</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.molbiolcell.org/content/24/10/1505.long">http://www.molbiolcell.org/content/24/10/1505.long</ext-link></mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Bollen</surname> <given-names>J</given-names></string-name>, <string-name><surname>Van de Sompel</surname> <given-names>H</given-names></string-name>, <string-name><surname>Hagberg</surname> <given-names>A</given-names></string-name>, <string-name><surname>Chute</surname> <given-names>R</given-names></string-name> (<year>2009</year>) <article-title>A principal component analysis of 39 scientific impact measures</article-title>. <source>PLoS One</source> <volume>4</volume>:<fpage>e6022</fpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosone/article?id&#x003D;10.1371/journal.pone.0006022">http://journals.plos.org/plosone/article?id&#x003D;10.1371/journal.pone.0006022</ext-link></mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Bornmann</surname> <given-names>L</given-names></string-name>, <string-name><surname>Leydesdorff</surname> <given-names>L</given-names></string-name> (<year>2013</year>) <article-title>The validation of (advanced) bibliometric indicators through peer assessments: A comparative study using data from InCites and F1000</article-title>. <source>J Informetr</source> <volume>7</volume>:<fpage>286</fpage>&#x2013;<lpage>291</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S175115771200106X">http://www.sciencedirect.com/science/article/pii/S175115771200106X</ext-link></mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Bornmann</surname> <given-names>L</given-names></string-name>, <string-name><surname>Marx</surname> <given-names>W</given-names></string-name> (<year>2013</year>) <article-title>How to evaluate individual researchers working in the natural and life sciences meaningfully</article-title>? <source>A proposal of methods based on percentiles of citations. Scientometrics</source> <volume>98</volume>:<fpage>487</fpage>&#x2013;<lpage>509</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.1007/s11192-013-1161-y">http://link.springer.com/10.1007/s11192-013-1161-y</ext-link></mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Casadevall</surname> <given-names>A</given-names></string-name>, <string-name><surname>Fang</surname> <given-names>FC</given-names></string-name> (<year>2014</year>) <article-title>Causes for the persistence of impact factor mania</article-title>. <source>MBio</source> <volume>5</volume>:<fpage>e00064</fpage>&#x2013;<lpage>14</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid&#x003D;3967521&#x0026;tool&#x003D;pmcentrez&#x0026;rendertype&#x003D;abstract">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid&#x003D;3967521&#x0026;tool&#x003D;pmcentrez&#x0026;rendertype&#x003D;abstract</ext-link></mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Contopoulos-Ioannidis</surname> <given-names>DG</given-names></string-name>, <string-name><surname>Alexiou</surname> <given-names>GA</given-names></string-name>, <string-name><surname>Gouvias</surname> <given-names>TC</given-names></string-name>, <string-name><surname>Ioannidis</surname> <given-names>JPA</given-names></string-name> (<year>2008</year>) <article-title>Medicine. Life cycle of translational research for medical interventions</article-title>. <source>Science</source> <volume>321</volume>:<fpage>1298</fpage>&#x2013;<lpage>1299</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencemag.org/content/321/5894/1298">http://www.sciencemag.org/content/321/5894/1298</ext-link></mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Couzin-Frankel</surname> <given-names>J</given-names></string-name> (<year>2013</year>) <article-title>Shaking up science</article-title>. <source>Science</source> <volume>339</volume>:<fpage>386</fpage>&#x2013;<lpage>389</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencemag.org/content/339/6118/386.short">http://www.sciencemag.org/content/339/6118/386.short</ext-link></mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Crous</surname> <given-names>CJ</given-names></string-name> (<year>2014</year>) <article-title>Judge research impact on a local scale</article-title>. <source>Nature</source> <volume>513</volume>:<fpage>7</fpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/25186869">http://www.ncbi.nlm.nih.gov/pubmed/25186869</ext-link></mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Danthi</surname> <given-names>N</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>CO</given-names></string-name>, <string-name><surname>Shi</surname> <given-names>P</given-names></string-name>, <string-name><surname>Lauer</surname> <given-names>M</given-names></string-name> (<year>2014</year>) <article-title>Percentile ranking and citation impact of a large cohort of National Heart, Lung, and Blood Institute-funded cardiovascular R01 grants</article-title>. <source>Circ Res</source> <volume>114</volume>:<fpage>600</fpage>&#x2013;<lpage>606</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/24406983">http://www.ncbi.nlm.nih.gov/pubmed/24406983</ext-link></mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Egghe</surname> <given-names>L</given-names></string-name> (<year>2009</year>) <article-title>Mathematical derivation of the impact factor distribution</article-title>. <source>J Informetr</source> <volume>3</volume>:<fpage>290</fpage>&#x2013;<lpage>295</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S175115770900011X">http://www.sciencedirect.com/science/article/pii/S175115770900011X</ext-link></mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Eom</surname> <given-names>Y-H</given-names></string-name>, <string-name><surname>Fortunato</surname> <given-names>S</given-names></string-name> (<year>2011</year>) <article-title>Characterizing and modeling citation dynamics</article-title>. <source>PLoS One</source> <volume>6</volume>:<fpage>e24926</fpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosone/article?id&#x003D;10.1371/journal.pone.0024926">http://journals.plos.org/plosone/article?id&#x003D;10.1371/journal.pone.0024926</ext-link></mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Garfield</surname> <given-names>E</given-names></string-name> (<year>1972</year>) <article-title>Citation analysis as a tool in journal evaluation</article-title>. <source>Science</source> <volume>178</volume>:<fpage>471</fpage>&#x2013;<lpage>479</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/5079701">http://www.ncbi.nlm.nih.gov/pubmed/5079701</ext-link></mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Garfield</surname> <given-names>E</given-names></string-name> (<year>2006</year>) <article-title>The history and meaning of the journal impact factor</article-title>. <source>JAMA</source> <volume>295</volume>:<fpage>90</fpage>&#x2013;<lpage>93</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://jama.jamanetwork.com/article.aspx?articleid&#x003D;202114">http://jama.jamanetwork.com/article.aspx?articleid&#x003D;202114</ext-link></mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Glanzel</surname> <given-names>W</given-names></string-name>, <string-name><surname>Moed</surname> <given-names>HF</given-names></string-name> (<year>2012</year>) <article-title>Opinion paper: thoughts and facts on bibliometric indicators</article-title>. <source>Scientometrics</source> <volume>96</volume>:<fpage>381</fpage>&#x2013;<lpage>394</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.1007/s11192-012-0898-z">http://link.springer.com/10.1007/s11192-012-0898-z</ext-link></mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Hicks</surname> <given-names>D</given-names></string-name>, <string-name><surname>Wouters</surname> <given-names>P</given-names></string-name>, <string-name><surname>Waltman</surname> <given-names>L</given-names></string-name>, <string-name><surname>de Rijcke</surname> <given-names>S</given-names></string-name>, <string-name><surname>Rafols</surname> <given-names>I</given-names></string-name> (<year>2015</year>) <article-title>Bibliometrics: The Leiden Manifesto for research metrics</article-title>. <source>Nature</source> <volume>520</volume>:<fpage>429</fpage>&#x2013;<lpage>431</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.nature.com/news/bibliometrics-the-leiden-manifesto-for-research-metrics-1.17351">http://www.nature.com/news/bibliometrics-the-leiden-manifesto-for-research-metrics-1.17351</ext-link></mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Hirsch</surname> <given-names>JE</given-names></string-name> (<year>2005</year>) <article-title>An index to quantify an individual&#x2019;s scientific research output</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>102</volume>:<fpage>16569</fpage>&#x2013;<lpage>16572</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/content/102/46/16569.full.pdf">http://www.pnas.org/content/102/46/16569.full.pdf</ext-link></mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Jeong</surname> <given-names>H</given-names></string-name>, <string-name><surname>N&#x00E9;da</surname> <given-names>Z</given-names></string-name>, <string-name><surname>Barab&#x00E1;si</surname> <given-names>AL</given-names></string-name> (<year>2003</year>) <article-title>Measuring preferential attachment in evolving networks</article-title>. <source>Europhys Lett</source> <volume>61</volume>:<fpage>567</fpage>&#x2013;<lpage>572</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://stacks.iop.org/0295-5075/61/i&#x003D;4/a&#x003D;567">http://stacks.iop.org/0295-5075/61/i&#x003D;4/a&#x003D;567</ext-link> [Accessed <date-in-citation content-type="access-date">June 23, 2015</date-in-citation>].</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Johnston</surname> <given-names>M</given-names></string-name> (<year>2013</year>) <article-title>We have met the enemy, and it is us</article-title>. <source>Genetics</source> <volume>194</volume>:<fpage>791</fpage>&#x2013;<lpage>792</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.genetics.org/content/194/4Z791.long">http://www.genetics.org/content/194/4Z791.long</ext-link></mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Kaltman</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Evans</surname> <given-names>FJ</given-names></string-name>, <string-name><surname>Danthi</surname> <given-names>NS</given-names></string-name>, <string-name><surname>Wu</surname> <given-names>CO</given-names></string-name>, <string-name><surname>DiMichele</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Lauer</surname> <given-names>MS</given-names></string-name> (<year>2014</year>) <article-title>Prior publication productivity, grant percentile ranking, and topic-normalized citation impact of NHLBI cardiovascular R01 grants</article-title>. <source>Circ Res</source> <volume>115</volume>:<fpage>617</fpage>&#x2013;<lpage>624</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/25214575">http://www.ncbi.nlm.nih.gov/pubmed/25214575</ext-link></mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="other"><string-name><surname>Leydesdorff</surname> <given-names>L</given-names></string-name>, <string-name><surname>Bornmann</surname> <given-names>L</given-names></string-name> (<year>2015</year>) <article-title>The operationalization of &#x201C;fields&#x201D; as WoS subject categories (WCs) in evaluative bibliometrics: The cases of &#x201C;library and information science&#x201D; and &#x201C;science &#x0026; technology studies.&#x201D;</article-title> <source>J Assoc Inf Sci Technol</source>:n/a&#x2014;n/a Available at: <ext-link ext-link-type="uri" xlink:href="http://doi.wiley.com/10.1002/asi.23408">http://doi.wiley.com/10.1002/asi.23408</ext-link></mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Mansilla</surname> <given-names>R</given-names></string-name>, <string-name><surname>Koppen</surname> <given-names>E</given-names></string-name>, <string-name><surname>Cocho</surname> <given-names>G</given-names></string-name>, <string-name><surname>Miramontes</surname> <given-names>P</given-names></string-name> (<year>2007</year>) <article-title>On the behavior of journal impact factor rank-order distribution</article-title>. <source>J Informetr</source> <volume>1</volume>:<fpage>155</fpage>&#x2013;<lpage>160</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S1751157707000296">http://www.sciencedirect.com/science/article/pii/S1751157707000296</ext-link></mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Mervis</surname> <given-names>J</given-names></string-name> (<year>2014</year>) <article-title>Peering into peer review</article-title>. <source>Science</source> <volume>343</volume>:<fpage>596</fpage>&#x2013;<lpage>598</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencemag.org/content/343/6171/596">http://www.sciencemag.org/content/343/6171/596</ext-link></mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Misteli</surname> <given-names>T</given-names></string-name> (<year>2013</year>) <article-title>Eliminating the impact of the Impact Factor</article-title>. <source>J Cell Biol</source> <volume>201</volume>:<fpage>651</fpage>&#x2013;<lpage>652</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://jcb.rupress.org/content/201/5/651.short">http://jcb.rupress.org/content/201/5/651.short</ext-link></mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Moed</surname> <given-names>HF</given-names></string-name> (<year>2010</year>) <article-title>Measuring contextual citation impact of scientific journals</article-title>. <source>J Informetr</source> <volume>4</volume>:<fpage>265</fpage>&#x2013;<lpage>277</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S1751157710000039">http://www.sciencedirect.com/science/article/pii/S1751157710000039</ext-link></mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Moed</surname> <given-names>HF</given-names></string-name>, <string-name><surname>Burger</surname> <given-names>WJM</given-names></string-name>, <string-name><surname>Frankfort</surname> <given-names>JG</given-names></string-name>, <string-name><surname>Van Raan</surname> <given-names>AFJ</given-names></string-name> (<year>1985</year>) <article-title>The use of bibliometric data for the measurement of university research performance</article-title>. <source>Res Policy</source> <volume>14</volume>:<fpage>131</fpage>&#x2013;<lpage>149</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/0048733385900125">http://www.sciencedirect.com/science/article/pii/0048733385900125</ext-link></mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><surname>Opthof</surname> <given-names>T</given-names></string-name>, <string-name><surname>Leydesdorff</surname> <given-names>L</given-names></string-name> (<year>2010</year>) <article-title>Caveats for the journal and field normalizations in the CWTS (&#x201C;Leiden&#x201D;) evaluations of research performance</article-title>. <source>J Informetr</source> <volume>4</volume>:<fpage>423</fpage>&#x2013;<lpage>430</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S1751157710000106">http://www.sciencedirect.com/science/article/pii/S1751157710000106</ext-link></mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Price</surname> <given-names>DDS</given-names></string-name> (<year>1976</year>) <article-title>A general theory of bibliometric and other cumulative advantage processes</article-title>. <source>J Am Soc Inf Sci</source> <volume>27</volume>:<fpage>292</fpage>&#x2013;<lpage>306</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi&#x003D;10.1.1.161.114">http://citeseerx.ist.psu.edu/viewdoc/summary?doi&#x003D;10.1.1.161.114</ext-link></mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Pulverer</surname> <given-names>B</given-names></string-name> (<year>2013</year>) <article-title>Impact fact-or fiction</article-title>? <source>EMBO J</source> <volume>32</volume>:<fpage>1651</fpage>&#x2013;<lpage>1652</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://emboj.embopress.org/content/32/12/1651.abstract">http://emboj.embopress.org/content/32/12/1651.abstract</ext-link></mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Radicchi</surname> <given-names>F</given-names></string-name>, <string-name><surname>Castellano</surname> <given-names>C</given-names></string-name> (<year>2012</year>) <article-title>Testing the fairness of citation indicators for comparison across scientific domains: The case of fractional citation counts</article-title>. <source>J Informetr</source> <volume>6</volume>:<fpage>121</fpage>&#x2013;<lpage>130</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S1751157711000794">http://www.sciencedirect.com/science/article/pii/S1751157711000794</ext-link></mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Radicchi</surname> <given-names>F</given-names></string-name>, <string-name><surname>Fortunato</surname> <given-names>S</given-names></string-name>, <string-name><surname>Castellano</surname> <given-names>C</given-names></string-name> (<year>2008</year>) <article-title>Universality of citation distributions: toward an objective measure of scientific impact</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>105</volume>:<fpage>17268</fpage>&#x2013;<lpage>17272</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/content/early/2008/10/30/0806977105.abstract">http://www.pnas.org/content/early/2008/10/30/0806977105.abstract</ext-link></mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Rand</surname> <given-names>DG</given-names></string-name>, <string-name><surname>Pfeiffer</surname> <given-names>T</given-names></string-name> (<year>2009</year>) <article-title>Systematic differences in impact across publication tracks at PNAS</article-title>. <source>PLoS One</source> <volume>4</volume>:<fpage>e8092</fpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plosone/article?id&#x003D;10.1371/journal.pone.0008092">http://journals.plos.org/plosone/article?id&#x003D;10.1371/journal.pone.0008092</ext-link></mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="other"><string-name><surname>Rousseau</surname> <given-names>R</given-names></string-name>, <string-name><surname>Leydesdorff</surname> <given-names>L</given-names></string-name> (<year>2011</year>) <article-title>Simple arithmetic versus intuitive understanding:The case of the impact factor</article-title>. <source>ISSI Newsl</source>:<fpage>10</fpage>&#x2013;<lpage>14</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://eprints.rclis.org/15478/1/Impact_factorarithmetic.pdf">http://eprints.rclis.org/15478/1/Impact_factorarithmetic.pdf</ext-link></mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Schekman</surname> <given-names>R</given-names></string-name>, <string-name><surname>Patterson</surname> <given-names>M</given-names></string-name> (<year>2013</year>) <article-title>Reforming research assessment</article-title>. <source>Elife</source> <volume>2</volume>:<fpage>e00855</fpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://elifesciences.org/content/2Ze00855.abstract">http://elifesciences.org/content/2Ze00855.abstract</ext-link></mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Seglen</surname> <given-names>PO</given-names></string-name> (<year>1997</year>) <article-title>Why the impact factor of journals should not be used for evaluating research</article-title>. <source>BMJ</source> <volume>314</volume>:<fpage>497</fpage>&#x2013;<lpage>497</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.bmj.com/content/314/7079/497.1.short">http://www.bmj.com/content/314/7079/497.1.short</ext-link></mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Shaw</surname> <given-names>D</given-names></string-name> (<year>2014</year>) <article-title>The prisoners&#x2019; dilemmas: Authorship guidelines and impact factors: between a rock and a hard place</article-title>. <source>EMBO Rep</source> <volume>15</volume>:<fpage>635</fpage>&#x2013;<lpage>637</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.ncbi.nlm.nih.gov/pubmed/24781693">http://www.ncbi.nlm.nih.gov/pubmed/24781693</ext-link></mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Small</surname> <given-names>H</given-names></string-name> (<year>1973</year>) <article-title>Co-citation in the scientific literature: A new measure of the relationship between two documents</article-title>. <source>J Am Soc Inf Sci</source> <volume>24</volume>:<fpage>265</fpage>&#x2013;<lpage>269</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://doi.wiley.com/10.1002/asi.4630240406">http://doi.wiley.com/10.1002/asi.4630240406</ext-link></mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Stallings</surname> <given-names>J</given-names></string-name>, <string-name><surname>Vance</surname> <given-names>E</given-names></string-name>, <string-name><surname>Yang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Vannier</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Liang</surname> <given-names>J</given-names></string-name>, <string-name><surname>Pang</surname> <given-names>L</given-names></string-name>, <string-name><surname>Dai</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ye</surname> <given-names>I</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>G</given-names></string-name> (<year>2013</year>) <article-title>Determining scientific impact using a collaboration index</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>110</volume>:<fpage>9680</fpage>&#x2013;<lpage>9685</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/content/110/24/9680.short">http://www.pnas.org/content/110/24/9680.short</ext-link></mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Stringer</surname> <given-names>MJ</given-names></string-name>, <string-name><surname>Sales-Pardo</surname> <given-names>M</given-names></string-name>, <string-name><surname>Nunes Amaral</surname> <given-names>LA</given-names></string-name> (<year>2010</year>) <article-title>Statistical validation of a global model for the distribution of the ultimate number of citations accrued by papers published in a scientific journal</article-title>. <source>J Am Soc Inf Sci Technol JASIST</source> <volume>61</volume>:<fpage>1377</fpage>&#x2013;<lpage>1385</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid&#x003D;3158611&#x0026;tool&#x003D;pmcentrez&#x0026;rendertype&#x003D;abstract">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid&#x003D;3158611&#x0026;tool&#x003D;pmcentrez&#x0026;rendertype&#x003D;abstract</ext-link></mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Suhrbier</surname> <given-names>A</given-names></string-name>, <string-name><surname>Poland</surname> <given-names>GA</given-names></string-name> (<year>2013</year>) <article-title>Are Impact Factors corrupting truth and utility in biomedical research</article-title>? <source>Vaccine</source> <volume>31</volume>:<fpage>6041</fpage>&#x2013;<lpage>6042</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S0264410X13014576">http://www.sciencedirect.com/science/article/pii/S0264410X13014576</ext-link></mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Van Raan</surname> <given-names>AFJ</given-names></string-name>, <string-name><surname>van Leeuwen</surname> <given-names>TN</given-names></string-name>, <string-name><surname>Visser</surname> <given-names>MS</given-names></string-name>, <string-name><surname>van Eck</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>Waltman</surname> <given-names>L</given-names></string-name> (<year>2010</year>) <article-title>Rivals for the crown: Reply to Opthof and Leydesdorff</article-title>. <source>J Informetr</source> <volume>4</volume>:<fpage>431</fpage>&#x2013;<lpage>435</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S1751157710000301">http://www.sciencedirect.com/science/article/pii/S1751157710000301</ext-link></mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Vinkler</surname> <given-names>P</given-names></string-name> (<year>2003</year>) <article-title>Relations of relative scientometric indicators</article-title>. <source>Scientometrics</source> <volume>58</volume>:<fpage>687</fpage>&#x2013;<lpage>694</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.1023/B:SCIE.0000006888.69146.24">http://link.springer.com/10.1023/B:SCIE.0000006888.69146.24</ext-link></mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="other"><string-name><surname>Walker</surname> <given-names>D</given-names></string-name>, <string-name><surname>Xie</surname> <given-names>H</given-names></string-name>, <string-name><surname>Yan</surname> <given-names>K-K</given-names></string-name>, <string-name><surname>Maslov</surname> <given-names>S</given-names></string-name> (<year>2007</year>) <article-title>Ranking scientific publications using a model of network traffic</article-title>. <source>J Stat Mech Theory Exp</source> 2007:<fpage>P06010</fpage>-<lpage>P06010</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.researchgate.net/publication/2177863_Ranking_Scientific_Publications_Using_a_Simple_Model_of_Network_Traffic">http://www.researchgate.net/publication/2177863_Ranking_Scientific_Publications_Using_a_Simple_Model_of_Network_Traffic</ext-link></mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Waltman</surname> <given-names>L</given-names></string-name>, <string-name><surname>van Eck</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>van Leeuwen</surname> <given-names>TN</given-names></string-name>, <string-name><surname>Visser</surname> <given-names>MS</given-names></string-name>, <string-name><surname>van Raan</surname> <given-names>AFJ</given-names></string-name> (<year>2011a</year>) <article-title>Towards a new crown indicator: Some theoretical considerations</article-title>. <source>J Informetr</source> <volume>5</volume>:<fpage>37</fpage>&#x2013;<lpage>47</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S1751157710000817">http://www.sciencedirect.com/science/article/pii/S1751157710000817</ext-link></mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="other"><string-name><surname>Waltman</surname> <given-names>L</given-names></string-name>, <string-name><surname>Yan</surname> <given-names>E</given-names></string-name>, <string-name><surname>van Eck</surname> <given-names>NJ</given-names></string-name> (<year>2011b</year>) <article-title>A recursive field-normalized bibliometric performance indicator: an application to the field of library and information science</article-title>. <source>Scientometrics</source> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.akademiai.com/doi/abs/10.1007/s11192-011-0449-z">http://www.akademiai.com/doi/abs/10.1007/s11192-011-0449-z</ext-link></mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Wang</surname> <given-names>D</given-names></string-name>, <string-name><surname>Song</surname> <given-names>C</given-names></string-name>, <string-name><surname>Barabasi</surname> <given-names>A-L</given-names></string-name> (<year>2013</year>) <article-title>Quantifying long-term scientific impact</article-title>. <source>Science</source> <volume>342</volume>:<fpage>127</fpage>&#x2013;<lpage>132</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencemag.org/content/342/6154/127.abstract">http://www.sciencemag.org/content/342/6154/127.abstract</ext-link></mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Weber</surname> <given-names>GM</given-names></string-name> (<year>2013</year>) <article-title>Identifying translational science within the triangle of biomedicine</article-title>. <source>J Transl Med</source> <volume>11</volume>:<fpage>126</fpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.translational-medicine.com/content/11/1/126">http://www.translational-medicine.com/content/11/1/126</ext-link></mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Zitt</surname> <given-names>M</given-names></string-name>, <string-name><surname>Small</surname> <given-names>H</given-names></string-name> (<year>2008</year>) <article-title>Modifying the journal impact factor by fractional citation weighting: The audience factor</article-title>. <source>J Am Soc Inf Sci Technol</source> <volume>59</volume>:<fpage>1856</fpage>&#x2013;<lpage>1860</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://doi.wiley.com/10.1002/asi.20880">http://doi.wiley.com/10.1002/asi.20880</ext-link></mixed-citation></ref>
</ref-list>
<sec id="s5" sec-type="supplementary-material"><title>Supplemental Information</title>
<sec id="s5a"><title>Characterization of the co-citation networks of single articles</title>
<p>Field normalization of citations is critical for cross-field comparisons, because of intrinsic differences in citation rates across disciplines. Our approach draws upon the idea of comparing the Article Citation Rate of an article (ACR) with an Expected Citation Rate (ECR), calculated based on peer performance in an article&#x2019;s area of research (<xref ref-type="bibr" rid="sc16">Schubert et al., 1986</xref>; <xref ref-type="bibr" rid="sc20">Vinkler, 2003</xref>). Calculating a robust ECR is challenging; other methods frequently employ journals or journal categories as a proxy for a scientific field (<xref ref-type="bibr" rid="sc9">Moed et al., 1985</xref>; <xref ref-type="bibr" rid="sc8">Lundberg, 2007</xref>; <xref ref-type="bibr" rid="sc23">Zitt and Small, 2008</xref>; <xref ref-type="bibr" rid="sc11">Opthof and Leydesdorff, 2010</xref>; <xref ref-type="bibr" rid="sc19">van Raan et al., 2010</xref>; <xref ref-type="bibr" rid="sc22">Waltman et al., 2011</xref>; <xref ref-type="bibr" rid="sc1">Bornmann and Leydesdorff, 2013</xref>). Unfortunately, these methods do not have sufficient precision to work well at the article level (<xref ref-type="bibr" rid="sc6">Leydesdorff and Bornmann, 2015</xref>). Other methods have increased granularity of field classification (<xref ref-type="bibr" rid="sc21">Waltman and van Eck, 2012</xref>; <xref ref-type="bibr" rid="sc15">Ruiz-Castillo and Waltman, 2015</xref>). However, a granularity threshold must be chosen for analysis, a choice that may differ based on the size of portfolios being compared. Because modern fields of biomedical research exist as a spectrum rather than discrete and separate fields (<xref ref-type="bibr" rid="sc18">Talley et al., 2011</xref>), we decided to take a more nuanced approach to defining an article&#x2019;s field. We constructed each article&#x2019;s co-citation network (<xref ref-type="bibr" rid="sc17">Small, 1973</xref>) and used that as a representative sample of its area of research. Simply put, when an article is first cited, the other papers appearing in the reference list along with the article comprise its co-citation network (<bold><xref ref-type="fig" rid="fig1">Figure 1</xref></bold>). As the article continues to be cited, the papers appearing in the new reference lists alongside it are added to its co-citation network. This network provides a dynamic view of the article&#x2019;s field of research, taking advantage of information provided by the experts who have found the study useful enough to cite.</p></sec>
<sec id="s5b"><title>Algorithm and calculations for Relative Citation Ratios (RCRs)</title>
<p>Our algorithm uses the following steps to calculate RCR values, giving a ratio of ACR to ECR that is benchmarked to papers funded through NIH R01s:</p>
<list list-type="order">
<list-item><p>Convert the RA citation counts to citations per year (<bold><xref ref-type="fig" rid="fig2">Figure 2</xref></bold> and <bold>Supplemental Equation 1</bold>).</p></list-item>
<list-item><p>Generate the RA&#x2019;s co-citation network. To do this, we assemble all articles citing the RA; the complete set of papers cited in the reference lists of these citing articles comprises the co-citation network (<bold><xref ref-type="fig" rid="fig1">Figure 1</xref></bold>).</p></list-item>
<list-item><p>Estimate the FCR of the RA by averaging the journal citation rates of the papers in the co-citation network (<bold>Supplemental Equation 3</bold>).</p></list-item>
<list-item><p>Generate an ECR from the benchmark set of papers. Using R01-funded papers published in a given year, a linear regression of the ACRs vs. FCRs is performed (<bold><xref ref-type="fig" rid="fig2">Figure 2</xref></bold> and <bold>Supplemental Equations 3&#x2013;4</bold>). Regressions are calculated for each publication year.
<list list-type="alpha-lower">
<list-item><p>The linear equation coefficients corresponding to the RA&#x2019;s publication year rescale its FCR into a denominator (ECR) that is benchmarked to the performance of R01-funded articles (<bold>Supplemental Equation 5</bold>).</p></list-item>
</list></p></list-item>
<list-item><p>The Relative Citation Ratio is the ratio of the ACR: ECR.</p></list-item>
</list>
<p>When converting raw citation counts to Article Citation Rate (<italic>Acr</italic>), the year in which the RA was published was excluded from the denominator.</p>
<sec id="s5b1"><title>Supplemental Equation 1:</title>
<disp-formula><alternatives><graphic xlink:href="029629_ueqn1.gif"/></alternatives></disp-formula>
<p>We made this design decision because the publication year is nearly always partial, and because articles receive a low number of citations in the calendar year of their publication compared to subsequent years (Supplemental <xref ref-type="fig" rid="figS2">Figure 2</xref>). In practice, the sum of the citations in years 0 and 1 (the year of publication and the following year) is close to the mean number of citations per year in the following 8 years (Supplemental <bold><xref ref-type="fig" rid="figS2">Figure 2</xref></bold>).</p>
<p>ACRs and journal citation rates vary widely from field to field. To compare the Relative Citation Ratios of small groups (like individual investigators), special care must be taken to adjust only the fraction of the citation rate that is due to between-field differences. We tested three methods for adjusting expected citation rates to a field. These approaches each use information from an article&#x2019;s citation network (see schematics in <bold><xref ref-type="fig" rid="fig1">Figure 1a</xref></bold>). The first method selects the reference article plus those cited in in its reference list (<bold><xref ref-type="fig" rid="fig1">Figure 1a</xref></bold>, bottom). The second approach instead selects subsequent papers citing the article (<bold><xref ref-type="fig" rid="fig1">Figure 1a</xref></bold>, top). Finally, the third uses the set of articles that are co-cited with the article of interest by subsequent papers (<bold><xref ref-type="fig" rid="fig1">Figure 1a</xref></bold>, middle). The Reference Article was always included in the set of papers selected to estimate the FCR, since an article is de facto part of its field. In all three cases, the average of the journal citation rates for the papers in the selected level of the citation network is used as the Field Citation Rate (<italic>Fcr</italic>).</p></sec>
<sec id="s5b2"><title>Supplemental Equation 2:</title>
<disp-formula><alternatives><graphic xlink:href="029629_ueqn2.gif"/></alternatives></disp-formula>
<p><italic>N</italic> is the number of papers in the selected level of the co-citation network (<bold><xref ref-type="fig" rid="fig1">Figure 1a</xref></bold>) and <italic>Jcr<sub>i</sub></italic> is the journal citation rate of each paper at the specified level of the citation network.</p>
<p>To generate an expectation of citation performance using this cohort of papers, we performed a linear regression of Article Citation Rate (<italic>Acr</italic>) in a baseline population against their Field Citation Rates (<italic>Fcr</italic>) from the same year (<bold><xref ref-type="fig" rid="fig1">Figure 1</xref></bold>). R01-funded articles were used as a benchmark population. This process was repeated for each year being analyzed to give regression coefficients (slope, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="029629_inline1.gif"/></alternatives></inline-formula> and intercept, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="029629_inline2.gif"/></alternatives></inline-formula>) for benchmarking articles.</p></sec>
<sec id="s5b3"><title>Supplemental Equation 3:</title>
<disp-formula><alternatives><graphic xlink:href="029629_ueqn3.gif"/></alternatives></disp-formula></sec>
<sec id="s5b4"><title>Supplemental Equation 4:</title>
<disp-formula><alternatives><graphic xlink:href="029629_ueqn4.gif"/></alternatives></disp-formula>
<p>Alternatively, if benchmarking to the median (rather than mean) field-normalized performance for the benchmark group is desired, quantile regression can be used in lieu of simple linear regression (<xref ref-type="bibr" rid="sc4">Koenker, 2005</xref>). In either case, the resulting regression line transforms a FCR into an Expected Citation Rate (<italic>Ecr<sup>Year</sup></italic>), corresponding to the ACR that R01-funded papers with the same FCRs and published in the same year were able to achieve, and this can be used outside of the baseline population as a benchmark for articles published in that year:</p></sec>
<sec id="s5b5"><title>Supplemental Equation 5:</title>
<disp-formula><alternatives><graphic xlink:href="029629_ueqn5.gif"/></alternatives></disp-formula>
<p>For the years analyzed here (2002&#x2013;2011), the resulting regression coefficients are given in Supplemental <xref ref-type="table" rid="tblS2">Table 2</xref>. RCR for each article is the ratio of that article&#x2019;s ACR divided by its ECR. Calculating the arithmetic mean is the preferred way of determining the RCR of an entire portfolio (<xref ref-type="bibr" rid="sc11">Opthof and Leydesdorff, 2010</xref>; <xref ref-type="bibr" rid="sc19">van Raan et al., 2010</xref>; <xref ref-type="bibr" rid="sc22">Waltman et al., 2011</xref>):</p></sec>
<sec id="s5b6"><title>Supplemental Equation 6:</title>
<disp-formula><alternatives><graphic xlink:href="029629_ueqn6.gif"/></alternatives></disp-formula>
<p>Where <italic>n</italic> is the number of papers being evaluated, <italic>Acr<sub>i</sub></italic> is the article citation rate and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="029629_inline3.gif"/></alternatives></inline-formula> the expected citation rate of each article found by transforming its <italic>Fcr<sub>i</sub></italic> with the regression coefficients for its publication year (<bold>Supplemental Equation 5</bold>).</p></sec>
</sec>
<sec id="s5c"><title>Evaluation of different levels of the citation network for field normalization</title>
<p>The aim of this adjustment is to accurately normalize an individual article&#x2019;s citation rate to its field&#x2019;s average citation rate, while preserving within-field differences between papers. Using a set of papers from 2009 matched to R01 grants active in the same year, we compared these three approaches. Calculated RCRs were very similar for all three groups at the article level (<bold>Supplemental <xref ref-type="table" rid="tblS3">Table 3</xref></bold>). This is not surprising, since at this granular scale the article&#x2019;s numerator (ACR) accounts for more of the variance than the denominator. However, accurate field-adjustment is especially important for large-scale analyses, where field differences in citation rates can dominate measurements, as article-level differences in ACR average out. A more accurate estimate of the field citation rate would be predicted to show a smaller correlation between article citation rate and field citation rate, as within-field differences are more effectively excluded. To measure the effectiveness of each level of the citation network, we calculated the correlations between ACRs and ECRs for each approach. Of the three, the &#x201C;Co-cited&#x201D; method shows the least correlation between article citations and expected citations (<bold>Supplemental <xref ref-type="table" rid="tblS4">Table 4</xref></bold>). In addition, the variance in expected citation rates should be lower in approaches that more successfully isolate the between-field differences in citation rate from the within-field differences. Again, the co-citation level of the citation network performed the best here (<bold><xref ref-type="table" rid="tbl1">Table 1</xref></bold>).</p></sec>
<sec id="s5d"><title>Validation of RCR with post-publication peer review</title>
<p>We extensively validated article-level RCRs against expert reviewer scores of the impact or value of papers, using post-publication peer review. Three independently collected sets of post-publication peer reviews were used for this analysis: Faculty of 1000 (F1000) (<xref ref-type="bibr" rid="sc7">Li and Thelwall, 2012</xref>; <xref ref-type="bibr" rid="sc1">Bornmann and Leydesdorff, 2013</xref>), a previous survey conducted by the Institute for Defense Analyses Science and Technology Policy Institute (STPI) (<xref ref-type="bibr" rid="sc5">Lal et al., 2012</xref>), and post-publication peer review conducted by NIH Intramural Research Program (IRP) Principal Investigators.</p>
<p>The first set of expert review scores was compiled from F1000, in which faculty review articles in their fields of expertise, and rate the articles on a scale of 1 to 3 (&#x201C;Good&#x201D;, &#x201C;Very Good&#x201D;, and &#x201C;Exceptional&#x201D;). Because the decision by the faculty members to review the article is itself a mark of merit, these scores are summed into a composite F1000 score (<bold>Supplemental <xref ref-type="fig" rid="figS3">Figure 3</xref></bold>). We downloaded scores in June 2014 for 2193 R01-funded articles published in 2009 and compared them to their RCRs. This yielded an article-level correlation coefficient <italic>r</italic> of 0.44 between RCR and F1000 scores (<bold><xref ref-type="fig" rid="fig3">Figure 3a</xref></bold>).</p>
<p>For a second set of expert review scores, we took advantage of a previous survey conducted by STPI, of papers funded through the Howard Hughes Medical Institute and NIH. In this survey, experts rated the impact of articles (shown here on a scale of 0 to 4, n &#x003D; 430 papers from 2005&#x2013;2011, <bold>Supplemental <xref ref-type="fig" rid="figS4">Figure 4</xref></bold>). Since citation data (including RCR) is highly skewed while survey ratings are not, RCR was log-transformed to bring these ranges into better alignment. The article-level correspondence of RCR with these review scores was similar to that observed with the F1000 scores (<bold><italic>r</italic></bold> &#x003D; 0.47, <bold><xref ref-type="fig" rid="fig3">Figure 3b</xref></bold>).</p>
<p>Finally, we recruited investigators from the NIH Intramural Research Program (IRP) to perform post-publication peer review of NIH-funded articles published in 2009 (<bold>Supplemental <xref ref-type="fig" rid="figS5">Figures 5</xref>&#x2013;<xref ref-type="fig" rid="figS7">7</xref></bold>). A total of 290 articles were independently reviewed by multiple investigators, yielding an article-level correlation of 0.56 (<bold><xref ref-type="fig" rid="fig3">Figure 3c</xref></bold>). The distribution of these impact scores is shown in <xref ref-type="fig" rid="fig2">Figure 2</xref> Supplement 8. Finally, we asked reviewers in the NIH Intramural Research Program to conduct post-publication peer review of R01-funded articles published in 2009 (reviews conducted by The Scientific Consulting Group, Gaithersburg, MD). Reviewers were asked to give scores on a scale of 1&#x2013;5 for the following questions:</p>
<list list-type="bullet">
<list-item><p>Rate whether the question being addressed is important to answer. (1 &#x003D; Not Important, 2 &#x003D; Slightly Important, 3 &#x003D; Important, 4 &#x003D; Highly Important, 5 &#x003D; Extremely Important)</p></list-item>
<list-item><p>Rate whether you agree that the methods are appropriate and the scope of the experiments adequate. (1 &#x003D; Strongly Disagree, 2 &#x003D; Disagree, 3 &#x003D; Neutral, 4 &#x003D; Agree, 5 &#x003D; Strongly Agree)</p></list-item>
<list-item><p>Rate how robust the study is based on the strength of the evidence presented. (1 &#x003D; Not Robust, 2 &#x003D; Slightly Robust, 3 &#x003D; Moderately Robust, 4 &#x003D; Highly Robust, 5 &#x003D; Extremely Robust)</p></list-item>
<list-item><p>Rate the likelihood that the results could ultimately have a substantial positive impact on human health outcomes. (1 &#x003D; Very unlikely, 2 &#x003D; Unlikely, 3 &#x003D; Foreseeable but uncertain, 4 &#x003D; Probable, 5 &#x003D; Almost Certainly)</p></list-item>
<list-item><p>Rate the impact that the research is likely to have or has already had. (1 &#x003D; Minimal Impact, 2 &#x003D; Some Impact, 3 &#x003D; Moderate Impact, 4 &#x003D; High Impact, 5 &#x003D; Extremely High Impact)</p></list-item>
<list-item><p>Provide your overall evaluation of the value and impact of this publication. (1 &#x003D; minimal or no value, 2 &#x003D; Moderate value, 3 &#x003D; Average value, 4 &#x003D; High value, 5&#x003D; Extremely high value)</p></list-item>
</list>
<p>The distribution of responses for each of these questions is shown in <bold>Supplemental <xref ref-type="fig" rid="figS5">Figure 5</xref></bold>. Multiple experts were asked to review each paper, and each set of papers were matched to the fields of expertise of the reviewers examining them. For correlating RCR to review scores, we used the average of the score for the final question (overall evaluation of the paper&#x2019;s value) for articles that were reviewed by at least two experts.</p>
<p>To determine post-hoc which of the first five criteria (importance of scientific question, appropriate methods, robustness of study, likelihood of health outcomes and likely impact) were associated with reviewers&#x2019; ratings of overall value, we first performed a Random Forest analysis using all 5 criteria. Perhaps unsurprisingly, this analysis showed that &#x201C;likely impact&#x201D; was most closely associated with assessment of overall value (<bold>Supplemental <xref ref-type="fig" rid="figS6">Figure 6a</xref></bold>). We subsequently removed this criterion from the analysis to determine the relative importance of the other 4 questions. In this analysis, &#x201C;Importance&#x201D; of the scientific question and &#x201C;Robustness&#x201D; were most closely linked to overall value (<bold>Supplemental <xref ref-type="fig" rid="figS6">Figure 6b</xref></bold>).</p>
<p>Should an article-level correlation of approximately 0.5 between RCR and expert reviewer scores be considered reliable? This level of correspondence is similar to that previously measured between bibliometric indicators and reviewer scores (<xref ref-type="bibr" rid="sc7">Li and Thelwall, 2012</xref>; <xref ref-type="bibr" rid="sc1">Bornmann and Leydesdorff, 2013</xref>). Given the partially overlapping datasets used here, we were able to calculate the correlation of expert reviewer scores with one another. The Pearson correlation coefficient between log-transformed F1000 scores and those from the STPI review was 0.35. In addition, the correlation of scores within a survey can be determined with statistical resampling. We selected papers with three reviews from the STPI and NIH IRP surveys. The order of reviewers was randomly shuffled, and the correlation coefficient between the first reviewer&#x2019;s score and the mean of the other two scores was determined and recorded. This process was repeated 10,000 times for each dataset. The distributions of the 10,000 recorded correlation coefficients are shown in <bold>Supplemental <xref ref-type="fig" rid="figS7">Figure 7</xref></bold>. This approach demonstrated an internal correlation of 0.32 for STPI reviews and 0.44 for the NIH IRP reviews. These values are similar to the correlation between RCR and each set of review scores. These internal correlations between reviewer scores likely represent an estimate of the degree to which it is possible for bibliometrics to correspond to expert opinions. Thus, RCR agrees with expert opinion scores as well as experts agree with one another.</p></sec>
<sec id="s5e"><title>Ranking invariance of RCR</title>
<p>One desirable property in a bibliometric indicator is that of ranking invariance (<xref ref-type="bibr" rid="sc14">Rousseau and Leydesdorff, 2011</xref>; <xref ref-type="bibr" rid="sc22">Waltman et al., 2011</xref>; <xref ref-type="bibr" rid="sc3">Gl&#x00E4;nzel and Moed, 2012</xref>). A citation metric is ranking invariant if, when two groups of articles are being compared using that indicator, their relative ranking does not change if the groups are inflated by the same amount of uncited papers (<xref ref-type="bibr" rid="sc14">Rousseau and Leydesdorff, 2011</xref>). RCR is ranking invariant under two cases: when the two comparison groups are the same size and the same absolute number of uncited papers is added to each group, and when the two comparison groups are different sizes and the same proportion of uncited papers is added to each comparison group.</p>
<p>For the first case (two groups of the same size, termed groups <italic>I</italic> and <italic>J</italic>, where group <italic>I</italic> has the greater RCR), the group RCRs are described by the following inequality:</p>
<sec id="s5f1"><title>Supplemental Equation 7:</title>
<disp-formula><alternatives><graphic xlink:href="029629_ueqn7.gif"/></alternatives></disp-formula>
<p>Where <italic>i</italic> and <italic>j</italic> are the individual papers from groups <italic>I</italic> and <italic>J</italic>, and <italic>n</italic> is the number of papers in these groups. Adding <italic>k</italic> &#x003E; 0 papers to each group, each with a constant RCR of <italic>a</italic> &#x2265; 0 (equal to 0 for uncited papers) yields the following inequality:</p></sec>
<sec id="s5f2"><title>Supplemental Equation 8:</title>
<disp-formula><alternatives><graphic xlink:href="029629_ueqn8.gif"/></alternatives></disp-formula>
<p>This simplifies to <bold>Supplemental Equation 9</bold>, demonstrating ranking invariance under this condition:</p></sec>
<sec id="s5f3"><title>Supplemental Equation 9:</title>
<disp-formula><alternatives><graphic xlink:href="029629_ueqn9.gif"/></alternatives></disp-formula>
<p>For the second case (two groups of unequal sizes, termed groups <italic>I</italic> and <italic>J</italic>, where group <italic>I</italic> has the greater RCR), the group RCRs are described by the following inequality:</p></sec>
<sec id="s5f4"><title>Supplemental Equation 10:</title>
<disp-formula><alternatives><graphic xlink:href="029629_ueqn10.gif"/></alternatives></disp-formula>
<p>Where <italic>n</italic> is the number of papers in group <italic>I</italic> and <italic>m</italic> is the number of papers in group <italic>J.</italic> Adding the same proportion <italic>k</italic> &#x003E; 0 of papers to each group, each with a constant RCR of <italic>a</italic> &#x2265; 0 (equal to 0 for uncited papers) yields the following inequality:</p></sec>
<sec id="s5f5"><title>Supplemental Equation 11:</title>
<disp-formula><alternatives><graphic xlink:href="029629_ueqn11.gif"/></alternatives></disp-formula>
<p>This simplifies back to <bold>Supplemental Equation 10</bold>, demonstrating ranking invariance under this condition as well. Note that while uncited papers correspond to <italic>a &#x003D;</italic> 0, any positive RCR <italic>a</italic> could be substituted and ranking invariance would hold.</p></sec>
</sec>
<sec id="s5g"><title>Comparison of co-citation vs. journal category denominators</title>
<p>For generating Relative Citation Ratio-type metrics (a ratio between observed and expected citations), other denominators have been explored in previous work. Early and more recent versions (<xref ref-type="bibr" rid="sc16">Schubert et al., 1986</xref>; <xref ref-type="bibr" rid="sc20">Vinkler, 2003</xref>; <xref ref-type="bibr" rid="sc10">Neill et al., 2015</xref>) used the Journal Impact Factor, or alternatively the average citation count of articles from the same journal, as an expected citation count. Journals tend to be arranged in hierarchies within their fields of research, and journals with higher Impact Factors are generally considered more prestigious. Using articles from the same journal to derive an expected citation boosts the Relative Citation Ratios of those publishing in less prestigious journals, a feature many consider undesirable. Thus, others have segregated journals into categories so that papers from within the entire hierarchy of journals can be used to generate an expected citation count as the denominator (<xref ref-type="bibr" rid="sc1">Bornmann and Leydesdorff, 2013</xref>; <xref ref-type="bibr" rid="sc2">Bornmann and Marx, 2013</xref>; <xref ref-type="bibr" rid="sc6">Leydesdorff and Bornmann, 2015</xref>). In principle, this approach could normalize out differences between fields as well. In practice, journal categorization has been found to be imprecise at the article level (<xref ref-type="bibr" rid="sc6">Leydesdorff and Bornmann, 2015</xref>), and one reason for this may be that category-based field normalization could go too far in boosting the scores of articles in fields calculated as having a low expected citation rate.</p>
<p>To compare our approach using co-citation networks and regression to R01-funded papers against the journal categorization approach, we examined these two ratios at the low end of field citation rates (&#x003C; 2.0) in one year of our dataset (2009, with a total of 34,520 R01-funded papers for which we could calculate both metrics (Supplemental <xref ref-type="fig" rid="figS9">Figure 9</xref>). There were 109 papers with an FCR of less than 2 when calculating RCR, and 544 with a denominator corresponding to an FCR of less than 2 for the journal categorization approach. In general, the journal categorization approach assigned higher scores to these papers (mean score, 1.67, <bold>Supplemental <xref ref-type="fig" rid="figS9">Figure 9</xref></bold>) compared to RCR (mean RCR, 0.59). The range of values for the journal categorization approach was 0.17 &#x2013; 23.4, while the range of RCRs was 0.15 &#x2013; 3.7. These data do not support the conclusion that the Relative Citation Ratio described here overweight articles with low Field Citation Rates compared to previous approaches.</p></sec>
</sec>
<sec id="s6" sec-type="supplementary-material"><title>Supplemental Figures</title>
<fig id="figS1" position="float" orientation="portrait" fig-type="figure">
<label>Supplemental Figure 1.</label>
<caption><p>Journal impact factor stability over time. (a) Journal Impact Factors for 12 selected journals from 2003 to 2011. (b) Pearson correlation coefficients <bold><italic>r</italic></bold> of the Journal Impact Factors for these 12 journals in 2003 vs. each of their respective Impact Factors in subsequent years. In each case, <bold><italic>r</italic></bold> is over 0.9.</p></caption>
<graphic xlink:href="029629_figS1.tif"/>
</fig>
<fig id="figS2" position="float" orientation="portrait" fig-type="figure">
<label>Supplemental Figure 2.</label>
<caption><p>Mean citations accrued each year for 608,058 papers published in 2003 appearing in the same journals as NIH-funded publications. Adding the values for 2003 and 2004 gives a value (2.26 citations per publication per year) close to the mean citations per year of the following years (2.36). Although these values may seem low, they are both similar to the global 2013 Aggregate Impact Factor metric for journals appearing in the Biology subcategory (2.56), which is also measured in citations per paper per year.</p></caption>
<graphic xlink:href="029629_figS2.tif"/>
</fig>
<fig id="figS3" position="float" orientation="portrait" fig-type="figure">
<label>Supplemental Figure 3.</label>
<caption><p>Distribution of Faculty of 1000 scores for 2193 R01-funded papers from 2009.</p></caption>
<graphic xlink:href="029629_figS3.tif"/>
</fig>
<fig id="figS4" position="float" orientation="portrait" fig-type="figure">
<label>Supplemental Figure 4.</label>
<caption><p>Distribution of scores from the Science and Technology Policy Institute survey.</p></caption>
<graphic xlink:href="029629_figS4.tif"/>
</fig>
<fig id="figS5" position="float" orientation="portrait" fig-type="figure">
<label>Supplemental Figure 5.</label>
<caption><p>Summary of NIH OIRP reviewer responses to post-publication peer review questions. Distribution of ratings to the questions: (a) Rate whether the question being addressed is important to answer. (b) Rate whether you agree that the methods are appropriate and the scope of the experiments adequate. (c) Rate how robust the study is based on the strength of the evidence presented. (d) Rate the likelihood that the results could ultimately have a substantial positive impact on human health outcomes. (e) Rate the impact that the research is likely to have or has already had. (f) Provide your overall evaluation of the value and impact of this publication.</p></caption>
<graphic xlink:href="029629_figS5.tif"/>
</fig>
<fig id="figS6" position="float" orientation="portrait" fig-type="figure">
<label>Supplemental Figure 6.</label>
<caption><p>Inferred criteria used by NIH IRP reviewers to rate overall value and impact of a publication. (a) Criteria most strongly linked to assessments of &#x201C;overall value&#x201D;, measured with Random Forest classification. Values indicate the mean decrease in Gini coefficient. (b) Criteria most strongly linked to assessments of &#x201C;overall value&#x201D;, excluding &#x201C;likely impact&#x201D;, and measured with Random Forest classification.</p></caption>
<graphic xlink:href="029629_figS6.tif"/>
</fig>
<fig id="figS7" position="float" orientation="portrait" fig-type="figure">
<label>Supplemental Figure 7.</label>
<caption><p>Internal correlation of post-publication peer review scores is similar to correlation between RCR and review scores. (a) Pearson correlation coefficients (<bold><italic>r</italic></bold>) of one randomly chosen reviewer score vs. the mean of the other two scores for that paper, determined by statistical resampling. Distribution of correlation coefficients determined by resampling from the STPI dataset (10,000 repetitions, mean <bold><italic>r</italic></bold> &#x003D; 0.32). (b) Pearson correlation coefficients (<bold><italic>r</italic></bold>) of one randomly chosen reviewer score vs. the mean of the other two scores for that paper, determined by statistical resampling. Distribution of correlation coefficients determined by resampling from the IRP dataset (10,000 repetitions, mean <bold><italic>r</italic></bold> &#x003D; 0.44).</p></caption>
<graphic xlink:href="029629_figS7.tif"/>
</fig>
<fig id="figS8" position="float" orientation="portrait" fig-type="figure">
<label>Supplemental Figure 8.</label>
<caption><p>Correlation of the average of different investigators&#x2019; article RCRs vs. the average of the Journal Impact Factors in which they published. Some investigators published very influential articles (high RCR) in lower-profile venues (low JIF) and vice versa. R<sup>2</sup> &#x003D; 0.23</p></caption>
<graphic xlink:href="029629_figS8.tif"/>
</fig>
<fig id="figS9" position="float" orientation="portrait" fig-type="figure">
<label>Supplemental Figure 9.</label>
<caption><p>Comparison of RCR and journal categorization approach (<xref ref-type="bibr" rid="sc12">Radicchi et al., 2008</xref>; <xref ref-type="bibr" rid="sc22">Waltman et al., 2011</xref>) for deriving expected citations for the ratio denominator. Fewer articles had Field Citation Rates less than 2.0 when calculating RCR (n &#x003D;109) vs. the journal categorization approach (n &#x003D; 544), of a possible R01-funded 34,520 papers with the necessary information to calculate both metrics. Data points are partially transparent to allow coordinates with multiple papers (darker) to be more clearly identified.</p></caption>
<graphic xlink:href="029629_figS9.tif"/>
</fig></sec>
<sec id="s7" sec-type="supplementary-material"><title>Supplemental Tables</title>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>Supplemental Table 1.</label>
<caption><p>Journal Impact Factor stability over time for 100 selected journals.</p></caption>
<graphic xlink:href="029629_tblS1a.tif"/>
<graphic xlink:href="029629_tblS1b.tif"/>
<graphic xlink:href="029629_tblS1c.tif"/>
</table-wrap>
<table-wrap id="tblS2" orientation="portrait" position="float">
<label>Supplemental Table 2.</label>
<caption><p>Regression coefficients (ACR on FCR, through 2012), for ordinary least squares linear regression (OLS) or quantile regression to the median (QR) for papers with concurrent R01 funding. One of the criticisms about Impact Factor is that it uses the mean, rather than median, of a skewed distribution (<xref ref-type="bibr" rid="sc13">Rossner et al., 2007</xref>); for RCR, quantile regression can be used to benchmark articles to the median citation performance in the benchmark group, rather than the mean.</p></caption>
<graphic xlink:href="029629_tblS2.tif"/>
</table-wrap>
<table-wrap id="tblS3" orientation="portrait" position="float">
<label>Supplemental Table 3.</label>
<caption><p>Correlation coefficients (<italic>r</italic>) between the log-transformed RCR values generated with the three different methods for estimating field citation rates with co-citation networks.</p></caption>
<graphic xlink:href="029629_tblS3.tif"/>
</table-wrap>
<table-wrap id="tblS4" orientation="portrait" position="float">
<label>Supplemental Table 4.</label>
<caption><p>Correlation between ACR vs. ECR.</p></caption>
<graphic xlink:href="029629_tblS4.tif"/>
</table-wrap></sec>
<ref-list><title>Supplemental References</title>
<ref id="sc1"><mixed-citation publication-type="journal"><string-name><surname>Bornmann</surname> <given-names>L</given-names></string-name>, <string-name><surname>Leydesdorff</surname> <given-names>L</given-names></string-name> (<year>2013</year>) <article-title>The validation of (advanced) bibliometric indicators through peer assessments: A comparative study using data from InCites and F1000</article-title>. <source>J Informetr</source> <volume>7</volume>:<fpage>286</fpage>&#x2013;<lpage>291</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S175115771200106X">http://www.sciencedirect.com/science/article/pii/S175115771200106X</ext-link></mixed-citation></ref>
<ref id="sc2"><mixed-citation publication-type="journal"><string-name><surname>Bornmann</surname> <given-names>L</given-names></string-name>, <string-name><surname>Marx</surname> <given-names>W</given-names></string-name> (<year>2013</year>) <article-title>How to evaluate individual researchers working in the natural and life sciences meaningfully</article-title>? <source>A proposal of methods based on percentiles of citations. Scientometrics</source> <volume>98</volume>:<fpage>487</fpage>&#x2013;<lpage>509</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.1007/s11192-013-1161-y">http://link.springer.com/10.1007/s11192-013-1161-y</ext-link></mixed-citation></ref>
<ref id="sc3"><mixed-citation publication-type="journal"><string-name><surname>Gl&#x00E4;nzel</surname> <given-names>W</given-names></string-name>, <string-name><surname>Moed</surname> <given-names>HF</given-names></string-name> (<year>2012</year>) <article-title>Opinion paper: thoughts and facts on bibliometric indicators</article-title>. <source>Scientometrics</source> <volume>96</volume>:<fpage>381</fpage>&#x2013;<lpage>394</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.1007/s11192-012-0898-z">http://link.springer.com/10.1007/s11192-012-0898-z</ext-link></mixed-citation></ref>
<ref id="sc4"><mixed-citation publication-type="book"><string-name><surname>Koenker</surname> <given-names>R</given-names></string-name> (<year>2005</year>) <source>Quantile Regression</source>. <publisher-name>Cambridge University Press</publisher-name>. Available at: <ext-link ext-link-type="uri" xlink:href="https://books.google.com/books?hl&#x003D;en&#x0026;lr&#x003D;&#x0026;id&#x003D;hdkt7V4NXsgC&#x0026;pgis&#x003D;1">https://books.google.com/books?hl&#x003D;en&#x0026;lr&#x003D;&#x0026;id&#x003D;hdkt7V4NXsgC&#x0026;pgis&#x003D;1</ext-link></mixed-citation></ref>
<ref id="sc5"><mixed-citation publication-type="other"><string-name><surname>Lal</surname> <given-names>B</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>AG</given-names></string-name>, <string-name><surname>Jonas</surname> <given-names>S</given-names></string-name>, <string-name><surname>Lee</surname> <given-names>EC</given-names></string-name>, <string-name><surname>Richards</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Pe&#x00F1;a</surname> <given-names>VI</given-names></string-name> (<year>2012</year>) <article-title>An Outcome Evaluation of the National Institutes of Health (NIH) Director&#x2019;s Pioneer Award (NDPA) Program</article-title>, <source>FY</source> <fpage>2004</fpage>&#x2013;<lpage>2006</lpage>.</mixed-citation></ref>
<ref id="sc6"><mixed-citation publication-type="other"><string-name><surname>Leydesdorff</surname> <given-names>L</given-names></string-name>, <string-name><surname>Bornmann</surname> <given-names>L</given-names></string-name> (<year>2015</year>) <article-title>The operationalization of &#x201C;fields&#x201D; as WoS subject categories (WCs) in evaluative bibliometrics: The cases of &#x201C;library and information science&#x201D; and &#x201C;science &#x0026; technology studies.&#x201D;</article-title> <source>J Assoc Inf Sci Technol</source>:n/a&#x2014;n/a Available at: <ext-link ext-link-type="uri" xlink:href="http://doi.wiley.com/10.1002/asi.23408">http://doi.wiley.com/10.1002/asi.23408</ext-link></mixed-citation></ref>
<ref id="sc7"><mixed-citation publication-type="journal"><string-name><surname>Li</surname> <given-names>X</given-names></string-name>, <string-name><surname>Thelwall</surname> <given-names>M</given-names></string-name> (<year>2012</year>) <article-title>F1000, Mendeley and Traditional Bibliometric Indicators</article-title>. <source>Proc 17th Int Conf Sci Technol Indic Montr&#x00E9;al Sci OST</source> <volume>3</volume>:<fpage>541</fpage>&#x2013;<lpage>551</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://2012.sticonference.org/Proceedings/vol2/Li_F1000_541.pdf">http://2012.sticonference.org/Proceedings/vol2/Li_F1000_541.pdf</ext-link></mixed-citation></ref>
<ref id="sc8"><mixed-citation publication-type="journal"><string-name><surname>Lundberg</surname> <given-names>J</given-names></string-name> (<year>2007</year>) <article-title>Lifting the crown&#x2014;citation z-score</article-title>. <source>J Informetr</source> <volume>1</volume>:<fpage>145</fpage>&#x2013;<lpage>154</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S1751157706000265">http://www.sciencedirect.com/science/article/pii/S1751157706000265</ext-link></mixed-citation></ref>
<ref id="sc9"><mixed-citation publication-type="journal"><string-name><surname>Moed</surname> <given-names>HF</given-names></string-name>, <string-name><surname>Burger</surname> <given-names>WJM</given-names></string-name>, <string-name><surname>Frankfort</surname> <given-names>JG</given-names></string-name>, <string-name><surname>Van Raan</surname> <given-names>AFJ</given-names></string-name> (<year>1985</year>) <article-title>The use of bibliometric data for the measurement of university research performance</article-title>. <source>Res Policy</source> <volume>14</volume>:<fpage>131</fpage>&#x2013;<lpage>149</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/0048733385900125">http://www.sciencedirect.com/science/article/pii/0048733385900125</ext-link></mixed-citation></ref>
<ref id="sc10"><mixed-citation publication-type="journal"><string-name><surname>Neill</surname> <given-names>US</given-names></string-name>, <string-name><surname>Thompson</surname> <given-names>CB</given-names></string-name>, <string-name><surname>Gibson</surname> <given-names>DS</given-names></string-name> (<year>2015</year>) <article-title>Assessing Research Productivity</article-title>. <source>Sci</source> <volume>29</volume>:<fpage>41682</fpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.the-scientist.com/?articles.view/articleNo/41682/title/Assessing-Research-Productivity">http://www.the-scientist.com/?articles.view/articleNo/41682/title/Assessing-Research-Productivity</ext-link></mixed-citation></ref>
<ref id="sc11"><mixed-citation publication-type="journal"><string-name><surname>Opthof</surname> <given-names>T</given-names></string-name>, <string-name><surname>Leydesdorff</surname> <given-names>L</given-names></string-name> (<year>2010</year>) <article-title>Caveats for the journal and field normalizations in the CWTS (&#x201C;Leiden&#x201D;) evaluations of research performance</article-title>. <source>J Informetr</source> <volume>4</volume>:<fpage>423</fpage>&#x2013;<lpage>430</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S1751157710000106">http://www.sciencedirect.com/science/article/pii/S1751157710000106</ext-link></mixed-citation></ref>
<ref id="sc12"><mixed-citation publication-type="journal"><string-name><surname>Radicchi</surname> <given-names>F</given-names></string-name>, <string-name><surname>Fortunato</surname> <given-names>S</given-names></string-name>, <string-name><surname>Castellano</surname> <given-names>C</given-names></string-name> (<year>2008</year>) <article-title>Universality of citation distributions: toward an objective measure of scientific impact</article-title>. <source>Proc Natl Acad Sci USA</source> <volume>105</volume>:<fpage>17268</fpage>&#x2013;<lpage>17272</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.pnas.org/content/early/2008/10/30/0806977105.abstract">http://www.pnas.org/content/early/2008/10/30/0806977105.abstract</ext-link></mixed-citation></ref>
<ref id="sc13"><mixed-citation publication-type="journal"><string-name><surname>Rossner</surname> <given-names>M</given-names></string-name>, <string-name><surname>Van Epps</surname> <given-names>H</given-names></string-name>, <string-name><surname>Hill</surname> <given-names>E</given-names></string-name> (<year>2007</year>) <article-title>Show me the data</article-title>. <source>J Cell Biol</source> <volume>179</volume>:<fpage>1091</fpage>&#x2013;<lpage>1092</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://jcb.rupress.org/content/179/6/1091.short">http://jcb.rupress.org/content/179/6/1091.short</ext-link></mixed-citation></ref>
<ref id="sc14"><mixed-citation publication-type="other"><string-name><surname>Rousseau</surname> <given-names>R</given-names></string-name>, <string-name><surname>Leydesdorff</surname> <given-names>L</given-names></string-name> (<year>2011</year>) <article-title>Simple arithmetic versus intuitive understanding:The case of the impact factor</article-title>. <source>ISSI Newsl</source>:<fpage>10</fpage>&#x2013;<lpage>14</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://eprints.rclis.org/15478/1/Impact_factorarithmetic.pdf">http://eprints.rclis.org/15478/1/Impact_factorarithmetic.pdf</ext-link></mixed-citation></ref>
<ref id="sc15"><mixed-citation publication-type="journal"><string-name><surname>Ruiz-Castillo</surname> <given-names>J</given-names></string-name>, <string-name><surname>Waltman</surname> <given-names>L</given-names></string-name> (<year>2015</year>) <article-title>Field-normalized citation impact indicators using algorithmically constructed classification systems of science</article-title>. <source>J Informetr</source> <volume>9</volume>:<fpage>102</fpage>&#x2013;<lpage>117</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S1751157714001126">http://www.sciencedirect.com/science/article/pii/S1751157714001126</ext-link></mixed-citation></ref>
<ref id="sc16"><mixed-citation publication-type="journal"><string-name><surname>Schubert</surname> <given-names>A</given-names></string-name>, <string-name><surname>Gl&#x00E4;nzel</surname> <given-names>W</given-names></string-name>, <string-name><surname>Braun</surname> <given-names>T</given-names></string-name> (<year>1986</year>) <article-title>Relative indicators of publication output and citation impact of european physics research, 1978-1980</article-title>. <source>Czechoslov J Phys</source> <volume>36</volume>:<fpage>126</fpage>&#x2013;<lpage>129</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.1007/BF01599744">http://link.springer.com/10.1007/BF01599744</ext-link></mixed-citation></ref>
<ref id="sc17"><mixed-citation publication-type="journal"><string-name><surname>Small</surname> <given-names>H</given-names></string-name> (<year>1973</year>) <article-title>Co-citation in the scientific literature: A new measure of the relationship between two documents</article-title>. <source>J Am Soc Inf Sci</source> <volume>24</volume>:<fpage>265</fpage>&#x2013;<lpage>269</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://doi.wiley.com/10.1002/asi.4630240406">http://doi.wiley.com/10.1002/asi.4630240406</ext-link></mixed-citation></ref>
<ref id="sc18"><mixed-citation publication-type="journal"><string-name><surname>Talley</surname> <given-names>EM</given-names></string-name>, <string-name><surname>Newman</surname> <given-names>D</given-names></string-name>, <string-name><surname>Mimno</surname> <given-names>D</given-names></string-name>, <string-name><surname>Herr</surname> <given-names>BW</given-names></string-name>, <string-name><surname>Wallach</surname> <given-names>HM</given-names></string-name>, <string-name><surname>Burns</surname> <given-names>GAPC</given-names></string-name>, <string-name><surname>Leenders</surname> <given-names>AGM</given-names></string-name>, <string-name><surname>McCallum</surname> <given-names>A</given-names></string-name> (<year>2011</year>) <article-title>Database of NIH grants using machine-learned categories and graphical clustering</article-title>. <source>Nat Methods</source> <volume>8</volume>:<fpage>443</fpage>&#x2013;<lpage>444</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1038/nmeth.1619">http://dx.doi.org/10.1038/nmeth.1619</ext-link></mixed-citation></ref>
<ref id="sc19"><mixed-citation publication-type="journal"><string-name><surname>Van Raan</surname> <given-names>AFJ</given-names></string-name>, <string-name><surname>van Leeuwen</surname> <given-names>TN</given-names></string-name>, <string-name><surname>Visser</surname> <given-names>MS</given-names></string-name>, <string-name><surname>van Eck</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>Waltman</surname> <given-names>L</given-names></string-name> (<year>2010</year>) <article-title>Rivals for the crown: Reply to Opthof and Leydesdorff</article-title>. <source>J Informetr</source> <volume>4</volume>:<fpage>431</fpage>&#x2013;<lpage>435</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S1751157710000301">http://www.sciencedirect.com/science/article/pii/S1751157710000301</ext-link></mixed-citation></ref>
<ref id="sc20"><mixed-citation publication-type="journal"><string-name><surname>Vinkler</surname> <given-names>P</given-names></string-name> (<year>2003</year>) <article-title>Relations of relative scientometric indicators</article-title>. <source>Scientometrics</source> <volume>58</volume>:<fpage>687</fpage>&#x2013;<lpage>694</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.1023/B:SCIE.0000006888.69146.24">http://link.springer.com/10.1023/B:SCIE.0000006888.69146.24</ext-link></mixed-citation></ref>
<ref id="sc21"><mixed-citation publication-type="journal"><string-name><surname>Waltman</surname> <given-names>L</given-names></string-name>, <string-name><surname>van Eck</surname> <given-names>NJ</given-names></string-name> (<year>2012</year>) <article-title>A new methodology for constructing a publication-level classification system of science</article-title>. <source>J Am Soc Inf Sci Technol</source> <volume>63</volume>:<fpage>2378</fpage>&#x2013;<lpage>2392</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://doi.wiley.com/10.1002/asi.22748">http://doi.wiley.com/10.1002/asi.22748</ext-link></mixed-citation></ref>
<ref id="sc22"><mixed-citation publication-type="journal"><string-name><surname>Waltman</surname> <given-names>L</given-names></string-name>, <string-name><surname>van Eck</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>van Leeuwen</surname> <given-names>TN</given-names></string-name>, <string-name><surname>Visser</surname> <given-names>MS</given-names></string-name>, <string-name><surname>van Raan</surname> <given-names>AFJ</given-names></string-name> (<year>2011</year>) <article-title>Towards a new crown indicator: Some theoretical considerations</article-title>. <source>J Informetr</source> <volume>5</volume>:<fpage>37</fpage>&#x2013;<lpage>47</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://www.sciencedirect.com/science/article/pii/S1751157710000817">http://www.sciencedirect.com/science/article/pii/S1751157710000817</ext-link></mixed-citation></ref>
<ref id="sc23"><mixed-citation publication-type="journal"><string-name><surname>Zitt</surname> <given-names>M</given-names></string-name>, <string-name><surname>Small</surname> <given-names>H</given-names></string-name> (<year>2008</year>) <article-title>Modifying the journal impact factor by fractional citation weighting: The audience factor</article-title>. <source>J Am Soc Inf Sci Technol</source> <volume>59</volume>:<fpage>1856</fpage>&#x2013;<lpage>1860</lpage> Available at: <ext-link ext-link-type="uri" xlink:href="http://doi.wiley.com/10.1002/asi.20880">http://doi.wiley.com/10.1002/asi.20880</ext-link></mixed-citation></ref>
</ref-list>
</back>
</article>