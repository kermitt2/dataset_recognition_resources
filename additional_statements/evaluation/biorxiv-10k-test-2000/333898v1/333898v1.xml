<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/333898</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>An Event-related Potential Comparison of Facial Expression Processing between Cartoon and Real Faces</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8404-9906</contrib-id>
<name>
<surname>Zhao</surname>
<given-names>Jiayin</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wang</surname>
<given-names>Yifang</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>An</surname>
<given-names>Licong</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Beijing Key Laboratory of Learning and Cognition, College of Education, Capital Normal University</institution>, Beijing, <country>China</country></aff>
</contrib-group>
<pub-date pub-type="epub">
<year>2018</year>
</pub-date>
<elocation-id>333898</elocation-id>
<history>
<date date-type="received">
<day>29</day>
<month>5</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>29</day>
<month>5</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>29</day>
<month>5</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="333898.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Faces play important roles in the social lives of humans. In addition to real faces, people also encounter numerous cartoon faces in daily life. These cartoon faces convey basic emotional states through facial expressions. Using a behavioral research methodology and event-related potentials (ERPs), we conducted a facial expression recognition experiment with 17 university students to compare the processing of cartoon faces with that of real faces. This study used face type (real vs. cartoon) and participant gender (male vs. female) as independent variables. Reaction time, recognition accuracy, and the amplitudes and latencies of emotion processing-related ERP components such as N170, vertex positive potential (VPP), and late positive potential (LPP) were used as dependent variables. The ERP results revealed that cartoon faces caused larger N170 and VPP amplitudes as well as a briefer N170 latency than did real faces; that real faces induced larger LPP amplitudes than did cartoon faces; and that angry faces induced larger LPP amplitudes than did happy faces. In addition, the results showed a significant difference in the brain regions associated with face processing as reflected in a right hemispheric advantage. The behavioral results showed that the reaction times for happy faces were shorter than those for angry faces; that females showed a higher facial expression recognition accuracy than did males; and that males showed a higher recognition accuracy for angry faces than happy faces. These results demonstrate differences in facial expression recognition and neurological processing between cartoon faces and real faces among adults. Cartoon faces showed a higher processing intensity and speed than real faces during the early processing stage. However, more attentional resources were allocated for real faces during the late processing stage.</p>
</abstract>
<kwd-group kwd-group-type="author">
<title>Keywords</title>
<kwd>cartoon face</kwd>
<kwd>real face</kwd>
<kwd>facial expression</kwd>
<kwd>ERP</kwd>
<kwd>N170</kwd>
<kwd>LPP</kwd>
</kwd-group>
<counts>
<page-count count="32"/>
</counts>
</article-meta>
</front>
<body>
<p>Faces play important roles in human social life. They convey unique identity information and basic emotions through facial expressions. Facial expression refers to the various emotional states that people present through both the automatic and intentional control of the eye, facial, and oral muscles. In daily life, facial expressions provide important non-verbal forms of information and communication (<xref ref-type="bibr" rid="c3">Batty &#x0026; Taylor, 2003</xref>). As a non-verbal signal, facial expressions are an important basis and prerequisite for emotional communication as well as the generation of emotional experience. The ability to recognize a facial expression reflects an individual&#x2019;s ability to infer the psychological states of others through emotional expressions (<xref rid="c42" ref-type="bibr">Nelson, 1979</xref>). Facial expression recognition not only helps to determine internal emotional states (<xref ref-type="bibr" rid="c53">Thompson &#x0026; Meltzer, 1964</xref>) and the intentions conveyed by an individual but also provides feedback and induces social interactions (<xref ref-type="bibr" rid="c19">Erickson &#x0026; Schulkin, 2003</xref>). <xref ref-type="bibr" rid="c16">Ekman and Friesen (1978)</xref> summarized six basic human facial expressions including happiness, sadness, surprise, fear, anger, and disgust. These facial expressions have been identified and confirmed across different cultural contexts (<xref rid="c16" ref-type="bibr">Ekmm &#x0026; Friesen, 1971</xref>).</p>
<p>With the development of social communication, cartoons have entered people&#x2019;s lives. In addition to real faces, people also encounter many cartoon faces on daily basis. Moreover, common social networks (e.g., WeChat) provide various cartoon face emoji for communicating and expressing emotions. Compared with real faces, cartoon faces usually have larger eyes, smaller noses, and finer skin texture (<xref ref-type="bibr" rid="c49">Schindler, Zell, Botsch &#x0026; Kissler, 2017</xref>). <xref ref-type="bibr" rid="c11">Chen and colleagues (2010)</xref> found that people developed a preference for real faces with larger eyes after adaption to cartoon faces with unusually large eyes in Japanese cartoons. Some researchers compared cartoon faces and real faces with regard to recognition accuracy and reaction time. <xref ref-type="bibr" rid="c35">Kendall, Raffaelli, Kingstone, and Todd (2016)</xref> asked participants to identify emotions on five sets of briefly presented faces that ranged from photorealistic to fully iconic. The results showed stronger emotion recognition accuracy for cartoonized faces. In another study (<xref ref-type="bibr" rid="c54">Wang, Wang, Wang &#x0026; Lu, 2012</xref>), participants showed faster reaction times to real faces than cartoon faces when they were required to determine whether an image was a face or a car. However, research on the recognition of cartoon and real faces has shown mixed results. Using synthesized emotion images, <xref ref-type="bibr" rid="c29">Hoptman and Levy (1988)</xref> studied the processing preference of left- and right-handed individuals for cartoon and real faces. The results failed to reveal significant difference between cartoon and real faces.</p>
<p>Both cartoon and real faces convey emotional information through facial expression. The six basic facial expressions can be categorized as positive or negative expressions. Mixed results have been reported by research on the reaction times and recognition accuracies of positive and negative facial expressions. Some believe that reaction times for positive expressions are faster than those for other facial expressions. <xref ref-type="bibr" rid="c18">Eimer, Holmes and Mcglone (2003)</xref> found that recognition of happiness is faster than that of other basic facial expressions. In an identification task regarding whether facial expressions were neutral or emotional, participants showed the shortest reaction time for happiness, the longest reaction times for sadness, the lowest error rate for surprise, and the highest error rate for sadness. <xref ref-type="bibr" rid="c10">Calvo and Lundqvist (2008)</xref> asked participants to press different buttons for each of the six basic facial expressions and found similar results. However, they found that the recognition accuracy for happiness was highest, whereas the recognition accuracy for fear was lowest. Other studies have suggested people recognize negative facial expressions faster than positive ones. <xref ref-type="bibr" rid="c23">Hansen and Hansen (1988)</xref> found that the search speed for angry face targets was faster when both angry and happy faces served as targets and distractors. <xref ref-type="bibr" rid="c15">Eastwood, Smilek and Merikle (2001)</xref> reported that the search for negative facial expressions (sadness) was faster than positive facial expressions (happiness) when neutral faces were used as distractors.</p>
<p>The above experiments investigated the differences among different facial types and facial expressions from the perspective of behavioral science. Using this behavioral research as a basis, other researchers have also used event-related potentials (ERPs) to study the neurophysiological basis behind these differences. ERPs refer to the changes in the electrical potential of various brain regions when a stimulus is applied or removed to the sensory system or a certain part of the brain (<xref ref-type="bibr" rid="c56">Wei &#x0026; Luo, 2002</xref>). ERPs directly reflect electrical neurological activity. ERPs have been widely applied in face processing research because it provides high temporal resolution, real-time and non-invasive measurement, and a connection among stimulus events, psychological reactions, and brain activity. ERPs can be used to classify different visual stimulus and differentiate disparate emotional states. Without any participant response, ERP testing enables the measurement of emotional attitudes that people are unwilling to express (<xref ref-type="bibr" rid="c6">Bernat, Bunce &#x0026; Shevrin, 2001</xref>).</p>
<p>The ERP components related to faces and facial expressions include N170, vertex positive potential (VPP), late positive potential (LPP), and others. N170 is primarily distributed in the occipito-temporal region of the brain and usually shows a larger response in the right hemisphere (<xref ref-type="bibr" rid="c46">Rossion &#x0026; Jacques, 2008</xref>). N170 is a face-specific ERP component, and its peak shows face selectivity. N170 is only induced by face stimuli (i.e., not by furniture, cars, hand gestures, or other stimuli)(<xref rid="c5" ref-type="bibr">Bentin, Allison, Puce &#x0026; McCarthy, 1996</xref>). Related to face type, research has shown that the N170 components induced by real and cartoon faces do not significantly differ (<xref ref-type="bibr" rid="c54">Wang et al., 2012</xref>). Another study (<xref ref-type="bibr" rid="c47">Sagiv &#x0026; Bentin, 2001</xref>) showed that real faces induced a stronger N170 effect than abstract sketches of faces. Compared with schematic faces, however, the difference was not significant. Facial expressions are also related to N170 during early processing (<xref ref-type="bibr" rid="c21">Galli, Feurra &#x0026; Viggiano, 2006</xref>). A meta-analysis revealed that larger N170 amplitudes are associated with facial expressions of anger, fear, and happiness compared with neutral facial expressions (<xref rid="c26" ref-type="bibr">Hinojosa, Mercado &#x0026; Carreti&#x00E9;, 2015</xref>). <xref ref-type="bibr" rid="c45">Rellecke, Sommer, and Schacht (2012)</xref> required participants to explicitly or implicitly process happiness, anger, and neutral faces. Their results showed that emotional faces induced larger N170 amplitudes than did neutral faces under both processing conditions. With respect to different facial expressions, <xref ref-type="bibr" rid="c3">Batty and Taylor (2003)</xref> recorded the ERPs of participants responding to the six basic facial expressions and neutral expressions. The results showed that positive expressions resulted in shorter N170 latencies than negative expressions and that fear expressions induced significantly larger amplitudes than did other expressions.</p>
<p>N170 has a corresponding positive component at the mind-central sites, namely VPP. VPP and N170 have similar functional properties. They are two manifestations of the same brain processes (<xref rid="c33" ref-type="bibr">Joycea &#x0026; Rossion, 2005</xref>). VPP sometimes shows more sensitivity to facial expression information than N170, and VPP is influenced by facial expressions when N 170 is not (<xref ref-type="bibr" rid="c1">Ashley, Vuilleumier &#x0026; Swick, 2004</xref>).</p>
<p>Additional processing of emotional expression is reflected by the LPP component (<xref ref-type="bibr" rid="c9">Bublatzky, Gerdes, White, Riemer &#x0026; Alpers, 2014</xref>). LPP waves originate from the occipital lobe and the posterior parietal cortex (<xref ref-type="bibr" rid="c34">Keil et al., 2002</xref>), reflecting the cerebral cortex&#x2019;s evaluation of emotional stimuli, working memory characterization, decision making, and response-related processing (<xref rid="c50" ref-type="bibr">Schupp, Flaisch, Stockburger &#x0026; Junghofer, 2006</xref>). LPP waves are sensitive to various emotional stimuli including faces (<xref ref-type="bibr" rid="c20">Flaisch, H&#x00E4;cker, Renner &#x0026; Schupp, 2011</xref>; <xref ref-type="bibr" rid="c48">Schindler &#x0026; Kissler, 2016</xref>; <xref ref-type="bibr" rid="c51">Schupp, Jungh&#x00F6;fer, Weike &#x0026; Hamm, 2004</xref>; <xref ref-type="bibr" rid="c52">Steppacher, Schindler &#x0026; Kissler, 2015</xref>; <xref rid="c60" ref-type="bibr">Wieser, Pauli, Reicherts &#x0026; M&#x00FC;hlberger, 2010</xref>). Using the International Affective Picture System (IAPS) to examine explicit emotional processing, researchers found that images of emotional scenes induced larger LPP amplitudes than did neutral scenes (<xref ref-type="bibr" rid="c22">Hajcak, Moser &#x0026; Simons, 2006</xref>). However, the findings related to the influence of facial expression on LPP are not consistent. Although some reports have concluded that negative expressions (e.g., sadness) induce smaller amplitudes than do positive expressions (e.g., happiness) (<xref ref-type="bibr" rid="c25">Hietanen &#x0026; Astikainen, 2013</xref>), others have found that negative expressions induce larger LPP components than do positive expressions (<xref ref-type="bibr" rid="c64">Zhu &#x0026; Liu, 2014</xref>). In addition, other studies have not found significant differences between the processing of positive and negative expressions (<xref ref-type="bibr" rid="c12">Codispoti, Ferrari &#x0026; Bradley, 2006</xref>; <xref ref-type="bibr" rid="c44">Recio, Sommer &#x0026; Schacht, 2011</xref>). <xref rid="c36" ref-type="bibr">Krolak-Salmon, Fischer, Vighetto and Maugui&#x00E8;re (2001)</xref> asked participants to view images of different facial expressions (e.g., fear, happiness, disgust, surprise, and neutral expressions) and recorded their ERPs during two different tasks with the same stimuli. First, participants were instructed to focus on the gender of the faces by counting the number of males or females. Second, they were asked to focus on facial expressions by counting the number of faces that looked surprised. The results showed significant differences between late-latency ERPs to emotional faces and those to neutral faces (between 250 and 550 ms of latency). The activation was symmetric in the occipital lobe. The ERP components of different facial expressions differed between 550 and 750 ms on the right side of the occipital cerebral region. Topographic maps of these differences showed specific right temporal activity related to each emotional expression.Differences were also found with regard to the processing of different face types. Researchers studied adults&#x2019; ERP processing of real and cartoon faces with neutral expressions (<xref ref-type="bibr" rid="c54">Wang et al., 2012</xref>). The results indicated that real faces induce significantly higher average LPP amplitudes in the occipital and temporal regions than do cartoon faces. The processing of the cartoon faces showed obvious lateralization, primarily in the right parietooccipital area, whereas the processing of the real faces was primarily in the parietooccipital areas of both sides. <xref ref-type="bibr" rid="c49">Schindler et al. (2017)</xref> employed six face-stylization levels varying from abstract to realistic and investigated the difference in the processing of real and cartoon faces. The results showed that the LPP amplitude increased as the faces became more realistic. The above studies suggest that different face types and facial expressions induce significantly different ERP components and amplitudes in different brain regions.</p>
<p>In addition, other studies have investigated the influence of participant gender on the recognition of facial expressions. Hoffmann and colleagues (<xref ref-type="bibr" rid="c27">Hoffmann, Kessler, Eppel, Rukavina &#x0026; Traue, 2010</xref>)asked male and female participants to identify six basic but subtle facial emotions (50&#x0025; emotional content). The results showed that women were more accurate than men at recognizing subtle facial displays of anger, disgust, and fear, suggesting that women are more sensitive to negative emotions. This result might be related to the role of women throughout evolution. The treatment of emotions comes from a corresponding neural basis. Even if the influence of gender is not reflected in the accuracy or speed of facial expression recognition, differences remain between men and women with regard to neural activity. <xref ref-type="bibr" rid="c61">Wildgruber, Pihan, Ackermann, Erb &#x0026; Grodd (2002)</xref> found no behavioral difference between males and females with regard to differentiating happy from sad sounds. However, higher response amplitudes within the left-hemisphere posterior middle temporal gyrus were found among women compared with men, whereas a larger increase of activation within the right middle frontal gyrus was observed among the latter. <xref ref-type="bibr" rid="c24">Han, Gao, Humphreys &#x0026; Ge (2008)</xref> found significant differences in the behaviors and brain activities between men and women during emotion-related tasks. Women showed faster threat detection times, while men showed stronger posterior parietal activation.</p>
<p>In summary, the existing research suggests that the N170, VPP, and LPP ERP components are closely related to facial expression processing and that each component presents different properties when processing the unique facial expressions of real faces. However, consistent conclusions do not exist regarding the comparison of processing methods, speeds, and intensities between cartoon faces and real faces. With respect to facial expression selection, happiness is usually used as a positive expression, whereas anger and sadness are usually selected as negative expressions (<xref ref-type="bibr" rid="c49">Schindler et al., 2017</xref>; <xref ref-type="bibr" rid="c23">Hansen &#x0026; Hansen, 1988</xref>; <xref ref-type="bibr" rid="c15">Eastwood et al., 2001</xref>; <xref ref-type="bibr" rid="c5">Bentin et al., 1996</xref>; <xref ref-type="bibr" rid="c25">Hietanen &#x0026; Astikainen, 2013</xref>). The present study used anger and happiness for comparison because the recognition accuracy of anger is higher than that of sadness (<xref ref-type="bibr" rid="c18">Eimer et al., 2003</xref>). Moreover, significant differences exist between males and females with regard to anger recognition accuracy (<xref ref-type="bibr" rid="c10">Calvo &#x0026; Lundqvist, 2008</xref>; <xref ref-type="bibr" rid="c27">Hoffmann et al., 2010</xref>). The present study used an ERP methodology to investigate the processing of real and cartoon facial expressions among men and women. We hypothesized that (1) recognition time would be faster with regarding to a positive emotion (i.e., happiness) than a negative emotion (i.e., anger); (2) women would recognize facial expressions faster and more accurately than would men; (3) the late component LPP, but not N170 or VPP, would be affected by emotional valance; and (4) face type (i.e., real and cartoon faces) would influence the amplitudes and latencies of N170 and VPP as well as the amplitude of LPP.</p>
<sec id="s1">
<title>Method</title>
<sec id="s1a">
<title>Participants</title>
<p>We recruited 17 participants (11 males, 6 females; average age = 24.18, SD = 2.32) from universities in Beijing. All participants were right-handed, had normal hearing and vision (with or without correction), and no history of hearing, neurological, or psychiatric disorders. Participants signed an informed consent document prior to the experiment and were compensated for their time after the experiment. This study was approved by the ethical committee of Beijing Key Laboratory of Learning and Cognition of Capital Normal University.</p>
</sec>
<sec id="s1b">
<title>Materials</title>
<p>The pictures used in the experiment were selected from the Chinese Facial Affective Picture System (CFAPS; <xref ref-type="bibr" rid="c55">Wang and Luo, 2005</xref>) and the Japanese Female Facial Expression (JAFEE) database. Fifty pictures of happy faces (25 males and 25 females) and 50 pictures of angry faces (25 males and 25 females) were selected from the two picture databases. In total, 100 pictures were selected. We used MYOTee (a cartoon image editor) to convert these faces into cartoon faces. Subsequently, we used Photoshop to overlay the cartoon faces onto the original pictures for fine-tuning, and we retained the same face structure and hairstyle to synthesize 100 cartoon facial expression pictures. In total, 200 pictures were used in this experiment. All pictures were presented in black and white with a resolution of 260 &#x00D7; 300 at a consistent contrast (<xref rid="fig1" ref-type="fig">Figure 1</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1.</label>
<caption><p>Real and cartoon example faces</p></caption>
<graphic xlink:href="333898_fig1.tif"/>
</fig>
<p>Twenty additional volunteers (non-participants; mean age = 25.3 years) evaluated the pictures. The evaluation included the identification of facial expression type (i.e., by pressing the &#x201C;G&#x201D; key for happiness and the &#x201C;F&#x201D; key for anger) and a Likert rating of the facial emotion (9 = extremely happy or angry; 1 = not at all happy or angry). The evaluation results revealed a recognition accuracy of 95.9&#x0025;, with an emotion intensity rating of 4.81 &#x00B1; 1.91 (<xref rid="tbl1" ref-type="table">Table 1</xref>). Therefore, all 200 pictures were retained as stimuli for the experiment.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><p>Means (and Standard Deviations) for Accuracy of Facial Expression Recognition and Valuation of Emotional Intensity</p></caption>
<graphic xlink:href="333898_tbl1.tif"/>
</table-wrap>
</sec>
<sec id="s1c">
<title>Procedure</title>
<p>The experiment was conducted in a quiet and dimly lit laboratory. The stimulus images were presented on a 16-inch CRT monitor with a screen resolution of 1920 x 1080. Participants were required to complete facial expression identification tasks according to instructions presented on the monitor. Their electroencephalogram (EEG) data were collected during the experiment. For each trial, a focus point was presented for 1,000 ms. Subsequently, a facial image was presented, and the participant was required to determine whether the face was happy or angry by pressing a button (happy = 1; angry = 2) within 1,000 ms. If a button was pressed within 1,000 ms, then the picture disappeared, and a blank screen was presented until the next picture appeared. If no button was pressed, then the picture disappeared after 1,000 ms, and a blank screen was presented until the next picture appeared. The duration of the blank screens varied randomly from 900 ms to 1,700 ms. <xref rid="fig2" ref-type="fig">Figure 2</xref> shows the experimental procedure. The experiment was divided into two blocks, each with 100 trials. The pictures within each block were balanced. Participants were given 2-3 min to rest between blocks.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2.</label>
<caption><p>Task timing and example stimulus</p></caption>
<graphic xlink:href="333898_fig2.tif"/>
</fig>
</sec>
<sec id="s1d">
<title>ERP measurement and data processing</title>
<p>We used a 64-lead EEG/ERP system (Electrical Geodesics, Inc., U.S.) for measurement and a potassium chloride solution as a conductive medium (skin impedance &#x003C; 70 K&#x03A9;, A/D conversion = 24 bits, input impedance = 200 M&#x03A9;, sampling rate = 20 kHz, sampling range = &#x00B1; 200 mV, amplifier noise = 1.4 &#x03BC;Vpp, common mode rejection ratio = 120 dB, bandwidth = 0 &#x223C; 4,000 Hz). Based on the overall mean chart, the early ERP components (N170 and VPP) generated by the stimuli showed clear peaks. A time window of 125-195 ms was used to measure the ERP peak and peak latency data collected at electrode locations in the left and right hemispheres (P7 and P8). For LPP, the average amplitude was calculated using the occipital lobe (O1, OZ, and O2), central zone (C3, CZ, and C4), and parietal lobe (P3, PZ, and P4) using a time window of 450-650 ms. Data processing, artifact elimination, and correction were performed.</p>
</sec>
<sec id="s1e">
<title>Statistical analyses</title>
<p>The behavioral and EEG data were analyzed using IBM SPSS 19.0 for Windows. N170 was analyzed using a 2 (face type: cartoon vs. real face) x 2 (emotional valance: happiness vs. anger) x 2 (gender: male vs. female) x 2 (hemisphere: left vs. right) repeated-measures analysis of variance (ANOVA). For VPP, a 2 (face type: cartoon vs. real face) x 2 (emotional valence: happiness vs. anger) x 2 (gender: male vs. female) repeated-measures ANOVA was used. For LPP, a 2 (face type: cartoon vs. real face) x 2 (emotion valence: happiness vs. anger) x 2 (gender: male vs. female) x 3 (brain region: occipital vs. central area vs. parietal) x 3 (lateralization: left vs. middle vs. right) repeated measures ANOVA was used.</p>
</sec>
</sec>
<sec id="s2">
<title>Result</title>
<sec id="s2a">
<title>Behavioral Performance</title>
<p>Mean response time (RT) and accuracy and standard deviations are shown in <xref rid="tbl2" ref-type="table">Table 2</xref>. Repeated-measures ANOVAs, with factors gender (male, female), face type (real, cartoon), emotion valence (happy, angry) as independent variables, with RT and accuracy as dependent variables, were conducted. For RT, a significant effect of emotion valence, F (1, 15) = 4.95, <italic>p</italic> &#x003C; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline1.gif"/></alternatives></inline-formula>, revealed that RT was faster for happy face than angry face. The main effects of gender and face type did not reach significance, F (1, 15) = 2.40, <italic>p</italic> &#x003E; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline2.gif"/></alternatives></inline-formula>, F (1, 15) = 2.17, <italic>p</italic> &#x003E;0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline3.gif"/></alternatives></inline-formula>, neither did all interactions (<italic>ps</italic> &#x003E; 0.05).</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><p>Means (and Standard Deviations) for Response Time (RT), Accuracy</p></caption>
<graphic xlink:href="333898_tbl2.tif"/>
</table-wrap>
<p>For accuracy, a significant effect of gender, F (1, 15) = 5.38, <italic>p</italic> &#x003C; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline4.gif"/></alternatives></inline-formula>, indicated that the female performed better than the male. Results also showed a significant interaction between gender and emotion valence, F (1, 15) = 5.50, <italic>p</italic> &#x003C; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline5.gif"/></alternatives></inline-formula>. Follow-up simple effect analysis showed that for the male, performance was better in condition of angry face than happy face, F (1, 15) = 10.48, <italic>p</italic> &#x003C; 0.01, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline6.gif"/></alternatives></inline-formula>; for the female, accuracy did not differ for happy face versus angry face, F (1, 15) = 0.03, <italic>p</italic> &#x003E; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline7.gif"/></alternatives></inline-formula>. The main effects of face type and emotion valence did not reach significance, F (1, 15) = 0.34, <italic>p</italic> &#x003E; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline8.gif"/></alternatives></inline-formula>, F (1, 15) = 4.40, <italic>p</italic> &#x003E; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline9.gif"/></alternatives></inline-formula>, neither did other interactions (<italic>ps</italic> &#x003E; 0.05).</p>
</sec>
<sec id="s2b">
<title>N170</title>
<p>Mean amplitudes and latency of N170 and VPP and standard deviations are shown in <xref rid="tbl3" ref-type="table">Table 3</xref>. Repeated-measures ANOVAs, with factors gender (male, female), face type (real, cartoon), emotion valence (happy, angry), lateralization (P7 for left hemisphere, P8 for right hemisphere) as independent variables, with amplitudes and latency as dependent variables, were conducted. For amplitudes, a significant effect of face type, F (1, 13) = 34.77, <italic>p</italic> &#x003C; 0.01, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline10.gif"/></alternatives></inline-formula>, revealed that amplitudes were bigger for cartoon face than real face. N170 mean amplitudes from electrode P7 and P8 are shown in <xref rid="fig3" ref-type="fig">Figure 3</xref>. Results also showed a significant interaction between face type and lateralization, F (1, 13) = 6.43, <italic>p</italic> &#x003C; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline11.gif"/></alternatives></inline-formula>. Differences between the amplitudes elicited by cartoon and real faces were bigger in right hemisphere (2.98&#x00B1;0.47&#x03BC;V) than left hemisphere (1.25&#x00B1;0.52&#x03BC;V). The main effects of gender, emotion valence and lateralization did not reach significance, F (1, 13) = 0.14, <italic>p</italic> &#x003E; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline12.gif"/></alternatives></inline-formula>, F (1, 13) = 0.97, <italic>p</italic> &#x003E; 0.05,<inline-formula><alternatives><inline-graphic xlink:href="333898_inline13.gif"/></alternatives></inline-formula>, F (1, 13) = 1.85, <italic>p</italic> &#x003E; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline14.gif"/></alternatives></inline-formula>, neither did other interactions (<italic>ps</italic> &#x003E; 0.05).</p>
<table-wrap id="tbl3" orientation="portrait" position="float">
<label>Table 3.</label>
<caption><p>Means (and Standard Deviations) for Amplitudes and Latency of N170 and VPP</p></caption>
<graphic xlink:href="333898_tbl3.tif"/>
</table-wrap>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3.</label>
<caption><p>N170 mean amplitudes from electrode P7 and P8</p></caption>
<graphic xlink:href="333898_fig3.tif"/>
</fig>
<p>For latency, a significant effect of face type, F (1, 13) = 15.16, <italic>p</italic> &#x003C; 0.01, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline15.gif"/></alternatives></inline-formula>, indicated that latency was longer for real face than cartoon face. Results also showed a significant interaction between face type and lateralization, F (1, 13) = 5.22, <italic>p</italic> &#x003C; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline16.gif"/></alternatives></inline-formula>. Follow-up simple effect analysis showed that for right hemisphere, latency was longer in condition of real face than cartoon face, F(1, 13) = 26.69, <italic>p</italic> &#x003C; 0.01, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline17.gif"/></alternatives></inline-formula>; for left hemisphere, latency did not differ between real face and cartoon face, F (1, 13) = 1.35, <italic>p</italic> &#x003E; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline18.gif"/></alternatives></inline-formula>. The main effects of gender, emotion valence and lateralization did not reach significance, F (1, 13) = 2.58, <italic>p</italic> &#x003E; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline19.gif"/></alternatives></inline-formula>, F (1, 13) = 0.07, <italic>p</italic> &#x003E; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline20.gif"/></alternatives></inline-formula>, F (1, 13) = 0.17, <italic>p</italic> &#x003E; 0.05,<inline-formula><alternatives><inline-graphic xlink:href="333898_inline21.gif"/></alternatives></inline-formula>, neither did other interactions (<italic>ps</italic> &#x003E; 0.05).</p>
</sec>
<sec id="s2c">
<title>VPP</title>
<p>Repeated-measures ANOVAs, with factors gender (male, female), face type (real, cartoon), emotion valence (happy, angry) as independent variables, with amplitudes and latency as dependent variables, were conducted. For amplitudes, a significant effect of face type, F (1, 13) = 4.94, <italic>p</italic> &#x003C; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline22.gif"/></alternatives></inline-formula>, revealed that amplitudes were bigger for cartoon face than real face. The main effects of gender and emotion valence did not reach significance, F (1, 13) = 4.44, <italic>p</italic> &#x003E; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline23.gif"/></alternatives></inline-formula>, F (1, 13) = 3.33, <italic>p</italic> &#x003E; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline24.gif"/></alternatives></inline-formula>, neither did all interactions (<italic>ps</italic>&#x003E;0.05).</p>
<p>For latency, results showed that interaction between face type and emotion valence was significant, F (1, 13) = 6.28, <italic>p</italic> &#x003C; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline25.gif"/></alternatives></inline-formula>. Follow-up simple effect analysis found no significant effect. The main effects of gender, face type and emotion valence did not reach significance, F (1, 13) = 0.39, <italic>p</italic> &#x003E; 0.05,<inline-formula><alternatives><inline-graphic xlink:href="333898_inline26.gif"/></alternatives></inline-formula>, F (1, 13) = 1.33, <italic>p</italic> &#x003E; 0.05,<inline-formula><alternatives><inline-graphic xlink:href="333898_inline27.gif"/></alternatives></inline-formula>, F (1, 13) = 0.46, <italic>p</italic> &#x003E; 0.05,<inline-formula><alternatives><inline-graphic xlink:href="333898_inline28.gif"/></alternatives></inline-formula>, neither did other interactions (<italic>ps</italic> &#x003E; 0.05).</p>
</sec>
<sec id="s2d">
<title>LPP</title>
<p>Repeated-measures ANOVAs, with factors gender (male, female), face type (real, cartoon), emotion valence (happy, angry), braion region (top, central, occipital), lateralization (left, central, right) as independent variables, with amplitudes as dependent variables, were conducted. A significant effect of emotion valence, F (1, 13) = 4.38, <italic>p</italic> &#x003C; 0.05,<inline-formula><alternatives><inline-graphic xlink:href="333898_inline29.gif"/></alternatives></inline-formula>, revealed that amplitudes were bigger for angry face (1.68&#x00B1; 0.40&#x03BC;V) than happy face (1.02&#x00B1;0.52&#x03BC;V). Results showed a significant effect of face type, F (1, 13) = 6.47, <italic>p</italic> &#x003C; 0.05,<inline-formula><alternatives><inline-graphic xlink:href="333898_inline30.gif"/></alternatives></inline-formula>, revealing that amplitudes were bigger for real face (2.02&#x00B1;0.49&#x03BC;V) than cartoon face (0.69&#x00B1;0.52&#x03BC;V). Another significant effect of lateralization, F (2, 12) = 10.38, <italic>p</italic> &#x003C; 0.01, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline31.gif"/></alternatives></inline-formula>, revealed that amplitudes were bigger for central (2.13&#x00B1;0.49&#x03BC;V) than left (0.72&#x00B1;0.51&#x03BC;V), and that amplitudes were bigger for right (1.21&#x00B1;0.56&#x03BC;V) than left. A significant effect of region, F (2, 12) = 12.23, <italic>p</italic> &#x003C; 0.01,<inline-formula><alternatives><inline-graphic xlink:href="333898_inline32.gif"/></alternatives></inline-formula>, revealed that amplitudes were bigger for central area (1.68&#x00B1;0.40&#x03BC;V) than occipital area (1.02&#x00B1;0.52&#x03BC;V), and that amplitudes were bigger for parietal area (1.82&#x00B1;0.46&#x03BC;V) than occipital area. Results also showed a significant interaction between region and lateralization, F (2, 12) = 5.20, <italic>p</italic> &#x003C; 0.05, <inline-formula><alternatives><inline-graphic xlink:href="333898_inline33.gif"/></alternatives></inline-formula>. Follow-up simple effect analysis showed that for occipital area, amplitudes were bigger in condition of middle area than left and right hemispheres; for parietal area, amplitudes did not differ between hemispheres; for central area, amplitudes were bigger in condition of middle area and right hemisphere than left hemisphere. LPP mean amplitudes over central, occipital and parietal electrode are shown in <xref rid="fig4" ref-type="fig">Figure 4</xref>.</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4.</label>
<caption><p>LPP mean amplitudes over central, occipital and parietal electrode</p></caption>
<graphic xlink:href="333898_fig4.tif"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<sec id="s3a">
<title>Processing differences between cartoon and real faces</title>
<p>The differences in the processing of the two face types were primarily reflected by the amplitudes and latencies of N170, VPP, and LPP. Although no significant difference was found with regard to reaction time, cartoon faces resulted in shorter N170 latencies than did real faces. This finding is not consistent with previous studies. <xref ref-type="bibr" rid="c54">Wang et al. (2012)</xref> found that adults&#x2019; N170 latencies were shorter when viewing real faces than cartoon faces. These inconsistent results might have been caused by the differences in stimulus materials. In Wang&#x2019;s study, real faces were collected from kindergarten children (mean age = &#x223C;6 years), and cartoon faces were obtained from screenshots of high-resolution DVD cartoons. In the present study, the real face stimuli were collected from adult facial expression databases (CFAPS and JAFFE), and the cartoon faces were converted from these real faces using MYOTee. The own-age bias (OAB, <xref ref-type="bibr" rid="c62">Wright &#x0026; Stroud,2002</xref>) might have caused these findings to differ from those of previous studies. The OAB states that people show better performance when recognizing faces from their own age group compared with other age groups. While both studies used adult participants, the age of viewed faces was closer to the participant age group in the present study.</p>
<p>A difference was also found in the N170 and VPP amplitudes during the early processing of real and cartoon faces. Our findings suggest a significant difference in brain processing intensity for the two face types during early processing. Cartoon faces were associated with significantly higher amplitudes than real faces. Previous studies (<xref ref-type="bibr" rid="c54">Wang et al., 2012</xref>; <xref ref-type="bibr" rid="c47">Sagiv &#x0026; Bentin, 2001</xref>) have indicated that real faces induce larger N170 and VPP amplitudes than do cartoon faces. The current different result might be because of the different levels of abstraction of the cartoon faces in the different studies. <xref ref-type="bibr" rid="c49">Schindler et al. (2017)</xref> employed six face-stylization levels varying from abstract to realistic and investigated the difference in the processing of real and cartoon faces. The results revealed a U-shape relationship between N170 and face realism. That is, both the most abstract and most realistic faces caused stronger reactions compared with medium-stylized faces.</p>
<p>The cartoon faces used in this study were converted from real faces using MYOTee. They are more simplified and abstract and, therefore, might have resulted in stronger N170 amplitudes. In addition, <xref ref-type="bibr" rid="c43">Proverbio, Riva, Martin and Zani (2010)</xref> found that infant faces elicited higher N170 amplitudes than did adult faces, most likely because of juvenile characteristics such as the larger proportion of the eyes. In the present study, the eyes of the cartoon faces were much larger than those of real people (including infants).</p>
<p>Real and cartoon faces also differed in LPP amplitude. The LPP amplitudes induced by real faces were significantly larger than those induced by cartoon faces. This finding is consistent with those of previous studies. When the neutral expressions of real faces and puppet faces were compared, no differences in N170 were observed. However, a stronger LPP was found with regard to real faces starting at 400 ms (<xref ref-type="bibr" rid="c37">Ma, Qian, Hu &#x0026; Wang, 2017</xref>). This effect is because of the significance and uniqueness of the real face as well as the understanding of the portrayed individual (<xref ref-type="bibr" rid="c59">Wheatley, Weinberg, Looser, Moran &#x0026; Hajcak, 2011</xref>) because computer-generated faces are usually more difficult to remember (<xref ref-type="bibr" rid="c2">Balas &#x0026; Pacella, 2015</xref>; <xref ref-type="bibr" rid="c13">Crookes et al., 2015</xref>). <xref ref-type="bibr" rid="c8">Bruce and Young (1986)</xref> considered facial feature encoding and identify recognition as the second stage of face recognition. This stage includes the accurate processing of facial information such as age, gender, race, and facial expression. Adults invest more psychological resources to real faces during late face processing. Compared with simplified cartoon faces, real faces convey more personal information and social meaning. LPP is related to facial attractiveness (<xref ref-type="bibr" rid="c37">Ma et al., 2017</xref>; <xref ref-type="bibr" rid="c38">Marzi &#x0026; Viggiano, 2010</xref>; <xref ref-type="bibr" rid="c57">Werheid, Schacht &#x0026; Sommer, 2007</xref>). Therefore, the results of the present study might suggest that real faces are more attractive than simplified cartoon faces to adults.</p>
<p>Significant differences were found among various brain regions. LPP amplitudes in the central and parietal areas were greater than those in the occipital area, indicating that the central and parietal areas are the major brain regions for face processing, which is consistent with Bauer&#x2019;s two-route model (1984). This model assumes that two routes exist for face recognition: the ventral route and the dorsal route. The dorsal route is primarily responsible for detecting the meaning of a face, beginning in the visual cortex, passing through the upper temporal sulcus, lower parietal lobe, and cingulate gyrus, eventually reaching the limbic system. In addition, the LPP amplitudes in the middle area and the right hemisphere were higher than those in the left hemisphere, suggesting a right hemisphere advantage. With regard to the N170 component, cartoon faces caused significantly smaller latencies than did real faces in the right hemisphere. However, no difference was observed in the left hemisphere. This finding is consistent with previous studies reporting a right hemisphere advantage with regard to face recognition intensity and speed (<xref ref-type="bibr" rid="c63">Yovel, Levy, Grabowecky &#x0026; Paller, 2003</xref>). Furthermore, cartoon face processing showed obvious lateralization, primarily in the right hemisphere, whereas real face processing is bilateral (<xref ref-type="bibr" rid="c54">Wang et al., 2012</xref>).</p>
</sec>
<sec id="s3b">
<title>Differences between angry and happy faces in face processing</title>
<p>The ERP results revealed that LPP amplitudes distinguished emotion type during late processing. Angry faces induced larger positive LPP waves than did happy faces. This finding is consistent with <xref ref-type="bibr" rid="c64">Zhu and Liu (2014)</xref>, who found that negative facial expressions were associated with significantly larger LPP amplitudes than were positive and neutral expressions. Previous studies (<xref ref-type="bibr" rid="c28">Holmes, Green &#x0026; Vuilleumier, 2005</xref>; <xref ref-type="bibr" rid="c58">Whalen et al., 1998</xref>; <xref ref-type="bibr" rid="c7">Bradley, Mogg &#x0026; Lee, 1997</xref>) reported bias in individual attention allocation when recognizing different facial expressions, especially those of negative emotions. Facial expression type did not influence N170. According to the face processing model of <xref ref-type="bibr" rid="c8">Bruce and Young (1986)</xref>, facial expression processing is independent from facial structural processing. That is, facial emotion information should not influence N170. The research of <xref ref-type="bibr" rid="c18">Eimer et al. (2003)</xref> and Ashley, Vuilleumier snd Swick (2004) support this hypothesis.</p>
</sec>
<sec id="s3c">
<title>Differences between men and women with regard to facial expression processing</title>
<p>Women showed a higher facial expression recognition accuracy than did men. Extensive research (<xref rid="c32" ref-type="bibr">Hall, Hutton &#x0026; Morgan, 2010</xref>; Brewster, Mullin, Dobrin &#x0026; Steeves, 2011; <xref ref-type="bibr" rid="c39">Mcbain, Norton &#x0026; Chen, 2009</xref>; <xref ref-type="bibr" rid="c40">Megreya, Bindemann &#x0026; Havard, 2011</xref>) has demonstrated that women have a face recognition advantage over men. In addition, an interaction effect was found between facial expression type and gender. Men showed a higher accuracy for recognizing angry faces, whereas women showed no difference between the two expression types. Previous studies did not find this interaction. Overall, the recognition accuracy of happy faces was higher for both genders compared with angry faces. The high accuracy of angry face recognition among men might be because they are more physically aggressive than women (<xref ref-type="bibr" rid="c31">Glascock, 2008</xref>; <xref ref-type="bibr" rid="c14">Eagly &#x0026; Steffen, 1986</xref>). High aggressiveness is positively correlated with serum testosterone concentration (<xref ref-type="bibr" rid="c30">Hu &#x0026; Li, 2014</xref>). Therefore, men are likely more sensitive to social signals that convey aggressiveness.</p>
</sec>
<sec id="s3d">
<title>Advantages and limitations</title>
<p>Our selected stimuli (i.e., the cartoon and real facial expression pictures) represent the advantage of this study. One important advantage is that the cartoon faces were converted from real faces and therefore retain the same facial structure and hairstyle. Most of the existing research has used screenshots of cartoon characters or sketched faces and expression icons (<xref ref-type="bibr" rid="c54">Wang et al., 2012</xref>; <xref ref-type="bibr" rid="c15">Eastwood et al., 2001</xref>; <xref ref-type="bibr" rid="c47">Sagiv &#x0026; Bentin, 2001</xref>); therefore, they cannot exclude nuanced information other than facial expressions compared with real faces. Another advantage is that all of the images used were of Asian adults, which prevented the introduction of cultural and age differences that might have been caused by use of western emotional faces. One limitation of this study is its limited sample size, which might have resulted in the large standard deviation in VPP latency. Although the interaction effect of face type x emotion valence was significant, the simple effects were not. Future research should increase the sample size to examine the interaction of cartoon and real faces with regard to facial expression type. Another limitation is that although ERP is advantageous for its temporal resolution, its special resolution is low. Future research should apply fMRI, which has a high spatial resolution. Finally, children are the primary audience for cartoons. Childhood is an important stage for developing emotional cognition. Based on the present study, future studies should compare adults and children with regard to the processing of cartoon and real facial expressions. This line of research might help draw a clearer picture of the developmental process associated with cartoon face processing.</p>
</sec>
<sec id="s3e">
<title>Conclusions</title>
<p>We used ERPs to measure the brain activity responses induced by the facial expressions of cartoon and real faces. According to the neurophysiological evidence in this study, face type has a strong but heterogeneous effect on the N170, VPP, and LPP components. During the early processing stage, adults process cartoon faces faster than real faces. However, adults allocate more attentional resources for real face processing during late processing stage. Facial expression type influenced late-stage component LPP, showing attentional bias for negative emotions; however, early-state N170 was not influenced. Future research should use larger sample sizes to examine the interaction between face type (real vs. cartoon) and facial expression.</p>
</sec>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Ashley</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Vuilleumier</surname>, <given-names>P.</given-names></string-name>, &#x0026; <string-name><surname>Swick</surname>, <given-names>D.</given-names></string-name> (<year>2004</year>). <article-title>Time course and specificity of event-related potentials to emotional expressions</article-title>. <source>Neuroreport</source>, <volume>15</volume>(<issue>1</issue>), <fpage>211</fpage>&#x2013;<lpage>216</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="journal"><string-name><surname>Balas</surname>, <given-names>B.</given-names></string-name>, &#x0026; <string-name><surname>Pacella</surname>, <given-names>J.</given-names></string-name> (<year>2015</year>). <article-title>Artificial faces are harder to remember</article-title>. <source>Computers in Human Behavior</source>, <volume>52</volume>, <fpage>331</fpage>&#x2013;<lpage>337</lpage>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Batty</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Taylor</surname>, <given-names>M. J.</given-names></string-name> (<year>2003</year>). <article-title>Early processing of the six basic facial emotional expressions</article-title>. <source>Brain Research Cognitive Brain Research</source>, <volume>17</volume>(<issue>3</issue>), <fpage>613</fpage>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="journal"><string-name><surname>Bauer</surname>, <given-names>R. M.</given-names></string-name> (<year>1984</year>). <article-title>Autonomic recognition of names and faces in prosopagnosia: a neuropsychological application of the guilty knowledge test</article-title>. <source>Neuropsychologia</source>, <volume>22</volume>(<issue>4</issue>), <fpage>457</fpage>&#x2013;<lpage>469</lpage>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Bentin</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Allison</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Puce</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Perez</surname>, <given-names>E.</given-names></string-name>, &#x0026; <string-name><surname>Mccarthy</surname>, <given-names>G.</given-names></string-name> (<year>1996</year>). <article-title>Electrophysiological studies of face perception in humans</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>8</volume>(<issue>6</issue>), <fpage>551</fpage>&#x2013;<lpage>565</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Bernat</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Bunce</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Shevrin</surname>, <given-names>H.</given-names></string-name> (<year>2001</year>). <article-title>Event-related brain potentials differentiate positive and negative mood adjectives during both supraliminal and subliminal visual processing</article-title>. <source>International Journal of Psychophysiology Official Journal of the International Organization of Psychophysiology</source>, <volume>42</volume>(<issue>1</issue>), <fpage>11</fpage>&#x2013;<lpage>34</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Bradley</surname>, <given-names>B. P.</given-names></string-name>, <string-name><surname>Mogg</surname>, <given-names>K.</given-names></string-name>, &#x0026; <string-name><surname>Lee</surname>, <given-names>S. C.</given-names></string-name> (<year>1997</year>). <article-title>Attentional biases for negative information in induced and naturally occurring dysphoria</article-title>. <source>Behaviour Research &#x0026; Therapy</source>, <volume>35</volume>(<issue>10</issue>), <fpage>911</fpage>&#x2013;<lpage>27</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Bruce</surname>, <given-names>V.</given-names></string-name>, &#x0026; <string-name><surname>Young</surname>, <given-names>A.</given-names></string-name> (<year>1986</year>). <article-title>Understanding face recognition</article-title>. <source>British Journal of Psychology</source>, <volume>77</volume>(<issue>3</issue>), <fpage>305</fpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Bublatzky</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Gerdes</surname>, <given-names>A. B.</given-names></string-name>, <string-name><surname>White</surname>, <given-names>A. J.</given-names></string-name>, <string-name><surname>Riemer</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Alpers</surname>, <given-names>G. W.</given-names></string-name> (<year>2014</year>). <article-title>Social and emotional relevance in face processing: happy faces of future interaction partners enhance the late positive potential</article-title>. <source>Frontiers in Human Neuroscience</source>, <volume>8</volume>, <fpage>1</fpage>&#x2013;<lpage>10</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Calvo</surname>, <given-names>M. G.</given-names></string-name>, &#x0026; <string-name><surname>Lundqvist</surname>, <given-names>D.</given-names></string-name> (<year>2008</year>). <article-title>Facial expressions of emotion (kdef): identification under different display-duration conditions</article-title>. <source>Behav Res Methods</source>, <volume>40</volume>(<issue>1</issue>), <fpage>109</fpage>&#x2013;<lpage>115</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="journal"><string-name><surname>Chen</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Russell</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Nakayama</surname>, <given-names>K.</given-names></string-name>, &#x0026; <string-name><surname>Livingstone</surname>, <given-names>M.</given-names></string-name> (<year>2010</year>). <article-title>Crossing the &#x2018;uncanny valley&#x2019;: adaptation to cartoon faces can influence perception of human faces</article-title>. <source>Perception</source>, <volume>39</volume>(<issue>3</issue>), <fpage>378</fpage>&#x2013;<lpage>386</lpage>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="journal"><string-name><surname>Codispoti</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ferrari</surname>, <given-names>V.</given-names></string-name>, &#x0026; <string-name><surname>Bradley</surname>, <given-names>M. M.</given-names></string-name> (<year>2006</year>). <article-title>Repetitive picture processing: autonomic and cortical correlates</article-title>. <source>Brain Research</source>, <volume>1068</volume>(<issue>1</issue>), <fpage>213</fpage>&#x2013;<lpage>220</lpage>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Crookes</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Ewing</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Gildenhuys</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Kloth</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Hayward</surname>, <given-names>W. G.</given-names></string-name>, &#x0026; <string-name><surname>Oxner</surname>, <given-names>M.</given-names></string-name>, <etal>et al.</etal> (<year>2015</year>). <article-title>How well do computer-generated faces tap face expertise?</article-title>. <source>Plos One</source>, <volume>10</volume>(<issue>11</issue>), <fpage>e0141353</fpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Eagly</surname>, <given-names>A. H.</given-names></string-name>, &#x0026; <string-name><surname>Steffen</surname>, <given-names>V. J.</given-names></string-name> (<year>1986</year>). <article-title>Gender and aggressive behavior: a meta-analytic review of the social psychological literature</article-title>. <source>Psychological Bulletin</source>, <volume>100</volume>(<issue>3</issue>), <fpage>309</fpage>&#x2013;<lpage>330</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Eastwood</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Smilek</surname>, <given-names>D.</given-names></string-name>, &#x0026; <string-name><surname>Merikle</surname>, <given-names>P. M.</given-names></string-name> (<year>2001</year>). <article-title>Differential attentional guidance by unattended faces expressing positive and negative emotion</article-title>. <source>Perception &#x0026; Psychophysics</source>, <volume>63</volume>(<issue>6</issue>), <fpage>1004</fpage>&#x2013;<lpage>1013</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Ekman</surname>, <given-names>P.</given-names></string-name>, &#x0026; <string-name><surname>Friesen</surname>, <given-names>W. V.</given-names></string-name> (<year>1978</year>). <article-title>Facial action coding system (facs): a technique for the measurement of facial actions</article-title>. <source>Rivista Di Psichiatria</source>,<volume>47</volume>(<issue>2</issue>), <fpage>126</fpage>&#x2013;<lpage>38</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Ekman</surname>, <given-names>P.</given-names></string-name>, &#x0026; <string-name><surname>Friesen</surname>, <given-names>W. V.</given-names></string-name> (<year>1971</year>). <article-title>Constants across cultures in the face and emotion</article-title>. <source>Journal of Personality &#x0026; Social Psychology</source>, <volume>17</volume>(<issue>2</issue>), <fpage>124</fpage>&#x2013;<lpage>129</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Eimer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Holmes</surname>, <given-names>A.</given-names></string-name>, &#x0026; <string-name><surname>Mcglone</surname>, <given-names>F. P.</given-names></string-name> (<year>2003</year>). <article-title>The role of spatial attention in the processing of facial expression: an erp study of rapid brain responses to six basic emotions</article-title>. <source>Cognitive Affective &#x0026; Behavioral Neuroscience</source>, <volume>3</volume>(<issue>2</issue>), <fpage>97</fpage>&#x2013;<lpage>110</lpage>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Erickson</surname>, <given-names>K.</given-names></string-name>, &#x0026; <string-name><surname>Schulkin</surname>, <given-names>J.</given-names></string-name> (<year>2003</year>). <article-title>Facial expressions of emotion: a cognitive neuroscience perspective</article-title>. <source>Brain &#x0026; Cognition</source>, <volume>52</volume>(<issue>1</issue>), <fpage>52</fpage>&#x2013;<lpage>60</lpage>.</mixed-citation></ref>
<ref id="c20"><mixed-citation publication-type="journal"><string-name><surname>Flaisch</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>H&#x00E4;cker</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Renner</surname>, <given-names>B.</given-names></string-name>, &#x0026; <string-name><surname>Schupp</surname>, <given-names>H. T.</given-names></string-name> (<year>2011</year>). <article-title>Emotion and the processing of symbolic gestures: an event-related brain potential study</article-title>. <source>Social Cognitive &#x0026; Affective Neuroscience</source>, <volume>6</volume> (<issue>1</issue>), <fpage>109</fpage>.</mixed-citation></ref>
<ref id="c21"><mixed-citation publication-type="journal"><string-name><surname>Galli</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Feurra</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Viggiano</surname>, <given-names>M. P.</given-names></string-name> (<year>2006</year>). <article-title>&#x201C;did you see him in the newspaper?&#x201D; electrophysiological correlates of context and valence in face processing</article-title>. <source>Brain Research</source>, <volume>1119</volume> (<issue>1</issue>), <fpage>190</fpage>&#x2013;<lpage>202</lpage>.</mixed-citation></ref>
<ref id="c22"><mixed-citation publication-type="journal"><string-name><surname>Hajcak</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Moser</surname>, <given-names>J. S.</given-names></string-name>, &#x0026; <string-name><surname>Simons</surname>, <given-names>R. F.</given-names></string-name> (<year>2006</year>). <article-title>Attending to affect: appraisal strategies modulate the electrocortical response to arousing pictures</article-title>. <source>Emotion</source>, <volume>6</volume>(<issue>3</issue>), <fpage>517</fpage>&#x2013;<lpage>522</lpage>.</mixed-citation></ref>
<ref id="c23"><mixed-citation publication-type="journal"><string-name><surname>Hansen</surname>, <given-names>C. H.</given-names></string-name>, &#x0026; <string-name><surname>Hansen</surname>, <given-names>R. D.</given-names></string-name> (<year>1988</year>). <article-title>Finding the face in the crowd: an anger superiority effect</article-title>. <source>Journal of Personality &#x0026; Social Psychology</source>,<volume>54</volume> (<issue>6</issue>), <fpage>917</fpage>&#x2013;<lpage>24</lpage>.</mixed-citation></ref>
<ref id="c24"><mixed-citation publication-type="journal"><string-name><surname>Han</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Gao</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Humphreys</surname>, <given-names>G. W.</given-names></string-name>, &#x0026; <string-name><surname>Ge</surname>, <given-names>J.</given-names></string-name> (<year>2008</year>). <article-title>Neural processing of threat cues in social environments</article-title>. <source>Human Brain Mapping</source>, <volume>29</volume>(<issue>8</issue>), <fpage>945</fpage>.</mixed-citation></ref>
<ref id="c25"><mixed-citation publication-type="journal"><string-name><surname>Hietanen</surname>, <given-names>J. K.</given-names></string-name>, &#x0026; <string-name><surname>Astikainen</surname>, <given-names>P.</given-names></string-name> (<year>2013</year>). <article-title>N170 response to facial expressions is modulated by the affective congruency between the emotional expression and preceding affective picture</article-title>. <source>Biological Psychology</source>, <volume>92</volume> (<issue>2</issue>), <fpage>114</fpage>&#x2013;<lpage>24</lpage>.</mixed-citation></ref>
<ref id="c26"><mixed-citation publication-type="journal"><string-name><surname>Hinojosa</surname>, <given-names>J. A.</given-names></string-name>, <string-name><surname>Mercado</surname>, <given-names>F.</given-names></string-name>, &#x0026; <string-name><surname>Carreti&#x00E9;</surname>, <given-names>L.</given-names></string-name> (<year>2015</year>). <article-title>N170 sensitivity to facial expression: a meta-analysis</article-title>. <source>Neuroscience &#x0026; Biobehavioral Reviews</source>, <volume>55</volume>, <fpage>498</fpage>&#x2013;<lpage>509</lpage>.</mixed-citation></ref>
<ref id="c27"><mixed-citation publication-type="journal"><string-name><surname>Hoffmann</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Kessler</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Eppel</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Rukavina</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Traue</surname>, <given-names>H. C.</given-names></string-name> (<year>2010</year>). <article-title>Expression intensity, gender and facial emotion recognition: women recognize only subtle facial emotions better than men</article-title>. <source>Acta Psychologica</source>, <volume>135</volume>(<issue>3</issue>), <fpage>278</fpage>&#x2013;<lpage>283</lpage>.</mixed-citation></ref>
<ref id="c28"><mixed-citation publication-type="journal"><string-name><surname>Holmes</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Green</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Vuilleumier</surname>, <given-names>P.</given-names></string-name> (<year>2005</year>). <article-title>The involvement of distinct visual channels in rapid attention towards fearful facial expressions</article-title>. <source>Cognition &#x0026; Emotion</source>, <volume>19</volume>(<issue>6</issue>), <fpage>899</fpage>&#x2013;<lpage>922</lpage>.</mixed-citation></ref>
<ref id="c29"><mixed-citation publication-type="journal"><string-name><surname>Hoptman</surname>, <given-names>M. J.</given-names></string-name>, &#x0026; <string-name><surname>Levy</surname>, <given-names>J.</given-names></string-name> (<year>1988</year>). <article-title>Perceptual asymmetries in left- and right-handers for cartoon and real faces</article-title>. <source>Brain &#x0026; Cognition</source>, <volume>8</volume>(<issue>2</issue>), <fpage>178</fpage>&#x2013;<lpage>188</lpage>.</mixed-citation></ref>
<ref id="c30"><mixed-citation publication-type="journal"><string-name><surname>Hu</surname>, <given-names>B. P.</given-names></string-name>, &#x0026; <string-name><surname>Li</surname>, <given-names>S. J.</given-names></string-name> (<year>2014</year>). <article-title>Studies on Serotonin, Testosterone and Total Cholesterol in the Blood of Highly Aggressive People</article-title>. <source>Journal of Physiology Studies</source>, <volume>02</volume>, <fpage>13</fpage>&#x2013;<lpage>18</lpage>.</mixed-citation></ref>
<ref id="c31"><mixed-citation publication-type="journal"><string-name><given-names>Jack</given-names> <surname>Glascock</surname> <suffix>Ph.D.</suffix></string-name> (<year>2008</year>). <article-title>Direct and indirect aggression on prime-time network television</article-title>. <source>Journal of Broadcasting &#x0026; Electronic Media</source>, <volume>52</volume>(<issue>2</issue>), <fpage>268</fpage>&#x2013;<lpage>281</lpage>.</mixed-citation></ref>
<ref id="c32"><mixed-citation publication-type="journal"><string-name><surname>Jessica</surname> <given-names>K. Hall</given-names></string-name>, <string-name><given-names>Sam B.</given-names> <surname>Hutton</surname></string-name>, &#x0026; <string-name><given-names>Michael J.</given-names> <surname>Morgan.</surname></string-name> (<year>2010</year>). <article-title>Sex differences in scanning faces: does attention to the eyes explain female superiority in facial expression recognition?</article-title>. <source>Cognition &#x0026; Emotion</source>, <volume>24</volume>(<issue>4</issue>), <fpage>629</fpage>&#x2013;<lpage>637</lpage>.</mixed-citation></ref>
<ref id="c33"><mixed-citation publication-type="journal"><string-name><surname>Joyce</surname>, <given-names>C.</given-names></string-name>, &#x0026; <string-name><surname>Rossion</surname>, <given-names>B.</given-names></string-name> (<year>2005</year>). <article-title>The face-sensitive n170 and vpp components manifest the same brain processes: the effect of reference electrode site</article-title>. <source>Clinical Neurophysiology</source>, <volume>116</volume>(<issue>11</issue>), <fpage>2613</fpage>&#x2013;<lpage>2631</lpage>.</mixed-citation></ref>
<ref id="c34"><mixed-citation publication-type="journal"><string-name><surname>Keil</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Bradley</surname>, <given-names>M. M.</given-names></string-name>, <string-name><surname>Hauk</surname>, <given-names>O.</given-names></string-name>, <string-name><surname>Rockstroh</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Elbert</surname>, <given-names>T.</given-names></string-name>, &#x0026; <string-name><surname>Lang</surname>, <given-names>P. J.</given-names></string-name> (<year>2002</year>). <article-title>Large-scale neural correlates of affective picture processing</article-title>. <source>Psychophysiology</source>, <volume>39</volume>(<issue>5</issue>), <fpage>641</fpage>&#x2013;<lpage>649</lpage>.</mixed-citation></ref>
<ref id="c35"><mixed-citation publication-type="journal"><string-name><surname>Kendall</surname>, <given-names>L. N.</given-names></string-name>, <string-name><surname>Raffaelli</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Kingstone</surname>, <given-names>A.</given-names></string-name>, &#x0026; <string-name><surname>Todd</surname>, <given-names>R. M.</given-names></string-name> (<year>2016</year>). <article-title>Iconic faces are not real faces: enhanced emotion detection and altered neural processing as faces become more iconic</article-title>. <source>Cognitive Research Principles &#x0026; Implications</source>, <volume>1</volume>(<issue>1</issue>), <fpage>19</fpage>.</mixed-citation></ref>
<ref id="c36"><mixed-citation publication-type="journal"><string-name><surname>Krolak-Salmon</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Fischer</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Vighetto</surname>, <given-names>A.</given-names></string-name>, &#x0026; <string-name><surname>Maugui&#x00E8;re</surname>, <given-names>F.</given-names></string-name> (<year>2001</year>). <article-title>Processing of facial emotional expression: spatio-temporal data as assessed by scalp event-related potentials</article-title>. <source>European Journal of Neuroscience</source>, <volume>13</volume>(<issue>5</issue>), <fpage>987</fpage>.</mixed-citation></ref>
<ref id="c37"><mixed-citation publication-type="journal"><string-name><surname>Ma</surname>, <given-names>Q.</given-names></string-name>, <string-name><surname>Qian</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hu</surname>, <given-names>L.</given-names></string-name>, &#x0026; <string-name><surname>Wang</surname>, <given-names>L.</given-names></string-name> (<year>2017</year>). <article-title>Hello handsome! male&#x2019;s facial attractiveness gives rise to female&#x2019;s fairness bias in ultimatum game scenarios-an erp study</article-title>. <source>Plos One</source>, <volume>12</volume>(<issue>7</issue>), <fpage>e0180459</fpage>.</mixed-citation></ref>
<ref id="c38"><mixed-citation publication-type="journal"><string-name><surname>Marzi</surname>, <given-names>T.</given-names></string-name>, &#x0026; <string-name><surname>Viggiano</surname>, <given-names>M. P.</given-names></string-name> (<year>2010</year>). <article-title>When memory meets beauty: insights from event-related potentials</article-title>. <source>Biological Psychology</source>, <volume>84</volume>(<issue>2</issue>), <fpage>192</fpage>&#x2013;<lpage>205</lpage>.</mixed-citation></ref>
<ref id="c39"><mixed-citation publication-type="journal"><string-name><surname>Mcbain</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Norton</surname>, <given-names>D.</given-names></string-name>, &#x0026; <string-name><surname>Chen</surname>, <given-names>Y.</given-names></string-name> (<year>2009</year>). <article-title>Females excel at basic face perception</article-title>. <source>Acta Psychologica</source>, <volume>130</volume>(<issue>2</issue>), <fpage>168</fpage>.</mixed-citation></ref>
<ref id="c40"><mixed-citation publication-type="journal"><string-name><surname>Megreya</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Bindemann</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Havard</surname>, <given-names>C.</given-names></string-name> (<year>2011</year>). <article-title>Sex differences in unfamiliar face identification: evidence from matching tasks</article-title>. <source>Acta Psychologica</source>, <volume>137</volume>(<issue>1</issue>), <fpage>83</fpage>&#x2013;<lpage>89</lpage>.</mixed-citation></ref>
<ref id="c41"><mixed-citation publication-type="journal"><string-name><surname>Mullin</surname>, <given-names>C. R.</given-names></string-name> (<year>2011</year>). <article-title>Sex differences in face processing are mediated by handedness and sexual orientation</article-title>. <source>Laterality</source>, <volume>16</volume>(<issue>2</issue>), <fpage>188</fpage>&#x2013;<lpage>200</lpage>.</mixed-citation></ref>
<ref id="c42"><mixed-citation publication-type="journal"><string-name><surname>Nelson</surname>, <given-names>C. A.</given-names></string-name>, <string-name><surname>Morse</surname>, <given-names>P. A.</given-names></string-name>, &#x0026; <string-name><surname>Leavitt</surname>, <given-names>L. A.</given-names></string-name> (<year>1979</year>). <article-title>Recognition of facial expressions by seven-month-old infants</article-title>. <source>Child Dev</source>, <volume>50</volume>(<issue>4</issue>), <fpage>1239</fpage>&#x2013;<lpage>1242</lpage>.</mixed-citation></ref>
<ref id="c43"><mixed-citation publication-type="journal"><string-name><surname>Proverbio</surname>, <given-names>A. M.</given-names></string-name>, <string-name><surname>Riva</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Martin</surname>, <given-names>E.</given-names></string-name>, &#x0026; <string-name><surname>Zani</surname>, <given-names>A.</given-names></string-name> (<year>2010</year>). <article-title>Face coding is bilateral in the female brain</article-title>. <source>Plos One</source>, <volume>5</volume>(<issue>6</issue>), <fpage>e11242</fpage>.</mixed-citation></ref>
<ref id="c44"><mixed-citation publication-type="journal"><string-name><surname>Recio</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Sommer</surname>, <given-names>W.</given-names></string-name>, &#x0026; <string-name><surname>Schacht</surname>, <given-names>A.</given-names></string-name> (<year>2011</year>). <article-title>Electrophysiological correlates of perceiving and evaluating static and dynamic facial emotional expressions</article-title>. <source>Brain Research</source>, <volume>1376</volume>(<issue>3</issue>), <fpage>66</fpage>.</mixed-citation></ref>
<ref id="c45"><mixed-citation publication-type="journal"><string-name><surname>Rellecke</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Sommer</surname>, <given-names>W.</given-names></string-name>, &#x0026; <string-name><surname>Schacht</surname>, <given-names>A.</given-names></string-name> (<year>2012</year>). <article-title>Does processing of emotional facial expressions depend on intention? time-resolved evidence from event-related brain potentials</article-title>. <source>Biological Psychology</source>,<volume>90</volume>(<issue>1</issue>), <fpage>23</fpage>&#x2013;<lpage>32</lpage>.</mixed-citation></ref>
<ref id="c46"><mixed-citation publication-type="journal"><string-name><surname>Rossion</surname>, <given-names>B.</given-names></string-name>, &#x0026; <string-name><surname>Jacques</surname>, <given-names>C.</given-names></string-name> (<year>2008</year>). <article-title>Does physical interstimulus variance account for early electrophysiological face sensitive responses in the human brain? ten lessons on the n170</article-title>. <source>Neuroimage</source>, <volume>39</volume>(<issue>4</issue>), <fpage>1959</fpage>&#x2013;<lpage>1979</lpage>.</mixed-citation></ref>
<ref id="c47"><mixed-citation publication-type="journal"><string-name><surname>Sagiv</surname>, <given-names>N.</given-names></string-name>, &#x0026; <string-name><surname>Bentin</surname>, <given-names>S.</given-names></string-name> (<year>2001</year>). <article-title>Structural Encoding of Human and Schematic Faces</article-title>: <source>Holistic and Part-Based Processes. MIT Press</source>.</mixed-citation></ref>
<ref id="c48"><mixed-citation publication-type="journal"><string-name><surname>Schindler</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Kissler</surname>, <given-names>J.</given-names></string-name> (<year>2016</year>). <article-title>People matter: perceived sender identity modulates cerebral processing of socio-emotional language feedback</article-title>. <source>Neuroimage</source>, <volume>134</volume>(<issue>2</issue>), <fpage>160</fpage>&#x2013;<lpage>169</lpage>.</mixed-citation></ref>
<ref id="c49"><mixed-citation publication-type="journal"><string-name><surname>Schindler</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Zell</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Botsch</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Kissler</surname>, <given-names>J.</given-names></string-name> (<year>2017</year>). <article-title>Differential effects of face-realism and emotion on event-related brain potentials and their implications for the uncanny valley theory</article-title>. <source>Scientific Reports</source>, <volume>7</volume>, <fpage>45003</fpage>.</mixed-citation></ref>
<ref id="c50"><mixed-citation publication-type="journal"><string-name><surname>Schupp</surname>, <given-names>H. T.</given-names></string-name>, <string-name><surname>Flaisch</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Stockburger</surname>, <given-names>J.</given-names></string-name>, &#x0026; <string-name><surname>Jungh&#x00F6;fer</surname>, <given-names>M.</given-names></string-name> (<year>2006</year>). <article-title>Emotion and attention: event-related brain potential studies</article-title>. <source>Progress in Brain Research</source>, <volume>156</volume>(<issue>156</issue>), <fpage>31</fpage>&#x2013;<lpage>51</lpage>.</mixed-citation></ref>
<ref id="c51"><mixed-citation publication-type="journal"><string-name><surname>Schupp</surname>, <given-names>H. T.</given-names></string-name>, <string-name><surname>Jungh&#x00F6;fer</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Weike</surname>, <given-names>A. I.</given-names></string-name>, &#x0026; <string-name><surname>Hamm</surname>, <given-names>A. O.</given-names></string-name> (<year>2004</year>). <article-title>The selective processing of briefly presented affective pictures: an erp analysis</article-title>. <source>Psychophysiology</source>, <volume>41</volume>(<issue>3</issue>), <fpage>441</fpage>&#x2013;<lpage>449</lpage>.</mixed-citation></ref>
<ref id="c52"><mixed-citation publication-type="journal"><string-name><surname>Steppacher</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Schindler</surname>, <given-names>S.</given-names></string-name>, &#x0026; <string-name><surname>Kissler</surname>, <given-names>J.</given-names></string-name> (<year>2015</year>). <article-title>Higher, faster, worse? an event-related potentials study of affective picture processing in migraine</article-title>. <source>Cephalalgia An International Journal of Headache</source>, <volume>36</volume>(<issue>3</issue>), <fpage>249</fpage>.</mixed-citation></ref>
<ref id="c53"><mixed-citation publication-type="journal"><string-name><surname>Thompson</surname>, <given-names>D. F.</given-names></string-name>, &#x0026; <string-name><surname>Meltzer</surname>, <given-names>L.</given-names></string-name> (<year>1964</year>). <article-title>Communication of emotional intent by facial expression</article-title>. <source>Journal of Abnormal Psychology</source>, <volume>68</volume>(<issue>2</issue>), <fpage>129</fpage>.</mixed-citation></ref>
<ref id="c54"><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>L.</given-names></string-name>,<string-name><surname>Wang</surname>, <given-names>J. M.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>J. L.</given-names></string-name>, &#x0026; <string-name><surname>Lu</surname>, <given-names>Y. J.</given-names></string-name> (<year>2012</year>). <source>A Comparative Study of Event-related Potential in Cartoon Faces and Real Faces Recognition. Psychological Research</source>, <volume>5</volume>(<issue>5</issue>), <fpage>19</fpage>&#x2013;<lpage>28</lpage>.</mixed-citation></ref>
<ref id="c55"><mixed-citation publication-type="journal"><string-name><surname>Wang</surname>, <given-names>Y.</given-names></string-name>, &#x0026; <string-name><surname>Luo</surname>, <given-names>Y. J.</given-names></string-name> (<year>2005</year>). <article-title>Standardization and Evaluation of College Students&#x2019; Facial Expression Materials</article-title>. <source>Chinese Journal of Clinical Psychology</source>, <volume>13</volume>(<issue>4</issue>), <fpage>396</fpage>&#x2013;<lpage>398</lpage>.</mixed-citation></ref>
<ref id="c56"><mixed-citation publication-type="journal"><string-name><surname>Wei</surname>, <given-names>J. H.</given-names></string-name>, &#x0026; <string-name><surname>Luo</surname>, <given-names>Y. J.</given-names></string-name> (<year>2002</year>). <source>Event-related Brain Potentials: The Cognitive ERP Textbook. Beijing: The Economic Daily Press, 2002</source>, <volume>5</volume>:<fpage>8</fpage>&#x2013;<lpage>32</lpage>.</mixed-citation></ref>
<ref id="c57"><mixed-citation publication-type="journal"><string-name><surname>Werheid</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Schacht</surname>, <given-names>A.</given-names></string-name>, &#x0026; <string-name><surname>Sommer</surname>, <given-names>W.</given-names></string-name> (<year>2007</year>). <article-title>Facial attractiveness modulates early and late event-related brain potentials</article-title>. <source>Biological Psychology</source>, <volume>76</volume>(<issue>2</issue>), <fpage>100</fpage>&#x2013;<lpage>108</lpage>.</mixed-citation></ref>
<ref id="c58"><mixed-citation publication-type="journal"><string-name><surname>Whalen</surname>, <given-names>P. J.</given-names></string-name>, <string-name><surname>Rauch</surname>, <given-names>S. L.</given-names></string-name>, <string-name><surname>Etcoff</surname>, <given-names>N. L.</given-names></string-name>, <string-name><surname>Mcinerney</surname>, <given-names>S. C.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>M. B.</given-names></string-name>, &#x0026; <string-name><surname>Jenike</surname>, <given-names>M. A.</given-names></string-name> (<year>1998</year>). <article-title>Masked presentations of emotional facial expressions modulate amygdala activity without explicit knowledge</article-title>. <source>Journal of Neuroscience</source>, <volume>18</volume>(<issue>1</issue>), <fpage>411</fpage>&#x2013;<lpage>418</lpage>.</mixed-citation></ref>
<ref id="c59"><mixed-citation publication-type="journal"><string-name><surname>Wheatley</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Weinberg</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Looser</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Moran</surname>, <given-names>T.</given-names></string-name>, &#x0026; <string-name><surname>Hajcak</surname>, <given-names>G.</given-names></string-name> (<year>2011</year>). <article-title>Mind perception: real but not artificial faces sustain neural activity beyond the n170/vpp</article-title>. <source>Plos One</source>, <volume>6</volume>(<issue>3</issue>), <fpage>e17960</fpage>.</mixed-citation></ref>
<ref id="c60"><mixed-citation publication-type="journal"><string-name><surname>Wieser</surname>, <given-names>M. J.</given-names></string-name>, <string-name><surname>Pauli</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Reicherts</surname>, <given-names>P.</given-names></string-name>, &#x0026; <string-name><surname>M&#x00FC;hlberger</surname>, <given-names>A.</given-names></string-name> (<year>2010</year>). <article-title>Don&#x2019;t look at me in anger! enhanced processing of angry faces in anticipation of public speaking</article-title>. <source>Psychophysiology</source>, <volume>47</volume>(<issue>2</issue>), <fpage>271</fpage>&#x2013;<lpage>280</lpage>.</mixed-citation></ref>
<ref id="c61"><mixed-citation publication-type="journal"><string-name><surname>Wildgruber</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Pihan</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Ackermann</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Erb</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Grodd</surname>, <given-names>W.</given-names></string-name> (<year>2002</year>). <article-title>Dynamic brain activation during processing of emotional intonation: influence of acoustic parameters, emotional valence, and sex</article-title>. <source>Neuroimage</source>, <volume>15</volume>(<issue>4</issue>), <fpage>856</fpage>&#x2013;<lpage>869</lpage>.</mixed-citation></ref>
<ref id="c62"><mixed-citation publication-type="journal"><string-name><surname>Wright</surname>, <given-names>D. B.</given-names></string-name>, &#x0026; <string-name><surname>Stroud</surname>, <given-names>J. N.</given-names></string-name> (<year>2002</year>). <article-title>Age differences in lineup identification accuracy: people are better with their own age</article-title>. <source>Law &#x0026; Human Behavior</source>, <volume>26</volume>(<issue>6</issue>), <fpage>641</fpage>&#x2013;<lpage>654</lpage>.</mixed-citation></ref>
<ref id="c63"><mixed-citation publication-type="journal"><string-name><surname>Yovel</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Levy</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Grabowecky</surname>, <given-names>M.</given-names></string-name>, &#x0026; <string-name><surname>Paller</surname>, <given-names>K. A.</given-names></string-name> (<year>2003</year>). <article-title>Neural correlates of the left-visual-field superiority in face perception appear at multiple stages of face processing</article-title>. <source>Cognitive Neuroscience Journal of</source>,<volume>15</volume>(<issue>3</issue>), <fpage>462</fpage>&#x2013;<lpage>474</lpage>.</mixed-citation></ref>
<ref id="c64"><mixed-citation publication-type="journal"><string-name><surname>Zhu</surname>, <given-names>Y. Y.</given-names></string-name>, &#x0026; <string-name><surname>Liu</surname>, <given-names>Z. X.</given-names></string-name> (<year>2014</year>). <article-title>An ERP Study of Dynamic Facial Expression Recognition under Different Attention Conditions</article-title>. <source>Chinese Journal of Applied Psychology</source>, <volume>20</volume>(<issue>4</issue>), <fpage>375</fpage>&#x2013;<lpage>384</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>