<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/245738</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>A scalable method to improve gray matter segmentation at ultra high field MRI</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7761-3727</contrib-id>
<name>
<surname>Gulban</surname>
<given-names>Omer Faruk</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">&#x262F;</xref>
<xref ref-type="author-notes" rid="n2">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3192-5316</contrib-id>
<name>
<surname>Schneider</surname>
<given-names>Marian</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">&#x262F;</xref>
<xref ref-type="author-notes" rid="n2">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5178-9951</contrib-id>
<name>
<surname>Marquardt</surname>
<given-names>Ingo</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Haast</surname>
<given-names>Roy A.M.</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>De Martino</surname>
<given-names>Federico</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Cognitive Neuroscience, Faculty of Psychology and Neuroscience, Maastricht University</institution>, Maastricht, The <country>Netherlands</country>.</aff>
<aff id="a2"><label>2</label><institution>Maastricht Centre for Systems Biology, Maastricht University</institution>, Maastricht, The <country>Netherlands</country></aff>
<aff id="a3"><label>3</label><institution>Center for Magnetic Resonance Research, University of Minnesota</institution>, Minneapolis, <country>USA</country>.</aff>
</contrib-group>
<author-notes>
<fn id="n1" fn-type="equal"><label>&#x262F;</label><p>These authors contributed equally to this work.</p></fn>
<fn id="n2"><label>&#x002A;</label><p><email>faruk.gulban@maastrichtuniversity.nl</email>, <email>marian.schneider@maastrichtuniversity.nl</email></p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2018</year>
</pub-date>
<elocation-id>245738</elocation-id>
<history>
<date date-type="received">
<day>09</day>
<month>1</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>09</day>
<month>1</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>10</day>
<month>1</month>
<year>2018</year>
</date>
</history><permissions><copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license></permissions>
<self-uri xlink:href="245738.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>High-resolution (functional) magnetic resonance imaging (MRI) at ultra high magnetic fields (7 Tesla and above) enables researchers to study how anatomical and functional properties change within the cortical ribbon, along surfaces and across cortical depths. These studies require an accurate delineation of the gray matter ribbon, which often suffers from inclusion of blood vessels, dura mater and other non-brain tissue. Residual segmentation errors are commonly corrected by browsing the data slice-by-slice and manually changing labels. This task becomes increasingly laborious and prone to error at higher resolutions since both work and error scale with the number of voxels. Here we show that mislabeled, non-brain voxels can be corrected more efficiently and semi-automatically by representing three-dimensional anatomical images using two-dimensional histograms. We propose both a uni-modal (based on first spatial derivative) and multi-modal (based on compositional data analysis) approach to this representation and quantify the benefits in 7 Tesla MRI data of nine volunteers. We present an openly accessible Python implementation of these approaches and demonstrate that editing cortical segmentations using two-dimensional histogram representations aids existing algorithms and yields improved gray matter borders. By making our data and corresponding expert (ground truth) segmentations available, we facilitate future efforts to develop and test segmentation algorithms on this challenging type of data.</p>
</abstract>
<counts>
<page-count count="35"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Magnetic resonance imaging (MRI) has become one of the most important tools to study human brain function and structure in vivo. Moving from high (3 Tesla [T]) to ultra high (7 and 9.4 T) magnetic fields (UHF), together with improvements in acquisition methods, leads to increases in signal and contrast to noise (SNR and CNR, respectively) [<xref ref-type="bibr" rid="c1">1</xref>&#x2013;<xref ref-type="bibr" rid="c3">3</xref>]. The increase in SNR can be leveraged to increase the voxels&#x2019; resolution of both functional and structural images to sub-millimeter scales. Such sub-millimeter spatial resolutions allow for in vivo studies that probe cortical properties at the mesoscale [<xref ref-type="bibr" rid="c4">4</xref>&#x2013;<xref ref-type="bibr" rid="c8">8</xref>]. These studies include (i) cortical-depth dependent analyses of function [<xref ref-type="bibr" rid="c9">9</xref>&#x2013;<xref ref-type="bibr" rid="c14">14</xref>] and structure [<xref ref-type="bibr" rid="c5">5</xref>, <xref ref-type="bibr" rid="c8">8</xref>, <xref ref-type="bibr" rid="c15">15</xref>], (ii) the mapping of cortical columnar structures [<xref ref-type="bibr" rid="c16">16</xref>&#x2013;<xref ref-type="bibr" rid="c21">21</xref>] as well as (iii) sub-millimeter cortical topography [<xref ref-type="bibr" rid="c18">18</xref>, <xref ref-type="bibr" rid="c22">22</xref>&#x2013;<xref ref-type="bibr" rid="c24">24</xref>].</p>
<p>Such studies crucially depend on accurate and precise delineations of the gray matter (GM) ribbon both at the inner (white matter; WM) and outer (cerebrospinal; CSF) border. Since the aim of these studies is to investigate how the (functional) MRI (f/MRI) signal varies as a function of small position changes in GM, systematic GM segmentation errors would invalidate the conclusions drawn from these studies. Consider, as an example, an fMRI study conducted with a voxel resolution of 0.8 mm isotropic. Assume an average thickness of human cortex of 2.4 mm and a true signal change at the upper cortical depth level. In this study, the functional resolution would allow a straight piece of cortical ribbon to be divided in three relative cortical depths, each of them one voxel thick. Falsely labeling an additional fourth voxel as GM would make the difference between reporting an fMRI signal change at most superficial (false voxel excluded) or mid-superficial (false voxel included) cortical depth level. This example stresses the importance of accurate and precise GM segmentations.</p>
<p>Obtaining accurate and precise definitions of the GM ribbon, however, is currently a difficult and time-consuming task for sub-millimeter UHF data. The increases in SNR, CNR and resolution attainable in UHF anatomical data as well as analysis [<xref ref-type="bibr" rid="c25">25</xref>] and reconstruction [<xref ref-type="bibr" rid="c26">26</xref>] strategies specific to UHF reveal several structures outside of the brain that were barely visible on images obtained at conventional field strengths (1.5 and 3 T) and lower resolution (&#x003E; 1 mm isotropic) [<xref ref-type="bibr" rid="c27">27</xref>]. Such structures include the dura mater [<xref ref-type="bibr" rid="c28">28</xref>], medium-sized blood vessels in the sulci [<xref ref-type="bibr" rid="c29">29</xref>] as well as draining sinuses and connective tissue adjacent to GM [<xref ref-type="bibr" rid="c30">30</xref>]. To date, many of the automatic brain segmentation algorithms have been developed and benchmarked on images collected at 1 mm isotropic resolution or lower and at conventional field strengths [<xref ref-type="bibr" rid="c31">31</xref>] (but see [<xref ref-type="bibr" rid="c32">32</xref>]). If segmentation algorithms do not model these non-brain structures they might falsely label part of these structures as GM. Faced with such segmentation errors, researchers may choose to correct the misclassified voxels manually. However, the increase in resolution leads to an exponential increase in the number of voxels, which renders manual correction a laborious task. Furthermore, manual correction is prone to error and may introduce an observer bias, thereby reducing the reproducibility of subsequent analyses [<xref ref-type="bibr" rid="c33">33</xref>]. This currently leaves researchers with the dilemma of accepting the likely erroneous outcome of automatic segmentation algorithms or performing a time-consuming and error-prone manual correction.</p>
<p>CBS tools [<xref ref-type="bibr" rid="c32">32</xref>] directly tackle many of the challenges of UHF high-resolution anatomical data by, for example, including pre-processing steps to estimate dura mater and CSF partial voluming. Consequently, these tools provide an improved initial GM segmentation compared to other solutions [<xref ref-type="bibr" rid="c32">32</xref>]. However, we show here that in many cases the initial CBS segmentation can still be further improved with the approaches we propose. Furthermore, CBS tools have been optimized for whole-brain data obtained with the MP2RAGE sequence [<xref ref-type="bibr" rid="c26">26</xref>]. While the MP2RAGE sequence is commonly used at UHF as a basis for brain tissue class segmentations, we note that many high-resolution studies at UHF also use alternative sequences to define GM [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c34">34</xref>, <xref ref-type="bibr" rid="c35">35</xref>], some of which offering partial coverage of the brain only [<xref ref-type="bibr" rid="c12">12</xref>, <xref ref-type="bibr" rid="c35">35</xref>]. In such cases, alternative approaches that do not depend on particular templates, atlases or other forms of prior information are useful and required.</p>
<p>Here, we show that non-brain voxels misclassified as GM can be corrected using a multi-dimensional transfer function that is specified based on a two-dimensional (2D) histogram representation [<xref ref-type="bibr" rid="c36">36</xref>&#x2013;<xref ref-type="bibr" rid="c39">39</xref>] of three-dimensional (3D) MRI brain data. We demonstrate that this transfer function offers an efficient way to single out non-brain tissue voxels. Removing these voxels from GM classifications found by automatic segmentation pipelines improves GM segmentations. This approach addresses the problems of an entirely manual correction, since it yields a meaningful summary representation of the data that allows to manipulate the data efficiently. As a consequence, it is both more time efficient than manual slice-by-slice correction and it reduces observer bias.</p>
<p>We structured the paper as follows. In <xref ref-type="sec" rid="s2">section 1</xref>, we introduce the technique of specifying transfer functions based on 2D histogram representations of voxel intensity and gradient magnitude. We offer theoretical considerations for why this technique is particularly suited to remove vessels and dura mater voxels in high-resolution MRI data (&#x003C; 1 mm isotropic voxel size). In <xref ref-type="sec" rid="s3">section 2</xref>, we extend the use of histogram-based transfer functions to multi-modal MRI datasets (e.g. T1 weighted [T1w], Proton Density weighted [PDw], T2&#x002A; weighted [T2&#x002A;w]) by considering MRI data in the compositional data analysis framework [<xref ref-type="bibr" rid="c40">40</xref>]. We show that this compositional framework yields an intuitive and useful summary representation of multi-modal MRI data which aids the creation of transfer functions. In <xref ref-type="sec" rid="s4">Section 3</xref> and <xref ref-type="sec" rid="s5">4</xref> we validate the suggested methods by evaluating obtained GM segmentation results against expert GM segmentations obtained for nine subjects recorded at 7 T. We demonstrate considerable improvement in segmentation performance metrics for the two methods suggested here. We have implemented the methods described here in a free and open Python software package [<xref ref-type="bibr" rid="c41">41</xref>]. Furthermore, we make available the data [<xref ref-type="bibr" rid="c42">42</xref>] and processing scripts [<xref ref-type="bibr" rid="c43">43</xref>] used to validate the proposed methods.</p>
</sec>
<sec id="s2">
<label>1</label>
<title>Theory I: Transfer functions and 2D histograms</title>
<sec id="s2a">
<label>1.1</label>
<title>Multi-dimensional transfer functions</title>
<p>In the context of MRI data visualization, a transfer function can be understood as a mapping of voxel data to optical properties such as color and opacity. Effective transfer functions make structures of interest in the data visible. This can, for example, be achieved by assigning low opacity values to voxels that make up irrelevant structures and by highlighting desired structures with high opacity and salient color values. Multi-dimensional transfer functions assign renderable properties based on a combination of values [<xref ref-type="bibr" rid="c36">36</xref>&#x2013;<xref ref-type="bibr" rid="c38">38</xref>, <xref ref-type="bibr" rid="c44">44</xref>]. This is important in the context of MRI data where features of interest are often difficult to extract based on a single value alone. Considering multiple values, such as the intensity in images acquired using different contrast weighting (e.g. T1, PD and T2&#x002A;), increases the chances of uniquely isolating a feature and making it visible [<xref ref-type="bibr" rid="c44">44</xref>].</p>
<p>In theory, multi-dimensional transfer functions could be used to perform exhaustive tissue-type segmentation of human brain MRI data. In this process, each voxel would be classified as either WM, GM, or CSF by specifying appropriate transfer functions. It has been shown, however, that this approach is less successful than established brain segmentation algorithms [<xref ref-type="bibr" rid="c45">45</xref>]. Here, we propose that transfer function-based methods still have a role to play in UHF MRI brain segmentation pipelines because they are well-suited for efficient removal of mislabeled non-brain tissue. We motivate this proposition by considering that brain and non-brain voxels become separable in 2D histogram representations.</p>
</sec>
<sec id="s2b">
<label>1.2</label>
<title>2D histogram representation</title>
<p>2D histogram representations have been shown to greatly facilitate the process of specifying effective, 2D transfer functions [<xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c38">38</xref>]. 2D histograms are obtained by taking an n-dimensional dataset and binning its data points along two dimensions. In principle, a 2D histogram can be obtained from any two sets of values. To identify tissue boundaries a 2D histogram, plotting gradient magnitude against image intensity, has been shown to be particularly useful [<xref ref-type="bibr" rid="c36">36</xref>, <xref ref-type="bibr" rid="c38">38</xref>]. The term gradient magnitude here refers to the magnitude of the vector that represents the spatial intensity gradient at every MRI voxel, where the spatial intensity gradient is equal to the first spatial derivative of the image intensity values.</p>
<p><xref ref-type="fig" rid="fig1">Fig 1</xref> shows how 3D MRI data of a human brain are represented in a 2D histogram (a T1w image was divided by a PDw image [<xref ref-type="bibr" rid="c25">25</xref>], multiplied by 500 and brain extracted; images were acquired with 0.7 mm isotropic resolution; for more details see <xref ref-type="sec" rid="s4">Section 3</xref>). The histogram is obtained by plotting gradient magnitude against image intensity. In this representation, different tissue types occupy different regions. CSF voxels are characterized by very low intensity and low gradient magnitude values and therefore occupy the lower-left space of the histogram. GM and WM voxels have medium to high intensities and very low gradient magnitudes. Therefore, these tissue classes form circular regions at the bottom-center of the histogram. Voxels at the GM-WM interface fall within an arc reaching from medium to high intensities, following a low to medium to low gradient magnitude trajectory. Similarly, voxels at the CSF-GM interface span an arc from low to medium intensities. Finally, blood vessels and dura mater are thin structures characterized by very high gradient magnitudes and medium to high intensities. Therefore, these structures occupy up-center and the up-right parts of the 2D histogram.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Fig 1.</label>
<caption><title>2D histogram representation for MRI image of a human brain.</title>
<p>(A) Intensity and (B) gradient magnitude values of a brain extracted T1w-divided-by-PDw MRI image are represented in a (C) 2D histogram. Darker regions in the histogram indicate that many voxels in the MRI image are characterized by this particular combination of image intensity and gradient magnitude. (D) The 2D histogram displays a characteristic pattern with tissue types occupying particular areas of the histogram. Voxels containing CSF, dura mater or blood vessels (black dashed lines and arrows) cover different regions of the histogram than voxels containing WM and GM (red dashed lines). As a result, brain tissue becomes separable from non-brain tissue.</p></caption>
<graphic xlink:href="245738_fig1.tif"/>
</fig>
<p>Since different tissue types occupy different regions in the 2D histogram, each tissue type and boundary can, in principle, be isolated using a 2D transfer function based on image intensity and gradient magnitude. For the purposes of this paper, we focus on the distinction between brain (WM, GM, GM-WM interface) and non-brain (CSF, CSF-GM interface, blood vessels, dura mater) voxels. The intensity-gradient magnitude histogram is particularly suited to distinguish non-brain tissue at high resolutions, because dura mater and vessels do not span more than one or two voxels, given the typical voxel sizes of current high resolution studies (around 0.5 mm - 1 mm isotropic). Therefore, gradient magnitude will be high in the entirety of these structures (see <xref ref-type="fig" rid="fig1">Fig 1B</xref> for an example) and the combination of high intensity and high gradient magnitude values renders these structures separable from WM and GM voxels. To capture more extended structures or structures at even higher resolutions (in the future), we recommend the use of multi-modal data to distinguish data points by their compositional characteristics (see <xref ref-type="sec" rid="s2">Section 2</xref>).</p>
</sec>
<sec id="s2c">
<label>1.3</label>
<title>Creating transfer functions</title>
<p>The simplest way to create a transfer function is to explore the data by moving widgets with a specified shape over the 2D histogram representation [<xref ref-type="bibr" rid="c38">38</xref>]. For example, <xref ref-type="fig" rid="fig2">Fig 2</xref> shows how a circular sector could be moved on top of the 2D histogram to highlight particular regions. In this case, only MRI voxels whose intensity-gradient magnitude combination falls within the highlighted region of the 2D histogram would be selected. Position and size of the circular sector can then be refined until the desired data has been isolated.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Fig 2.</label>
<caption><title>Creation of 2D transfer functions with pre-defined shapes.</title>
<p>(A) Intensity and (B) gradient magnitude values of of a brain extracted T1w-divided-by-PDw MRI image are represented in a 2D histogram. By moving widgets of pre-defined shape, e.g. a circle, over the (C) 2D histogram and (D) concurrent visualization of selected voxels on a 2D slice of brain, positions of different tissue types in the 2D histogram can be probed and transfer functions can be created. In this example, the different probe positions (yellow, orange and red circles) appear to contain different aspects of GM.</p></caption>
<graphic xlink:href="245738_fig2.tif"/>
</fig>
<p>Using such a straightforward process of exploration and refinement [<xref ref-type="bibr" rid="c44">44</xref>], however, might yield slightly sub-optimal results. The shape of the widget might not capture the ideal shape given the data or the user might lack the prior knowledge that is required for this task. Alternatively, hierarchical exploration of normalized graph cut decision trees [<xref ref-type="bibr" rid="c39">39</xref>] can be used. This graph cut method results in a set of components (i.e. clusters) of the histogram that are mutually exclusive and collectively exhaustive. This allows the user to split and merge clusters in a data-driven and intuitive way that can be aided by the immediate visualization of the resulting segmentation (<xref ref-type="fig" rid="fig3">Fig 3</xref>). The method allows for semi-automatic tissue selection, i.e. the shape of the clusters is data-driven but the decision which clusters to join and which to divide is made by the user.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Fig 3.</label>
<caption><title>Creation of 2D transfer functions with data-driven shapes.</title>
<p>(A) The user starts with the 2D histogram representation of image intensity and gradient magnitude (left side) and concurrent visualization of the original brain data (right side). The user can then interact with and select data in the 2D histogram to specify transfer functions. In this example, this was done with the help of a normalized graph cut decision tree. (B) The interaction with the 2D histogram results in data-driven shapes of selected areas, here shaded in pink, green and blue (left side). Voxels selected by those areas are highlighted in corresponding colors against the backdrop of the original brain data (right side). The visualization reveals that the area of the 2D histogram shaded in blue selects brain voxels, while the areas shaded in green and pink select CSF&#x002A; and blood vessel voxels&#x002A;&#x002A;/dura mater&#x002A;&#x002A;&#x002A;, respectively.</p></caption>
<graphic xlink:href="245738_fig3.tif"/>
</fig>
</sec>
</sec>
<sec id="s3">
<label>2</label>
<title>Theory II: Multi-modal MRI data analysis</title>
<sec id="s3a">
<label>2.1</label>
<title>Compositional analysis for MRI data</title>
<p>More than one MRI contrast is often available and a combination of different contrasts can be useful in distinguishing different tissue types by differentially highlighting unique intrinsic properties. Two images with different contrast weighting can be combined using, for example, a ratio image [<xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c46">46</xref>, <xref ref-type="bibr" rid="c47">47</xref>]. This approach is beneficial for two reasons:1) it reduces image biases as all acquisitions are affected by the same sensitivity profile of the receive elements in the radio frequency coil, and 2) if the images carry opposing contrast for the tissues of interest, the ratio increases contrast and benefits the delineation of the structures (tissue) of interest.</p>
<p>The ratio image approach, however, is limited to pairs of images. To operate on the relative information of more than two images, we propose to use the barycentric coordinate system [<xref ref-type="bibr" rid="c48">48</xref>]. In the barycentric coordinate system, coordinates of a point represent a simplex whose center of mass is determined by the weights at its vertices (the term <italic>n-simplex</italic> in geometry is the generalized form of the triangle [<xref ref-type="bibr" rid="c49">49</xref>]; for example the 0-simplex is a point, the 1-simplex is a line segment, the 2-simplex is a triangle, the 3-simplex is a tetrahedron and so on). In other words, points in the barycentric coordinate system represent compositions of non-negative fractions whose sum of components gives a constant value. When a set of measurements is represented in the barycentric coordinate system (that is when the sampling space is an n-simplex), the compositional data (CoDa) analysis methods [<xref ref-type="bibr" rid="c40">40</xref>, <xref ref-type="bibr" rid="c50">50</xref>] can be used. The compositional data analysis offers a set of principled operations taking the geometry of the simplex sample space into account. Although MRI magnitude images are not strictly compositional (since they do not represent percentages), they still fulfill the technical requirements for treating each voxel as a composition. We therefore propose to process them in the CoDa framework, which allows us to exploit the principled operations of that framework. Although the general framework for CoDa analysis and its fundamental operations have already been rigorously documented in [<xref ref-type="bibr" rid="c40">40</xref>], for completeness we provide a step-by-step illustration of how multi-modal MRI data with three image contrasts (here T1w, PDw and T2&#x002A;w magnitude images) can be processed under the compositional data analysis framework to acquire a useful representation of different tissue types.</p>
<p>Let multi-modal MRI data, <italic>V<sub>cMRI</sub></italic>, be defined as: <disp-formula id="eqn1"><alternatives><graphic xlink:href="245738_eqn1.gif"/></alternatives></disp-formula> where <italic>&#x03C5;<sub>i</sub></italic> is the composition of voxel <italic>i</italic>. Let the composition <italic>&#x03C5;<sub>i</sub></italic> in this example consist of T1w, PDw and T2&#x002A;w measurements.</p>
<p>The first step in compositional MRI data analysis is to convert the data components from Cartesian coordinates in real space (<italic>R</italic><sup>3</sup>) to barycentric coordinates in simplex space (<italic>S</italic><sup>3</sup>), using a voxel-wise <italic>closure</italic> operation: <disp-formula id="eqn2"><alternatives><graphic xlink:href="245738_eqn2.gif"/></alternatives></disp-formula> where <italic>k</italic> is an arbitrary constant (here selected as 1) and <italic>I</italic>(<italic>&#x03C5;<sub>i</sub></italic>) is the total amount of voxel intensity in a specific voxel: <disp-formula id="eqn3"><alternatives><graphic xlink:href="245738_eqn3.gif"/></alternatives></disp-formula></p>
<p>As the next step, the barycentric coordinates of compositions (<italic>X</italic>) are centered by finding the <italic>sample center</italic> and <italic>perturbing</italic> each composition with the inverse of the sample center: <disp-formula id="eqn4"><alternatives><graphic xlink:href="245738_eqn4.gif"/></alternatives></disp-formula> where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="245738_inline1.gif"/></alternatives></inline-formula> stands for the component-wise geometric mean across all voxels <disp-formula id="eqn5"><alternatives><graphic xlink:href="245738_eqn5.gif"/></alternatives></disp-formula> and &#x2295; denotes the perturbation operation defined in multi-dimensional simplex space (<italic>S<sup>D</sup></italic>), which can be considered as an analogue of addition in real space: <disp-formula id="eqn6"><alternatives><graphic xlink:href="245738_eqn6.gif"/></alternatives></disp-formula></p>
<p>In the 3D case of (T1w, PDw, T2&#x002A;w) we obtain: <disp-formula id="eqn7"><alternatives><graphic xlink:href="245738_eqn7.gif"/></alternatives></disp-formula></p>
<p>After centering, the data is standardized: <disp-formula id="eqn8"><alternatives><graphic xlink:href="245738_eqn8.gif"/></alternatives></disp-formula> where the total variance is computed by <disp-formula id="eqn9"><alternatives><graphic xlink:href="245738_eqn9.gif"/></alternatives></disp-formula> and the &#x2299; symbol stands for the power operation defined in simplex space, which can be considered as an analogue of multiplication in real space: <disp-formula id="eqn10">
<alternatives><graphic xlink:href="245738_eqn10.gif"/></alternatives></disp-formula></p>
<p>After standardization, the barycentric coordinates are transformed from the 3D simplex space (<italic>S</italic><sup>3</sup>) to 2D real space (<italic>R</italic><sup>2</sup>) by using the isometric logratio (<italic>ilr</italic>) transformation [<xref ref-type="bibr" rid="c51">51</xref>]: <disp-formula id="eqn11"><alternatives><graphic xlink:href="245738_eqn11.gif"/></alternatives></disp-formula> where <italic>H</italic> indicates a Helmert matrix [<xref ref-type="bibr" rid="c52">52</xref>] of 3 rows and 2 columns. The purpose of the transformation is to visualize the distribution in a 2D histogram.</p>
<p>Note that the closure operation described in eq. (2) implies scale invariance. If the receive (and in some cases transmit) field (B1) inhomogeneities for MRI data are similar across modalities and assumed to be having a multiplicative effect on the measured signal, applying closure will mitigate inhomogeneities by canceling out the common multiplicative term (ie. bias field) in each image modality. For instance, assume two voxels contain the same tissue type but have dissimilar intensities due to a multiplicative effect. If before the closure operation voxel 1 has an intensity of 100 in all recorded modalities and voxel 2 has an intensity of 500 in all modalities, then after the closure operation both voxels will have the same compositional description, which would be desired. It should be noted that if B1 inhomogeneities differ significantly across modalities, the closure operation will yield inaccurate compositional descriptions. In this case, we recommend to use bias field correction algorithms before using the compositional data analysis framework. A practical example for this case is that for magnetization-prepared rapid acquisition gradient-echo (MPRAGE) sequences, the transmit field in T1w image is effected by an inversion pulse which is not present in PDw and T2&#x002A;w images. In such cases, individual image bias field correction is recommended.</p>
</sec>
<sec id="s3b">
<label>2.2</label>
<title>2D histogram representation and creation of transfer functions</title>
<p><xref ref-type="fig" rid="fig4">Fig 4</xref> shows how three different 3D MRI contrast images of a human brain (T1w, PDw and T2&#x002A;w brain extracted images; 0.7 mm isotropic resolution; for more details see <xref ref-type="sec" rid="s4">Section 3</xref>) can be represented in a 2D histogram. The 2D histogram is obtained by taking the three MRI contrast images as an input and performing the operations of the CoDa analysis framework described above. In particular, applying the <italic>ilr</italic> transformation to the barycentric coordinates allows the three images to be represented along two dimensions. Different tissue types have different compositional characteristics and therefore occupy different regions in the resulting 2D histogram. WM and GM voxels are separated in two distinct clusters which mainly differ along the T1w axis. CSF voxels occupy the lower left corner of the histogram, which represents a combination of low T1w with high PDw and T2&#x002A;w values. CSF voxels still differ from WM and GM voxels mainly along the T1w axis. In contrast, vessel and dura mater voxels differ from WM, GM and CSF voxels also along the PDw and T2&#x002A;w axes, which makes these voxels to be spread out in the direction orthogonal to the T1w axis. To see how a combination of two MP2RAGE images (UNI, INV2) and one T2&#x002A; image estimated from a multi-echo 3D gradient recalled echo (GRE) sequence are represented in a 2D histogram, please see <xref ref-type="fig" rid="figS1">S1 Fig.</xref></p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Fig 4.</label>
<caption><title>2D histogram representation of three 3D MRI contrast images.</title>
<p>(A) Each voxel is considered as a 3 part composition in 3D real space. The barycentric coordinates of each composition which reside in 3D simplex space are represented in 2D real space after using a isometric log-ratio (ilr) transformation. (B) The ilr coordinates are used to create 2D histograms representing all voxels in the images. The blue lines are the embedded 3D real space primary axes. It should be noted that in this case the ilr coordinates are not easily interpretable by themselves but they are useful to visualize the barycentric coordinates which are interpretable via the embedded real space primary axes. Darker regions in the histogram indicate that many voxels are characterized by this particular scale invariant combination of the image contrasts. In this representation, brain tissue (WM and GM, red dashed lines) becomes separable from non-brain tissue (black dashed lines and arrows).</p></caption>
<graphic xlink:href="245738_fig4.tif"/>
</fig>
<p>The dimensionality reduction accomplished by the <italic>ilr</italic> transformation allows to specify 2D transfer functions even though the input consists of three channels. <xref ref-type="fig" rid="fig5">Fig 5</xref> shows how normalized graph cuts can be used on 2D histogram representation of <italic>ilr</italic> coordinates to create transfer functions. The resulting transfer functions highlight specific clusters that readily separate brain tissue from non-brain tissue.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Fig 5.</label>
<caption><title>Creation of transfer functions using ilr coordinates.</title>
<p>(A) The user starts with the 2D histogram representation of ilr coordinates 1 and 2 (left side) and concurrent visualization of the original brain data (right side). The user can then interact with and select data as described in <xref ref-type="fig" rid="fig3">Fig 3</xref>. (B) The interaction with the 2D histogram results in data-driven shapes, here shaded in pink, green and blue (left side). Voxels selected by those areas are highlighted in corresponding colors against the backdrop of the original brain data (right side). The visualization reveals that the area of the 2D histogram shaded in blue selects brain voxels, while the areas shaded in green and pink select CSF&#x002A; and blood vessel voxels&#x002A;&#x002A;/dura mater&#x002A;&#x002A;&#x002A;, respectively. The arrow with exclamation mark (!) indicates an area affected by T2&#x002A;w image artifacts.</p></caption>
<graphic xlink:href="245738_fig5.tif"/>
</fig>
</sec>
</sec>
<sec id="s4">
<label>3</label>
<title>Validation methods</title>
<sec id="s4a">
<label>3.1</label>
<title>Validation data set overview</title>
<p>In order to validate the methods proposed above, we created two validation data sets based on the acquisition of high-resolution 7 T data of nine subjects and corresponding manually-guided expert segmentations of GM. In particular, we created two validation sets based of on two of the most common acquisition sequences. For five subjects, we collected MPRAGE T1w, PDw, and T2&#x002A; data (we refer to this data set as the MPRAGE data set below). For four different subjects, we collected MP2RAGE data, to obtain unbiased (uni) images, and multi-echo 3D GRE data, to obtain T2&#x002A; maps (we refer to this data set as the MP2RAGE data set below). The MPRAGE data set can be downloaded from [<xref ref-type="bibr" rid="c42">42</xref>]. The MPR2AGE data can be obtained by contacting the corresponding authors.</p>
<sec id="s4a1">
<label>3.1.1</label>
<title>Ethics statement</title>
<p>The experimental procedures were approved by the ethics committee of the Faculty for Psychology and Neuroscience (MPRAGE data set) or the Medical Ethical Committee at Faculty of Health, Medicine and Life Sciences (MP2RAGE data set) at Maastricht University, and were performed in accordance with the approved guidelines and the Declaration of Helsinki. Informed consent was obtained from each participant before conducting the experiments.</p>
</sec>
<sec id="s4a2">
<label>3.1.2</label>
<title>MRI acquisition parameters</title>
<p>All images were acquired on a Siemens 7 T whole body scanner (Siemens Medical Solutions, Erlangen, Germany) using a head radio-frequency (RF) coil (Nova Medical, Wilmington, MA, USA; single transmit, 32 receive channels). In all acquisitions, we used dielectric pads [<xref ref-type="bibr" rid="c53">53</xref>].</p>
<p>For <italic>n</italic>&#x003D;5 subjects (age range 24&#x2013;30, 2 females, no medical condition), the MPRAGE data set consisted of: a T1w image using a 3D MPRAGE sequence (repetition time [TR] &#x003D; 3100 ms; time to inversion [TI] &#x003D; 1500 ms [adiabatic non-selective inversion pulse]; time echo [TE] &#x003D; 2.42 ms; flip angle &#x003D; 5&#x00B0;; generalized auto-calibrating partially parallel acquisitions [GRAPPA] &#x003D; 3 [<xref ref-type="bibr" rid="c54">54</xref>]; field of view [FOV] &#x003D; 224 <italic>&#x00D7;</italic> 224 mm2; matrix size &#x003D; 320 &#x00D7; 320; 256 slices; 0.7 mm isotropic voxels; pixel bandwidth &#x003D; 182 Hz/pixel; first phase encode direction anterior to posterior; second phase encode direction left to right), a PDw image (0.7 mm isotropic) with the same MPRAGE as for the T1w image but without the inversion pulse (TR &#x003D; 1380 ms; TE &#x003D; 2.42 ms; flip angle &#x003D; 5&#x00B0;; GRAPPA &#x003D; 3; FOV &#x003D; 224 &#x00D7; 224 mm; matrix size &#x003D; 320 &#x00D7; 320; 256 slices; 0.7 mm iso. voxels; pixel bandwidth &#x003D; 182 Hz/pixel; first phase encode direction anterior to posterior; second phase encode direction left to right), and a T2&#x002A;w anatomical image using a modified MPRAGE sequence that allows freely setting the TE (TR &#x003D; 4910 ms; TE &#x003D; 16 ms; flip angle &#x003D; 5&#x00B0;; GRAPPA&#x003D; 3; FOV &#x003D; 224 &#x00D7; 224 mm; matrix size &#x003D; 320 &#x00D7; 320; 256 slices; 0.7 mm iso. voxels; pixel bandwidth &#x003D; 473 Hz/pixel; first phase encode direction anterior to posterior; second phase encode direction left to right).</p>
<p>For <italic>n</italic>&#x003D;4 subjects (age range 24&#x2013;58, 2 female, no medical condition) the MP2RAGE data set consisted of: 3D MP2RAGE data (TR &#x003D; 5000 ms; TI1/TI2 &#x003D; 900/2750 ms; TE &#x003D; 2.46 ms; FA1/FA2 &#x003D; 5&#x00B0;/3&#x00B0;; FOV &#x003D; 224&#x00D7;224 mm2; matrix size &#x003D; 320 &#x00D7; 320; slices &#x003D; 240; 0.7 mm iso voxels) [<xref ref-type="bibr" rid="c26">26</xref>]. For the same subjects, T2&#x002A;w images were obtained with a multi-echo 3D GRE sequence (TR &#x003D; 33 ms; TE1/TE2/TE3/TE4 &#x003D;2.53/7.03/12.55/20.35 ms; FA1 &#x003D; 11&#x00B0;; FOV &#x003D; 224 &#x00D7; 159 mm2; matrix &#x003D; 320 &#x00D7; 227; slices &#x003D; 208; 0.7 mm iso voxels). More details on the MP2RAGE data acquisition and the T2&#x002A; estimation can be found in [<xref ref-type="bibr" rid="c55">55</xref>].</p>
</sec>
<sec id="s4a3">
<label>3.1.3</label>
<title>Manually-guided expert segmentations</title>
<p>For every subject, we established &#x201C;ground truth&#x201D; GM classifications via manually-guided expert segmentations. All segmentations were created manually by the same expert (OFG), using ITK-SNAP [<xref ref-type="bibr" rid="c56">56</xref>] and a graphics tablet (Intuos Art; Wacom Co. Ltd; Kazo, Saitama, Japan). To avoid resulting tissue type classification to be ragged, the expert followed a particular processing sequence. The brain was first traversed in a single direction (e.g. sagittally) and the ground truth was established slice-by-slice. Subsequently, the brain was traversed in the two other directions (e.g. axially, then coronally). This sequence was repeated several times across several regions until the GM segmentation of the whole brain was considered of good quality.</p>
</sec>
</sec>
<sec id="s4b">
<label>3.2</label>
<title>Software implementation</title>
<p>We implemented the creation of transfer function based on 2D histograms in an open source Python package called Segmentator [<xref ref-type="bibr" rid="c41">41</xref>], which is built upon several other scientific packages such as Numpy [<xref ref-type="bibr" rid="c57">57</xref>], Scipy [<xref ref-type="bibr" rid="c58">58</xref>], Matplotlib [<xref ref-type="bibr" rid="c59">59</xref>] and Nibabel [<xref ref-type="bibr" rid="c60">60</xref>]. Segmentator allows for selection of data points in a 2D histogram (for example gradient magnitude over intensity) and concurrent visualization of selected brain voxels on a 2D slice. Data points can be selected using a circular sector widget with variable reflex angle and radius. Alternatively, data selection can be performed using the normalized graph cut (n-cut) method (i.e. spectral clustering) as described above. The n-cut algorithm from Scikit-image [<xref ref-type="bibr" rid="c61">61</xref>] was modified to export an augmented output which provides step-wise access to independent branches of the decision tree and employed in Segmentator (the modification is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/ofgulban/scikit-image/tree/ncut-rag-options">https://github.com/ofgulban/scikit-image/tree/ncut-rag-options</ext-link>).</p>
<p>The package provides several options to calculate the gradient magnitude image. All the 2D histogram analyses described in this paper were based on gradient magnitude images that were computed as the Euclidean norm of the first spatial derivative estimated using a 3 &#x00D7; 3 &#x00D7; 3 Scharr kernel [<xref ref-type="bibr" rid="c62">62</xref>, <xref ref-type="bibr" rid="c63">63</xref>]. Subsequently, transfer functions were specified using the normalized graph cut algorithm and user intervention for the selection of the non-brain tissue transfer functions. Processing data for a single subject took about 10 minutes on average. The Segmentator package is openly and freely accessible at <ext-link ext-link-type="uri" xlink:href="https://github.com/ofgulban/segmentator">https://github.com/ofgulban/segmentator</ext-link>. All the operations of the CoDa analysis described above have been implemented as a separate open source Python package [<xref ref-type="bibr" rid="c64">64</xref>] freely accessible at <ext-link ext-link-type="uri" xlink:href="https://github.com/ofgulban/tetrahydra">https://github.com/ofgulban/tetrahydra</ext-link>. This package uses Numpy [<xref ref-type="bibr" rid="c57">57</xref>] and Scipy [<xref ref-type="bibr" rid="c58">58</xref>].</p>
</sec>
<sec id="s4c">
<label>3.3</label>
<title>Segmentation procedure</title>
<p>For both validation data sets, we followed similar procedures, with modifications where necessary to accommodate for differences in the sequences&#x2019; output. Our goal was to obtain initial GM segmentations from established segmentation algorithms and to quantify the improvement in segmentation accuracy that can be obtained when using the methods described here as post-processing steps. To establish the initial GM segmentations we used FSL FAST [<xref ref-type="bibr" rid="c65">65</xref>] and the SPM 12 &#x201C;unified segmentation&#x201D; algorithm [<xref ref-type="bibr" rid="c66">66</xref>] for the MPRAGE data set and FSL FAST and CBS tools [<xref ref-type="bibr" rid="c32">32</xref>] for the MP2RAGE data set. SPM and CBS tools have been developed and benchmarked on MPRAGE and MP2RAGE images respectively. FSL FAST is suited to process either type, so we used it for both data sets. We then quantified the impact of the following additional post-processing steps: (i) using uni-modal input and transfer functions based on 2D histogram representations of intensity and gradient magnitude (see <xref ref-type="sec" rid="s2">Section 1</xref>) or (ii) using multi-modal input and the compositional data analysis framework (see <xref ref-type="sec" rid="s3">Section 2</xref>). These two procedures will be referred to below as the gradient magnitude (GraMag) and the compositional data analysis (CoDa) method, respectively. Both methods resulted in masks that could be used to further refine the initial GM segmentation, e.g. by removing blood vessels and dura mater that were falsely labeled as GM initially. In total, we thus used 2 (MPRAGE and MP2RAGE data set) &#x00D7; 2 (GraMag and CoDa) &#x003D; 4 analysis procedures. All four procedures are summarized in flow chart diagrams (<xref ref-type="fig" rid="figS2">S2 Fig</xref>, <xref ref-type="fig" rid="figS3">S3 Fig</xref>, <xref ref-type="fig" rid="figS4">S4 Fig</xref>, <xref ref-type="fig" rid="figS5">S5 Fig</xref>). Furthermore, in an effort to make our analyses fully reproducible, we made the Python and bash scripts used for pipeline processing openly available at [<xref ref-type="bibr" rid="c43">43</xref>].</p>
<p>For the MPRAGE dataset, we first computed ratio images (T1w divided by PDw) [<xref ref-type="bibr" rid="c25">25</xref>] to reduce inhomogeneities. Ratio images were input to either FSL FAST or SPM 12. FSL FAST was used with default values. The FAST algorithm requires an initial brain extraction procedure that we performed using FSL BET [<xref ref-type="bibr" rid="c67">67</xref>]. Additionally, we masked the images to exclude cerebellum and brain stem tissue with masks that were created manually for all subjects (below referred to as &#x201C;nosub mask&#x201D;). In SPM 12 we used default settings with one exception. We set the number of Gaussians to be modeled to 3 for GM and 2 for WM (default values are 1 and 1). As part of their standard segmentation routine, both FSL FAST and SPM 12 perform initial inhomogeneity correction. We output and inspected the bias corrected images to ensure that the algorithms had converged on plausible solutions. We specified for the FSL FAST algorithm to output hard segmentation labels. Since SPM 12 outputs probabilities for six tissue classes, we transformed this soft output to hard segmentation labels by assigning each voxel to the tissue class with the highest posterior probability. Since the SPM segmentation algorithm works best with unmasked images, we applied the nosub mask only to the resulting SPM GM segmentations, not to the input data. The resulting GM segmentations from FSL and SPM were saved for later evaluation.</p>
<p>For the GraMag method (<xref ref-type="fig" rid="figS2">S2 Fig</xref>) we proceeded with bias-corrected ratio images from either SPM or FSL. Since the GraMag method works best with brain extracted images, we combined SPM&#x2019;s WM and GM segmentation outcomes to form a brain mask and performed brain extraction of the ratio images from SPM. After brain extraction, we also excluded cerebellum and brain stem tissue using the nosub mask. FSL&#x2019;s bias-corrected ratio images images did not require masking as the brain extraction (and cerebellum removal) was already performed before segmentation. We then used the 2D histogram representation of intensity and gradient magnitude together with the hierarchical exploration of normalized graph cut decision trees (as described in <xref ref-type="sec" rid="s2">Section 1</xref>) to create transfer functions. The resulting transfer function was used to separate brain from non-brain tissue voxels. Non-brain tissue voxels were removed from GM if they were included in the initial FSL and SPM segmentations.</p>
<p>For the CoDa method (<xref ref-type="fig" rid="figS3">S3 Fig</xref>) we followed a similar procedure, except that we started from three separate images &#x2013; the bias-corrected T1w, PDw and T2&#x002A;w images. Again, these images were brain extracted and cerebellum and brain stem tissue were removed using the nosub mask. These images were transformed into barycentric coordinates, using the closure operator (as outlined in in <xref ref-type="sec" rid="s3">Section 2</xref>). In this case, there were three barycentric coordinates per voxel constrained to a 2-simplex vector space structure. The triplets of barycentric coordinates were mapped to 2D real-space using the <italic>ilr</italic> transformation. We could therefore proceed with the 2D histogram representation using the first and the second real-space coordinates of the compositions and the hierarchical exploration of normalized graph cut decision trees in this 2D space to separate brain from non-brain tissue voxels. Non-brain tissue voxels were again removed from GM if included in the initial segmentations (of SPM and FSL).</p>
<p>For the MP2RAGE dataset, the T1 map, T1w (uni) and second inversion image from the MP2RAGE sequence were input to CBS tools [<xref ref-type="bibr" rid="c32">32</xref>]. Only the brain-extracted [<xref ref-type="bibr" rid="c67">67</xref>] uni image was input to FSL FAST, since this resulted in higher performance than inputting all three images. Both FSL FAST and CBS tools were run with default settings. Note that the default settings for CBS tools include removal of non brain tissue by estimating dura mater and CSF partial voluming. The resulting GM segmentations from FSL and CBS were saved for later evaluation. For the GraMag method (<xref ref-type="fig" rid="figS4">S4 Fig</xref>), we proceeded with the FSL FAST bias-corrected, brain-extracted and nosub masked uni image and proceeded as for the MPRAGE data set to obtain a secondary brain mask. For the CoDa method (<xref ref-type="fig" rid="figS5">S5 Fig</xref>), we used FSL FAST bias-corrected, brain-extracted and nosub masked uni, second inversion and T2&#x002A; images but otherwise proceeded as for the MPRAGE data set.</p>
<p>We observed susceptibility artifacts in some regions of the brain (mostly inferior frontal lobe) in the T2&#x002A;w images. These artifacts make the affected regions noisy and reduce the effectiveness of using T2&#x002A;w images in the CoDa method. To quantify the effect of these artifacts, we created masks for the artifact-affected regions and ran all our analyses both with and without the artifact regions included. Results shown in the paper were obtained with the affected regions excluded. Results with the affected regions included are shown in the Supplementary Materials.</p>
</sec>
<sec id="s4d">
<label>3.4</label>
<title>Quantification</title>
<p>The segmentation procedures resulted in three different GM segmentations for each data set and initial segmentation algorithm (SPM or CBS and FSL FAST): (i) an initial segmentation without any further changes, (ii) after correction using the GraMag method and (iii) after correction using the CoDa method. To compare segmentation quality among these three outcomes, we calculated the Dice coefficient (DICE) and the Average Hausdorff Distance (AVHD) using the openly available EvaluateSegmentation Tool (2016; VISCERAL, <ext-link ext-link-type="uri" xlink:href="http://www.visceral.eu">http://www.visceral.eu</ext-link>).</p>
<p>The DICE is an overlap-based metric and it is the most popular choices for validating volume segmentations [<xref ref-type="bibr" rid="c68">68</xref>]. We included it here as a familiar reference for the reader. However, overlap-based metrics like the DICE are not recommended for validating segmentation boundaries against the ground truth, as is our aim here, since they are relatively insensitive to boundary errors. In contrast, the AVHD is a distance-based metric and is sensitive to boundary errors [<xref ref-type="bibr" rid="c68">68</xref>]. We therefore consider the AVHD to be a more suitable metric for our purposes and we based our conclusions on the comparisons made with the AVHD.</p>
<p>Given that the AVHD quantifies the similarity of two boundaries, we first extracted WM-GM and GM-CSF boundaries from the ground truth segmentations and the six different GM segmentations before calculating the AVHD. Here, an AVHD of 0 indicates a perfect match between the segmentation and ground truth boundaries, while values &#x003E; 0 indicate a mismatch. In this case, the value represents the average number of voxels by which the two boundaries deviate from one another. For example, an AVHD of 1 indicates that the segmentation boundary, on average, deviates of one voxel from the ground truth.</p>
</sec>
</sec>
<sec id="s5">
<label>4</label>
<title>Validation results</title>
<p>Visual inspection revealed that applying the GraMag method to the MPRAGE data set excluded most of the vessels and dura mater voxels and resulted in a more plausible GM matter definition. The CoDa method equally removed most of the vessels and dura mater voxels. Additionally, the CoDa method excluded structures like the sagittal sinus from the GM definition (see <xref ref-type="fig" rid="fig6">Fig 6</xref> and <xref ref-type="fig" rid="fig7">Fig 7</xref>).</p>
<fig id="fig6" position="float" orientation="portrait" fig-type="figure">
<label>Fig 6.</label>
<caption><title>Comparison of GM segmentation results for MPRAGE data.</title>
<p>GM segmentation results are shown for one representative subject on a transverse (upper row) and a sagittal slice (lower row) of the brain before and after applying the GraMag and CoDa methods. The original image that is input to the segmentation is shown on the left. The original GM segmentation obtained from SPM 12 is shown in red (middle and right column). GM segmentations after additional polishing with brain mask obtained with either the GraMag (middle column) or the CoDa method (right column) are overlaid in blue. Additional masking removes blood vessels, CSF (arrow &#x002A;) and most of dura mater (arrow &#x2020;) voxels from the SPM GM definition. Because of its unique compositional properties, connective tissue from the sagittal sinus can be captured and excluded using the CoDa method (arrow &#x002A;&#x002A;). An area badly affected by the CoDa mask is also indicated with arrow &#x002A;&#x002A;&#x002A;.</p></caption>
<graphic xlink:href="245738_fig6.tif"/>
</fig>
<fig id="fig7" position="float" orientation="portrait" fig-type="figure">
<label>Fig 7.</label>
<caption><title>Comparison of GM segmentation results for MP2RAGE data.</title>
<p>Same conventions as in <xref ref-type="fig" rid="fig6">Fig 6</xref>) but with initial segmentation results obtained with CBS tools instead of SPM 12.</p></caption>
<graphic xlink:href="245738_fig7.tif"/>
</fig>
<p><xref ref-type="table" rid="tbl1">Table 1</xref> compares segmentation performance before and after applying GraMag and CoDa methods to the initial GM segmentations of the MPRAGE dataset. The GraMag method led to an improvement of GM segmentations in all subjects, independently of whether the initial segmentation was done by SPM 12 or FSL FAST. On average, the AVHD decreased from 0.729 &#x00B1;0.087 (mean &#x00B1;standard deviation across subjects) to 0.547 &#x00B1;0.051 for SPM 12 and from 0.531 &#x00B1;0.109 to 0.472 &#x00B1;0.089 for FSL FAST. The GraMag method also increased the DICE coefficient in all subjects individually and, on average, from 0.861 &#x00B1;0.020 to 0.882 &#x00B1;0.016 for SPM 12 and from 0.892 &#x00B1;0.027 to 0.900 &#x00B1;0.089 for FSL FAST. The CoDa method equally yielded improved segmentation performance. Compared to the initial segmentation, the AVHD decreased in all subjects and, on average, to 0.542 &#x00B1;0.054 for SPM 12 and to 0.479 &#x00B1;0.033 for FSL FAST. The DICE coefficent increased for SPM 12 segmentations to 0.871 &#x00B1;0.021 but it decreased for FSL FAST segmentations to 0.879 &#x00B1;0.013. All these results were obtained after exclusion of areas affected by artifacts in the T2s image. For results obtained without the artifact masks, please see <xref ref-type="table" rid="tblS1">S1 Table</xref>.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Segmentation performance scores MPRAGE data set.</title>
<p>The table shows the DICE (larger is better) and AVHD (less is better) for the initial SPM 12 and FSL FAST GM segmentations as well as after additional polishing, using either the gradient magnitude or the compositional data method.</p></caption>
<graphic xlink:href="245738_tbl1.tif"/>
</table-wrap>
<p><xref ref-type="table" rid="tbl2">Table 2</xref> compares segmentation performances before and after applying the GraMag and CoDa methods to the initial GM segmentations of the MP2RAGE dataset. The GraMag method decreased AVHD for every subject and, on average, from 0.413 &#x00B1;0.088 to 0.347 &#x00B1;0.083 for CBS tools and from 0.988 &#x00B1;0.062 to 0.790 &#x00B1;0.088 for FSL FAST. It decreased the DICE coefficient, on average, from 0.907 &#x00B1;to 0.898 &#x00B1;for CBS tools and from 0.831 &#x00B1;to 0.806 &#x00B1;for FSL FAST. The CoDa method decreased the AVDH, on average, to 0.379 &#x00B1;0.082 for CBS tools and to 0.624 &#x00B1;0.069 for FSL FAST. It also increased the DICE coefficient to 0.850 &#x00B1;0.030 for FSL FAST but decreased it to 0.895 &#x00B1;0.024 for CBS tools. For results for the MP2RAGE data obtained without the artifact masks, please see <xref ref-type="table" rid="tblS2">S2 Table</xref>.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption><title>Segmentation performance scores MP2RAGE data set.</title>
<p>The table shows the DICE (larger is better) and AVHD (less is better) for the initial CBS tools and FSL FAST GM segmentations as well as after additional masking, using either the gradient magnitude or the compositional data method.</p></caption>
<graphic xlink:href="245738_tbl2.tif"/>
</table-wrap>
</sec>
<sec id="s6">
<label>5</label>
<title>Discussion</title>
<p>Functional and anatomical MRI studies at the mesoscale (&#x003C; 1 mm isotropic) require accurate and precise definitions of the GM ribbon. Creating such definitions is currently a challenging task since sub-millimeter UHF data bring non-brain structures like blood vessels and dura mater into sharper focus. As a result, segmentation algorithms that have been benchmarked at lower resolution data might falsely label part of these structures as GM. Here we presented two methods (GraMag and CoDa) to correct such mislabeled non-brain voxels efficiently and semi-automatically. The two methods are based on theoretical expectations of how 3D brain data is to be represented in 2D histograms. These expectations imply that brain and non-brain tissue should become separable in 2D histogram representations that are either based on gradient magnitude and intensity or on compositional dimensions. We validated these expectations by implementing the suggested methods in an openly available software package and by quantifying their benefit using a new high-resolution validation data set. The suggested methods improved the initial GM segmentations in general. However, we found some differences in the degree of improvement with respect to (i) the two presented methods, the (ii) type of data and (iii) the algorithm used for initial segmentation.</p>
<p>We will discuss these three influences in turn. First, the two methods differ in their prerequisites and their segmentation improvement. The GraMag method only requires uni-modal input such as T1w/PDw or MP2RAGE uni images, while the CoDa method requires multi-modal input of images with different contrast weightings. This makes the GraMag method the method of choice when only a single input image is available. In accordance with our theoretical expectation, the GraMag method identified and removed blood vessels and dura mater tissue. If multi-dimensional input is available, even bigger improvements might be obtained with the CoDa method. Notably, in contrast to the GraMag method, the CoDa method can additionally capture and remove connected tissue of the sagittal sinus. This tissue is usually falsely labeled as GM because of similar intensity values and spatial proximity. It then requires tedious manual removal. How well the CoDa method performs, however, critically depends on the quality of all the input images and the specific combination of contrasts. Performance can be affected by low quality on a single input image, as was the case here with T2&#x002A; images due to susceptibility artifacts. Furthermore, performance will depend on the specific choice of contrasts and whether these contrasts maximize the compositional difference between brain and non-brain tissue.</p>
<p>Second, we found that the improvements were slightly larger and more consistent across subjects for the MPRAGE than for the MP2RAGE data set. This might be explained by the fact that the MPRAGE data conformed more to our theoretical expectations than the MP2RAGE data set. Especially, we found GM values in the MP2RAGE uni image to be less focused on one particular area of the 2D histogram (<xref ref-type="fig" rid="figS6">S6 Fig</xref>) than the MPRAGE division image. This might result from differences in myelination level across cortical areas and depth [<xref ref-type="bibr" rid="c47">47</xref>, <xref ref-type="bibr" rid="c69">69</xref>, <xref ref-type="bibr" rid="c70">70</xref>], which the MP2RAGE uni image might pick up more than MPRAGE division image [<xref ref-type="bibr" rid="c71">71</xref>].</p>
<p>Third, we observed that the performance of the initial segmentation algorithm had an influence on how much we could further improve the GM segmentation. If performance of the initial segmentation algorithm was already relatively high, the improvement obtained with our methods tended to be smaller. Differences in initial segmentation performance might be explained by whether the algorithm has been benchmarked on this particular type of data. We assume FSL FAST and CBS tools to have been benchmarked on MPRAGE and MP2RAGE data respectively, which would explain their relative high performance for these data types.</p>
<p>Importantly, our goal here was to aid established segmentation pipelines to deal with UHF sub-millimeter resolution data, not to replace those pipelines. Instead, the methods presented here should be considered as an alternative to manual slice-by-slice polishing of segmentations. Manually correcting the labels for even a minority of voxels is very time-consuming and can quickly become unreliable. In contrast, our methods greatly reduce the time required for manual polishing because they offer an efficient 2D summary and are more reliable because they are semi-automatic. We estimated that for whole brain cortical ribbon segmentation on average 7.5 hours of manual work could be saved by using our methods (for more details on this estimation see S1 Appendix). We expect our methods to be most useful for UHF sub-millimeter resolution data since we expect vessels and dura mater to be less visible in images obtained with typical acquisition parameters at conventional field strength. This is because acquisitions at field strength of 3 T and below have less CNR and processing strategies [<xref ref-type="bibr" rid="c25">25</xref>, <xref ref-type="bibr" rid="c26">26</xref>], that usually highlight non-brain tissue at UHF, are absent.</p>
<p>Moreover, we introduced the compositional data analysis framework to the neuroimaging community. Here, we used this framework to combine MRI acquisitions with three different image contrasts in order to derive improved tissue type segmentations. It should be emphasized, however, that the framework can be employed more generally. MRI can provide a multitude of informative images that weight tissue properties to generate the image contrast. The compositional data framework is ideally suited for the analysis and visualization of multiple images as it provides a principled way to combine any number of images. An envisioned future application of the compositional framework to MRI data is to use it to single out targeted cortical or subcortical structures based on their compositional properties. For an example of identifying subcortical structures see <xref ref-type="fig" rid="figS1">S1 Fig</xref>. For discussion of the broader implications of the application of compositional data analysis to images in general please see [<xref ref-type="bibr" rid="c72">72</xref>].</p>
<p>Our theoretical expectations implied that the methods presented here require high-resolution data (&#x003C;1 mm). This requirement was unfortunately not met by most available segmentation validation data sets. Simulated phantom (&#x201C;BrainWeb&#x201D;) data [<xref ref-type="bibr" rid="c73">73</xref>] are available at 1 mm and thus fell short of the resolution required for our purposes. Although an updated data set (&#x201C;updated BrainWeb&#x201D;, [<xref ref-type="bibr" rid="c74">74</xref>, <xref ref-type="bibr" rid="c75">75</xref>]) is available at higher resolution, the simulations in this data set were based on initial 3T MRI acquisitions. As a consequence, the updated BrainWeb data revealed considerably less bright vessel and dura mater voxels than 7 T data usually does and was not suitable to validate our methods.</p>
<p>These considerations led us to create our own high-resolution segmentation validation data sets for which we established the &#x201C;ground truth&#x201D; via manually-guided expert segmentation. While expert segmentations have well-known drawbacks [<xref ref-type="bibr" rid="c33">33</xref>, <xref ref-type="bibr" rid="c76">76</xref>], they also have important advantages to alternative methods of establishing the ground truth, such as simulated phantom data. In particular, creating a validation data set based on empirical data and expert segmentations allowed us to benchmark our methods under conditions where our theoretical assumptions were met and the expected distribution of image intensities in 7 T brain data was respected. Being aware of the problems with expert segmentations, we alleviate concerns about the quality of our expert judgment and consequently the validity of the results presented here by making the data sets and corresponding ground truth segmentations as well as our precessing scripts available. This will allow other researchers to come up with their own judgment of the quality of the ground truth segmentation and validation data. In case changes to the ground truth are suggested and implemented, quantification could be re-run easily.</p>
<p>The 2D histogram method presented here is, in principle, capable of generating its own exhaustive tissue-type classifications, i.e. it does not necessarily depend on established segmentation pipelines to derive GM and WM labels. While we expect the 2D histogram method to give no advantage over established segmentation algorithms under standard conditions, the histogram method will compare well in cases where standard algorithms fail. Importantly, the 2D histogram method used here does not assume the data to conform to any atlas or template shape. Therefore, it is suitable also for acquisitions with only partial coverage (surface coils) or for specific populations (e.g. infants).</p>
<p>Using histogram-based methods would be more attractive if the process of specifying transfer functions was fully automatic. We note that there is no principled obstacle to doing this. Indeed, information-theoretic measures have been suggested [<xref ref-type="bibr" rid="c39">39</xref>] that would make the normalized graph-cut application fully automatic, given the specification of an appropriate stopping criterion. The transfer functions (i.e. the circles and arcs applied to our 2D histograms) that we observed for the different brain tissue types were stable across subjects. This would allow to define probabilistic templates in the histogram space and transform the methods proposed here to a fully automatic exhaustive tissue-type classifications.</p>
<p>By making our validation data set(s) publicly available, we hope to inspire further algorithmic testing and development. There is currently a lack of validation data for the performance of tissue-type classification of MRI data acquired at ultra-high fields with sub-millimeter resolution. By publishing our data, our code and our work flow, we invite fellow scientists to benefit from our work but also to further contribute to it. The neuroimaging community can use our data to test the performance of entirely new methods or modifications to existing segmentation algorithms. Contributions could be made in the form of additional high-resolution data, more ground truth segmentations and algorithmic improvement. Anticipating such algorithmic improvements, we envision a future where segmentation of volumetric images will become gradually less laborious despite increasing resolution and volume of the data.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We would like to thank Thomas Emmerling for his guidance and advise on software implementation and Rainer Goebel for his encouragement and valuable comments. This work was financed by the Netherlands Organisation for Scientific Research (NWO). The authors O.F.G. and F.D.M. as well as data acquisition for the MPRAGE data set were supported by NWO VIDI grant 864-13-012. Author M.S. was supported by NWO research talent grant 406-14-108. Author I.M. was supported by NWO research talent grant 406-14-085. Author R.H. and acquisition of the MP2RAGE was funded by Technology Foundation STW (grant 12724).</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Vaughan</surname> <given-names>JT</given-names></string-name>, <string-name><surname>Garwood</surname> <given-names>M</given-names></string-name>, <string-name><surname>Collins</surname> <given-names>CM</given-names></string-name>, <string-name><surname>Liu</surname> <given-names>W</given-names></string-name>, <string-name><surname>Delabarre</surname> <given-names>L</given-names></string-name>, <string-name><surname>Adriany</surname> <given-names>G</given-names></string-name>, <etal>et al.</etal> <article-title>7T vs. 4T: RF power, homogeneity, and signal-to-noise comparison in head images</article-title>. <source>Magnetic Resonance in Medicine</source>. <year>2001</year>;<volume>46</volume>(<issue>1</issue>):<fpage>24</fpage>&#x2013;<lpage>30</lpage>. doi:<pub-id pub-id-type="doi">10.1002/mrm.1156</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="other"><string-name><surname>Duyn</surname> <given-names>JH</given-names></string-name>. <chapter-title>The future of ultra-high field MRI and fMRI for study of the human brain</chapter-title>; <year>2012</year>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Ugurbil</surname> <given-names>K.</given-names></string-name> <article-title>Magnetic resonance imaging at ultrahigh fields</article-title>. <source>IEEE Transactions on Biomedical Engineering</source>. <year>2014</year>;<volume>61</volume>(<issue>5</issue>):<fpage>1364</fpage>&#x2013;<lpage>1379</lpage>. doi:<pub-id pub-id-type="doi">10.1109/TBME.2014.2313619</pub-id>.</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="other"><string-name><surname>Martino</surname> <given-names>FD</given-names></string-name>, <string-name><surname>Yacoub</surname> <given-names>E</given-names></string-name>, <string-name><surname>Kemper</surname> <given-names>V</given-names></string-name>, <string-name><surname>Moerel</surname> <given-names>M</given-names></string-name>, <string-name><surname>Uludag</surname> <given-names>K</given-names></string-name>, <string-name><surname>Weerd</surname> <given-names>PD</given-names></string-name>, <etal>et al.</etal> <article-title>The impact of ultra-high field MRI on cognitive and computational neuroimaging</article-title>. <source>NeuroImage</source>. <year>2017</year>;doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2017.03.060">https://doi.org/10.1016/j.neuroimage.2017.03.060</ext-link>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="other"><string-name><surname>Kemper</surname> <given-names>VG</given-names></string-name>, <string-name><surname>Martino</surname> <given-names>FD</given-names></string-name>, <string-name><surname>Emmerling</surname> <given-names>TC</given-names></string-name>, <string-name><surname>Yacoub</surname> <given-names>E</given-names></string-name>, <string-name><surname>Goebel</surname> <given-names>R.</given-names></string-name> <article-title>High resolution data analysis strategies for mesoscale human functional MRI at 7 and 9.4T</article-title>. <source>NeuroImage</source>. <year>2017</year>;doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2017.03.058">https://doi.org/10.1016/j.neuroimage.2017.03.058</ext-link>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="other"><string-name><surname>Dumoulin</surname> <given-names>SO</given-names></string-name>, <string-name><surname>Fracasso</surname> <given-names>A</given-names></string-name>, <string-name><surname>van der Zwaag</surname> <given-names>W</given-names></string-name>, <string-name><surname>Siero</surname> <given-names>JCW</given-names></string-name>, <string-name><surname>Petridou</surname> <given-names>N</given-names></string-name>. <article-title>Ultra-high field MRI: Advancing systems neuroscience towards mesoscopic human brain function</article-title>. <source>NeuroImage</source>. <year>2017</year>;doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2017.01.028">https://doi.org/10.1016/j.neuroimage.2017.01.028</ext-link>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="other"><string-name><surname>Polimeni</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Renvall</surname> <given-names>V</given-names></string-name>, <string-name><surname>Zaretskaya</surname> <given-names>N</given-names></string-name>, <string-name><surname>Fischl</surname> <given-names>B.</given-names></string-name> <article-title>Analysis strategies for high-resolution UHF-fMRI data</article-title>. <source>NeuroImage</source>. <year>2017</year>;doi:<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2017.04.053">https://doi.org/10.1016/j.neuroimage.2017.04.053</ext-link>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="other"><string-name><surname>Trampel</surname> <given-names>R</given-names></string-name>, <string-name><surname>Bazin</surname> <given-names>PL</given-names></string-name>, <string-name><surname>Pine</surname> <given-names>K</given-names></string-name>, <string-name><surname>Weiskopf</surname> <given-names>N.</given-names></string-name> <article-title>In-vivo magnetic resonance imaging (MRI) of laminae in the human cortex</article-title>. <source>NeuroImage</source>. <year>2017</year>;(September):<fpage>1</fpage>&#x2013;<lpage>9</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.09.037</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Polimeni</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Fischl</surname> <given-names>B</given-names></string-name>, <string-name><surname>Greve</surname> <given-names>DN</given-names></string-name>, <string-name><surname>Wald</surname> <given-names>LL.</given-names></string-name> <article-title>Laminar analysis of 7T BOLD using an imposed spatial activation pattern in human V1</article-title>. <source>NeuroImage</source>. <year>2010</year>;<volume>52</volume>(<issue>4</issue>):<fpage>1334</fpage>&#x2013;<lpage>1346</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.05.005</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Koopmans</surname> <given-names>PJ</given-names></string-name>, <string-name><surname>Barth</surname> <given-names>M</given-names></string-name>, <string-name><surname>Orzada</surname> <given-names>S</given-names></string-name>, <string-name><surname>Norris</surname> <given-names>DG.</given-names></string-name> <article-title>Multi-echo fMRI of the cortical laminae in humans at 7 T</article-title>. <source>NeuroImage</source>. <year>2011</year>;<volume>56</volume>(<issue>3</issue>):<fpage>1276</fpage>&#x2013;<lpage>1285</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.02.042</pub-id>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>De Martino</surname> <given-names>F</given-names></string-name>, <string-name><surname>Zimmermann</surname> <given-names>J</given-names></string-name>, <string-name><surname>Muckli</surname> <given-names>L</given-names></string-name>, <string-name><surname>Ugurbil</surname> <given-names>K</given-names></string-name>, <string-name><surname>Yacoub</surname> <given-names>E</given-names></string-name>, <string-name><surname>Goebel</surname> <given-names>R.</given-names></string-name> <article-title>Cortical Depth Dependent Functional Responses in Humans at 7T: Improved Specificity with 3D GRASE</article-title>. <source>PLoS ONE</source>. <year>2013</year>;<volume>8</volume>(<issue>3</issue>):<fpage>e60514</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0060514</pub-id>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="other"><string-name><surname>Huber</surname> <given-names>L</given-names></string-name>, <string-name><surname>Goense</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kennerley</surname> <given-names>AJ</given-names></string-name>, <string-name><surname>Trampel</surname> <given-names>R</given-names></string-name>, <string-name><surname>Guidi</surname> <given-names>M</given-names></string-name>, <string-name><surname>Reimer</surname> <given-names>E</given-names></string-name>, <etal>et al.</etal> <article-title>Cortical lamina-dependent blood volume changes in human brain at 7 T</article-title>. <source>NeuroImage</source>. <year>2015</year>;107:<fpage>23</fpage>&#x2013;<lpage>33</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.11.046</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Muckli</surname> <given-names>L</given-names></string-name>, <string-name><surname>De Martino</surname> <given-names>F</given-names></string-name>, <string-name><surname>Vizioli</surname> <given-names>L</given-names></string-name>, <string-name><surname>Petro</surname> <given-names>LS</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>FW</given-names></string-name>, <string-name><surname>Ugurbil</surname> <given-names>K</given-names></string-name>, <etal>et al.</etal> <article-title>Contextual Feedback to Superficial Layers of V1</article-title>. <source>Current Biology</source>. <year>2015</year>;<volume>25</volume>(<issue>20</issue>):<fpage>2690</fpage>&#x2013;<lpage>2695</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2015.08.057</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Kok</surname> <given-names>P</given-names></string-name>, <string-name><surname>Bains</surname> <given-names>LJ</given-names></string-name>, <string-name><surname>Van Mourik</surname> <given-names>T</given-names></string-name>, <string-name><surname>Norris</surname> <given-names>DG</given-names></string-name>, <string-name><surname>De Lange</surname> <given-names>FP</given-names></string-name>. <article-title>Selective activation of the deep layers of the human primary visual cortex by top-down feedback</article-title>. <source>Current Biology</source>. <year>2016</year>;<volume>26</volume>(<issue>3</issue>):<fpage>371</fpage>&#x2013;<lpage>376</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cub.2015.12.038</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Waehnert</surname> <given-names>MD</given-names></string-name>, <string-name><surname>Dinse</surname> <given-names>J</given-names></string-name>, <string-name><surname>Weiss</surname> <given-names>M</given-names></string-name>, <string-name><surname>Streicher</surname> <given-names>MN</given-names></string-name>, <string-name><surname>Waehnert</surname> <given-names>P</given-names></string-name>, <string-name><surname>Geyer</surname> <given-names>S</given-names></string-name>, <etal>et al.</etal> <article-title>Anatomically motivated modeling of cortical laminae</article-title>. <source>NeuroImage</source>. <year>2014</year>;<volume>93</volume>:<fpage>210</fpage>&#x2013;<lpage>220</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.03.078</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Yacoub</surname> <given-names>E</given-names></string-name>, <string-name><surname>Harel</surname> <given-names>N</given-names></string-name>, <string-name><surname>Ugurbil</surname> <given-names>K.</given-names></string-name> <article-title>High-field fMRI unveils orientation columns in humans</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2008</year>;<volume>105</volume>(<issue>30</issue>):<fpage>10607</fpage>&#x2013;<lpage>10612</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.0804110105</pub-id>.</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Zimmermann</surname> <given-names>J</given-names></string-name>, <string-name><surname>Goebel</surname> <given-names>R</given-names></string-name>, <string-name><surname>de Martino</surname> <given-names>F</given-names></string-name>, <string-name><surname>van de Moortele</surname> <given-names>PF</given-names></string-name>, <string-name><surname>Feinberg</surname> <given-names>D</given-names></string-name>, <string-name><surname>Adriany</surname> <given-names>G</given-names></string-name>, <etal>et al.</etal> <article-title>Mapping the organization of axis of motion selective features in human area mt using high-field fmri</article-title>. <source>PLoS ONE</source>. <year>2011</year>;<volume>6</volume>(<issue>12</issue>):<fpage>1</fpage>&#x2013;<lpage>10</lpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0028716</pub-id>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>De Martino</surname> <given-names>F</given-names></string-name>, <string-name><surname>Moerel</surname> <given-names>M</given-names></string-name>, <string-name><surname>Ugurbil</surname> <given-names>K</given-names></string-name>, <string-name><surname>Goebel</surname> <given-names>R</given-names></string-name>, <string-name><surname>Yacoub</surname> <given-names>E</given-names></string-name>, <string-name><surname>Formisano</surname> <given-names>E.</given-names></string-name> <article-title>Frequency preference and attention effects across cortical depths in the human primary auditory cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2015</year>;<volume>112</volume>(<issue>52</issue>):<fpage>16036</fpage>&#x2013;<lpage>16041</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1507552112</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Goncalves</surname> <given-names>NR</given-names></string-name>, <string-name><surname>Ban</surname> <given-names>H</given-names></string-name>, <string-name><surname>Sanchez-Panchuelo</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Francis</surname> <given-names>ST</given-names></string-name>, <string-name><surname>Schluppeck</surname> <given-names>D</given-names></string-name>, <string-name><surname>Welchman</surname> <given-names>AE.</given-names></string-name> <article-title>7 Tesla fMRI Reveals Systematic Functional Organization for Binocular Disparity in Dorsal Visual Cortex</article-title>. <source>Journal of Neuroscience</source>. <year>2015</year>;<volume>35</volume>(<issue>7</issue>):<fpage>3056</fpage>&#x2013;<lpage>3072</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.3047-14.2015</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Nasr</surname> <given-names>S</given-names></string-name>, <string-name><surname>Polimeni</surname> <given-names>JR</given-names></string-name>, <string-name><surname>Tootell</surname> <given-names>RBH.</given-names></string-name> <article-title>Interdigitated Color-and Disparity-Selective Columns within Human Visual Cortical Areas V2 and V3</article-title>. <source>Journal of Neuroscience</source>. <year>2016</year>;<volume>36</volume>(<issue>6</issue>). doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.3518-15.2016</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Tootell</surname> <given-names>RBH</given-names></string-name>, <string-name><surname>Nasr</surname> <given-names>S</given-names></string-name>. <article-title>Columnar Segregation of Magnocellular and Parvocellular Streams in Human Extrastriate Cortex</article-title>. <source>The Journal of Neuroscience</source>. <year>2017</year>; p. <fpage>0690</fpage>&#x2013;<lpage>17</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.0690-17.2017</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Harvey</surname> <given-names>BM</given-names></string-name>, <string-name><surname>Klein</surname> <given-names>BP</given-names></string-name>, <string-name><surname>Petridou</surname> <given-names>N</given-names></string-name>, <string-name><surname>Dumoulin</surname> <given-names>SO.</given-names></string-name> <article-title>Topographic Representation of Numerosity in the Human Parietal Cortex</article-title>. <source>Science</source>. <year>2013</year>;<volume>341</volume>(<issue>6150</issue>):<fpage>1123</fpage>&#x2013;<lpage>1126</lpage>. doi:<pub-id pub-id-type="doi">10.1126/science.1239052</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Harvey</surname> <given-names>BM</given-names></string-name>, <string-name><surname>Fracasso</surname> <given-names>A</given-names></string-name>, <string-name><surname>Petridou</surname> <given-names>N</given-names></string-name>, <string-name><surname>Dumoulin</surname> <given-names>SO.</given-names></string-name> <article-title>Topographic representations of object size and relationships with numerosity reveal generalized quantity processing in human parietal cortex</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>. <year>2015</year>;<volume>112</volume>(<issue>44</issue>):<fpage>13525</fpage>&#x2013;<lpage>30</lpage>. doi:<pub-id pub-id-type="doi">10.1073/pnas.1515414112</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Fracasso</surname> <given-names>A</given-names></string-name>, <string-name><surname>Petridou</surname> <given-names>N</given-names></string-name>, <string-name><surname>Dumoulin</surname> <given-names>SO.</given-names></string-name> <article-title>Systematic variation of population receptive field properties across cortical depth in human visual cortex</article-title>. <source>NeuroImage</source>. <year>2016</year>;<volume>139</volume>:<fpage>427</fpage>&#x2013;<lpage>438</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.06.048</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Van de Moortele</surname> <given-names>PF</given-names></string-name>, <string-name><surname>Auerbach</surname> <given-names>EJ</given-names></string-name>, <string-name><surname>Olman</surname> <given-names>C</given-names></string-name>, <string-name><surname>Yacoub</surname> <given-names>E</given-names></string-name>, <string-name><surname>U&#x011F;urbil</surname> <given-names>K</given-names></string-name>, <string-name><surname>Moeller</surname> <given-names>S</given-names></string-name>. <article-title>T1 weighted brain images at 7 Tesla unbiased for Proton Density, T2&#x002A; contrast and RF coil receive B1 sensitivity with simultaneous vessel visualization</article-title>. <source>NeuroImage</source>. <year>2009</year>;<volume>46</volume>(<issue>2</issue>):<fpage>432</fpage>&#x2013;<lpage>446</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.02.009</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Marques</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Kober</surname> <given-names>T</given-names></string-name>, <string-name><surname>Krueger</surname> <given-names>G</given-names></string-name>, <string-name><surname>van der Zwaag</surname> <given-names>W</given-names></string-name>, <string-name><surname>Van de Moortele</surname> <given-names>PF</given-names></string-name>, <string-name><surname>Gruetter</surname> <given-names>R.</given-names></string-name> <article-title>MP2RAGE, a self bias-field corrected sequence for improved segmentation and T1-mapping at high field</article-title>. <source>NeuroImage</source>. <year>2010</year>;<volume>49</volume>(<issue>2</issue>):<fpage>1271</fpage>&#x2013;<lpage>1281</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.10.002</pub-id>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Viviani</surname> <given-names>R</given-names></string-name>, <string-name><surname>Pracht</surname> <given-names>ED</given-names></string-name>, <string-name><surname>Brenner</surname> <given-names>D</given-names></string-name>, <string-name><surname>Beschoner</surname> <given-names>P</given-names></string-name>, <string-name><surname>Stingl</surname> <given-names>JC</given-names></string-name>, <string-name><surname>St&#x00F6;cker</surname> <given-names>T</given-names></string-name>. <article-title>Multimodal MEMPRAGE, FLAIR, and R2&#x002A; segmentation to resolve dura and vessels from cortical gray matter</article-title>. <source>Frontiers in Neuroscience</source>. <year>2017</year>;<volume>11</volume>(MAY):<fpage>1</fpage>&#x2013;<lpage>13</lpage>. doi:<pub-id pub-id-type="doi">10.3389/fnins.2017.00258</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>van der Kouwe</surname> <given-names>AJW</given-names></string-name>, <string-name><surname>Benner</surname> <given-names>T</given-names></string-name>, <string-name><surname>Salat</surname> <given-names>DH</given-names></string-name>, <string-name><surname>Fischl</surname> <given-names>B.</given-names></string-name> <article-title>Brain morphometry with multiecho MPRAGE</article-title>. <source>NeuroImage</source>. <year>2008</year>;<volume>40</volume>(<issue>2</issue>):<fpage>559</fpage>&#x2013;<lpage>569</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.12.025</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Viviani</surname> <given-names>R.</given-names></string-name> <article-title>A Digital Atlas of Middle to Large Brain Vessels and Their Relation to Cortical and Subcortical Structures</article-title>. <source>Frontiers in Neuroanatomy</source>. <year>2016</year>;<volume>10</volume>(February):<fpage>12</fpage>. doi:<pub-id pub-id-type="doi">10.3389/FNANA.2016.00012</pub-id>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Helms</surname> <given-names>G</given-names></string-name>, <string-name><surname>Kallenberg</surname> <given-names>K</given-names></string-name>, <string-name><surname>Dechent</surname> <given-names>P.</given-names></string-name> <article-title>Contrast-driven approach to intracranial segmentation using a combination of T2-and T1-weighted 3D MRI data sets</article-title>. <source>Journal of Magnetic Resonance Imaging</source>. <year>2006</year>;<volume>24</volume>(<issue>4</issue>):<fpage>790</fpage>&#x2013;<lpage>795</lpage>. doi:<pub-id pub-id-type="doi">10.1002/jmri.20692</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Helms</surname> <given-names>G.</given-names></string-name> <article-title>Segmentation of human brain using structural MRI</article-title>. <source>Magnetic Resonance Materials in Physics, Biology and Medicine</source>. <year>2016</year>;<volume>29</volume>(<issue>2</issue>):<fpage>111</fpage>&#x2013;<lpage>124</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s10334-015-0518-z</pub-id>.</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Bazin</surname> <given-names>PL</given-names></string-name>, <string-name><surname>Weiss</surname> <given-names>M</given-names></string-name>, <string-name><surname>Dinse</surname> <given-names>J</given-names></string-name>, <string-name><surname>Sch??fer</surname> <given-names>A</given-names></string-name>, <string-name><surname>Trampel</surname> <given-names>R</given-names></string-name>, <string-name><surname>Turner</surname> <given-names>R.</given-names></string-name> <article-title>A computational framework for ultra-high resolution cortical segmentation at 7 Tesla</article-title>. <source>NeuroImage</source>. <year>2014</year>;<volume>93</volume>:<fpage>201</fpage>&#x2013;<lpage>209</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.03.077</pub-id>.</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Despotovi&#x0107;</surname> <given-names>I</given-names></string-name>, <string-name><surname>Goossens</surname> <given-names>B</given-names></string-name>, <string-name><surname>Philips</surname> <given-names>W.</given-names></string-name> <article-title>MRI segmentation of the human brain: challenges, methods, and applications</article-title>. <source>Computational and mathematical methods in medicine</source>. <year>2015</year>;<volume>2015</volume>:<fpage>450341</fpage>. doi:<pub-id pub-id-type="doi">10.1155/2015/450341</pub-id>.</mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Renvall</surname> <given-names>V</given-names></string-name>, <string-name><surname>Witzel</surname> <given-names>T</given-names></string-name>, <string-name><surname>Wald</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Polimeni</surname> <given-names>JR.</given-names></string-name> <article-title>Automatic cortical surface reconstruction of high-resolution T1 echo planar imaging data</article-title>. <source>NeuroImage</source>. <year>2016</year>;<volume>134</volume>:<fpage>338</fpage>&#x2013;<lpage>354</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.04.004</pub-id>.</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="other"><string-name><surname>Kashyap</surname> <given-names>S</given-names></string-name>, <string-name><surname>Ivanov</surname> <given-names>D</given-names></string-name>, <string-name><surname>Havlicek</surname> <given-names>M</given-names></string-name>, <string-name><surname>Poser</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Uluda&#x011F;</surname> <given-names>K</given-names></string-name>. <chapter-title>Impact of acquisition and analysis strategies on cortical depth-dependent fMRI</chapter-title>; <year>2017</year>.</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="book"><string-name><surname>Kindlmann</surname> <given-names>G</given-names></string-name>, <string-name><surname>Durkin</surname> <given-names>JW</given-names></string-name>. <chapter-title>Semi-automatic generation of transfer functions for direct volume rendering</chapter-title>. In: <conf-name>Proceedings of the 1998 IEEE symposium on Volume visualization &#x2013; VVS &#x2018;98</conf-name>. <publisher-loc>New York, New York, USA</publisher-loc>: <publisher-name>ACM Press</publisher-name>; <year>1998</year>. p. <fpage>79</fpage>&#x2013;<lpage>86</lpage>.</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="other"><string-name><surname>Kniss</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kindlmann</surname> <given-names>G</given-names></string-name>, <string-name><surname>Hansen</surname> <given-names>C.</given-names></string-name> <article-title>Interactive volume rendering using multi-dimensional transfer functions and direct manipulation widgets</article-title>. In: <conf-name>Proceedings Visualization, 2001. VIS &#x2018;01. IEEE</conf-name>; <year>2001</year>. p. <fpage>255</fpage>&#x2013;<lpage>562</lpage>.</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="other"><string-name><surname>Kniss</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kindlmann</surname> <given-names>G</given-names></string-name>, <string-name><surname>Hansen</surname> <given-names>CD.</given-names></string-name> <article-title>Multidimensional transfer functions for volume rendering</article-title>. <source>Visualization Handbook</source>. <year>2005</year>; p. <fpage>189</fpage>&#x2013;<lpage>209</lpage>. doi:<pub-id pub-id-type="doi">10.1016/B978-012387582-2/50011-3</pub-id>.</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Ip</surname> <given-names>CY</given-names></string-name>, <string-name><surname>Varshney</surname> <given-names>A</given-names></string-name>, <string-name><surname>Jaja</surname> <given-names>J.</given-names></string-name> <article-title>Hierarchical exploration of volumes using multilevel segmentation of the intensity-gradient histograms</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source>. <year>2012</year>;<volume>18</volume>(<issue>12</issue>):<fpage>2355</fpage>&#x2013;<lpage>2363</lpage>. doi:<pub-id pub-id-type="doi">10.1109/TVCG.2012.231</pub-id>.</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="book"><string-name><surname>Pawlowsky-Glahn</surname> <given-names>V</given-names></string-name>, <string-name><surname>Egozcue</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Tolosana-Delgado</surname> <given-names>R</given-names></string-name>. <chapter-title>Modelling and Analysis of Compositional Data</chapter-title>. <publisher-loc>Chichester, UK</publisher-loc>: <publisher-name>John Wiley &#x0026; Sons, Ltd</publisher-name>; <year>2015</year>.</mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="other"><string-name><surname>Gulban</surname> <given-names>OF</given-names></string-name>, <string-name><surname>Schneider</surname> <given-names>M.</given-names></string-name> <source>Segmentator v1.3.0</source>; <year>2017</year>. Available from: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.999487">https://doi.org/10.5281/zenodo.999487</ext-link>.</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="other"><string-name><surname>Gulban</surname> <given-names>OF</given-names></string-name>, <string-name><surname>Schneider</surname> <given-names>M</given-names></string-name>, <string-name><surname>Marquardt</surname> <given-names>I</given-names></string-name>, <string-name><surname>Haast</surname> <given-names>RAM</given-names></string-name>, <string-name><surname>Martino</surname> <given-names>FD</given-names></string-name>. <article-title>Dataset: A scalable method to improve gray matter segmentation at ultra high field MRI</article-title>.; <year>2017</year>. Available from: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1117859">https://doi.org/10.5281/zenodo.1117859</ext-link>.</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="other"><string-name><surname>Schneider</surname> <given-names>M</given-names></string-name>, <string-name><surname>Gulban</surname> <given-names>OF</given-names></string-name>. <article-title>Segmentator paper processing scripts</article-title>; <year>2017</year>. Available from: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1098294">https://doi.org/10.5281/zenodo.1098294</ext-link>.</mixed-citation></ref>
<ref id="c44"><label>44.</label><mixed-citation publication-type="journal"><string-name><surname>Kniss</surname> <given-names>J</given-names></string-name>, <string-name><surname>Kindlmann</surname> <given-names>G</given-names></string-name>, <string-name><surname>Hansen</surname> <given-names>C.</given-names></string-name> <article-title>Multidimensional transfer functions for interactive volume rendering</article-title>. <source>IEEE Transactions on Visualization and Computer Graphics</source>. <year>2002</year>;<volume>8</volume>(<issue>3</issue>):<fpage>270</fpage>&#x2013;<lpage>285</lpage>. doi:<pub-id pub-id-type="doi">10.1109/TVCG.2002.1021579</pub-id>.</mixed-citation></ref>
<ref id="c45"><label>45.</label><mixed-citation publication-type="journal"><string-name><surname>Ljung</surname> <given-names>P</given-names></string-name>, <string-name><surname>Kr&#x00FC;ger</surname> <given-names>J</given-names></string-name>, <string-name><surname>Groller</surname> <given-names>E</given-names></string-name>, <string-name><surname>Hadwiger</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hansen</surname> <given-names>CD</given-names></string-name>, <string-name><surname>Ynnerman</surname> <given-names>A.</given-names></string-name> <article-title>State of the Art in Transfer Functions for Direct Volume Rendering</article-title>. <source>Computer Graphics Forum</source>. <year>2016</year>;<volume>35</volume>(<issue>3</issue>):<fpage>669</fpage>&#x2013;<lpage>691</lpage>. doi:<pub-id pub-id-type="doi">10.1111/cgf.12934</pub-id>.</mixed-citation></ref>
<ref id="c46"><label>46.</label><mixed-citation publication-type="journal"><string-name><surname>Glasser</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Van Essen</surname> <given-names>DC</given-names></string-name>. <article-title>Mapping Human Cortical Areas In Vivo Based on Myelin Content as Revealed by T1-and T2-Weighted MRI</article-title>. <source>Journal of Neuroscience</source>. <year>2011</year>;<volume>31</volume>(<issue>32</issue>):<fpage>11597</fpage>&#x2013;<lpage>11616</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.2180-11.2011</pub-id>.</mixed-citation></ref>
<ref id="c47"><label>47.</label><mixed-citation publication-type="journal"><string-name><surname>De Martino</surname> <given-names>F</given-names></string-name>, <string-name><surname>Moerel</surname> <given-names>M</given-names></string-name>, <string-name><surname>Xu</surname> <given-names>J</given-names></string-name>, <string-name><surname>Van De Moortele</surname> <given-names>PF</given-names></string-name>, <string-name><surname>Ugurbil</surname> <given-names>K</given-names></string-name>, <string-name><surname>Goebel</surname> <given-names>R</given-names></string-name>, <etal>et al.</etal> <article-title>High-resolution mapping of myeloarchitecture in vivo: Localization of auditory areas in the human brain</article-title>. <source>Cerebral Cortex</source>. <year>2015</year>;<volume>25</volume>(<issue>10</issue>):<fpage>3394</fpage>&#x2013;<lpage>3405</lpage>. doi:<pub-id pub-id-type="doi">10.1093/cercor/bhu150</pub-id>.</mixed-citation></ref>
<ref id="c48"><label>48.</label><mixed-citation publication-type="book"><string-name><surname>Fauvel</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wilson</surname> <given-names>R</given-names></string-name>, <string-name><surname>Flood</surname> <given-names>R.</given-names></string-name> <chapter-title>M&#x00F6;bius and his Band: Mathematics and astronomy in nineteenth-century Germany</chapter-title>. <publisher-loc>Oxford, England</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>1993</year>.</mixed-citation></ref>
<ref id="c49"><label>49.</label><mixed-citation publication-type="book"><string-name><surname>Munkres</surname> <given-names>JR.</given-names></string-name> <chapter-title>Elements of Algebraic Topology</chapter-title>. <edition>1st ed</edition>. <publisher-name>Addison-Wesley</publisher-name>; <year>1984</year>.</mixed-citation></ref>
<ref id="c50"><label>50.</label><mixed-citation publication-type="journal"><string-name><surname>Aitchison</surname> <given-names>J.</given-names></string-name> <article-title>The Statistical Analysis of Compositional Data</article-title>. <source>Journal of the Royal Statistical Society</source>. <year>1982</year>;<volume>44</volume>(<issue>2</issue>):<fpage>139</fpage>&#x2013;<lpage>177</lpage>.</mixed-citation></ref>
<ref id="c51"><label>51.</label><mixed-citation publication-type="journal"><string-name><surname>Egozcue</surname> <given-names>JJ</given-names></string-name>, <string-name><surname>Pawlowsky-Glahn</surname> <given-names>V</given-names></string-name>, <string-name><surname>Mateu-Figueras</surname> <given-names>G</given-names></string-name>, <string-name><surname>Barcel&#x00F3;-Vidal</surname> <given-names>C</given-names></string-name>. <article-title>Isometric Logratio Transformations for Compositional Data Analysis</article-title>. <source>Mathematical Geology</source>. <year>2003</year>;<volume>35</volume>(<issue>3</issue>):<fpage>279</fpage>&#x2013;<lpage>300</lpage>. doi:<pub-id pub-id-type="doi">10.1023/A:1023818214614</pub-id>.</mixed-citation></ref>
<ref id="c52"><label>52.</label><mixed-citation publication-type="journal"><string-name><surname>Lancaster</surname> <given-names>HO.</given-names></string-name> <article-title>The Helmert Matrices</article-title>. <source>The American Mathematical Monthly</source>. <year>1965</year>;<volume>72</volume>(<issue>1</issue>):<fpage>4</fpage>. doi:<pub-id pub-id-type="doi">10.2307/2312989</pub-id>.</mixed-citation></ref>
<ref id="c53"><label>53.</label><mixed-citation publication-type="journal"><string-name><surname>Teeuwisse</surname> <given-names>WM</given-names></string-name>, <string-name><surname>Brink</surname> <given-names>WM</given-names></string-name>, <string-name><surname>Webb</surname> <given-names>AG.</given-names></string-name> <article-title>Quantitative assessment of the effects of high-permittivity pads in 7 tesla MRI of the brain</article-title>. <source>Magnetic resonance in medicine</source>. <year>2012</year>;<volume>67</volume>(<issue>5</issue>):<fpage>1285</fpage>&#x2013;<lpage>1293</lpage>.</mixed-citation></ref>
<ref id="c54"><label>54.</label><mixed-citation publication-type="journal"><string-name><surname>Griswold</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Jakob</surname> <given-names>PM</given-names></string-name>, <string-name><surname>Heidemann</surname> <given-names>RM</given-names></string-name>, <string-name><surname>Nittka</surname> <given-names>M</given-names></string-name>, <string-name><surname>Jellus</surname> <given-names>V</given-names></string-name>, <string-name><surname>Wang</surname> <given-names>J</given-names></string-name>, <etal>et al.</etal> <article-title>Generalized Autocalibrating Partially Parallel Acquisitions (GRAPPA)</article-title>. <source>Magnetic Resonance in Medicine</source>. <year>2002</year>;<volume>47</volume>(<issue>6</issue>):<fpage>1202</fpage>&#x2013;<lpage>1210</lpage>. doi:<pub-id pub-id-type="doi">10.1002/mrm.10171</pub-id>.</mixed-citation></ref>
<ref id="c55"><label>55.</label><mixed-citation publication-type="journal"><string-name><surname>Haast</surname> <given-names>RAM</given-names></string-name>, <string-name><surname>Ivanov</surname> <given-names>D</given-names></string-name>, <string-name><surname>Formisano</surname> <given-names>E</given-names></string-name>, <string-name><surname>Ulud&#x011F;</surname> <given-names>K</given-names></string-name>. <article-title>Reproducibility and Reliability of Quantitative and Weighted T1 and T2(&#x002A;) Mapping for Myelin-Based Cortical Parcellation at 7 Tesla</article-title>. <source>Frontiers in neuroanatomy</source>. <year>2016</year>;<volume>10</volume>:<fpage>112</fpage>. doi:<pub-id pub-id-type="doi">10.3389/fnana.2016.00112</pub-id>.</mixed-citation></ref>
<ref id="c56"><label>56.</label><mixed-citation publication-type="journal"><string-name><surname>Yushkevich</surname> <given-names>PA</given-names></string-name>, <string-name><surname>Piven</surname> <given-names>J</given-names></string-name>, <string-name><surname>Cody Hazlett</surname> <given-names>H</given-names></string-name>, <string-name><surname>Gimpel Smith</surname> <given-names>R</given-names></string-name>, <string-name><surname>Ho</surname> <given-names>S</given-names></string-name>, <string-name><surname>Gee</surname> <given-names>JC</given-names></string-name>, <etal>et al.</etal> <article-title>User-Guided 3D Active Contour Segmentation of Anatomical Structures: Significantly Improved Efficiency and Reliability</article-title>. <source>Neuroimage</source>. <year>2006</year>;<volume>31</volume>(<issue>3</issue>):<fpage>1116</fpage>&#x2013;<lpage>1128</lpage>.</mixed-citation></ref>
<ref id="c57"><label>57.</label><mixed-citation publication-type="journal"><string-name><surname>Van Der Walt</surname> <given-names>S</given-names></string-name>, <string-name><surname>Colbert</surname> <given-names>SC</given-names></string-name>, <string-name><surname>Varoquaux</surname> <given-names>G.</given-names></string-name> <article-title>The NumPy array: a structure for efficient numerical computation</article-title>. <source>Computing in Science &#x0026; Engineering</source>. <year>2011</year>;<volume>13</volume>(<issue>2</issue>):<fpage>22</fpage>&#x2013;<lpage>30</lpage>.</mixed-citation></ref>
<ref id="c58"><label>58.</label><mixed-citation publication-type="other"><string-name><surname>Jones</surname> <given-names>E</given-names></string-name>, <string-name><surname>Oliphant</surname> <given-names>T</given-names></string-name>, <string-name><surname>Peterson</surname> <given-names>P</given-names></string-name>, <etal>et al.</etal> <article-title>SciPy: Open source scientific tools for Python</article-title>; <year>2001</year>&#x2013;. <source>Available from</source>: <ext-link ext-link-type="uri" xlink:href="http://www.scipy.org/">http://www.scipy.org/</ext-link>.</mixed-citation></ref>
<ref id="c59"><label>59.</label><mixed-citation publication-type="journal"><string-name><surname>Hunter</surname> <given-names>JD.</given-names></string-name> <article-title>Matplotlib: A 2D graphics environment</article-title>. <source>Computing In Science &#x0026; Engineering</source>. <year>2007</year>;<volume>9</volume>(<issue>3</issue>):<fpage>90</fpage>&#x2013;<lpage>95</lpage>. doi:<pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id>.</mixed-citation></ref>
<ref id="c60"><label>60.</label><mixed-citation publication-type="other"><string-name><surname>Brett</surname> <given-names>M</given-names></string-name>, <string-name><surname>Hanke</surname> <given-names>M</given-names></string-name>, <string-name><surname>C&#x00F4;t&#x00E9;</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Markiewicz</surname> <given-names>C</given-names></string-name>, <string-name><surname>Ghosh</surname> <given-names>S</given-names></string-name>, <string-name><surname>Wassermann</surname> <given-names>D</given-names></string-name>, <etal>et al.</etal> <article-title>nipy/nibabel: 2.2.0</article-title>; <year>2017</year>. Available from: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1011207">https://doi.org/10.5281/zenodo.1011207</ext-link>.</mixed-citation></ref>
<ref id="c61"><label>61.</label><mixed-citation publication-type="journal"><string-name><surname>van der Walt</surname> <given-names>S</given-names></string-name>, <string-name><surname>Sch&#x00F6;nberger</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Nunez-Iglesias</surname> <given-names>J</given-names></string-name>, <string-name><surname>Boulogne</surname> <given-names>F</given-names></string-name>, <string-name><surname>Warner</surname> <given-names>JD</given-names></string-name>, <string-name><surname>Yager</surname> <given-names>N</given-names></string-name>, <etal>et al.</etal> <article-title>scikit-image: image processing in Python</article-title>. <source>PeerJ</source>. <year>2014</year>;<volume>2</volume>:<fpage>e453</fpage>. doi:<pub-id pub-id-type="doi">10.7717/peerj.453</pub-id>.</mixed-citation></ref>
<ref id="c62"><label>62.</label><mixed-citation publication-type="other"><string-name><surname>Scharr</surname> <given-names>H.</given-names></string-name> <chapter-title>Optimale operatoren in der digitalen bildverarbeitung</chapter-title>; <year>2000</year>.</mixed-citation></ref>
<ref id="c63"><label>63.</label><mixed-citation publication-type="journal"><string-name><surname>J&#x00E4;hne</surname> <given-names>B</given-names></string-name>, <string-name><surname>Hau&#x00DF;ecker</surname> <given-names>H.</given-names></string-name> <article-title>Handbook of computer vision and applications</article-title>. <volume>vol. 2</volume>. <source>Elsevier</source>; <year>2000</year>.</mixed-citation></ref>
<ref id="c64"><label>64.</label><mixed-citation publication-type="other"><string-name><surname>Gulban</surname> <given-names>OF.</given-names></string-name> <article-title>ofgulban/tetrahydra: Tetrahydra v0.2.1</article-title>; <year>2017</year>. Available from: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.1007952">https://doi.org/10.5281/zenodo.1007952</ext-link>.</mixed-citation></ref>
<ref id="c65"><label>65.</label><mixed-citation publication-type="journal"><string-name><surname>Zhang</surname> <given-names>Y</given-names></string-name>, <string-name><surname>Brady</surname> <given-names>M</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>S.</given-names></string-name> <article-title>Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm</article-title>. <source>IEEE Transactions on Medical Imaging</source>. <year>2001</year>;<volume>20</volume>(<issue>1</issue>):<fpage>45</fpage>&#x2013;<lpage>57</lpage>. doi:<pub-id pub-id-type="doi">10.1109/42.906424</pub-id>.</mixed-citation></ref>
<ref id="c66"><label>66.</label><mixed-citation publication-type="journal"><string-name><surname>Ashburner</surname> <given-names>J</given-names></string-name>, <string-name><surname>Friston</surname> <given-names>KJ</given-names></string-name>. <article-title>Unified segmentation</article-title>. <source>NeuroImage</source>. <year>2005</year>;<volume>26</volume>(<issue>3</issue>):<fpage>839</fpage>&#x2013;<lpage>851</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2005.02.018</pub-id>.</mixed-citation></ref>
<ref id="c67"><label>67.</label><mixed-citation publication-type="journal"><string-name><surname>Smith</surname> <given-names>SM.</given-names></string-name> <article-title>Fast robust automated brain extraction</article-title>. <source>Human Brain Mapping</source>. <year>2002</year>;<volume>17</volume>(<issue>3</issue>):<fpage>143</fpage>&#x2013;<lpage>155</lpage>. doi:<pub-id pub-id-type="doi">10.1002/hbm.10062</pub-id>.</mixed-citation></ref>
<ref id="c68"><label>68.</label><mixed-citation publication-type="journal"><string-name><surname>Taha</surname> <given-names>AA</given-names></string-name>, <string-name><surname>Hanbury</surname> <given-names>A.</given-names></string-name> <article-title>Metrics for evaluating 3D medical image segmentation: analysis, selection, and tool</article-title>. <source>BMC Medical Imaging</source>. <year>2015</year>;<volume>15</volume>(<issue>1</issue>):<fpage>29</fpage>. doi:<pub-id pub-id-type="doi">10.1186/s12880-015-0068-x</pub-id>.</mixed-citation></ref>
<ref id="c69"><label>69.</label><mixed-citation publication-type="journal"><string-name><surname>Sereno</surname> <given-names>MI</given-names></string-name>, <string-name><surname>Lutti</surname> <given-names>A</given-names></string-name>, <string-name><surname>Weiskopf</surname> <given-names>N</given-names></string-name>, <string-name><surname>Dick</surname> <given-names>F.</given-names></string-name> <article-title>Mapping the human cortical surface by combining quantitative T1 with retinotopy</article-title>. <source>Cerebral Cortex</source>. <year>2013</year>;<volume>23</volume>(<issue>9</issue>):<fpage>2261</fpage>&#x2013;<lpage>2268</lpage>. doi:<pub-id pub-id-type="doi">10.1093/cercor/bhs213</pub-id>.</mixed-citation></ref>
<ref id="c70"><label>70.</label><mixed-citation publication-type="journal"><string-name><surname>Dick</surname> <given-names>F</given-names></string-name>, <string-name><surname>Taylor Tierney</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lutti</surname> <given-names>A</given-names></string-name>, <string-name><surname>Josephs</surname> <given-names>O</given-names></string-name>, <string-name><surname>Sereno</surname> <given-names>MI</given-names></string-name>, <string-name><surname>Weiskopf</surname> <given-names>N.</given-names></string-name> <article-title>In Vivo Functional and Myeloarchitectonic Mapping of Human Primary Auditory Areas</article-title>. <source>Journal of Neuroscience</source>. <year>2012</year>;<volume>32</volume>(<issue>46</issue>):<fpage>16095</fpage>&#x2013;<lpage>16105</lpage>. doi:<pub-id pub-id-type="doi">10.1523/JNEUROSCI.1712-12.2012</pub-id>.</mixed-citation></ref>
<ref id="c71"><label>71.</label><mixed-citation publication-type="journal"><string-name><surname>Marques</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Gruetter</surname> <given-names>R.</given-names></string-name> <article-title>New Developments and Applications of the MP2RAGE Sequence &#x2013; Focusing the Contrast and High Spatial Resolution R1 Mapping</article-title>. <source>PLoS ONE</source>. <year>2013</year>;<volume>8</volume>(<issue>7</issue>). doi:<pub-id pub-id-type="doi">10.1371/journal.pone.0069294</pub-id>.</mixed-citation></ref>
<ref id="c72"><label>72.</label><mixed-citation publication-type="other"><string-name><surname>Gulban</surname> <given-names>OF.</given-names></string-name> <article-title>The relation between color spaces and compositional data analysis demonstrated with magnetic resonance image processing applications</article-title>; <year>2017</year>. Available from: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1705.03457">https://arxiv.org/abs/1705.03457</ext-link>.</mixed-citation></ref>
<ref id="c73"><label>73.</label><mixed-citation publication-type="journal"><string-name><surname>Collins</surname> <given-names>DL</given-names></string-name>, <string-name><surname>Zijdenbos</surname> <given-names>aP</given-names></string-name>, <string-name><surname>Kollokian</surname> <given-names>V</given-names></string-name>, <string-name><surname>Sled</surname> <given-names>JG</given-names></string-name>, <string-name><surname>Kabani</surname> <given-names>NJ</given-names></string-name>, <string-name><surname>Holmes</surname> <given-names>CJ</given-names></string-name>, <etal>et al.</etal> <article-title>Design and construction of a realistic digital brain phantom</article-title>. <source>IEEE Transactions on Medical Imaging</source>. <year>1998</year>;<volume>17</volume>(<issue>3</issue>):<fpage>463</fpage>&#x2013;<lpage>468</lpage>. doi:<pub-id pub-id-type="doi">10.1109/42.712135</pub-id>.</mixed-citation></ref>
<ref id="c74"><label>74.</label><mixed-citation publication-type="journal"><string-name><surname>Aubert-Broche</surname> <given-names>B</given-names></string-name>, <string-name><surname>Evans</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Collins</surname> <given-names>L.</given-names></string-name> <article-title>A new improved version of the realistic digital brain phantom</article-title>. <source>NeuroImage</source>. <year>2006</year>;<volume>32</volume>(<issue>1</issue>):<fpage>138</fpage>&#x2013;<lpage>145</lpage>. doi:<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuroimage.2006.03.052">https://doi.org/10.1016/j.neuroimage.2006.03.052</ext-link>.</mixed-citation></ref>
<ref id="c75"><label>75.</label><mixed-citation publication-type="journal"><string-name><surname>Aubert-Broche</surname> <given-names>B</given-names></string-name>, <string-name><surname>Griffin</surname> <given-names>M</given-names></string-name>, <string-name><surname>Pike</surname> <given-names>GB</given-names></string-name>, <string-name><surname>Evans</surname> <given-names>AC</given-names></string-name>, <string-name><surname>Collins</surname> <given-names>DL.</given-names></string-name> <article-title>Twenty New Digital Brain Phantoms for Creation of Validation Image Data Bases</article-title>. <source>IEEE Trans Med Imaging</source>. <year>2006</year>;<volume>25</volume>(<issue>11</issue>):<fpage>1410</fpage>&#x2013;<lpage>1416</lpage>.</mixed-citation></ref>
<ref id="c76"><label>76.</label><mixed-citation publication-type="journal"><string-name><surname>Valverde</surname> <given-names>S</given-names></string-name>, <string-name><surname>Oliver</surname> <given-names>A</given-names></string-name>, <string-name><surname>Cabezas</surname> <given-names>M</given-names></string-name>, <string-name><surname>Roura</surname> <given-names>E</given-names></string-name>, <string-name><surname>Llad&#x00F3;</surname> <given-names>X</given-names></string-name>. <article-title>Comparison of 10 brain tissue segmentation methods using revisited IBSR annotations</article-title>. <source>Journal of Magnetic Resonance Imaging</source>. <year>2015</year>;<volume>41</volume>(<issue>1</issue>):<fpage>93</fpage>&#x2013;<lpage>101</lpage>. doi:<pub-id pub-id-type="doi">10.1002/jmri.24517</pub-id>.</mixed-citation></ref>
</ref-list>
<sec id="s7" sec-type="supplementary-material">
<title>Supporting information</title>
<sec>
<title>S1 Appendix. Time benefit estimation</title>
<p>To estimate the time saved by substituting manual correction with Segmentator, we did the following calculation: Assuming that the objective of manually correcting mislabeled GM voxels is to reduce the number of false positives and to increase the number of true positives, we computed the number of false and true positives both before and after Segmentator intervention. We computed the difference in true positives and false positives before and after the application of segmentator and subtracted the true positive difference from the false positive difference. The resulting number is assumed to indicate the number of voxels which would have to be subtracted in the process of manually correcting all the voxels without using Segmentator. For the MRI data presented here, this number amounted to around 200000 voxels to be corrected (out of a total number of 1100000 voxels in the cortical ribbon). On average 35000 out of those 200000 voxels could be corrected using Segmentator. Assuming that a trained operator can manually correct one voxel per second, on average, this amounts to 7.5 hours of manual work that can be substituted with 10 minutes of Segmentator usage for the whole brain GM ribbon segmentation at 0.7 mm isotropic resolution. The time that can potentially be saved by using Segmentator will scale with the total number of GM voxels - it will be higher for high resolution acquisition (more voxels) and lower at low resolution (less voxels). The script used for our estimation is available at [<xref ref-type="bibr" rid="c43">43</xref>].</p>
<fig id="figS1" position="float" orientation="portrait" fig-type="figure">
<label>S1 Fig.</label>
<caption><title>2D histogram representation of three 3D MRI contrast images.</title>
<p>(A) Each voxel is considered as a 3 part composition. The barycentric coordinates of each composition which reside in 3D simplex space are represented in 2D real space after using a isometric log-ratio (ilr) transformation. (B) The ilr coordinates are used to create 2D histograms representing all voxels in the images. The blue lines are the embedded 3D real space primary axes (note that the input image units were initially normalized to have similar dynamic ranges to account for the large scale difference between T2&#x002A; and MP2RAGE images). In this case, the ilr coordinates are not easily interpretable by themselves but they are useful to visualize the barycentric coordinates which are interpretable via the embedded real space axes. Darker regions in the histogram indicate that many voxels are characterized by this particular scale invariant combination of the image contrasts. In this representation, brain tissue (WM and GM, red dashed lines) becomes separable from non-brain tissue (black dashed lines and arrows). If desired, subcortical structures like the red nucleus, the globus pallidus and the subthalamic nucleus can additionally be identified (white circle).</p></caption>
<graphic xlink:href="245738_figS1.tif"/>
</fig>
<fig id="figS2" position="float" orientation="portrait" fig-type="figure">
<label>S2 Fig.</label>
<caption><title>Flowchart diagram MPRAGE GraMag pipeline.</title>
<p>This diagram provides a detailed overview of all the inputs, processing steps and outputs for MPRAGE GraMag pipeline. Rectangular shapes represent processing steps, rhombic shapes represent input or outputs and cylindrical shapes represent input or output locations.</p></caption>
<graphic xlink:href="245738_figS2.tif"/>
</fig>
<fig id="figS3" position="float" orientation="portrait" fig-type="figure">
<label>S3 Fig.</label>
<caption><title>Flowchart diagram MPRAGE CoDa pipeline.</title>
<p>This diagram provides a detailed overview of all the inputs, processing steps and outputs for MPRAGE CoDa pipeline. Rectangular shapes represent processing steps, rhombic shapes represent input or outputs and cylindrical shapes represent input or output locations.</p></caption>
<graphic xlink:href="245738_figS3.tif"/>
</fig>
<fig id="figS4" position="float" orientation="portrait" fig-type="figure">
<label>S4 Fig.</label>
<caption><title>Flowchart diagram MP2RAGE GraMag pipeline.</title>
<p>This diagram provides a detailed overview of all the inputs, processing steps and outputs for MP2RAGE GraMag pipeline. Rectangular shapes represent processing steps, rhombic shapes represent input or outputs and cylindrical shapes represent input or output locations.</p></caption>
<graphic xlink:href="245738_figS4.tif"/>
</fig>
<fig id="figS5" position="float" orientation="portrait" fig-type="figure">
<label>S5 Fig.</label>
<caption><title>Flowchart diagram MP2RAGE CoDa pipeline.</title>
<p>This diagram provides a detailed overview of all the inputs, processing steps and outputs for MP2RAGE CoDa pipeline. Rectangular shapes represent processing steps, rhombic shapes represent input or outputs and cylindrical shapes represent input or output locations.</p></caption>
<graphic xlink:href="245738_figS5.tif"/>
</fig>
<fig id="figS6" position="float" orientation="portrait" fig-type="figure">
<label>S6 Fig.</label>
<caption><title>2D Histogram Representation for MRI Image of a Human Brain.</title>
<p>The intensity (A) and gradient magnitude (B) values of a T1w-divided-by-PDw MRI image (MP2RAGE, 0.7 mm isotropic resolution) are represented in a 2D histogram (C). Darker regions in the histogram indicate that many voxels are characterized by this particular combination of image intensity and gradient magnitude. The 2D histogram displays a characteristic pattern with tissue types occupying particular areas of the histogram (D). Voxels containing CSF, dura mater or blood vessels (black dashed lines and arrows) cover different regions of the histogram than voxels containing WM and GM (red dashed lines). As a result, brain tissue becomes separable from non-brain tissue.</p></caption>
<graphic xlink:href="245738_figS6.tif"/>
</fig>
<table-wrap id="tblS1" orientation="portrait" position="float">
<label>S1 Table.</label>
<caption><title>Segmentation performance MPRAGE data without artifact masking.</title>
<p>The table shows the DICE (larger is better) and AVHD (less is better) for the initial SPM 12 and FSL FAST GM segmentations as well as after additional polishing, using either the gradient magnitude or the compositional data method.</p></caption>
<graphic xlink:href="245738_tblS1.tif"/>
</table-wrap>
<table-wrap id="tblS2" orientation="portrait" position="float">
<label>S2 Table.</label>
<caption><title>Segmentation performance MP2RAGE data without artifact masking.</title>
<p>The table shows the DICE (larger is better) and AVHD (less is better) for the initial CBS tools and FSL FAST GM segmentations as well as after additional masking, using either the gradient magnitude or the compositional data method.</p></caption>
<graphic xlink:href="245738_tblS2.tif"/>
</table-wrap>
</sec>
</sec>
</back>
</article>