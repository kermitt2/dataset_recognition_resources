<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/335943</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Bioinformatics</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Continuous embeddings of DNA sequencing reads, and application to metagenomics</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Menegaux</surname>
<given-names>Romain</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Vert</surname>
<given-names>Jean-Philippe</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="aff" rid="a2">2</xref>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<aff id="a1"><label>1</label><institution>MINES ParisTech, PSL Research University, CBIO - Centre for Computational Biology</institution>, F-75006 Paris, <country>France</country></aff>
<aff id="a2"><label>2</label><institution>Institut Curie, PSL Research University, INSERM</institution>, U900, F-75005 Paris, <country>France</country></aff>
<aff id="a3"><label>3</label><institution>Ecole Normale Sup&#x00E9;rieure, Department of Mathematics and Applications, CNRS, PSL Research University</institution>, F-75005 Paris, <country>France</country></aff>
</contrib-group>
<author-notes>
<fn fn-type="other"><p><email>firstname.lastname@mines-paristech.fr</email></p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2018</year>
</pub-date>
<elocation-id>335943</elocation-id>
<history>
<date date-type="received">
<day>31</day>
<month>5</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>31</day>
<month>5</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>31</day>
<month>5</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="335943.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>We propose a new model for fast classification of DNA sequences output by next generation sequencing machines. The model, which we call <monospace>fastDNA</monospace>, embeds DNA sequences in a vector space by learning continuous low-dimensional representations of the <italic>k</italic>-mers it contains. We show on metagenomics benchmarks that it outperforms state-of-the-art methods in terms of accuracy and scalability.</p>
</abstract>
<counts>
<page-count count="11"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction</title>
<p>The cost of DNA sequencing has been divided by 100,000 in the last 10 years. With less than &#x0024;1,000 to sequence a human-size genome, it is now so cheap that it has quickly become a routine technique to characterize the genome of biological samples with numerous applications in health, food or energy. Besides the genome, many techniques have been developed to measure other molecular informations using DNA sequencing, e.g., gene expression using RNA-seq, protein-DNA interactions using ChiP-seq, or 3D structural informations using Hi-C, to name just a few. In short, DNA sequencing is the swiss army knife of modern genomics and epigenomics. As a consequence, the rate of production of DNA sequences has exploded in recent years, and the storage, processing and analysis of these sequences is increasingly a bottleneck.</p>
<p>While new, so-called <italic>long-read</italic> technologies are under active development and may become dominant in the future, the current market of DNA sequencing technologies is dominated by so-called <italic>next-generation sequencing</italic> (NGS) technologies which break long strands of DNA into short fragments of typically 50 to 400 bases each, and &#x201C;read&#x201D; the sequence of bases that compose each fragment. The output of a typical DNA sequencing experiment is therefore a set of millions or billions of short sequences, called <italic>reads</italic>, of lengths 50&#x223C;400 in the {<italic>A, C, G, T</italic>} alphabet; these billions of reads are then automatically processed and analyzed by computers to get some biological information such as the presence of particular bacterial species in a sample, or of a specific mutation in a cancer.</p>
<p>Standard pipelines to process the raw reads depend on the target applications, but typically involve discrete operations such as aligning them to some reference genome using string algorithms. In this paper, we investigate the feasibility of directly representing DNA reads as continuous vectors instead, and replacing some discrete operations by continuous calculus in this embedding.</p>
<p>To illustrate this idea, we focus on an important application in metagenomics, where one sequences the DNA present in an environmental sample to characterize the microbes it contains [<xref rid="c21" ref-type="bibr">21</xref>, <xref rid="c4" ref-type="bibr">4</xref>]. An important problem in metagenomics is <italic>taxonomic binning</italic>, where each of the billions of sequenced reads must be assigned to a species, given a database of genomes characteristic of each species considered [<xref rid="c16" ref-type="bibr">16</xref>]. Standard computational approaches for taxonomic binning try to align each read to a reference sequence database with sequence alignment tools like BLAST [<xref rid="c5" ref-type="bibr">5</xref>] or short read mapping tools such as BWA [<xref rid="c14" ref-type="bibr">14</xref>] or BOWTIE [<xref rid="c10" ref-type="bibr">10</xref>]. However, the computational cost of these techniques becomes prohibitive with current large sequence datasets. Alternatively, compositional approaches rephrase the problem as a multiclass classification problem, and employ machine learning methods such as a naive Bayes (NB) classifier [<xref rid="c23" ref-type="bibr">23</xref>, <xref rid="c19" ref-type="bibr">19</xref>] or a support vector machine [SVM, 17, 20, 22] after representing each read by the vector of <italic>k</italic>-mer<sup><xref rid="fn1" ref-type="fn">1</xref></sup> counts it contains. Interestingly, [<xref rid="c22" ref-type="bibr">22</xref>] showed that compositional approaches can be competitive in accuracy with alignment-based methods, while maintaining a computational advantage, by using large-scale machine learning approaches. However, a good accuracy is only achieved with <italic>k</italic>-mers of length at least <italic>k</italic> = 12, corresponding to representing each read as a sparse vector in <italic>N</italic> = 4<sup><italic>k</italic></sup> dimensions. This representation raises computational challenges both at training time ([<xref rid="c22" ref-type="bibr">22</xref>] push VowPal Wabbit to its limit) and at test time (a <italic>N</italic> &#x00D7; <italic>T</italic> matrix of weights must be stored to model <italic>T</italic> species).</p>
<p>In this work, we propose to extend state-of-the-art compositional approaches by embedding the set of DNA reads to &#x211D;<sup><italic>d</italic></sup>, with <italic>d</italic> &#x226A; <italic>N</italic>. For that purpose, we still extract the <italic>k</italic>-mer composition of each read, but replace the <italic>N</italic>-dimensional one-hot encoding of each <italic>k</italic>-mer by a <italic>d</italic>-dimensional encoding, optimized to solve the task. This approach is similar to, e.g., the <monospace>fastText</monospace> model for natural language sequences of [<xref rid="c7" ref-type="bibr">7</xref>, <xref rid="c3" ref-type="bibr">3</xref>] or <monospace>word2vec</monospace> [<xref rid="c18" ref-type="bibr">18</xref>], with a different notion of <italic>words</italic> to embed, and a direct optimization of the classification error to learn the representation. This can reduces the memory requirements to store the model and accelerate classification time when <italic>d &#x003C; T</italic>, since the <italic>N</italic> &#x00D7; <italic>T</italic> matrix of weights is replaced by a <italic>N</italic> &#x00D7; <italic>d</italic> matrix of embedding, and a <italic>d</italic> &#x00D7; <italic>T</italic> matrix of weights.</p>
<p>After presenting in more detail the models and its optimization, we experimentally study the speed/performance trade-off on metagenomics experiments by varying the embedding dimension, and demonstrate the potential of the approach which outperforms state-of-the-art compositional approaches.</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Method</title>
<sec id="s2a">
<label>2.1</label>
<title>Embedding of DNA reads</title>
<p>Given the alphabet of nucleotides <italic>&#x1D49C;</italic> = {<italic>A, C, G, T</italic>}, a <italic>DNA read</italic> of length <italic>L &#x2208;</italic>&#x2115;<sup><italic>&#x002A;</italic></sup> is a sequence <bold>x</bold> = <italic>x</italic><sub>1</sub> <italic>&#x2026; x</italic><sub><italic>L</italic></sub> <italic>&#x2208;&#x1D49C;</italic><sup><italic>L</italic></sup>. Depending on the sequencing technology, <italic>L</italic> is typically in the range 50 &#x223C; 400, and we fix <italic>L</italic> = 200 in the experiments below. For any 1 <italic>&#x2264; a &#x2264; b &#x2264; L</italic> we denote by <bold>x</bold><sub>[<italic>a,b</italic>]</sub> = <italic>x</italic><sub><italic>a</italic></sub><italic>x</italic><sub><italic>a</italic>&#x002B;1</sub> <italic>&#x2026; x</italic><sub><italic>b</italic></sub> the substring of <bold>x</bold> from position <italic>a</italic> to <italic>b</italic>. For any <italic>d &#x2208;</italic>&#x2115;<sup><italic>&#x002A;</italic></sup>, an <italic>embedding</italic> of DNA reads to &#x211D;<sup>d</sup> is a mapping &#x03A6;: <italic>&#x1D49C;</italic><sup><italic>L</italic></sup> &#x2192; &#x211D;<sup>d</sup> to represent each read <bold>x</bold> <italic>&#x2208;&#x1D49C;</italic><sup><italic>L</italic></sup> by a vector &#x03A6;(<bold>x</bold>) <italic>&#x2208;</italic> &#x211D;<sup>d</sup>, which can then be used for subsequent classification tasks.</p>
<p>For a given <italic>k &#x2208;</italic>&#x2115;, <italic>k</italic>-spectral embedding represents a sequence by its <italic>k</italic>-mer profile [<xref rid="c11" ref-type="bibr">11</xref>]: it is an <inline-formula><alternatives><inline-graphic xlink:href="335943_inline1.gif"/></alternatives></inline-formula> in <italic>d</italic> = &#x007C;&#x1D49C;&#x007C;<sup><italic>k</italic></sup> dimensions indexed by all strings of length <italic>k;</italic> where for any such string <italic>u &#x2208;&#x1D49C;</italic><sup><italic>k</italic></sup> one defines:
<disp-formula id="ueqn1">
<alternatives><graphic xlink:href="335943_ueqn1.gif"/></alternatives>
</disp-formula>
where <italic>&#x03B4;</italic><sub><italic>u</italic></sub>(<italic>v</italic>) = 1 if <italic>u</italic> = <italic>v</italic>, 0 otherwise. The <italic>k</italic>-spectral encoding of DNA reads is used in state-of-the-art compositional approaches to assign reads to species with machine learning techniques [<xref rid="c23" ref-type="bibr">23</xref>, <xref rid="c19" ref-type="bibr">19</xref>, <xref rid="c17" ref-type="bibr">17</xref>, <xref rid="c20" ref-type="bibr">20</xref>, <xref rid="c22" ref-type="bibr">22</xref>].</p>
<p>Given <italic>d &#x2208;</italic> &#x2115;<sup><italic>&#x002A;</italic></sup> and a <italic>d</italic> &#x00D7; <italic>N</italic> matrix <italic>M</italic> = (<italic>M</italic><sub><italic>u</italic></sub>)<sub><italic>u&#x2208;&#x1D49C;</italic></sub><italic>k</italic> associating a vector <italic>M</italic><sub><italic>u</italic></sub> <italic>&#x2208;</italic> &#x211D;<sup>d</sup> to each <italic>k</italic>-mer <italic>u &#x2208; &#x1D49C;</italic><sup><italic>k</italic></sup>, we now consider a <italic>d</italic> dimensional embedding &#x03A6;<sup><italic>M</italic></sup> of DNA reads by summing the vectors associated to the read&#x2019;s <italic>k</italic>-mers:
<disp-formula id="ueqn2">
<alternatives><graphic xlink:href="335943_ueqn2.gif"/></alternatives>
</disp-formula></p>
<p>In matrix form, one easily sees that the <italic>d</italic>-dimensional embedding &#x03A6;<sup><italic>M</italic></sup> can be obtained from the <italic>k</italic>-spectral representation by the formula:
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="335943_eqn1.gif"/></alternatives>
</disp-formula>
showing in particular that the <italic>k</italic>-spectral embedding is a particular case of &#x03A6;<sup><italic>M</italic></sup> by taking <italic>d</italic> = <italic>N</italic> and <italic>M</italic> = <italic>Id</italic>. Changing <italic>M</italic> allows to create correlations between <italic>k</italic>-mers in the embedding space. For example, the (<italic>k, m</italic>)-mismatch kernel [<xref rid="c12" ref-type="bibr">12</xref>] also corresponds to an embedding &#x03A6;<sup><italic>M</italic></sup> with <italic>d</italic> = <italic>N</italic>, but where <italic>M</italic><sub><italic>u,v</italic></sub> = 1 when the Hamming distance between <italic>u</italic> and <italic>v</italic> is at most <italic>m</italic>, 0 otherwise. Changing <italic>d</italic> further allows to vary the dimension of the embedding, which can not only be beneficial for memory and computational reasons, but also help statistical inference by reducing the number of parameters of the embedding.</p>
</sec>
<sec id="s2b">
<label>2.2</label>
<title>Learning the embedding</title>
<p>While several existing embeddings such as the <italic>k</italic>-spectral or (<italic>k, m</italic>)-mismatch embeddings correspond to &#x03A6;<sup><italic>M</italic></sup> for specific matrices <italic>M</italic>, we propose to &#x201C;learn&#x201D; <italic>M</italic> as part of the overall classification or regression task that must be solved. In our metagenomics problem, this is a multiclass classification problem where each of the <italic>T</italic> bacterial species is a class and each read must be assigned to a class. Given an embedding &#x03A6;<sub><italic>M</italic></sub>, we consider a linear model of the form:
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="335943_eqn2.gif"/></alternatives>
</disp-formula>
where <italic>W &#x2208;</italic>&#x211D;<sup><italic>T</italic> &#x00D7;<italic>d</italic></sup> is a matrix of weights, and the prediction rule:
<disp-formula id="ueqn3">
<alternatives><graphic xlink:href="335943_ueqn3.gif"/></alternatives>
</disp-formula></p>
<p>To learn the embedding <italic>M</italic> and the linear model <italic>M</italic>, we assume given a training set of examples (<bold>x</bold><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>)<italic>i</italic>=1, <italic>&#x2026;,n</italic><sub><italic>train</italic></sub> where <italic>x</italic><sub><italic>i</italic></sub> <italic>&#x2208;&#x1D49C;</italic><sup><italic>L</italic></sup> and <italic>y</italic><sub><italic>i</italic></sub> <italic>&#x2208;</italic> {1, <italic>&#x2026;, T</italic>}, and numerically minimize an empirical risk:</p>
<p>
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="335943_eqn3.gif"/></alternatives>
</disp-formula>
where for the loss <italic>&#x1D4C1;</italic> we choose the standard cross-entropy loss after transforming the scores to probability with the softmax function:
<disp-formula id="ueqn4">
<alternatives><graphic xlink:href="335943_ueqn4.gif"/></alternatives>
</disp-formula></p>
<p>We solve (3) by stochastic gradient descent (SGD). Note that when <italic>d &#x003C; T</italic>, the problem is usually non-convex and SGD may only converge to a local optimum.</p>
<p>Combining (1) and (2), we further notice that for any embedding <italic>M</italic> and weights <italic>W</italic>,</p>
<p>
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="335943_eqn4.gif"/></alternatives>
</disp-formula>
</p>
<p>This clarifies that <italic>f</italic> <sup><italic>M,W</italic></sup> boils down to a linear model in the <italic>k</italic>-spectral representation, with a weight matrix <italic>WM</italic> <sup>T</sup> of rank at most <italic>d</italic>. When <italic>d &#x003C;</italic> min(<italic>N, T</italic>) this creates a low-rank regularization that can be beneficial for statistical inference, in addition to reducing the memory footprint of the model and speeding up the prediction time.</p>
</sec>
<sec id="s2c">
<label>2.3</label>
<title>Implementation</title>
<p>We implemented the model (3) by modifying the <monospace>fastText</monospace> open-source library [<xref rid="c7" ref-type="bibr">7</xref>, <xref rid="c8" ref-type="bibr">8</xref>], which involves a similar model with <italic>k</italic>-mer embedding for natural language. There are some important differences between DNA reads and standard NLP applications, though: (i) One is that the concept of a &#x201C;word&#x201D;, space-delimited groups of letters, does not exist in DNA sequence data. Hence we resort to a distributed representation of overlapping <italic>k</italic>-mers only, and not to words as in <monospace>word2vec</monospace> or <monospace>fastText</monospace>. (ii) Second is the number of training examples to be seen by the model is very large: up to 5 &#x00D7; 10<sup>9</sup> if we were to achieve full coverage of the <italic>large</italic> database below, for example. (iii) Third, the vocabulary is different, as it is of known size (4<sup><italic>k</italic></sup>) and is densely represented for relatively small values of <italic>k</italic>. For greater values of <italic>k</italic> than those considered in this paper, <italic>k</italic>-mers become rare and individual long <italic>k</italic>-mers can become discriminative. This is in fact used by some other compositional algorithms such as Kraken [<xref rid="c24" ref-type="bibr">24</xref>].</p>
<p>For these reasons we rewrote part of the <monospace>fastText</monospace> software to extract overlapping <italic>k</italic>-mers rather than words. As appropriate for (iii), the <italic>k</italic>-mer embeddings are stored in a fixed-size table of dimension (4<sup><italic>k</italic></sup>, <italic>d</italic>), each row corresponding to the vector of a different <italic>k</italic>-mer. For a given <italic>k</italic>-mer <bold>b</bold> = <italic>b</italic><sub>1</sub> <italic>&#x2026; b</italic><sub><italic>k</italic></sub> <italic>&#x2208;&#x1D49C;</italic><sup><italic>k</italic></sup>, the index of its corresponding row in <italic>M</italic> is <inline-formula><alternatives><inline-graphic xlink:href="335943_inline3.gif"/></alternatives></inline-formula>, where <italic>h</italic>(<italic>A</italic>) =0, <italic>h</italic>(<italic>C</italic>) = 1, <italic>h</italic>(<italic>G</italic>) = 2, and <italic>h</italic>(<italic>T</italic>) = 3. This allows to quickly find <italic>ind</italic>(<bold>x</bold><sub><italic>i</italic>&#x002B;1:<italic>i</italic>&#x002B;<italic>k</italic></sub>) from <italic>ind</italic>(<italic>i</italic>: <italic>i</italic> &#x002B; <italic>k &#x2013;</italic> 1) as explained for example in [<xref rid="c22" ref-type="bibr">22</xref>]. To address (ii) we generate random fragments on the fly directly from the full genomes, rather than reading text samples line by line as in <monospace>fastText</monospace>.</p>
</sec>
<sec id="s2d">
<label>2.4</label>
<title>Regularization with noise</title>
<p>As a form of regularization, we also add random mutations in the training fragments. When reading the fragment from the reference genome, nucleotide by nucleotide, we introduce a chance <italic>r</italic> of replacing the nucleotide by a random one (equally distributed on {<italic>A, C, G, T</italic>}). This is akin to the dataset augmentations commonly used in image classification tasks, and in our case promotes the nearby embeddings of <italic>k</italic>-mers similar in terms of Hamming distance.</p>
</sec>
</sec>
<sec id="s3">
<label>3</label>
<title>Experiments</title>
<sec id="s3a">
<label>3.1</label>
<title>Data</title>
<p>We test our model on two benchmarks proposed by [<xref rid="c22" ref-type="bibr">22</xref>]: a <monospace>small</monospace> one, useful mostly for parameter tuning, and a <monospace>large</monospace> one. Both benchmarks involve a <italic>training</italic> database of genomes organized by species, and a <italic>validation</italic> set of genomes coming from the same species as the training database, but from different strains. Reads are randomly sampled, with or without noise, from the validation set of genomes, and the goal is to predict, for each read, from which species it comes from. The <monospace>small</monospace> database contains 356 complete genomes, belonging to 51 species of bacteria; its validation set is composed of 52 genomes, belonging to the same 51 species. The <monospace>large</monospace> database contains 2,961 genomes belonging to 774 species, which is closer to real-life situations. The validation set is composed of 193 genomes, each from a separate species.</p>
<p>For both training and testing, reads of length <italic>L</italic> = 200 are extracted from the genomes. The validation datasets are built by extracting fragments such that their coverage &#x2013; the average number of times each nucleotide is present &#x2013; is 1. This amounts to a total of 134, 319 validation samples for the <italic>small</italic> database, and &#x223C; 3.5M samples for the <italic>large</italic> database. The machine learning-based models are trained on reads sampled from the reference genomes and their known taxonomic labels, while alignment-based methods simply align validation reads to the training reference genomes. To account for the fact that DNA is double-stranded and that when a read is sequenced it can come from any of the two strands, which are reverse-complement to each other, we systematically add the reverse-complement of each read with the same label at training time.</p>
<p>In addition, we consider several <italic>noisy</italic> validation sets as in [<xref rid="c22" ref-type="bibr">22</xref>], where each fragment sampled from a genome is modified to mimick sequencing errors of actual sequencing machines, in particular substitutions, insertions and deletions of nucleotides. We use the specially-designed <monospace>grinder</monospace> software [<xref rid="c1" ref-type="bibr">1</xref>] to simulate 3 new sets of validation reads. The <italic>Balzer</italic> validation set is simulated with a homopolymeric error model, designed to emulate the Roche 454 technology [<xref rid="c2" ref-type="bibr">2</xref>]. The <italic>mutation-2</italic> and <italic>mutation-5</italic> sets are simulated with the 4th degree polynomial proposed by [<xref rid="c9" ref-type="bibr">9</xref>] to study general mutations (insertion/deletions and substitutions). The median mutation rates for these simulated reads are 2&#x0025; and 5&#x0025;, respectively. <italic>Balzer</italic> and <italic>mutation-2</italic> are meant to contain a realistic proportion of errors, and <italic>mutation-5</italic> is added as a more challenging set.</p>
</sec>
<sec id="s3b">
<label>3.2</label>
<title>Reference methods</title>
<p>We compare our methods, which we call <monospace>fastDNA</monospace> in the rest of the text, to two other strategies. One is the BWA-MEM sequence aligner [<xref rid="c13" ref-type="bibr">13</xref>] and the other is the linear SVM classifier on the <italic>k</italic>-spectral representation, implemented using the Vowpal Wabbit software in [<xref rid="c22" ref-type="bibr">22</xref>]. We name the latter method VW in the rest of the paper. We follow exactly the same configurations as [<xref rid="c22" ref-type="bibr">22</xref>] for both methods.</p>
</sec>
<sec id="s3c">
<label>3.3</label>
<title>Small dataset</title>
<sec id="s3c1">
<label>3.3.1</label>
<title>Memory footprint</title>
<p>As the embeddings matrix <italic>M</italic> is loaded in memory both for training and classification, <monospace>fastDNA</monospace> models have a significant memory footprint. [<xref rid="c7" ref-type="bibr">7</xref>] discuss various strategies to reduce it. The vocabulary cannot be pruned for our range of <italic>k</italic> as the <italic>k</italic>-mers are densely distributed, so the dimensions of <italic>M</italic> are fixed. The size of <italic>M</italic> in memory can however be reduced by quantization. We use Product Quantization (PQ, [<xref rid="c6" ref-type="bibr">6</xref>]), with the option to cluster separately the vector norms and directions (option <monospace>qnorm</monospace>). The linear layer is then retrained to account for the change in the embeddings. This compresses the model size by almost an order of magnitude without noticeably impacting the performance.</p>
<p>While PQ can make deploying and classifying more accessible, training the model still requires the full embedding matrix, as its quantized version is not trainable. We restrain our parameter choices (<italic>k</italic> and <italic>d</italic>) to models that fit on 64GB machines. The dimension of the embedding table (4<sup><italic>k</italic></sup>) is encoded on 32-bits, which further limits the value of <italic>k</italic> to <italic>k</italic><sub>max</sub> = 15. The largest models we consider are <italic>k</italic> = 13, <italic>d</italic> = 100 and <italic>k</italic> = 15, <italic>d</italic> = 10. Their memory footprints are available in <xref rid="fig1" ref-type="fig">figure 1</xref>.</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Memory requirement of <monospace>fastDNA</monospace> models as a function of <italic>k</italic>-mer sizes. The embedding dimensions <italic>d</italic> shown are 10, 100 and 1000. The reported value is the size in GB of the model binary, the minimal size required both in RAM to train and load the model, and on disk to save it.</title></caption>
<graphic xlink:href="335943_fig1.tif"/>
</fig>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Comparison between <monospace>fastDNA</monospace> and reference methods on the <italic>small</italic> dataset. This figure shows the average species-level recall obtained by <monospace>fastDNA</monospace> trained for 50 epochs and a learning rate 0.1, for different values of <italic>k</italic> and <italic>d</italic>. The results are compared with VW for different values of <italic>k</italic> and an alignment-based approach BWA-MEM.</title></caption>
<graphic xlink:href="335943_fig2.tif"/>
</fig>
<p>Contrary to VW, model size is completely determined by the user and does not depend on the number of possible classes <italic>T</italic> or on the vocabulary of the training database, which can become an advantage when <italic>T</italic> is large.</p>
</sec>
<sec id="s3c2">
<label>3.3.2</label>
<title>Coverage</title>
<p>The model is trained by picking a position at random in the reference genomes and using the 200-bp read starting from that position as a sample. One epoch of training consists of drawing enough random reads to cover each nucleotide of the reference genomes once on average (coverage of 1). We found that models with no training noise had converged by 50 epochs, and those with training noise benefited from extra epochs.</p>
<p>The results in this paper were obtained by training with a fixed number of epochs 50, and a learning rate of 0.1, chosen by a standard grid-search.</p>
</sec>
<sec id="s3c3">
<label>3.3.3</label>
<title>Performance</title>
<p>One first result from 3.3.3, is that increasing the embedding dimension <italic>d</italic> above 100 brings no added benefit in classification quality, at the cost of larger prediction times and memory footprint. This could in theory be expected from 4. Once <italic>d</italic> is greater than the number of classes, the matrix <italic>WM</italic> <sup><italic>T</italic></sup> has maximal rank <italic>T</italic>, so the model is virtually the same as the standard &#x201C;Bag of Words&#x201D; model VW. The differences observed between the two are likely due to different optimization procedures and implementations.</p>
<p>Excessively lowering the dimension <italic>d</italic> under <italic>T</italic> harms the performance, especially for shorter <italic>k</italic>-mers. However, the gap between models <italic>d</italic> = 10 and models <italic>d</italic> = 100 vanishes as <italic>k</italic> increases, suggesting that &#x2013; provided a sufficient vocabulary size &#x2013; projecting <italic>k</italic>-mers to a lower-dimensional space comes at little cost. Furthermore, models with longer <italic>k</italic>-mers but smaller dimension can achieve the same performance as a model with shorter <italic>k</italic> and greater dimension. The model <italic>k</italic> = 15, <italic>d</italic> = 10 has the same performance as <italic>k</italic> = 13, <italic>d</italic> = 100.</p>
<p>Finally, classification performance for values of <italic>k</italic> greater than 12 is competitive with alignment-based method BWA, which confirms machine learning approaches can be relevant for this problem of taxonomic binning.</p>
</sec>
</sec>
<sec id="s3d">
<label>3.4</label>
<title>Large dataset</title>
<p>We report the classification performance, measured by average species-level recall and precision of <monospace>fastDNA</monospace> on the validation sets described in 3.1. The influence of the training noise is shown in 3.4.</p>
<p>As could be expected, greater levels of sequencing noise in the validation sets lead to degraded performances. Adding random mutations to the training reads curbs this effect. The greater the mutation rate is, the more robust the model becomes to higher levels of sequencing noise. Somewhat more surprisingly, a certain range of mutation rates also increases the performance on the validation reads with no sequencing noise. The regularization induced by these artificial errors is therefore beneficial for both sequencing noise and intra-species heterogeneity. We found that the best rates of mutation were between 2 and 5&#x0025;. The models trained with these levels of noise are better all-around than their no-noise counterparts.</p>
<p>We compare the performance of <monospace>fastDNA</monospace> against that of VW and BWA in 3.4. For small levels of sequencing errors, <monospace>fastDNA</monospace> is competitive with BWA. Greater levels in sequencing noise widens the gap between the two, as BWA is very robust to sequencing noise, dropping less than 1&#x0025; for <italic>mutation-5</italic>.</p>
</sec>
<sec id="s3e">
<label>3.5</label>
<title>Classification speed</title>
<p>Speed is of critical importance in taxonomic binning and is the main motivation behind exploring machine learning techniques. Classifying a read with <monospace>fastDNA</monospace> can be separated in two parts. It first reads the sequence, computes the indices of the <italic>k</italic>-mers contained in the read and computes the read embedding by summing the <italic>k</italic>-mer embeddings. This step is of complexity &#x1D4AA;(<italic>dL</italic>), where <italic>L</italic> is the read length (constant in our experiments). Second, class probabilities are computed by applying the linear classifier, this step is of complexity &#x1D4AA;(<italic>dT</italic>). <monospace>fasttext</monospace> and <monospace>fastDNA</monospace> offer a different loss function, the hierarchical softmax, that reduces this step to &#x1D4AA;(<italic>d</italic> log(<italic>T</italic>)), which can become useful in the case of very large <italic>T</italic>. To this per-read time-complexity must be added a fixed overhead, the time necessary for the model to be loaded from disk, of complexity &#x1D4AA;(<italic>dN</italic> &#x002B; <italic>T</italic>), where <italic>N</italic> = 4<sup><italic>k</italic></sup> is the vocabulary size. Due to the large values of 4<sup><italic>k</italic></sup>, this is actually a significant portion of the total time in our experiments, and the main explanation to the time gap observed between classification with models of same embedding dimension <italic>d</italic> but different <italic>k</italic>.</p>
<p>The total time complexity for predicting a dataset with <italic>n</italic> samples is therefore &#x1D4AA; (<italic>n</italic>(<italic>dL</italic> &#x002B; <italic>dT</italic>) &#x002B; <italic>dN</italic>).</p>
<p>With reads of constant length, <monospace>fastDNA</monospace> and VW classify reads indiscriminately of their content, and therefore yield equal classification times across the validation sets. On the other hand, BWA&#x2019;s speed degrades with the sequencing noise, which is easily explained. BWA searches iteratively on the number of mismatches <italic>z</italic> and stops once it gets a hit. It will therefore be slower if there are more mismatches between the testing and reference data.</p>
<p>We show in <xref rid="fig3" ref-type="fig">figure 3</xref>.<xref rid="fig5" ref-type="fig">5</xref> the classification speeds measured for the <italic>large</italic> dataset. We show <monospace>fastDNA</monospace> for <italic>d</italic> = 100 and <italic>k</italic> = 12, 13 and VW. <monospace>fastDNA</monospace> and VW have similar classification times of &#x223C;6, 7 &#x00B7; 10<sup>3</sup> reads per minute. As remarked in [<xref rid="c22" ref-type="bibr">22</xref>], compositional approaches offer systematically better prediction times than BWA, with improvements of 2 9&#x00D7;. This speed improvement increases with the mismatch between predicted sequences and reference genomes, therefore with both sequencing noise and intra-species variations.</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Figure 3:</label>
<caption><title>Performance on the large dataset of <monospace>fastDNA</monospace> trained with different mutation rates. <italic>k</italic>-mer size and embedding dimension <italic>d</italic> are 13 and 100, respectively. The classification quality is measured on test sets generated with different sequencing error models.</title></caption>
<graphic xlink:href="335943_fig3.tif"/>
</fig>
<fig id="fig4" position="float" fig-type="figure">
<label>Figure 4:</label>
<caption><title>Comparison between <monospace>fastDNA</monospace> and reference methods on the <italic>large</italic> dataset. This figure shows the average species-level recall and precision obtained by <monospace>fastDNA</monospace>, VW and BWA on the different validation sets.</title></caption>
<graphic xlink:href="335943_fig4.tif"/>
</fig>
<fig id="fig5" position="float" fig-type="figure">
<label>Figure 5:</label>
<caption><title>Comparison between <monospace>fastDNA</monospace> and reference methods on the <italic>large</italic> dataset. This figure shows the average classification speed of the methods on the different test sets. Two versions of <monospace>fastDNA</monospace> are shown, one with <italic>k</italic>-mer size 12, the other with <italic>k</italic>-mer size 13. Both have embedding dimension <italic>d</italic> = 100. The four test sets used were simulated with different sequencing error models.</title></caption>
<graphic xlink:href="335943_fig5.tif"/>
</fig>
</sec>
</sec>
<sec id="s4">
<label>4</label>
<title>Conclusion</title>
<p>We demonstrated that learning a low-dimensional representation of DNA reads based on their <italic>k</italic>-mer composition is feasible, and outperforms state-of-the-art compositional approaches that work directly on the high-dimensional, <italic>k</italic>-spectral representation of DNA sequences. Controlling the dimension <italic>d</italic> of the embedding allows to consider longer <italic>k</italic>-mer for a given memory footprint. As other compositional methods, <monospace>fastDNA</monospace> is significantly faster than alignment-based methods, and is well adapted to classification onto many classes.</p>
<p>There are two immediate possible extensions of this work. One is to use a more realistic error model than uniform substitutions for training, to better mimick the expected noise in the data. The other is to extend the notion of &#x201C;word&#x201D; from contiguous <italic>k</italic>-mers to gap-seeded <italic>k</italic>-mers or Bloom filters [<xref rid="c15" ref-type="bibr">15</xref>], hopefully capturing longer-range dependencies. In terms of applications, assigning RNA-seq reads to genes to quantify their expression can also be formulated as a classification problem with typically <italic>T</italic> &#x223C; 22<italic>k</italic> classes, and may be well adapted to <monospace>fastDNA</monospace> as well.</p>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><given-names>F. E.</given-names> <surname>Angly</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Willner</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Rohwer</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Hugenholtz</surname></string-name>, and <string-name><given-names>G. W.</given-names> <surname>Tyson</surname></string-name>. <article-title>Grinder: a versatile amplicon and shotgun sequence simulator</article-title>. <source>Nucleic Acids Res</source>., <volume>40</volume>:<fpage>e94</fpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Balzer</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Malde</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Lanz&#x00E9;n</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Sharma</surname></string-name>, and <string-name><given-names>I.</given-names> <surname>Jonassen</surname></string-name>. <article-title>Characteristics of 454 pyrosequencing data&#x2013;enabling realistic simulation with flowsim</article-title>. <source>Bioinformatics</source>, <volume>26</volume>:<fpage>i420</fpage>&#x2013;<lpage>i425</lpage>, <year>2010</year>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="journal"><string-name><given-names>P.</given-names> <surname>Bojanowski</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Grave</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Joulin</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Mikolov</surname></string-name>. <article-title>Enriching word vectors with subword information</article-title>. <source>Transactions of the Association for Computational Linguistics</source>, <volume>5</volume>:<fpage>135</fpage>&#x2013;<lpage>146</lpage>, <year>2017</year>.</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="journal"><string-name><given-names>N. H. W.</given-names> <surname>Group</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Peterson</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Garges</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Giovanni</surname></string-name>, <string-name><given-names>P.</given-names> <surname>McInnes</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>J. A.</given-names> <surname>Schloss</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Bonazzi</surname></string-name>, <string-name><given-names>J. E.</given-names> <surname>McEwen</surname></string-name>, <string-name><given-names>K. A.</given-names> <surname>Wetterstrand</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Deal</surname></string-name>, <string-name><given-names>C. C.</given-names> <surname>Baker</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Di Francesco</surname></string-name>, <string-name><given-names>T. K.</given-names> <surname>Howcroft</surname></string-name>, <string-name><given-names>R. W.</given-names> <surname>Karp</surname></string-name>, <string-name><given-names>R. D.</given-names> <surname>Lunsford</surname></string-name>, <string-name><given-names>C. R.</given-names> <surname>Wellington</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Belachew</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Wright</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Giblin</surname></string-name>, <string-name><given-names>H.</given-names> <surname>David</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Mills</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Salomon</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Mullins</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Akolkar</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Begg</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Davis</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Grandison</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Humble</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Khalsa</surname></string-name>, <string-name><given-names>A. R.</given-names> <surname>Little</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Peavy</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Pontzer</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Portnoy</surname></string-name>, <string-name><given-names>M. H.</given-names> <surname>Sayre</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Starke-Reed</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Zakhari</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Read</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Watson</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Guyer</surname></string-name>. <article-title>The NIH human microbiome project</article-title>. <source>Genome research</source>, <volume>19</volume>:<fpage>2317</fpage>&#x2013;<lpage>2323</lpage>, <month>Dec</month>. <year>2009</year>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><string-name><given-names>D. H.</given-names> <surname>Huson</surname></string-name>, <string-name><given-names>A. F.</given-names> <surname>Auch</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Qi</surname></string-name>, and <string-name><given-names>S. C.</given-names> <surname>Schuster</surname></string-name>. <article-title>MEGAN analysis of metagenomic data</article-title>. <source>Genome Res</source>., <volume>17</volume>:<fpage>377</fpage>&#x2013;<lpage>386</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="journal"><string-name><given-names>H.</given-names> <surname>Jegou</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Douze</surname></string-name>, and <string-name><given-names>C.</given-names> <surname>Schmid</surname></string-name>. <article-title>Product quantization for nearest neighbor search</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>, <volume>33</volume>(<issue>1</issue>):<fpage>117</fpage>&#x2013;<lpage>128</lpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="other"><string-name><given-names>A.</given-names> <surname>Joulin</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Grave</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Bojanowski</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Douze</surname></string-name>, <string-name><given-names>H.</given-names> <surname>J&#x00E9;gou</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Mikolov</surname></string-name>. <article-title>Fasttext.zip: Compressing text classification models</article-title>. <source>arXiv preprint arXiv:1612.03651</source>, <year>2016</year>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="book"><string-name><given-names>A.</given-names> <surname>Joulin</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Grave</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Bojanowski</surname></string-name>, and <string-name><given-names>T.</given-names> <surname>Mikolov</surname></string-name>. <chapter-title>Bag of tricks for efficient text classification</chapter-title>. <source>In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</source>: Volume <volume>2</volume>, <italic>Short Papers</italic>, pages <fpage>427</fpage>&#x2013;<lpage>431</lpage>. <publisher-name>Association for Computational Linguistics</publisher-name>, <month>April</month> <year>2017</year>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Korbel</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Abyzov</surname></string-name>, <string-name><given-names>X.</given-names> <surname>Mu</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Carriero</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Cayting</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Zhang</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Snyder</surname></string-name>, and <string-name><given-names>M.</given-names> <surname>Gerstein</surname></string-name>. <article-title>PEMer: a computational framework with simulation-based error models for inferring genomic structural variants from massive paired-end sequencing data</article-title>. <source>Genome Biol</source>., <volume>10</volume>(<issue>2</issue>):<fpage>R23</fpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Langmead</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Trapnell</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Pop</surname></string-name>, and <string-name><given-names>S. L.</given-names> <surname>Salzberg</surname></string-name>. <article-title>Ultrafast and memory-efficient alignment of short DNA sequences to the human genome</article-title>. <source>Genome Biol</source>, <volume>10</volume>(<issue>3</issue>):<fpage>R25</fpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="book"><string-name><given-names>C.</given-names> <surname>Leslie</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Eskin</surname></string-name>, and <string-name><given-names>W.</given-names> <surname>Noble</surname></string-name>. <chapter-title>The spectrum kernel: a string kernel for SVM protein classification</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>R. B.</given-names> <surname>Altman</surname></string-name>, <string-name><given-names>A. K.</given-names> <surname>Dunker</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Hunter</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Lauerdale</surname></string-name>, and <string-name><given-names>T. E.</given-names> <surname>Klein</surname></string-name></person-group>, editors, <source>Proceedings of the Pacific Symposium on Biocomputing 2002</source>, pages <fpage>564</fpage>&#x2013;<lpage>575</lpage>, <publisher-loc>Singapore</publisher-loc>, <year>2002</year>. <publisher-name>World Scientific</publisher-name>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="book"><string-name><given-names>C.</given-names> <surname>Leslie</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Eskin</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Weston</surname></string-name>, and <string-name><given-names>W.</given-names> <surname>Noble</surname></string-name>. <chapter-title>Mismatch String Kernels for SVM Protein Classification</chapter-title>. In <person-group person-group-type="editor"><string-name><given-names>S.</given-names> <surname>Becker</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Thrun</surname></string-name>, and <string-name><given-names>K.</given-names> <surname>Obermayer</surname></string-name></person-group>, editors, <source>Advances in Neural Information Processing Systems</source> <volume>15</volume>. <publisher-name>MIT Press</publisher-name>, <year>2003</year>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="other"><string-name><given-names>H.</given-names> <surname>Li</surname></string-name>. <article-title>Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM</article-title>. <source>Technical Report 1303.3997, arXiv</source>, <year>2013</year>.</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="journal"><string-name><given-names>H.</given-names> <surname>Li</surname></string-name> and <string-name><given-names>R.</given-names> <surname>Durbin</surname></string-name>. <article-title>Fast and accurate short read alignment with Burrows-Wheeler transform</article-title>. <source>Bioinformatics</source>, <volume>25</volume>:<fpage>1754</fpage>&#x2013;<lpage>1760</lpage>, <year>2009</year>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="other"><string-name><given-names>Y.</given-names> <surname>Luo</surname></string-name>, <string-name><given-names>Y. W.</given-names> <surname>Yu</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Zeng</surname></string-name>, <string-name><given-names>B.</given-names> <surname>Berger</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Peng</surname></string-name>. <article-title>Metagenomic binning through low density hashing</article-title>. <source>bioRxiv</source>, <year>2017</year>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="journal"><string-name><given-names>S. S.</given-names> <surname>Mande</surname></string-name>, <string-name><given-names>M. H.</given-names> <surname>Mohammed</surname></string-name>, and <string-name><given-names>T. S.</given-names> <surname>Ghosh</surname></string-name>. <article-title>Classification of metagenomic sequences: methods and challenges</article-title>. <source>Briefings Bioinf</source>, <volume>13</volume>:<fpage>669</fpage>&#x2013;<lpage>681</lpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><string-name><given-names>A. C.</given-names> <surname>McHardy</surname></string-name>, <string-name><given-names>H. G.</given-names> <surname>Mart&#x00ED;n</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Tsirigos</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Hugenholtz</surname></string-name>, and <string-name><given-names>I.</given-names> <surname>Rigoutsos</surname></string-name>. <article-title>Accurate phylogenetic classification of variable-length DNA fragments</article-title>. <source>Nat. Methods</source>, <volume>4</volume>(<issue>1</issue>):<fpage>63</fpage>&#x2013;<lpage>72</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="other"><string-name><given-names>T.</given-names> <surname>Mikolov</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Chen</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Corrado</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Dean</surname></string-name>. <article-title>Efficient estimation of word representations in vector space</article-title>. <source>Technical Report 1301.3781, arXiv</source>, <year>2013</year>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><string-name><given-names>D. H.</given-names> <surname>Parks</surname></string-name>, <string-name><given-names>N. J.</given-names> <surname>MacDonald</surname></string-name>, and <string-name><given-names>R. G.</given-names> <surname>Beiko</surname></string-name>. <article-title>Classifying short genomic fragments from novel lineages using composition and homology</article-title>. <source>BMC Bioinf</source>., <volume>12</volume>:<fpage>328</fpage>, <year>2011</year>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="journal"><string-name><given-names>K. R.</given-names> <surname>Patil</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Roune</surname></string-name>, and <string-name><given-names>A. C.</given-names> <surname>McHardy</surname></string-name>. <article-title>The PhyloPythiaS web server for taxonomic assignment of metagenome sequences</article-title>. <source>PLoS One</source>, <volume>7</volume>:<fpage>e38581</fpage>, <year>2012</year>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><string-name><given-names>C. S.</given-names> <surname>Riesenfeld</surname></string-name>, <string-name><given-names>P. D.</given-names> <surname>Schloss</surname></string-name>, and <string-name><given-names>J.</given-names> <surname>Handelsman</surname></string-name>. <article-title>Metagenomics: genomic analysis of microbial communities</article-title>. <source>Annu. Rev. Genet</source>., <volume>38</volume>:<fpage>525</fpage>&#x2013;<lpage>552</lpage>, <year>2004</year>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><string-name><given-names>K.</given-names> <surname>Vervier</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Mah&#x00E9;</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Tournoud</surname></string-name>, <string-name><given-names>J.-B.</given-names> <surname>Veyrieras</surname></string-name>, and <string-name><given-names>J.-P.</given-names> <surname>Vert</surname></string-name>. <article-title>Large-scale machine learning for metagenomics sequence classification</article-title>. <source>Bioinformatics</source>, <volume>32</volume>:<fpage>1023</fpage>&#x2013;<lpage>1032</lpage>, <year>2016</year>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><string-name><given-names>Q.</given-names> <surname>Wang</surname></string-name>, <string-name><given-names>G. M.</given-names> <surname>Garrity</surname></string-name>, <string-name><given-names>J. M.</given-names> <surname>Tiedje</surname></string-name>, and <string-name><given-names>J. R.</given-names> <surname>Cole</surname></string-name>. <article-title>Naive bayesian classifier for rapid assignment of rRNA sequences into the new bacterial taxonomy</article-title>. <source>Appl. Environ. Microbiol</source>., <volume>73</volume>:<fpage>5261</fpage>&#x2013;<lpage>5267</lpage>, <year>2007</year>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><string-name><given-names>D. E.</given-names> <surname>Wood</surname></string-name> and <string-name><given-names>S. L.</given-names> <surname>Salzberg</surname></string-name>. <article-title>Kraken: ultrafast metagenomic sequence classification using exact alignments</article-title>. <source>Genome Biol</source>., <volume>15</volume>(<issue>3</issue>):<fpage>R46</fpage>, <year>2014</year>.</mixed-citation></ref>
</ref-list>
<fn-group>
<fn id="fn1"><label><sup>1</sup></label><p>A <italic>k</italic>-mer is a contiguous subsequence of <italic>k</italic> letters.</p></fn>
</fn-group>
</back>
</article>