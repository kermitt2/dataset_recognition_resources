<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/111294</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>Confirmatory Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Bioinformatics</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>MRIQC: Predicting Quality in Manual MRI Assessment Protocols Using No-Reference Image Quality Measures</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8435-6191</contrib-id>
<name><surname>Esteban</surname><given-names>Oscar</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">&#x002A;</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3748-6289</contrib-id>
<name><surname>Birman</surname><given-names>Daniel</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8479-6365</contrib-id>
<name><surname>Schaer</surname><given-names>Marie</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4023-419X</contrib-id>
<name><surname>Koyejo</surname><given-names>Oluwasanmi O.</given-names></name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6755-0259</contrib-id>
<name><surname>Poldrack</surname><given-names>Russell A.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3321-7583</contrib-id>
<name><surname>Gorgolewski</surname><given-names>Krzysztof J.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Department of Psychology, Stanford University</institution>, Stanford, California, <country>USA</country></aff>
<aff id="a2"><label>2</label><institution>Department of Psychiatry, University of Geneva School of Medicine</institution>, Geneva, <country>Switzerland</country></aff>
<aff id="a3"><label>3</label><institution>Department of Computer Science, University of Illinois at Urbana-Champaign</institution>, Illinois, <country>USA</country></aff>
</contrib-group>
<author-notes>
<fn id="n1"><label>&#x002A;</label><p><email>phd@oscaresteban.es</email></p></fn>
</author-notes>
<pub-date pub-type="epub"><year>2017</year></pub-date>
<elocation-id>111294</elocation-id>
<history>
<date date-type="received"><day>23</day><month>2</month><year>2017</year></date>
<date date-type="accepted"><day>24</day><month>2</month><year>2017</year></date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2017, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2017</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="111294.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Quality control of MR images is essential for excluding problematic acquisitions and avoiding bias in subsequent image processing and analysis. However, the visual inspection of individual images is time-consuming and limited by both intra- and inter-rater variance. The difficulty of visual inspection scales with study size and with the heterogeneity of multi-site data. Here, we describe a tool for the automated assessment of Tl-weighted MR images of the brain &#x2013; <italic>MRIQC.</italic> MRIQC calculates a set of quality measures from each image and uses them as features in a binary (include/exclude) classifier. The classifier was designed to ensure generalization to new samples acquired in different centers and using different scanning parameters from our training dataset. To achieve that goal, the classifier was trained on the Autism Brain Imaging Data Exchange (ABIDE) dataset (N&#x003D;1102), acquired at 17 locations with heterogeneous scanning parameters. We selected random forests from a set of models and pre-processing options using nested cross-validation on the ABIDE dataset. We report a performance of &#x007E;89&#x0025; accuracy of the best model evaluated with nested cross-validation. The best performing classifier was then evaluated on a held-out (unseen) dataset, unrelated to ABIDE and labeled by a different expert, yielding &#x007E;73&#x0025; accuracy. The MRIQC software package and the trained classifier are released as an open source project, so that individual researchers and large consortia can readily curate their data regardless the size of their databases. Robust QC is crucial to identify early structured imaging artifacts in ongoing acquisition efforts, and helps detect individual substandard images that may bias downstream analyses.</p>
</abstract>
<counts>
<page-count count="18"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Image analysis can lead to erroneous conclusions when the original data are of low quality. MRI images are unlikely to be artifact-free, and assessing the quality of images produced by MR scanning systems has long been a challenging issue [<xref ref-type="bibr" rid="c1">1</xref>]. Traditionally, all images in the sample under analysis are visually inspected by one or more experts, and those showing an insufficient level of quality are excluded (some examples are given in <xref ref-type="fig" rid="fig1">Fig 1A</xref>). Visual assessment is time consuming and prone to variability due to inter-rater differences (see <xref ref-type="fig" rid="fig1">Fig 1B</xref>), as well as intra-rater differences arising from factors such as practice or fatigue. An additional concern is that some artifacts evade human detection entirely [<xref ref-type="bibr" rid="c2">2</xref>] for example those due to improper choice of acquisition parameters. Even though magnetic resonance (MR) systems undergo periodic inspections and service, some machine-related artifacts persist unnoticed due to lenient vendor quality checks, and drift from the system calibration settings. In our experience, automated Quality Control (QC) protocols help detect these issues early in the processing stream. The current trend of neuroimaging towards acquiring very large samples across multiple scanning sites [<xref ref-type="bibr" rid="c3">3</xref>&#x2013;<xref ref-type="bibr" rid="c5">5</xref>] introduces additional concerns. These large scale imaging efforts render the visual inspection of every image infeasible and add the possibility of between-site variability. Therefore, there is a need for fully-automated, robust, and minimally biased QC protocols. These properties are difficult to achieve for three reasons: 1) the absence of a gold standard impedes the definition of sensitive quality metrics; 2) human experts introduce biases with their visual assessment; and 3) cross-study and inter-site acquisition differences also introduce uncharacterized variability.</p>
<fig id="fig1" position="float" orientation="portrait" fig-type="figure">
<label>Figure 1.</label>
<caption>
<title>Visual assessment of MR scans and the quality control of the ABIDE dataset.</title>
<p><bold>A.</bold> Two images with prominent artifacts from the ABIDE database are presented. An example scan (top) is shown with severe motion artifacts. The arrows point to signal spillover through the phase-encoding axis (right-to-left &#x2013;RL&#x2013;) due to eye movements (green) and vessel pulsations (red). A second example scan (bottom) shows severe coil artifacts. This figure caption is extended in Figure SI1.</p>
<p><bold>B.</bold> Info-graphic of the visual assessment of the Tl-weighted (T1w) MR images of the ABIDE dataset performed by three different experts, split by scanning site. Each scanning site has one stripe with three rows of colored circles, except sites with large samples where the ratings are wrapped in two stripes. Each circle represents the rating of one image by one of the experts, with the color encoding the quality label (green is &#x201C;accept&#x201D;, yellow is &#x201C;doubtful&#x201D;, red is &#x201C;exclude&#x201D; and white denotes missing ratings). Each row is a different expert, thus the inter-rater consistency can be checked column-wise. A perfect agreement occurs when the three circles of a column show the same color (for example, the first image of the &#x201C;CALTECH&#x201D; scanning site). Some images yielded no agreement across raters (e.g. the second participant in the &#x201C;PITT&#x201D; sample). Next to each site label, the spread of ratings is reported. Rows are the three raters, and columns the ratings. First (in green color) for &#x201C;accept&#x201D;, second (gray) for &#x201C;doubtful&#x201D;, and third (red) is &#x201C;exclude&#x201D;. The aggregated (all sites of ABIDE) rates are presented in the top-right box.</p>
</caption>
<graphic xlink:href="111294_fig1.tif"/>
</fig>
<p>Machine-specific artifacts have been traditionally tracked down using phantoms [<xref ref-type="bibr" rid="c6">6</xref>] in a quantitative manner. However, many forms of image degradation are participant-specific or arise from practical settings (see <xref ref-type="fig" rid="fig1">Fig 1</xref>, panel A). Woodard and Carley-Spencer [<xref ref-type="bibr" rid="c7">7</xref>] conducted one of the earliest evaluations of automated quality assessment on a large dataset of 1001 T1w images from 143 participants. They defined a set of 239 <italic>no-reference</italic><xref ref-type="fn" rid="fn1"><sup>1</sup></xref> image-quality metrics (IQMs). The IQMs belonged to two families depending on whether they were derived from Natural Scene Statistics or quality indices defined by the JPEG consortium. The IQMs were calculated on image pairs with and without several synthetic distortions. In an analysis of variance, the IQM from both families reliably discriminated among undistorted images, noisy images, and images distorted by intensity non-uniformity (INU). Mortamet et al. [<xref ref-type="bibr" rid="c8">8</xref>] proposed two quality indices focused on detecting artifacts in the air region surrounding the head, and analyzing the goodness-of-fit of a model for the background noise in that air area. One principle underlying their proposal is that most of the artifact signal propagates over the image and into the background. They applied these two IQMs in 749 T1w scans from the Alzheimer&#x2019;s Disease Neuroimaging Initiative (ADNI) database. Different cutoff thresholds were defined for the two IQMs and compared to a binary (high/low quality) classification performed by a human rater, concluding that more specific research was required to determine these thresholds and generalize them to different datasets. They achieved an 85&#x0025; accuracy in an intra-site validation approach. However, many potential sources of uncontrolled variability exist between studies and sites, including magnetic resonance imaging (MRI) protocols (scanner manufacturer, MR sequence parameters, etc.), scanning settings, participant instructions, inclusion criteria, etc. For these reasons, the thresholds they proposed on their IQMs are unlikely to generalize beyond the ADNI database. Recently, Pizarro et al. [<xref ref-type="bibr" rid="c9">9</xref>] proposed the use of a support-vector machine classifier (SVC) trained on 1457 structural MRI images acquired in one site with constant scanning parameters. They proposed three volumetric features and three features targeting particular artifacts. The volumetric features were the normalized histogram, the tissue-wise histogram and the ratio of the modes of gray matter (GM) and white matter (WM). The artifacts addressed were the eye motion spillover in the anterior-to-posterior phase-encoding direction, the head-motion spillover over the nasio-cerebellum axis (which they call <italic>ringing artifact</italic>) and the so-called wrap-around (which they refer to as <italic>aliasing artifact</italic>). They reported a prediction accuracy around 80&#x0025;, assessed using 10-fold cross-validation. Some other recent efforts to develop IQMs appropriate for MRI include the Quality Assessment Protocol<xref ref-type="fn" rid="fn2"><sup>2</sup></xref> (QAP) under the preprocessed-connectomes project (PCP), and the UK Biobank [<xref ref-type="bibr" rid="c10">10</xref>].</p>
<p>The hypothesis behind this study is that we can predict the quality ratings of an expert on previously unseen datasets (with dataset-specific scanning parameters) in a supervised learning approach that uses features derived from a broad selection of IQMs. To demonstrate that the trained classifier correctly predicts the quality of new data, we used two unrelated databases to configure the training and held-out (test) datasets [<xref ref-type="bibr" rid="c11">11</xref>]. We first select the best performing model on the training dataset using a grid strategy in a nested cross-validation setup. We use the ABIDE database [<xref ref-type="bibr" rid="c4">4</xref>] for the training set because data are acquired in 17 different scanning sites with varying acquisition parameters (<xref ref-type="table" rid="tbl1">Table 1</xref>). These data show great variability in terms of imaging settings and parameters, what represents the heterogeneity of real world data. The best performing classifier is then trained in the full ABIDE dataset, and tested in the held-out dataset [<xref ref-type="bibr" rid="c12">12</xref>] to assess whether the performance on unseen data falls within the range predicted by the nested cross-validation.</p>
<table-wrap id="tbl1" orientation="portrait" position="float">
<label>Table 1.</label>
<caption><title>Summary table of the train and test datasets.</title>
<p>The ABIDE dataset is publicly available<sup>a</sup>, and contains images acquired at 17 sites, with a diverse set of acquisition settings and parameters. This heterogeneity makes it a good candidate to train machine learning models that can generalize well to novel samples from other sites We selected ds030 [<xref ref-type="bibr" rid="c12">12</xref>] from OpenfMRI<sup>b</sup> as held-out dataset to evaluate the performance on data unrelated to the training set. A table summarizing the heterogeneity of parameters within the ABIDE dataset and also ds030 is provided as supplemental material (Table SI1).</p>
</caption>
<graphic xlink:href="111294_tbl1.tif"/>
</table-wrap>
<p>The contributions of this work are summarized as follows. First, we release a software tool called MRIQC (described in The MRIQC tool) to extract of a number of IQMs (Extracting the Image Quality Metrics) that characterize each input image. Second, MRIQC includes a visual reporting system (described in the Visual reports section) to ease the manual investigation of potential quality issues. These visual reports allow researchers to quickly evaluate the cases flagged by the MRIQC classifier or visually identify potential images to be flagged by looking at the group distributions of IQMs. Finally, we report the results from a pre-registered analysis of this study (<ext-link ext-link-type="uri" xlink:href="https://osf.io/haf97/">https://osf.io/haf97/</ext-link>) on the feasibility of automatic quality control labeling (sections Supervised classification and Results).</p>
</sec>
<sec id="s2">
<title>Materials and Methods</title>
<sec id="s2a">
<title>Training and test datasets</title>
<p>A total of 1375 T1w scans are used as training (1102 from ABIDE) and test (273 from ds030) samples. These databases were intentionally selected for their heterogeneity to match the purpose of the study. A brief summary illustrating the diversity of acquisition parameters is presented in <xref ref-type="table" rid="tbl1">Table 1</xref>, and a full-detail table in Table SI1.</p>
<sec id="s2a1">
<title>Labeling protocol</title>
<p>The labeling process is aided by surface reconstruction, using the so-called <italic>white</italic> (WM-GM interface) and the <italic>pial</italic> (delineating the outer interface of the cortex) surfaces as visual cues for the rater. We utilize <italic>FreeSurfer</italic> [<xref ref-type="bibr" rid="c13">13</xref>] to reconstruct the surfaces. <italic>FreeSurfer</italic> has been recently proposed as a visual aid tool to assess T1w images [<xref ref-type="bibr" rid="c14">14</xref>]. For run-time considerations, and to avoid circular evaluations of <italic>FreeSurfer</italic>, this tool is not used in the MRIQC workflow (see The MRIQC tool section).</p>
<p>The following protocol was used for the manual assessment of T1w images: 1) The 3D cortical surfaces were reconstructed using <italic>FreeSurfer 5.3.0.</italic> 2) An animated GIF (graphics interchange format) file was generated from the coronal slices of the 3D volume, including the projection of the 3D cortical surfaces in each slice<xref ref-type="fn" rid="fn3"><sup>3</sup></xref>. Each animation had a duration of around 20s. 3) A trained expert inspected the animation several times (generally, three times), and assigned a quality level (&#x201C;exclude&#x201D;/&#x201C;doubtful&#x201D;/&#x201C;accept&#x201D;).</p>
<p>During the visualization, the rater assessed the overall quality of the image. The <italic>white</italic> and <italic>pial</italic> contours were used as evaluation surrogates, given that &#x201C;exclude&#x201D; images usually exhibit imperfections and inaccuracies on these surfaces. When the expert found general quality issues or the reconstructed surfaces revealed more specific artifacts, the &#x201C;exclude&#x201D; label was assigned and the rater noted a brief description, for example: &#x201C;low signal-to-nose ratio (SNR)&#x201D;, &#x201C;poor image contrast&#x201D;, &#x201C;ringing artifacts&#x201D;, &#x201C;head motion&#x201D;, etc.). The images in ds030 were randomized before rating.</p>
</sec>
</sec>
<sec id="s2b">
<title>Software instruments and calculation of the IQMs</title>
<sec id="s2b1">
<title>The MRIQC tool</title>
<p>MRIQC is an open-source project, developed under the following software engineering principles. 1) <italic>Modularity and integrability</italic>: MRIQC implements a nipype [<xref ref-type="bibr" rid="c15">15</xref>] workflow (see <xref ref-type="fig" rid="fig2">Fig 2</xref>) to integrate modular sub-workflows that rely upon third party software toolboxes such as <italic>FSL</italic> [<xref ref-type="bibr" rid="c16">16</xref>], <italic>ANTs</italic> [<xref ref-type="bibr" rid="c17">17</xref>] and <italic>AFNI</italic> [<xref ref-type="bibr" rid="c18">18</xref>]. 2) <italic>Minimal preprocessing</italic>: the workflow described before should be as minimal as possible to estimate the IQMs on the original data or their minimally processed derivatives. 3) <italic>Interoperability and standards</italic>: MRIQC follows the the brain imaging data structure (BIDS, [<xref ref-type="bibr" rid="c19">19</xref>]), and it adopts the BIDS-App [<xref ref-type="bibr" rid="c20">20</xref>] standard. An example of the ease of running MRIQC is presented in Listing SI1. 4) <italic>Reliability and robustness</italic>: the software undergoes frequent vetting sprints by testing its robustness against data variability (acquisition parameters, physiological differences, etc.) using images from the OpenfMRI resource. Reliability is checked and maintained with the use of a continuous integration service.</p>
<fig id="fig2" position="float" orientation="portrait" fig-type="figure">
<label>Figure 2.</label>
<caption>
<title>MRIQC&#x2019;s processing data flow.</title>
<p>Images undergo an optimized processing pipeline to: 1) realign images in a conformed space using AFNI realign; 2) INU estimation using N4ITK; 3) skull-stripping using AFNI 3dSkullStrip; 4) brain tissue segmentation using FSL-FAST; 5) computation of an air/tissue mask using the magnitude of the gradient image [<xref ref-type="bibr" rid="c8">8</xref>]; 6) mapping of an exclusion mask defined in MNI space into the subject using an affine registration scheme with ANTs; 7) computation of an air mask excluding the region below the plane crossing the nasio-cerebellum axis; 8) computation of artifactual regions [<xref ref-type="bibr" rid="c8">8</xref>]; and 9) computation of a surrounding air-mask without artifacts; 10) projection of all the computed masks and segmentations to the original (native) space of the image volume</p>
</caption>
<graphic xlink:href="111294_fig2.tif"/>
</fig>
</sec>
<sec id="s2b2">
<title>Extracting the Image Quality Metrics</title>
<p>The final steps of the MRIQC&#x2019;s workflow compute the different IQMs, and a summary JSON file per subject is generated. The IQMs can be grouped in four broad categories (see <xref ref-type="table" rid="tbl2">Table 2</xref>), providing a vector of 56 features per anatomical image. Some measures characterize the impact of noise and/or evaluate the fitness of a noise model. A second family of measures use information theory and prescribed masks to evaluate the spatial distribution of information. A third family of measures look for the presence and impact of particular artifacts. Specifically, the INU artifact, and the signal leakage due to rapid motion (e.g. eyes motion or blood vessel pulsation) are identified. Finally, some measures that do not fit within the previous categories characterize the statistical properties of tissue distributions, volume overlap of tissues with respect to the volumes projected from MNI space, the sharpness/blurriness of the images, etc. The ABIDE and ds030 datasets were processed with <italic>MRIQC-&#x03C5;.0.9.0-rc2</italic> using the <italic>Lonestar5</italic> supercomputer at the Texas Advanced Computing Center, University of Texas, TX, USA.</p>
<table-wrap id="tbl2" orientation="portrait" position="float">
<label>Table 2.</label>
<caption>
<title>Summary table of IQMs.</title>
<p>The 14 IQMs spawn a vector of 56 features per anatomical image on which the classifier is learned and tested.</p>
</caption>
<graphic xlink:href="111294_tbl2.tif"/>
</table-wrap>
</sec>
<sec id="s2b3">
<title>Visual reports</title>
<p>In order to ease the screening process of individual images, MRIQC generates individual reports with mosaic views of a number of cutting planes and supporting information (for example, segmentation contours). The most straightforward use-case is the visualization of those images flagged as low-quality by the classifier.</p>
<p>After the extraction of IQMs in all the images of our sample, a group report is generated (<xref ref-type="fig" rid="fig3">Fig 3</xref>). The group report shows a scatter plot for each of the IQMs, so it is particularly easy to identify the cases that are outliers for each metric. The plots are interactive, such that clicking on any particular sample opens the corresponding individual report of that case. Examples of group and individual reports for the ABIDE dataset are available online at <ext-link ext-link-type="uri" xlink:href="http\\mriqc.org">mriqc.org</ext-link>.</p>
<fig id="fig3" position="float" orientation="portrait" fig-type="figure">
<label>Figure 3.</label>
<caption>
<title>Visual reports.</title>
<p>MRIQC generates one individual report per subject in the input folder and one group report including all subjects. To visually assess MRI samples, the first step (1) is opening the group report. This report shows boxplots and strip-plots for each of the IQMs. Looking at the distribution, it is possible to find images that potentially show low-quality as they are generally reflected as outliers in one or more strip-plot. For instance, in (2) hovering a suspicious sample within the coefficient of joint variation (CJV) plot, the subject identifier is presented (&#x201C;sub-51296&#x201D;). Clicking on that sample will open the individual report for that specific subject (3). This particular example of individual report is available online at <ext-link ext-link-type="uri" xlink:href="https://web.stanford.edu/group/poldracklab/mriqc/reports/sub-51296_T1w.html">https://web.stanford.edu/group/poldracklab/mriqc/reports/sub-51296_T1w.html</ext-link>.</p>
</caption>
<graphic xlink:href="111294_fig3.tif"/>
</fig>
</sec>
</sec>
<sec id="s2c">
<title>Supervised classification</title>
<p>Our supervised learning approach to predicting the binary ratings of a human expert is structured in two steps. First, we perform a preliminary model selection and evaluation using repeated (x1000) and nested cross-validation, on the ABIDE dataset (see Step 1: Tested models and selection). Then, a second optimization in a refined grid of hyper-parameters for the model selected previously is performed with a single-loop cross-validation on the ABIDE dataset. The best performing model of this second cross-validation step is evaluated using the held-out dataset (see Step 2: Validation on the held-out dataset). The cross-validation workflows are built upon scikit-learn [<xref ref-type="bibr" rid="c26">26</xref>] and run using the <italic>Stampede</italic> supercomputer at the Texas Advanced Computing Center, University of Texas, TX, USA.</p>
<sec id="s2c1">
<label>Step 1:</label>
<title>Tested models and selection</title>
<p>Based on the number of features (56) and training data available (&#x007E;1100 data points), we compare two families of classifiers: SVCs and random forests classifiers (RFCs). Given the diversity of scanning sites, in the model selection loop we also investigate the need for normalizing (<italic>zscoring</italic>) features. In the following, models including a preliminary <italic>zscoring</italic> will show the suffix &#x201C;<italic>&#x2212;zs</italic>&#x201D; while those using the original features without such transformation are noted with the suffix &#x201C;<italic>&#x2212;nzs</italic>&#x201D;.</p>
<sec id="s2c1a">
<title>The support-vector machine classifier (SVC)</title>
<p>A support-vector machine [<xref ref-type="bibr" rid="c27">27</xref>] finds a hyperplane in the high-dimensional space of the features that robustly classifies the data. The SVC then uses the hyperplane to decide the class that is assigned to new samples in the space of features. Two hyper-parameters define the support-vector machine algorithm: a kernel function that defines the similarity between data points to ultimately compute a distance to the hyperplane, and a regularization weight <italic>C</italic>. In particular, we analyzed here the linear SVC implementation (as of now, &#x201C;SVC-lin&#x201D;) and the one based on radial basis functions (denoted by &#x201C;SVC-rbf&#x201D;). During model selection, we evaluated the regularization weight <italic>C</italic> and the <italic>&#x03B3;</italic> parameter (kernel width) of the SVC-rbf.</p>
</sec>
<sec id="s2c1b">
<title>The random forests classifier (RFC)</title>
<p>Random forests [<xref ref-type="bibr" rid="c28">28</xref>] are an nonparametric ensemble learning method that builds multiple decision trees. Then, a RFC assigns to each new sample the mode of the predicted classes of all decision trees in the ensemble. In this case, random forests are driven by a larger number of hyper-parameters. Particularly, in this work we analyze the maximum tree-depth, the minimum number of samples per split and the total number of decision trees.</p>
</sec>
<sec id="s2c1c">
<title>Objective function</title>
<p>The performance of each given model and parameter selection can be quantified with different metrics. Given the imbalance of positive and negative cases &#x2013;with lower prevalence of &#x201C;reject&#x201D; samples&#x2013;, we select the area under the curve (AUC) of the receiver-operator characteristic as objective score. We also report the classification accuracy as an additional performance measure.</p>
</sec>
<sec id="s2c1d">
<title>Cross-validation and nested cross-validation</title>
<p>Cross-validation is a model selection and validation technique robust to inhomogeneities [<xref ref-type="bibr" rid="c29">29</xref>]. We use nested cross-validation, which divides the process in two validation loops: an inner loop for selecting the best model and hyper-parameters, and an outer loop for evaluation. In cross-validation, the data are split into a number of folds, each containing a training and a test set. For each fold, the classifier is trained on the first set and evaluated on the latter. When cross-validation is nested, the training set is split again into folds within the inner loop, and training/evaluation are performed to optimize the model parameters. Only the best performing model of the inner loop is then cross-validated in the outer loop. In order to increase the robustness against model variability, we repeat the nested cross-validation procedure 1000 times.</p>
</sec>
<sec id="s2c1e">
<title>Data split scheme</title>
<p>Since we wanted to estimate the performance in datasets acquired at sites and with parameters different from those in the ABIDE dataset, we selected a <italic>leave-one-site-out (LoSo)</italic> partition strategy for the outer loop of cross-validation. The LoSo split leaves a whole site as a test set at each cross-validation iteration. Therefore, no knowledge of the testing set is leaked into the training set (the remaining <italic>N</italic> &#x2014; 1 sites). For the inner loop (model selection) we compared the performance of a stratified 10-fold and LoSo over the remaining 16 sites (one site is held out by the outer loop). All the possible combinations of models and their hyper-parameters (over 5000) are evaluated repeatedly (1000 times) in a grid search for the best average AUC score in the inner cross-validation loop.</p>
</sec>
<sec id="s2c1f">
<title>Feature ranking</title>
<p>One tool to improve the interpretability of the RFC is the calculation of feature rankings [<xref ref-type="bibr" rid="c28">28</xref>] by means of variable importance or Gini importance. Since we use scikit-learn, the implementation uses Gini importance, defined for a single tree as the total decrease in node impurity weighted by the probability of reaching that node. We finally report the median feature importance over all trees of the ensemble.</p>
</sec>
</sec>
<sec id="s2c2">
<label>Step 2:</label>
<title>Validation on the held-out dataset</title>
<p>In the second step, we cross-validated the model selected in step 1, optimizing a grid search refined to the selection of parameters done before. For this second cross-validation, we use the LoSo split strategy given the results obtained in the previous step. The best performing model is then trained on the full ABIDE dataset and the resulting classifier is used in the prediction of quality ratings of the held-out dataset (ds030).</p>
</sec>
</sec>
</sec>
<sec id="s3">
<title>Results</title>
<p>All images included in the selected datasets were processed with MRIQC. After extraction of the IQMs from the ABIDE, a total of 1102 images had both quality ratings and quality features (ten T1w images of the ABIDE are missing in the database). In the case of ds030, 265 images had the necessary quality ratings and features (eight images were not rated and/or failed during feature extraction).</p>
<sec id="s3a">
<title>Model selection</title>
<p>The results of the step 1 (nested cross-validation) are summarized in <xref ref-type="fig" rid="fig4">Fig 4</xref>. The best performing model, regardless of inner loop split strategy, was the random forests classifier without <italic>zscoring</italic> (RFC-nzs). The RFC-nzs using LoSo in the inner loop yielded the following averaged scores off all repeated outer loops: AUC&#x003D;0.862 (<italic>&#x03C3;</italic>&#x003D;&#x00B1;0.121) and accuracy of 89.4&#x0025; (<italic>&#x03C3;</italic>&#x003D;&#x00B1;9.95&#x0025;). The corresponding averaged scores for the 10-fold strategy were: AUC&#x003D;0.848 (<italic>&#x03C3;</italic>&#x003D;&#x00B1;0.135) and accuracy of 88.6&#x0025; (<italic>&#x03C3;</italic>&#x003D;&#x00B1;11.5&#x0025;). These results indicated that there is no practical difference between the two split strategies as regards model selection through cross-validation on this dataset. Therefore, since the averaged scores using LoSo cross-validation in the inner loop were slightly higher, it was selected as split strategy for the cross-validation in step 2. Note that the split strategy is not a model feature, and thus this decision can be made based on the results of the outer loop of nested cross-validation, as opposed to the model selection that is done based on the inner cross-validation loops.</p>
<fig id="fig4" position="float" orientation="portrait" fig-type="figure">
<label>Figure 4.</label>
<caption>
<title>Results of the step 1, nested cross-validation. (Left) Model selection &#x2013; inner loop</title>
<p>All the possible combinations of model, <italic>zscoring</italic> and hyper-parameters were evaluated. The violinplot shows the distribution of scores of the best performing hyper-parameters per model and preprocessing combination. The scores obtained using the stratified 10-fold split are presented in blue. In orange, the results corresponding to LoSo. In general, the 10-fold splitting was more optimistic for all models, whereas the LoSo scores are closer to results obtained in the outer (evaluation) loop. In all iteration loops, regardless of split strategy and cross-validation repetition, the RFC-nzs achieved the best score, with varying parameters. As expected, <italic>zscoring</italic> the features was necessary for both SVC_lin and SVC_rbf to exhibit acceptable performances, but always below that of the RFC-nzs. <bold>(Right) Evaluation &#x2013; outer loop</bold>. On the right hand panel, colors represent again the split strategy used in the inner loop. With colored markers, the average cross-validated score is annotated in a box with the &#x03BC; symbol. Below, the spread of the distribution is noted. Please note that, since the scores are bounded above 1.0, the values of the standard deviation &#x03C3; are probably underestimated. The distributions of nested cross-validated scores for both AUC and accuracy were rather independent from the split strategy used in the inner loop. The results for both metrics obtained in the evaluation of the held-out dataset (ds030) are represented in the corresponding distribution of nested cross-validation scores, showing that the performance on unseen data falls very close to one standard deviation below the average score. In this case, the average cross-validated score was higher for LoSo (AUC&#x003D;0.862/accuracy&#x2248;89.4&#x0025;) as compared to the 10-fold split (AUC&#x003D;0.848/accuracy&#x2248;88.6&#x0025;). Also the spread of cross-validated scores is slightly lower for LoSo (AUC&#x003D;&#x00B1;0.121/accuracy&#x003D;&#x00B1;9.95&#x0025; vs. AUC&#x003D;&#x00B1;0.135/accuracy&#x003D;&#x00B1;11.5&#x0025;).</p>
</caption>
<graphic xlink:href="111294_fig4.tif"/>
</fig>
<p>The best performing model and parameters selected as the maximum average of the AUC score in the inner loop, across all repetitions of the nested cross-validation was the RFC-nzs, with 50 trees (<monospace>n_estimators</monospace>), maximum tree depth (<monospace>max_depth</monospace>) of 20, and a; minimum of 2 samples per split (<monospace>min_samples_split</monospace>). However, the cross-validation was very variable in the selection of hyper-parameters, indicating that there was little difference in performance for all the points in the hyper-parameters grid.</p>
</sec>
<sec id="s3b">
<title>Evaluation on held-out data</title>
<p>In the second cross-validation step, only the previously selected RFC-nzs model was optimized, in a refined grid centered around the best performing parameters of step 1 (<monospace>n_estimators&#x003D;50, max_depth&#x003D;20, min_samples_split&#x003D;2</monospace>). The AUC on the evaluation set was 0.695 and the accuracy 72.83&#x0025;. We also analyzed the relevance of each feature in the overall forest decision (<xref ref-type="fig" rid="fig5">Fig 5</xref>). The most relevant features are the coefficient of joint variation (CJV) and the SNR measured on the WM tissue mask.</p>
<fig id="fig5" position="float" orientation="portrait" fig-type="figure">
<label>Figure 5.</label>
<caption>
<title>Feature importances in the final classifier.</title>
<p>The features used by the best-performing RFC-nzs classifier used to evaluate the held-out dataset are presented. For each feature the boxplot represents the distribution of feature importances within the trees in the ensemble. The features are ordered from highest median importance (the CJV) to lowest (average SNR computed as in [<xref ref-type="bibr" rid="c23">23</xref>], <monospace>snrd_total</monospace>).</p>
</caption>
<graphic xlink:href="111294_fig5.tif"/>
</fig>
</sec>
</sec>
<sec id="s4">
<title>Discussion</title>
<p>We propose a quantitative approach to quality control T1w MRI of the brain, enabling the automatic identification of sub-standard acquisitions. Quality control protocols are implemented to exclude faulty datasets that can bias the final results. Human brain images can be degraded by various sources of artifacts, related to the scanning device and settings or due to the participants themselves. Machine-derived artifacts are efficiently mitigated in a quantitative manner with calibration. However, due to the lack of reliable quality quantification tools, subject-specific artifacts and drifts from the service settings are assessed visually. The visual inspection of every MRI acquisition of the brain is a time-consuming and bias-prone task that would be ideally replaced by decision algorithms. Automating the QC process is particularly necessary for ongoing studies such as the UK Biobank that will collect data from tens of thousands of individuals.</p>
<p>Previous efforts [<xref ref-type="bibr" rid="c7">7</xref>, <xref ref-type="bibr" rid="c30">30</xref>] in the quantification of image quality for their assessment included the definition of image-quality metrics (IQMs). However, the approach was unfeasible for a total automation due to the limited sensitivity of the available IQMs to the most prevalent artifacts. Subsequent efforts [<xref ref-type="bibr" rid="c8">8</xref>] were focused on specific samples, setting generalization to new datasets as a future line of their work. Pizarro et al. [<xref ref-type="bibr" rid="c9">9</xref>] recently presented a similar approach to quality control images. They obtain a cross-validated accuracy of &#x007E;80&#x0025; for their support-vector machine classifier (SVC), in a single-site sample with homogeneous acquisition parameters.</p>
<p>In this work, we train a random forests classifier and evaluate its performance to predict the quality assessment of human raters on completely novel samples. We show that linear SVCs do not perform well on heterogeneous samples with diversity of acquisition parameters, and they always require normalization of features derived from multi-site data. Our results invariably indicated a better performance of a random forests classifier (RFC), with and without normalization of features. Particularly, the best performing model (RFC-nzs, for &#x201C;not zscored&#x201D;) achieved a &#x007E;89.4&#x0025; (<italic>&#x03C3;</italic>&#x003D;&#x00B1;9.95&#x0025;) accuracy. This improved performance over the one reported by Pizarro et al. may also be related to the selection of classification features proposed in this paper. Even though they reported that classification improved with the addition of features addressing certain artifacts, in our feature importance analysis the first IQM addressing a specific artifact was ranked in 9<sup>th</sup> position. This result suggests that there are complex relationships between the features (in multi-site studies) that may not be captured by SVCs. When tested on unseen data, the RFC-nzs classifier yielded an area under the curve (AUC) score of &#x007E;0.695 and accuracy of 73&#x0025;. This performance falls within the performance previously evaluated with nested cross-validation. We could not compare these results with [<xref ref-type="bibr" rid="c9">9</xref>] since they did not test their resulting classifier on a held-out dataset. The performance drop between the nested cross-validated score (&#x007E;89&#x0025;) and the score obtained on the held-out data (&#x007E;73&#x0025;) may be explained by the interplay of several factors. First, we introduced an unplanned inter-rater bias since the held-out dataset could not be rated by the same expert who rated the ABIDE dataset. This limitation could be reduced by calibrating the ratings of the held-out data having the second expert rate a random subsample of the training dataset. Second, the share of scanning vendors, models and corresponding images in the ABIDE dataset is not uniform. The use of a more uniform training dataset could potentially help generalize better to new datasets.</p>
<p>We used nested cross-validation to select the most predictive classifier, ensuring that the evaluation loop was unbiased using a leave-one-site-out (LoSo) splitting strategy. In this cross-validation scheme, the accuracy is bound below by that measured during the test validation loop. Therefore, the final classifier is ultimately trained using all the available data to push its predictive accuracy above the evaluated performance.</p>
<p>This quantitative assessment of quality is the central piece of the three-fold contribution of this paper. The first outcome of this study is the MRIQC toolbox, a set of open-source tools which compute quality features. Second, MRIQC generates interactive visual reports that allow further interpretation of the decisions made by the classifier. Finally we propose the automated quality control tool described before to generate include/exclude decisions. The MRIQC toolbox is a fork of the Quality Assessment Protocol (QAP). Since MRIQC was started as a standalone project, the implementation of most of the IQMs have been revised, and some are supported with unit tests. As QAP, MRIQC also implements a functional MRI (fMRI) workflow to extract IQMs and generate their corresponding visual reports. Some new IQMs have been added (for instance, the CJV, those features measuring the INU artifacts, or the rPVEs). The group and individual visual reports for structural and functional data are also new contributions to MRIQC with respect to the fork from QAP. The last diverging feature of MRIQC with respect to QAP is the cross-validation work and the release of the trained classifier.</p>
<p>MRIQC is one effort to standardize methodologies that make data-driven and objective QC decisions. Automated QC can provide unbiased exclusion criteria for neuroimaging studies, helping avoid &#x201C;cherry-picking&#x201D; of data. A second potential application is the use of automated QC predictions as data descriptors to support the recently born &#x201C;data papers&#x201D; track of many journals and public databases like OpenfMRI [<xref ref-type="bibr" rid="c31">31</xref>]. The ultimate goal of the proposed classifier is its inclusion in automatic QC workflows, before image processing and analysis. Ideally, minimizing the run time of MRIQC, the extraction and classification process could be streamlined in the acquisition, allowing for the immediate repetition of ruled out scans. Integrating MRIQC in our research workflow allowed us to adjust reconstruction methodologies, tweak the instructions given to the participant during scanning, and minimize the time required to visually assess one image with the visual reports.</p>
</sec>
<sec id="s5">
<title>Conclusion</title>
<p>We propose MRIQC, a quality control software tool to assess structural MRI of the human brain. MRIQC generates visual reports to speed the screening process, and a set of features which were used to train an automated decision tool. We trained a random forests classifier on the ABIDE dataset (N&#x003D;1102), acquired at 17 scanning sites with diverse acquisition parameters. We utilized repeated-and-nested cross-validation, with a leave-one-site-out splitting strategy. This avoided hidden feature relationships leaking from the site under test to the training set, ensuring that the evaluated performance was agnostic to site and ultimately represented well the generalization of performance to unseen data. The nested cross-validation evaluation yielded a &#x007E;89.4&#x0025; (<italic>&#x03C3;</italic>&#x003D;&#x00B1;9.95&#x0025;) accuracy. We double checked this generalization evaluating the performance of the classifier in a previously unseen dataset (N&#x003D;265) unrelated to ABIDE. The performance on the held-out dataset was &#x007E;73&#x0025; accuracy. This performance fell within the spread of the cross-validated evaluation. We release MRIQC open-source, along with the best performing classifier. The automatic QC of MRI scans, and the implementation of tools to assist the visual assessment of individual images are two tools in high demand for neuroimaging research.</p>
</sec>
</body>
<back>
<sec>
<title>Author contributions</title>
<p>OE lead the development of MRIQC, implemented the cross-validation workflow, pre-registered the report, drafted the manuscript, run the experiments and interpreted the results. KJG devised the machine learning approach to quality control, coordinated the project, contributed to MRIQC and the cross-validation workflow, pre-registered the report, and interpreted the results. MS rated the ABIDE dataset, helped understanding the problems of inter- and intra- rater variabilities. DB rated the ds030 dataset. OOK contributed in the design of the cross-validation workflow and interpreted the results. RAP devised and coordinated the project, advised in all aspects of MRIQC, the cross-validation workflow and the manuscript design, pre-registered the report and interpreted the results. All the authors have read and edited the manuscript.</p>
</sec>
<sec sec-type="availability">
<title>Availability of MRIQC and the trained classifier</title>
<p>MRIQC is available under the BSD 3-clause license. Source code is publicly accessible through GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/poldracklab/mriqc">https://github.com/poldracklab/mriqc</ext-link>). We provide four different installation options: 1) using the source code downloaded from the GitHub repository; 2) using the PyPi distribution system of Python; 3) using the poldracklab/mriqc Docker image; or 4) using BIDS-Apps [<xref ref-type="bibr" rid="c20">20</xref>]. For detailed information on installation and the user guide, please access <ext-link ext-link-type="uri" xlink:href="http://mriqc.rtfd.io">http://mriqc.rtfd.io</ext-link>. A distributable version of the classifier is also released, trained on all the available data (including the full-ABIDE and the ds030 datasets).</p>
</sec>
<ack>
<title>Acknowledgments</title>
<p>This work was supported by the Laura and John Arnold Foundation. The authors want to thank the QAP developers (C. Craddock, S. Giavasis, D. Clark, Z. Shezhad, and J. Pellman) for the initial base of code which MRIQC was forked from, W. Triplett and CA. Moodie for their initial contributions with bugfixes and documentation, and J. Varada for his contributions on th source code. JM. Shine and PG. Bissett reviewed the first draft of this manuscript, and helped debug early versions of MRIQC. S. Bhogawar, J. Durnez, I. Eisenberg and JB. Wexler routinely use and help debug the tool.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Kaufman</surname> <given-names>L</given-names></string-name>, <string-name><surname>Kramer</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Crooks</surname> <given-names>LE</given-names></string-name>, <string-name><surname>Ortendahl</surname> <given-names>DA</given-names></string-name>. <article-title>Measuring signal-to-noise ratios in MR imaging</article-title>. <source>Radiology</source>. <year>1989</year>;<volume>173</volume>(<issue>1</issue>):<fpage>265</fpage>&#x2013;<lpage>267</lpage>. doi: <pub-id pub-id-type="doi">10.1148/radiology.173.1.2781018</pub-id>.</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Gardner</surname> <given-names>EA</given-names></string-name>, <string-name><surname>Ellis</surname> <given-names>JH</given-names></string-name>, <string-name><surname>Hyde</surname> <given-names>RJ</given-names></string-name>, <string-name><surname>Aisen</surname> <given-names>AM</given-names></string-name>, <string-name><surname>Quint</surname> <given-names>DJ</given-names></string-name>, <string-name><surname>Carson</surname> <given-names>PL</given-names></string-name>. <article-title>Detection of degradation of magnetic resonance (MR) images: Comparison of an automated MR image-quality analysis system with trained human observers</article-title>. <source>Academic Radiology</source>. <year>1995</year>;<volume>2</volume>(<issue>4</issue>):<fpage>277</fpage>&#x2013;<lpage>281</lpage>. doi: <pub-id pub-id-type="doi">10.1016/S1076-6332(05)80184-9</pub-id>.</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Van Essen</surname> <given-names>DC</given-names></string-name>, <string-name><surname>Ugurbil</surname> <given-names>K</given-names></string-name>, <string-name><surname>Auerbach</surname> <given-names>E</given-names></string-name>, <string-name><surname>Barch</surname> <given-names>D</given-names></string-name>, <string-name><surname>Behrens</surname> <given-names>TEJ</given-names></string-name>, <string-name><surname>Bucholz</surname> <given-names>R</given-names></string-name>, <etal>et al.</etal> <article-title>The Human Connectome Project: A data acquisition perspective</article-title>. <source>Neuroimage</source>. <year>2012</year>;<volume>62</volume>(<issue>4</issue>):<fpage>2222</fpage>&#x2013;<lpage>2231</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.018</pub-id></mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Di Martino</surname> <given-names>A</given-names></string-name>, <string-name><surname>Yan</surname> <given-names>CG</given-names></string-name>, <string-name><surname>Li</surname> <given-names>Q</given-names></string-name>, <string-name><surname>Denio</surname> <given-names>E</given-names></string-name>, <string-name><surname>Castellanos</surname> <given-names>FX</given-names></string-name>, <string-name><surname>Alaerts</surname> <given-names>K</given-names></string-name>, <etal>et al.</etal> <article-title>The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism</article-title>. <source>Molecular Psychiatry</source>. <year>2014</year>;<volume>19</volume>(<issue>6</issue>):<fpage>659</fpage>&#x2013;<lpage>667</lpage>. doi: <pub-id pub-id-type="doi">10.1038/mp.2013.78</pub-id>.</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="book"><string-name><surname>Miller</surname> <given-names>KL</given-names></string-name>, <string-name><surname>Alfaro-Almagro</surname> <given-names>F</given-names></string-name>, <string-name><surname>Bangerter</surname> <given-names>NK</given-names></string-name>, <string-name><surname>Thomas</surname> <given-names>DL</given-names></string-name>, <string-name><surname>Yacoub</surname> <given-names>E</given-names></string-name>, <string-name><surname>Xu</surname> <given-names>J</given-names></string-name>, <etal>et al.</etal> <chapter-title>Multimodal population brain imaging in the UK Biobank prospective epidemiological study</chapter-title>. <source>Nature Neuroscience</source>. <year>2016</year>;<publisher-name>advance online publication</publisher-name>. doi: <pub-id pub-id-type="doi">10.1038/nn.4393</pub-id>.</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Price</surname> <given-names>RR</given-names></string-name>, <string-name><surname>Axel</surname> <given-names>L</given-names></string-name>, <string-name><surname>Morgan</surname> <given-names>T</given-names></string-name>, <string-name><surname>Newman</surname> <given-names>R</given-names></string-name>, <string-name><surname>Perman</surname> <given-names>W</given-names></string-name>, <string-name><surname>Schneiders</surname> <given-names>N</given-names></string-name>, <etal>et al.</etal> <article-title>Quality assurance methods and phantoms for magnetic resonance imaging: Report of AAPM nuclear magnetic resonance Task Group No. 1</article-title>. <source>Medical Physics</source>. <year>1990</year>;<volume>17</volume>(<issue>2</issue>):<fpage>287</fpage>&#x2013;<lpage>295</lpage>. doi: <pub-id pub-id-type="doi">10.11 18/1.596566</pub-id>.</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Woodard</surname> <given-names>JP</given-names></string-name>, <string-name><surname>Carley-Spencer</surname> <given-names>MP</given-names></string-name>. <article-title>No-Reference image quality metrics for structural MRI</article-title>. <source>Neuroinformatics</source>. <year>2006</year>;<volume>4</volume>(<issue>3</issue>):<fpage>243</fpage>&#x2013;<lpage>262</lpage>. doi: <pub-id pub-id-type="doi">10.1385/NI:4:3:243</pub-id>.</mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Mortamet</surname> <given-names>B</given-names></string-name>, <string-name><surname>Bernstein</surname> <given-names>MA</given-names></string-name>, <string-name><surname>Jack</surname> <given-names>CR</given-names></string-name>, <string-name><surname>Gunter</surname> <given-names>JL</given-names></string-name>, <string-name><surname>Ward</surname> <given-names>C</given-names></string-name>, <string-name><surname>Britson</surname> <given-names>PJ</given-names></string-name>, <etal>et al.</etal> <article-title>Automatic quality assessment in structural brain magnetic resonance imaging</article-title>. <source>Magnetic Resonance in Medicine</source>. <year>2009</year>;<volume>62</volume>(<issue>2</issue>):<fpage>365</fpage>&#x2013;<lpage>372</lpage>. doi: <pub-id pub-id-type="doi">10.1002/mrm.21992</pub-id>.</mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Pizarro</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Cheng</surname> <given-names>X</given-names></string-name>, <string-name><surname>Barnett</surname> <given-names>A</given-names></string-name>, <string-name><surname>Lemaitre</surname> <given-names>H</given-names></string-name>, <string-name><surname>Verchinski</surname> <given-names>BA</given-names></string-name>, <string-name><surname>Goldman</surname> <given-names>AL</given-names></string-name>, <etal>et al.</etal> <article-title>Automated Quality Assessment of Structural Magnetic Resonance Brain Images Based on a Supervised Machine Learning Algorithm</article-title>. <source>Frontiers in Neuroinformatics</source>. <year>2016</year>;<volume>10</volume>. doi: <pub-id pub-id-type="doi">10.3389/fninf.2016.00052</pub-id>.</mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Alfaro-Almagro</surname> <given-names>F</given-names></string-name>, <string-name><surname>Jenkinson</surname> <given-names>M</given-names></string-name>, <string-name><surname>Bangerter</surname> <given-names>N</given-names></string-name>, <string-name><surname>Andersson</surname> <given-names>J</given-names></string-name>, <string-name><surname>Griffanti</surname> <given-names>L</given-names></string-name>, <string-name><surname>Douaud</surname> <given-names>G</given-names></string-name>, <etal>et al.</etal> <article-title>UK Biobank Brain Imaging: Automated Processing Pipeline and Quality Control for 100,000 subjects</article-title>. In: <source>Organization for Human Brain Mapping. Geneve, Switzerland</source>; <year>2016</year>. p. <fpage>1877</fpage>. Available from: <ext-link ext-link-type="uri" xlink:href="https://ww5.aievolution.com/hbm1601/index.cfm?do&#x003D;abs.viewAbs&#x0026;abs&#x003D;3664">https://ww5.aievolution.com/hbm1601/index.cfm?do&#x003D;abs.viewAbs&#x0026;abs&#x003D;3664</ext-link>.</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="book"><string-name><surname>Ripley</surname> <given-names>BD</given-names></string-name>. <chapter-title>Pattern recognition and neural networks</chapter-title>. <edition>7th</edition> ed. <publisher-name>United Kingdom</publisher-name>: <publisher-loc>Cambridge University Press</publisher-loc>; <year>2007</year>.</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Poldrack</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Congdon</surname> <given-names>E</given-names></string-name>, <string-name><surname>Triplett</surname> <given-names>W</given-names></string-name>, <string-name><surname>Gorgolewski</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Karlsgodt</surname> <given-names>KH</given-names></string-name>, <string-name><surname>Mumford</surname> <given-names>JA</given-names></string-name>, <etal>et al.</etal> <article-title>A phenome-wide examination of neural and cognitive function</article-title>. <source>Scientific Data</source>. <year>2016</year>;<volume>3</volume>:<fpage>160110</fpage>. doi: <pub-id pub-id-type="doi">10.1038/sdata.2016.110</pub-id>.</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="journal"><string-name><surname>Fischl</surname> <given-names>B</given-names></string-name>. <article-title>FreeSurfer</article-title>. <source>Neuroimage</source>. <year>2012</year>;<volume>62</volume>(<issue>2</issue>):<fpage>774</fpage>&#x2013;<lpage>781</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id>.</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="journal"><string-name><surname>Backhausen</surname> <given-names>LL</given-names></string-name>, <string-name><surname>Herting</surname> <given-names>MM</given-names></string-name>, <string-name><surname>Buse</surname> <given-names>J</given-names></string-name>, <string-name><surname>Roessner</surname> <given-names>V</given-names></string-name>, <string-name><surname>Smolka</surname> <given-names>MN</given-names></string-name>, <string-name><surname>Vetter</surname> <given-names>NC</given-names></string-name>. <article-title>Quality Control of Structural MRI Images Applied Using FreeSurfer-A Hands-On Workflow to Rate Motion Artifacts</article-title>. <source>Frontiers in Neuroscience</source>. <year>2016</year>;<volume>10</volume>. doi: <pub-id pub-id-type="doi">10.3389/fnins.2016.00558</pub-id>.</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Gorgolewski</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Esteban</surname> <given-names>O</given-names></string-name>, <string-name><surname>Burns</surname> <given-names>C</given-names></string-name>, <string-name><surname>Ziegler</surname> <given-names>E</given-names></string-name>, <string-name><surname>Pinsard</surname> <given-names>B</given-names></string-name>, <string-name><surname>Madison</surname> <given-names>C</given-names></string-name>, <etal>et al.</etal> <article-title>Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in Python</article-title>. <source>Zenodo [Software]</source>. <year>2016</year>;doi: <pub-id pub-id-type="doi">10.5281/zenodo.50186</pub-id>.</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Jenkinson</surname> <given-names>M</given-names></string-name>, <string-name><surname>Beckmann</surname> <given-names>CF</given-names></string-name>, <string-name><surname>Behrens</surname> <given-names>TEJ</given-names></string-name>, <string-name><surname>Woolrich</surname> <given-names>MW</given-names></string-name>, <string-name><surname>Smith</surname> <given-names>SM</given-names></string-name>. <article-title>FSL</article-title>. <source>Neuroimage</source>. <year>2012</year>;<volume>62</volume>(<issue>2</issue>):<fpage>782</fpage>&#x2013;<lpage>790</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.09.015</pub-id></mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="website"><string-name><surname>Avants</surname> <given-names>B</given-names></string-name>, <string-name><surname>Duda</surname> <given-names>J</given-names></string-name>, <string-name><surname>Song</surname> <given-names>G</given-names></string-name>, <string-name><surname>Das</surname> <given-names>S</given-names></string-name>, <string-name><surname>Pluta</surname> <given-names>J</given-names></string-name>, <string-name><surname>Tustison</surname> <given-names>N</given-names></string-name>. <article-title>ANTs: Advanced Normalization Tools [software]</article-title>; <year>2013</year>. Available from: <ext-link ext-link-type="uri" xlink:href="http://www.picsl.upenn.edu/ANTS/">http://www.picsl.upenn.edu/ANTS/</ext-link>.</mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Cox</surname> <given-names>RW</given-names></string-name>, <string-name><surname>Hyde</surname> <given-names>JS</given-names></string-name>. <article-title>Software tools for analysis and visualization of fMRI data</article-title>. <source>NMR in Biomedicine</source>. <year>1997</year>;<volume>10</volume>(<issue>4&#x2013;5</issue>):<fpage>171</fpage>&#x2013;<lpage>178</lpage>. doi: <pub-id pub-id-type="doi">10.1002/(SICI)1099-1492(199706/08)10:4/5&#x003C;171::AID-NBM453&#x003E;3.0.CO;2-L</pub-id>.</mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Gorgolewski</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Auer</surname> <given-names>T</given-names></string-name>, <string-name><surname>Calhoun</surname> <given-names>VD</given-names></string-name>, <string-name><surname>Craddock</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Das</surname> <given-names>S</given-names></string-name>, <string-name><surname>Duff</surname> <given-names>EP</given-names></string-name>, <etal>et al.</etal> <article-title>The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments</article-title>. <source>Scientific Data</source>. <year>2016</year>;<volume>3</volume>:<fpage>160044</fpage>. doi: <pub-id pub-id-type="doi">10.1038/sdata.2016.44</pub-id>.</mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Gorgolewski</surname> <given-names>KJ</given-names></string-name>, <string-name><surname>Alfaro-Almagro</surname> <given-names>F</given-names></string-name>, <string-name><surname>Auer</surname> <given-names>T</given-names></string-name>, <string-name><surname>Bellec</surname> <given-names>P</given-names></string-name>, <string-name><surname>Capota</surname> <given-names>M</given-names></string-name>, <string-name><surname>Chakravarty</surname> <given-names>M</given-names></string-name>, <etal>et al.</etal> <article-title>BIDS Apps: Improving ease of use, accessibility and reproducibility of neuroimaging data analysis methods</article-title>. <source>bioRxiv</source>. <year>2016</year>; p. <fpage>079145</fpage>. doi: <pub-id pub-id-type="doi">10.1101/079145</pub-id>.</mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Ganzetti</surname> <given-names>M</given-names></string-name>, <string-name><surname>Wenderoth</surname> <given-names>N</given-names></string-name>, <string-name><surname>Mantini</surname> <given-names>D</given-names></string-name>. <article-title>Intensity Inhomogeneity Correction of Structural MR Images: A Data-Driven Approach to Define Input Algorithm Parameters</article-title>. <source>Frontiers in Neuroinformatics</source>. <year>2016</year>; p. <fpage>10</fpage>. doi: <pub-id pub-id-type="doi">10.3389/fninf.2016.00010</pub-id>.</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="journal"><string-name><surname>Magnotta</surname> <given-names>VA</given-names></string-name>, <string-name><surname>Friedman</surname> <given-names>L</given-names></string-name>, <string-name><surname>Birn</surname> <given-names>F</given-names></string-name>. <article-title>Measurement of Signal-to-Noise and Contrast-to-Noise in the fBIRN Multicenter Imaging Study</article-title>. <source>Journal of Digital Imaging</source>. <year>2006</year>;<volume>19</volume>(<issue>2</issue>):<fpage>140</fpage>&#x2013;<lpage>147</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s10278-006-0264-x</pub-id>.</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Dietrich</surname> <given-names>O</given-names></string-name>, <string-name><surname>Raya</surname> <given-names>JG</given-names></string-name>, <string-name><surname>Reeder</surname> <given-names>SB</given-names></string-name>, <string-name><surname>Reiser</surname> <given-names>MF</given-names></string-name>, <string-name><surname>Schoenberg</surname> <given-names>SO</given-names></string-name>. <article-title>Measurement of signal-to-noise ratios in MR images: Influence of multichannel coils, parallel imaging, and reconstruction filters</article-title>. <source>Journal of Magnetic Resonance Imaging</source>. <year>2007</year>;<volume>26</volume>(<issue>2</issue>):<fpage>375</fpage>&#x2013;<lpage>385</lpage>. doi: <pub-id pub-id-type="doi">10.1002/jmri.20969</pub-id>.</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Atkinson</surname> <given-names>D</given-names></string-name>, <string-name><surname>Hill</surname> <given-names>DLG</given-names></string-name>, <string-name><surname>Stoyle</surname> <given-names>PNR</given-names></string-name>, <string-name><surname>Summers</surname> <given-names>PE</given-names></string-name>, <string-name><surname>Keevil</surname> <given-names>SF</given-names></string-name>. <article-title>Automatic correction of motion artifacts in magnetic resonance images using an entropy focus criterion</article-title>. <source>IEEE Transactions on Medical Imaging</source>. <year>1997</year>;<volume>16</volume>(<issue>6</issue>):<fpage>903</fpage>&#x2013;<lpage>910</lpage>. doi: <pub-id pub-id-type="doi">10.1109/42.650886</pub-id>.</mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Fonov</surname> <given-names>V</given-names></string-name>, <string-name><surname>Evans</surname> <given-names>A</given-names></string-name>, <string-name><surname>McKinstry</surname> <given-names>R</given-names></string-name>, <string-name><surname>Almli</surname> <given-names>C</given-names></string-name>, <string-name><surname>Collins</surname> <given-names>D</given-names></string-name>. <article-title>Unbiased nonlinear average age-appropriate brain templates from birth to adulthood</article-title>. <source>Neuroimage</source>. <year>2009</year>;<volume>47</volume>, <issue>Supplement 1</issue>:<fpage>S102</fpage>. doi: <pub-id pub-id-type="doi">10.1016/S1053-8119(09)70884-5</pub-id>.</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Pedregosa</surname> <given-names>F</given-names></string-name>, <string-name><surname>Varoquaux</surname> <given-names>G</given-names></string-name>, <string-name><surname>Gramfort</surname> <given-names>A</given-names></string-name>, <string-name><surname>Michel</surname> <given-names>V</given-names></string-name>, <string-name><surname>Thirion</surname> <given-names>B</given-names></string-name>, <string-name><surname>Grisel</surname> <given-names>O</given-names></string-name>, <etal>et al.</etal> <article-title>Scikit-learn: Machine Learning in Python</article-title>. <source>Journal of Machine Learning Research</source>. <year>2011</year>;<volume>12</volume>:<fpage>2825</fpage>&#x2013;<lpage>2830</lpage>.</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Cortes</surname> <given-names>C</given-names></string-name>, <string-name><surname>Vapnik</surname> <given-names>V</given-names></string-name>. <article-title>Support-vector networks</article-title>. <source>Machine Learning</source>. <year>1995</year>;<volume>20</volume>(<issue>3</issue>):<fpage>273</fpage>&#x2013;<lpage>297</lpage>. doi: <pub-id pub-id-type="doi">10.1007/BF00994018</pub-id>.</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Breiman</surname> <given-names>L</given-names></string-name>. <article-title>Random Forests</article-title>. <source>Machine Learning</source>. <year>2001</year>;<volume>45</volume>(<issue>1</issue>):<fpage>5</fpage>&#x2013;<lpage>32</lpage>. doi: <pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>.</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Abraham</surname> <given-names>A</given-names></string-name>, <string-name><surname>Milham</surname> <given-names>MP</given-names></string-name>, <string-name><surname>Di Martino</surname> <given-names>A</given-names></string-name>, <string-name><surname>Craddock</surname> <given-names>RC</given-names></string-name>, <string-name><surname>Samaras</surname> <given-names>D</given-names></string-name>, <string-name><surname>Thirion</surname> <given-names>B</given-names></string-name>, <etal>et al.</etal> <article-title>Deriving reproducible biomarkers from multi-site resting-state data: An Autism-based example</article-title>. <source>Neuroimage</source>. <year>2017</year>;<volume>147</volume>:<fpage>736</fpage>&#x2013;<lpage>745</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.10.045</pub-id>.</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Gedamu</surname> <given-names>EL</given-names></string-name>, <string-name><surname>Collins</surname> <given-names>Dl</given-names></string-name>, <string-name><surname>Arnold</surname> <given-names>DL</given-names></string-name>. <article-title>Automated quality control of brain MR images</article-title>. <source>Journal of Magnetic Resonance Imaging</source>. <year>2008</year>;<volume>28</volume>(<issue>2</issue>):<fpage>308</fpage>&#x2013;<lpage>319</lpage>. doi: <pub-id pub-id-type="doi">10.1002/jmri.21434</pub-id>.</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Poldrack</surname> <given-names>RA</given-names></string-name>, <string-name><surname>Barch</surname> <given-names>DM</given-names></string-name>, <string-name><surname>Mitchell</surname> <given-names>J</given-names></string-name>, <string-name><surname>Wager</surname> <given-names>T</given-names></string-name>, <string-name><surname>Wagner</surname> <given-names>AD</given-names></string-name>, <string-name><surname>Devlin</surname> <given-names>JT</given-names></string-name>, <etal>et al.</etal> <article-title>Toward open sharing of task-based fMRI data: the OpenfMRI project</article-title>. <source>Frontiers in Neuroinformatics</source>. <year>2013</year>;<volume>7</volume>:<fpage>12</fpage>. doi: <pub-id pub-id-type="doi">10.3389/fninf.2013.00012</pub-id>.</mixed-citation></ref>
</ref-list>
<sec>
<title>Supporting Information</title>
<sec>
<title>Table SI1 Image acquisition parameters</title>
<p>A table containing all the acquisition parameters is maintained in GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/oesteban/mriqc/blob/c9bdfa863ca47894d5cdcb605071a5088840afcc/mriqc/data/csv/scan_parameters.tsv">https://github.com/oesteban/mriqc/blob/c9bdfa863ca47894d5cdcb605071a5088840afcc/mriqc/data/csv/scan_parameters.tsv</ext-link>.</p>
</sec>
<sec>
<title>Listing SI1 Running MRIQC</title>
<p>The BIDS standard makes MRIQC compatible with almost any input dataset without need for custom settings. Since all the metadata associated to the dataset are found in bids-data/, the following example would nicely run without further settings. The second positional argument, out/ indicates where the outputs will be written, and finally, the participant keyword instructs MRIQC to run the first level analysis as specified in BIDS Apps.
<boxed-text>
<p><monospace>mriqc bids-data/ out/ participant</monospace></p>
<p><monospace>mriqc bids-data/ out/ participant --participant_label S001 S002</monospace></p>
</boxed-text></p>
</sec>
<sec>
<title>Listing SI2 Running MRIQC &#x2013; Group Level</title>
<p>If the participant level was run setting some --participant_label, the group level is not triggered by default. It can be done manually, pointing the input data folder to the derivatives folder generated with the participant level analysis:
<boxed-text>
<p><monospace>mriqc out/derivatives/ out/ group</monospace></p>
</boxed-text></p>
</sec>
<sec>
<title>Listing SI2 Predicting quality</title>
<p>Although the group runlevel will generate a CSV table with the quality label predicted for each sample, it is possible to run the classifier individually:
<boxed-text>
<p><monospace>mriqc_clf --load - classifier -X aMRIQC.csv -o mypredictions.csv</monospace></p>
</boxed-text></p>
<p>The default classifier can be replaced by a custom one using:
<boxed-text>
<p><monospace>mriqc_clf --load - classifier my_custom_classifier. pk1z -X aMRIQC.csv -o mypredi ctions.csv</monospace></p>
</boxed-text></p>
<p>The documentation website contains more detailed information on how to train custom classifiers, or generate refined results from prediction: <ext-link ext-link-type="uri" xlink:href="http://mriqc.readthedocs.io/en/latest/classifier.html">http://mriqc.readthedocs.io/en/latest/classifier.html</ext-link>.</p>
</sec>
<sec>
<title>Figure SI1 Extended caption of Fig 1A</title>
<p>An example scan (top) is shown with severe motion artifacts. The reduced contrast between tissues and the ringing intensity waves in the anterior region of the brain in the presented slice suggest a large head movement occurred during acquisition. The green arrows point to signal spillover due to eye movements through the phase-encoding axis (in this case, right-to-left &#x2013;RL&#x2013;). Oftentimes, the RL or LR axes are selected for phase-encoding because the signal leakage from the eyeballs does not overlap with brain tissue, as opposed to selecting anterior-posterior directions. However, the red arrows point to signal spillover caused by vessel pulsations. Given the location of the vessel, in this case signal leakage overlaps brain tissue and affects the quality of this image. The phase-encoding axis has less bandwidth and thus, is more sensitive to movement. For that reason, it is generally selected to have the shortest field of view. A second example scan (bottom) shows severe coil artifacts.</p>
</sec>
</sec>
<fn-group>
<fn id="fn1">
<label>1</label>
<p>A measure is called &#x201C;no-reference&#x201D; when no ground-truth of the same image without degradation is available.</p>
</fn>
<fn id="fn2">
<label>2</label>
<p>Available online: <ext-link ext-link-type="uri" xlink:href="http://preprocessed-connectomes-project.org/quality-assessment-protocol/">http://preprocessed-connectomes-project.org/quality-assessment-protocol/</ext-link>.</p>
</fn>
<fn id="fn3">
<label>3</label>
<p>We distribute with MRIQC the script fs2gif which produces such animations. The animations used to evaluate the ds030 dataset are found here <ext-link ext-link-type="uri" xlink:href="https://drive.google.eom/drive/u/1/folders/0BxI12kyv2olZTDhiUVVMc2FyRDg">https://drive.google.eom/drive/u/1/folders/0BxI12kyv2olZTDhiUVVMc2FyRDg</ext-link>.</p>
</fn>
</fn-group>
</back>
</article>