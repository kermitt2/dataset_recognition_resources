<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/329193</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Maximum-entropy and representative samples of neuronal activity: a dilemma</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Porta Mana</surname>
<given-names>P.G.L.</given-names>
</name>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Rostami</surname>
<given-names>V.</given-names>
</name>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Torre</surname>
<given-names>E.</given-names>
</name>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Roudi</surname>
<given-names>Y.</given-names>
</name>
</contrib>
</contrib-group>
<author-notes>
<fn id="n1"><p><email>piero.mana@ntnu.no</email>, <email>vrostami@uni-koeln.de</email>, <email>torre@ibk.baug.ethz.ch</email>, <email>yasser.roudi@ntnu.no</email></p></fn>
</author-notes>
<pub-date pub-type="epub">
<year>2018</year>
</pub-date>
<elocation-id>329193</elocation-id>
<history>
<date date-type="received">
<day>23</day>
<month>5</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>23</day>
<month>5</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>23</day>
<month>5</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="329193.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>The present work shows that the maximum-entropy method can be applied to a sample of neuronal recordings along two different routes: (1) apply to the sample; or (2) apply to a larger, unsampled neuronal population from which the sample is drawn, and then marginalize to the sample. These two routes give inequivalent results. The second route can be further generalized to the case where the size of the larger population is unknown. Which route should be chosen? Some arguments are presented in favour of the second. This work also presents and discusses probability formulae that relate states of knowledge about a population and its samples, and that may be useful for sampling problems in neuroscience.</p>
</abstract>
<counts>
<page-count count="11"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<label>1</label>
<title>Introduction: maximum-entropy and recordings of neuronal activity</title>
<p>Suppose that we have recorded the firing activity of a hundred neurons, sampled from a particular brain area. What are we to do with such data? Gerstein, Perkel, Dayhoff [<xref rid="c1" ref-type="bibr">1</xref>] posed this question very tersely (our emphasis):
<disp-quote>
<p>The principal conceptual problems are (1) <italic>defining cooperativity or functional grouping</italic> among neurons and (2) <italic>formulating quantitative criteria</italic> for recognizing and characterizing such cooperativity.</p></disp-quote></p>
<p>These questions have a long history, of course; see for instance the 1966 review by Moore et al. [<xref rid="c2" ref-type="bibr">2</xref>]. The neuroscientific literature has offered several mathematical definitions of &#x2018;cooperativity&#x2019; or &#x2018;functional grouping&#x2019; and criteria to quantify it.</p>
<p>One such quantitative criterion relies on the maximum-entropy or relative-maximum-entropy method [<xref rid="c3" ref-type="bibr">3</xref>&#x2013;<xref rid="c7" ref-type="bibr">7</xref>]. This criterion has been used in neuroscience at least since the 1990s, applied to data recorded from brain areas as diverse as retina and motor cortex [<xref rid="c8" ref-type="bibr">8</xref>&#x2013;<xref rid="c24" ref-type="bibr">24</xref>], and it has been subjected to mathematical and conceptual scrutiny [<xref rid="c25" ref-type="bibr">25</xref>&#x2013;<xref rid="c31" ref-type="bibr">31</xref>].</p>
<p>&#x2018;Cooperativity&#x2019; can be quantified and characterized with maximum-entropy methods in several ways. The simplest way roughly proceeds along the following steps. Consider the recorded activity of a sample of <italic>n</italic> neurons.</p>
<list list-type="order">
<list-item><p>The activity of each neuron, a continuous signal, is divided into <italic>T</italic> time bins and binarized in intensity, and thus transformed into a sequence of digits &#x2018;0&#x2019;s (inactive) and &#x2018;1&#x2019;s [cf. 32; 33]. Let the variable <italic>s</italic> <sub><italic>i</italic></sub> (<italic>t</italic>) &#x2208; {0, 1} denote the activi ty of the <italic>i</italic>th sampled neuron at time bin <italic>t</italic>. Collectively denote the <italic>n</italic> activities with <bold>s</bold>(<italic>t</italic>) =<italic>s</italic> <sub><italic>1</italic></sub> (<italic>t</italic>), <italic>&#x2026;, s</italic> <sub><italic>n</italic></sub> (<italic>t</italic>). The population-averaged activity at that bin is <inline-formula><alternatives><inline-graphic xlink:href="329193_inline1.gif"/></alternatives></inline-formula> If we count the number of distinct pairs of active neurons at that bin we combinatorially find <inline-formula><alternatives><inline-graphic xlink:href="329193_inline2.gif"/></alternatives></inline-formula>There can be at most <inline-formula><alternatives><inline-graphic xlink:href="329193_inline3.gif"/></alternatives></inline-formula>simultaneously active pairs, so the population-averaged pair activity is <inline-formula><alternatives><inline-graphic xlink:href="329193_inline4.gif"/></alternatives></inline-formula>. With some combinatorics we see that the population-averaged activity of <italic>m</italic>-tuples of neurons is
<disp-formula id="eqn1">
<alternatives><graphic xlink:href="329193_eqn1.gif"/></alternatives>
</disp-formula></p>
<p>For brevity let us agree to simply call &#x2018;activity&#x2019; the, average <inline-formula><alternatives><inline-graphic xlink:href="329193_inline5.gif"/></alternatives></inline-formula>,&#x2018;pair-activity&#x2019; the average<inline-formula><alternatives><inline-graphic xlink:href="329193_inline6.gif"/></alternatives></inline-formula>, and so on.</p></list-item>
<list-item><p>Construct a sequence of relative-maximum-entropy distributions for the activity <inline-formula><alternatives><inline-graphic xlink:href="329193_inline5a.gif"/></alternatives></inline-formula>, using this sequence of constraints:</p>
<list list-type="bullet">
<list-item><p>the time averages of the activity and of the pair-activity <inline-formula><alternatives><inline-graphic xlink:href="329193_inline7.gif"/></alternatives></inline-formula>;</p></list-item>
<list-item><p>the time average of the activity <inline-formula><alternatives><inline-graphic xlink:href="329193_inline8.gif"/></alternatives></inline-formula>;</p></list-item>
<list-item><p>&#x2026;</p></list-item>
<list-item><p>the time averages of the activity, of the pair-activity, and so on, up to the <italic>k</italic>-activity.</p>
<p>Call the resulting distributions<inline-formula><alternatives><inline-graphic xlink:href="329193_inline9.gif"/></alternatives></inline-formula>. The time-bin dependence is now absent because these distributions can be interpreted as referring to any one of the time bins <italic>t</italic>, or to a new time bin (in the future or in the past) containing new data.</p>
<p>We also have the empirical frequency distribution of the total activity, <inline-formula><alternatives><inline-graphic xlink:href="329193_inline10.gif"/></alternatives></inline-formula>, counted from the time bins.</p></list-item>
</list></list-item>
<list-item><p>Now compare the distributions above with one another and with the frequency distribution, using some probability-space distance like the relative entropy or discrimination information [<xref rid="c34" ref-type="bibr">34</xref>; <xref rid="c4" ref-type="bibr">4</xref>; <xref rid="c35" ref-type="bibr">35</xref>; <xref rid="c5" ref-type="bibr">5</xref>]. If we find, say, that such distance is very high between <italic>p</italic> <sub><italic>1</italic></sub> and <italic>f</italic>, very low between <italic>p</italic> <sub><italic>2</italic></sub> and <italic>f</italic>, and is more or less the same between all <italic>p</italic> <sub><italic>m</italic></sub> and <italic>f</italic> for <italic>m &#x2A7E;</italic> 2, then we can say that there is a &#x2018;pairwise cooperativity&#x2019;, and that any higher-order cooperativity is just a reflection or consequence of the pairwise one. The reason is that the information from higher-order simultaneous activities did not lead to appreciable changes in the distribution obtained from pair activities.</p></list-item>
</list>
<p>The protocol above needs to be made precise by specifying various parameters, such as the width of the time bins or the probability distance used.</p>
<p>We hurry to say that the description just given is just <italic>one</italic> way to quantify and characterize cooperativity and functional grouping, not <italic>the only</italic> way. It can surely be criticized from many points of view. Yet, it is quantitative and bears a more precise meaning than an undefined, vague notion of &#x2018;cooperativity&#x2019;. Two persons who apply this procedure to the same data will obtain the same numbers. Different protocols can be based on the maximum-entropy method, for instance protocols that take into account the activities or pair activities of specific neurons rather than population averages, or even protocols that take into account time dependence.</p>
<p>The purpose of the present work is not to assess the merits of maximum-entropy methods with respect to other methods. Its main purpose is to show that there is a problem in the way the maximum-entropy method itself, as sketched above, is applied to the activity of the recorded neurons. We believe that this problem is at the root of some quirks about this method that were pointed out in the literature [<xref rid="c27" ref-type="bibr">27</xref>]. This problems extends also to more complex versions of the method, possibly except version that use &#x2018;hidden&#x2019; neurons [<xref rid="c36" ref-type="bibr">36</xref>&#x2013;<xref rid="c39" ref-type="bibr">39</xref>]. The problem is that the recorded neurons are a <italic>sample</italic> from a larger, unrecorded population, but the maximum-entropy method as applied above is treating them as isolated from the rest of the brain. Hence, the results it provides cannot rightfully be extrapolated. We will give a mathematical proof of this. Let us first analyse this issue in more detail.</p>
<p>Suppose that the neurons were recorded with electrodes covering an area of some square millimetres [cf. 40]. This recording is a sample of the activity of the neuronal population under the recording device; which can amount to tens of thousands of neurons [<xref rid="c41" ref-type="bibr">41</xref>]. We could even consider the recorded neurons as a sample of a brain area more extended than the recording device.</p>
<p>The characterization of the cooperativity of the recorded sample would have little meaning if we did not expect its results to generalize to a larger, unrecorded population &#x2013; at the very least that under there cording device. In other words, we expect that the conclusions drawn with the maximum-entropy methods about the sampled neurons should somehow extrapolate to unrecorded neurons in some larger area, from which the recorded neurons were sampled. In statistical terms we are assuming that the recorded neurons are a <italic>representative sample</italic> of some larger neuronal population. Probability theory tells us how to make inferences from a sample to the larger population from which it is sampled (see references below).</p>
<p>We can apply the maximum-entropy method to the sample, as described in the above protocol, to generate probability distributions for the activity of the sample. But, given that our sample is representative of a larger population, we can also apply the maximum-entropy method to the larger (unrecorded) population. The constraints are the same: the time averages of the sampled data, since they constitute representative data about the larger population as well. The method thus yields a probability distribution for the larger population, and the distribution for the sample is obtained by marginalization. The problem is that <italic>the distributions obtained from these two applications differ</italic>. Which choice is most meaningful?</p>
<p>In this work we develop the second way of applying the maximum-entropy method, at the level of the larger population, and show that its results differ from the application at the sample level. We also consider the case where the size of the larger population is unknown.</p>
<p>To apply the maximum-entropy method to the larger, unsampled population, it is necessary to use probability relations relevant to sampling [<xref rid="c42" ref-type="bibr">42</xref>; 43 parts I, VI; 44 ch. 8; 45 ch. 3]. The relations we present are well-known in survey sampling and in the pedagogic problem of drawing from an urn without replacement, yet they are somewhat hard to find explicitly written in the neuroscientific literature. We present and discuss them in the next section. A minor purpose of this paper is to make these relations more widely known, because they can be useful independently of maximum-entropy methods.</p>
<p>The notation and terminology in the present work follow ISO and ANSI standards [<xref rid="c46" ref-type="bibr">46</xref>&#x2013;<xref rid="c50" ref-type="bibr">50</xref>] but for the use of the comma &#x2018;,&#x2019; to denote logical conjunction. Probability notation follows Jaynes [<xref rid="c45" ref-type="bibr">45</xref>]. By &#x2018;probability&#x2019; we mean a degree of belief which &#x2018;would be agreed by all rational men if there were any rational men&#x2019; [<xref rid="c51" ref-type="bibr">51</xref>].</p>
</sec>
<sec id="s2">
<label>2</label>
<title>Probability relations between population and sample</title>
<p>We have already introduced the notation for the sample neurons. We introduce an analogous notation for the <italic>v</italic> neurons constituting the larger population, but using the corresponding Greek letters: <italic>&#x03C3;</italic><sub><italic>l</italic></sub> (<italic>t</italic>) is the activity of the <italic>v</italic>th neuron at time bin <italic>t</italic>, <inline-formula><alternatives><inline-graphic xlink:href="329193_inline11.gif"/></alternatives></inline-formula> is the activity at that bin averaged over the larger population, and so on.</p>
<p>The probability relations between sample and larger population are valid at every time bin. As we mentioned above, the maximum-entropy distribution refers to any time bin or to a new bin. For these reasons we will now omit the time-bin argument &#x2018;(t)&#x2019; from our expressions.</p>
<p>Probabilities refer to statements about the quantities we observe. We use the standard notation:
<disp-formula id="eqn2">
<alternatives><graphic xlink:href="329193_eqn2.gif"/></alternatives>
</disp-formula>
and similarly for other quantities.</p>
<p>If <italic>K</italic> denotes our state of knowledge &#x2013; the evidence and assumptions backing our probability assignments &#x2013; our uncertainty about the full activity of the larger population is expressed by the joint probability distribution
<disp-formula id="eqn3">
<alternatives><graphic xlink:href="329193_eqn3.gif"/></alternatives>
</disp-formula></p>
<p>Our uncertainty about the state of the sample is likewise expressed by
<disp-formula id="eqn4">
<alternatives><graphic xlink:href="329193_eqn4.gif"/></alternatives>
</disp-formula></p>
<p>The theory of statistical sampling is covered in many excellent texts, for example Ghosh &#x0026; Meeden[<xref rid="c42" ref-type="bibr">42</xref>] or Freedman, Pisani, &#x0026; Purves [43 parts I, VI]; summaries can be found in Gelman et al. [44 ch. 8] and Jaynes [45 ch. 3].</p>
<p>We need to make an initial probability assignment for the state of the full population before any experimental observations are made. This initial assignment will be modified by our experimental observations, and these can involve just a sample of the population. Our state of knowledge and initial probability assignment should reflect that samples are somehow representative of the whole population.</p>
<p>In this state of knowledge, denoted <italic>I</italic>, we know that the neurons in the population are biologically or functionally similar, for example in morphology or the kind of input or output they receive or give. But we are completely ignorant about the physical details of the individual neurons. Our ignorance is therefore symmetric under permutations of neuron identities. This ignorance is represented by a probability distribution that is symmetric under permutations of neuron identities; such a distribution is usually called <italic>finitely exchangeable</italic> [52; 42 ch. 1]. We stress that this probability assignment is just an expression of the symmetry of our <italic>ignorance</italic> about the state of the population, not an expression of some biologic or physical symmetry or identity of the neurons.</p>
<p>The <italic>representation theorem for finite exchangeability</italic> states that, in the state of knowledge <italic>I</italic>, the symmetric distribution for the full activity is completely determined by the distribution for its population-average:
<disp-formula id="eqn5">
<alternatives><graphic xlink:href="329193_eqn5.gif"/></alternatives>
</disp-formula></p>
<p>The equivalence on the left is just an application of the law of total probability; the equality on the right is the statement of the theorem. This result is intuitive: owing to symmetry, we must assign equal probabilities to all <inline-formula><alternatives><inline-graphic xlink:href="329193_inline12.gif"/></alternatives></inline-formula>activity vectors with <inline-formula><alternatives><inline-graphic xlink:href="329193_inline13.gif"/></alternatives></inline-formula>active neurons; the probability of each activity vector is therefore given by that of the average activity divided by the number of possible vector values. Proof of this theorem and generalizations to non-binary and continuum cases are given by de Finetti [<xref rid="c53" ref-type="bibr">53</xref>], Kendall [<xref rid="c54" ref-type="bibr">54</xref>], Ericson [<xref rid="c55" ref-type="bibr">55</xref>], Diaconis &#x0026; Freedman [<xref rid="c56" ref-type="bibr">56</xref>; <xref rid="c57" ref-type="bibr">57</xref>], Heath &#x0026; Sudderth [<xref rid="c58" ref-type="bibr">58</xref>].</p>
<p>Our uncertainties about the full population and the sample are connected via the conditional probability
<disp-formula id="eqn6">
<alternatives><graphic xlink:href="329193_eqn6.gif"/></alternatives>
</disp-formula>
which is a hypergeometric distribution, typical of &#x2018;drawing without replacement&#x2019; problems. The combinatorial proof of this expression is in fact the same as for this class of problems [45 ch. 3; 59 &#x00A7; 4.8.3; 60 &#x00A7; II.6].</p>
<p>Using the conditional probability above we obtain the probability for the activity of the sample:
<disp-formula id="eqn7">
<alternatives><graphic xlink:href="329193_eqn7.gif"/></alternatives>
</disp-formula></p>
<p>It should be proved that the probability distribution for the full activity of the sample is also symmetric and completely determined by the distribution of its population-averaged activity:
<disp-formula id="eqn8">
<alternatives><graphic xlink:href="329193_eqn8.gif"/></alternatives>
</disp-formula></p>
<p>This is intuitively clear: our initial symmetric ignorance should also apply to the sample. The distribution for the sample (7) indeed satisfies the same representation theorem (5) as the distribution for the full population.</p>
<p>The conditional probability <inline-formula><alternatives><inline-graphic xlink:href="329193_inline14.gif"/></alternatives></inline-formula>, besides relating the distributions for the population and sample activities via marginalization, also allows us to express the expectation value of any function of the sample activity, <inline-formula><alternatives><inline-graphic xlink:href="329193_inline15.gif"/></alternatives></inline-formula>, in terms of the distribution for the full population, as follows:
<disp-formula id="eqn9">
<alternatives><graphic xlink:href="329193_eqn9.gif"/></alternatives>
</disp-formula>
where the second step uses eq. (7). The last expression shows that the expectation of the function <inline-formula><alternatives><inline-graphic xlink:href="329193_inline16.gif"/></alternatives></inline-formula> is equal to the expectation of the function<inline-formula><alternatives><inline-graphic xlink:href="329193_inline17.gif"/></alternatives></inline-formula>.</p>
<p>The final expression in eq. (9) is important for our maximum-entropy application: the requirement that the function g, defined for the sample, have a value <italic>c</italic> obtained from observed data, <italic>translates into a linear constraint for the distribution of the full population</italic>:
<disp-formula id="eqn10">
<alternatives><graphic xlink:href="329193_eqn10.gif"/></alternatives>
</disp-formula></p>
<p>In particular, when the function g is the <italic>m</italic>-activity of the sample, <inline-formula><alternatives><inline-graphic xlink:href="329193_inline18.gif"/></alternatives></inline-formula>we find
<disp-formula id="eqn11">
<alternatives><graphic xlink:href="329193_eqn11.gif"/></alternatives>
</disp-formula>
that is, <italic>the expected values of the m-activities of the sample and of the full population are equal</italic>. The proof of the middle equality uses the expression for the <italic>m</italic>th factorial moment of the hypergeometric distribution and can be found in Potts [<xref rid="c61" ref-type="bibr">61</xref>]. Similar relations can be found for the raw moments <inline-formula><alternatives><inline-graphic xlink:href="329193_inline19.gif"/></alternatives></inline-formula>and<inline-formula><alternatives><inline-graphic xlink:href="329193_inline20.gif"/></alternatives></inline-formula>, which can be written in terms of the product expectations using eq. (1).</p>
<p>Thus, in a maximum-entropy application, when we require the expectation of the <italic>m</italic>-activity of a sample to have a particular value, we are also requiring the expectation of the <italic>m</italic>-activity of the full population to have the same value.</p>
<p>These expectation equalities between sample and full population should not be surprising: we intuitively <italic>expect</italic> that the proportion of coloured balls sampled from an urn should be roughly equal to the proportion of coloured ball contained in the urn. The formulae in the present section formalize and mathematically express our intuition. The hypergeometric distribution <italic>v</italic> <inline-formula><alternatives><inline-graphic xlink:href="329193_inline21.gif"/></alternatives></inline-formula> plays an important role in this formalization. A look at its plot, <xref rid="fig1" ref-type="fig">fig. 1</xref>, reveals that it is a sort of &#x2018;fuzzy identity transformation&#x2019;, or fuzzy Kronecker delta, between the <inline-formula><alternatives><inline-graphic xlink:href="329193_inline22.gif"/></alternatives></inline-formula>-space {0, <italic>&#x2026;, v</italic>} and <inline-formula><alternatives><inline-graphic xlink:href="329193_inline22a.gif"/></alternatives></inline-formula>-space{0, <italic>&#x2026;, n</italic>}. From eq. (8) we thus have that
<disp-formula id="eqn12">
<alternatives><graphic xlink:href="329193_eqn12.gif"/></alternatives>
</disp-formula>
where g is any smooth function defined on [0, 1]. These approximate equalities express the intuitive fact that <italic>our uncertainty about the sample is representative of our uncertainty about the population</italic></p>
<fig id="fig1" position="float" fig-type="figure">
<label>Figure 1:</label>
<caption><title>Log-density plot of the hypergeometric distribution <inline-formula><alternatives><inline-graphic xlink:href="329193_inline23.gif"/></alternatives></inline-formula> for <italic>v</italic>=5000,<italic>n</italic>=200 (Band artifacts may appear in the colourbar depending on your PDF viewer.) <italic>and about other samples</italic>, and vice versa. When <inline-formula><alternatives><inline-graphic xlink:href="329193_inline24.gif"/></alternatives></inline-formula> becomes the identity matrix and the approximate equalities above become exact &#x2013;of course, since we have sampled the full population.</title></caption>
<graphic xlink:href="329193_fig1.tif"/>
</fig>
<p>But the approximate equalities above may miss important features of the two probability distributions. In the next section we will in fact emphasize their differences. If the distribution for the population average <inline-formula><alternatives><inline-graphic xlink:href="329193_inline25.gif"/></alternatives></inline-formula> is bimodal, for example, the bimodality can be lost in the distribution for the sample average <inline-formula><alternatives><inline-graphic xlink:href="329193_inline26.gif"/></alternatives></inline-formula>, owing to the coarsening effect of <inline-formula><alternatives><inline-graphic xlink:href="329193_inline27.gif"/></alternatives></inline-formula>.</p>
</sec>
<sec id="s3">
<label>3</label>
<title>Maximum-entropy: sample level vs full-population level</title>
<p>In the previous section we have seen that observations about a sample can be used as constraints on the distribution for the activity of the full population. Let us use such constraints with the maximum-entropy method. Suppose that we want to constrain <italic>m</italic> functions of the sample activity, vectorially written <bold>g</bold> = (g <sub>1</sub>, <italic>&#x2026;,</italic> g <sub><italic>m</italic></sub>), to <italic>m</italic> values <bold>c</bold> = (<italic>c</italic> <sub><italic>1</italic></sub>, <italic>&#x2026;, c</italic> <sub><italic>m</italic></sub>). These functions are typically <italic>k</italic>-activities<inline-formula><alternatives><inline-graphic xlink:href="329193_inline28.gif"/></alternatives></inline-formula>, and the values are typically the time averages of the observed sample, as discussed in &#x00A7; 1: <inline-formula><alternatives><inline-graphic xlink:href="329193_inline29.gif"/></alternatives></inline-formula>.</p>
<p>Let us apply the relative-maximum-entropy method [<xref rid="c6" ref-type="bibr">6</xref>; <xref rid="c7" ref-type="bibr">7</xref>] directly to sampled neurons; denote this approach by <italic>I</italic> <sub><italic>s</italic></sub>. Then we apply the method to the full population of neurons, most of which are unsampled; denote this approach by <italic>I</italic> <sub><italic>p</italic></sub>.</p>
<p>Applied directly to the sampled neurons, the method yields the distribution
<disp-formula id="eqn13">
<alternatives><graphic xlink:href="329193_eqn13.gif"/></alternatives>
</disp-formula>
where <italic>&#x1D4B5;</italic> (<bold><italic>l</italic></bold>) is a normalization constant. The binomial in front of the exponential appears because we must account for the multiplicity by which the population-average activity <inline-formula><alternatives><inline-graphic xlink:href="329193_inline30.gif"/></alternatives></inline-formula> can be realized: <inline-formula><alternatives><inline-graphic xlink:href="329193_inline31.gif"/></alternatives></inline-formula> can be realized in only one way (all neurons inactive), <inline-formula><alternatives><inline-graphic xlink:href="329193_inline32.gif"/></alternatives></inline-formula> can be realized in <italic>n</italic> ways (one active neuron out of <italic>n</italic>), and so on. This term is analogous to the &#x2018;density of states&#x2019; in front of the Boltzmann factor in statistical mechanics [62 ch. 16]. The <italic>m</italic> Lagrange multipliers <bold><italic>l</italic></bold> = (<italic>l</italic> <sub><italic>1</italic></sub>, <italic>&#x2026;, l</italic> <sub><italic>m</italic></sub>) must satisfy the <italic>m</italic> constraint equations
<disp-formula id="eqn14">
<alternatives><graphic xlink:href="329193_eqn14.gif"/></alternatives>
</disp-formula></p>
<p>Applied to the full population, using the constraint expression (10) derived in the previous section, the method yields the distribution for the full-population activity
<disp-formula id="eqn15">
<alternatives><graphic xlink:href="329193_eqn15.gif"/></alternatives>
</disp-formula></p>
<p>The <italic>m</italic> Lagrange multipliers <italic>&#x03BB;</italic> = (<italic>&#x03BB;</italic> <sub>1</sub>, <italic>&#x2026;, &#x03BB;</italic> <sub><italic>m</italic></sub>) must satisfy the <italic>m</italic> constraint equations
<disp-formula id="eqn16">
<alternatives><graphic xlink:href="329193_eqn16.gif"/></alternatives>
</disp-formula></p>
<p>We obtain the distribution for the sample activity by marginalization, using eq. (8):
<disp-formula id="eqn17">
<alternatives><graphic xlink:href="329193_eqn17.gif"/></alternatives>
</disp-formula></p>
<p>The distributions for the sample activity, eqs (17) and (13), obtained with the two approaches <italic>I</italic> <sub><italic>s</italic></sub> and <italic>I</italic> <sub><italic>p</italic></sub>, are different. From the discussion in the previous section we expect them to be vaguely similar; yet they cannot be exactly equal, because their equality would require the 2<italic>m</italic> quantities <italic>&#x03BB;</italic> and <bold>l</bold> to satisfy the constraint equations (16) and (14), and in addition also the <italic>n</italic> equations <inline-formula><alternatives><inline-graphic xlink:href="329193_inline33.gif"/></alternatives></inline-formula> (one equation is taken care of by the normalization of the distributions). We would have a set of 2<italic>m</italic> &#x002B; <italic>n</italic> equations in 2<italic>m</italic> unknowns.</p>
<p>Hence, <italic>the applications of maximum-entropy at the sample level and at the full-population level are inequivalent</italic>. They lead to numerically different distributions for the sample activity <bold><italic>s</italic></bold>.</p>
<p>The distribution obtained at the sample level will show different features from the one obtained at the population level, like displaced or additional modes or particular tail behaviour. We show an example of this discrepancy in <xref rid="fig2" ref-type="fig">fig. 2</xref>, for <italic>v</italic> = 10 000, <italic>n</italic> = 200, and the two constraints
<disp-formula id="eqn18">
<alternatives><graphic xlink:href="329193_eqn18.gif"/></alternatives>
</disp-formula>
which come from the actual recording of circa 200 neurons from macaque motor cortex [<xref rid="c31" ref-type="bibr">31</xref>]. The distribution obtained at the population level (blue triangles) has a higher and displaced mode and a quite different behaviour for activities around 0.5 than the distribution obtained at the sample level (red squares).</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Figure 2:</label>
<caption><title>Linear and log-plots of <inline-formula><alternatives><inline-graphic xlink:href="329193_inline34.gif"/></alternatives></inline-formula>for a sample of <italic>n</italic> = 200 and constraints as in eq. (18), constructed by: <bold>red squares:</bold> maximum-entropy at the sample level, eq. (13); <bold>blue triangles:</bold> maximum-entropy at the population level, eq. (17) with <italic>v</italic> = 10 000, followed by sample marginalization; <bold>yellow circles:</bold> maximum-entropy at the population level with unknown population size, eq. (19), according to the distribution (20) for the population.</title></caption>
<graphic xlink:href="329193_fig2.tif"/>
</fig>
<p>In our discussion we have so far assumed the size <italic>&#x03BD;</italic> of the larger population to be known. This is rarely the case, however. We usually are uncertain about <italic>&#x03BD;</italic> and can only guess its order of magnitude. In such a state of knowledge <italic>I</italic> <sub><italic>u</italic></sub> our ignorance about the possible value of <italic>v</italic> is expressed by a probability distribution P(<italic>N</italic> = <italic>&#x03BD;&#x007C;I</italic> <sub><italic>v</italic></sub>) = <italic>h</italic>(<italic>&#x03BD;</italic>), and the marginal distribution for the sample activity (17) is modified, by the law of total probability, to
<disp-formula id="eqn19">
<alternatives><graphic xlink:href="329193_eqn19.gif"/></alternatives>
</disp-formula>
where the Lagrange multipliers <italic>&#x03BB;</italic> <sub><italic>&#x03BD;</italic></sub> and the summation range for <inline-formula><alternatives><inline-graphic xlink:href="329193_inline35.gif"/></alternatives></inline-formula>depend on <italic>&#x03BD;</italic>.</p>
<p>As a proof of concept, <xref rid="fig2" ref-type="fig">fig. 2</xref> also shows such a distribution (yellow circles) for the same constraints as above, and a probability distribution for <italic>v</italic> inspired by Jeffreys [63 &#x00A7; 4.8]:
<disp-formula id="eqn20">
<alternatives><graphic xlink:href="329193_eqn20.gif"/></alternatives>
</disp-formula></p>
</sec>
<sec id="s4">
<label>4</label>
<title>Discussion</title>
<p>The purpose of the present work was to point out and show, in a simple set-up, that the maximum-entropy method can be applied to recorded neuronal data in a way that accounts for the larger population from which the data are sampled, eqs (15)&#x2013;(17). This application leads to results that differ from the standard application which only considers the sample in isolation, eqs (13)&#x2013;(14). We gave a numerical example of this difference. We have also shown how to extend the new application when the size of the larger population is unknown, eq. (19).</p>
<p>The latter formula, in particular, shows that the standard way of applying maximum-entropy implicitly assumes that <italic>no</italic> larger population exists beyond the recorded sample of neurons. One could in fact object to the application at the population level, and say that the traditional way of applying maximum-entropy, eq. (13), yields different results because it does not make assumptions about the size <italic>v</italic> of a possibly existing larger population. Such a state of uncertainty, however, is correctly formalized according to the laws of probability by introducing a probability distribution for <italic>&#x03BD;</italic>, and is expressed by eq. (19). This expression cannot generally be equal to (13) unless the distribution for <italic>&#x03BD;</italic> gives unit probability to <italic>&#x03BD;</italic> = <italic>n</italic>; that is, unless the sample <italic>is</italic> the full population, and no larger population exists.</p>
<p>The standard maximum-entropy approach therefore assumes that the recorded neurons constitute a special subnetwork, isolated from the larger network of neurons in which it is embedded, and which was also present under the recording device. This assumption is unrealistic. The maximum-entropy approach at the population level does not make such assumption and is therefore preferable. It may reveal features in a data set that were unnoticed by the standard maximum-entropy approach.</p>
<p>The difference in the resulting distributions between the applications at the sample and at the population levels appears in the use of Boltzmann machines with hidden units [<xref rid="c64" ref-type="bibr">64</xref>], although by a different conceptual route. It also appears in statistical mechanics: if a system is statistically described by a maximum-entropy Gibbs state, its subsystems cannot be described by a Gibbs state [<xref rid="c65" ref-type="bibr">65</xref>]. A somewhat similar situation also appears in the statistical description of the final state of a non-equilibrium process starting and ending in two equilibrium states: we can describe our knowledge about the final state either by (1) a Gibbs distribution, calculated from the final equilibrium macrovariables, or (2) by the distribution obtained from the Liouville evolution of the Gibbs distribution assigned to the initial state. The two distributions differ (even though the final <italic>physical</italic> state is obviously exactly the same [66 &#x00A7; 4]), and the second allows us to make sharper predictions about the final physical state thanks to our knowledge of its preceding dynamics. In this example, though, both distributions are usually extremely sharp and practically lead to the same predictions. In neuroscientific applications, the difference in predictions of the sample vs full-population applications can instead be very relevant.</p>
<p>The idea of the new application leads in fact to more questions. For instance:</p>
<list list-type="bullet">
<list-item><p>Do the standard and new applications lead to different or contrasting conclusions about &#x2018;cooperativity&#x2019;, when applied to real data sets?</p></list-item>
<list-item><p>How to extend the new application to the &#x2018;inhomogeneous&#x2019; case [<xref rid="c12" ref-type="bibr">12</xref>; <xref rid="c13" ref-type="bibr">13</xref>; <xref rid="c27" ref-type="bibr">27</xref>], in which expectations for individual neurons or groups of neurons are constrained?</p></list-item>
<list-item><p>What is the mathematical relation between the new application and maximum-entropy models with hidden neurons [<xref rid="c36" ref-type="bibr">36</xref>&#x2013;<xref rid="c39" ref-type="bibr">39</xref>]?</p></list-item>
</list>
<p>Owing to space limitations we must leave a thorough investigation of these questions to future work.</p>
<p>Finally, we would like to point out the usefulness and importance of the probability formulae that relate our states of knowledge about a population and its samples, presented in &#x00A7; 2. This kind of formulae is essential in neuroscience, where we try to understand properties of extended brain regions from partial observations. The formulae presented here reflect a simple, symmetric state of ignorance. More work is needed [cf. 67] to extend these formulae to account for finer knowledge of the cerebral cortex and its network properties.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>PGLPM thanks Mari &#x0026; Miri for continuous encouragement and affection; Buster Keaton for filling life with awe and inspiration; the developers and maintainers of LATEX, Emacs, AUCTEX, Open Science Framework, Python, Inkscape, Sci-Hub for making a free and unfiltered scientific exchange possible.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="c1"><label>[1]</label><mixed-citation publication-type="journal"><string-name><given-names>G. L.</given-names> <surname>Gerstein</surname></string-name>, <string-name><given-names>D. H.</given-names> <surname>Perkel</surname></string-name>, <string-name><given-names>J. E.</given-names> <surname>Dayhoff</surname></string-name>: <article-title>Cooperative firing activity in simultaneously recorded populations of neurons: detection and measurement</article-title>. <source>J. Neurosci</source>. <volume>5</volume><issue>4</issue> (<year>1985</year>), <fpage>881</fpage>&#x2013;<lpage>889</lpage>.</mixed-citation></ref>
<ref id="c2"><label>[2]</label><mixed-citation publication-type="journal"><string-name><given-names>G. P.</given-names> <surname>Moore</surname></string-name>, <string-name><given-names>D. H.</given-names> <surname>Perkel</surname></string-name>, <string-name><given-names>J. P.</given-names> <surname>Segundo</surname></string-name>: <article-title>Statistical analysis and functional interpretation of neuronal spike data</article-title>. <source>Annu. Rev. Physiol</source>. <volume>28</volume> (<year>1966</year>), <fpage>493</fpage>&#x2013;<lpage>522</lpage>.</mixed-citation></ref>
<ref id="c3"><label>[3]</label><mixed-citation publication-type="website"><string-name><given-names>E. T.</given-names> <surname>Jaynes</surname></string-name>: <article-title>Information theory and statistical mechanics</article-title>. <source>Phys. Rev</source>. <volume>106</volume><issue>4</issue> (<year>1957</year>), <fpage>620</fpage>&#x2013;<lpage>630</lpage>. <ext-link ext-link-type="uri" xlink:href="http://bayes.wustl.edu/etj/node1.html">http://bayes.wustl.edu/etj/node1.html</ext-link>, see also ref. [68].</mixed-citation></ref>
<ref id="c4"><label>[4]</label><mixed-citation publication-type="website"><string-name><given-names>E. T.</given-names> <surname>Jaynes</surname></string-name>: <article-title>Information theory and statistical mechanics</article-title>. <source>In: Ford [69]</source> (<year>1963</year>), <fpage>181</fpage>&#x2013;<lpage>218</lpage>. Repr. in ref. [70], ch. 4, 39&#x2013;76. <ext-link ext-link-type="uri" xlink:href="http://bayes.wustl.edu/etj/node1.html">http://bayes.wustl.edu/etj/node1.html</ext-link>.</mixed-citation></ref>
<ref id="c5"><label>[5]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Hobson</surname></string-name>, <string-name><given-names>B.-K.</given-names> <surname>Cheng</surname></string-name>: <article-title>A comparison of the Shannon and Kullback information measures</article-title>. <source>J. Stat. Phys</source>. <volume>7</volume><issue>4</issue> (<year>1973</year>), <fpage>301</fpage>&#x2013;<lpage>310</lpage>.</mixed-citation></ref>
<ref id="c6"><label>[6]</label><mixed-citation publication-type="book"><string-name><given-names>D. S.</given-names> <surname>Sivia</surname></string-name>: <source>Data Analysis: A Bayesian Tutorial</source>, <edition>2nd ed</edition>. <publisher-name>Oxford University Press</publisher-name>, <publisher-loc>Oxford</publisher-loc> (<year>2006</year>). <comment>Written with J. Skilling. First publ. 1996</comment>.</mixed-citation></ref>
<ref id="c7"><label>[7]</label><mixed-citation publication-type="website"><string-name><given-names>L. R.</given-names> <surname>Mead</surname></string-name>, <string-name><given-names>N.</given-names> <surname>Papanicolaou</surname></string-name>: <article-title>Maximum entropy in the problem of moments</article-title>. <source>J. Math. Phys</source>. <volume>25</volume><issue>8</issue> (<year>1984</year>), <fpage>2404</fpage>&#x2013;<lpage>2417</lpage>. <ext-link ext-link-type="uri" xlink:href="http://bayes.wustl.edu/Manual/MeadPapanicolaou.pdf">http://bayes.wustl.edu/Manual/MeadPapanicolaou.pdf</ext-link>.</mixed-citation></ref>
<ref id="c8"><label>[8]</label><mixed-citation publication-type="journal"><string-name><given-names>D. J. C.</given-names> <surname>MacKay</surname></string-name>: <article-title>Maximum entropy connections: neural networks</article-title>. <source>In: Grandy, Schick [</source><volume>71]</volume> (<year>1991</year>), <fpage>237</fpage>&#x2013;<lpage>244</lpage>.</mixed-citation></ref>
<ref id="c9"><label>[9]</label><mixed-citation publication-type="journal"><string-name><given-names>L.</given-names> <surname>Martignon</surname></string-name>, <string-name><given-names>H. V.</given-names> <surname>Hassein</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Gr&#x00FC;n</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Aertsen</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Palm</surname></string-name>: <article-title>Detecting higher-order interactions among the spiking events in a group of neurons</article-title>. <source>Biol. Cybern</source>. <volume>73</volume><issue>1</issue> (<year>1995</year>), <fpage>69</fpage>&#x2013;<lpage>81</lpage>.</mixed-citation></ref>
<ref id="c10"><label>[10]</label><mixed-citation publication-type="journal"><string-name><given-names>S. M.</given-names> <surname>Bohte</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Spekreijse</surname></string-name>, <string-name><given-names>P. R.</given-names> <surname>Roelfsema</surname></string-name>: <article-title>The effects of pair-wise and higher-order correlations on the firing rate of a postsynaptic neuron</article-title>. <source>Neural Comp</source>. <volume>12</volume><issue>1</issue> (<year>2000</year>), <fpage>153</fpage>&#x2013;<lpage>179</lpage>.</mixed-citation></ref>
<ref id="c11"><label>[11]</label><mixed-citation publication-type="journal"><string-name><given-names>S.-i.</given-names> <surname>Amari</surname></string-name>, <string-name><given-names>H.</given-names> <surname>Nakahara</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Wu</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Sakai</surname></string-name>: <article-title>Synchronous firing and higher-order interactions in neuron pool</article-title>. <source>Neural Comp</source>. <volume>15</volume><issue>1</issue> (<year>2003</year>), <fpage>127</fpage>&#x2013;<lpage>142</lpage>.</mixed-citation></ref>
<ref id="c12"><label>[12]</label><mixed-citation publication-type="website"><string-name><given-names>E.</given-names> <surname>Schneidman</surname></string-name>, <string-name><given-names>M. J.</given-names> <surname>Berry</surname></string-name> <edition>II</edition>, <string-name><given-names>R.</given-names> <surname>Segev</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Bialek</surname></string-name>: <article-title>Weak pairwise correlations imply strongly correlated network states in a neural population</article-title>. <source>Nature</source> <volume>440</volume><issue>7087</issue> (<year>2006</year>), <fpage>1007</fpage>&#x2013;<lpage>1012</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.weizmann.ac.il/neurobiology/labs/schneidman/The&#x005F;Schneidman&#x005F;Lab/Publications.html">http://www.weizmann.ac.il/neurobiology/labs/schneidman/The&#x005F;Schneidman&#x005F;Lab/Publications.html</ext-link>.</mixed-citation></ref>
<ref id="c13"><label>[13]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Shlens</surname></string-name>, <string-name><given-names>G. D.</given-names> <surname>Field</surname></string-name>, <string-name><given-names>J. L.</given-names> <surname>Gauthier</surname></string-name>, <string-name><given-names>M. I.</given-names> <surname>Grivich</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Petrusca</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Sher</surname></string-name>, <string-name><given-names>A. M.</given-names> <surname>Litke</surname></string-name>, <string-name><given-names>E. J.</given-names> <surname>Chichilnisky</surname></string-name>: <article-title>The structure of multi-neuron firing patterns in primate retina</article-title>. <source>J. Neurosci</source>. <volume>26</volume><issue>32</issue> (<year>2006</year>), <fpage>8254</fpage>&#x2013;<lpage>8266</lpage>. See also correction ref. [72].</mixed-citation></ref>
<ref id="c14"><label>[14]</label><mixed-citation publication-type="website"><string-name><given-names>J. H.</given-names> <surname>Macke</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Opper</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Bethge</surname></string-name>: <article-title>The effect of pairwise neural correlations on global population statistics</article-title>. <source>Tech. rep. 183</source>. <publisher-name>Max-Planck-Institut f&#x00FC;r biologische Kybernetik</publisher-name>, <publisher-loc>T&#x00FC;bingen</publisher-loc> (<year>2009</year>). <ext-link ext-link-type="uri" xlink:href="http://www.kyb.tuebingen.mpg.de/publications/attachments/MPIK-TR-183&#x005F;&#x0025;5B0&#x0025;5D.pdf">http://www.kyb.tuebingen.mpg.de/publications/attachments/MPIK-TR-183&#x005F;&#x0025;5B0&#x0025;5D.pdf</ext-link>.</mixed-citation></ref>
<ref id="c15"><label>[15]</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Roudi</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Tyrcha</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Hertz</surname></string-name>: <article-title>Ising model for neural data: model quality and approximate methods for extracting functional connectivity</article-title>. <source>Phys. Rev. E</source> <volume>79</volume><issue>5</issue> (<year>2009</year>), <fpage>051915</fpage>.</mixed-citation></ref>
<ref id="c16"><label>[16]</label><mixed-citation publication-type="other"><string-name><given-names>G.</given-names> <surname>Tka&#x010D;ik</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Schneidman</surname></string-name>, <string-name><given-names>M. J.</given-names> <surname>Berry</surname> <suffix>II</suffix></string-name>, <string-name><given-names>W.</given-names> <surname>Bialek</surname></string-name>: <article-title>Spin glass models for a network of real neurons</article-title>. (<year>2009</year>). <source>arXiv:0912.5409</source>.</mixed-citation></ref>
<ref id="c17"><label>[17]</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Gerwinn</surname></string-name>, <string-name><given-names>P.</given-names> <surname>Berens</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Bethge</surname></string-name>: <article-title>A joint maximum-entropy model for binary neural population patterns and continuous signals</article-title>. <source>Adv. Neural Information Processing Systems (NIPS proceedings)</source> <volume>22</volume> (<year>2009</year>), <fpage>620</fpage>&#x2013;<lpage>628</lpage>.</mixed-citation></ref>
<ref id="c18"><label>[18]</label><mixed-citation publication-type="journal"><string-name><given-names>J. H.</given-names> <surname>Macke</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Opper</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Bethge</surname></string-name>: <article-title>Common input explains higher-order correlations and entropy in a simple model of neural population activity</article-title>. <source>Phys. Rev. Lett</source>. <volume>106</volume><issue>20</issue> (<year>2011</year>), <fpage>208102</fpage>.</mixed-citation></ref>
<ref id="c19"><label>[19]</label><mixed-citation publication-type="journal"><string-name><given-names>J. H.</given-names> <surname>Macke</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Buesing</surname></string-name>, <string-name><given-names>J. P.</given-names> <surname>Cunningham</surname></string-name>, <string-name><given-names>B. M.</given-names> <surname>Yu</surname></string-name>, <string-name><given-names>K. V.</given-names> <surname>Shenoy</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Sahani</surname></string-name>: <article-title>Empirical models of spiking in neural populations</article-title>. <source>Adv. Neural Information Processing Systems (NIPS proceedings)</source> <volume>24</volume> (<year>2011</year>), <fpage>1350</fpage>&#x2013;<lpage>1358</lpage>.</mixed-citation></ref>
<ref id="c20"><label>[20]</label><mixed-citation publication-type="website"><string-name><given-names>E.</given-names> <surname>Ganmor</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Segev</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Schneidman</surname></string-name>: <article-title>Sparse low-order interaction network underlies a highly correlated and learnable neural population code</article-title>. <source>Proc. Natl. Acad. Sci. (USA)</source> <volume>108</volume><issue>23</issue> (<year>2011</year>), <fpage>9679</fpage>&#x2013;<lpage>9684</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.weizmann.ac.il/neurobiology/labs/schneidman/The&#x005F;Schneidman&#x005F;Lab/Publications.html">http://www.weizmann.ac.il/neurobiology/labs/schneidman/The&#x005F;Schneidman&#x005F;Lab/Publications.html</ext-link>.</mixed-citation></ref>
<ref id="c21"><label>[21]</label><mixed-citation publication-type="journal"><string-name><given-names>E.</given-names> <surname>Granot-Atedgi</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Tka&#x010D;ik</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Segev</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Schneidman</surname></string-name>: <article-title>Stimulus-dependent maximum entropy models of neural population codes</article-title>. <source>Plos Comput. Biol</source>. <volume>9</volume><issue>3</issue> (<year>2013</year>), <fpage>e1002922</fpage>.</mixed-citation></ref>
<ref id="c22"><label>[22]</label><mixed-citation publication-type="journal"><string-name><given-names>G.</given-names> <surname>Tka&#x010D;ik</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Mora</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Marre</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Amodei</surname></string-name>, <string-name><given-names>S. E.</given-names> <surname>Palmer</surname></string-name>, <string-name><given-names>M. J.</given-names> <surname>Berry</surname></string-name> <edition>II</edition>, <string-name><given-names>W.</given-names> <surname>Bialek</surname></string-name>: <article-title>Thermodynamics and signatures of criticality in a network of neurons</article-title>. <source>Proc. Natl. Acad. Sci. (USA)</source> <volume>112</volume><issue>37</issue> (<year>2014</year>), <fpage>11508</fpage>&#x2013;<lpage>11513</lpage>.</mixed-citation></ref>
<ref id="c23"><label>[23]</label><mixed-citation publication-type="journal"><string-name><given-names>T.</given-names> <surname>Mora</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Deny</surname></string-name>, <string-name><given-names>O.</given-names> <surname>Marre</surname></string-name>: <article-title>Dynamical criticality in the collective activity of a population of retinal neurons</article-title>. <source>Phys. Rev. Lett</source>. <volume>114</volume><issue>7</issue> (<year>2015</year>), <fpage>078105</fpage>.</mixed-citation></ref>
<ref id="c24"><label>[24]</label><mixed-citation publication-type="journal"><string-name><given-names>H.</given-names> <surname>Shimazaki</surname></string-name>, <string-name><given-names>K.</given-names> <surname>Sadeghi</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Ishikawa</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Ikegaya</surname></string-name>, <string-name><given-names>T.</given-names> <surname>Toyoizumi</surname></string-name>: <article-title>Simultaneous silence organizes structured higher-order interactions in neural populations</article-title>. <source>Sci. Rep</source>. <volume>5</volume> (<year>2015</year>), <fpage>9821</fpage>.</mixed-citation></ref>
<ref id="c25"><label>[25]</label><mixed-citation publication-type="other"><string-name><given-names>G.</given-names> <surname>Tka&#x010D;ik</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Schneidman</surname></string-name>, <string-name><given-names>M. J.</given-names> <surname>Berry</surname></string-name> <edition>II</edition>, <string-name><given-names>W.</given-names> <surname>Bialek</surname></string-name>: <article-title>Ising models for networks of real neurons</article-title>. (<year>2006</year>). <source>arXiv:q-bio/0611072</source>.</mixed-citation></ref>
<ref id="c26"><label>[26]</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Roudi</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Aurell</surname></string-name>, <string-name><given-names>J. A.</given-names> <surname>Hertz</surname></string-name>: <article-title>Statistical physics of pairwise probability models</article-title>. <source>Front. Comput. Neurosci</source>. <volume>3</volume> (<year>2009</year>), <fpage>22</fpage>.</mixed-citation></ref>
<ref id="c27"><label>[27]</label><mixed-citation publication-type="journal"><string-name><given-names>Y.</given-names> <surname>Roudi</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Nirenberg</surname></string-name>, <string-name><given-names>P. E.</given-names> <surname>Latham</surname></string-name>: <article-title>Pairwise maximum entropy models for studying large biological systems: when they can work and when they can&#x2019;t</article-title>. <source>Plos Comput. Biol</source>. <volume>5</volume><issue>5</issue> (<year>2009</year>), <fpage>e1000380</fpage>.</mixed-citation></ref>
<ref id="c28"><label>[28]</label><mixed-citation publication-type="other"><string-name><given-names>A. K.</given-names> <surname>Barreiro</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Gjorgjieva</surname></string-name>, <string-name><given-names>F. M.</given-names> <surname>Rieke</surname></string-name>, <string-name><given-names>E. T.</given-names> <surname>Shea-Brown</surname></string-name>: <article-title>When are microcircuits well-modeled by maximum entropy methods?</article-title> (<year>2010</year>). <source>arXiv:1011.2797</source>.</mixed-citation></ref>
<ref id="c29"><label>[29]</label><mixed-citation publication-type="journal"><string-name><given-names>A. K.</given-names> <surname>Barreiro</surname></string-name>, <string-name><given-names>E. T.</given-names> <surname>Shea-Brown</surname></string-name>, <string-name><given-names>F. M.</given-names> <surname>Rieke</surname></string-name>, <string-name><given-names>J.</given-names> <surname>Gjorgjieva</surname></string-name>: <article-title>When are microcircuits well-modeled by maximum entropy methods?</article-title> <source>BMC Neurosci</source>. <volume>11</volume><issue>Suppl. 1</issue> (<year>2010</year>), <fpage>P65</fpage>.</mixed-citation></ref>
<ref id="c30"><label>[30]</label><mixed-citation publication-type="website"><string-name><given-names>J. H.</given-names> <surname>Macke</surname></string-name>, <string-name><given-names>I.</given-names> <surname>Murray</surname></string-name>, <string-name><given-names>P. E.</given-names> <surname>Latham</surname></string-name>: <article-title>Estimation bias in maximum entropy models</article-title>. <source>Entropy</source> <volume>15</volume><issue>8</issue> (<year>2013</year>), <fpage>3109</fpage>&#x2013;<lpage>3129</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.gatsby.ucl.ac.uk/&#x223C;pel/papers/maxentbias.pdf">http://www.gatsby.ucl.ac.uk/&#x223C;pel/papers/maxentbias.pdf</ext-link>.</mixed-citation></ref>
<ref id="c31"><label>[31]</label><mixed-citation publication-type="journal"><string-name><given-names>V.</given-names> <surname>Rostami</surname></string-name>, <string-name><given-names>P. G. L. Porta</given-names> <surname>Mana</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Gr&#x00FC;n</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Helias</surname></string-name>: <article-title>Bistability, non-ergodicity, and inhibition in pairwise maximum-entropy models</article-title>. <source>Plos Comput. Biol</source>. <volume>13</volume><issue>10</issue> (<year>2017</year>), <fpage>e1005762</fpage>. See also the slightly different version arXiv:1605.04740.</mixed-citation></ref>
<ref id="c32"><label>[32]</label><mixed-citation publication-type="journal"><string-name><given-names>E. R.</given-names> <surname>Caianiello</surname></string-name>: <article-title>Outline of a theory of thought-processes and thinking machines</article-title>. <source>J. Theor. Biol</source>. <volume>1</volume><issue>2</issue> (<year>1961</year>), <fpage>204</fpage>&#x2013;<lpage>235</lpage>.</mixed-citation></ref>
<ref id="c33"><label>[33]</label><mixed-citation publication-type="journal"><string-name><given-names>E. R.</given-names> <surname>Caianiello</surname></string-name>: <article-title>Neuronic equations revisited and completely solved</article-title>. <source>In: Palm, Aertsen</source> [<volume>73]</volume> (<year>1986</year>), <fpage>147</fpage>&#x2013;<lpage>160</lpage>.</mixed-citation></ref>
<ref id="c34"><label>[34]</label><mixed-citation publication-type="journal"><string-name><given-names>S.</given-names> <surname>Kullback</surname></string-name>: <article-title>The Kullback-Leibler distance</article-title>. <source>American Statistician</source> <volume>41</volume><issue>4</issue> (<year>1987</year>), <fpage>340</fpage>&#x2013;<lpage>341</lpage>.</mixed-citation></ref>
<ref id="c35"><label>[35]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Hobson</surname></string-name>: <article-title>A new theorem of information theory</article-title>. <source>J. Stat. Phys</source>. <volume>1</volume><issue>3</issue> (<year>1969</year>), <fpage>383</fpage>&#x2013;<lpage>391</lpage>.</mixed-citation></ref>
<ref id="c36"><label>[36]</label><mixed-citation publication-type="book"><string-name><given-names>P.</given-names> <surname>Smolensky</surname></string-name>: <chapter-title>Information processing in dynamical systems: foundations of harmony theory</chapter-title>. <source>In: Rumelhart, McClelland</source>, <publisher-name>PDP Research Group [</publisher-name><volume>74]</volume> (<year>1986</year>), ch. 6, <fpage>194</fpage>&#x2013;<lpage>281</lpage>.</mixed-citation></ref>
<ref id="c37"><label>[37]</label><mixed-citation publication-type="journal"><string-name><given-names>J. E.</given-names> <surname>Kulkarni</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Paninski</surname></string-name>: <article-title>Common-input models for multiple neural spike-train data</article-title>. <source>Netw</source>. <volume>18</volume><issue>4</issue> (<year>2007</year>), <fpage>375</fpage>&#x2013;<lpage>407</lpage>.</mixed-citation></ref>
<ref id="c38"><label>[38]</label><mixed-citation publication-type="journal"><string-name><given-names>H.</given-names> <surname>Huang</surname></string-name>: <article-title>Effects of hidden nodes on network structure inference</article-title>. <source>J. Phys. A</source> <volume>48</volume><issue>35</issue> (<year>2015</year>), <fpage>355002</fpage>.</mixed-citation></ref>
<ref id="c39"><label>[39]</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>Dunn</surname></string-name>, <string-name><given-names>C.</given-names> <surname>Battistin</surname></string-name>: <article-title>The appropriateness of ignorance in the inverse kinetic Ising model</article-title>. <source>J. Phys. A</source> <volume>50</volume><issue>12</issue> (<year>2017</year>), <fpage>124002</fpage>.</mixed-citation></ref>
<ref id="c40"><label>[40]</label><mixed-citation publication-type="website"><string-name><given-names>A.</given-names> <surname>Ber&#x00E9;nyi</surname></string-name>, <string-name><given-names>Z.</given-names> <surname>Somogyv&#x00E1;ri</surname></string-name>, <string-name><given-names>A. J.</given-names> <surname>Nagy</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Roux</surname></string-name>, <string-name><given-names>J. D.</given-names> <surname>Long</surname></string-name>, <string-name><given-names>S.</given-names> <surname>Fujisawa</surname></string-name>, <string-name><given-names>E.</given-names> <surname>Stark</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Leonardo</surname></string-name> <etal>et al.</etal>: <article-title>Large-scale, high-density (up to 512 channels) recording of local circuits in behaving animals</article-title>. <source>J. Neurophysiol</source>. <volume>111</volume><issue>5</issue> (<year>2014</year>), <fpage>1132</fpage>&#x2013;<lpage>1149</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.buzsakilab.com/content/PDFs/Berenyi2013.pdf">http://www.buzsakilab.com/content/PDFs/Berenyi2013.pdf</ext-link>.</mixed-citation></ref>
<ref id="c41"><label>[41]</label><mixed-citation publication-type="book"><string-name><given-names>M.</given-names> <surname>Abeles</surname></string-name>: <source>Corticonics: Neural circuits of the cerebral cortex</source>. <publisher-name>Cambridge University Press</publisher-name>, <publisher-loc>Cambridge</publisher-loc> (<year>1991</year>).</mixed-citation></ref>
<ref id="c42"><label>[42]</label><mixed-citation publication-type="book"><string-name><given-names>M.</given-names> <surname>Ghosh</surname></string-name>, <string-name><given-names>G.</given-names> <surname>Meeden</surname></string-name>: <source>Bayesian Methods for Finite Population Sampling, reprint</source>. <publisher-name>Springer</publisher-name>, <publisher-loc>Dordrecht</publisher-loc> (<year>1997</year>).</mixed-citation></ref>
<ref id="c43"><label>[43]</label><mixed-citation publication-type="book"><string-name><given-names>D.</given-names> <surname>Freedman</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Pisani</surname></string-name>, <string-name><given-names>R.</given-names> <surname>Purves</surname></string-name>: <source>Statistics</source>, <edition>4th ed</edition>. <publisher-loc>Norton, London</publisher-loc> (<year>2007</year>). First publ. 1978.</mixed-citation></ref>
<ref id="c44"><label>[44]</label><mixed-citation publication-type="book"><string-name><given-names>A.</given-names> <surname>Gelman</surname></string-name>, <string-name><given-names>J. B.</given-names> <surname>Carlin</surname></string-name>, <string-name><given-names>H. S.</given-names> <surname>Stern</surname></string-name>, <string-name><given-names>D. B.</given-names> <surname>Dunson</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Vehtari</surname></string-name>, <string-name><given-names>D. B.</given-names> <surname>Rubin</surname></string-name>: <source>Bayesian Data Analysis</source>, <edition>3rd ed</edition>. <publisher-name>Chapman &#x0026; Hall/CRC</publisher-name>, <publisher-loc>Boca Raton, USA</publisher-loc> (<year>2014</year>). First publ. 1995.</mixed-citation></ref>
<ref id="c45"><label>[45]</label><mixed-citation publication-type="website"><string-name><given-names>E. T.</given-names> <surname>Jaynes</surname></string-name>: <source>Probability Theory: The Logic of Science</source>. <publisher-name>Cambridge University Press</publisher-name>, <publisher-loc>Cambridge</publisher-loc> (<year>2003</year>). Ed. by <person-group person-group-type="editor"><string-name><given-names>G. Larry</given-names> <surname>Bretthorst</surname></string-name></person-group>. First publ. 1994. <ext-link ext-link-type="uri" xlink:href="https://archive.org/details/XQUHIUXHIQUHIQXUIHX2">https://archive.org/details/XQUHIUXHIQUHIQXUIHX2</ext-link>, <ext-link ext-link-type="uri" xlink:href="http://www-biba.inrialpes.fr/Jaynes/prob.html">http://www-biba.inrialpes.fr/Jaynes/prob.html</ext-link>, <ext-link ext-link-type="uri" xlink:href="http://omega.albany.edu:8008/JaynesBook.html">http://omega.albany.edu:8008/JaynesBook.html</ext-link>.</mixed-citation></ref>
<ref id="c46"><label>[46]</label><mixed-citation publication-type="book"><collab>ISO</collab>: <source>Quantities and units</source>, <edition>3rd ed</edition>. <publisher-name>International Organization for Standardization</publisher-name>. <publisher-loc>Geneva</publisher-loc> (<year>1993</year>).</mixed-citation></ref>
<ref id="c47"><label>[47]</label><mixed-citation publication-type="book"><collab>IEEE</collab>: <chapter-title>ANSI/IEEE Std 260.3-1993</chapter-title>: <source>American National Standard: Mathematical signs and symbols for use in physical sciences and technology</source>. <publisher-name>Institute of Electrical and Electronics Engineers</publisher-name>. <publisher-loc>New York</publisher-loc> (<year>1993</year>).</mixed-citation></ref>
<ref id="c48"><label>[48]</label><mixed-citation publication-type="website"><collab><italic>Guide for the Use of the International System of Units (SI): NIST special publication 811</italic></collab>, <edition>1995 edition</edition>. <publisher-name>National Institute of Standards and Technology</publisher-name>. <publisher-loc>Washington, D.C</publisher-loc>. (<year>1995</year>). <ext-link ext-link-type="uri" xlink:href="http://physics.nist.gov/cuu/Uncertainty/bibliography.html">http://physics.nist.gov/cuu/Uncertainty/bibliography.html</ext-link>.</mixed-citation></ref>
<ref id="c49"><label>[49]</label><mixed-citation publication-type="book"><collab>ISO</collab>: <source>ISO 3534-1:2006: Statistics &#x2013; Vocabulary and symbols &#x2013; Part 1: General statistical terms and terms used in probability</source>. <publisher-name>International Organization for Standardization</publisher-name>. <publisher-loc>Geneva</publisher-loc> (<year>2006</year>).</mixed-citation></ref>
<ref id="c50"><label>[50]</label><mixed-citation publication-type="book"><collab>ISO</collab>: <source>ISO 3534-2:2006: Statistics &#x2013; Vocabulary and symbols &#x2013; Part 2: Applied statistics</source>. <publisher-name>International Organization for Standardization</publisher-name>. <publisher-loc>Geneva</publisher-loc> (<year>2006</year>).</mixed-citation></ref>
<ref id="c51"><label>[51]</label><mixed-citation publication-type="journal"><string-name><given-names>I. J.</given-names> <surname>Good</surname></string-name>: <article-title>How to estimate probabilities</article-title>. <source>J. Inst. Maths. Applics</source> <volume>2</volume><issue>4</issue> (<year>1966</year>), <fpage>364</fpage>&#x2013;<lpage>383</lpage>.</mixed-citation></ref>
<ref id="c52"><label>[52]</label><mixed-citation publication-type="website"><string-name><given-names>W. A.</given-names> <surname>Ericson</surname></string-name>: <article-title>Subjective Bayesian models in sampling finite populations</article-title>. <source>J. Roy. Stat. Soc. B</source> <volume>31</volume><issue>2</issue> (<year>1969</year>), <fpage>195</fpage>&#x2013;<lpage>224</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.stat.cmu.edu/&#x223C;brian/905-2008/papers/Ericson-JRSSB-1969.pdf">http://www.stat.cmu.edu/&#x223C;brian/905-2008/papers/Ericson-JRSSB-1969.pdf</ext-link>. See also discussion ref. [75].</mixed-citation></ref>
<ref id="c53"><label>[53]</label><mixed-citation publication-type="journal"><string-name><given-names>B.</given-names> <surname>de Finetti</surname></string-name>: <article-title>La probabilit&#x00E0; e la statistica nei rapporti con l&#x2019;induzione, secondo i diversi punti di vista</article-title>. <source>In: de Finetti</source> [<volume>76</volume>] (<year>1959</year>), <comment>1&#x2013;115. Transl. in ref. [77], ch. 9</comment>, pp. <fpage>147</fpage>&#x2013;<lpage>227</lpage>.</mixed-citation></ref>
<ref id="c54"><label>[54]</label><mixed-citation publication-type="journal"><string-name><given-names>D. G.</given-names> <surname>Kendall</surname></string-name>: <article-title>On finite and infinite sequences of exchangeable events</article-title>. <source>Studia Sci. Math. Hung</source>. <volume>2</volume> (<year>1967</year>), <fpage>319</fpage>&#x2013;<lpage>327</lpage>.</mixed-citation></ref>
<ref id="c55"><label>[55]</label><mixed-citation publication-type="website"><string-name><given-names>W. A.</given-names> <surname>Ericson</surname></string-name>: <article-title>A Bayesian approach to two-stage sampling</article-title>. <source>Tech. rep. AFFDL-TR-75-145</source>. <publisher-name>University of Michigan</publisher-name>, <publisher-loc>Ann Arbor, USA</publisher-loc> (<year>1976</year>). <ext-link ext-link-type="uri" xlink:href="http://hdl.handle.net/2027.42/4819">http://hdl.handle.net/2027.42/4819</ext-link>.</mixed-citation></ref>
<ref id="c56"><label>[56]</label><mixed-citation publication-type="website"><string-name><given-names>P.</given-names> <surname>Diaconis</surname></string-name>: <article-title>Finite forms of de Finetti&#x2019;s theorem on exchangeability</article-title>. <source>Synthese</source> <volume>36</volume><issue>2</issue> (<year>1977</year>), <fpage>271</fpage>&#x2013;<lpage>281</lpage>. <ext-link ext-link-type="uri" xlink:href="http://statweb.stanford.edu/&#x223C;cgates/PERSI/year.html">http://statweb.stanford.edu/&#x223C;cgates/PERSI/year.html</ext-link>.</mixed-citation></ref>
<ref id="c57"><label>[57]</label><mixed-citation publication-type="journal"><string-name><given-names>P.</given-names> <surname>Diaconis</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Freedman</surname></string-name>: <article-title>Finite exchangeable sequences</article-title>. <source>Ann. Prob</source>. <volume>8</volume><issue>4</issue> (<year>1980</year>), <fpage>745</fpage>&#x2013;<lpage>764</lpage>.</mixed-citation></ref>
<ref id="c58"><label>[58]</label><mixed-citation publication-type="journal"><string-name><given-names>D.</given-names> <surname>Heath</surname></string-name>, <string-name><given-names>W.</given-names> <surname>Sudderth</surname></string-name>: <article-title>De Finetti&#x2019;s theorem on exchangeable variables</article-title>. <source>American Statistician</source> <volume>30</volume><issue>4</issue> (<year>1976</year>), <fpage>188</fpage>&#x2013;<lpage>189</lpage>.</mixed-citation></ref>
<ref id="c59"><label>[59]</label><mixed-citation publication-type="book"><string-name><given-names>S.</given-names> <surname>Ross</surname></string-name>: <source>A First Course in Probability</source>, <edition>8th ed</edition>. <publisher-loc>Pearson, Upper Saddle River, USA</publisher-loc> (<year>2010</year>). First publ. 1976.</mixed-citation></ref>
<ref id="c60"><label>[60]</label><mixed-citation publication-type="book"><string-name><given-names>W.</given-names> <surname>Feller</surname></string-name>: <source>An Introduction to Probability Theory and Its Applications</source>. Vol. <volume>I</volume>, 3rd ed. <publisher-name>Wiley</publisher-name>, <publisher-loc>New York</publisher-loc> (<year>1968</year>). First publ. 1950.</mixed-citation></ref>
<ref id="c61"><label>[61]</label><mixed-citation publication-type="journal"><string-name><given-names>R. B.</given-names> <surname>Potts</surname></string-name>: <article-title>Note on the factorial moments of standard distributions</article-title>. <source>Aust. J. Phys</source>. <volume>6</volume><issue>4</issue> (<year>1953</year>), <fpage>498</fpage>&#x2013;<lpage>499</lpage>.</mixed-citation></ref>
<ref id="c62"><label>[62]</label><mixed-citation publication-type="book"><string-name><given-names>H. B.</given-names> <surname>Callen</surname></string-name>: <source>Thermodynamics and an Introduction to Thermostatistics</source>, <edition>2nd ed</edition>. <publisher-name>Wiley</publisher-name>, <publisher-loc>New York</publisher-loc> (<year>1985</year>). First publ. 1960.</mixed-citation></ref>
<ref id="c63"><label>[63]</label><mixed-citation publication-type="book"><string-name><given-names>H.</given-names> <surname>Jeffreys</surname></string-name>: <source>Theory of Probability</source>, <edition>3rd ed</edition>. <publisher-name>Oxford University Press</publisher-name>, <publisher-loc>London</publisher-loc> (<year>2003</year>). First publ. 1939.</mixed-citation></ref>
<ref id="c64"><label>[64]</label><mixed-citation publication-type="journal"><string-name><given-names>N.</given-names> <surname>Le Roux</surname></string-name>, <string-name><given-names>Y.</given-names> <surname>Bengio</surname></string-name>: <article-title>Representational power of restricted Boltzmann machines and deep belief networks</article-title>. <source>Neural Comp</source>. <volume>20</volume><issue>6</issue> (<year>2008</year>), <fpage>1631</fpage>&#x2013;<lpage>1649</lpage>.</mixed-citation></ref>
<ref id="c65"><label>[65]</label><mixed-citation publication-type="journal"><string-name><given-names>C.</given-names> <surname>Maes</surname></string-name>, <string-name><given-names>F.</given-names> <surname>Redig</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Van Moffaert</surname></string-name>: <article-title>The restriction of the Ising model to a layer</article-title>. <source>J. Stat. Phys</source>. <volume>96</volume><issue>1</issue> (<year>1999</year>), <fpage>69</fpage>&#x2013;<lpage>107</lpage>.</mixed-citation></ref>
<ref id="c66"><label>[66]</label><mixed-citation publication-type="website"><string-name><given-names>E. T.</given-names> <surname>Jaynes</surname></string-name>: <article-title>Inferential scattering</article-title>. (<year>1993</year>). <ext-link ext-link-type="uri" xlink:href="http://bayes.wustl.edu/etj/node1.html">http://bayes.wustl.edu/etj/node1.html</ext-link>. <source>Extensively rewritten version of a paper first publ. 1985 in ref. [78]</source>, pp. <fpage>377</fpage>&#x2013;<lpage>398</lpage>.</mixed-citation></ref>
<ref id="c67"><label>[67]</label><mixed-citation publication-type="journal"><string-name><given-names>A.</given-names> <surname>Levina</surname></string-name>, <string-name><given-names>V.</given-names> <surname>Priesemann</surname></string-name>: <article-title>Subsampling scaling</article-title>. <source>Nat. Comm</source>. <volume>8</volume> (<year>2017</year>), <fpage>15140</fpage>.</mixed-citation></ref>
<ref id="c68"><label>[68]</label><mixed-citation publication-type="website"><string-name><given-names>E. T.</given-names> <surname>Jaynes</surname></string-name>: <article-title>Information theory and statistical mechanics. II</article-title>. <source>Phys. Rev</source>. <volume>108</volume><issue>2</issue> (<year>1957</year>), <fpage>171</fpage>&#x2013;<lpage>190</lpage>. <ext-link ext-link-type="uri" xlink:href="http://bayes.wustl.edu/etj/node1.html">http://bayes.wustl.edu/etj/node1.html</ext-link>, see also ref. [3].</mixed-citation></ref>
<ref id="c69"><label>[69]</label><mixed-citation publication-type="book"><person-group person-group-type="editor"><string-name><given-names>K. W.</given-names> <surname>Ford</surname></string-name></person-group>, ed.: <source>Statistical Physics</source>. <publisher-name>Benjamin</publisher-name>, <publisher-loc>New York</publisher-loc> (<year>1963</year>).</mixed-citation></ref>
<ref id="c70"><label>[70]</label><mixed-citation publication-type="book"><string-name><given-names>E. T.</given-names> <surname>Jaynes</surname></string-name>: <chapter-title>E. T. Jaynes: Papers on Probability</chapter-title>, <source>Statistics and Statistical Physics, reprint</source>. <publisher-loc>Kluwer, Dordrecht</publisher-loc> (<year>1989</year>). Ed. by <person-group person-group-type="editor"><string-name><given-names>R. D.</given-names> <surname>Rosenkrantz</surname></string-name></person-group>. First publ. 1983.</mixed-citation></ref>
<ref id="c71"><label>[71]</label><mixed-citation publication-type="book"><person-group person-group-type="editor"><string-name><given-names>W. T.</given-names> <surname>Grandy</surname> <suffix>Jr.</suffix></string-name>, <string-name><given-names>L. H.</given-names> <surname>Schick</surname></string-name></person-group>, eds.: <source>Maximum Entropy and Bayesian Methods: Laramie, Wyoming</source>, <year>1990</year>. <publisher-loc>Kluwer, Dordrecht</publisher-loc> (1991).</mixed-citation></ref>
<ref id="c72"><label>[72]</label><mixed-citation publication-type="journal"><string-name><given-names>J.</given-names> <surname>Shlens</surname></string-name>, <string-name><given-names>G. D.</given-names> <surname>Field</surname></string-name>, <string-name><given-names>J. L.</given-names> <surname>Gauthier</surname></string-name>, <string-name><given-names>M. I.</given-names> <surname>Grivich</surname></string-name>, <string-name><given-names>D.</given-names> <surname>Petrusca</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Sher</surname></string-name>, <string-name><given-names>A. M.</given-names> <surname>Litke</surname></string-name>, <string-name><given-names>E. J.</given-names> <surname>Chichilnisky</surname></string-name>: <article-title>Correction, the structure of multi-neuron firing patterns in primate retina</article-title>. <source>J. Neurosci</source>. <volume>28</volume><issue>5</issue> (<year>2008</year>), <fpage>1246</fpage>. See ref. [13].</mixed-citation></ref>
<ref id="c73"><label>[73]</label><mixed-citation publication-type="book"><person-group person-group-type="editor"><string-name><given-names>G.</given-names> <surname>Palm</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Aertsen</surname></string-name></person-group>, eds.: <source>Brain Theory</source>. <publisher-name>Springer</publisher-name>, <publisher-loc>Berlin</publisher-loc> (<year>1986</year>).</mixed-citation></ref>
<ref id="c74"><label>[74]</label><mixed-citation publication-type="book"><person-group person-group-type="editor"><string-name><given-names>D. E.</given-names> <surname>Rumelhart</surname></string-name>, <string-name><given-names>J. L.</given-names> <surname>McClelland</surname></string-name>, <collab>PDP Research Group</collab></person-group>, eds.: <chapter-title>Parallel Distributed Processing</chapter-title>: <source>Explorations in the Microstructure of Cognition</source>. Vol. <volume>1</volume>: Foundations, 12th printing. <publisher-name>MIT Press</publisher-name>, <publisher-loc>Cambridge, USA</publisher-loc> (<year>1999</year>).</mixed-citation></ref>
<ref id="c75"><label>[75]</label><mixed-citation publication-type="website"><string-name><given-names>M. R.</given-names> <surname>Sampford</surname></string-name>, <string-name><given-names>A.</given-names> <surname>Scott</surname></string-name>, <string-name><given-names>M.</given-names> <surname>Stone</surname></string-name>, <string-name><given-names>D. V.</given-names> <surname>Lindley</surname></string-name>, <string-name><given-names>T. M. F.</given-names> <surname>Smith</surname></string-name>, <string-name><given-names>D. F.</given-names> <surname>Kerridge</surname></string-name>, <string-name><given-names>V. P.</given-names> <surname>Godambe</surname></string-name>, <string-name><given-names>L.</given-names> <surname>Kish</surname></string-name> <etal>et al.</etal>: <article-title>Discussion on professor Ericson&#x2019;s paper</article-title>. <source>J. Roy. Stat. Soc. B</source> <volume>31</volume><issue>2</issue> (<year>1969</year>), <fpage>224</fpage>&#x2013;<lpage>233</lpage>. <ext-link ext-link-type="uri" xlink:href="http://www.stat.cmu.edu/&#x223C;brian/905-2008/papers/Ericson-JRSSB-1969.pdf">http://www.stat.cmu.edu/&#x223C;brian/905-2008/papers/Ericson-JRSSB-1969.pdf</ext-link>. See ref.</mixed-citation></ref>
<ref id="c76"><label>[76]</label><mixed-citation publication-type="book"><person-group person-group-type="editor"><string-name><given-names>B.</given-names> <surname>de Finetti</surname></string-name></person-group>, ed.: <source>Induzione e statistica, reprint</source>. <publisher-name>Springer</publisher-name>, <publisher-loc>Berlin</publisher-loc> (<year>2011</year>). First publ. 1959.</mixed-citation></ref>
<ref id="c77"><label>[77]</label><mixed-citation publication-type="book"><string-name><given-names>B.</given-names> <surname>de Finetti</surname></string-name>: <source>Probability, Induction and Statistics: The art of guessing</source>. <publisher-name>Wiley</publisher-name>, <publisher-loc>London</publisher-loc> (<year>1972</year>).</mixed-citation></ref>
<ref id="c78"><label>[78]</label><mixed-citation publication-type="book"><person-group person-group-type="editor"><string-name><given-names>C. R.</given-names> <surname>Smith</surname></string-name>, <string-name><given-names>W. T.</given-names> <surname>Grandy</surname> <suffix>Jr.</suffix></string-name></person-group>, eds.: <source>Maximum-Entropy and Bayesian Methods in Inverse Problems</source>. <publisher-name>D. Reidel</publisher-name>, <publisher-loc>Dordrecht</publisher-loc> (<year>1985</year>).</mixed-citation></ref>
<ref id="c79"><label>[79]</label><mixed-citation publication-type="journal"><string-name><given-names>W. A.</given-names> <surname>Ericson</surname></string-name>: <article-title>A note on the posterior mean of a population mean</article-title>. <source>J. Roy. Stat. Soc. B</source> <volume>31</volume><issue>2</issue> (<year>1969</year>), <fpage>332</fpage>&#x2013;<lpage>334</lpage>.</mixed-citation></ref>
</ref-list>
</back>
</article>
