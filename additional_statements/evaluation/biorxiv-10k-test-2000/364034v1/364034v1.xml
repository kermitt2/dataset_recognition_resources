<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/364034</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Neuroscience</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Learning cellular morphology with neural networks</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Schubert</surname>
<given-names>Philipp J</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n1">&#x2217;</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Dorkenwald</surname>
<given-names>Sven</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Januszewski</surname>
<given-names>Micha&#x0142;</given-names>
</name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Jain</surname>
<given-names>Viren</given-names>
</name>
<xref ref-type="aff" rid="a3">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kornfeld</surname>
<given-names>Joergen</given-names>
</name>
<xref ref-type="aff" rid="a1">1</xref>
<xref ref-type="author-notes" rid="n2">&#x2217;&#x2217;</xref>
</contrib>
<aff id="a1"><label>1</label><institution>Max Planck Institute of Neurobiology, Electrons - Photons - Neurons</institution>, Planegg-Martinsried, 82152, <country>Germany</country></aff>
<aff id="a2"><label>2</label><institution>Google AI</institution>, Zurich, <country>Switzerland</country></aff>
<aff id="a3"><label>3</label><institution>Google AI</institution>, Mountain View, <country>USA</country></aff>
</contrib-group>
<author-notes>
<fn id="n1"><label>&#x2217;</label><p><email>pschubert@neuro.mpg.de</email></p></fn>
<fn id="n2"><label>&#x2217;&#x2217;</label><p><email>kornfeld@neuro.mpg.de</email></p></fn>
</author-notes>
<pub-date pub-type="epub"><year>2018</year></pub-date>
<elocation-id>364034</elocation-id>
<history>
<date date-type="received">
<day>06</day>
<month>7</month>
<year>2018</year>
</date>
<date date-type="rev-recd">
<day>06</day>
<month>7</month>
<year>2018</year>
</date>
<date date-type="accepted">
<day>06</day>
<month>7</month>
<year>2018</year>
</date>
</history>
<permissions>
<copyright-statement>&#x00A9; 2018, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2018</copyright-year>
<license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license>
</permissions>
<self-uri xlink:href="364034.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract>
<title>Abstract</title>
<p>Reconstruction and annotation of volume electron microscopy data sets of brain tissue is challenging, but can reveal invaluable information about neuronal circuits. Significant progress has recently been made in automated neuron reconstruction, as well as automated detection of synapses. However, methods for automating the morphological analysis of nanometer-resolution reconstructions are less established, despite their diverse application possibilities. Here, we introduce cellular morphology neural networks (CMNs), based on multi-view projections sampled from automatically reconstructed cellular fragments of arbitrary size and shape. Using unsupervised training we inferred morphology embeddings (&#x201C;Neuron2vec&#x201D;) of neuron reconstructions and trained CMNs to identify glia cells in a supervised classification paradigm which was used to resolve neuron reconstruction errors. Finally, we demonstrate that CMNs can be used to identify subcellular compartments and the cell types of neuron reconstructions.</p>
</abstract>
<counts>
<page-count count="23"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1">
<title>Introduction</title>
<p>Advances in volume electron microscopy (VEM) have led to increasingly large 3D images of brain tissue, making manual analysis infeasible<sup><xref ref-type="bibr" rid="c1">1</xref></sup>. Multi-beam scanning electron microscopes<sup><xref ref-type="bibr" rid="c2">2</xref></sup> and transmission electron microscopes equipped with fast camera arrays can now generate data sets exceeding 100 TB<sup><xref ref-type="bibr" rid="c3">3</xref></sup>, a development which was fortunately accompanied by substantial progress in neuron reconstruction<sup><xref ref-type="bibr" rid="c4">4</xref>&#x2013;<xref ref-type="bibr" rid="c9">9</xref></sup> and the automatic analysis of synapses<sup><xref ref-type="bibr" rid="c10">10</xref>&#x2013;<xref ref-type="bibr" rid="c12">12</xref></sup>. These advances enable now automatic morphology analyses on the neuron (fragment) scale, which were previously restricted to direct segmentation error detection<sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c13">13</xref></sup>, or used manual skeletons with data-specific hand-crafted features<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c14">14</xref>,<xref ref-type="bibr" rid="c15">15</xref></sup>. Cell types, and other biological properties that can be inferred from the morphology of a neuron, are required for the interpretation of a connectome<sup><xref ref-type="bibr" rid="c11">11</xref></sup> and can also constrain automatic neuron reconstruction itself<sup><xref ref-type="bibr" rid="c16">16</xref></sup>.</p>
<p>Outside of the field of connectomics, many approaches have been developed for automated 3D shape analysis, including multi-view 2D projection based neural network models<sup><xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c18">18</xref></sup> as well as voxel- and point cloud-occupancy based 3D networks<sup><xref ref-type="bibr" rid="c19">19</xref>,<xref ref-type="bibr" rid="c20">20</xref></sup>. Interestingly, projection based models appear to often outperform 3D architectures, possibly because of higher-resolution input due to decreased model complexity<sup><xref ref-type="bibr" rid="c17">17</xref></sup>.</p>
<p>Here, we present cellular morphology neural networks (CMNs), which use multi-view projections to enable the supervised and unsupervised analysis of cell fragments of arbitrary size while retaining high-resolution. First, we demonstrate that CMNs can be used to automate morphology feature extraction itself, by inferring low-dimensional embeddings, dubbed &#x201C;Neuron2vec&#x201D;, through unsupervised triplet-loss training<sup><xref ref-type="bibr" rid="c21">21</xref>,<xref ref-type="bibr" rid="c22">22</xref></sup>. Second, we apply them to the supervised classification of glia cells and use these data to demonstrate the feasibility of a simple top-down false merger resolution strategy. Third, we identify neuronal cell types and compartments, outperforming methods with hand-crafted features based on skeleton representations on the same data, and finally perform high-resolution cell surface segmentation to identify dendritic spines.</p>
</sec>
<sec id="s2">
<title>Results</title>
<sec id="s2a">
<title>Cellular morphology learning networks</title>
<p>CMNs are convolutional neural networks (CNNs) optimized for the analysis of multi-channel 2D projections of cell reconstructions, inspired by multi-view CNNs for the classification of objects fitting into projections from one rendering site<sup><xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c18">18</xref></sup>.</p>
<p>Su et al.<sup><xref ref-type="bibr" rid="c17">17</xref></sup> rendered views of an entire object, potentially sacrificing crucial detail when applied to reconstructions of entire neurons (or very large objects in general), which can have processes as thin as 50 nm extending over millimeters<sup><xref ref-type="bibr" rid="c23">23</xref></sup>. To address this problem, we used a sampling algorithm that homogeneously probes an entire cell at many locations (Methods; <xref ref-type="fig" rid="fig1">Fig. 1a,b</xref>) that are then analyzed either individually or in combination by a CMN. The used neuron reconstructions were from a songbird basal ganglia data set and consisted of flood-filling network (FFN) created supervoxels (SV)<sup><xref ref-type="bibr" rid="c24">24</xref></sup>, which were agglomerated to super-SVs (SSV)<sup><xref ref-type="bibr" rid="c8">8</xref></sup>, representing the current best combination of SVs to a single neuron. A rectangular field of view (FoV) was chosen for the projection to capture the elongated shape of most neuronal arbors better. Additionally, we incorporated image channels beyond the cell shape (here represented through depth-map projections) of the same rendering perspective. This extension allows the CMN to analyze the geometry and density of objects contained in a cell, such as mitochondria and other organelles (<xref ref-type="fig" rid="fig1">Fig. 1c-d</xref>).</p>
<fig id="fig1" position="float" fig-type="figure">
<label>Fig. 1</label>
<caption><p>Multi-view generation for a given neurite reconstruction and model architecture. <bold>a</bold> FFN reconstruction and agglomeration of a dendrite, with SVs individually colored. <bold>b</bold> Multi-view projection locations were sampled and <bold>c</bold> used as center for the 2D projections. Each location was rendered from two different perspectives, one orthogonal to the 1st and 2nd principal component (p.c.), the other one rotated by &#x03C6; &#x003D; 90&#x00B0;around the 1st p.c., i.e. orthogonal to the 1st and 3rd p.c&#x002E;&#x002E; <bold>d</bold> The model architecture for the general N-view case consisted of seven 2D CNN layers that extract view features, three fully connected layers (MLP), and a final softmax layer that computes class-wise probabilities (C classes). Scale bars are 10 &#x03BC;m in <bold>a</bold> and 2 &#x03BC;m in <bold>d</bold>.</p></caption>
<graphic xlink:href="364034_fig1.tif"/>
</fig>
<p>We implemented the view rendering engine with OpenGL and the neural network models using the ElektroNN neural network library (<ext-link ext-link-type="uri" xlink:href="http://www.elektronn.org">www.elektronn.org</ext-link>). The models were trained using standard loss optimization procedures (Adam or stochastic gradient descent) on various supervised and unsupervised tasks, which are described in the following to demonstrate the versatility of the approach.</p>
</sec>
<sec id="s2b">
<title>Neuron2Vec embedding</title>
<p>Supervised models require often hard to obtain manual ground truth, making alternative objective functions and models which exploit intrinsic properties of the underlying data instead desirable. We trained a CMN using triplet loss<sup><xref ref-type="bibr" rid="c22">22</xref></sup> to learn a latent space (embedding; dimension d&#x003D;25) of single renderings based on prior information about the similarity of three inputs <italic>x<sub>ref</sub></italic>, <italic>x</italic><sub>&#x002B;</sub> and <italic>x</italic><sub>&#x2013;</sub>. At every location two orthogonal renderings (see above) were generated which served as similar pair (<italic>x<sub>ref</sub></italic> and <italic>x</italic><sub>&#x002B;</sub>). In contrast, a single rendering at a different, randomly sampled location was used as the dissimilar example <italic>x</italic><sub>&#x2013;</sub>. During training, the views were sampled from a subset of all SVs of the VEM data set (Methods).</p>
<p>Axons, or thin processes in general, and dendrite/soma regions can be readily identified based on a color-projection of the embedding using principal component analysis (PCA, <xref ref-type="fig" rid="fig2">Fig. 2a</xref>; top three p.c.&#x2019;s covered 99.2&#x0025; of the variance), whereas, for example, spines covered a wide range (<xref ref-type="fig" rid="fig2">Fig. 2b, c</xref>). We explored the information content of the embedding by fitting a k-nearest neighbor classifier (k&#x003D;5) to the Neuron2vec encoding extracted from a set of neurites with cellular compartment ground truth annotations (total path length: 30.16 mm; 3.05 giga voxel (GV); 4947 &#x03BC;m<sup>3</sup>). The predictive performance on the validation set was particularly low for dendrites (F1-score for dendrite: 0.29, axon: 0.64, soma: 0.84) which, given the proximity between dendrite and soma in the latent space according to the PC analysis (<xref ref-type="fig" rid="fig2">Fig. 2b</xref>), was to be expected (binary classification F1-score for dendrite: 0.51 and axon: 0.78). We therefore took a supervised end-to-end approach and examined ground truth based CMN models.</p>
<fig id="fig2" position="float" fig-type="figure">
<label>Fig. 2</label>
<caption><p>Neuron2Vec embeddings learned by a triplet-CMN. <bold>a</bold> Example triplet-net prediction (top three p.c. used for RGB coloring) of a neuron reconstruction. Insets i-iii show example renderings at three different locations. <bold>b</bold> Resulting PCA transformed latent values for the example views of an axon (i), spine head of a dendrite (ii) and a soma with neurite outgrowth (iii). Scale bars in <bold>a</bold> are 10 &#x03BC;m and 2 &#x03BC;m in the insets.</p></caption>
<graphic xlink:href="364034_fig2.tif"/>
</fig>
</sec>
<sec id="s2c">
<title>Glia detection and top-down segmentation error correction</title>
<p>Due to the dense heavy-metal staining used for VEM data, the acquired images contain all cell types, including astrocytes and other glia types, which are usually not considered for connectomic analysis. Astrocytes have tight surface contacts with neurons to supply them with nutrients and regulate the local environment<sup><xref ref-type="bibr" rid="c25">25</xref></sup>, making astrocyte-neurite mergers a problem in automated reconstruction pipelines. While it can be difficult for humans to determine from the raw EM data whether a process belongs to a glia cell (Fig. S1), it seemed straightforward to solve this classification task using the established multi-view representation, due to their distinct shape (<xref ref-type="fig" rid="fig3">Fig. 3a</xref>). A CMN was trained and validated on a set of manually annotated SVs (from 34 neurite reconstructions and 118 glia SVs; 368.74 mm; 16.48 GV; 26,695 &#x03BC;m<sup>3</sup>), achieving an average F1-score of 0.979 (precision: 0.985; recall: 0,974; N&#x003D;9,695 multi-views; Fig. S2). We then explored the effect of varying data context and view resolution on classification performance and found that, as expected, context is crucial (largest context tested: 8 &#x00D7; 4 &#x00D7; 4 &#x03BC;m<sup>3</sup>; F1-score of 0.900 upon 75&#x0025; reduction; Fig. S2), while reduced resolution barely affects the performance of CMNs (F1-score of 0.967 upon 75&#x0025; reduction). In agreement with this, SVs (N<sub>Neuron</sub>: 84; N<sub>Glia</sub>: 85; 27 &#x03BC;m<sup>3</sup>) part of larger automatic neurite reconstructions, i.e. with a bounding box diagonal (BBD) above 8 &#x03BC;m, showed a significantly better performance per SV (F1 score 0.964) in comparison to all SVs (F1 score 0.868; <xref ref-type="fig" rid="fig3">Fig. 3b</xref>).</p>
<fig id="fig3" position="float" fig-type="figure">
<label>Fig. 3</label>
<caption><p>Glia prediction and reconstruction splitting procedure. <bold>a</bold> Each SV (unique coloring) of a neurite reconstruction was classified as glia (red) or neuron (blue) based on the CMN output. <bold>b</bold> Corresponding neurite length (BBDs) distribution for 169 randomly drawn SVs (light grey) vs. SVs with classification errors (dark grey). Arrows indicate errors of SVs with a BBD &#x2265; 10 &#x03BC;m (see Fig. S3 for example renderings). <bold>c</bold> SSV-graph representation (spring-layout) of a glia-predicted neurite reconstruction (SV volume represented by node size). <bold>d</bold> Connected component (CC) meshes and SSV graph indicating CCs after the splitting procedure. <bold>e</bold> Boxplot (box: mean, Q1-Q3; whiskers: 1.5 IQR) of SV classification performance with unit weights (light grey; mean: 0.938; sample standard deviation, s.d.: 0.039) and volume weighted (dark grey; mean: 0.981; s.d.: 0.027) splitting performance of 12 neurites. Scale bars are 10 &#x03BC;m for <bold>a</bold>, <bold>b</bold> and 20 &#x03BC;m for <bold>d</bold>.</p></caption>
<graphic xlink:href="364034_fig3.tif"/>
</fig>
<p>Can we exploit the excellent glia classification results to reduce the rate of false mergers in a segmentation? The naive solution would be the simple removal of all SV classified as glia fragments, which could however induce large-scale false splits in the case of false positive classifications (Fig. S4f). We therefore developed a more sophisticated splitting heuristic, ensuring that large neuron fragments remained connected in the presence of small misclassifications.</p>
<p>The agglomeration of SVs can be represented as a graph, connecting adjacent SVs (nodes) of the same SSV (neurite) with edges. In this graph glia predictions were used as node properties to identify connected glia and neuron components with their respective sizes. Sufficiently big (BBD &#x2265; 8.0 &#x03BC;m) glia components were removed and added to the glia graph, while small glia components (BBD&#x003C; 8.0 &#x03BC;m) remained in the neuron graph. The resulting connected components of the graphs were stored as individual glia and neuron SSVs, respectively (<xref ref-type="fig" rid="fig3">Fig. 3d</xref>). We evaluated the SV labels of 12 neurite reconstructions with predicted glia mergers (N<sub>Neuron</sub>: 321 SV; N<sub>Glia</sub>: 295 SV; 4181 &#x03BC;m<sup>3</sup>; Fig. S4) after the splitting procedure and found a significantly better classification performance when including the SV volume compared to unit weights (F1-score of 0.995 vs 0.937; Wilcoxon rank-sum test: <italic>p</italic> &#x003D; 0.028 &#x003C; <italic>&#x03B1;</italic> &#x003D; 0.05 with N&#x003D;12). Using this approach 882 splits were introduced in 181 neurites with a total of 3,866,049 SVs classified as neuron (606,922 &#x03BC;m<sup>3</sup>) and 2,154,960 as glia (110,342 &#x03BC;m<sup>3</sup>), yielding an astrocyte volume fraction of about 0.154. It should be noted that the EM data set was prepared in a way to preserve the ECS, which may affect the morphology of the glial processes<sup><xref ref-type="bibr" rid="c26">26</xref></sup>.</p>
<p>We then attempted to reconstruct the 27 astrocytes identified in the data set starting at their somata and assigning each glial fragment to the closest astrocyte soma (see Methods), justified by reports that astrocytes establish roughly spherical, non-overlapping territories<sup><xref ref-type="bibr" rid="c27">27</xref></sup>. While the CMN-based glia identification appears in principle promising (see Fig. S5 for an analysis of automatically extracted glia-blood vessel contacts), our approach likely merged arbors from other glia cells, due to them reaching into the data set from the outside.</p>
</sec>
<sec id="s2d">
<title>Neuron type classification</title>
<p>Similar to glia cells, many neuronal types can be identified based on their morphology<sup><xref ref-type="bibr" rid="c14">14</xref></sup>, a feature that was used well before connectivity-based methods<sup><xref ref-type="bibr" rid="c28">28</xref></sup> and other approaches (reviewed in <sup><xref ref-type="bibr" rid="c29">29</xref></sup>). We recently demonstrated on the same songbird data set that a feature-based method with random forest classifiers (RFCs) can be used to identify the four main cell types (excitatory axons (EA), medium spiny neurons (MSN), pallidal-like neurons (GP) and interneurons (INT)<sup><xref ref-type="bibr" rid="c11">11</xref></sup>. However, manual neurite skeletons, as well as hand-designed feature vectors were required as basis for this classification method.</p>
<p>In contrast to the classification of astrocytes, many neuronal types are indistinguishable when using only a local view (i.e. spatially focused) of a neurite (F1-scores for N&#x003D;1: 0.885 and after majority vote: 0.884; training set: 145.98 mm, 17.21 GV, 27,872 &#x03BC;m<sup>3</sup>; validation set: 65.04 mm, 7.31 GV, 11,843 &#x03BC;m<sup>3</sup>). A simple solution would be to increase the FoV for a single view, capturing the entire extent of a neuron, similar to the proposed object classification method by Su et al.<sup><xref ref-type="bibr" rid="c17">17</xref></sup>. However, this approach reduces the resolution for a given view-size, obscuring potentially important details. Alternatively, local views from different locations can be sampled at random and combined to a global representation (multi-views of size N, further called N-views; <xref ref-type="fig" rid="fig4">Fig. 4a</xref>) by the neural network model. This approach does not sacrifice resolution and increased the classification performance substantially (N&#x003D;10 views: F1-score of 0.987, and 0.968 on SSV; <xref ref-type="fig" rid="fig4">Fig. 4b</xref>). It should be noted that lower resolution, zoomed out views might still be beneficial, which was albeit not explored further by us. Interestingly, a greater number of sample locations did not necessarily increase performance further (F1-score for N&#x003D;60: 0.984, SSV: 0.966; <xref ref-type="fig" rid="fig4">Fig. 4b</xref>), possibly due to increasing model complexity and decreasing diversity of the training data through fewer independent view samples per neuron. This result was consistent with the observation that the N-view F1-scores throughout all Ns were significantly lower without shuffling (views were analyzed in the order they were created) (<xref ref-type="fig" rid="fig4">Fig. 4c</xref>), which led to spatially correlated N-view sets, i.e. they likely consisted of views from a single compartment type only. For all N, additional models were trained to predict sets of views without cell organelle channels, which, in agreement with our previous results using RFCs<sup><xref ref-type="bibr" rid="c11">11</xref></sup>, reduced the F1-score substantially (e.g. F1-score reduction of 0.10 for N&#x003D;20; <xref ref-type="fig" rid="fig4">Fig. 4b</xref>). Interestingly, cell organelles have shown to be especially crucial for the correct cell type prediction of reconstructions that contained only few N-views (<xref ref-type="fig" rid="fig4">Fig. 4b</xref>).</p>
<fig id="fig4" position="float" fig-type="figure">
<label>Fig. 4</label>
<caption><p>Cell type inference using CMNs. <bold>a</bold> N-view fingerprint of a neuron reconstruction. <bold>b</bold> Weighted F1-score of cell types with and without cell organelles (no c.o.). Light grey and light blue represent performance of individual N-views (class support for N&#x003D;1: EA: 16,774, MSN: 90,662, GP: 3958, INT: 8806; N&#x003D;10: EA: 1658, MSN: 9048, GP: 394, INT: 880; N&#x003D;20: EA: 812, MSN: 4514, GP: 196, INT: 440; N&#x003D;40: EA: 388, MSN: 2245, GP: 97, INT: 220; N&#x003D;60: EA: 251, MSN 1489, GP: 65, INT: 146); dark tones represent the average F1-score on neurite level after majority voting (class support EA: 60, MSN: 39). <bold>c</bold> F1-score for shuffled (dark grey) and unshuffled (light grey) N-view stacks (class support as in <bold>b</bold>). Scale bar is 10 &#x03BC;m.</p></caption>
<graphic xlink:href="364034_fig4.tif"/>
</fig>
</sec>
<sec id="s2e">
<title>Cellular compartment identification</title>
<p>We next attempted to analyze the FFN neuron segmentation by identifying subcellular compartments at single multi-view locations (axon, dendrite and soma; <xref ref-type="fig" rid="fig5">Fig. 5a</xref>).</p>
<p>Similar to the glia model, a reduction of the original FoV of 8 &#x00D7; 4 &#x00D7; 4 &#x03BC;m<sup>3</sup> to 2 &#x00D7; 1 &#x00D7; 1 &#x03BC;m<sup>3</sup> reduced the performance (F1-score on validation set of 0.913 vs. 0.996 with full res.), while a 4-fold downsampling of the multi-views had almost no effect (reduction of 0.014, <xref ref-type="fig" rid="fig5">Fig. 5b</xref>). Exclusion of the cell organelle information reduced the F1-score by 0.085 compared to full resolution multi-views. Not surprisingly, the performance of the soma class was barely affected by this or any other changes to the input, whereas the discrimination between axons and dendrites was strongly dependent on organelle information (<xref ref-type="fig" rid="fig5">Fig. 5a</xref>).</p>
<p>The performance of the CMN approach was compared with the skeleton-based RF classification developed before by us<sup><xref ref-type="bibr" rid="c11">11</xref></sup> on a set of 28 manually annotated reconstructions (20.75 mm; 1.31 GV; 2130 &#x03BC;m<sup>3</sup>). Two different FoVs (implemented as maximum skeleton traversal distances for the RFC approach, RFC-4, with 4 &#x03BC;m and RFC-8 with 8 &#x03BC;m) were tested with morphology features extracted from these FoVs (see Methods) and fitted to the same training data as the CMN (path length 30.16 mm; 3.05 GV; 4947 &#x03BC;m<sup>3</sup>). We further evaluated the RFC models (RFC<sup>&#x22C6;</sup>-4, RFC<sup>&#x22C6;</sup>-8) on test data including entire axons and dendrites, to exclude that performance differences originate from the slightly different window-size selection methods (skeleton traversal distance vs. multi-view FoV) or fragmented reconstructions of somata (Fig. S6c). Although the CMN (2 views; <xref ref-type="fig" rid="fig5">Fig. 5b</xref>) was operating on less context (multi-views were based on a 8 &#x03BC;m &#x00D7; 4 &#x03BC;m &#x00D7; 4 &#x03BC;m subvolume vs. a maximum possible subvolume of 16 &#x03BC;m &#x00D7; 16 &#x03BC;m &#x00D7; 16 &#x03BC;m for the RFC-8 model), it outperformed both skeleton-based models and the Neuron2Vec-kNN classifier (<xref ref-type="fig" rid="fig5">Fig 5c</xref>). By applying a sliding window majority vote, performance of most of the approaches was improved further (Fig. S6a,b).</p>
<fig id="fig5" position="float" fig-type="figure">
<label>Fig. 5</label>
<caption><p>CMN-prediction of subcellular compartments. <bold>a</bold> The 3,909 rendering locations of the reconstruction were predicted as axon, dendrite or soma. Local errors are indicated as follows: <sup>&#x22C6;&#x22C6;</sup> indicate locations where vesicle clouds were falsely mapped to the cell (see inlay); <sup>&#x22C6;&#x22C6;</sup> indicate branch points in axons where synaptic junctions occurred without vesicle clouds; <sup>&#x22C6;&#x22C6;&#x22C6;</sup> indicate false dendrite predictions at neurite outgrowths. <bold>b</bold> Performance on the validation set (axon: 1,078; dendrite: 1,823; soma: 6,139 multi-views) for different inputs sorted by the number of input pixels. Left to right: FoV reduction by image cropping (3/8 on each side); resolution reduction by 4x downsampling; input views without cell object channels; FoV reduction by cropping (1/4); resolution reduction by 2x downsampling; only the view perpendicular to the 1st and 2nd p.c. was used; FoV reduction by cropping (1/8); FoV reduction by cropping (1/16); input views were binarized; both views were used at full resolution (256 &#x00D7; 128 px). <bold>c</bold> Comparison of skeleton and multi-view-based classifications measured on a skeleton node test set (color-code as in b) of 28 SSVs (N<sub>axon</sub>: 29,243; N<sub>dendrite</sub>: 38,170; N<sub>soma</sub>: 32,702). <sup>&#x22C6;</sup> indicates that the RFC model was trained on binary data (axon vs. dendrite) only. The CNN model was the 2-view model evaluated in <bold>b</bold>. Scale bars are 10 &#x03BC;m.</p></caption>
<graphic xlink:href="364034_fig5.tif"/>
</fig>
</sec>
<sec id="s2f">
<title>High-resolution semantic segmentation of neurite surfaces</title>
<p>The so far described classification of neurites is restricted in its spatial resolution to the minimum size of at least one view rendering (8&#x00D7;4&#x00D7;4 &#x03BC;m). This property makes the approach suitable for the identification of larger neuronal compartments (axons, dendrites and somata), which even benefits from large spatial context (<xref ref-type="fig" rid="fig5">Fig. 5b</xref>), but does not allow semantic segmentation of the morphology of a cell at sub-micron resolution. Higher resolution classification requires a dense analysis of the rendered views. Similar to the approach taken by Boulch et al.<sup><xref ref-type="bibr" rid="c30">30</xref></sup>, we solved the resulting image-to-surface mapping problem by rendering the cell views with spatially subdivided unique colors<sup><xref ref-type="bibr" rid="c31">31</xref></sup>, thereby creating an efficient reversible mapping between the 2D view space and the 3D surface (<xref ref-type="fig" rid="fig6">Fig. 6a-c</xref>). We then trained a VGG13 and FCN<sup><xref ref-type="bibr" rid="c32">32</xref>,<xref ref-type="bibr" rid="c33">33</xref></sup> based model on the identification of dendritic spines (<xref ref-type="fig" rid="fig6">Fig. 6d,e</xref>; the training set contained five MSN reconstructions; 12.59 mm; 1.01 GV; 1652 &#x03BC;m<sup>3</sup>), a classification problem that was previously solved on manually traced skeletons, which made the automated identification of spines easy<sup><xref ref-type="bibr" rid="c11">11</xref>,<xref ref-type="bibr" rid="c34">34</xref></sup>. Automatically generated skeletons or surface meshes do not have the advantage that many skeleton endings are dendritic spines, which is likely a result of the implicit knowledge of human annotators. Instead of evaluating the classification performance on sampled locations, as done before by us <sup><xref ref-type="bibr" rid="c11">11</xref></sup> (vertex-based evaluation in suppl. Text S1), we tested the performance on a test set of 182 manually annotated synaptic contacts, that were morphologically classified as a spinous synapse (N&#x003D;88) or dendritic shaft synapse (N&#x003D;94), F1-score 0.978 (prec. 0.978, recall 0.978, F1-score spine head only 0.977; 2 views per location and k&#x003D;20). Interestingly, the classification performance did not improve further with more views (F1-score for k&#x003D;20 and 6 views: 0.978), likely because the PCA view alignment along the dendritic process optimizes already the coverage. Note that the coverage saturates below 1.0, due to non-surface triangles (see <xref ref-type="fig" rid="fig6">Fig. 6c</xref>).</p>
<fig id="fig6" position="float" fig-type="figure">
<label>Fig. 6</label>
<caption><p>Sub-micron resolution semantic segmentation of cellular surfaces. <bold>a</bold> Bottom left: Wire-frame rendering of a FFN reconstructed mesh with mapped organelles. Top Left: Depth map projection with organelle channels. Bottom right: Reversible 2D-to-3D mapping based on unique color rendering. Top right: Dense prediction of semantic pixel labels (here spine head (red), neck (grey), dendritic shaft (black)) on rendered view. <bold>b</bold> Pixel labels are mapped back onto mesh faces through the unique colors, enabling a high-resolution 3D surface classification. <bold>c</bold> Mean ratio of triangle faces covered by all reconstruction multi-views depending on the number of views rendered per location (error bars: s.d.). Scale bars in <bold>a</bold> are 2 &#x03BC;m, 10 &#x03BC;m in <bold>b</bold> and 2 &#x03BC;m in the inset.</p></caption>
<graphic xlink:href="364034_fig6.tif"/>
</fig>
</sec>
</sec>
<sec id="s3">
<title>Discussion</title>
<p>We demonstrated that CMNs are a versatile tool to analyze reconstruction fragments or entire cell reconstructions. Our approach adapts to the neurons&#x2019; sparse coverage of the 3D volume by using distributed 2D multi-view projections instead of dense 3D models that end up processing many empty voxels<sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c13">13</xref>,<xref ref-type="bibr" rid="c17">17</xref>,<xref ref-type="bibr" rid="c19">19</xref></sup>. While mainly developed for the analysis of neuron reconstructions, our proposed view-sampling method seems generally beneficial for the analysis of 3D objects that span large distances but still require high-resolution representations, as demonstrated by SnapNet which was developed independently for the semantic labeling of point clouds<sup><xref ref-type="bibr" rid="c30">30</xref></sup>.</p>
<p>By combining the concept of CMNs with unsupervised triplet loss training<sup><xref ref-type="bibr" rid="c21">21</xref></sup>, we created Neuron2Vec embeddings, that could serve as the basis for an unbiased morphological comparison of cells without requiring hand-designed features, or allow neuron-database queries using example neurites<sup><xref ref-type="bibr" rid="c15">15</xref></sup>. Additionally, the embedding can be used for visualization purposes e.g. through coloring (<xref ref-type="fig" rid="fig2">Fig. 2</xref>), making morphologically different, cellular regions salient. The relatively poor k-NN classification performance on the triplet-loss embedding needs to be further explored and compared to alternative unsupervised training paradigms (e.g. autoencoders<sup><xref ref-type="bibr" rid="c35">35</xref>,<xref ref-type="bibr" rid="c36">36</xref></sup> or generative query networks<sup><xref ref-type="bibr" rid="c37">37</xref></sup>), since an unbiased extraction of cellular features appears to be beneficial in light of the otherwise excellent supervised CMN training results. CMNs showed improved performance in cell compartment classification in comparison to the previously used hand-designed features<sup><xref ref-type="bibr" rid="c11">11</xref></sup> and are likely more generalizable, since the morphology does not need to be parametrized first.</p>
<p>We used the classification results of the glia CMN for top-down neurite segmentation, where we evaluated so far only the splitting of falsely merged cells. However, the other identified cell types and neuronal compartments could be used in a similar way, or used as input to a graph cut segmentation algorithm, as proposed by Krasowski et al.<sup><xref ref-type="bibr" rid="c16">16</xref></sup>. Another application could be to directly evaluate the shape-plausibility of neurite fragments and detection of errors<sup><xref ref-type="bibr" rid="c5">5</xref>,<xref ref-type="bibr" rid="c13">13</xref></sup>, or to estimate the probability that separate SVs should be combined<sup><xref ref-type="bibr" rid="c38">38</xref></sup>. Especially applications requiring high-resolution segmentation (e.g. the localization of reconstruction errors) should benefit from the cell surface analysis that we used here for the classification of postsynaptic dendritic morphology with excellent performance (F1-score 0.98). PointNets<sup><xref ref-type="bibr" rid="c20">20</xref></sup> or PointCNNs<sup><xref ref-type="bibr" rid="c39">39</xref></sup>, which can operate directly on mesh vertex data, might be an alternative, but their effectiveness for neuronal morphology classification remains to be demonstrated and compared to the projection-based CMNs.</p>
</sec>
<sec id="s4">
<title>Methods</title>
<sec id="s4a">
<title>EM data and used segmentation</title>
<p>The analyzed EM data set (Area X, adult male zebra finch, &#x003E;120 days post hatching) was acquired by JK through serial block-face scanning electron microscopy and had an extent of 96 &#x00D7; 98 &#x00D7; 114 &#x03BC;m<sup>3</sup> with an xyz-resolution of 9 &#x00D7; 9 &#x00D7; 20 nm<sup><xref ref-type="bibr" rid="c3">3</xref></sup>. It contains about 664 GV (10664 &#x00D7; 10914 &#x00D7; 5701 voxel). Meshes and skeletons were based on a FFN segmentation by MJ and VJ<sup><xref ref-type="bibr" rid="c8">8</xref></sup>, including the over segmentation (all SVs) and the post-agglomeration SV-graph (defines SSVs).</p>
</sec>
<sec id="s4b">
<title>Local scene rendering</title>
<p>We used SV triangle meshes to efficiently render depth-maps with PyOpenGL (<ext-link ext-link-type="uri" xlink:href="http://pyopengl.sourceforge.net/">http://pyopengl.sourceforge.net/</ext-link>) and OSMesa (required for off screen rendering). For rendering, the modelview matrix was rotated such that the first axis of the view was parallel to the main principal component of the object and clipped to an extent of 8 &#x00D7; 4 &#x00D7; 4 &#x03BC;m<sup>3</sup>. The rendering had a throughput of about 3 multi-views/s (see Tab. S1) on a single CPU core, making it scalable in a cloud - or high-performance computing environment. We explicitly chose software rendering, since the view generation on the GPU was overall less economical, likely due to the overhead of main memory to GPU memory data transfers and higher availability of CPU cores.</p>
<p>PCA was applied to a subset (0.125) of the vertices within the clipping box which yielded the axes with highest variance (<italic>x</italic>, <italic>y</italic> and <italic>z</italic>; decreasing variance) in good approximation. In this system, <italic>xy</italic> and <italic>xz</italic> represent the planes with the highest spatial variance, allowing alignment with the elongated neurite structure. By rotating around the <italic>x</italic>-axis, orthographic depth-map projections of size 256 &#x00D7; 128 pixels were rendered and stored as unsigned 8-bit integer. For every SV, rendering locations were obtained by calculating its vertex density &#x03C1; in a grid of voxels with size 2 &#x00D7; 2 &#x00D7; 2 &#x03BC;m<sup>3</sup>. In order to not oversample dense regions, the center coordinate of every voxel with &#x03C1; &#x003E; 0 was used to calculate the mean of the vertices within a radius of 1 &#x03BC;m. The resulting set of coordinates, effectively a downsampled point cloud, was stored as the SVs rendering locations.</p>
</sec>
<sec id="s4c">
<title>Rendering cell organelles</title>
<p>Three additional channels were generated next to the 2D depth-map views which contained the rendering of mapped cell organelles. Only cell organelles (mitochondrion: MI; synaptic junction: SJ; vesicle cloud: VC) with a relative overlap with SSVs above or within a certain threshold range (MI: &#x003E; 0.5, VC: &#x003E; 0.5, SJ: 0.2&#x2013;0.8) were kept. Only objects with a minimum size (number of voxels) were taken into account when calculating the overlap (SJ: 498; VC: 1584; MI: 2786).</p>
<p>Meshes for the associated objects were extracted from a Gaussian-smoothed (&#x03C3; &#x003D; 1), distance-transformed (<ext-link ext-link-type="uri" xlink:href="https://ukoethe.github.io/vigra/">https://ukoethe.github.io/vigra/</ext-link>) binary 3D mask with marching cubes (contour value of 0; scikit-image <ext-link ext-link-type="uri" xlink:href="http://scikit-image.org/">http://scikit-image.org/</ext-link>).</p>
<p>The cell object meshes were rendered from the same perspective as the corresponding SV views. In summary, two orthogonal views of 256 &#x00D7; 128 px with four channels each were rendered per location.</p>
</sec>
<sec id="s4d">
<title>Automatic skeletonization of cell reconstructions</title>
<p>The skeletons of SV (provided by MJ and VJ and created using the TAESAR<sup><xref ref-type="bibr" rid="c40">40</xref></sup> algorithm) belonging to an SSV were combined iteratively. Edges between the spatially closest pair of nodes of all connected components were repeatedly added until only a single connected component remained.</p>
<p>We decreased the average edge length in the skeleton representations to approx. 150 nm by removing skeleton nodes (ignoring branch and end nodes). Nodes of degree 2 were removed and replaced by a single edge if the summed length &#x03BB; of the adjacent edges was below a threshold (<italic>&#x03BB;</italic>&#x2264;50 nm) or if the dot product of their edges was higher than 0.8 in combination with <italic>&#x03BB;</italic> &#x003C; 500 nm.</p>
<p>At every node, the cell radius was estimated by the median of the distance to the ten nearest vertices of the mesh. Total path lengths were calculated as the sum of all edges.</p>
</sec>
<sec id="s4e">
<title>Multi-view models for type classification</title>
<p>Multi-views were sampled from the joint set of SV meshes of anentire SSV, and renderings generated at locations as described above.</p>
<p>To overcome RAM limitations, large SSVs (&#x003E;10<sup>4</sup> SVs) were processed as subgraphs, defined by a breadth-first-search (extending 40 nodes) on the SV graph and starting at each SV. Multi-views were generated from the joint meshes of the 40 SVs at the sampled locations of the source SV.</p>
<p>The multi-view CNNs used seven convolutional layers (number of filters, filter size, max-pooling size), each followed by a max-pooling layer, three fully connected layers and a soft-max layer:</p>
<list list-type="simple">
<list-item><label>-</label><p>conv. L1: (13, (1, 5, 5), (1, 2, 2))</p></list-item>
<list-item><label>-</label><p>conv. L2: (17, (1, 5, 5), (1, 2, 2))</p></list-item>
<list-item><label>-</label><p>conv. L3: (21, (1, 4, 4), (1, 2, 2))</p></list-item>
<list-item><label>-</label><p>conv. L4: (25, (1, 4, 4), (1, 2, 2))</p></list-item>
<list-item><label>-</label><p>conv. L5: (29, (1, 2, 2), (1, 2, 2))</p></list-item>
<list-item><label>-</label><p>conv. L6: (30, (1, 1, 1), (1, 2, 2))</p></list-item>
<list-item><label>-</label><p>conv. L7: (31, (1, 1, 1), (1, 1, 1))</p></list-item>
<list-item><label>-</label><p>f.c. L1: 50 neurons</p></list-item>
<list-item><label>-</label><p>f.c. L2: 30 neurons</p></list-item>
<list-item><label>-</label><p>f.c. L3: 3 neurons</p></list-item>
<list-item><label>-</label><p>softmax-layer</p></list-item>
</list>
<p>The input to a model contained multi-views with either one or four channels (c) and with shape ((b, 20), (f, c), (z, 2), (x, 256), (y, 128)), with batch size b, initial filter or channels f and spatial axes z, x and y. Note the auxiliary z-axis which was introduced to share filters for all views (z-filter size of 1). The models were trained using backpropagation with mini-batches (samples were drawn uniformly). During training, the ordering of the two views was inverted with probability <italic>p</italic> &#x003D; 0.5.</p>
<p>If not stated otherwise, hyper-parameters were chosen to be:</p>
<list list-type="simple">
<list-item><label>-</label><p>batch size: 20</p></list-item>
<list-item><label>-</label><p>dropout rate <sup><xref ref-type="bibr" rid="c41">41</xref></sup> : 0.1</p></list-item>
<list-item><label>-</label><p>activation function: ReLu</p></list-item>
<list-item><label>-</label><p>Adam optimizer <sup><xref ref-type="bibr" rid="c42">42</xref></sup></p></list-item>
<list-item><label>-</label><p>learning rate: 1 &#x00B7; 10<sup>&#x2212;4</sup></p></list-item>
<list-item><label>-</label><p>momentum &#x03B2; <sub>1,2</sub>: 0.9, 0.99</p></list-item>
<list-item><label>-</label><p>weight decay: 5 &#x00B7; 10<sup>&#x2212;4</sup></p></list-item>
</list>
<p>A model with two output classes and one input channel had 35,770 trainable parameters and a computational cost of 2.4 Giga Ops. 350,000 training iterations (with batch size of 20) on a GeForce GTX 980Ti took 20.1h, which was about 0.0103s per sample. Inference of 2,182 samples took 16.33s, approx. 0.0075s per sample (see Table S1).</p>
</sec>
<sec id="s4f">
<title>Neuron2vec embedding</title>
<p>The architecture of the CMN encoder was used to learn a projection from the single view space &#x0211D;<sup>256&#x00B7;128&#x00B7;4</sup> to a lower dimensional (<italic>d</italic> &#x003D; 25) latent space (embedding) &#x0211D;<sup><italic>d</italic></sup> based on the triplet loss described in <sup><xref ref-type="bibr" rid="c22">22</xref></sup>. The objective function was to keep the distance of the reference <italic>x<sub>ref</sub></italic> to the similar input <italic>x</italic><sub>&#x002B;</sub> below the distance of <italic>x<sub>ref</sub></italic> to the dissimilar input <italic>x</italic><sub>&#x2013;</sub>. For the two similar views <italic>x<sub>ref</sub></italic> and <italic>x</italic><sub>&#x002B;</sub> we used the two orthogonal views of the same rendering location, while the dissimilar view <italic>x</italic><sub>&#x2013;</sub> was sampled randomly from a different SV.</p>
<p>Oversampling of soma views or vice versa undersampling of axons, due to their volume differences, was prevented by linear weighting according to the SSV bounding box diagonal (BBD; 0: 0-8 &#x03BC;m, 1: 9-28 &#x03BC;m, 2: 29-38 &#x03BC;m, 3: 39-48 &#x03BC;m, 5: &#x003E;49 &#x03BC;m). The network&#x2019;s objective function was defined as <italic>L</italic><sub><italic>&#x03B1;</italic>&#x003E;0</sub> &#x003D; <italic>&#x03B1;</italic> and <italic>L</italic><sub><italic>&#x03B1;</italic>&#x2264;0</sub> &#x003D; 0, with <italic>&#x03B1;</italic> &#x003D; <italic>r</italic><sub>&#x002B;</sub> &#x2013; <italic>r</italic><sub>&#x2013;</sub> &#x002B; <italic>&#x03BB;</italic>, <italic>r</italic><sub>&#x002B;</sub> &#x003D; | <italic>&#x0078;&#x0303;<sub>ref</sub></italic> &#x2013; <italic>&#x0078;&#x0303;</italic><sub>&#x002B;</sub>|<sub>2</sub> and <italic>r</italic><sub>&#x2013;</sub> &#x003D; |<italic>&#x0078;&#x0303;<sub>ref</sub></italic> &#x2013; <italic>&#x0078;&#x0303;</italic><sub>&#x2013;</sub>|<sub>2</sub> with <italic>&#x03BB;</italic> &#x003D; 0.2 being a regularization parameter to avoid convergence to the trivial null-vector solution, whereby <italic>&#x0078;&#x0303;</italic> &#x2282; &#x0211D;<sup><italic>d</italic></sup> represents the latent space of the triplet net with <italic>d</italic> &#x003D; 25. The subcellular compartment classification performance was evaluated by fitting a kNN classifier to a manually annotated set of multi-views (same as in &#x201C;Cellular compartment identification&#x201D;)</p>
<p>In total, a random subset of 2,319,510 neuron SV, as predicted by the glia model (see &#x201C;Glia detection and top-down segmentation&#x201D;), were processed (5,095,015 multi-views) to extract their embedding vectors (only using the first of the two views). A PCA was performed on the global set and applied to all view embedding vectors of a SSV. By taking only the first three principal components, every multi-view location was assigned an RGB value. The mesh was colored according to the nearest view color of every vertex.</p>
</sec>
<sec id="s4g">
<title>Glia classification</title>
<p>For the glia classification model, only the depth maps of the SV were given as input. The training set contained 88,022 multi-views (N<sub>neuron</sub>: 69,068; N<sub>glia</sub>: 18,954) and the validation set 9,695 (N<sub>neuron</sub>: 7588; N<sub>glia</sub>: 2107). Neuron views from the subcellular compartment ground truth (see next section) were extended by two additional axon reconstructions and used as samples for the negative class (31.29 mm 3.08 GV; 4989 &#x03BC;m<sup>3</sup>). Glia views were generated from 118 manually annotated glia SV (path length: 337.45; 13.40 GV; 21706 &#x03BC;m<sup>3</sup>). The performance was calculated based on multi-views and measured as F1-score. The classification threshold &#x03B8; was set to the optimal F1-score on the validation set. The best model was retrained on training and validation set and applied to the whole data set. To remove background structures not connected to the central object of interest, connected component analysis was performed on the 2D multi-view images, followed by masking of the unconnected pixels.</p>
<p>The SVs were then classified by calculating the mean of all its multi-view predictions and thresholding with &#x03B8;. In addition, at least 70&#x0025; of all multi-view predictions of a SV had to be glia for the assignment of this label.</p>
<p>Classification performance was measured by manually annotating 169 SV (N<sub>glial</sub>: 85; N<sub>neuron</sub>: 84; N<sub>BBD&#x003C;8&#x03BC;m</sub>: 57; N<sub>BBD&#x2265;8&#x03BC;m</sub>: 112). These SVs were sampled from 20,000 randomly drawn SSVs (training and validation samples were excluded), weighted by the number of views per SV. Only SSVs within the segmentation data set bounding box [470, 730, 30] to [10200, 10200, 5670] were taken into account.</p>
</sec>
<sec id="s4h">
<title>Top-down glia splitting</title>
<p>In order to split glia fragments from neurons, a CC analysis was applied to the glia and neuron SV-graphs identified by the SV glia predictions of every SSV. The glia and neuron CC size was estimated by calculating their BBD and glia CCs with a BBD &#x2265; 8.0 &#x03BC;m were removed from the SV-graph first. The remaining, small glia CCs (BBD&#x003C;8.0 &#x03BC;m) were assigned the neuron class and the BBD was re-evaluated. Neuron CCs with a BBD below 8 &#x03BC;m were removed and added to the glia graph. The purpose of this was to bridge small false glia/neuron predictions and thereby avoid false splits.</p>
<p>Splitting performance was evaluated on twelve randomly drawn SSVs with at least one split introduced during the splitting procedure. Inspected SVs were sorted by volume and the average inspected volume coverage was 0.905 (proportion of inspected SVs weighted by their volume).</p>
</sec>
<sec id="s4i">
<title>Neuron type classification</title>
<p>The 2-views were re-used to construct the N-views by the following procedure: The collection of all M 2-views of an SSV was split into 2<italic>M/N</italic> random sets (drawn without replacement) each of size <italic>N</italic>. If 2<italic>M</italic> &#x003C; <italic>N</italic>, the set was filled by randomly drawing from the existing views.</p>
<p>The model architecture was identical to the model used for glia classification, except for a reduced batch size, a dropout rate of 0.08 and a learning rate schedule defined as exponential decay, with decay rate of 0.98 per 1000 steps. The input shape was (1, 4, N, 256, 128).</p>
<p>We used 402 manually traced (skeletonized) cells to identify their corresponding SSV which were split into training (N<sub>train</sub>: 301; N<sub>EA</sub>: 177, N<sub>MSN</sub>: 114, N<sub>GP</sub>: 6, N<sub>INT</sub>: 6) and test set (N<sub>test</sub>: 101; N<sub>EA</sub>: 60, N<sub>MSN</sub>: 39, N<sub>GP</sub>: 3, N<sub>INT</sub>: 2) with the following labels, that correspond to the broad biological classes found in this data set (excitatory axons (EA), medium spiny neurons (MSN), pallidal like neurons (GP), interneurons (INT)).</p>
<p>During batch creation while training, the N-views were generated by randomly drawing from the corresponding SSV views. Every batch contained an equal number of SSVs for each class. The classification was performed using arg-max on the output of the softmax layer and the majority vote of the corresponding N-view classifications was used for SSV classification. Performance was evaluated on N-views and on a SSV level after majority vote. The latter was measured as unweighted F1-score of the classes EA and MSN due to the little support for INT and GP (EA: 60, MSN: 39, GP: 3, INT: 2).</p>
</sec>
<sec id="s4j">
<title>Subcellular compartment classification</title>
<p>The cellular compartments of 34 neurites were manually annotated and axon, dendrite and soma views generated, which were split into a training set (N<sub>train</sub>: 80,370 views; N<sub>dendrite</sub>: 10,004; N<sub>axon</sub>: 41,424; N<sub>soma</sub>: 28,942) and a validation set (N<sub>validation</sub>: 9,04&#x00B0;; N<sub>dendrite</sub>: 1,078 N<sub>axon</sub>: 1,823 N<sub>soma</sub>: 6139). During training we applied class weights for loss computation to address imbalances in their frequency (dendrite: 2, axon: 1, soma: 1). Performance was measured with the F1-score of the multi-view classification using argmax on the softmax output. The best model was again retrained on the whole ground truth data for the data set prediction.</p>
<p>Classification of arbitrary locations within neurons was performed by assigning the unclassified location the label of the closest classified location (Voronoi partitioning with Euclidean distance).</p>
<p><italic>Comparison with RFC.</italic> All SSV skeleton nodes were manually labeled (N<sub>axon</sub>: 29,243; N<sub>dendrite</sub>: 38,170; N<sub>soma</sub>: 32,702) by a human expert using KNOSSOS. In order to enable a direct comparison between the two models, the skeleton-node locations were used for the extraction of the hand-designed features. CMN and kNN predictions were mapped to the skeleton nodes using the nearest neighbors on the multi-view locations. As in <sup><xref ref-type="bibr" rid="c11">11</xref></sup>, hand-designed features were computed for every skeleton node (context of 4,000 nm and 8,000 nm maximal traversed path length from the source node). Only properties of nodes visited during the traversal were taken into account for the source node statistics.</p>
<p>A total of 22 features were extracted from the collected properties at each node: Mean and standard deviation (s.d.) and histogram (10 bins) of the encountered node diameters, mean of node degrees, number of cell organelles and mean and s.d. of their size for mitochondria, synaptic junctions, vesicle clouds. The RFC was trained on the same training data as the CMN to classify each node as axon, dendrite or soma using argmax on the resulting class probability.</p>
<p>A majority vote was carried out on the set of node/multi-view predictions collected within a 12.5 &#x03BC;m traversal depth for every skeleton node for the RFC and CMN/kNN, respectively.</p>
</sec>
<sec id="s4k">
<title>High-resolution semantic segmentation of surfaces</title>
<p>The training data was generated by rendering multi-views (5 different perspectives) from the rendering locations of 5 reconstructions (training: 24,248 views; validation: 6,062 views) with label-dependent vertex colors. Skeleton nodes were manually annotated as either neck, head, shaft or soma/axon, which were then mapped to the mesh vertices with Voronoi-partitioning (Euclidean distance). To smooth label boundaries, each vertex was assigned the majority label of 40 vertices found by a BFS on the vertex graph of the reconstruction. Graph edges were added between vertices with a distance of up to 120 nm. Only rendering locations with annotated skeleton nodes within 2 &#x03BC;m were taken into account.</p>
<p>We used the FCN-VGG13 architecture<sup><xref ref-type="bibr" rid="c33">33</xref></sup> (adopted from <ext-link ext-link-type="uri" xlink:href="https://github.com/pochih/FCN-pvtorch">https://github.com/pochih/FCN-pvtorch</ext-link>) to perform pixel-wise multi-class (neck, head, shaft, background and axon/soma) classification on single views with four channels (cell, mitochondria, synaptic junctions and vesicle cloud shapes). It was trained using backpropagation with mini-batches (images were flipped in x/y with probability <italic>p</italic> &#x003D; 0.5), Adam optimizer (&#x03B2; <sub>1,2</sub>: 0.9, 0.999; weight decay: 5 &#x00B7; 10<sup>&#x2212;4</sup>), initial learning rate of 4 &#x00B7; 10<sup>&#x2212;3</sup> (exponential decay with 0.99) and Lov&#x00E1;sz-Softmax loss<sup><xref ref-type="bibr" rid="c43">43</xref></sup>.</p>
<p>To assign pixel labels back to the mesh vertices an additional view of the faces was rendered using color buffering with a unique ID per face. This allowed to perform a majority vote of all collected labels corresponding to a single vertex as classification. Subsequently, we applied a kNN classification to propagate predicted labels to vertices which were not covered by the rendered color map.</p>
<p>The synapse test set was generated as a random set of head and shaft synapses collected from 4 different reconstructions. The dendritic tree for the per-vertex evaluation was manually annotated on skeleton node level which were then propagated to the mesh vertices as described above for the GT generation.</p>
</sec>
<sec id="s4l">
<title>Reconstructing glial cells</title>
<p>SV graph edges were added between the sample locations of the collection of all splitted glia SV by identifying the k-nearest neighbors (k&#x003D;15, maximum distance: 10 &#x03BC;m; weighted by euclidean distance).</p>
<p>27 somata of putative astrocytes were identified in the data set and every glia SV was assigned to its closest soma (shortest path using Dijkstra&#x2019;s algorithm).</p>
</sec>
<sec id="s4m">
<title>Bloodvessel prediction</title>
<p>The input data (zxy ordering) for the bloodvessel CNN was downsampled by a factor of 8 in all dimensions. A cube of size (256, 437, 287) was densely labeled using KNOSSOS to obtain training data. The used network had the following architecture:</p>
<list list-type="simple">
<list-item><label>-</label><p>conv. L1: (24, (1, 6, 6), (1, 2, 2))</p></list-item>
<list-item><label>-</label><p>conv. L2: (27, (1, 5, 5), (1, 2, 2))</p></list-item>
<list-item><label>-</label><p>conv. L3: (30, (1, 5, 5), (1, 1, 1))</p></list-item>
<list-item><label>-</label><p>conv. L4: (33, (1, 4, 4), (2, 1, 1))</p></list-item>
<list-item><label>-</label><p>conv. L5: (36, (3, 4, 4), (1, 1, 1))</p></list-item>
<list-item><label>-</label><p>conv. L6: (39, (3, 4, 4), (1, 1, 1))</p></list-item>
<list-item><label>-</label><p>conv. L7: (42, (2, 4, 4), (1, 1, 1))</p></list-item>
<list-item><label>-</label><p>conv. L8: (45, (1, 4, 4), (1, 1, 1))</p></list-item>
<list-item><label>-</label><p>conv. L9: (48, (1, 4, 4), (1, 1, 1))</p></list-item>
<list-item><label>-</label><p>conv. L10: (48, (1, 1, 1), (1, 1, 1))</p></list-item>
<list-item><label>-</label><p>conv. L11: (2, (1, 1, 1), (1, 1, 1))</p></list-item>
<list-item><label>-</label><p>softmax-layer</p></list-item>
</list>
<p>The dense predictions of the dataset were thresholded at 0.98. Meshes were created as described above (&#x201C;Rendering cell organelles&#x201D;).</p>
</sec>
<sec id="s4n">
<title>Computing infrastructure</title>
<p>The used parallel computing environment consisted of 18 nodes, each equipped with 20 cores (Intel(R) Xeon(R) CPU E5-2660 v3 &#x0040; 2.60GHz), 2 GeForce GTX 980Ti and 256 GB of RAM. Compute jobs were managed using openmp in combination with SGE QSUB.</p>
</sec>
<sec id="s4o" sec-type="availability">
<title>Code availability</title>
<p>The used network architectures, classes for handling the inference, processing and storage of the segmentation data can be found in the SyConn GitHub repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/StructuralNeurobiologvLab/SvConn/">https://github.com/StructuralNeurobiologvLab/SvConn/</ext-link>).</p>
</sec>
</sec>
</body>
<back>
<ack>
<title>Acknowledgements</title>
<p>We would like to thank W. Denk for enabling this work in his department and many helpful discussions, M. Fee and M. Kormacheva for helpful comments, the KNOSSOS team for support, R. Saxena and M. Shumliakivska for code contributions, M. Killinger and M. Drawitsch for help with the ELEKTRONN library and J. Rogowska and M. Shumliakivska for proofreading tracings and ground truth generation.</p>
</ack>
<sec>
<title>Author contributions statement</title>
<p>PS performed and designed experiments and wrote the manuscript.</p>
<p>MJ and VJ contributed the data set FFN segmentation and wrote the manuscript.</p>
<p>SD contributed code for the experiments.</p>
<p>JK designed experiments and wrote the manuscript.</p>
</sec>
<sec>
<title>Conflict of interest declaration</title>
<p>JK holds shares of ariadne-service gmbh.</p>
</sec>
<ref-list>
<ref id="c1"><label>1.</label><mixed-citation publication-type="journal"><string-name><surname>Helmstaedter</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>Connectomic reconstruction of the inner plexiform layer in the mouse retina</article-title>. <source>Nature</source> <fpage>500</fpage>, (<year>2013</year>).</mixed-citation></ref>
<ref id="c2"><label>2.</label><mixed-citation publication-type="journal"><string-name><surname>Eberle</surname>, <given-names>A. L.</given-names></string-name> <etal>et al.</etal> <article-title>High-resolution, high-throughput imaging with a multibeam scanning electron microscope</article-title>. <source>J. Microsc</source>. <volume>259</volume>, <fpage>114</fpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c3"><label>3.</label><mixed-citation publication-type="journal"><string-name><surname>Kornfeld</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Denk</surname>, <given-names>W.</given-names></string-name> <article-title>Progress and remaining challenges in high-throughput volume electron microscopy</article-title>. <source>Curr. Opin. Neurobiol</source>. <volume>50</volume>, <fpage>261</fpage>&#x2013;<lpage>267</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c4"><label>4.</label><mixed-citation publication-type="journal"><string-name><surname>Beier</surname>, <given-names>T.</given-names></string-name> <etal>et al.</etal> <article-title>Multicut brings automated neurite segmentation closer to human performance</article-title>. <source>Nat. Methods</source> <volume>14</volume>, <fpage>101</fpage>&#x2013;<lpage>102</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c5"><label>5.</label><mixed-citation publication-type="book"><string-name><surname>Zung</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Tartavull</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Lee</surname>, <given-names>K.</given-names></string-name> &#x0026; <string-name><surname>Seung</surname>, <given-names>H. S.</given-names></string-name> <chapter-title>An Error Detection and Correction <italic>Framework for Connectomics</italic></chapter-title>. in <source>Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</source> <year>2017</year>, 4-9 <month>December</month> 2017, <publisher-loc>Long Beach, CA, USA</publisher-loc> <fpage>6821</fpage>&#x2013;<lpage>6832</lpage> (2017).</mixed-citation></ref>
<ref id="c6"><label>6.</label><mixed-citation publication-type="journal"><string-name><surname>Meirovitch</surname>, <given-names>Y.</given-names></string-name> <etal>et al.</etal> <article-title>A Multi-Pass Approach to Large-Scale Connectomics</article-title>. <source>CoRR</source> abs/1612.02120, (<year>2016</year>).</mixed-citation></ref>
<ref id="c7"><label>7.</label><mixed-citation publication-type="journal"><string-name><surname>Wolf</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Schott</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Kothe</surname>, <given-names>U.</given-names></string-name> &#x0026; <string-name><surname>Hamprecht</surname>, <given-names>F.</given-names></string-name> <article-title>Learned Watershed: End-to-End Learning of Seeded Segmentation</article-title>. in <source>2017 IEEE International Conference on Computer Vision (ICCV)</source> (<year>2017</year>). doi:<pub-id pub-id-type="doi">10.1109/iccv.2017.222</pub-id></mixed-citation></ref>
<ref id="c8"><label>8.</label><mixed-citation publication-type="journal"><string-name><surname>Januszewski</surname>, <given-names>M.</given-names></string-name> <etal>et al.</etal> <article-title>High-Precision Automated Reconstruction of Neurons with Flood-filling Networks</article-title>. <source>bioRxiv</source> (<year>2017</year>). doi:<pub-id pub-id-type="doi">10.1101/200675</pub-id></mixed-citation></ref>
<ref id="c9"><label>9.</label><mixed-citation publication-type="journal"><string-name><surname>Funke</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title>Large Scale Image Segmentation with Structured Loss based Deep Learning for Connectome Reconstruction</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source> (<year>2018</year>). doi:<pub-id pub-id-type="doi">10.1109/TPAMI.2018.2835450</pub-id></mixed-citation></ref>
<ref id="c10"><label>10.</label><mixed-citation publication-type="journal"><string-name><surname>Staffier</surname>, <given-names>B.</given-names></string-name> <etal>et al.</etal> <article-title>SynEM, automated synapse detection for connectomics</article-title>. <source>Elife</source> <fpage>6</fpage>, (<year>2017</year>).</mixed-citation></ref>
<ref id="c11"><label>11.</label><mixed-citation publication-type="journal"><string-name><surname>Dorkenwald</surname>, <given-names>S.</given-names></string-name> <etal>et al.</etal> <article-title>Automated synaptic connectivity inference for volume electron microscopy</article-title>. <source>Nat. Methods</source> <volume>14</volume>, <fpage>435</fpage>&#x2013;<lpage>442</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c12"><label>12.</label><mixed-citation publication-type="journal"><string-name><surname>Heinrich</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Funke</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Pape</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Nunez-Iglesias</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Saalfeld</surname>, <given-names>S.</given-names></string-name> <source>Synaptic Cleft Segmentation in Non-Isotropic Volume Electron Microscopy of the Complete Drosophila Brain</source>. (<year>2018</year>).</mixed-citation></ref>
<ref id="c13"><label>13.</label><mixed-citation publication-type="other"><string-name><surname>Rolnick</surname>, <given-names>D.</given-names></string-name> <etal>et al.</etal> <article-title>Morphological Error Detection in 3D Segmentations</article-title>. <source>CoRR</source> (<year>2017</year>).</mixed-citation></ref>
<ref id="c14"><label>14.</label><mixed-citation publication-type="other"><string-name><surname>Zhao</surname>, <given-names>T.</given-names></string-name> &#x0026; <string-name><surname>Plaza</surname>, <given-names>S. M.</given-names></string-name> <article-title>Automatic Neuron Type Identification by Neurite Localization in the Drosophila Medulla</article-title>. <source>CoRR</source> (<year>2014</year>).</mixed-citation></ref>
<ref id="c15"><label>15.</label><mixed-citation publication-type="journal"><string-name><surname>Costa</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Manton</surname>, <given-names>J. D.</given-names></string-name>, <string-name><surname>Ostrovsky</surname>, <given-names>A. D.</given-names></string-name>, <string-name><surname>Prohaska</surname>, <given-names>S.</given-names></string-name> &#x0026; <string-name><surname>Jefferis</surname>, <given-names>G. S.</given-names></string-name> <article-title>X. E. NBLAST: Rapid, Sensitive Comparison of Neuronal Structure and Construction of Neuron Family Databases</article-title>. <source>Neuron</source> <volume>91</volume>, <fpage>293</fpage>&#x2013;<lpage>311</lpage> (<year>2016</year>).</mixed-citation></ref>
<ref id="c16"><label>16.</label><mixed-citation publication-type="journal"><string-name><surname>Krasowski</surname>, <given-names>N. E.</given-names></string-name> <etal>et al.</etal> <article-title>Neuron Segmentation With High-Level Biological Priors</article-title>. <source>IEEE Trans. Med. Imaging</source> <volume>37</volume>, <fpage>829</fpage>&#x2013;<lpage>839</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c17"><label>17.</label><mixed-citation publication-type="journal"><string-name><surname>Su</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Maji</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Kalogerakis</surname>, <given-names>E.</given-names></string-name> &#x0026; <string-name><surname>Learned-Miller</surname>, <given-names>E.</given-names></string-name> <article-title>Multi-view Convolutional Neural Networks for 3D Shape Recognition. in 2015</article-title> <source>IEEE International Conference on Computer Vision (ICCV)</source> (<year>2015</year>). doi:<pub-id pub-id-type="doi">10.1109/iccv.2015.114</pub-id></mixed-citation></ref>
<ref id="c18"><label>18.</label><mixed-citation publication-type="journal"><string-name><surname>Qi</surname>, <given-names>C. R.</given-names></string-name> <etal>et al.</etal> <article-title>Volumetric and Multi-view CNNs for Object Classification on 3D Data. in 2016</article-title> <source>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source> (<year>2016</year>). doi:<pub-id pub-id-type="doi">10.1109/cvpr.2016.609</pub-id></mixed-citation></ref>
<ref id="c19"><label>19.</label><mixed-citation publication-type="journal"><string-name><surname>Wu</surname>, <given-names>Z.</given-names></string-name> <etal>et al.</etal> <article-title>3D ShapeNets: A deep representation for volumetric shapes. in 2015</article-title> <source>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source> (<year>2015</year>). doi:<pub-id pub-id-type="doi">10.1109/cvpr.2015.7298801</pub-id></mixed-citation></ref>
<ref id="c20"><label>20.</label><mixed-citation publication-type="journal"><string-name><surname>Qi</surname>, <given-names>C. R.</given-names></string-name>, <string-name><surname>Su</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Kaichun</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>Guibas</surname>, <given-names>L. J.</given-names></string-name> <article-title>PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation. in 2017</article-title> <source>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source> (<year>2017</year>). doi:<pub-id pub-id-type="doi">10.1109/cvpr.2017.16</pub-id></mixed-citation></ref>
<ref id="c21"><label>21.</label><mixed-citation publication-type="journal"><string-name><surname>Hoffer</surname>, <given-names>E.</given-names></string-name> &#x0026; <string-name><surname>Ailon</surname>, <given-names>N.</given-names></string-name> <article-title>Deep Metric Learning Using Triplet Network</article-title>. in <source>Lecture Notes in Computer Science</source> <fpage>84</fpage>&#x2013;<lpage>92</lpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c22"><label>22.</label><mixed-citation publication-type="book"><string-name><surname>Wang</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <chapter-title>Learning Fine-Grained Image Similarity with Deep Ranking. in <italic>Proceedings of the 2014</italic></chapter-title> <source>IEEE Conference on Computer Vision and Pattern Recognition</source> <fpage>1386</fpage>&#x2013;<lpage>1393</lpage> (<publisher-name>IEEE Computer Society</publisher-name>, <year>2014</year>).</mixed-citation></ref>
<ref id="c23"><label>23.</label><mixed-citation publication-type="journal"><string-name><surname>Helmstaedter</surname>, <given-names>M.</given-names></string-name> <article-title>Cellular-resolution connectomics: challenges of dense neural circuit reconstruction</article-title>. <source>Nat. Methods</source> <volume>10</volume>, <fpage>501</fpage>&#x2013;<lpage>507</lpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c24"><label>24.</label><mixed-citation publication-type="journal"><string-name><surname>Ren</surname>, <given-names>X.</given-names></string-name> &#x0026; <string-name><surname>Malik</surname>, <given-names>J.</given-names></string-name> <article-title>Learning a classification model for segmentation</article-title>. in <source>Proceedings Ninth IEEE International Conference on Computer Vision</source> (<year>2003</year>). doi: <pub-id pub-id-type="doi">10.1109/iccv.2003.1238308</pub-id></mixed-citation></ref>
<ref id="c25"><label>25.</label><mixed-citation publication-type="journal"><string-name><surname>Fields</surname>, <given-names>R. D.</given-names></string-name> &#x0026; <string-name><surname>Stevens-Graham</surname>, <given-names>B.</given-names></string-name> <article-title>New insights into neuron-glia communication</article-title>. <source>Science</source> <volume>298</volume>, <fpage>556</fpage>&#x2013;<lpage>562</lpage> (<year>2002</year>).</mixed-citation></ref>
<ref id="c26"><label>26.</label><mixed-citation publication-type="journal"><string-name><surname>Pallotto</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Watkins</surname>, <given-names>P. V.</given-names></string-name>, <string-name><surname>Fubara</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Singer</surname>, <given-names>J. H.</given-names></string-name> &#x0026; <string-name><surname>Briggman</surname>, <given-names>K. L.</given-names></string-name> <article-title>Extracellular space preservation aids the connectomic analysis of neural circuits</article-title>. <source>Elife</source> <fpage>4</fpage>, (<year>2015</year>).</mixed-citation></ref>
<ref id="c27"><label>27.</label><mixed-citation publication-type="journal"><string-name><surname>Nedergaard</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ransom</surname>, <given-names>B.</given-names></string-name> &#x0026; <string-name><surname>Goldman</surname>, <given-names>S. A.</given-names></string-name> <article-title>New roles for astrocytes: redefining the functional architecture of the brain</article-title>. <source>Trends Neurosci</source>. <volume>26</volume>, <fpage>523</fpage>&#x2013;<lpage>530</lpage> (<year>2003</year>).</mixed-citation></ref>
<ref id="c28"><label>28.</label><mixed-citation publication-type="journal"><string-name><surname>Jonas</surname>, <given-names>E.</given-names></string-name> &#x0026; <string-name><surname>Kording</surname>, <given-names>K.</given-names></string-name> <article-title>Automatic discovery of cell types and microcircuitry from neural connectomics</article-title>. <source>Elife</source> <volume>4</volume>, <fpage>e04250</fpage> (<year>2015</year>).</mixed-citation></ref>
<ref id="c29"><label>29.</label><mixed-citation publication-type="journal"><string-name><surname>Zeng</surname>, <given-names>H.</given-names></string-name> &#x0026; <string-name><surname>Sanes</surname>, <given-names>J. R.</given-names></string-name> <article-title>Neuronal cell-type classification: challenges, opportunities and the path forward</article-title>. <source>Nat. Rev. Neurosci</source>. <volume>18</volume>, <fpage>530</fpage>&#x2013;<lpage>546</lpage> (<year>2017</year>).</mixed-citation></ref>
<ref id="c30"><label>30.</label><mixed-citation publication-type="journal"><string-name><surname>Boulch</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Guerry</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Le Saux</surname>, <given-names>B.</given-names></string-name> &#x0026; <string-name><surname>Audebert</surname>, <given-names>N.</given-names></string-name> <article-title>SnapNet: 3D point cloud semantic labeling with 2D deep segmentation networks</article-title>. <source>Comput. Graph</source>. <volume>71</volume>, <fpage>189</fpage>&#x2013;<lpage>198</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c31"><label>31.</label><mixed-citation publication-type="journal"><string-name><surname>Schneider</surname>, <given-names>B.-O.</given-names></string-name> <article-title>Method and apparatus for improved graphics picking using auxiliary buffer information</article-title>. <source>US Patent</source> (<year>2000</year>).</mixed-citation></ref>
<ref id="c32"><label>32.</label><mixed-citation publication-type="journal"><string-name><surname>Simonyan</surname>, <given-names>K.</given-names></string-name> &#x0026; <string-name><surname>Zisserman</surname>, <given-names>A.</given-names></string-name> <source>Very Deep Convolutional Networks for Large-Scale Image Recognition</source>. (<year>2014</year>).</mixed-citation></ref>
<ref id="c33"><label>33.</label><mixed-citation publication-type="journal"><string-name><surname>Long</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Shelhamer</surname>, <given-names>E.</given-names></string-name> &#x0026; <string-name><surname>Darrell</surname>, <given-names>T.</given-names></string-name> <article-title>Fully convolutional networks for semantic segmentation. in 2015</article-title> <source>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source> (<year>2015</year>). doi:<pub-id pub-id-type="doi">10.1109/cvpr.2015.7298965</pub-id></mixed-citation></ref>
<ref id="c34"><label>34.</label><mixed-citation publication-type="journal"><string-name><surname>Kornfeld</surname>, <given-names>J.</given-names></string-name> <etal>et al.</etal> <article-title>EM connectomics reveals axonal target variation in a sequence-generating network</article-title>. <source>Elife</source> <fpage>6</fpage>, (<year>2017</year>).</mixed-citation></ref>
<ref id="c35"><label>35.</label><mixed-citation publication-type="journal"><string-name><surname>Bourlard</surname>, <given-names>H.</given-names></string-name> &#x0026; <string-name><surname>Kamp</surname>, <given-names>Y.</given-names></string-name> <article-title>Auto-association by multilayer perceptrons and singular value decomposition</article-title>. <source>Biol. Cybern</source>. <volume>59</volume>, <fpage>291</fpage>&#x2013;<lpage>294</lpage> (<year>1988</year>).</mixed-citation></ref>
<ref id="c36"><label>36.</label><mixed-citation publication-type="journal"><string-name><surname>Hinton</surname>, <given-names>G. E.</given-names></string-name> &#x0026; <string-name><surname>Zemel</surname>, <given-names>R. S.</given-names></string-name> <article-title>Autoencoders, Minimum Description Length and Helmholtz Free Energy</article-title>. in <source>Advances in Neural Information Processing Systems</source> <fpage>3</fpage>&#x2013;<lpage>10</lpage> (<year>1994</year>).</mixed-citation></ref>
<ref id="c37"><label>37.</label><mixed-citation publication-type="journal"><string-name><surname>Eslami</surname>, <given-names>S. M. A.</given-names></string-name> <etal>et al.</etal> <article-title>Neural scene representation and rendering</article-title>. <source>Science</source> <volume>360</volume>, <fpage>1204</fpage>&#x2013;<lpage>1210</lpage> (<year>2018</year>).</mixed-citation></ref>
<ref id="c38"><label>38.</label><mixed-citation publication-type="journal"><string-name><surname>Nunez-Iglesias</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Kennedy</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Parag</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Shi</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Chklovskii</surname>, <given-names>D. B.</given-names></string-name> <article-title>Machine Learning of Hierarchical Clustering to Segment 2D and 3D Images</article-title>. <source>PLoS One</source> <volume>8</volume>, <fpage>e71715</fpage> (<year>2013</year>).</mixed-citation></ref>
<ref id="c39"><label>39.</label><mixed-citation publication-type="journal"><string-name><surname>Li</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Bu</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>Chen</surname>, <given-names>B.</given-names></string-name> <source>PointCNN</source>. (<year>2018</year>).</mixed-citation></ref>
<ref id="c40"><label>40.</label><mixed-citation publication-type="journal"><string-name><surname>Sato</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Bitter</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Bender</surname>, <given-names>M. A.</given-names></string-name>, <string-name><surname>Kaufman</surname>, <given-names>A. E.</given-names></string-name> &#x0026; <string-name><surname>Nakajima</surname>, <given-names>M.</given-names></string-name> <article-title>TEASAR: tree-structure extraction algorithm for accurate and robust skeletons</article-title>. in <source>Proceedings the Eighth Pacific Conference on Computer Graphics and Applications</source> (<year>2000</year>). doi:<pub-id pub-id-type="doi">10.1109/pccga.2000.883951</pub-id></mixed-citation></ref>
<ref id="c41"><label>41.</label><mixed-citation publication-type="journal"><string-name><surname>Srivastava</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Hinton</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sutskever</surname>, <given-names>I.</given-names></string-name> &#x0026; <string-name><surname>Salakhutdinov</surname>, <given-names>R.</given-names></string-name> <article-title>Dropout: A simple way to prevent neural networks from overfitting</article-title>. <source>J. Mach. Learn. Res</source>. <volume>15</volume>, <fpage>192</fpage>&#x2013;<lpage>1958</lpage> (<year>2014</year>).</mixed-citation></ref>
<ref id="c42"><label>42.</label><mixed-citation publication-type="journal"><string-name><surname>Kingma</surname>, <given-names>D. P.</given-names></string-name> &#x0026; <string-name><surname>Ba</surname>, <given-names>J. L.</given-names></string-name> <article-title>Adam: A Method for Stochastic Optimization</article-title>. <source>CoRR</source> (<year>2014</year>).</mixed-citation></ref>
<ref id="c43"><label>43.</label><mixed-citation publication-type="other"><string-name><surname>Berman</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Triki</surname>, <given-names>A. R.</given-names></string-name> &#x0026; <string-name><surname>Blaschko</surname>, <given-names>M. B.</given-names></string-name> <source>The Lov&#x00E1;sz-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</source>. (<year>2017</year>).</mixed-citation></ref>
</ref-list>
</back>
</article>
