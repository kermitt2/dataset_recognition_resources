<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2d1 20170631//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="article" dtd-version="1.2d1" specific-use="production" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BIORXIV</journal-id>
<journal-title-group>
<journal-title>bioRxiv</journal-title>
<abbrev-journal-title abbrev-type="publisher">bioRxiv</abbrev-journal-title>
</journal-title-group>
<publisher>
<publisher-name>Cold Spring Harbor Laboratory</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1101/040618</article-id>
<article-version>1.1</article-version>
<article-categories>
<subj-group subj-group-type="author-type">
<subject>Regular Article</subject>
</subj-group>
<subj-group subj-group-type="heading">
<subject>New Results</subject>
</subj-group>
<subj-group subj-group-type="hwp-journal-coll">
<subject>Evolutionary Biology</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Analytical results for directional and quadratic selection gradients for log-linear models of fitness functions</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Morrissey</surname><given-names>Michael B.</given-names></name>
<xref ref-type="aff" rid="a1">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Goudie</surname><given-names>I. B. J.</given-names></name>
<xref ref-type="aff" rid="a2">2</xref>
</contrib>
<aff id="a1"><label>1</label><institution>School of Biology, University of St Andrews</institution></aff>
<aff id="a2"><label>2</label><institution>School of Mathematics and Statistics, University of St Andrews</institution></aff>
</contrib-group>
<author-notes>
<corresp>contact email: <email>michael.morrissey@st-andrews.ac.uk</email> phone:&#x002B;44 (0) 1334 463738, fax: &#x002B;44 (0) 1334 463366, post: Dyers Brae House, School of Biology, University of St Andrews, St Andrews, Fife, UK, KY16 9TH</corresp>
<corresp>contact email: <email>ig@st-andrews.ac.uk</email> phone:&#x002B;44 (0) 1334 463705, fax: &#x002B;44 (0) 1334 463748, post: School of Mathematics and Statistics, University of St Andrews St Andrews, Fife, UK, KY16 9SS</corresp>
</author-notes>
<pub-date pub-type="epub">
<year>2016</year>
</pub-date>
<elocation-id>040618</elocation-id>
<history>
<date date-type="received">
<day>22</day>
<month>2</month>
<year>2016</year>
</date>
<date date-type="accepted">
<day>22</day>
<month>2</month>
<year>2016</year>
</date>
</history><permissions><copyright-statement>&#x00A9; 2016, Posted by Cold Spring Harbor Laboratory</copyright-statement>
<copyright-year>2016</copyright-year><license license-type="creative-commons" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link></license-p></license></permissions>
<self-uri xlink:href="040618.pdf" content-type="pdf" xlink:role="full-text"/>
<abstract><title>Abstract</title>
<list list-type="order">
<list-item><p>Established methods for inference about selection gradients involve least-squares regression of fitness on phenotype. While these methods are simple and may generally be quite robust, they do not account well for distributions of fitness.</p></list-item>
<list-item><p>Some progress has previously been made in relating inferences about trait-fitness relationships from generalised linear models to selection gradients in the formal quantitative genetic sense. These approaches involve numerical calculation of average derivatives of relative fitness with respect to phenotype.</p></list-item>
<list-item><p>We present analytical results expressing selection gradients as functions of the coefficients of generalised linear models for fitness in terms of traits. The analytical results allow calculation of univariate and multivariate directional, quadratic, and correlational selection gradients from log-linear and log-quadratic models.</p></list-item>
<list-item><p>The results should be quite generally applicable in selection analysis. They apply to any generalised linear model with a log link function. Furthermore, we show how they apply to some situations including inference of selection from (molecular) paternity data, capture-mark-recapture analysis, and survival analysis. Finally, the results may bridge some gaps between typical approaches in empirical and theoretical studies of natural selection.</p></list-item>
</list>
</abstract>
<kwd-group kwd-group-type="author"><title>Keywords</title>
<kwd>natural selection</kwd>
<kwd>selection gradients</kwd>
<kwd>fitness</kwd>
<kwd>generalised linear model</kwd>
<kwd>capture-mark-recapture</kwd>
<kwd>survival analysis</kwd>
</kwd-group>
<counts>
<page-count count="21"/>
</counts>
</article-meta>
</front>
<body>
<sec id="s1"><label>1</label><title>Introduction</title>
<p>The characterisation of natural selection, especially in the wild, has long been a major research theme in evolutionary ecology and evolutionary quantitative genetics (<xref rid="c2" ref-type="bibr">Endler, 1986</xref>; <xref rid="c7" ref-type="bibr">Kingsolver et al., 2001</xref>; <xref rid="c9" ref-type="bibr">Lande &#x0026; Arnold, 1983</xref>; <xref rid="c12" ref-type="bibr">Manly, 1985</xref>; <xref rid="c19" ref-type="bibr">Weldon, 1901</xref>). In recent decades, regression-based approaches have been used to obtain direct selection gradients (especially following <xref rid="c9" ref-type="bibr">Lande &#x0026; Arnold 1983</xref>), which represent the direct effects of traits on fitness. These, and related, measures of selection have an explicit justification in quantitative genetic theory (<xref rid="c8" ref-type="bibr">Lande, 1979</xref>; <xref rid="c9" ref-type="bibr">Lande &#x0026; Arnold, 1983</xref>), which provides the basis for comparison among traits, taxa, etc., and ultimately allows meta-analysis (e.g., <xref rid="c7" ref-type="bibr">Kingsolver et al. 2001</xref>). Selection gradients can characterise both directional selection and aspects of non-linear selection, and so are a very powerful concept in evolutionary quantitative genetics.</p>
<p>Formally, the selection gradient is the vector of partial derivatives of relative fitness with respect to phenotype, averaged over the distribution of phenotype observed in a population. Given an arbitrary function <bold><italic>W</italic>(z)</bold> for expected fitness of a (multivariate) phenotype <bold>z</bold>, a general expression for the directional selection gradient <bold><italic>&#x03B2;</italic></bold> is
<disp-formula id="eqn1"><alternatives><graphic xlink:href="040618_eqn1.gif"/></alternatives></disp-formula>
where <italic>p</italic>(<bold>z</bold>) is the probability density function of phenotype, and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline1.gif"/></alternatives></inline-formula> is mean fitness. Mean fitness can itself be obtained by &#x222B;<italic>W</italic>(<bold>z</bold>)<italic>p</italic>(<bold>z</bold>)<italic>d</italic><bold>z</bold>. A quadratic selection gradient can also be defined as the average curvature (similarly standardised), rather than the average slope, of the relative fitness function,
<disp-formula id="eqn2"><alternatives><graphic xlink:href="040618_eqn2.gif"/></alternatives></disp-formula></p>
<p>The directional selection gradient has a direct relationship to evolutionary change, assuming that breeding values (the additive genetic component of individual phenotype, <xref rid="c3" ref-type="bibr">Falconer 1960</xref>) are multivariate normally-distributed, following the <xref rid="c8" ref-type="bibr">Lande (1979)</xref> equation
<disp-formula id="eqn3"><alternatives><graphic xlink:href="040618_eqn3.gif"/></alternatives></disp-formula>
where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline2.gif"/></alternatives></inline-formula> is per-generation evolutionary change, and <bold>G</bold> is the additive genetic covariance matrix, i.e., the (co)variances among individuals of breeding values. The quadratic selection gradient matrix has direct relationships to the change in the distribution of breeding values due to selection, but not with such simple relationships between generations as for the directional selection gradient and the change in the mean (<xref rid="c9" ref-type="bibr">Lande &#x0026; Arnold, 1983</xref>).</p>
<p>The primary method for obtaining selection gradient estimates has been a simple and robust approach justified in <xref rid="c9" ref-type="bibr">Lande &#x0026; Arnold (1983)</xref>. The method involves least-squares multiple regression of relative fitness, i.e., absolute fitness divided by the mean observed in any comparable group of individuals over a specific period of the life cycle, potentially the entire life cycle, on measures of phenotype. Fitness, or any component of fitness, will typically have highly non-normal residuals in such a regression. Nonetheless, the simple least-squares methods are unbiased (see <xref rid="c4" ref-type="bibr">Geyer &#x0026; Shaw 2010</xref>). However, methods that account for distributions of residuals that arise in regressions involving fitness as a response variable may provide better precision and more reliable statements about uncertainty (i.e., standard errors, p-values, etc.).</p>
<p>Some progress has been made at developing generalised regression model methods for inference of selection gradients. <xref rid="c6" ref-type="bibr">Janzen &#x0026; Stern (1998)</xref> proposed a method for binomial responses (e.g., per-interval survival, mated vs. not mated). The <xref rid="c6" ref-type="bibr">Janzen &#x0026; Stern (1998)</xref> method provides estimates of <bold><italic>&#x03B2;</italic></bold>, and requires fitting a logistic model with linear terms only, calculating the average derivatives at each phenotypic value observed in a sample, and then standardising to the relative fitness scale. <xref rid="c14" ref-type="bibr">Morrissey &#x0026; Sakrejda (2013)</xref> expanded Janzen &#x0026; Stern&#x2019;s (1998) basic approach to arbitrary fitness functions (i.e., not necessarily linear) and arbitrary response variable distributions, retaining the basic idea of numerically averaging the slope (and curvature) of the fitness function over the distribution of observed phenotype. <xref rid="c15" ref-type="bibr">Shaw &#x0026; Geyer (2010</xref>) developed a framework for characterising the distributions of fitness (and fitness residuals) that arise in complex life cycles, and also showed how the method could be applied to estimate selection gradients by averaging the slope or curvature of the fitness function over the observed values of phenotype in a sample.</p>
<p>Of the many forms regression analyses of trait-fitness relationships might take, log-linear or log-quadratic models of the relationship between traits and expected absolute fitness may be particularly useful. In generalised linear models, the log link function is often useful and pragmatic. Fitness is a necessarily non-negative quantity, and expected fitness will typically best be modelled as a strictly positive quantity. This will indeed be the case if expected fitness is an exponential function of the sum of the predictors of the regression model, or, equivalently, a log link is used. Also, a log link function is compatible with generalised linear models with various distributions that could be useful for modelling fitness or fitness components. For example, it can be used with the Poisson distribution (counts, e.g., number of mates or offspring), the negative binomial distribution (for counts that are overdispersed relative to the Poisson distribution, potentially including lifetime production of offspring), and the exponential and geometric distributions (e.g., for continuous and discrete measures of longevity). The purpose of this short paper is to investigate the relationships between log-linear and log-quadratic models of fitness functions, and selection gradients.</p>
</sec>
<sec id="s2"><label>2</label><title>Log-linear and log-quadratic fitness functions, and selection gradients</title>
<p>Selection gradients turn out to have very simple relationships to the coefficients of log-linear regression models predicting expected fitness from (potentially multivariate) phenotype. Suppose that there are k traits in the analysis and that the absolute fitness function, <italic>W</italic>(<bold>z</bold>) takes the form
<disp-formula id="eqn4"><alternatives><graphic xlink:href="040618_eqn4.gif"/></alternatives></disp-formula>
where <italic>a</italic> is a log-scale intercept, and the <italic>b</italic><sub><italic>i</italic></sub> are log-scale regression coefficients relating the traits (<italic>z</italic><sub><italic>i</italic></sub>) to expected fitness. The equation for the directional selection gradient (<xref ref-type="disp-formula" rid="eqn1">equation 1</xref>) can then be simplified. Focusing on the selection gradient for a specific trait, i, in a log-linear model of <italic>W</italic>(<bold>z</bold>),
<disp-formula><alternatives><graphic xlink:href="040618_ueqn1.gif"/></alternatives></disp-formula>
and hence
<disp-formula id="eqn5"><alternatives><graphic xlink:href="040618_eqn5.gif"/></alternatives></disp-formula></p>
<p>This result could be quite useful. In any log-linear model regressing expected absolute fitness, or a component of fitness, on trait values, the linear predictor-scale regression coefficients are the directional selection gradients.</p>
<p>The situation is a little bit more complicated if a log-quadratic model is fitted. If <italic>W</italic>(<bold>z</bold>) takes the form
<disp-formula id="eqn6"><alternatives><graphic xlink:href="040618_eqn6.gif"/></alternatives></disp-formula>
i.e., of a log-scale regression model with linear and quadratic terms, plus first-order interactions, then the b<sub>i</sub> coefficients are not necessarily the directional selection gradients, nor are the <italic>g</italic><sub><italic>i</italic></sub> and <italic>g</italic><sub><italic>ij</italic></sub> coefficients the quadratic and correlational selection gradients, as they would be in a least squares analysis following <xref rid="c9" ref-type="bibr">Lande &#x0026; Arnold (1983)</xref>. However, we can use the log-scale quadratic fitness function with the general definitions of selection gradients (<xref ref-type="disp-formula" rid="eqn1">equations 1</xref> and <xref ref-type="disp-formula" rid="eqn2">2</xref>) to obtain analytical solutions for <bold><italic>&#x03B2;</italic></bold> and <bold>&#x03B3;</bold>.</p>
<p>The factor of <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline3.gif"/></alternatives></inline-formula> associated with the quadratic terms in <xref ref-type="disp-formula" rid="eqn6">equation 6</xref> is a potential source of confusion, analogous to that surrounding a similar factor in <xref rid="c9" ref-type="bibr">Lande &#x0026; Arnold&#x2019;s (1983)</xref> paper (see <xref rid="c17" ref-type="bibr">Stinchcombe <italic>et al.</italic> 2008</xref>). In order to obtain the correct values of the <italic>gi</italic> coefficients, the covariate values for quadratic terms should be (1) mean-centred, then (2) squared, and then (3) halved. An alternative analysis is possible, where the squared covariate values are not halved, but the estimated coefficient estimates are doubled (analogous to procedures discussed by <xref rid="c17" ref-type="bibr">Stinchcombe <italic>et al.</italic> 2008</xref>). However, this alternative analysis leads to an additional, and potentially confusing, step in the calculation of standard errors (detailed in the appendix).</p>
<p>Define a vector <bold>b</bold> = (<italic>b</italic><sub>1</sub>,&#x2019;, <italic>b</italic><sub>k</sub>)&#x2032; containing the coefficients of the linear terms in the exponent of the model in <xref ref-type="disp-formula" rid="eqn6">equation 6</xref>, and a matrix <bold>g</bold> = (<italic>g<sub>ij</sub></italic>) containing the coefficients of the corresponding quadratic form. We can then write the fitness function more conveniently in matrix form
<disp-formula id="eqn7a"><alternatives><graphic xlink:href="040618_eqn7a.gif"/></alternatives></disp-formula>
<disp-formula id="eqn7b"><alternatives><graphic xlink:href="040618_eqn7b.gif"/></alternatives></disp-formula></p>
<p>Let <bold>d</bold> be a vector of the expectations of the first order partial derivatives of <italic>W</italic>(<bold>z</bold>) and let <bold>H</bold> be the matrix of expectations of the second order partial derivatives of <italic>W</italic>(<bold>z</bold>). Thus the elements of <bold>d</bold> are <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline4.gif"/></alternatives></inline-formula> and the elements of <bold>H</bold> are <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline5.gif"/></alternatives></inline-formula>. We can now rewrite the expressions for directional and quadratic selection gradients as
<disp-formula id="eqn8"><alternatives><graphic xlink:href="040618_eqn8.gif"/></alternatives></disp-formula>
and
<disp-formula id="eqn9"><alternatives><graphic xlink:href="040618_eqn9.gif"/></alternatives></disp-formula></p>
<p>Differentiating <xref ref-type="disp-formula" rid="eqn7a">equation 7</xref> gives
<disp-formula id="eqn10"><alternatives><graphic xlink:href="040618_eqn10.gif"/></alternatives></disp-formula>
and
<disp-formula id="eqn11"><alternatives><graphic xlink:href="040618_eqn11.gif"/></alternatives></disp-formula></p>
<p>Assume that the phenotype <bold>z</bold> is multivariate normal, with mean <bold><italic>&#x03BC;</italic></bold> and covariance matrix <bold>&#x03A3;</bold>, and denote its probability density by <italic>p</italic><sub><italic>&#x03BC;</italic>,&#x03A3;</sub>(<bold>z</bold>). Provided <italic>e</italic><sup><italic>f</italic>(<bold>z</bold>)</sup> has a finite expectation, the function
<disp-formula id="eqn12"><alternatives><graphic xlink:href="040618_eqn12.gif"/></alternatives></disp-formula>
is a probability density function. Define the matrix <bold>&#x03A9;</bold><sup>&#x2212;1</sup> = <bold>&#x03A3;</bold><sup>&#x2212;1</sup>-<bold>g</bold> and the vector <italic>v</italic> = <italic>&#x03BC;</italic>&#x002B;<bold>&#x03A9;</bold>(<bold>b&#x002B;g&#x03BC;</bold>). We show in the Appendix that <bold>&#x03A9;</bold> is symmetric. Provided it is also positive definite, it is a valid covariance matrix, and, by <xref ref-type="disp-formula" rid="eqnA7">equation A7</xref>,
<disp-formula id="eqn13"><alternatives><graphic xlink:href="040618_eqn13.gif"/></alternatives></disp-formula></p>
<p>As <italic>K</italic> is a probability density function this implies,
<disp-formula id="eqn14"><alternatives><graphic xlink:href="040618_eqn14.gif"/></alternatives></disp-formula></p>
<p>Define <bold>Q</bold><sup>&#x2212;1</sup> = <bold>&#x03A9;</bold><sup>&#x2212;1</sup>&#x03A3; = <bold>I</bold><sub>k</sub>-g<bold>&#x03A3;</bold>. Combining <xref ref-type="disp-formula" rid="eqn8">equations 8</xref>, <xref ref-type="disp-formula" rid="eqn10">10</xref> and <xref ref-type="disp-formula" rid="eqn14">14</xref> yields <bold><italic>&#x03B2;</italic></bold> = <italic>E</italic>[<bold>b&#x002B;gz</bold>], where the expectation is taken with respect to <italic>K</italic>. This is an expectation of a linear function of <bold>z</bold>, and so
<disp-formula id="eqn15"><alternatives><graphic xlink:href="040618_eqn15.gif"/></alternatives></disp-formula>
by use of <xref ref-type="disp-formula" rid="eqnA4">equation A4</xref>.</p>
<p>Combining <xref ref-type="disp-formula" rid="eqn9">equations 9</xref>, <xref ref-type="disp-formula" rid="eqn11">11</xref> and <xref ref-type="disp-formula" rid="eqn14">14</xref> yields <bold><italic>&#x03B3;</italic></bold> = <italic>E</italic>[<bold>g &#x002B; (b &#x002B; gz) (b &#x002B; gz)</bold>&#x2019;], where the expectation is taken with respect to <italic>K</italic>. Hence
<disp-formula id="eqn16"><alternatives><graphic xlink:href="040618_eqn16.gif"/></alternatives></disp-formula>
where we have noted that <bold>g</bold> is symmetric and used <xref ref-type="disp-formula" rid="eqnA4">equation A4</xref>.</p>
<p>In univariate analyses, the matrix machinery necessary for implementing the general formulae in <xref ref-type="disp-formula" rid="eqn15">equations 15</xref> and <xref ref-type="disp-formula" rid="eqn16">16</xref> can be avoided. If the fitness function is <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline6.gif"/></alternatives></inline-formula> (note, again, that the quadratic coefficient is that for centred, then squared, and then halved values of z<xref ref-type="fn" rid="fn1"><sup>1</sup></xref>), and <italic>z</italic> has a mean of <italic>&#x03BC;</italic> and a variance of &#x03C3;<sup>2</sup> and then <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline7.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline8.gif"/></alternatives></inline-formula>. These expressions will hold for any univariate analysis, and can be applied to get mean-standardised, variance-standardised, and unstandardised selection gradients, when appropriate values of and &#x03C3;<sup>2</sup> are used, and applied to log-quadratic models of <italic>W</italic>(<bold><italic>z</italic></bold>) where the phenotypic records have been correspondingly standardised. For the common case where the trait is mean-centred and (unit) variance standardised, the expressions simplify further to <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline9.gif"/></alternatives></inline-formula> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline10.gif"/></alternatives></inline-formula></p>
<p>The equivalence of the regression coefficients of a log-linear fitness model with directional selection gradients (<xref ref-type="disp-formula" rid="eqn5">equation 5</xref>) of course requires that the regression model provides a reasonable description of the relationship between a trait and expected fitness and makes reasonable assumptions about fitness residuals. Otherwise, the relationship is relatively unburdened by assumptions. For example, it does not require any specific distribution of phenotype. The use of selection gradients obtained from log-linear regressions to predict evolution using the Lande equation (<xref ref-type="disp-formula" rid="eqn3">equation 3</xref>) does assume that breeding values are multivariate normal (see <xref rid="c13" ref-type="bibr">Morrissey 2014</xref> for a discussion of selection gradients and associated assumptions about multivariate normality of phenotype and breeding values). The expressions for <bold><italic>&#x03B2;</italic></bold> and <bold><italic>&#x03B3;</italic></bold> given a log-quadratic fitness model (<xref ref-type="disp-formula" rid="eqn15">equations 15</xref> and <xref ref-type="disp-formula" rid="eqn16">16</xref>) do assume multivariate normality of phenotype. <xref ref-type="disp-formula" rid="eqn15">Equations 15</xref> and <xref ref-type="disp-formula" rid="eqn16">16</xref> further require that <bold>&#x03A9;</bold> is positive definite. In univariate analyses, this condition reduces to <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline11.gif"/></alternatives></inline-formula>, implying that the fitness function should not curve upwards too sharply within the range of observed phenotype.</p>
<p>A very convenient feature of the expressions for <bold><italic>&#x03B2;</italic></bold> and <bold><italic>&#x03B3;</italic></bold> in <xref ref-type="disp-formula" rid="eqn5">equations 5</xref>, <xref ref-type="disp-formula" rid="eqn15">15</xref> and <xref ref-type="disp-formula" rid="eqn16">16</xref> is that the model (log) intercept does not influence the selection gradients. This means that the range of modelling techniques that yield selection gradients can be even further expanded. For example, adding fixed and random effects to <xref rid="c9" ref-type="bibr">Lande &#x0026; Arnold&#x2019;s (1983)</xref> least squares analysis will generally result in estimated regression coefficients that are not interpretable as selection gradients. For example, it might be desirable to estimate a single selection gradient across two sexes, if data are limited and sex-differences in selection are not anticipated. In such an analysis, it might seem sensible to include an effect of sex, to account for differences in mean fitness between the sexes. However, such an analysis would not yield correct selection gradients, because the theory underlying the least squares-based regression analysis of selection requires that mean relative fitness is one, and this would not be the case when different strata within an analysis have different intercepts. On the other hand, adding such an effect to a log-scale model of absolute fitness, and then deriving selection gradients using <xref ref-type="disp-formula" rid="eqn5">equations 5</xref>, <xref ref-type="disp-formula" rid="eqn15">15</xref> and <xref ref-type="disp-formula" rid="eqn16">16</xref> will yield correct selection gradients. Other effects, such as random effects to account for individual heterogeneity in expected fitness, beyond that explained by the traits (or correlated, unmeasured traits), will be usable as well, while still retaining the ability to obtain correct selection gradients.</p>
</sec>
<sec id="s3"><label>3</label><title>Statistical uncertainty</title>
<p>The expressions for selection gradients, given the parameters of a log-quadratic fitness function (<xref ref-type="disp-formula" rid="eqn15">equations 15</xref> and <xref ref-type="disp-formula" rid="eqn16">16</xref>) give the selection gradients conditional on the estimated values of <bold>b</bold> and <bold>g</bold>. However, <bold>b</bold> and <bold>g</bold> will not typically be known quantities in empirical studies of natural selection, but rather will be estimates with error. Because <xref ref-type="disp-formula" rid="eqn15">equations 15</xref> and <xref ref-type="disp-formula" rid="eqn16">16</xref> are non-linear functions of one or more regression coefficients, unconditional estimators of <bold><italic>&#x03B2;</italic></bold> and <bold><italic>&#x03B3;</italic></bold> would have to be obtained by integrating the expressions for <bold><italic>&#x03B2;</italic></bold> and <bold><italic>&#x03B3;</italic></bold> over the sampling distributions of the estimated values of <bold>b</bold> and <bold>g</bold>. Such details are not normally considered in calculations of derived parameters (e.g., heritabilities) in evolutionary studies. Such integration could be achieved using approximations, bootstrapping, or MCMC methods. Alternatively, application of <xref ref-type="disp-formula" rid="eqn15">equations 15</xref> and <xref ref-type="disp-formula" rid="eqn16">16</xref> directly to estimated values of b and g may be sufficient in practice. Similarly, while standard errors of the parameters b and g are not directly interpretable as standard errors of corresponding values of <bold><italic>&#x03B2;</italic></bold> and <italic><bold><italic>&#x03B2;</italic></bold></italic>, approximations, bootstrapping, and MCMC methods may all potentially be useful in practice. In particular, approximation of standard errors by a first-order Taylor approximation (the &#x201C;delta method&#x201D;; <xref rid="c11" ref-type="bibr">Lynch &#x0026; Walsh 1998</xref>) may generally be pragmatic. Formulae for approximate standard errors by this method are given in the appendix. For univariate analysis, with phenotype standardised to <italic>&#x03BC;</italic> = 0 and <italic>&#x03C3;</italic><sup>2</sup> = 1, the approximate standard errors of <italic>&#x03B2;</italic> and <italic>&#x03B3;</italic> are given by
<disp-formula id="eqn17"><alternatives><graphic xlink:href="040618_eqn17.gif"/></alternatives></disp-formula>
and
<disp-formula id="eqn18"><alternatives><graphic xlink:href="040618_eqn18.gif"/></alternatives></disp-formula></p>
<p>Where &#x03A3;[b] and &#x03A3;[g] represent the sampling variances of the estimated <italic>b</italic> and <italic>g</italic> terms. These are the squares of their standard errors. &#x03A3;[<italic>b, g</italic>] is the sampling covariance of the <italic>b</italic> and <italic>g</italic> terms. This is not always reported, but can usually be obtained. For example, in R, it can be extracted from a fitted glm object using the function vcov().</p>
<p>We performed a small simulation study to assess the extent of any bias in the estimators <bold><italic>&#x03B2;</italic></bold> and <bold><italic>&#x03B3;</italic></bold> and the adequacy of their standard errors. We simulated univariate directional selection, with values of <italic>b</italic> between &#x2212;0.5 and 0.5, and with <italic>g</italic> = &#x2212;0.5,0 and 0.2. Because <italic>&#x03B2;</italic> and <italic>&#x03B3;</italic> are nonlinear functions of <italic>g</italic>, it is not possible to simultaneously investigate ranges of parameter values with regular intervals of values of both <italic>g</italic> and selection gradients. These values of <italic>g</italic> represent a compromise between investigating a regular range of <italic>g</italic> and <italic>&#x03B3;</italic>. We used a (log) intercept of the fitness function of <italic>a</italic> = 0. We simulated a sample size of 200 individuals. This sample size reflects a very modest-sized study with respect to precision in inference of non-linear selection, and is therefore a useful scenario in which to judge performance of different methods for calculating standard errors. Fitness was simulated as a Poisson variable with expectations defined by the ranges of values of <italic>b</italic> and <italic>g</italic>, and with phenotypes sampled from a standard normal distribution.</p>
<p>Firstly we analysed each simulated dataset using the OLS regression described by <xref rid="c9" ref-type="bibr">Lande &#x0026; Arnold (1983)</xref>, i.e., <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline12.gif"/></alternatives></inline-formula>, using the R function lm(). For the OLS regressions, we calculated standard errors assuming normality using the standard method implemented in the R function summary. lm (), and by case-bootstrapping, by generating 1000 bootstrapped datasets by sampling with replacement, running the OLS regression analysis, and calculating the standard deviation of the bootstrapped selection gradient estimates. Secondly we fitted a Poisson glm with a linear and quadratic terms, using the R function glm(). We then calculated conditional selection gradient estimates using <xref ref-type="disp-formula" rid="eqn15">equations 15</xref> and <xref ref-type="disp-formula" rid="eqn16">16</xref>. We obtained standard errors by using a first-order Taylor series approximation (the &#x201C;delta method&#x201D;; <xref rid="c11" ref-type="bibr">Lynch &#x0026; Walsh 1998</xref>, appendix A1). For each method of obtaining estimates and standard errors, we calculated the standard deviation of replicate simulated estimates. We could thus evaluate the performance of different methods of obtaining standard errors by their ability to reflect this sampling standard deviation. We also calculated mean absolute errors for both estimators of <italic>&#x03B2;</italic> and <italic>&#x03B3;</italic> for all scenarios. Every simulation scenario and associated analysis of selection gradients was repeated 1000 times.</p>
<p>Selection gradient estimates obtained by all three methods were essentially unbiased (<xref ref-type="fig" rid="fig1">figure 1a,d,g,j,m,p</xref>), except for small biases that occurred when the fitness function was very curved. Thus, glm-derived values of selection gradients, conditional on estimated values of <italic>b</italic> and <italic>g</italic> performed very well as estimators of <italic>&#x03B2;</italic> and <italic>&#x03B3;</italic> in our simulations. Similarly, first-order approximations of standard errors of the glm-derived estimates of <italic>&#x03B2;</italic> and <italic>&#x03B3;</italic> closely reflected the simulated standard deviations of the estimators (<xref ref-type="fig" rid="fig1">figure 1</xref>). All methods for obtaining standard errors performed well for estimates of <italic>&#x03B2;</italic> in the pure log-linear selection simulations (<xref ref-type="fig" rid="fig1">figure 1h,k</xref>). OLS standard errors performed reasonably well under most simulation scenarios, except when g was positive (<xref ref-type="fig" rid="fig1">figure 1n,q</xref>); across all scenarios bootstrap standard errors of the OLS estimators outperformed standard OLS standard errors. Mean absolute error of the glm estimators was always smaller than that of the OLS estimators of <italic>&#x03B2;</italic> and <italic>&#x03B3;</italic>. This is unsurprising, as the simulation scheme corresponded closely to the glm model. These results demonstrate the usefulness of the conditional values of <italic>&#x03B2;</italic> and &#x03B3; as estimators, and show that gains in precision and accuracy can be obtained when glm models of fitness functions fit the data well. It remains plausible that the OLS estimators motivated by <xref rid="c9" ref-type="bibr">Lande &#x0026; Arnold&#x2019;s (1983)</xref> work could outperform glm-based analyses in some scenarios.</p>
<fig id="fig1" position="float" fig-type="figure"><label>Figure 1:</label>
<caption><p>Simulation results for the performance of <xref rid="c9" ref-type="bibr">Lande &#x0026; Arnold&#x2019;s (1983)</xref> least squares-based (OLS) estimators (red lines), and log-quadratic (GLM) estimators (blue lines), of directional and quadratic selection gradients. The first column shows bias in estimates of <italic>&#x03B2;</italic> and <italic>&#x03B3;</italic>, where departure from the grey line (the simulated truth) indicates bias. The middle column shows the performance of OLS standard errors (red dashed lines), bootstrap standard errors (red dotted lines), and first-order approximations (blue dashed lines) of the standard errors of the GLM estimators. Ideally, all values of estimated mean standard errors would fall on the simulated standard deviation of their associated estimators, shown as solid lines. The right column shows the mean absolute errors of the OLS and GLM estimators.</p></caption>
<graphic xlink:href="040618_fig1.tif"/>
</fig>
</sec>
<sec id="s4"><label>4</label><title>Other analyses that correspond to log-linear fitness functions</title>
<p>In addition to generalised linear models with log link functions, there may be other cases where models of trait-fitness relationships may correspond to log-linear or log-quadratic fitness functions. In paternity inference, some methods have been proposed wherein the probability that candidate father i is the father of a given offspring is modelled according to
<disp-formula><alternatives><graphic xlink:href="040618_ueqn2.gif"/></alternatives></disp-formula>
and where realised paternities of a given offspring array are then modelled according to a multinomial distribution, potentially integrating over uncertainty in paternity assignments based on molecular data (<xref rid="c5" ref-type="bibr">Hadfield et al., 2006</xref>; <xref rid="c16" ref-type="bibr">Smouse et al., 1999</xref>). When f(z) is a linear function, Smouse, Meagher &#x0026; Korbak (1999; T. Meagher, personal communication) interpreted the analysis as analogous to <xref rid="c9" ref-type="bibr">Lande and Arnold&#x2019;s 1983</xref>, but not necessarily identical. For a linear f (z), this analysis does in fact yield estimates of <bold><italic>&#x03B2;</italic></bold>, and for a quadratic function, directional and quadratic selection gradients can be obtained using <xref ref-type="disp-formula" rid="eqn15">equations 15</xref> and <xref ref-type="disp-formula" rid="eqn16">16</xref>. This can be seen by noting that expected fitness, given phenotype, of candidate fathers for any given offspring array will be, in the log-linear case,
<disp-formula><alternatives><graphic xlink:href="040618_ueqn3.gif"/></alternatives></disp-formula>
where <italic>c</italic> is a constant. In application of the expressions yielding <xref ref-type="disp-formula" rid="eqn5">equation 5</xref>, c appears in both the numerator and the denominator, yielding <bold><italic>&#x03B2;</italic></bold> = <bold>b</bold>.</p>
<p>Another case where our formulae may be applicable pertains to inferences of survival rate. Often, data about trait-dependent survival rates may be assessed over discrete intervals. While the experimental unit of time may be an interval (e.g., a day or a year), the biologically-relevant aspect of variation in survival may be longevity, i.e., for how many intervals an individual survives. One such situation arises when per-interval survival rate is assessed via a logistic regression analysis, and trait-dependent survival rates are (or may be assumed to be) constant across intervals. A common case of logistic regression analysis that satisfies this first condition is often implemented in capture-mark-recapture procedures. Suppose that per-interval survival rate, given phenotype, may be assumed to be constant, and that fitness is defined to be the expected survival time. Then fitness will be given by the mean of a geometric distribution where death in a particular interval of an individual with phenotype z occurs with probability &#x03C1;(<bold>z</bold>),
<disp-formula><alternatives><graphic xlink:href="040618_ueqn4.gif"/></alternatives></disp-formula></p>
<p>If trait-dependent per-interval survival probability is denoted <italic>&#x03C6;</italic>(<bold>z</bold>) (<italic>&#x03C6;</italic> being the standard symbol for survival rate in capture-mark-recapture analyses; <xref rid="c10" ref-type="bibr">Lebreton et al. 1992</xref>), then the fitness function in terms of expected number of intervals lived is <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline13.gif"/></alternatives></inline-formula>. If per-interval survival rate has been modelled as a logistic regression, i.e.,
<disp-formula><alternatives><graphic xlink:href="040618_ueqn5.gif"/></alternatives></disp-formula>
where &#x03C6;(<bold>z</bold>) denotes the per-interval fitness function, and <italic>f</italic>(<bold>z</bold>) is the fitness function on the logistic scale, then the fitness function on the discrete longevity scale is
<disp-formula><alternatives><graphic xlink:href="040618_ueqn6.gif"/></alternatives></disp-formula></p>
<p>Therefore, if <italic>f</italic>(<bold>z</bold>) is a linear function, then its terms are the directional selection gradients on the discrete-longevity scale. If <italic>f</italic>(<bold>z</bold>) is a quadratic function, then the corresponding directional and quadratic selection gradients, again if the relevant aspect of fitness is the number of intervals survived, can be obtained using <xref ref-type="disp-formula" rid="eqn15">equations 15</xref> and <xref ref-type="disp-formula" rid="eqn16">16</xref>. <xref rid="c18" ref-type="bibr">Waller and Svensson (2016;</xref> this issue) takes advantage of these relationships to compare inference of trait-dependent survival in capture-mark-recpature models to classical inference using <xref rid="c9" ref-type="bibr">Lande &#x0026; Arnold&#x2019;s (1983)</xref> least-squares regression analysis where fitness is assessed as the number of intervals that individuals survive.</p>
<p>It must be stressed that these results do not justify interpretation of logistic regression coefficients of survival probability as selection gradients in a general way. Such coefficients differ from selection gradients for three reasons: (1) they pertain to a linear predictor scale, and natural selection plays out on the data scale, (2) they directly model absolute fitness, not relative fitness, and (3) they pertain to per-interval survival, which may not necessarily be the aspect of survival that best reflects fitness in any given study. It is only when the number of intervals survived is of interest (and mean survival can be assumed to be constant across intervals) that these three different aspects of scale cancel out such that the parameters of a logistic regression are selection gradients.</p>
<p>Finally, another situation where an important analysis for understanding trait-fitness relationships that has an immediate &#x2013; but not necessarily immediately apparent &#x2013; relationship to selection gradients, arises in survival analysis. In a proportional hazards model (<xref rid="c1" ref-type="bibr">Cox, 1972</xref>), the instantaneous probability of mortality experienced by live individuals, the hazard &#x03BB;(t), as a function of their phenotype could be modelled as
<disp-formula><alternatives><graphic xlink:href="040618_ueqn7.gif"/></alternatives></disp-formula>
where &#x03BB;<sub>0</sub> is the baseline hazard, and the <italic>e<sup>f(z)</sup></italic> part of the function describes individual deviations from this baseline hazard. If the baseline hazard is constant in time, then survival distributions conditional on phenotype are exponential, and have mean &#x03BB;<sup>&#x2212;1</sup>. So, if fitness is taken to be expected longevity (as a continuous variable now, not discrete number of intervals as in the relations given above between logistic models of per-interval survival and selection gradients) then
<disp-formula><alternatives><graphic xlink:href="040618_ueqn8.gif"/></alternatives></disp-formula>
</p>
<p>In expressions for selection gradients (<xref ref-type="disp-formula" rid="eqn1">equations 1</xref> and <xref ref-type="disp-formula" rid="eqn2">2</xref>), <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline14.gif"/></alternatives></inline-formula> would be a constant in the integrals in both the numerators and denominators, and therefore cancels in calculations of selection gradients. Therefore, if proportional hazards are modelled with <italic>f(z)</italic> as a linear or quadratic function, then the expressions for selection gradients (<xref ref-type="disp-formula" rid="eqn5">equations 5</xref>, <xref ref-type="disp-formula" rid="eqn15">15</xref> and <xref ref-type="disp-formula" rid="eqn16">16</xref>) hold, but the coefficients of the trait-dependent hazard function must be multiplied by &#x2212;1.</p>
</sec>
<sec id="s5"><label>5</label><title>Conclusion</title>
<p>We have provided analytical expressions for selection gradients, given the parameters of log-linear and log-quadratic functions describing expected fitness. These functions can be applied in conjunction with a range of generalised linear model approaches, specific situations in capture-mark-recapture analysis, and relate to fitness functions used in theoretical studies. The general relationship of selection gradients to the coefficients of log-linear and log-quadratic models, in particular, various generalised linear models, are probably the most generally useful feature of our results. In empirical applications, our preliminary simulation results indicate that, given an appropriate model of a log-scale fitness function, inference using log-linear and log-quadratic models may be very robust, and could provide more reliable statements about uncertainty (e.g., reasonable standard errors) than the main methods used to date. Furthermore, the relationships given here between log-quadratic fitness functions and selection gradients could lead to better integration between empirical and theoretical strategies for modelling selection. In theoretical studies, Gaussian fitness functions are often used. These are simply log-quadratic functions that are parameterised in terms of a location parameter (phenotype of maximum fitness), and a width parameter. A relationship between the parameters of a Gaussian fitness function and directional selection gradients (<xref rid="c8" ref-type="bibr">Lande 1979</xref>; the expression we give for <italic>&#x03B2;</italic> is an alternative formulation) is already widely used in the theoretical literature. For any given distribution of phenotype, these parameters correspond directly to linear and quadratic (log-scale) regression parameters, and so can be directly related to selection gradients in empirical studies.</p>
</sec>
</body>
<back>
<ack><title>Acknowledgements</title>
<p>We thank Andy Gardner, Graeme Ruxton, and Kerry Johnson for discussions, comments, and advice. Peter Jupp provided particular insights that improved this paper. MBM is supported by a Royal Society (London) University Research Fellowship.</p>
</ack>
<ref-list><title>References</title>
<ref id="c1"><mixed-citation publication-type="journal"><string-name><surname>Cox</surname>, <given-names>D.R</given-names></string-name>. (<year>1972</year>) <article-title>Regression models and life-tables</article-title>. <source>Journal of the Royal Statistical Society Series B</source>, <volume>34</volume>, <fpage>187</fpage>&#x2013;<lpage>220</lpage>.</mixed-citation></ref>
<ref id="c2"><mixed-citation publication-type="book"><string-name><surname>Endler</surname>, <given-names>J.A</given-names></string-name>. (<year>1986</year>) <chapter-title>Natural selection in the wild</chapter-title>. <publisher-name>Princeton University Press</publisher-name>.</mixed-citation></ref>
<ref id="c3"><mixed-citation publication-type="journal"><string-name><surname>Falconer</surname>, <given-names>D.S</given-names></string-name>. (<year>1960</year>) <article-title>Introduction to Quantitative Genetics</article-title>. <source>Oliver and Boyd</source>.</mixed-citation></ref>
<ref id="c4"><mixed-citation publication-type="book"><string-name><surname>Geyer</surname>, <given-names>C.J.</given-names></string-name> &#x0026; <string-name><surname>Shaw</surname>, <given-names>R.G.</given-names></string-name> (<year>2010</year>) <chapter-title>Aster models and the Lande-Arnold beta</chapter-title>. <source>Technical report</source>, <publisher-name>University of Minnesota</publisher-name>.</mixed-citation></ref>
<ref id="c5"><mixed-citation publication-type="journal"><string-name><surname>Hadfield</surname>, <given-names>J.D.</given-names></string-name>, <string-name><surname>Richardson</surname>, <given-names>D.S.</given-names></string-name> &#x0026; <string-name><surname>Burke</surname>, <given-names>T</given-names></string-name>. (<year>2006</year>) <article-title>Towards unbiased parentage assignment: combining genetic, behavioural and spatial data in a Bayesian framework</article-title>. <source>Molecular Ecology</source>, <volume>15</volume>, <fpage>3715</fpage>&#x2013;<lpage>3731</lpage>.</mixed-citation></ref>
<ref id="c6"><mixed-citation publication-type="journal"><string-name><surname>Janzen</surname>, <given-names>F.J.</given-names></string-name> &#x0026; <string-name><surname>Stern</surname>, <given-names>H.S.</given-names></string-name> (<year>1998</year>) <article-title>Logistic regression for empirical studies of multivariate selection</article-title>. <source>Evolution</source>, pp. <fpage>1564</fpage>&#x2013;<lpage>1571</lpage>.</mixed-citation></ref>
<ref id="c7"><mixed-citation publication-type="journal"><string-name><surname>Kingsolver</surname>, <given-names>J.G.</given-names></string-name>, <string-name><surname>Hoekstra</surname>, <given-names>H.E.</given-names></string-name>, <string-name><surname>Hoekstra</surname>, <given-names>J.M.</given-names></string-name>, <string-name><surname>Vignieri</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Berrigan</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hill</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Hoang</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Gilbert</surname>, <given-names>P.</given-names></string-name> &#x0026; <string-name><surname>Beerli</surname>, <given-names>P.</given-names></string-name> (<year>2001</year>) <article-title>The strength of phenotypic selection in natural populations</article-title>. <source>The American Naturalist</source>, <volume>157</volume>, <fpage>245</fpage>&#x2013;<lpage>261</lpage>.</mixed-citation></ref>
<ref id="c8"><mixed-citation publication-type="journal"><string-name><surname>Lande</surname>, <given-names>R</given-names></string-name>. (<year>1979</year>) <article-title>Quantitative genetic analysis of multivariate evolution, applied to brain:body size allometry</article-title>. <source>Evolution</source>, <volume>33</volume>, <fpage>402</fpage>&#x2013;<lpage>416</lpage>.</mixed-citation></ref>
<ref id="c9"><mixed-citation publication-type="journal"><string-name><surname>Lande</surname>, <given-names>R</given-names></string-name>. &#x0026; <string-name><surname>Arnold</surname>, <given-names>S.J</given-names></string-name>. (<year>1983</year>) <article-title>The measurement of selection on correlated characters</article-title>. <source>Evolution</source>, <volume>37</volume>, <fpage>1210</fpage>&#x2013;<lpage>1226</lpage>.</mixed-citation></ref>
<ref id="c10"><mixed-citation publication-type="journal"><string-name><surname>Lebreton</surname>, <given-names>J.D.</given-names></string-name>, <string-name><surname>Burnham</surname>, <given-names>K.P.</given-names></string-name>, <string-name><surname>Colbert</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Anderson</surname>, <given-names>D.R.</given-names></string-name> (<year>1992</year>) <article-title>Modeling survival and testing biological hypotheses using marked animals: a unified approach with case studies</article-title>. <source>Ecological Monographs</source>, <volume>62</volume>, <fpage>67</fpage>&#x2013;<lpage>118</lpage>.</mixed-citation></ref>
<ref id="c11"><mixed-citation publication-type="book"><string-name><surname>Lynch</surname>, <given-names>M.</given-names></string-name> &#x0026; <string-name><surname>Walsh</surname>, <given-names>B.</given-names></string-name> (<year>1998</year>) <chapter-title>Genetics and analysis of quantitative traits</chapter-title>. <publisher-name>Sinauer</publisher-name>, <publisher-loc>Sunderland, MA</publisher-loc>.</mixed-citation></ref>
<ref id="c12"><mixed-citation publication-type="book"><string-name><surname>Manly</surname>, <given-names>B.F.J.</given-names></string-name> (<year>1985</year>) <chapter-title>The statistics of natural selection</chapter-title>. <publisher-name>Chapman and Hall</publisher-name>, <publisher-loc>New York</publisher-loc>.</mixed-citation></ref>
<ref id="c13"><mixed-citation publication-type="journal"><string-name><surname>Morrissey</surname>, <given-names>M.B.</given-names></string-name> (<year>2014</year>) <article-title>In search of the best methods for multivariate selection analysis</article-title>. <source>Methods in Ecology and Evolution</source>, <volume>5</volume>, <fpage>1095</fpage>&#x2013;<lpage>1109</lpage>.</mixed-citation></ref>
<ref id="c14"><mixed-citation publication-type="journal"><string-name><surname>Morrissey</surname>, <given-names>M.B.</given-names></string-name> &#x0026; <string-name><surname>Sakrejda</surname>, <given-names>K.</given-names></string-name> (<year>2013</year>) <article-title>Unification of regression-based approaches to the analysis of natural selection</article-title>. <source>Evolution</source>, <volume>67</volume>, <fpage>2094</fpage>&#x2013;<lpage>2100</lpage>.</mixed-citation></ref>
<ref id="c15"><mixed-citation publication-type="journal"><string-name><surname>Shaw</surname>, <given-names>R.G.</given-names></string-name> &#x0026; <string-name><surname>Geyer</surname>, <given-names>C.J.</given-names></string-name> (<year>2010</year>) <article-title>Inferring fitness landscapes</article-title>. <source>Evolution</source>, <volume>64</volume>, <fpage>2510</fpage>&#x2013;<lpage>2520</lpage>.</mixed-citation></ref>
<ref id="c16"><mixed-citation publication-type="journal"><string-name><surname>Smouse</surname>, <given-names>P.E.</given-names></string-name>, <string-name><surname>Meagher</surname>, <given-names>T.R.</given-names></string-name> &#x0026; <string-name><surname>Kobak</surname>, <given-names>C.J.</given-names></string-name> (<year>1999</year>) <article-title>Parentage analysis in Chamaelirium luteum (L.) gray (Liliaceae): why do some males have higher reproductive contributions?</article-title> <source>Journal of Evolutionary Biology</source>, <volume>12</volume>, <fpage>1069</fpage>&#x2013;<lpage>1077</lpage>.</mixed-citation></ref>
<ref id="c17"><mixed-citation publication-type="journal"><string-name><surname>Stinchcombe</surname>, <given-names>J.R.</given-names></string-name>, <string-name><surname>Agrawal</surname>, <given-names>A.F.</given-names></string-name>, <string-name><surname>Hohenlohe</surname>, <given-names>P.A.</given-names></string-name>, <string-name><surname>Arnold</surname>, <given-names>S.J.</given-names></string-name> &#x0026; <string-name><surname>Blows</surname>, <given-names>M.W.</given-names></string-name> (<year>2008</year>) <article-title>Estimating nonlinear selection gradients using quadratic regression coefficients: Dougle or nothing?</article-title> <source>Evolution</source>, <volume>62</volume>, <fpage>2435</fpage>&#x2013;<lpage>2440</lpage>.</mixed-citation></ref>
<ref id="c18"><mixed-citation publication-type="journal"><string-name><surname>Waller</surname>, <given-names>J.</given-names></string-name> &#x0026; <string-name><surname>Svensson</surname>, <given-names>E.</given-names></string-name> (<year>2016</year>) <article-title>The measurement of selection when detection is imperfect: how good are na&#x00EF;ve methods?</article-title> <source>Methods in Ecology and Evolution</source>.</mixed-citation></ref>
<ref id="c19"><mixed-citation publication-type="journal"><string-name><surname>Weldon</surname>, <given-names>W.F.R</given-names></string-name>. (<year>1901</year>) <article-title>A first study of natural selection in Clausilia italica (von martens)</article-title>. <source>Biometrika</source>, <volume>1</volume>, <fpage>109</fpage>&#x2013;<lpage>124</lpage>.</mixed-citation></ref>
</ref-list>
<app-group>
<app id="app1"><label>Appendix</label>
<p>Denote a vector containing all unique elements of <bold><italic>&#x03B3;</italic></bold> by <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline15.gif"/></alternatives></inline-formula> The following assumes that <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline16.gif"/></alternatives></inline-formula> is composed by vertically stacking the columns of the diagonal and sub-diagonal elements of <italic>y.</italic> For example, in an analysis with three traits, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline17.gif"/></alternatives></inline-formula> = &#x03B3;<sub>1,1</sub>,&#x03B3;<sub>2,1</sub>,&#x03B3;<sub>3,1</sub>,&#x03B3;<sub>2,2</sub>,&#x03B3;<sub>3,2</sub>,&#x03B3;<sub>3,3</sub>. Let <bold>v</bold>() denote the function mapping the distinct elements of a symmetric matrix <bold>r</bold> onto the column vector <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline18.gif"/></alternatives></inline-formula>.</p>
<p>The first-order approximation to the sampling covariance matrix of the elements of <bold><italic>&#x03B2;</italic></bold> and <bold><italic>&#x03B3;</italic></bold> is then given by <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline19.gif"/></alternatives></inline-formula>, where <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline20.gif"/></alternatives></inline-formula> is the sampling covariance matrix of a vector containing the elements of <bold>b</bold> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline21.gif"/></alternatives></inline-formula>, where the latter is a column vector containing the distinct elements of <bold>g</bold> arranged according to the same scheme that defines <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline22.gif"/></alternatives></inline-formula>. <bold>J</bold> is the Jacobian, or gradient matrix of first order partial derivatives, of <bold><italic>&#x03B2;</italic></bold> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline23.gif"/></alternatives></inline-formula> with respect to <bold>b</bold> and <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline24.gif"/></alternatives></inline-formula>, i.e.,
<disp-formula><alternatives><graphic xlink:href="040618_ueqn9.gif"/></alternatives></disp-formula>
evaluated at the estimated values of <bold>b</bold> and <bold>g</bold>.</p>
<p>Note that some users may prefer to fit the model 6 with <italic>g</italic><sub><italic>ii</italic></sub> replaced by <italic>2g</italic><sub><italic>i</italic></sub>, say. The formulae for <bold><italic>&#x03B2;</italic></bold> and <bold><italic>&#x03B3;</italic></bold> are readily re-expressed in terms of these variables by making this substitution. If <bold>&#x03A3;</bold><sub>1</sub> denotes the covariance matrix obtained when fitting this revised model, the required covariance matrix <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline25.gif"/></alternatives></inline-formula> can be calculated using <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline26.gif"/></alternatives></inline-formula>, where <bold>D</bold> is a diagonal matrix with all the diagonal elements equal to one, apart from those corresponding to the variables <italic>g</italic><sub>ii</sub> which equal 2.</p>
<p>The four submatrices of <bold>J</bold> can be treated separately. Noting that <bold><italic>&#x03B2;</italic></bold> = <bold>Q (b &#x002B; g<italic>&#x03BC;</italic>)</bold> (<xref ref-type="disp-formula" rid="eqn15">equation 15</xref>),
<disp-formula id="eqnA1"><alternatives><graphic xlink:href="040618_eqnA1.gif"/></alternatives></disp-formula></p>
<p>Let <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline27.gif"/></alternatives></inline-formula>, where <italic>k</italic> is the number of traits in the analysis, and let <bold>e</bold><sub>1</sub>,&#x2026;, <bold>e</bold><sub>s</sub> be the standard basis for an <italic>s</italic> dimensional space (i.e., <bold>e</bold>1 = [1,0,&#x2026;,0]&#x2019;, etc.). Define an indicator matrix <bold>C</bold><sub>m</sub> = <bold>C</bold><sup><italic>(i,j)</italic></sup> where <bold>C</bold><sup><italic>(i,j)</italic></sup> is a <italic>k</italic> by <italic>k</italic> matrix in which
<disp-formula><alternatives><graphic xlink:href="040618_ueqn10.gif"/></alternatives></disp-formula></p>
<p>Using the standard expression for the derivative of the inverse of a matrix with respect to a scalar, we can obtain <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline28.gif"/></alternatives></inline-formula>, i.e., the upper-right sub-matrix of <bold>J</bold>.
<disp-formula id="eqnA2"><alternatives><graphic xlink:href="040618_eqnA2.gif"/></alternatives></disp-formula></p>
<p>Let <bold>Q</bold><sub>[u]</sub> denote the <italic>u</italic><sup>th</sup> column of <bold>Q</bold>. Using the previous relation <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline29.gif"/></alternatives></inline-formula>, we can obtain <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline30.gif"/></alternatives></inline-formula>, i.e., the lower-left sub-matrix of <bold>J</bold>.
<disp-formula id="eqnA3"><alternatives><graphic xlink:href="040618_eqnA3.gif"/></alternatives></disp-formula></p>
<p>Let <bold>M</bold><sup>(m)</sup> = <bold>QC</bold><sub>m</sub> (<bold>&#x03A3;</bold><italic>&#x03B2;</italic>&#x002B;<italic>&#x03BC;</italic>)<bold><italic>&#x03B2;</italic></bold>&#x2032; Note that <bold>Q</bold><sup>&#x2212;1</sup> = <bold>&#x03A9;</bold><sup>&#x2212;1</sup>&#x03A3; implies <bold>&#x03A9; = EQ</bold>. Moreover <bold>&#x03A9;</bold><sup>&#x2212;1</sup> = <bold>&#x03A3;</bold><sup>&#x2212;1</sup> implies firstly that
<disp-formula id="eqnA4"><alternatives><graphic xlink:href="040618_eqnA4.gif"/></alternatives></disp-formula>
and secondly that <bold>&#x03A9;</bold> is symmetric, since <bold>&#x03A3;</bold> and <bold>g</bold> are both symmetric. It follows that
<disp-formula id="eqnA5"><alternatives><graphic xlink:href="040618_eqnA5.gif"/></alternatives></disp-formula></p>
<p>The lower-right sub-matrix of <bold>J</bold> can then be derived.
<disp-formula id="eqnA6"><alternatives><graphic xlink:href="040618_eqnA6.gif"/></alternatives></disp-formula>
by use of <xref ref-type="disp-formula" rid="eqnA5">equation A5</xref>.</p>
<p>Finally note that <xref ref-type="disp-formula" rid="eqnA4">equations A4</xref> and <xref ref-type="disp-formula" rid="eqnA5">A5</xref> are also relevant to the derivation of formula 13. By definition, <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline31.gif"/></alternatives></inline-formula>, and we have log <inline-formula><alternatives><inline-graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="040618_inline32.gif"/></alternatives></inline-formula> where <italic>&#x03B1;</italic> does not depend on <bold>z</bold>. Thus, if <italic>&#x03B1;</italic>&#x2032; = <italic>&#x03B1;</italic> &#x002B; <italic>a</italic>, it follows that, as a function of <bold>z</bold>,
<disp-formula><alternatives><graphic xlink:href="040618_ueqn11.gif"/></alternatives></disp-formula></p>
<p>Now, by A4 and A5, we have <bold>&#x03A9;(b&#x002B;&#x03A3;<sup>&#x2212;1</sup><italic>&#x03BC;</italic>)=&#x03A9;b &#x002B; (&#x03A3;<sup>&#x2212;1</sup>&#x03A9;)&#x2032;<italic>&#x03BC;</italic>) = &#x03A9;b &#x002B; Q&#x2032;<italic>&#x03BC;</italic> = &#x03A9;b &#x002B; (I<sub>k</sub> &#x002B; &#x03A9;g)<italic>&#x03BC;</italic></bold> = v, implying that
<disp-formula id="eqnA7"><alternatives><graphic xlink:href="040618_eqnA7.gif"/></alternatives></disp-formula>
where <italic>&#x03B1;&#x2033;</italic> is constant as a function of <bold>z</bold>. The exponent of <italic>e</italic><sup><italic>f</italic>(<bold>z</bold>)</sup><italic>p</italic><sub><italic>&#x03BC;</italic>&#x03A3;(<bold>z</bold>)</sub> is thus identical, as a function of <bold>z</bold>, to that of <italic>p</italic><sub>v&#x03A9;(<bold>z</bold>)</sub>. Hence formula 13 holds.</p>
</app>
</app-group>
<fn-group>
<fn id="fn1"><label><sup>1</sup></label><p>This can be accomplished easily in R. Assume that <italic>W</italic> and <italic>z</italic> are variables in memory representing absolute fitness and phenotypic data, and that residuals of <italic>W</italic> are assumed to follow a Poisson distribution. The regression could be implemented by glm(W&#x02DC;z&#x002B;I(0.5&#x002A;(z-mean(z))&#x005E;2), family=poisson(link=&#x201C;log&#x201D;)).</p></fn>
</fn-group>
</back>
</article>
