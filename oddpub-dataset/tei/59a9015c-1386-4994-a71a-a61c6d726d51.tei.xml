<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EEG-based classification of video quality perception using steady state visual evoked potentials (SSVEPs)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-03-13">13 March 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Laura</forename><surname>Acqualagna</surname></persName>
							<email>laura.acqualagna@tu-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Neurotechnology Group</orgName>
								<orgName type="institution">Technische Universität Berlin</orgName>
								<address>
									<country>Berlin</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Bosse</surname></persName>
							<email>sebastian.bosse@hhi.fraunhofer.de</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Fraunhofer Institute for Telecommunications</orgName>
								<orgName type="institution" key="instit2">Heinrich Hertz Institute</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anne</forename><forename type="middle">K</forename><surname>Porbadnigk</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Machine Learning Group</orgName>
								<orgName type="institution">Technische Universität Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gabriel</forename><surname>Curio</surname></persName>
							<email>gabriel.curio@charite.de</email>
							<affiliation key="aff3">
								<orgName type="institution">Neurophysics Group</orgName>
								<address>
									<settlement>Charité, Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
							<email>klaus-robert.mueller@tu-berlin.de</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Machine Learning Group</orgName>
								<orgName type="institution">Technische Universität Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Brain and Cognitive Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Wiegand</surname></persName>
							<email>thomas.wiegand@hhi.fraunhofer.de</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Fraunhofer Institute for Telecommunications</orgName>
								<orgName type="institution" key="instit2">Heinrich Hertz Institute</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Technische Universität Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Blankertz</surname></persName>
							<email>benjamin.blankertz@tu-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Neurotechnology Group</orgName>
								<orgName type="institution">Technische Universität Berlin</orgName>
								<address>
									<country>Berlin</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Bernstein Focus Neurotechnology</orgName>
								<address>
									<settlement>Berlin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EEG-based classification of video quality perception using steady state visual evoked potentials (SSVEPs)</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-03-13">13 March 2015</date>
						</imprint>
					</monogr>
					<idno type="MD5">D65EC87FF99209606A2FE042F3486A9E</idno>
					<idno type="DOI">10.1088/1741-2560/12/2/026012</idno>
					<note type="submission">Received 13 October 2014, revised 22 January 2015 Accepted for publication 30 January 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2-SNAPSHOT" ident="GROBID" when="2022-05-18T11:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>EEG</term>
					<term>SSVEPs</term>
					<term>video quality assessment</term>
					<term>classification</term>
					<term>MOS</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s>Objective.</s><s>Recent studies exploit the neural signal recorded via electroencephalography (EEG) to get a more objective measurement of perceived video quality.</s><s>Most of these studies capitalize on the event-related potential component P3.</s><s>We follow an alternative approach to the measurement problem investigating steady state visual evoked potentials (SSVEPs) as EEG correlates of quality changes.</s><s>Unlike the P3, SSVEPs are directly linked to the sensory processing of the stimuli and do not require long experimental sessions to get a sufficient signal-to-noise ratio.</s><s>Furthermore, we investigate the correlation of the EEG-based measures with the outcome of the standard behavioral assessment.</s><s>Approach.</s><s>As stimulus material, we used six gray-level natural images in six levels of degradation that were created by coding the images with the HM10.0 test model of the high efficiency video coding (H.265/MPEG-HEVC) using six different compression rates.</s><s>The degraded images were presented in rapid alternation with the original images.</s><s>In this setting, the presence of SSVEPs is a neural marker that objectively indicates the neural processing of the quality changes that are induced by the video coding.</s><s>We tested two different machine learning methods to classify such potentials based on the modulation of the brain rhythm and on time-locked components, respectively.</s><s>Main results.</s><s>Results show high accuracies in classification of the neural signal over the threshold of the perception of the quality changes.</s><s>Accuracies significantly correlate with the mean opinion scores given by the participants in the standardized degradation category rating quality assessment of the same group of images.</s><s>Significance.</s><s>The results show that neural assessment of video quality based on</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p><s>Perceived quality of a set of images or videos is usually assessed using some opinion tests, in which participants are asked to rate the quality of a given visual stimulus on a rating scale <ref type="bibr">(ITU 2002</ref><ref type="bibr">(ITU , 2008))</ref>.</s><s>This type of behavioral tests has some well known limitations: first, they require a large number of participants to obtain statistical significance and the results often suffer from large inter-subject variance.</s><s>Second, the judgment can be biased by several factors not depending on the quality of the stimuli, such as the nature of the task, the rating scale used, the internal state of the participant, expectations, emotions.</s><s>A possible way to quantify visual distortions more objectively would be modeling some key features of the visual system (Jayant et al 1993, Seshadrinathan and Bovic 2010).</s><s>Visual perception is a complex process.</s><s>The visual stimulus passes through the optical system and excites the photoreceptors of the retina.</s><s>The resulting signal is transferred to the visual cortex and processed following further elaboration by higher levels of the visual system.</s><s>Humans are assumed to have an internal threshold, which makes them decide at the cognitive level whether they noticed a distortion, or not.</s><s>So far, a precise model of subjective perception is not yet available and the process underlying the judgment itself is not fully understood.</s><s>Therefore, in recent years, there has been an increasing interest to assess perceived image quality through the direct measurement of brain signals.</s><s>For this purpose, one technology that proved to be suitable is electroencephalography <ref type="bibr">(EEG)</ref>, which records the ongoing electrical brain activity.</s><s>EEG has been widely exploited in visual and audio perception research <ref type="bibr" target="#b5">(Babiloni et al 2006</ref><ref type="bibr">, Demiralp et al 2007</ref><ref type="bibr">, Busch et al 2009</ref><ref type="bibr">, Porbadnigk et al 2013)</ref> and recently also for assessing user's perceived multimedia quality <ref type="bibr">(Hayashi et al 2000</ref><ref type="bibr" target="#b25">, Porbadnigk et al 2010</ref><ref type="bibr">, 2011</ref> Most of these studies focus on the measurement of specific EEG components called event related potentials (ERPs).</s><s>ERPs are brain responses arising time locked to the onset of an external significant stimulus <ref type="bibr">(Donchin 1979</ref><ref type="bibr" target="#b22">, Pfurtscheller and Lopes da Silva 1999</ref><ref type="bibr" target="#b24">, Picton et al 2000)</ref>.</s><s>There are several categories of ERPs, associated to different steps of the processing of the stimulus, with different latencies and scalp topographies.</s><s>In video quality assessment studies, the most exploited ERP component has been the P3, which is a positive deflection of the brain activity arising between 300 and 500 ms after the stimulus onset in the central-parietal cortex <ref type="bibr" target="#b38">(Smith et al 1990</ref><ref type="bibr" target="#b23">, Picton 1992</ref><ref type="bibr">, Johnson 1993)</ref>, largely independent on the sensory modality.</s><s>In <ref type="bibr" target="#b35">Scholler et al (2012)</ref>, a P3-based EEG measurement is used to directly assess the perception of quality changes in 8 s video clips, in which distortions are introduced by a hybrid video codec.</s><s>They find that quality changes elicit a P3 component that is positively correlated with the magnitude of the change, which can be classified on a single-trial basis.</s><s>Also they report that a participant shows brain responses to low distortion stimuli, although the participant did not report perceiving them.</s><s>In another study by <ref type="bibr">Lindemann and Magnor (2011)</ref>, the use of EEG as a tool for evaluating image quality for JPEG-compressed images is investigated.</s><s>They show that the presence of JPEG artifacts elicits ERPs whose amplitudes vary with the compression ratio.</s><s>In a follow-up study <ref type="bibr">(Lindemann et al 2011)</ref>, they present a first evaluation of artifacts in simple video stimuli to verify whether they can be assessed using ERP analysis.</s><s>By comparing the ERPs elicited by videos with and without two types of artifacts, they show that artifacts produce measurable ERPs whose shape depends on the strength of the artifacts.</s><s>Another approach for single-trial classification of artifacts in videos is proposed by <ref type="bibr" target="#b15">Mustafa et al (2012)</ref>, where the focus is the classification of artifacts that usually occur in image-based rendering techniques.</s><s>They also show that it is possible to distinguish with a certain degree of accuracy between different types of artifacts.</s><s>All these works demonstrate that an EEG-based approach in classifying quality changes or artifacts in videos and images is a feasible measurement that, together with behavioral tests, can lead to a more objective rating of perceived quality.</s><s>Notwithstanding, all studies that capitalize on the P3 component have several limitations.</s><s>First, the P3 component is not directly linked to sensory processing, but reflects higher cognitive processing of the stimulus.</s><s>It is elicited by an oddball paradigm, in which the user pays attention to the occurrences of a target event among more frequent non-targets.</s><s>The paradigm usually requires a high number of trials in order to have a sufficient number of target events and a good signal-to-noise ratio.</s><s>Besides, the design of such experiments needs to be carefully done.</s><s>For example, the target events have to be sufficiently distant in time between each other, in order to produce the 'surprise effect' which would significantly modulate the brain signal <ref type="bibr">(Duncan-Johnson and</ref><ref type="bibr">Donchin 1977, Sellers et al 2006)</ref>.</s></p><p><s>In this study, we investigated an EEG-based approach to video quality assessment, exploiting steady state visual evoked potentials (SSVEPs).</s><s>SSVEPs are oscillatory brain responses elicited in the visual cortex by a repetitive visual stimulus <ref type="bibr">(Calhoun and McMillan 1996</ref><ref type="bibr">, Herrmann 2001</ref><ref type="bibr" target="#b42">, Vialatte et al 2010)</ref>.</s><s>For example, a flickering light would synchronize the neurons of the visual cortex at the same frequency of the driving stimulus and higher harmonics <ref type="bibr">(Regan n.d, year, Rager and Singer 1998)</ref>.</s><s>They have been employed successfully in brain computer interfaces (BCIs) <ref type="bibr">(Gao et al 2003</ref><ref type="bibr" target="#b14">, Müller-Putz et al 2005</ref><ref type="bibr">, Friman et al 2007</ref><ref type="bibr" target="#b1">, Allison et al 2008</ref><ref type="bibr" target="#b18">, Parini et al 2009)</ref> and in basic research on the human visual system (Morgan et al 1996, <ref type="bibr" target="#b11">Müller and Hübner 2002</ref><ref type="bibr">, Keil et al 2003</ref><ref type="bibr" target="#b19">, Pastor et al 2003)</ref>, proving to be robust and reliable signals to classify.</s><s>Previous work on using SSVEPs has already demonstrated the basic suitability of the targeted method for video quality measurement <ref type="bibr" target="#b16">(Norcia et al 2014)</ref>.</s><s>Using a SSVEP-based paradigm in which the flickering effect is caused by changes in video quality (owing to compression), we show that it is possible to elicit a direct visual response and to obtain a meaningful quantification by single-trial classification using machine learning techniques.</s><s>A characteristic that makes SSVEPs preferable to P3 in video quality assessment is that SSVEPs are directly linked to the sensory processing and can give a more straightforward indication of the level of perception of the visual stimuli.</s><s>Moreover, for the reasons described above, a P3-based approach requires long inter (target-) stimulus intervals in order to achieve a suitable signal-to-noise ratio, while a SSVEP-based approach allows to collect a large number of trials within a short amount of time.</s><s>For example, in the study of <ref type="bibr" target="#b35">Scholler et al (2012)</ref>, a total of 600 ERPs was collected in 120 min, while more than 10.000 ERPs (single VEPs of the SSVEPs) can be collected using the proposed SSVEP-based approach within the same amount of time.</s><s>Of course, such a comparison can only give an indication of the potential to speed up the assessment by using SSVEPs, since comparing just the number of trials ignores that both studies had different stimulus material and presumably a different signal-to-noise ratio in single-trials.</s><s>Another limitation of previous studies is that they do not investigate directly the correlation between the neural assessment and the behavioral ratings.</s><s>An exception can be found in the recent study of <ref type="bibr" target="#b4">Arndt et al (2014)</ref>, in which they performed a series of experiments assessing video and audiovisual quality degradation.</s><s>They show an average significant correlation between the P3 amplitudes and the mean opinion scores (MOS) in three out of five experiments.</s><s>In this study, we performed the standard behavioral test for video quality assessment either before or after the EEG measurement and the MOS for the same groups of textures was collected.</s><s>Therefore, we could correlate the classified SSVEP features not only with the quality changes intrinsic to the video signals, but also with the judgments of the participants.</s><s>We used textures in condition of natural luminance as stimuli instead of artificially generated stimuli.</s><s>This makes the stimuli more realistic and the experimental condition closer to real world video sequences.</s><s>Yet, the textures were simple enough to avoid influences of semantic content, and chosen to be homogeneous in order to minimize the effects of eye movements and to avoid that attention was captured by salient objects not related to the purpose of the experiment.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Stimuli</head><p><s>Six gray-level texture images <ref type="bibr" target="#b17">(Ojala et al 2002</ref><ref type="bibr">, Kylberg 2011)</ref> were chosen as the basis for stimulus generation (figure <ref type="figure" target="#fig_1">1</ref>).</s><s>The size was 512 × 512 pixels and they all had the same mean luminance.</s><s>In order to make the measurement independent of the image statistics and of the actual gaze position during the experiment, the texture images were spatially roughly stationary.</s><s>The quality of each texture image was then degraded in six different levels.</s><s>The distortions were introduced by coding the textures using the HM10.0 test model (JCT-VC 2014) of the emerging high efficiency video coding (H.265/MPEG-HEVC) standard <ref type="bibr" target="#b40">(Sullivan et al 2012)</ref>.</s><s>In this standard, statistical redundancies are exploited by block-wise temporal and spatial linear prediction.</s><s>The residual signal is transformed block-wise, and coefficients are quantized in the transform domain.</s><s>The quantization is controlled by the quantization parameter (QP).</s><s>Coding artifacts, which are perceived by the human observer as a loss of visual quality, are introduced by the quantization of the transform coefficients.</s><s>In order to investigate how the visual cortex responds to distortions at the threshold of perception, the first three distortion levels are chosen to be perceived as high quality.</s><s>The QP-values used in the experiment were estimated in a pilot study in order to meet consistent MOS.</s><s>All the texture images in all the different levels of degradation were displayed as videos, 114 s long.</s><s>Details about the structure of the videos are described in section 2.2.3.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Experimental design</head><p><s>2.2.1.</s><s>Participants.</s><s>Sixteen participants (seven females and nine males, in the age group 21-46) took part in the experiment.</s><s>All had normal or corrected-to-normal vision and none of them had a history of neurological diseases.</s><s>They were all native German speakers or at least with a level of German comprehension of five, on the six level scale of competence laid down by the Common European Framework of reference for languages (Little 2007).</s><s>All of them were naïve in respect of video quality assessment studies and were paid for their participation.</s><s>The study was performed in accordance with the declaration of Helsinki and all participants gave written informed consent.</s></p><p><s>2.2.2.</s><s>Apparatus.</s><s>EEG was recorded with sampling frequency of 1000 Hz using BrainAmp amplifiers and an ActiCap active electrode system with 64 channels (both by Brain Products, Munich, Germany).</s><s>The electrodes used were <ref type="bibr">Fp1,</ref><ref type="bibr">2,</ref><ref type="bibr">AF3,</ref><ref type="bibr">4,</ref><ref type="bibr">7,</ref><ref type="bibr">8,</ref><ref type="bibr">Fz,</ref><ref type="bibr">FCz,</ref><ref type="bibr">FT7,</ref><ref type="bibr">8 Cz,</ref><ref type="bibr">T7,</ref><ref type="bibr">CPz,</ref><ref type="bibr">TP7,</ref><ref type="bibr">8,</ref><ref type="bibr">Pz,</ref><ref type="bibr">POz,</ref><ref type="bibr">PO3,</ref><ref type="bibr">4,</ref><ref type="bibr">7,</ref><ref type="bibr">8,</ref><ref type="bibr">Oz,</ref><ref type="bibr">O1,</ref><ref type="bibr">2.</ref></s><s>The electrode that in the standard EEG montage is placed at T8 was placed under the right eye and used to measure eye movements.</s><s>All the electrodes were referenced to the left mastoid, using a forehead ground.</s><s>For offline analyses, electrodes were re-referenced to linked mastoids.</s><s>All impedances were kept below 10 kOhm.</s><s>The stimuli were  Each texture was displayed distorted for 333 ms, followed by the undistorted form for 333 ms (D0) and the same succession was repeated four times for each level.</s><s>Such alternation of quality changes produced a flickering effect eliciting SSVEPs when perceived by the participant.</s></p><p><s>shown on a 23' screen (Dell U2311H) with a native resolution of 1920 × 1080 pixels at a refresh rate of 60 Hz.</s><s>The screen was normalized according to the specifications in ITU ( <ref type="formula">2002</ref>).</s><s>The stimuli resolution was 512 × 512 pixels (128 × 128 mm), which corresponds to 7.15 visual angle.</s><s>The size of the images in the behavioral part of the experiment was the same as in the videos.</s><s>The viewing distance was 110 cm, in compliance with specifications in the ITU-T Recommendation P.910 <ref type="bibr">(ITU 2008)</ref>.</s><s>Subjects sat in front of the display in a dimly light room.</s></p><p><s>2.2.3.</s><s>Procedure.</s><s>The experiment consisted of an EEG measurement and a behavioral part.</s><s>After a general introduction to the experiment and the preparation of the EEG cap, half of the participants started with the EEG recording and half with the behavioral assessment.</s><s>In the EEG part, they had to watch a series of 51 videos, divided into three runs (20 + 15 + 16).</s><s>Between each run, there was a break of about 10 min, to give the participants the chance to relax and stretch.</s><s>Each video had a length of 114 s , followed by a pause of 5 s.</s><s>Each video began with a fixation cross that was displayed for 3 s at the center.</s><s>In order to minimize artifacts, participants were instructed to not move their eyes during the presentation of the video and to blink as little as possible.</s><s>Figure <ref type="figure" target="#fig_2">2</ref> depicts the structure of one video, which will be addressed as 'trial' in the remainder of the paper.</s><s>Each trial comprises all six textures and degrees of quality, presented in random order.</s><s>The stimulus onset asynchrony (SOA) was 333 ms, that is, each texture image was displayed for 333 ms.</s><s>At the beginning, the texture was presented in its undistorted form (D0) for 2664 ms (333 ms × 8).</s><s>Then, the first quality change occurred and the distorted texture was displyed for 333 ms, followed by 333 ms of the same texture in its undistorted version.</s><s>This cycle 'distorted-undistorted' was repeated four times for a total of 2664 ms.</s><s>Then, the same texture was displayed with another level of quality change, for a cycle 'distorted-undistorted' of the same length.</s><s>This procedure was performed until all the distortion levels were displayed for that texture (randomized order).</s><s>After that, the texture was switched and the new one was displayed at the beginning in its undistorted version for 2664 ms, before starting the cycle of quality changes.</s><s>This presentation elicits SSVEPs if the changes due to altered quality are processed in the visual cortex.</s><s>A SOA of 333 ms results in a flickering frequency of 3 Hz.</s><s>Before starting the main EEG recording we performed some additional measurements, comprising a relax measurement and an artifact measurement.</s><s>In the relax measurement, EEG was acquired during rest alternating with eyes open and eyes closed (10 s each) in order obtain a standard measure of the participant's occipital alpha rhythm.</s><s>In the former phase, they had to look at a simple colored geometrical shape moving in the center of the display.</s><s>The cycle 'eyes closed-eyes open' was repeated ten times.</s><s>In the artifact measurement, five crosses were displayed: one in the center, the others respectively at the left and right side, upper and below the central one.</s><s>The distance of the four external crosses matched the size of the videos.</s><s>Participants were instructed to fixate the central cross and then promptly move the eyes to one of other four, according to the instructions of a recorded voice.</s><s>In the behavioral part of the experiment, participants had to evaluate the perceived quality of the textures, following the standardized degradation category rating quality assessment (ITU 2002) in a presentation mode.</s><s>Each texture was presented in the display in pairs for 10 s: on the right-hand side in its original undistorted version (reference) and simultaneously on the left-hand side with changed quality (figure <ref type="figure" target="#fig_3">3</ref>).</s><s>After that, a new window was displayed, with the nine-grade degradation (distortion) scale, according to the ITU-T P-910 recommendation <ref type="bibr">(ITU 2008)</ref>.</s><s>The scale was displayed in German language, and in English can be translated in the following: 1-very annoying; 3annoying; 5-slightly annoying; 7-perceptible, but not annoying; 9-imperceptible.</s><s>Grade 8 is commonly interpreted as the perceptibility threshold, that is the distortion level where the observer is not completely sure to perceive the distortion.</s><s>Participants had up to 10 s to decide about the level of distortion of the previously displayed left image compared to the reference one on the right, scrolling a bar until the selected grade and confirming by button press.</s><s>After that, the presentation switched to the next pair of textures.</s><s>If the person did not make any choice within the 10 s, the presentation automatically went ahead with the next comparison.</s><s>In the behavioral assessment, each texture image was presented in all the level of distortions (comprising the comparison reference-reference).</s><s>For each level, there were three evaluations.</s><s>The order of the evaluations was randomly shuffled.</s><s>At the beginning of the assessment, a calibration block was displayed in order to make the participants confident with the test: each texture was displayed for just two evaluations, worst quality level versus reference and reference versus reference, for a total of 12 calibration evaluations.</s><s>Like in the actual behavioral assessment, in this short test participants were not aware of the quality level of the displayed textures.</s><s>The data of this calibration block were not considered in the analysis.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Preprocessing and data analysis</head><p><s>EEG signal was lowpass filtered from 0 to 40 Hz with a Chebyshev filter of order ten (3 dB of ripple in the passband and 40 dB of attenuation in the stopband) and down-sampled to 100 Hz.</s><s>For the SSVEP visualization, the continuous signal was divided into epochs ranging from 0 to 2664 ms, relative to the onset of the first distorted texture for each quality change.</s><s>Each epoch comprises all four repetitions of the same quality level.</s><s>Epochs referring to the same distortion level were averaged over all the trials and all the textures.</s><s>In the frequency-domain analysis, EEG spectra were calculated between 1 Hz and 18 Hz for all epochs and then averaged for each distortion level, over all trials and textures.</s><s>For singletrial offline classification, we tested two different methods.</s><s>The first one exploited the oscillatory nature of the SSVEPs and used common spatial pattern (CSP) analysis to enhance the discrimination of the event-related synchronization <ref type="bibr">(Koles et al 1990</ref><ref type="bibr" target="#b30">, Ramoser et al 2000</ref><ref type="bibr">, Blankertz et al 2008</ref><ref type="bibr" target="#b18">, Parini et al 2009)</ref>.</s><s>The second one is based on methods used in ERP analysis and exploits spatio-temporal features <ref type="bibr" target="#b7">(Blankertz et al 2011)</ref>.</s></p><p><s>2.3.1.</s><s>CSP method.</s><s>CSP was used to extract spatial filters in order to enhance the signal of interest in the occipital cortex.</s><s>In general, CSP aims at maximizing the variance on the spatially filtered signals under one condition while minimizing it for the other condition.</s><s>In our case, one condition would be the neural signal associated to the distorted images and the other condition would be that corresponding to the undistorted images presented at the beginning of each block of textures.</s><s>Since variance of bandpass filtered signals is equal to band-power, CSP analysis is applied to band-pass filtered signals in order to obtain an effective discrimination of mental states between the two conditions.</s><s>CSP projects the signal</s></p><formula xml:id="formula_0">∈  t x( ) C in the original sensor space to ∈  x C CSP</formula><p><s>, which lives in the surrogate sensor space, as follows:</s></p><formula xml:id="formula_1">= t t x W x ( ) ( ) .</formula><p><s>( 1 )</s></p><formula xml:id="formula_2">T CSP Each column vector ∈  w j C (j = 1, ... ,C ) of W is a spatial filter; each column vector ∈  a j C (j = 1, ... ,C) of a matrix = ∈ −  A W ( ) T C x C 1</formula><p><s>is a spatial pattern.</s><s>While for classification only the spatial filters are used, only the patterns allow for a physiological interpretation of the CSP components, see <ref type="bibr">Blankertz et al (2008)</ref> and <ref type="bibr">Haufe et al (2014)</ref>.</s><s>For a more detailed review on CSP analysis and its application to EEG signal processing, please refer to Lemm et al (2005), Lotte et al (2007), <ref type="bibr">Blankertz et al (2008)</ref>, <ref type="bibr" target="#b34">Sannelli et al (2011)</ref>, <ref type="bibr" target="#b33">Samek et al (2012)</ref>.</s><s>In our case, the CSP filters were calculated between the epochs of maximum distortion level, which will be named class D6, and epochs of the undistorted level, which will be named class D0.</s><s>Since the performance of this spatial filter depends on the operational frequency band of interest, manually selecting a specific frequency range is commonly used with the CSP algorithm (Dornhege et al 2006, <ref type="bibr" target="#b2">Ang et al 2008)</ref>.</s><s>In our case, CSP was performed after filtering the data with a 5th order Butterworth filter centered at 3 Hz (pass-band 2-4 Hz), 6 Hz (pass-band 5-7 Hz) and both simultaneously (filter bank).</s><s>The filter bank concatenated data filtered at 3 and 6 Hz, which were subsequently spatially filtered with the respective CSPs.</s><s>In this way, features referring to both frequencies could be exploited simultaneously.</s><s>In all the three cases, the continuous EEG signal was divided into epochs of 667 ms length, time-locked to the onset of the distorted image.</s><s>For each texture and level of distortion, the four repetitions of the cycle 'distorted-undistorted' were averaged.</s><s>For D0, the first block of epochs at the beginning of each video was discarded, because it is often affected by artifacts due to subject's movements between the videos.</s><s>For training and testing, the epochs were split into a subset of epochs with even IDs (training) and odd IDs (testing), in order to prevent that longer-term changes in the EEG during the experiment could bias classification.</s><s>For the calculation of the CSP filters (training), the epochs with even IDs were considered.</s><s>One up to three CSP filters per class were automatically selected for each subject and checked visually.</s><s>The CSP filters that maximize the variance for class D6 while minimizing the variance for D0 were used.</s><s>The selected filters were then applied to the training epochs comprising D6 and D0 and the log-variance in three equally spaced intervals of the filtered data was used as feature matrix for training a classifier based on linear discriminant analysis (LDA) (Lemm et al 2011).</s><s>The CSP filters were then applied to the testing epochs, from D1 to D6, and classification was made between each level of distortion and D0.</s><s>Classification performance was measured by the area under the curve (AUC) of the receiver operating characteristic <ref type="bibr">(Hanley and McNeil 1982)</ref>.</s></p><p><s>2.3.2.</s><s>Spatio-temporal features method.</s><s>In this method, we classified single-trial visual evoked potentials time-locked to the onset of the distorted texture, using spatio-temporal features <ref type="bibr" target="#b41">(Tomioka and</ref><ref type="bibr">Müller 2010, Blankertz et al 2011)</ref>.</s><s>The EEG data was divided into epochs as follows: for the 'class 1' events, epochs were time-locked to the onset of the distorted textures.</s><s>For the 'class 2' events, the starting point of epochs was shifted 160 ms after the onset.</s><s>Being the period of the oscillatory visual response of about 333 ms, a shift of 160 ms would lead to a high discrimination between the two classes.</s><s>In both classes, the length of the epochs was 667 ms.</s><s>For feature extraction, five temporal windows were selected individually for each participant by a heuristic <ref type="bibr" target="#b7">(Blankertz et al 2011)</ref>, based on the pointwise biserial correlation coefficient (r-values).</s><s>More specifically, we used the − r sign 2 as a measure of separability between the two classes.</s><s>The aim of the heuristic is to find time intervals that have a fairly constant pattern (class 1 minus class 2 difference) and maximal r 2 differences.</s><s>Features were calculated from 36 channels <ref type="bibr">(FC1,</ref><ref type="bibr">3,</ref><ref type="bibr">z,</ref><ref type="bibr">2,</ref><ref type="bibr">4,</ref><ref type="bibr">C1,</ref><ref type="bibr">3,</ref><ref type="bibr">5,</ref><ref type="bibr">z,</ref><ref type="bibr">2,</ref><ref type="bibr">4,</ref><ref type="bibr">6,</ref><ref type="bibr">CP1,</ref><ref type="bibr">3,</ref><ref type="bibr">5,</ref><ref type="bibr">z,</ref><ref type="bibr">2,</ref><ref type="bibr">4,</ref><ref type="bibr">6,</ref><ref type="bibr">P1,</ref><ref type="bibr">3,</ref><ref type="bibr">2,</ref><ref type="bibr">4 Pz,</ref><ref type="bibr">7,</ref><ref type="bibr">9,</ref><ref type="bibr">8,</ref><ref type="bibr">10,</ref><ref type="bibr">PO3,</ref><ref type="bibr">7,</ref><ref type="bibr">4,</ref><ref type="bibr">8,</ref><ref type="bibr">O1,</ref><ref type="bibr">z,</ref><ref type="bibr">2)</ref> by averaging voltages within each of the five chosen time windows resulting in 36 × 5 = 180 dimensional feature vectors.</s><s>Classification was performed using ten-fold crossvalidation and LDA with shrinkage of the covariance matrix <ref type="bibr" target="#b7">(Blankertz et al 2011)</ref>.</s></p><p><s>Classification results were correlated with the average magnitude of the alpha rhythm during the experiment.</s><s>The alpha peak was searched within a range of frequencies.</s><s>In order to estimate alpha power, the difference between the value of the alpha power and the linear interpolation between the flanking frequencies (as baseline) was calculated.</s><s>For this estimate, flanking frequencies have been searched within the intervals 6-10 Hz and 11-13 Hz, and the alpha peak was determined as maximum of the spectra between the flanking frequencies.</s><s>For all participants, alpha peak was detected between 9 and 12 Hz.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Behavioral data</head><p><s>Figure <ref type="figure" target="#fig_4">4</ref> displays on the left the nine-degradation scale of the MOS, in which grade 8 corresponds to the perception threshold of the degradation, that is the level where the observer is not completely sure anymore to perceive any degradation.</s><s>On the right, the mean MOS values over all the participants, textures and repetitions is plotted as a function of the distortion level.</s><s>The error bars represent the standard deviation as a measure of the variation of the mean MOS across participants.</s><s>The plot displays that on the behavioral level the participants were not able to discriminate the two most subtle distortion levels (D1 and D2) from the reference image (D0).</s><s>For level D3, the mean MOS is lower but still remains above the value of the perception threshold.</s><s>From distortion level D4 to D6, the mean MOS values decrease linearly with the level of distortion, and the error bars also display an increase in variability among participants.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Neurophysiological data</head><p><s>Figure <ref type="figure" target="#fig_5">5</ref> (left) displays representative SSVEP waveforms of participant VPib recorded at an occipital scalp site (electrode position Oz).</s><s>The plots are ordered according to increasing distortion levels, that is, the top row refers to the reference texture of highest quality (D0) and the bottom row to the maximum distortion level (D6).</s><s>Each plot represents the average EEG activity over all the trials and textures at that specific quality level.</s><s>The time zero is locked to the onset of the first repetition of the texture of each block.</s><s>On the x axes, Dist refers to the onset of the distorted texture, and Ref to the onset of the reference image (D0).</s><s>As described in the methods section, the time interval between the onset of each frame is 333 ms.</s><s>For the first three levels of distortion (D1-D3), which are around the perception threshold, the ongoing EEG activity is not visibly modulated by the quality change.</s><s>From D4 onwards, the SSVEPs become clearer and their amplitude increases with increasing distortion.</s><s>In all the plots where the SSVEPs are evident, it can be noticed that the onset of the reference images (Ref) elicits a more pronounced negative peak than the onset of the distorted ones, at the same latency.</s><s>This result suggests that the transition Dist-Ref has a stronger impact on the modulation of the visual evoked potentials than the transition Ref-Dist.</s><s>The results of the analysis in the frequency domain analysis of the SSVEPs (electrode position Oz) of the same participant are displayed in figure 5 (right).</s><s>Power spectra were calculated on single trials before averaging over the trials and textures.</s><s>The plots represent the average power for each level of distortion, coded with different colors.</s><s>For the first three levels of distortion (yellow, light green, emerald green) there is no clear increase of the power in any of the frequencies of interest (3 Hz and higher harmonics).</s><s>From D4 to D6 (blue, violet, magenta), the spectra display two clear peaks at 3 and 6 Hz, whose amplitudes increase significantly with increasing distortion level (p &lt; 0.01).</s><s>At D5 and D6 a small peak at 9 Hz becomes visible, but this modulation is much smaller compared to the first two harmonics (therefore not taken into account in further analysis).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Classification</head><p><s>CSP filtering is a gold standard of the processing of EEG oscillatory signals in BCIs.</s><s>In the same way, spatio-temporal features have been successfully used in the analysis of ERPs <ref type="bibr" target="#b7">(Blankertz et al 2011</ref><ref type="bibr" target="#b7">, Lemm et al 2011)</ref>.</s><s>We considered both processing methods in combination with LDA (with shrinkage when necessary) in order to exploit both natures of the SSVEPs.</s></p><p><s>3.3.1.</s><s>CSP method.</s><s>Figure <ref type="figure" target="#fig_6">6</ref> displays the results of the CSP analysis as color coded scalp topographies (for one representative participant, VPib).</s><s>Figure <ref type="figure" target="#fig_6">6</ref> on the left refers to data filtered around 3 Hz, and in the middle to data filtered around 6 Hz.</s><s>The upper plots display the spatial filter coefficients, that is, the interpolation of the values of the components of the vector w j , the jth columns of W, at each electrode position.</s><s>The bottom rows represent the corresponding pattern of activation in the brain, that is, the interpolation of the components of a j , the jth column of</s></p><formula xml:id="formula_3">= − A W ( ) T 1 .</formula><p><s>For each participant, only those CSP filters were selected which maximize the variance of D6 while minimizing the variance of D0, since we are interested in finding the spatial patterns which discriminate the activity elicited by the distorted texture versus the reference.</s><s>(Note that we use a colormap that has no direct association to signs because the signs of the vectors are irrelevant in our analysis.)</s><s>After the training of the LDA classifier and the application of the CSP filters to the testing data, the classifier was evaluated for all the levels of distortions.</s><s>The results are displayed in the bar plot in figure 6 (right), for 3, 6 Hz and the filter bank which considers both frequencies.</s><s>The error bars indicate the standard deviation of the single participants with respect to the mean accuracy.</s><s>The x axes represent the pair of classes between which the classification was performed, and the y axes the classification performances.</s><s>Independent of the chosen frequency, the mean classification accuracy for the first three levels of distortion (D1 to D3) is around chance level and not affected by the quality change.</s><s>Between D4 and D6, the mean classification accuracy increases linearly and significantly with increasing distortion, reaching at D6 0.68 (SD = 0.13) for 3 Hz, 0.71 (SD = 0.13) for 6 Hz and 0.74 (SD = 0.14) for the filter bank.</s><s>In the CSP method a slight increase on average performances is visible when data are preprocessed with the filter bank, exploiting the features and filters of both the frequencies of interest.</s><s>Repeated measurement of ANOVA 8 run on the AUC values with factors 'distortion level' and 'frequency' display a statistically significant difference for the first factor (p &lt; 0.01), but not for 'frequency' (F = 0.57, p = 0.56).</s><s>Even if the results show higher average accuracy when considering the filter bank of 3 and 6 Hz, the choice of the discriminant frequency does not affect significantly the classification accuracy.</s></p><p><s>3.3.2.</s><s>Spatio-temporal features method.</s><s>In the second method of classification based on spatio-temporal features, epochs of 667 ms length were considered time-locked to the onset of the distorted images for class 1, and with a lag of 160 ms for class 2. Figure <ref type="figure" target="#fig_7">7</ref> (left) displays the grand average of evoked potentials over all participants and trials for distortion level D6.</s><s>The line colored in magenta displays the EEG activity referred to class 1, the gray line to class 2. Considering just the magenta plot, a first negative deflection is visible between 150 and 210 ms after the onset of the distorted image, followed by a pronounced positive peak around 270 ms.</s><s>As already displayed in figure 5, the onset of the reference texture at 333 ms elicits a more pronounced negative visual evoked potential than that elicited by the onset of the distorted texture.</s><s>The scalp plots underneath display the topographies in two time intervals where the class difference is large (shaded gray in the plot above).</s><s>They visualize the distribution of the signed r 2 values as a measure of discriminability between classes 1 and 2, which is highest The patterns clearly display that the highest variance of the neural signal takes place in the occipital cortex, where the visual information is processed.</s><s>Right: average classification accuracies using LDA after CSP filtering, for features at 3 Hz (red), 6 Hz (green) and filter bank (blue), respectively.</s><s>Error bars refer to the standard deviation of the participants accuracies from the mean.</s><s>Classification accuracy increases significantly from D4 to D6, but no significant difference is found in the choice of the filtering frequency.</s></p><p><s>test under the null hypothesis that the samples are drawn from a standard normal distribution.</s><s>The null hypothesis was not rejected at the 5% significance level.</s></p><p><s>in the occipital cortex with focus around the central channels.</s><s>In other words, the visual processing of the stimuli leads to maximal discrimination between good and distorted quality.</s><s>Based on the signed r 2 values, features for offline single-trial classification were determined (ten-fold cross-validation).</s><s>A mean accuracy of 0.84 (SD = 0.1) is achieved at the maximal distortion level D6.</s><s>At D5 and D4 the mean accuracy drops to 0.75 (SD = 0.12) and 0.61 (SD = 0.1) respectively.</s><s>The mean accuracy at D3 down to D0 are around chance level.</s><s>Figure <ref type="figure" target="#fig_7">7</ref> (right) displays the trend of the classification accuracy for all participants (colored lines) and the mean (black thick line) as a function of the distortion level.</s><s>From distortion D3 upwards, the mean accuracy increases significantly with the level of distortion (p &lt; 0.01).</s><s>Since classification is based on the spatio-temporal features derived from the evoked potentials, this trend tightly follows the trend of the modulation of the visual evoked potentials in the occipital cortex.</s><s>The SSVEP plots in figure 5 refer to participant VPib, who reaches the highest classification accuracy in the spatio-temporal classification method, that is 0.99, and also in the CSP method, that is 0.96 (filter bank).</s><s>Participant VPib shows clear visual evoked potentials and spectra peaks at both 3 and 6 Hz.</s></p><p><s>MOS values significantly linearly correlated with classification accuracy obtained using the spatio-temporal features for all the participants (p &lt; 0.01).</s><s>MOS values were also correlated with the accuracy obtained by the CSP method with filter bank, which takes into account contemporary both the frequencies of interest.</s><s>In this case, we found a significant linear correlation (p &lt; 0.05) for all but two participants.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Understanding individual differences</head><p><s>While for the first three levels of distortion there are no substantial inter-participant differences in classification results, the variability of the classification accuracy at higher levels of distortion becomes statistically significant (p &lt; 0.05).</s><s>For example, in the classification based on spatio-temporal features five participants reach a mean classification accuracy of more than 0.9 for the maximal level of distortion D6, while five participants never exceed 0.7.</s><s>The latter even do not pass chance level in the classification of distortion level D4, thus seeming less sensitive in general to changes of visual quality.</s><s>In order to find neurophysiological correlates of classification performance, we calculated the average peak magnitudes of the alpha rhythm during the experiment for each participant and correlated it with the classification result that the participant reached at the maximal level of distortion.</s><s>The results are displayed in figure 8 as scatter plots, for the two classification methods described in the previous section.</s><s>Each dot represents a participant.</s><s>The black line is the result of the linear regression between the average magnitudes of the alpha peaks of each participant and their classification accuracies.</s><s>The yellow dots represent participants having the 10% largest Mahalanobis distances to the data center, and therefore considered as outliers and removed from the analysis <ref type="bibr">(Huber and Ronchetti 1975)</ref>.</s><s>In both classification methods used, we 2 for each channel.</s><s>The highest discrimination between the two classes takes place in the occipital cortex.</s><s>Right: classification accuracies using shrinkage LDA for all participants (colored lines) and mean (black thick line).</s><s>Classification remains at chance level for all participants until D3, and then increases significantly with increasing distortion level.</s><s>Participant VPib reaches the highest accuracy, 0.99 at D6. found a significant linear negative correlation between the average alpha activity during the experiment and the accuracy of classification of the quality change.</s><s>In the CSP analysis, this is the case for all spectral features we considered (3, 6 Hz, filter bank).</s><s>The Pearson correlation coefficients were r = −0.64 for classification based on spatio-temporal features (p &lt; 0.02), r = −0.70 for CSP classification at 3 Hz (p &lt; 0.01), r = −0.60 for the CSP classification at 6 Hz (p &lt; 0.05), r = −0.69 for CSP classification with the filter bank (p &lt; 0.01).</s><s>Accordingly, brain signals of participants with a high level of alpha activity were less modulated by quality changes.</s><s>One reason for an increased level of alpha activity is decreased attention.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p><s>EEG data can be analyzed from two main points of view, the one that refers to the modulation of brain rhythms and the one that takes into account the time-locked components of brain activity (Lemm et al 2011).</s><s>In motor imagery-based BCIs a well established method for signal processing is the CSP filtering, which finds spatial filters that discriminate the areas of the motor cortex where the modulation of the oscillatory idle state is the highest <ref type="bibr">(Blankertz et al 2008)</ref>.</s><s>In ERP-based BCIs instead, since the ERPs are time locked to the target event to classify, it is crucial to identify the time intervals in which the discrimination between the targets and non-targets Figure <ref type="figure">8</ref>. Correlation amplitude of alpha rhythm-classification accuracy at D6.</s><s>The upper plots refer to the classification accuracy based on the CSP method at 3 Hz (left) and 6 Hz (right), the lower plots to CSP with filter bank (left) and to classification based on spatio-temporal features (right).</s><s>Each dot represents one participant, the yellow ones refer to participants considered as outliers.</s><s>Regression lines are also displayed.</s><s>For all the methods the amplitude of the average alpha rhythm during the experiment significantly negatively correlated with the average accuracy at D6.</s><s>This suggest that the alpha rhythm plays a role in the maximum classification accuracy achieved based on SSVEPs.</s></p><p><s>is the highest.</s><s>The classification of spatio-temporal features using LDA with a regularization by shrinkage of the covariance matrix was found to be successful <ref type="bibr">(Blankertz et al 2011, Bartz and</ref><ref type="bibr">Müller 2013)</ref>.</s><s>SSVEPs are evoked potentials timelocked to the attended stimulus, which differ from ERPs by being in steady state and not transient.</s><s>In most of the BCIs studies based on SSVEPs the EEG signal is processed in the frequency domain because usually the stimuli have different frequencies in order to differentiate several target events.</s><s>In our case, we adopted one frequency for the stimulation, since our purpose was not to operate a BCI (Cheng et al 2002, <ref type="bibr" target="#b14">Müller-Putz et al 2005</ref><ref type="bibr" target="#b1">, Allison et al 2008)</ref> but to modulate the EEG activity according to the quality change of the stimuli.</s><s>We can still exploit the oscillatory nature of the signal with a narrow band-pass filtering around the frequencies in which the variance of the signal is enhanced compared to the background state.</s><s>But also the temporal evolution of the evoked potential can be investigated, since the SSVEPs are time locked to the quality changes.</s><s>The two methods used in this study for offline analysis take into account both phenomena, and lead to results with comparable trends but different accuracies.</s><s>Classification results show mean offline accuracies up to 0.74 and 0.84 for the maximum distortion level D6, respectively for the CSP method and the spatiotemporal features method.</s><s>Both methods show the same trend in the classification results, that is, the performances do not exceed chance level significantly for the first three levels of distortion, while from D4 to D6 they increase significantly and linearly with the quality change.</s><s>These results suggest that the quality changes introduced in the textures by the compression algorithm modulate the ongoing EEG activity eliciting SSVEPs which can be classified over the perception threshold.</s><s>The accuracies achieved in the proposed design are in the same range of those exploiting the P3 component as neural feature for video quality assessment via EEG.</s><s>Note that, performances between these two types of studies are very difficult to compare because of the different design of the stimuli used in P3-based and SSVEP-based experiments.</s><s><ref type="bibr" target="#b15">Mustafa et al (2012)</ref> reach a mean single-trial classification accuracy up to 85% in classifying the presence of artifacts in videos versus ground truth, using a wavelet-based classification.</s><s>The most severe artifacts reach a mean detection of 93%.</s><s>Lindemann et al (2011) also perform single trial classification of the artifact in videos vs ground truth, using principal component analysis (PCA) for dimensionality reduction and support vector machine (SVM) as classifier.</s><s>They use only the trials correctly detected by the participants via button press, achieving a mean classification accuracy of 76.5% for the most obvious artifacts and 73.5% for the less obvious.</s><s>Scholler et al (2012) report a single-trial classification with AUC values close to 1 for the highest level of distortion in most the subjects.</s><s>This result is obtained after filtering raw data with a LDA filter, in which the weights are computed based on the signed squared biserial correlation coefficient between the trials with the highest quality change and the trials without quality change.</s><s>The classification accuracies reported by Scholler and Lindemann refer only to trials correctly identified by the participants at the behavioral level.</s></p><p><s>Scholler et al also report for three participants an average 65% accuracy in classifying the trials in which the quality change was present but not detected by the subjects, advancing the hypothesis of higher sensitivity of the EEG compared to the behavioral response.</s><s>Since our study capitalizes on SSVEPs elicited in condition of passive viewing, we could not differentiate between trials potentially 'detected' by the participants and trials in which participants might have had a lower level of attention.</s><s>So our accuracies refer to the overall number trials, without pre-screening or rejections.</s><s>About the number of trials, it has to be pointed out that a SSVEP-based paradigm allows the recording of an amount much higher compared to P3-based paradigms.</s><s>In fact, in the latter the number of epochs containing the actual target event is just a fraction of the total amount of epochs used in the experiment.</s><s>In our design, each epoch is a 'target', meaning that all the trials contain useful information for the quality assessment.</s><s>As already mentioned in the introduction, the number of events (as event referring to the occurrence of evoked potentials useful for the evaluation) is increased more than ten times if compared to the P3-based study of <ref type="bibr" target="#b35">Scholler et al (2012)</ref>.</s><s>That is to say that a SSVEP-based assessment can be more than ten times faster than a P3 one.</s><s>If we want to quantify the speed of detection in terms of bit rate <ref type="bibr" target="#b44">(Wolpaw et al 1998)</ref>, our system reaches about 27 bits min −1 for classification of D6 with the spatio-temporal features method.</s><s>This performance is achieved using just 13 min of EEG recording for the training of the classifier.</s></p><p><s>Another fundamental aspect to consider is the comparison between the neural assessment and the behavioral one.</s><s>Studies which foresee participants pressing a button at detection of artifacts or distortions allow a more straightforward comparison between the neural and the behavioral response <ref type="bibr">(Porbadnigk et al 2013)</ref>.</s><s>However, in these cases there is no indication of the actual level of quality perceived by the participants, that is, how well they could detect the distortions and how annoying the detected artifact could be.</s><s>And this is actually an important detail to take into account in the implementation of video codecs.</s><s>Therefore, unlike most of previous works, we performed the behavioral assessment in a separate test in which we collected the MOS values that represent the actual subjective rate normally used in image quality assessment.</s><s>We then linked these results to the accuracies of the classification of the SSVEPs modulated by the quality changes.</s><s>This represents a key factor in the design of the paradigm, since previous studies usually refer the classification performances to the presence of an artifact or correlate the results with the absolute value of the quality change.</s><s>Also in <ref type="bibr" target="#b15">Mustafa et al (2012)</ref> participants performed the assessment via a MOS scale after each video, but they do not report the correlation with the classification performances.</s><s>As already mentioned in the introduction, <ref type="bibr" target="#b4">Arndt et al (2014)</ref> correlate the P3 amplitudes with the MOS values of the participants.</s><s>They found an average significant correlation in three experiments, in which video quality degradation is caused by artificial blockiness.</s><s>Anyway, this correlation is not reported in the experiment in which the authors investigate a more realistic scenario, using a real existing codec for introducing video distortions.</s><s>In our case, we could prove a significant correlation between the neural and the behavioral assessment using the MOS values.</s><s>This result is valid for all participants for the spatial-temporal features classification, and for all except two for the CSP based classification.</s><s>The two participants who show no correlation also have very low performances in general in CSP classification, both with maximum accuracy of 0.57 at D6.</s><s>The spectra of these two participants also do not display evident peaks at 3 and 6 Hz until level D5.</s><s>This result suggests that for those participants the spectral features might not be very informative and the spatio-temporal feature classification is preferable (classification accuracy at D6 of 0.76 and 0.75, respectively).</s><s>In both the methods we used, mean classification remains at chance level for distortions D1, D2, D3 and increases linearly and significantly from D4 to D6.</s><s>In general, the same trend is visible in most of the participants.</s><s>Clearly, not all participants are expected to be sensitive in the same manner to the flicker of the quality change.</s><s>For example, considering the classification method based on spatio-temporal features it can be noticed that five participants have a classification accuracy which stays at chance level at D4, a distortion level which is supposed to be above perception threshold.</s><s>Even though also for these participants accuracy increases significantly with increasing distortion, at D6 it does not go beyond 75%.</s><s>For these participants the SSVEPs are less pronounced in the time domain, as well as the spectra peaks at the discriminative frequencies.</s><s>They seem to be 'less sensitive' to the quality changes in general.</s><s>There might be different reasons for such phenomenon, which lie in the nature of perception itself.</s><s>One further reason can be found in the results of the analysis of the occipital alpha rhythm during the experiment.</s><s>A significant negative correlation was found between the classification performances and the mean magnitude of the alpha peak in the occipital cortex, evaluated at electrode Oz.</s><s>In literature, the relationship between the spontaneous oscillatory activity when the stimulus is presented and the perception of the stimulus itself has been widely investigated.</s><s>In particular, different studies <ref type="bibr">(Brandt and Jansen 1991</ref><ref type="bibr">, Jansen and Brandt 1991</ref><ref type="bibr">, Ergenoglu et al 2004</ref><ref type="bibr">, Hanslmayr et al 2005</ref><ref type="bibr">, 2007</ref><ref type="bibr" target="#b32">, Romei et al 2008</ref><ref type="bibr">, Busch et al 2009</ref><ref type="bibr">, Busch and VanRullen 2010)</ref> show that oscillations in the alpha frequency band interfere with the processing of the visual information and modulate the gain of the visual system.</s><s>A decreased oscillatory activity is thought to reflect a state of enhanced cortical excitability, and increased activity to reflect a state of cortical idling or inhibition in which excitability is reduced.</s><s>Our results show that the magnitude of the alphapower during the experiment significantly correlates negatively with the classification accuracies, that directly reflect the trend of the SSVEPs.</s><s>This could justify the attenuated modulation and poor classification performances of some participants.</s><s>The high alpha rhythm during the experiment could have kept their occipital cortex in an idle state preventing a strong modulation by the flickering of the quality change.</s><s>Despite not showing a pronounced neural modulation at high levels of quality changes (for example D4), these participants were nevertheless able to perceive the corresponding quality changes during the behavioral assessment.</s><s>Clearly, conclusions on these findings have to be made carefully.</s><s>First, the procedure of the rating and the EEG experiment are intrinsically different: the behavioral assessment is based on the comparison between the undistorted and the distorted image, which are presented simultaneously in the display, and participants have 10 s to carefully look at them in a free viewing condition.</s><s>Participants are then asked to use a mouse to select the decided score, moving a scroll bar between one and nine.</s><s>All these tasks require people to be active and overtly engaged, and maybe more concentrated in recognizing a possible distortion in the texture to evaluate.</s><s>During the EEG recording, participants were asked to focus straight in the center of the video, limiting any kind of movement.</s><s>This task is quite tiring and a reason why for some people it could have been much more difficult to concentrate.</s><s>The state of excessive relax reflected by the high alpha rhythm could prevent them to properly pay attention to the quality changes.</s><s>In general, it is well known in SSVEP literature that the amplitude of the modulation is substantially increased by attention <ref type="bibr" target="#b13">(Müller et al 1998</ref><ref type="bibr" target="#b12">(Müller et al , 2003) )</ref> and that an attended flickering stimulus elicits a larger steady-state response than the same stimulus when unattended <ref type="bibr">(Ding et al 2006</ref><ref type="bibr" target="#b10">, Müller et al 2006)</ref>.</s><s>This is also the main principle used by SSVEP-based BCIs (Cheng et al 2002, <ref type="bibr" target="#b14">Müller-Putz et al 2005</ref><ref type="bibr" target="#b1">, Allison et al 2008)</ref>, in which stimuli are presented simultaneously in different locations and frequencies.</s><s>The user can focus the attention on the target stimulus whose frequency would modulate the steady-state response accordingly.</s><s>Another reason for the poorer performances of these participants could lie in the choice of the stimulus frequency.</s><s>For some people not all the harmonics are modulated by attention in the same way.</s><s>For example, in the study of <ref type="bibr" target="#b20">Pei et al (2002)</ref> stimuli at 2.4 and 3 Hz were chosen and the harmonic responses at 4.8 and 6 Hz were modulated by attention, while the responses at 9.8 and 12 Hz were not.</s><s>We had a similar result showing the steady-state response present mainly at 3 and 6 Hz, while the modulation of higher harmonics was negligible.</s><s>The choice of the stimulation frequency was also addressed in the study of <ref type="bibr">Kelly et al (2005)</ref>.</s><s>They developed a SSVEP-based BCI in which stimuli were presented at a flickering frequency within and outside the alpha range, respectively.</s><s>Results show that no advantage is gained by using one or the other solution when working with SSVEP features, but the authors suggest that the choice of the stimulus frequency should be specific to individual subjects.</s><s>In our study, the frequency was chosen beforehand and kept constant for all the subjects.</s><s>Some of them might have been less sensitive to that specific frequency, no matters their engagement in the task or the idle state of their alpha rhythm.</s><s>Some research show a percentage of people being not able to operate an SSVEP-based BCI at all <ref type="bibr" target="#b1">(Allison et al 2008</ref><ref type="bibr">, 2010</ref><ref type="bibr" target="#b43">, Volosyak et al 2011)</ref>.</s><s>This percentage is lower than the estimation of the 'BCI-illiterates' in motor-imaging based BCIs (between 15-30%, <ref type="bibr" target="#b9">Blankertz et al (2010</ref><ref type="bibr">), Hammer et al (2012)</ref>, <ref type="bibr" target="#b39">Suk et al (2014)</ref>), but it is an aspect worthwhile to study in the future.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Limitation and future developments</head><p><s>The six levels of distortions were associated to the QP which were chosen in a pilot study such that the perceived degradation at the behavioral level was the same for all the textures.</s><s>In the pilot study we performed a behavioral test like described in the methods section, and finally we chose the QP for each texture such that the average MOS values across subjects was slightly below perception threshold for D2 and slightly above for D3.</s><s>In the main study we could not reproduce the same results for a new pool of subjects.</s><s>On average, both D2 and D3 were below the perception threshold both on the behavioral level and on the EEG level, therefore not discriminable by the classifier.</s><s>Future studies could take into account more levels of the QP, especially increasing the number of the levels around the perception threshold in order to investigate the sensitivity of the EEG in relationship with the behavioral results.</s><s>In our results we did not find an evidence of higher sensitivity of the EEG compared to the overt response, suggesting that there were no distortions processed unconsciously which would not result at the behavioral level.</s><s>Previous ERP studies <ref type="bibr" target="#b27">(Porbadnigk et al 2011</ref><ref type="bibr" target="#b35">, Scholler et al 2012)</ref> suggest that some people might show an unconscious neural processing of quality changes.</s><s>Therefore, it is crucial to investigate more in depth the relationship between EEG response and behavioral one at the perception threshold, in order to prevent the implementation of video coding methods which would introduce distortions potentially perceived by the most sensitive people.</s><s>As already mentioned, the stimuli presentation in the behavioral test and in the EEG part are intrinsically different.</s><s>In the EEG part, participants had to attend to a video in which the textures were presented in succession with a quality changes at a frequency of 3 Hz and in the behavioral assessment they had to rate the quality of the same textures displayed together with the reference image (following the video quality assessment standard tests performed according to <ref type="bibr">ITU (2002)</ref>).</s><s>A future study could consider to present the images to be assessed behaviorally in a way that would be more consistent with the stimuli presentation in the EEG part.</s><s>For example, the reference image and the distorted one can be presented right after each other for several seconds before the rating.</s><s>The behavioral assessment could be made after each video, helping participants to be more awake and concentrated during the recordings.</s><s>We showed the correlation between the perceived quality change and the size of the neural effect.</s><s>In this study image degradation was caused by a specific compression algorithm, which introduces changes in more fundamental image features.</s><s>The identification of which specific feature is responsible of the generation of the SSVEPs was beyond the scope of the paper.</s><s>This leads to the conclusion that the applicability of this method to other image compression algorithms has to be considered with caution, since other algorithms may control the image features in a different way.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p><s>We showed that the quality changes introduced in natural textures by the HM10.0 test model of the H.265/MPEG-HEVC standard could be measured by EEG.</s><s>For that, the distorted signal modulated the neural signal, eliciting SSVEPs that could be classified with high accuracies over the perception threshold.</s><s>The proposed experimental design let us collect a number of epochs an order of magnitude higher than previous P3-based designs, in the same period of time.</s><s>The results of the neural assessment significantly correlated with the MOS values of the behavioral assessment.</s><s>Taking into account the characteristic of the visual system which sees people having different sensitivity to the stimuli, this design could be further improved choosing subject-specific stimulation frequencies, increasing the number of the assessed quality levels, and monitoring the level of the alpha rhythm during the experiment.</s><s>In general, one can conclude that assessing video quality via the SSVEP-based paradigm is not only a useful complement to the standard behavioral tests but also a significantly faster alternative to P3-based paradigms.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc><div><p><s>, Lindemann et al 2011, Lindemann and Magnor 2011, Antons et al 2012, Mustafa et al 2012, Scholler et al 2012, Perez and Delechelle 2013, Porbadnigk et al 2013, Moldovan et al 2013, Arndt et al 2014, Bosse et al 2014, Kroupi et al 2014).</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc><div><p><s>Figure 1.</s><s>Stimuli.</s><s>Six natural images were chosen as a basis for stimuli generation.</s><s>The textures in the upper row represent stone, scarf and oatmeal, in the lower row gray rubber, gray flakes and blanket, all in their undistorted form.</s><s>They have been degraded in six levels of quality coded with the HM10.0 test model HEVC standard and grouped together to form videos with a frame rate of 3 Hz.</s></p></div></figDesc><graphic coords="4,80.48,69.08,437.10,288.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc><div><p><s>Figure2.</s><s>Video structure.</s><s>Each video (trial) comprised the six textures presented in all the levels of distortion (D1, ... , D6) in a random order.</s><s>Each texture was displayed distorted for 333 ms, followed by the undistorted form for 333 ms (D0) and the same succession was repeated four times for each level.</s><s>Such alternation of quality changes produced a flickering effect eliciting SSVEPs when perceived by the participant.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc><div><p><s>Figure3.</s><s>Behavioral assessment.</s><s>Textures were presented in a random order, the distorted form on the left side of the display and the undistorted on the right (here an example of blanket with degradation level D6).</s><s>Participants had 10 s to look at them in free viewing condition before the evaluation of the quality level.</s><s>Each level of distortion was presented for three times, for each texture.</s></p></div></figDesc><graphic coords="5,110.42,69.08,377.00,185.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc><div><p><s>Figure 4. Behavioral assessment.</s><s>Left: table representing the nine level scale of scores that the participants could give to the quality of the distorted texture compared to the undistorted.</s><s>Right: mean opinion scores given by participants for each quality level.</s><s>Error bars refer to the standard deviation from the mean.</s><s>Until level D3, mean MOS values remain above or at the level of perception threshold (level 8) and from D4 they decrease linearly with increasing distortion.</s></p></div></figDesc><graphic coords="7,294.08,69.08,192.30,202.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc><div><p><s>Figure 5. SSVEPs.</s><s>Left: brain activity of participant VPib averaged over trials and textures at channel Oz is displayed from D0 to D6. Dist refers to the onset of the distorted texture, Ref to the undistorted.</s><s>Until level D3 there is no clear modulation of the neural signal by the quality changes.</s><s>From level D4 SSVEPs are visible with amplitude increasing with increasing distortion level.</s><s>Right: spectra of the brain activity of participant VPib at channel Oz averaged over trials and textures.</s><s>As in the time domain analysis, no clear modulation is visible until D3.</s><s>From D4 to D6 (blue, violet and magenta lines), peaks at 3, 6 and 9 Hz are evident.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc><div><p><s>Figure6.</s><s>CSP analysis.</s><s>Left and middle: scalp plots display the CSP filters (upper row) and patterns (lower row) of participant VPib, for brain activity filtered around 3 Hz (left) and 6 Hz (middle).</s><s>CSP filters were calculated considering a subgroup of epochs referring to D6 and D0.</s><s>The patterns clearly display that the highest variance of the neural signal takes place in the occipital cortex, where the visual information is processed.</s><s>Right: average classification accuracies using LDA after CSP filtering, for features at 3 Hz (red), 6 Hz (green) and filter bank (blue), respectively.</s><s>Error bars refer to the standard deviation of the participants accuracies from the mean.</s><s>Classification accuracy increases significantly from D4 to D6, but no significant difference is found in the choice of the filtering frequency.</s></p></div></figDesc><graphic coords="9,89.78,86.06,226.87,207.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc><div><p><s>Figure 7. Classification based on spatio-temporal features.</s><s>Left: grand average brain activity over all participants at channel Oz, at maximum distortion level D6.</s><s>The magenta line represents class 1 and the gray line class 2. Scalp plots underneath refer to the shaded areas in the time plot and display the magnitude of the − r sign2 for each channel.</s><s>The highest discrimination between the two classes takes place in the occipital cortex.</s><s>Right: classification accuracies using shrinkage LDA for all participants (colored lines) and mean (black thick line).</s><s>Classification remains at chance level for all participants until D3, and then increases significantly with increasing distortion level.</s><s>Participant VPib reaches the highest accuracy, 0.99 at D6.</s></p></div></figDesc><graphic coords="10,165.56,213.56,113.04,62.35" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">J. Neural Eng. 12 (2015) 026012 L Acqualagna et al</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Data distribution was checked with the one-sample Kolmogorov-Smirnov</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p><s>We acknowledge financial support by the BMBF Grant N. 01GQ0850.</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bci demographics: how many (and what kinds of) people can use anssvep bci?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Valbuena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Teymourian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Volosyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graser</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNSRE.2009.2039495</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards an independent brain-computer interface using steady state visual evoked potentials</title>
		<author>
			<persName><forename type="first">B Z</forename><surname>Allison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D J</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schalk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M M</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J R</forename><surname>Wolpaw</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.clinph.2007.09.121</idno>
	</analytic>
	<monogr>
		<title level="j">Clin. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="399" to="408" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Filter bank common spatial pattern (fbcsp) in brain-computer interface IEEE Int</title>
		<author>
			<persName><forename type="first">K K</forename><surname>Ang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z Y</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Neural Networks pp</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="2390" to="2397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analyzing speech quality perception using electroencephalography</title>
		<author>
			<persName><forename type="first">J-N</forename><surname>Antons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arndt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Porbadnigk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Curio</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTSP.2012.2191936</idno>
	</analytic>
	<monogr>
		<title level="j">J. Sel. Top. Signal Process. IEEE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="721" to="731" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using electroencephalography to measure perceived video quality</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arndt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-N</forename><surname>Antons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Curio</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSTSP.2014.2313026</idno>
	</analytic>
	<monogr>
		<title level="j">J. Sel. Top. Signal Process. IEEE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="366" to="376" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pre-and poststimulus alpha rhythms are related to conscious visual perception: a high-resolution EEG study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vecchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bultrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G L</forename><surname>Romani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P M</forename><surname>Rossini</surname></persName>
		</author>
		<idno type="DOI">10.1093/cercor/bhj104</idno>
	</analytic>
	<monogr>
		<title level="j">Cereb Cortex</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1690" to="1700" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generalizing analytic shrinkage for arbitrary covariance structures</title>
		<author>
			<persName><forename type="first">D M</forename><surname>Bartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Proc. Syst</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1869" to="1877" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Blankertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lemm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M S</forename><surname>Treder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haufe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-R</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Single-trial analysis and classification of ERP components-a tutorial Neuroimage</title>
		<idno type="DOI">10.1016/j.neuroimage.2010.06.048</idno>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="814" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Blankertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sannelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Halder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E M</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Curio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dickhaus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature-selective attention enhances color signals in early visual areas of the human brain Proc</title>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Trujillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valdes-Sosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malinowski</forename></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hillyard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0606668103</idno>
	</analytic>
	<monogr>
		<title level="j">Natl Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="14250" to="14254" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Can the spotlight of attention be shaped like a doughnut? evidence from steady-state visual evoked potentials</title>
		<author>
			<persName><forename type="first">M M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hübner</surname></persName>
		</author>
		<idno type="DOI">10.1111/1467-9280.00422</idno>
	</analytic>
	<monogr>
		<title level="j">Psychol. Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="119" to="124" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sustained division of the attentional spotlight</title>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hillyard</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature01812</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">424</biblScope>
			<biblScope unit="page" from="309" to="312" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Effects of spatial selective attention on the steady-state visual evoked potential in the 20-28 Hz range Cogn</title>
		<author>
			<persName><forename type="first">M M</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T W</forename><surname>Picton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valdes-Sosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W A</forename><surname>Teder-Sälejärvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S A</forename><surname>Hillyard</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0926-6410(97)00036-0</idno>
	</analytic>
	<monogr>
		<title level="j">Brain Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="249" to="261" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Steady-state visual evoked potential (ssvep)-based communication: impact of harmonic frequency components</title>
		<author>
			<persName><forename type="first">G R</forename><surname>Müller-Putz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brauneis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pfurtscheller</surname></persName>
		</author>
		<idno type="DOI">10.1088/1741-2560/2/4/008</idno>
	</analytic>
	<monogr>
		<title level="j">J. Neural Eng</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">123</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Single-trial EEG classification of artifacts in videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
		<idno type="DOI">10.1145/2325722.2325725</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Appl. Percept</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Measuring perceptual differences between compressed and uncompressed video sequences using the swept-parameter visual evoked potential</title>
		<author>
			<persName><forename type="first">A</forename><surname>Norcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cooper</forename><forename type="middle">E</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<idno type="DOI">10.1167/14.10.649</idno>
	</analytic>
	<monogr>
		<title level="j">J. Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="649" to="649" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Outex-new framework for empirical evaluation of texture analysis algorithms Conf. on Pattern Recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Viertola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kyllonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huovinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Int</title>
				<meeting>16th Int<address><addrLine>Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="701" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A robust and self-paced bci system based on a four class ssvep paradigm: algorithms and protocols for a high-transfer-rate direct brain communication</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Turconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Andreoni</surname></persName>
		</author>
		<idno type="DOI">10.1155/2009/864564</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Intell. Neurosci</title>
		<imprint>
			<biblScope unit="page">864564</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human cerebral activation during steady-state visual-evoked responses</title>
		<author>
			<persName><forename type="first">M A</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Artieda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arbizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valencia</forename><forename type="middle">M</forename><surname>Masdeu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="11621" to="11627" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural correlates of objectbased attention</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M W</forename><surname>Pettet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Norcia</surname></persName>
		</author>
		<idno type="DOI">10.1167/2.9.1</idno>
	</analytic>
	<monogr>
		<title level="j">J. Vision</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the measurement of image quality perception using frontal eeg analysis Int</title>
		<author>
			<persName><forename type="first">J</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Delechelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Smart Communications in Network Technologies (SaCoNeT)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Event-related eeg/meg synchronization and desynchronization: basic principles</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pfurtscheller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lopes Da Silva</surname></persName>
		</author>
		<idno type="DOI">10.1016/S1388-2457(99)00141-8</idno>
	</analytic>
	<monogr>
		<title level="j">Clin. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="1842" to="1857" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The p300 wave of the human event-related potential</title>
		<author>
			<persName><forename type="first">T</forename><surname>Picton</surname></persName>
		</author>
		<idno type="DOI">10.1097/00004691-199210000-00002</idno>
	</analytic>
	<monogr>
		<title level="j">J. Clin. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="456" to="479" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Guidelines for using human event-related potentials to study cognition: recording standards and publication criteria</title>
		<author>
			<persName><forename type="first">T</forename><surname>Picton</surname></persName>
		</author>
		<idno type="DOI">10.1111/1469-8986.3720127</idno>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="127" to="152" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using ERPs for assessing the (sub) conscious perception of noise</title>
		<author>
			<persName><forename type="first">A K</forename><surname>Porbadnigk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-N</forename><surname>Antons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Blankertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M S</forename><surname>Treder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Curio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf. Proc. IEEE Eng. Med. Biol. Soc</title>
		<imprint>
			<biblScope unit="volume">2010</biblScope>
			<biblScope unit="page" from="2690" to="2693" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoding brain states during auditory perception by supervising unsupervised learning</title>
		<author>
			<persName><forename type="first">A K</forename><surname>Porbadnigk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Görnitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-R</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.5626/JCSE.2013.7.2.112</idno>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="112" to="121" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Revealing the neural response to imperceptible peripheral flicker with machine learning</title>
		<author>
			<persName><forename type="first">A K</forename><surname>Porbadnigk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scholler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Blankertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Born</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scholl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curio</forename><forename type="middle">G</forename><surname>Treder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf. Proc. IEEE Eng. Med. Biol. Soc</title>
		<imprint>
			<biblScope unit="page" from="3692" to="3695" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Singletrial analysis of the neural correlates of speech quality perception</title>
		<author>
			<persName><forename type="first">A K</forename><surname>Porbadnigk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M S</forename><surname>Treder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Blankertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J N</forename><surname>Antons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Curio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-R</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1088/1741-2560/10/5/056003</idno>
	</analytic>
	<monogr>
		<title level="j">J. Neural. Eng</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">56003</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The response of cat visual cortex to flicker stimuli of variable frequency</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Singer</surname></persName>
		</author>
		<idno type="DOI">10.1046/j.1460-9568.1998.00197.x</idno>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1856" to="1877" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Optimal spatial filtering of single trial eeg during imagined hand movement</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ramoser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Müller-Gerking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pfurtscheller</surname></persName>
		</author>
		<idno type="DOI">10.1109/86.895946</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="441" to="446" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Human Brain Electrophysiology: Evoked Potentials and Evoked Magnetic Fields in Science and Medicine</title>
		<author>
			<persName><forename type="first">D</forename><surname>Regan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Elsevier</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Resting electroencephalogram alpha-power over posterior sites indexes baseline visual cortex excitability</title>
		<author>
			<persName><forename type="first">V</forename><surname>Romei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rihs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Brodbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroreport</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="203" to="208" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stationary common spatial patterns for brain-computer interfacing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vidaurre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-R</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
		<idno type="DOI">10.1088/1741-2560/9/2/026013</idno>
	</analytic>
	<monogr>
		<title level="j">J. Neural Eng</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">26013</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Common spatial pattern patches-an optimized filter ensemble for adaptive brain-computer interfaces</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sannelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vidaurre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Blankertz</surname></persName>
		</author>
		<idno type="DOI">10.1088/1741-2560/8/2/025012</idno>
	</analytic>
	<monogr>
		<title level="j">J. Neural Eng</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">25012</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards a direct measure of video quality perception using EEG</title>
		<author>
			<persName><forename type="first">S</forename><surname>Scholler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M S</forename><surname>Treder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Blankertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Curio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-R</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2012.2187672</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="2619" to="2629" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A p300 event-related potential braincomputer interface (bci): the effects of matrix size and inter stimulus interval on performance</title>
		<author>
			<persName><forename type="first">E W</forename><surname>Sellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D J</forename><surname>Krusienski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D J</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T M</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J R</forename><surname>Wolpaw</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.biopsycho.2006.04.007</idno>
	</analytic>
	<monogr>
		<title level="j">Biol. Psychol</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="242" to="252" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Motion tuned spatiotemporal quality assessment of natural videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A C</forename><surname>Bovic</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2009.2034992</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="335" to="350" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The intracranial topography of the p3 event-related potential elicited during auditory oddball</title>
		<author>
			<persName><forename type="first">M E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Halgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sokolik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baudena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Musolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liegeois-Chauvel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chauvel</surname></persName>
		</author>
		<idno type="DOI">10.1016/0013-4694(90)90018-F</idno>
	</analytic>
	<monogr>
		<title level="j">Electroencephalogr. Clin. Neurophysiol</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="235" to="248" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Predicting bci subject performance using probabilistic spatiotemporal filters</title>
		<author>
			<persName><forename type="first">H-I</forename><surname>Suk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fazli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mehnert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee S-W</forename></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0087056</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">e87056</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Overview of the high efficiency video coding (hevc) standard</title>
		<author>
			<persName><forename type="first">G J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W-J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2012.2221191</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1649" to="1668" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A regularized discriminative framework for eeg analysis with application to brain-computer interface</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-R</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2009.07.045</idno>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="415" to="432" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Steadystate visually evoked potentials: focus on essential paradigms and future perspectives</title>
		<author>
			<persName><forename type="first">F-B</forename><surname>Vialatte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurice</forename><forename type="middle">M</forename><surname>Dauwels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.pneurobio.2009.11.005</idno>
	</analytic>
	<monogr>
		<title level="j">Prog. Neurobiol</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="418" to="438" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bci demographics ii: how many (and what kinds of) people can use a high-frequency ssvep bci?</title>
		<author>
			<persName><forename type="first">I</forename><surname>Volosyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Valbuena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Malechka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graser</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNSRE.2011.2121919</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Syst. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="232" to="239" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">EEG-based communication: improved accuracy by response verification</title>
		<author>
			<persName><forename type="first">J R</forename><surname>Wolpaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramoser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D J</forename><surname>Mcfarland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pfurtscheller</surname></persName>
		</author>
		<idno type="DOI">10.1109/86.712231</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Rehabil. Eng</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="326" to="333" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
