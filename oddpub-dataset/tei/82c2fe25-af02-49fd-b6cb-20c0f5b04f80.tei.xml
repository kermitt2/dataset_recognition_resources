<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Valid population inference for information-based imaging: From the second-level t-test to prevalence inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-08-10">10 Aug 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Carsten</forename><surname>Allefeld</surname></persName>
							<email>carsten.allefeld@bccn-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center of Advanced Neuroimaging</orgName>
								<orgName type="institution">Bernstein Center for Computational Neuroscience</orgName>
								<address>
									<settlement>Berlin</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Neurology, and Excellence Cluster NeuroCure</orgName>
								<orgName type="institution">Charité -Universitätsmedizin Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Görgen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Center of Advanced Neuroimaging</orgName>
								<orgName type="institution">Bernstein Center for Computational Neuroscience</orgName>
								<address>
									<settlement>Berlin</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Neurology, and Excellence Cluster NeuroCure</orgName>
								<orgName type="institution">Charité -Universitätsmedizin Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John-Dylan</forename><surname>Haynes</surname></persName>
							<email>haynes@bccn-berlin.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center of Advanced Neuroimaging</orgName>
								<orgName type="institution">Bernstein Center for Computational Neuroscience</orgName>
								<address>
									<settlement>Berlin</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Neurology, and Excellence Cluster NeuroCure</orgName>
								<orgName type="institution">Charité -Universitätsmedizin Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Mind and Brain and Department of Psychology</orgName>
								<orgName type="institution">Humboldt-Universität zu Berlin</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Charité-Campus Mitte</orgName>
								<address>
									<addrLine>Philippstr. 13, Haus 6</addrLine>
									<postCode>10115</postCode>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Valid population inference for information-based imaging: From the second-level t-test to prevalence inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-08-10">10 Aug 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">2F5B8804254264A057219760081A33A5</idno>
					<idno type="DOI">10.1016/j.neuroimage.2016.07.040</idno>
					<idno type="arXiv">arXiv:1512.00810v3[q-bio.NC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2-SNAPSHOT" ident="GROBID" when="2022-05-18T11:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Allefeld</term>
					<term>K. Görgen</term>
					<term>J.-D. Haynes. Valid population inference for information-based imaging: From the second-level t-test to prevalence inference. NeuroImage</term>
					<term>141: 378-392</term>
					<term>2016 information-based imaging</term>
					<term>multivariate pattern analysis</term>
					<term>t-test</term>
					<term>population inference</term>
					<term>effect prevalence</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s>In multivariate pattern analysis of neuroimaging data, 'second-level' inference is often performed by entering classification accuracies into a t-test vs chance level across subjects.</s><s>We argue that while the random-effects analysis implemented by the t-test does provide population inference if applied to activation differences, it fails to do so in the case of classification accuracy or other 'information-like' measures, because the true value of such measures can never be below chance level.</s><s>This constraint changes the meaning of the population-level null hypothesis being tested, which becomes equivalent to the global null hypothesis that there is no effect in any subject in the population.</s><s>Consequently, rejecting it only allows to infer that there are some subjects in which there is an information effect, but not that it generalizes, rendering it effectively equivalent to fixed-effects analysis.</s><s>This statement is supported by theoretical arguments as well as simulations.</s><s>We review possible alternative approaches to population inference for information-based imaging, converging on the idea that it should not target the mean, but the prevalence of the effect in the population.</s><s>One method to do so, 'permutationbased information prevalence inference using the minimum statistic', is described in detail and applied to empirical data.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p><s>Since the seminal work of <ref type="bibr" target="#b25">Haxby et al. (2001)</ref>, an increasing number of neuroimaging studies have employed multivariate methods to complement the established massunivariate approach <ref type="bibr" target="#b18">(Friston et al., 1995)</ref> to the analysis of functional magnetic resonance imaging (fMRI) data, a field now known as multivariate pattern analysis (MVPA; <ref type="bibr" target="#b47">Norman et al., 2006)</ref>.</s><s>Most MVPA studies use classification <ref type="bibr" target="#b52">(Pereira et al., 2009)</ref> to examine activation patterns; the accuracy of a classifier in distinguishing activation patterns associated with different experimental conditions serves as a measure of multivariate effect strength.</s><s>Since the target of MVPA is not a generally increased or decreased level of activation but the information content of activation patterns (cf.</s><s><ref type="bibr" target="#b51">Pereira and Botvinick, 2011)</ref>, it has also been characterized as information-based imaging and distinguished from traditional activation-based imaging <ref type="bibr" target="#b38">(Kriegeskorte et al., 2006)</ref>.</s></p><p><s>Many methodological aspects of MVPA have already been discussed in detail: what kind of classifier to use <ref type="bibr" target="#b7">(Cox and Savoy, 2003;</ref><ref type="bibr" target="#b47">Norman et al., 2006)</ref>, whether to adapt parametric multivariate statistics instead of classifiers <ref type="bibr" target="#b0">(Allefeld and Haynes, 2014;</ref><ref type="bibr" target="#b45">Nili et al., 2014)</ref>, how to understand searchlight-based accuracy maps <ref type="bibr">(Etzel et al., 2013)</ref>, or how classifier weights can be made interpretable <ref type="bibr" target="#b23">(Haufe et al., 2014;</ref><ref type="bibr" target="#b33">Hoyos-Idrobo et al., 2015)</ref>.</s><s>By contrast, the topic of population inference based on per-subject measures of information content, i.e. the question whether an information effect observed in a sample of subjects generalizes to the population these subjects were recruited from, has not yet received sufficient attention (but see <ref type="bibr" target="#b2">Brodersen et al., 2013)</ref>.</s></p><p><s>In univariate analysis of multi-subject fMRI studies, the standard way to achieve population inference is to perform a 'second-level' null hypothesis test <ref type="bibr" target="#b32">(Holmes and Friston, 1998)</ref>.</s><s>For each subject, a 'first-level' contrast (activation difference) is computed, and this contrast enters a second-level analysis, a t-test or an ANOVA.</s><s>Specifically for a simple one-sided t-test vs 0, reaching statistical significance allows to infer that the experimental manipulation is associated with an increase of activation on average in the population of subjects.</s><s>This is interpreted in such a way that the effect is 'common' or 'stereotypical' in that population <ref type="bibr">(Penny and Holmes, 2007, p. 156)</ref>.</s></p><p><s>With the adoption of information-based imaging, it has become accepted practice to apply the same second-level inferential procedures to the results of first-level multivariate analyses, in particular classification accuracy (see e.g.</s><s><ref type="bibr" target="#b25">Haxby et al., 2001;</ref><ref type="bibr" target="#b62">Spiridon and Kanwisher, 2002;</ref><ref type="bibr" target="#b30">Haynes et al., 2007)</ref>: A classifier is trained on part of the data and is tested on another part, using each part for testing once (cross-validation), and the classification performance is quantified in the form of an accuracy, the fraction of correctly classified test data points.</s><s>Applied for example to two different experimental conditions, if there was no multivariate difference in the data between conditions, the classifier would operate at 'chance level', i.e. it would on average achieve a classification accuracy of 50 %.</s><s>At the second level, accuracies from different subjects are then entered into a one-sided one-sample t-test vs 50 %, in order to show that the ability to classify above chance and therefore the presence of an information effect is typical in the population the subjects were recruited from.</s></p><p><s>In this paper we argue that despite of the seemingly analogous statistical procedure, a t-test vs chance level applied to accuracies cannot provide evidence that the corresponding effect is typical in the population.</s><s>In contrast to other criticisms of this use of the t-test (see below), in our view the problem is not so much that the estimation distribution of cross-validated accuracies is not normal or even symmetric, or that a normal distribution model is generally inadequate for a quantity bounded to an interval [0 %, 100 %].</s><s>Rather, the problem is that other than estimated accuracies, the true single-subject accuracy can never be below chance level because it measures an amount of information. 1 We will show that this restriction changes the meaning of the t-test: It now tests the global null hypothesis <ref type="bibr" target="#b43">(Nichols et al., 2005)</ref> that there is no information in any subject in the population.</s><s>As a consequence, achieving a significant test result allows us only to infer that there are people in which there is an effect, but not that the presence of information generalizes to the population.</s><s>The argument does not only hold for classification accuracy, but also for other 'information-like' measures.</s></p><p><s>The t-test on accuracies has been criticized before <ref type="bibr" target="#b63">(Stelzer et al., 2013;</ref><ref type="bibr" target="#b2">Brodersen et al., 2013)</ref> on the grounds that its distributional assumptions are not fulfilled for crossvalidated classification accuracies.</s><s>Such a distributional error invalidates the calculation of critical values for the t-statistic and can therefore lead to an increased rate of false positives.</s><s>This problem may be solved by better distribution models <ref type="bibr" target="#b2">(Brodersen et al., 2013)</ref> or the use of non-parametric statistics <ref type="bibr" target="#b63">(Stelzer et al., 2013)</ref>.</s><s>Our criticism goes significantly beyond that: Not only is the t-test quantitatively wrong, but it effectively tests a null hypothesis that is qualitatively different from its use with univariate statistics, with the consequence that rejection of this null hypothesis no longer supports population inference.</s></p><p><s>Please note that our criticism pertains specifically to a second-level t-test applied to per-subject classification accuracies or similar measures.</s><s>It does not apply to the classification of subjects, e.g.</s><s>into different patient groups in medical applications <ref type="bibr" target="#b58">(Sabuncu and Van Leemput, 2012;</ref><ref type="bibr" target="#b57">Sabuncu, 2014)</ref>, or to the classification of condition-specific patterns across subjects <ref type="bibr" target="#b41">(Mourao-Miranda et al., 2005)</ref>.</s><s>Moreover, it only concerns quantities that measure the information content of data, but not related quantities like classifier weights <ref type="bibr" target="#b69">(Wang et al., 2007;</ref><ref type="bibr" target="#b19">Gaonkar and Davatzikos, 2013;</ref><ref type="bibr">Gaonkar et al., 2015, see below)</ref>.</s></p><p><s>The organization of the paper is as follows: In Part 2 we detail how a second-level t-test achieves population inference for univariate contrasts.</s><s>We then explain that MVPA measures are 'information-like' and show, both theoretically and using simulations, that for such measures the t-test effectively tests the global null hypothesis that there is no effect in any subject.</s><s>Part 3 reviews possible alternatives to the t-test on accuracies, converging on the idea that population inference for information-based imaging should target the proportion of subjects in the population with an effect.</s><s>One way to implement such an 'information prevalence inference' is described in detail in Part 4, and results of its application to real data are compared with those of the t-test.</s><s>We conclude with the discussion of a number of questions surrounding the problem of population inference for information-based imaging.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The problem with the t-test on accuracies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Population inference in univariate fMRI analysis</head><p><s>To see why the t-test on accuracies cannot provide population inference, we briefly recapitulate how standard univariate analysis does achieve it.</s><s>In a single subject, an activation difference or contrast ∆β is estimated based on the general linear model (GLM; <ref type="bibr" target="#b18">Friston et al., 1995)</ref>.</s><s>Because it is obtained from noisy data, the estimate is itself noisy,</s></p><formula xml:id="formula_0">∆β ∼ N (∆β, σ 2 1 ),<label>(1)</label></formula><p><s>where σ 2 1 denotes the estimation variance of the contrast (cf.</s><s>Fig. <ref type="figure" target="#fig_0">1a</ref>).</s><s>If several subjects are included in a study, the true activation difference ∆β varies across subjects (Fig. <ref type="figure" target="#fig_0">1a</ref>):</s></p><formula xml:id="formula_1">∆β k ∼ N (∆µ, σ 2 2 ) (2)</formula><p><s>where ∆µ is the average true activation difference in the population of subjects and σ 2 2 the population variance of the effect (Fig. <ref type="figure" target="#fig_0">1b</ref>).</s><s>The added subscript k indicates that we now consider the subject as randomly sampled from the population.</s><s>The estimated contrast in several subjects therefore shows variation for two reasons -they are noisy estimates (σ 2 1 ), and different subjects respond differently (σ 2 2 ):</s></p><formula xml:id="formula_2">∆β k ∼ N (∆µ, σ 2 1 + σ 2 2 ).<label>(3)</label></formula><p><s>The symbol ∆β k indicates that this contrast is both estimated and sampled.</s></p><p><s>A one-sided t-test applied to the ∆β k from a sample of subjects k = 1 . . .</s><s>N has the null hypothesis ∆µ = 0.</s><s>If it can be rejected (∆µ &gt; 0), this allows us to make a statement about the population of subjects because ∆µ is a parameter of a population model (Eq.</s><s>2).</s><s>And this statement concerns a typical effect because ∆µ is the mean, median, and mode of the assumed normal distribution.</s><s>This kind of test is also called random-effects analysis (RFX) because it treats subjects as as randomly sampled from a population <ref type="bibr" target="#b60">(Searle et al., 1992)</ref>.</s><s>It was introduced into fMRI by <ref type="bibr" target="#b32">Holmes and Friston (1998)</ref> to replace previous fixed-effects analyses (FFX), which did not account for population variation and therefore did not provide population inference. 2</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The t-test on accuracies</head><p><s>Using a second-level t-test vs chance level with classification accuracies implies that an analogous random-effects model applies: In each subject (first level) we obtain an estimated accuracy which varies with an estimation variance ς 2 1 , â ∼ N (a, ς 2 1 ).</s></p><p><s>(4)</s></p><p><s>The underlying true accuracy a (see App. A) varies across subjects (second level) with a population variance ς 2 2 ,</s></p><formula xml:id="formula_3">a k ∼ N ( ā, ς 2 2 ).<label>(5)</label></formula><p><s>Therefore estimated accuracies vary across subjects with the combined variance</s></p><formula xml:id="formula_4">ς 2 1 + ς 2 2 , âk ∼ N ( ā, ς 2 1 + ς 2 2 ). (<label>6</label></formula><formula xml:id="formula_5">)</formula><p><s>Here ā is the average true classification accuracy in the population, and the null hypothesis is that this population average is at chance level, ā = a 0 .</s><s>Again, the symbol âk indicates that this accuracy is both estimated and sampled.</s><s>Though a normal distribution cannot hold exactly since both â and a k are limited to [0 %, 100 %], we can argue that the t-test is robust against violations of normality <ref type="bibr" target="#b54">(Rasch and Guiard, 2004</ref>).</s></p><p><s>It appears that we have a viable random-effects model to justify the application of a second-level t-test to accuracies.</s><s>But there is a problem with this simple transfer: In contrast to estimated accuracies â, the true accuracy a can never be below the chance level a 0 .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">There is no negative information</head><p><s>To understand why a ≥ a 0 must hold, it helps to recognize that MVPA aims at a generic kind of effect, namely, whether or not information about experimental conditions is present It is a symmetric function of ∆β, which makes accuracy an information-like measure, with a minimal value a 0 = 50 %.</s><s>-e) Estimation variation of accuracy (6-fold cross-validation) in the three subjects (red curves).</s><s>As apparent for subject 2, the distributions can deviate strongly from normality.</s><s>While estimated accuracies can be below chance (gray line), true accuracies (black bars; a = 67.7,</s><s>51.5, 61.3 %) cannot.</s><s>-f) The population variation (black curve) and combined variation (population + estimation, red curve) of accuracy that result from the population distribution of contrasts ∆β k (b, black line), the functional relationship between ∆β and a (d), and the estimation distributions (e).</s><s>The population distribution is restricted to a ≥ a 0 and in this example shows a spike at 50 % and a weaker maximum at 56 %. -For further details, see App.</s><s>B. in the experimental data <ref type="bibr" target="#b51">(Pereira and Botvinick, 2011)</ref>.</s><s>Instead of making the common distinction between univariate and multivariate fMRI analysis, we follow <ref type="bibr" target="#b38">Kriegeskorte et al. (2006;</ref><ref type="bibr" target="#b36">2007)</ref> in distinguishing between activation-based and information-based imaging.</s><s>Activation-based analysis is interested in whether there is a specific change in the activation of a voxel (average BOLD signal) corresponding to an experimental manipulation, normally an increase.</s><s>Information-based analysis determines whether there is any change at all, be it an increase or decrease.</s><s>It looks for brain areas where the difference of conditions makes any kind of difference with respect to the fMRI signal, i.e. for information <ref type="bibr" target="#b1">(Bateson, 1972)</ref>.</s></p><p><s>Because information-based analysis disregards the sign of activation differences, it has itself an unsigned outcome: There either is a difference, then there is above-zero information, or there is no difference, then there is zero information ('chance level').</s><s>This holds for information-theoretic measures in the strict sense (cf.</s><s><ref type="bibr" target="#b6">Cover and Thomas, 2012)</ref>, in particular mutual information (Fig. <ref type="figure" target="#fig_0">1c</ref>), but also for the true value of information-like measures including averaged absolute t and Mahalanobis distance ∆ (used by <ref type="bibr" target="#b38">Kriegeskorte et al., 2006)</ref>, Wilks' Λ (used by <ref type="bibr" target="#b29">Haynes and Rees, 2005b)</ref>, linear discriminant t (LD-t, <ref type="bibr" target="#b45">Nili et al., 2014)</ref>, pattern distinctness D <ref type="bibr" target="#b0">(Allefeld and Haynes, 2014)</ref>, or classification accuracy a. 3 The true single-subject accuracy is either above chance level, a &gt; a 0 , if it is possible to extract information, or it is at chance level, a = a 0 , if not -but never below (Fig. <ref type="figure" target="#fig_0">1d</ref>).</s><s>Estimated accuracies can be below chance, but only due to imprecise estimation of a by â, i.e. below-chance accuracies are accounted for by the first-level model of Eq. 4, not the second level of Eq. 5.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">What does a t-test on accuracies mean?</head><p><s>This restriction creates a problem for the second-level null hypothesis, which qualitatively changes its meaning.</s><s>If H 0 : ā = a 0 (7) is true, then a k ∼ N (a 0 , ς 2 2 ) (Eq. 5), which means that while half the people in the population exhibit true above-chance classification, the other half is assumed to systematically exhibit true below-chance classification, contradicting our insight that a ≥ a 0 .</s><s>The null hypothesis of the t-test can only be made compatible with this constraint by additionally assuming that there is no population variation at all, H 0 : ā = a 0 ∧ ς 2 2 = 0. 4 And this means that the the true accuracy is at chance level for everybody,</s></p><formula xml:id="formula_6">H 0 : ∀ k a k = a 0 ,<label>(8)</label></formula><p><s>-there is no information in any subject in the population.</s><s>Such a null hypothesis, which is a logical conjunction of many simpler null hypotheses, has been called 'global null hypothesis' by <ref type="bibr" target="#b43">Nichols et al. (2005)</ref>.</s></p><p><s>Note that this conclusion does not depend on the assumption of normality in Eq. 5 (which entered through the analogy to Eq. 2): For any distribution of true accuracies, if its mean is at chance but none of its realizations can be below chance, it follows that there can be no above-chance realizations either.</s><s>Therefore the only possible form in which the null hypothesis formulated in Eq. 7 can hold is given by Eq. 8.</s></p><p><s>The seemingly small constraint a ≥ a 0 has strong consequences for inference.</s><s>If the t-test allows us to reject the null hypothesis, this provides evidence for the alternative, its logical negation.</s><s>Since the global null hypothesis (Eq.</s><s>8) is a universal statement, its negation is a statement of existence:</s></p><formula xml:id="formula_7">¬H 0 : ∃ k a k &gt; a 0 . (<label>9</label></formula><formula xml:id="formula_8">)</formula><p><s>This means we have reason to believe that there are some people in the population whose fMRI data carry information about the experimental condition -but we have no grounds to believe that we have found an effect that is typical in the population.</s><s>The constraint on a neutralizes the RFX modeling of between-subject variation, making the t-test applied to accuracies effectively an FFX analysis. 5</s><s>  Since the constraint that the true value cannot be below chance level applies to all information-like measures, the problems for population inference demonstrated here hold for information-based imaging in general.</s><s>However, in the interest of conciseness our discussion focuses on cross-validated classification accuracy as the most commonly used measure in MVPA.</s></p><p><s>At this point, the main argument of this paper is concluded: The t-test on accuracies does not provide population inference, but effectively implements fixed-effects analysis.</s></p><p><s>The following section further illustrates and practically demonstrates this fact using simulated data.</s><s>Readers which are already convinced by the theoretical argument may skip forward to Part 3 which discusses information prevalence inference as an alternative to the t-test.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">What does a t-test on accuracies do?</head><p><s>In the previous sections we gave a theoretical argument that the null hypothesis of a second-level t-test changes its meaning under the constraint that holds for informationlike measures including classification accuracy.</s><s>But does this argument have practical relevance -after all, we never see 'true accuracies' but only estimates, which can be below chance?</s><s>Does our observation actually affect how a t-test on accuracies behaves?</s></p><p><s>To investigate this question, we simulate fMRI data according to the first-level GLM (see Eq. 28) of a simplified experiment containing two experimental conditions, for a sample of subjects.</s><s>A simulation is parametrized by the population mean activation difference between conditions, ∆µ, and the population variation σ 2 (Eq.</s><s>2).</s><s>The estimation variation (Eq. 1) is kept at σ 1 = 1.</s><s>Accuracies for the classification of single-trial data by a linear support vector machine are estimated using run-wise cross-validation,  <ref type="figure">-a</ref>) Second-level one-sided one-sample t-test applied to estimated classification accuracies vs chance level a 0 = 50 %.</s><s>The smallest rejection probability is reached if both ∆µ = 0 and σ 2 = 0. -b) Second-level two-sided one-sample t-test applied to estimated activation differences vs 0 (univariate RFX analysis).</s><s>The smallest rejection probability is reached for ∆µ = 0. -c) Fixed-effects analysis of activation differences.</s><s>The smallest rejection probability is reached if both ∆µ = 0 and σ 2 = 0. -d) Test based on classification across subjects.</s><s>The smallest rejection probability is reached for ∆µ = 0. and these accuracies are entered into a one-sided one-sample t-test vs chance level a 0 = 50 % across subjects at α = 0.05.</s><s>This is repeated many times to determine the probability to reject the null hypothesis.</s><s>For full details of the simulations, see App.</s><s>B and C.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.1">Classification of univariate data with a normal population model</head><p><s>For simplicity, we first examine the rejection probability of the t-test on accuracies for the classification of univariate data; Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the distributions arising in this case.</s><s>The resulting rejection probability as a function of the simulation parameters ∆µ and σ 2 is shown in Fig. <ref type="figure" target="#fig_1">2a</ref>.</s></p><p><s>The rejection probability function is a standard tool to check whether a test is valid for a given null hypothesis and how powerful it is <ref type="bibr" target="#b40">(Lehmann and Romano, 2005)</ref>.</s><s>Here, we use the function in the opposite direction: We define the test's effective null hypothesis as that set of parameter values where the rejection probability remains at or below the specified significance level α.</s></p><p><s>For the t-test on accuracies (Fig. <ref type="figure" target="#fig_1">2a</ref>) the result is that strictly there are no such parameter values: the smallest rejection probability is 0.055.</s><s>This is most likely because the normality assumption of Eq. 6 holds only approximately; the slight increase of the α-error is however within the bounds used by <ref type="bibr" target="#b54">Rasch and Guiard (2004)</ref> when stating that the t-test is robust against violations of normality.</s><s>If we disregard this deviation, the effective null hypothesis turns out to be ∆µ = 0 ∧ σ 2 2 = 0: no population variation, and no activation effect in any subject.</s><s>This means that the true activation difference is zero in all subjects, ∀ k ∆β k = 0, and since the true accuracy corresponding to a zero activation difference is at chance level, a(∆β = 0) = a 0 , it is equivalent to H 0 : ∀ k a k = a 0 -no information in any subject in the population.</s><s>The simulation confirms the practical relevance of our previous theoretical conclusion that the t-test on accuracies tests the global null.</s></p><p><s>For comparison, Fig. <ref type="figure" target="#fig_1">2b</ref> shows the rejection probability function of a second-level twosided t-test on first-level activation differences (univariate RFX analysis).</s><s>In accordance with the design of the test, its rejection probability remains at 0.05 if and only if ∆µ = 0, and increases monotonically with |∆µ|.</s><s>Though σ 2 2 is not part of the specification of the null hypothesis, we observe that for stronger population variation it becomes harder to reach a significant effect.</s><s>This is in stark contrast to the t-test on accuracies (Fig. <ref type="figure" target="#fig_1">2a</ref>), where increasing σ 2 2 makes it easier to reject the null hypothesis.</s><s>The more inconsistent activation differences (or by extension: patterns; see below) are in the population, the more likely it is that the t-test on accuracies will indicate the presence of an information effect that is supposedly 'typical' in the population! 6</s><s>As we noted above, univariate fixed-effects analysis does not provide populationlevel inference simply because its null hypothesis (that the sample mean is 0) does not reference a population distribution.</s><s>We can however apply an FFX test to our simulated univariate RFX data and thereby determine the null hypothesis it effectively implements in this context.</s><s>The result shown in Fig. <ref type="figure" target="#fig_1">2c</ref> demonstrates that the effective null hypothesis of FFX analysis is ∆µ = 0 ∧ σ 2 2 = 0. Though their rejection probability functions are different in detail, a t-test on accuracies and a fixed-effects analysis of activation differences operate qualitatively identical: they both detect deviations from a zero population mean contrast ∆µ as well as from a zero population variance of constrasts σ 2 2 .</s><s>This agreement supports our earlier statement that even though a ttest on accuracies formally acknowledges population variation, it does not provide population inference any more than FFX analysis does.</s></p><p><s>To complete the picture, Fig. <ref type="figure" target="#fig_1">2d</ref> shows the rejection probability function of a test based on classification across subjects (cf.</s><s><ref type="bibr" target="#b41">Mourao-Miranda et al., 2005)</ref>.</s><s>The single-subject univariate contrast estimates in each of the two conditions form the data set that is used to train and test a classifier in leave-one-subject-out cross-validation, and the distribution of accuracies is determined for different simulation parameters.</s><s>For a given simulated data set the null hypothesis (true accuracy = chance level) is rejected if the accuracy reaches or exceeds the critical value of 67.6 %, which is the approximate 95th percentile of the distribution of accuracies under the null hypothesis.</s><s>The result demonstrates that this test behaves in a way that is very similar to the second-level t-test applied to the same first-level activation differences (Fig. <ref type="figure" target="#fig_1">2b</ref>).</s><s>Both provide population inference because they both implement the null hypothesis ∆µ = 0 for arbitrary population variance σ 2 2 .</s><s>The difference is that while the second-level t-test is limited to univariate contrasts, the test based on classification across subjects can just as well be applied to multivariate data, where the null hypothesis becomes ∆ µ = 0.</s><s>If a significant effect is found, this provides evidence that there is a pattern difference ∆ µ that is typical in the population.</s></p><p><s>Since this implies that the presence of information is typical in the population, it appears that the problem of population inference for information-based imaging could simply be solved by applying classifiers or other MVPA methods always across subjects.</s><s>Unfortunately, spatial normalization algorithms do not achieve precise voxel-level anatomical alignment <ref type="bibr" target="#b66">(Thirion et al., 2006)</ref>, and moreover we cannot assume that a oneto-one correspondence of informative patterns between subjects always exists <ref type="bibr" target="#b27">(Haynes and Rees, 2006;</ref><ref type="bibr" target="#b37">Kriegeskorte and Bandettini, 2007;</ref><ref type="bibr" target="#b24">Haxby, 2012)</ref>, so that classification across subjects is often bound to fail for a trivial reason. 7</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.2">Classification of multivariate data with a normal population model</head><p><s>An important observation in the last section was that the larger the population variance σ 2 2 , i.e. the more inconsistent activation differences are across subjects, the easier it becomes for the t-test on accuracies to achieve significance.</s><s>To make sure that this effect is not limited to univariate data, Fig. <ref type="figure" target="#fig_2">3a</ref> shows the rejection probability function for different numbers of dimensions, p = 1, 2, and 10.</s><s>Data are generated as before, but for p different voxels in parallel, with both first-and second-level variation uncorrelated between voxels.</s><s>Accordingly, the classifier is trained and tested in a p-dimensional space.</s><s>To facilitate visualization, we kept the population mean activation difference at zero in all voxels, ∆µ = 0.</s><s>The result demonstrates that the effect of population variance on the rejection probability stays qualitatively the same, but is even stronger in higher dimensions.</s><s>We can therefore assume that the effective null hypothesis of the t-test on accuracies generalizes from the univariate ∆µ = 0 ∧ σ 2 2 = 0 to the multivariate ∆ µ = 0 ∧ Σ 2 = 0, where Σ 2 is the multivariate population variance (variance-covariance matrix).</s><s>That implies ∀ k ∆ β k = 0 -there is no informative pattern in any subject in the population -which is again equivalent to H 0 : ∀ k a k = a 0 , the global null.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.3">Classification of univariate data with a population proportion model</head><p><s>Our simulation-based finding that the effective null hypothesis of a t-test applied to first-level accuracies is H 0 : ∀ k a k = a 0 (the global null) was so far obtained using the standard normal distribution population model for activation differences (Eq.</s><s>2), which may not be correct (cf.</s><s><ref type="bibr" target="#b55">Rosenblatt et al., 2014)</ref>.</s><s>With respect to true accuracies, this model for true contrasts has a peculiar consequence: As soon as there is any population variation, i.e. σ 2 2 &gt; 0, the probability that the true contrast in a given subject k is exactly ∆β k = 0 becomes zero, which implies that almost always ∀ k ∆β k = 0. Since for nonzero true contrast the true accuracy is above chance, a(∆β = 0) &gt; a 0 (cf.</s><s>Fig. <ref type="figure" target="#fig_0">1d</ref>), this further implies that almost always ∀ k a k &gt; a 0 .</s><s>Therefore the assumption of a normal population distribution of activation differences allows only two possibilities: Either there is no information in the data of any subject (the effective null hypothesis), or there is information in the data of every subject.</s><s>There is nothing in between.</s></p><p><s>To see how the t-test on accuracies reacts to a situation between these extremes, we use an alternative population model for activation differences (replacing Eq. 2; modified from <ref type="bibr" target="#b55">Rosenblatt et al., 2014)</ref>: Assume that the true contrast has a fixed value ∆β * in a certain proportion γ ∈ [0, 1] of the population and the fixed value 0 in the rest.</s><s>If now a subject k is randomly selected from the population, this means that</s></p><formula xml:id="formula_9">∆β k = 0 with probability 1 − γ ∆β * with probability γ . (<label>10</label></formula><formula xml:id="formula_10">)</formula><p><s>Fig. <ref type="figure" target="#fig_2">3b</ref> shows the behavior of the t-test in a simulation using this model.</s><s>The result is that for different values of ∆β * , the rejection probability reaches the significance level α always at γ = 0.</s><s>This effective null hypothesis is again equivalent to the global null, H 0 : ∀ k a k = a 0 .</s><s>Moreover, the simulation demonstrates that the t-test on accuracies may with high probability declare an information effect to be 'typical' even though it is only present in a small minority of subjects in the population!</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">An alternative: Information prevalence inference</head><p><s>In the previous part we established that the second-level t-test applied to accuracies is not able to provide population inference.</s><s>We now discuss alternative approaches, leading us to the idea that population inference for information-based imaging should target the proportion of people in the population in which there is an information effect.</s></p><p><s>Within the MVPA literature, there are three alternative proposals.</s><s>First, <ref type="bibr" target="#b37">Kriegeskorte and Bandettini (2007)</ref> recommend to apply the methods for 'combining brains' collected by <ref type="bibr" target="#b39">Lazar et al. (2002)</ref>.</s><s>However, except for the summary-statistic approach of Holmes and Friston (1998), all of these methods 8 are meta-analytic procedures which explicitly test the global null.</s><s>Second, <ref type="bibr" target="#b63">Stelzer et al. (2013)</ref> propose to generate single-subject permutation statistics, and then to construct a second-level permutation distribution by randomly selecting first-level permutations in each subject.</s><s>Because each permutation realizing the second-level null hypothesis is a combination of permutations realizing the first-level null hypotheses in every subject, this again tests the global null hypothesis of no information in any subject.</s><s>And third, <ref type="bibr" target="#b2">Brodersen et al. (2013)</ref> follow <ref type="bibr" target="#b48">Olivetti et al. (2012)</ref> and <ref type="bibr" target="#b3">Brodersen et al. (2012)</ref> in describing a mixed-effects analysis for MVPA, introducing explicit estimation and population models for classification accuracies.</s><s>This approach offers several improvements over the t-test on accuracies: Their model can account for different estimation variances (ς 2 1 ) in different subjects, and it uses more realistic distributional assumptions.</s><s>However, the authors do not consider the fact that true accuracies are limited to the range [a 0 , 1] (for binary classification: [50 %, 100 %]).</s><s>Unfortunately, this renders their approach only an improved version of the t-test on accuracies.</s></p><p><s>As detailed in Sec.</s><s>2.4, the flaw of the t-test on accuracies lies in the fact that the distributional assumption of the underlying second-level model (Eq.</s><s>5) is incompatible with the restriction a ≥ a 0 , unless ς 2 = 0.</s><s>This could conceivably be fixed by using another population model which adheres to that restriction by design.</s><s>However, the simulated population distribution of true accuracies for univariate classification in Fig. <ref type="figure" target="#fig_0">1f</ref> does not look like it could be appropriately captured by a parametric model.</s><s>And even if that were the case, the distribution would likely have a different shape for classification in higher dimensions or for multi-class classification.</s><s>It might additionally depend on the specific classification algorithm, and it would certainly be different again for other information-like measures.</s></p><p><s>Moreover, inference with respect to the population mean is generally inadequate for information-like measures no matter how well the actual population distribution can be modeled.</s><s>The reason is that the true mean is above chance as soon as there is a true above-chance effect in a small fraction of the population (cf.</s><s>Fig. <ref type="figure" target="#fig_2">3b</ref>; because there can be no true below-chance effect).</s><s>This problem can be resolved by inference that does not target the mean effect, but the proportion of subjects in the population in which there is an information effect, i.e. the prevalence of information.</s><s>Such an approach is followed by <ref type="bibr" target="#b56">Rouder et al. (2007)</ref> in the context of signal detection theory with their 'mass-at-chance model'.</s><s>They assume a probit-normal population distribution of true detection accuracies, which however is truncated at 50 %, such that all subjects that are nominally below chance are instead located at chance.</s><s>Inference with respect to the prevalence of an effect has also been advocated by <ref type="bibr" target="#b55">Rosenblatt et al. (2014)</ref> for mass-univariate analysis.</s><s>The authors argue that imperfect alignment of single-subject activation maps leads to areas where only a subset of subjects have a non-zero activation, and propose to replace the standard normal population model (Eq.</s><s>2) by a mixture model.</s><s>We used a simplified version of this in the population proportion simulation (Eq. 10 and Fig. <ref type="figure" target="#fig_2">3b</ref>).</s><s><ref type="bibr" target="#b64">Stephan et al. (2009)</ref> propose a Bayesian form of prevalence inference as RFX analysis for dynamic causal models (DCM), extending first-level model selection to a posterior distribution over the space of different model frequencies.</s><s>In a specific application this discrete distribution may describe the distinction between a zero effect and a generic non-zero effect. 9</s><s><ref type="bibr" target="#b16">Friston et al. (1999a)</ref> build upon their previous idea of a conjunction test <ref type="bibr" target="#b53">(Price and Friston, 1997;</ref><ref type="bibr" target="#b70">Worsley and Friston, 2000)</ref> and introduce the minimum-statistic approach.</s><s>Their analysis proceeds in two steps: first, the minimum statistic is used to derive a p-value for the null hypothesis that there is no effect in any subject in the population, the global null.</s><s>In a second step, a correction is applied that allows to test the null hypothesis that the proportion of subjects in which there is an effect, γ, is at or below a threshold γ 0 .</s><s>The rejection of this null hypothesis therefore allows to infer that γ &gt; γ 0 .</s></p><p><s>The approaches of <ref type="bibr" target="#b56">Rouder et al. (2007)</ref>, <ref type="bibr" target="#b55">Rosenblatt et al. (2014)</ref>, <ref type="bibr" target="#b64">Stephan et al. (2009), and</ref><ref type="bibr" target="#b16">Friston et al. (1999a)</ref> are all candidates to be adapted for information-based imaging, to provide population inference with respect to the prevalence of an information effect.</s><s>In the following we demonstrate this in detail for the method of Friston and colleagues.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Permutation-based information prevalence inference using the minimum statistic</head><p><s>In this part we recapitulate the minimum-statistic approach to prevalence inference developed by <ref type="bibr" target="#b16">Friston et al. (1999a)</ref>, adapt it to be based on permutation statistics, and detail the resulting algorithm.</s><s>Applied to information-like measures this method allows us to achieve information prevalence inference, i.e. inference with respect to the proportion of subjects in the population that exhibit an information effect.</s><s>We demonstrate the method using an example data set.</s></p><p><s>The advantage of Friston et al.'s approach is that it can be implemented based on known permutation methods at the single-subject level <ref type="bibr" target="#b21">(Golland and Fischl, 2003;</ref><ref type="bibr">Etzel and Braver, 2013;</ref><ref type="bibr" target="#b59">Schreiber and Krekelberg, 2013;</ref><ref type="bibr" target="#b63">Stelzer et al., 2013;</ref><ref type="bibr" target="#b0">Allefeld and Haynes, 2014</ref>; see also <ref type="bibr" target="#b10">Ernst, 2004</ref>).</s><s>The method is discussed with respect to classification accuracy, but the test logic applies equally to other information-like measures.</s></p><p><s>A note on notation: We previously used variables without index (e.g.</s><s>a) when talking about the 'first level' of a given single subject, and symbols with index (e.g. a k ) when considering a subject as randomly selected from the population, or referring to all members of the population (∀ k ).</s><s>This notation is still followed, but we now extend it such that an index associated with an explicit range, k = 1 . . .</s><s>N, refers to the specific subjects included in a given sample.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The minimum statistic and the global null</head><p><s>In a single subject, an estimated classification accuracy â has an associated p-value p( â), which is the probability to observe an accuracy that large or larger given that the true accuracy is at chance level (a = a 0 ).</s></p><p><s>For a sample of N subjects with estimated classification accuracies âk , k = 1 . . .</s><s>N, we choose the minimum statistic (smallest observed accuracy) as the second-level test statistic,</s></p><formula xml:id="formula_11">m = N min k=1 âk . (<label>11</label></formula><formula xml:id="formula_12">)</formula><p><s>As a second-level null hypothesis we first consider the global null hypothesis,</s></p><formula xml:id="formula_13">H 0 : ∀ k a k = a 0 , (<label>12</label></formula><formula xml:id="formula_14">)</formula><p><s>that there is no effect in any subject in the population.</s><s>In order to test this null hypothesis, we need the p-value of m, p N (m), with respect to the global null.</s></p><p><s>To say that the minimum of estimated accuracies is at or larger than a given value m is the same as saying that all of the estimated accuracies âk , k = 1 . . .</s><s>N, are at or larger than m.</s><s>Since subjects are independently drawn from the population, the probability to observe a minimum of m or larger under the global null is the product of probabilities to observe an estimated accuracy of m or larger in each subject in the sample:</s></p><formula xml:id="formula_15">p N (m) = N ∏ k=1 p(m) = p(m) N , (<label>13</label></formula><formula xml:id="formula_16">)</formula><p><s>where p(m) is the single-subject p-value for â = m.</s></p><p><s>If p N (m) ≤ α, then we can reject H 0 and infer that there are some subjects in the population in which there is an above-chance effect.</s><s>Since this is a statement of existence, this does not provide evidence that the effect is typical in the population.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The prevalence null</head><p><s>We now consider a population model which contains the global null as a special case:</s></p><p><s>The information effect targeted by the classification procedure has a prevalence γ, i.e. a proportion γ ∈ [0, 1] of subjects in the population have an above-chance effect, the others no effect.</s><s>If a subject k is selected at random from this population, then</s></p><formula xml:id="formula_17">a k = a 0 with probability 1 − γ, a k &gt; a 0 with probability γ. (<label>14</label></formula><formula xml:id="formula_18">)</formula><p><s>This is similar to the population proportion model for activation differences used above (Eq.</s><s>10), but in contrast no assumption is made about the size and distribution of above-chance effects.</s></p><p><s>An estimated accuracy â in a single subject can be larger than or equal to m either purely by chance (p(m), with probability 1 − γ) or because there is actually an effect in that subject (p(m|a &gt; a 0 ), with probability γ).</s><s>The probability to observe a sample minimum of m or larger if the prevalence is γ is therefore</s></p><formula xml:id="formula_19">p N (m|γ) = N ∏ k=1 [(1 − γ) p(m) + γ p(m|a &gt; a 0 )] = [(1 − γ) p(m) + γ p(m|a &gt; a 0 )] N . (<label>15</label></formula><formula xml:id="formula_20">)</formula><p><s>Here p(m|a &gt; a 0 ) is the probability to observe an estimated accuracy of m or larger in a single subject given a true accuracy of a, where we only know that a &gt; a 0 .</s><s>Because the size of the above-chance effect a &gt; a 0 is not specified, we cannot know this probability precisely; but because it is a probability, we know that it is smaller than or equal to one, p(m|a &gt; a 0 ) ≤ 1, and therefore</s></p><formula xml:id="formula_21">p N (m|γ) ≤ [(1 − γ) p(m) + γ] N . (<label>16</label></formula><formula xml:id="formula_22">)</formula><p><s>The reason we chose the minimum statistic as the second-level test statistic is that it enables us to formulate this inequality for the prevalence model, i.e. to determine a p-value without specifying the size and distribution of above-chance effects.</s></p><p><s>The prevalence model allows us to formulate the prevalence null hypothesis,</s></p><formula xml:id="formula_23">H 0 : γ ≤ γ 0 ,<label>(17)</label></formula><p><s>that the prevalence is smaller than or equal to a threshold prevalence γ 0 .</s><s>The global null (Eq.</s><s>12) is a special case of the prevalence null where γ 0 = 0.</s></p><p><s>The prevalence null hypothesis is a complex null hypothesis, i.e. it can be realized by different values of the parameter γ.</s><s>In such a case, the p-value associated with a test statistic is the probability to observe the given value or larger, maximized over all situations consistent with H 0 .</s><s>Therefore</s></p><formula xml:id="formula_24">p N (m|γ ≤ γ 0 ) = [(1 − γ 0 ) p(m) + γ 0 ] N . (<label>18</label></formula><formula xml:id="formula_25">)</formula><p><s>In a permutation approach, p N (m) can be more precisely determined than p(m) (see step 3 under Algorithm).</s><s>We therefore express the prevalence null p-value p N (m|γ ≤ γ 0 ) in terms of the global null p-value p N (m), using the relation p N (m) = p(m) N (Eq.</s><s>13):</s></p><formula xml:id="formula_26">p N (m|γ ≤ γ 0 ) = [(1 − γ 0 ) N p N (m) + γ 0 ] N . (<label>19</label></formula><formula xml:id="formula_27">)</formula><p><s>If p N (m|γ ≤ γ 0 ) ≤ α, then we can reject H 0 and infer that the prevalence γ is significantly larger than γ 0 , i.e. more than a proportion γ 0 of the population have an effect.</s></p><p><s>As an alternative to fixing a threshold prevalence γ 0 in advance and then testing the corresponding prevalence null, we can compute the largest γ 0 such that the corresponding null hypothesis can still be rejected at the given significance level α:</s></p><formula xml:id="formula_28">γ 0 = N √ α − N p N (m) 1 − N p N (m) . (<label>20</label></formula><formula xml:id="formula_29">)</formula><p><s>Note that this is not an estimator for the true prevalence γ, but ]γ 0 , 1] is a one-sided (1 − α)-confidence interval for it.</s></p><p><s>This confidence interval will often be too wide because of the inequality used above (Eq.</s><s>16).</s><s>Moreover, even for the strongest possible effect (p N (m) = 0) it holds γ 0 = N √ α, meaning that the strength of the population inference is limited by the number of subjects N and the chosen significance level α.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Information maps</head><p><s>So far we have considered a single second-level test based on classification accuracies âk from N different subjects.</s><s>But in a common variant of MVPA, searchlight analysis <ref type="bibr" target="#b38">(Kriegeskorte et al., 2006)</ref>, we have maps of classification accuracies and perform a test at each voxel in these maps, i.e. we have to adjust for multiple comparisons.</s></p><p><s>To do so, we need to specify a spatially extended version of the prevalence null.</s><s>Again following <ref type="bibr" target="#b16">Friston et al. (1999a)</ref>, our spatially extended null hypothesis is: -there is an effect with prevalence γ ≤ γ 0 in a small area, -and no effect everywhere else.</s><s>The justification for this is that in experiments investigating the localization of information we normally expect this information to be restricted to specialized brain areas.</s></p><p><s>Under this null hypothesis, a sample minimum of m or larger can occur either purely by chance (no true effect), with a probability that is increased because we examine many voxels at once (p * N (m)); or if that is not the case (1 − p * N (m)) it can occur because there actually is an effect in a sub-threshold proportion of the population, with a probability that is not increased because the effect is only present in a small area (p N (m|γ ≤ γ 0 )).</s></p><p><s>Here p * N (m) is the p-value for the spatially extended global null, corrected for multiple comparisons using a standard method (see step 4 under Algorithm).</s><s>Taken together, the probability to observe a sample minimum of m or larger at a given voxel, corrected for multiple comparisons according to the spatially extended prevalence null hypothesis is</s></p><formula xml:id="formula_30">p * N (m|γ ≤ γ 0 ) = p * N (m) + [1 − p * N (m)] p N (m|γ ≤ γ 0 ).<label>(21)</label></formula><p><s>For given threshold γ 0 , the spatially extended prevalence null can be rejected at a particular voxel if p * N (m|γ ≤ γ 0 ) ≤ α.</s><s>Equivalently, we can define a significance level that is corrected for multiple comparisons,</s></p><formula xml:id="formula_31">α * = α − p * N (m) 1 − p * N (m) , (<label>22</label></formula><formula xml:id="formula_32">)</formula><p><s>and reject the spatially extended prevalence null if the uncorrected p-value is at or below the corrected level, p N (m|γ ≤ γ 0 ) ≤ α * .</s><s>Note that because m is voxel-specific, α * is, too.</s></p><p><s>Again, we can alternatively compute the largest γ 0 such that the corresponding spatially extended prevalence null can still be rejected:</s></p><formula xml:id="formula_33">γ * 0 = N √ α * − N p N (m) 1 − N p N (m) ,<label>(23)</label></formula><p><s>which results in a map of lower confidence bounds of the prevalence of the effect, an information prevalence map.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Algorithm</head><p><s>We now explain in detail how the computations derived above can be implemented based on first-level permutation statistics.</s></p><p><s>Step 1: For each subject, classification accuracies âv are computed for each voxel v.</s></p><p><s>Additionally, classification accuracies are computed for data where the class labels have been permuted, âvi with i = 1 . . .</s><s>P 1 , where P 1 is the number of available first-level permutations.</s><s>i = 1 denotes the neutral permutation, i.e. âv1 = âv .</s></p><p><s>Step 2: The minimum classification accuracy m v across subjects (Eq.</s><s>11) is computed at each voxel v. Additionally, the minimum accuracy is computed for each second-level permutation, m vj with j = 1 . . .</s><s>P 2 .</s><s>A second-level permutation is a combination of firstlevel permutations, and therefore there are P N 1 possible second-level permutations.</s><s>If there are too many combined permutations, a subset can be selected randomly (Monte Carlo estimation), but it has to be made sure that j = 1 denotes the combination of first-level neutral permutations (i = 1 in all subjects).</s><s>This procedure of combined permutations is identical to the one employed by <ref type="bibr" target="#b63">Stelzer et al. (2013)</ref>.</s></p><p><s>Step 3: The uncorrected p-value for the global null hypothesis is determined at each voxel (Eq.</s><s>13) as</s></p><formula xml:id="formula_34">p N (m v ) = 1 P 2 P 2 ∑ j=1 [m v ≤ m vj ] (<label>24</label></formula><formula xml:id="formula_35">)</formula><p><s>where the so-called Iverson bracket <ref type="bibr">[•]</ref> has the value 1 for a true condition and the value 0 for a false condition.</s><s>That is, p N (m v ) is the fraction of combined-permutation values of the minimum statistic larger than or equal to the actual value.</s><s>Because m v1 = m v , the smallest possible p-value is 1 P 2 .</s></p><p><s>Step 4: To correct p N (m v ) for multiple comparisons, the maximum statistic across voxels is computed for each combined permutation (see <ref type="bibr" target="#b44">Nichols and Holmes, 2001)</ref>,</s></p><formula xml:id="formula_36">M j = max v m vj , (<label>25</label></formula><formula xml:id="formula_37">)</formula><p><s>and then</s></p><formula xml:id="formula_38">p * N (m v ) = 1 P 2 P 2 ∑ j=1 [m v ≤ M j ] (26) is determined. p * N (m v )</formula><p><s>is the p-value for the spatially extended global null hypothesis.</s></p><p><s>Step 5a: To determine where the spatially extended prevalence null hypothesis for a given threshold γ 0 can be rejected, at each voxel Eq. 19 is used to compute</s></p><formula xml:id="formula_39">p N (m v |γ ≤ γ 0 ) from p N (m v ), Eq. 21 to compute p * N (m v |γ ≤ γ 0 ) from p * N (m v ) and p N (m v |γ ≤ γ 0 ), and it is checked whether p * N (m v |γ ≤ γ 0 ) ≤ α.</formula><p><s>Step 5b: Alternatively, to determine for each voxel the largest threshold γ * 0 at which the spatially extended prevalence null hypothesis can be rejected, Eq. 22 is used to compute</s></p><formula xml:id="formula_40">α * v from p * N (m v ). Then p N (m v ) ≤ α *</formula><p><s>v is checked to see whether the spatially extended global null hypothesis can be rejected.</s><s>If that is not the case, the largest threshold γ * 0 is not defined for that voxel (the prevalence null cannot be rejected, even at γ * 0 = 0).</s><s>For all voxels where the spatially extended global null hypothesis can be rejected, Eq. 23 is used to compute γ * 0v from α * v and p N (m v ).</s><s>Note that the maximally possible γ * 0v determined this way is limited not just by the chosen significance level α and the number of subjects N, but also by the number of second-level permutations P 2 ; it is</s></p><formula xml:id="formula_41">γ * 0max = N √ α * max − N √ 1/P 2 1 − N √ 1/P 2 with α * max = α − 1/P 2 1 − 1/P 2 . (<label>27</label></formula><formula xml:id="formula_42">)</formula><p><s>For large P 2 , α * max is approximately equal to α.</s><s>A problem for this method may arise from the fact that both the minimum statistic underlying prevalence inference and the maximum statistic used to correct for multiple comparisons do not produce new values (unlike e.g. the mean, which in general differs from all the values it is calculated from).</s><s>Since the number of possible classification accuracies is limited because of a limited number of data points, this may lead to a large number of permutations where the statistic attains the same value (tied permutation values), which inflates the p-values computed in steps 2 and 3 above.</s><s>This problem can be solved by using spatially smoothed accuracy maps as inputs (which is also advisable to reduce residual anatomical misalignment between subjects), or by using a continuously-valued information-like measure like pattern distinctness <ref type="bibr" target="#b0">(Allefeld and Haynes, 2014)</ref> instead.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Application</head><p><s>In order to illustrate our permutation-based implementation of information prevalence inference, we re-use the data of <ref type="bibr" target="#b5">Cichy et al. (2011)</ref>.</s><s>Twelve different visual stimuli belonging to four different categories were presented either to the left or the right of fixation (24 experimental conditions) to N = 12 subjects.</s><s>There were four different trials per condition in each of five different runs.</s><s>fMRI data were recorded from a field of view covering the ventral visual cortex at an isotropic resolution of 2 mm.</s><s>Data were preprocessed and normalized to the MNI template.</s><s>A linear SVM with parameter C = 1 was trained on GLM parameter estimates from four of the runs, and tested on the fifth run, in a leave-one-run-out cross-validation scheme.</s><s>Classification was pairwise (24 • 23 / 2 = 276 pairs) and accuracies were averaged across pairs combining different factor levels, so that the chance-level accuracy was a 0 = 50 %.</s><s>For permutation statistics, class labels were exchanged in each of the five runs separately, which lead to P 1 = 2 5−1 = 16 unique first-level permutations.</s><s>The analysis was performed using a searchlight of radius 4 voxels (comprising 257 voxels).</s><s>The resulting accuracy maps were smoothed with a Gaussian kernel of 6 mm FWHM.</s><s>For more details, see <ref type="bibr" target="#b5">Cichy et al. (2011)</ref> and <ref type="bibr" target="#b0">Allefeld and Haynes (2014)</ref>.</s><s>For information prevalence inference, we randomly selected P 2 = 10 7 out of P N 1 = 2.8 • 10 14 possible combined permutations at the second level.</s><s>All the following results are corrected for multiple comparisons, but for simplicity we omit the superscript ' * '. -c) t-test on accuracies vs chance level a 0 = 50 %.</s><s>For those areas where the null hypothesis ā = a 0 can be rejected at a level of α = 0.05, colors visualize the underlying t-value.</s></p><p><s>The results for the classification of stimulus category are shown in Fig. <ref type="figure" target="#fig_3">4</ref>. The spatially extended global null hypothesis of no information in any subject in the population can be rejected at a level of α = 0.05 in about 27 % of in-mask voxels.</s><s>For those voxels, the largest lower bound γ 0 at which the spatially extended prevalence null hypothesis can be rejected is shown in Fig. <ref type="figure" target="#fig_3">4a</ref>.</s><s>In about half of those voxels, γ 0 reaches the maximally possible value (for the given sample size N, significance level α, and number of secondlevel permutations P 2 ) of γ 0max = 0.701 (Eq.</s><s>27).</s></p><p><s>For those voxels where the largest lower bound γ 0 is larger than or equal to 0.5, i.e.</s><s>where we can infer that in the majority of subjects in the population the data contain information about the stimulus category, the median estimated accuracy is shown in Fig. <ref type="figure" target="#fig_3">4b</ref>.</s><s>We chose the median across all subjects in the sample as a descriptive statistic to accompany our prevalence results, because it can be considered as a cautious and robust estimator of the typical above-chance classification accuracy. 10</s></p><p><s>For comparison, the result of a second-level t-test on accuracies vs chance level is shown in Fig. <ref type="figure" target="#fig_3">4c</ref>.</s><s>Although the effective null hypothesis of this test is identical to the global null hypothesis γ = 0 explicitly tested in Fig. <ref type="figure" target="#fig_3">4a</ref>, the number of voxels at which it can be rejected is much smaller (about 14 % of in-mask voxels at α = 0.05, FWE-corrected), indicating that in this case the t-test is less sensitive.</s><s>The picture is the same for the comparison with the test of the prevalence null hypothesis γ ≤ γ 0 at γ 0 = 0.5 shown in Fig. <ref type="figure" target="#fig_3">4b</ref>.</s><s>Information prevalence inference therefore allows us to draw conclusions that are stronger than those provided by the t-test on accuracies, concerning both interpretation -population inference -and, for this data set, statistical power.</s><s>However, the result of Fig. <ref type="figure" target="#fig_3">4b</ref> also calls into question whether the assumption that an effect is constrained to a 'small area' (adopted from <ref type="bibr" target="#b16">Friston et al., 1999a</ref>) is generally adequate.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p><s>In this paper we have shown that the t-test on accuracies commonly used in MVPA studies is not able to provide population inference because the true single-subject accuracy a can never be below chance level.</s><s>This constraint makes the effective null hypothesis of the test the global null hypothesis that there is no effect in any subject in the population, which means that in rejecting that null hypothesis we can only infer that there are some subjects in which there is an effect.</s><s>This is in stark contrast to the standard interpretation of a significant result of a second-level t-test.</s><s>We supported our statement both by theoretical arguments, in particular detailing that classification accuracy in MVPA is an information-like measure, as well as by simulations of the relevant distributions to investigate the practical behavior of the t-test applied to accuracy data.</s><s>Finally, we reviewed possible alternative inference methods and described one approach that can be implemented based on known first-level permutation statistics in combination with the minimum statistic as the second-level test statistic.</s><s>In the following we discuss a number of possible counter-arguments and questions.</s></p><p><s>Does this mean that the t-test fails, is not robust enough?</s><s>-Not really.</s><s>In all the instances we examined, the t-test does what it is supposed to do: Check whether the population mean is increased.</s><s>The point is that under the constraint a ≥ a 0 , rejecting H 0 : ā = a 0 no longer tells us that the effect is typical in the population; an increased accuracy in a small fraction of the population is sufficient to increase the population mean (cf.</s><s>Fig. <ref type="figure" target="#fig_2">3b</ref>).</s><s>Average information content is not typical information content.</s><s>Concerning robustness, for the case we simulated (Fig. <ref type="figure" target="#fig_1">2a</ref>) the α-error was only slightly increased, consistent with the notion that the t-test is indeed robust.</s></p><p><s>Is it wrong to test the global null hypothesis?</s><s>-No.</s><s>The decision which null hypothesis to test rests with the researchers performing a study.</s><s>The problem lies with the interpretation of a significant result, which for second-level analysis -at least tacitly -is to infer that the effect is typical in the population.</s><s>Moreover, the step from sample to population inference (FFX to RFX) as the gold standard has been taken already a long time ago for univariate analyses <ref type="bibr" target="#b32">(Holmes and Friston, 1998)</ref> and more recently for DCM <ref type="bibr" target="#b64">(Stephan et al., 2009)</ref>; it would therefore be natural to expect that it also becomes the standard for information-based imaging.</s><s>In particular, any claim that MVPA is more sensitive than univariate analysis <ref type="bibr" target="#b47">(Norman et al., 2006)</ref> is meaningless if MVPA is not held up to the same inferential standards.</s></p><p><s>We observe below-chance accuracies all the time.</s><s>-There are two aspects to this.</s><s>For one, our statement that accuracies cannot be below chance refers to true accuracies, not to estimated accuracies.</s><s>Second, it is sometimes the case that estimated accuracies strongly suggest that the true accuracy is below-chance, too. 11</s><s>This is most likely due to the circumstance that a crucial assumption of cross-validation is not met, namely that the different parts of the data (here: fMRI recording sessions) come from the same distribution <ref type="bibr" target="#b9">(Efron and Tibshirani, 1994)</ref>.</s><s>If there are systematic changes across data parts, for example because of a confound either in the data or in the experimental design <ref type="bibr" target="#b22">(Görgen et al., 2014)</ref>, this can induce a negative bias, including the possibility that classifier performance lies systematically below chance.</s><s>(For another tentative explanation, see <ref type="bibr" target="#b36">Kowalczyk, 2007.)</ref></s><s>This does of course not mean that now there is 'negative information', but only that cross-validated accuracy is not a meaningful information measure under such circumstances.</s><s>This possibility does not invalidate the argument made in this paper, but points to another problem that needs a separate remedy.</s></p><p><s>A classifier could be designed to systematically give the wrong answer, leading to a true accuracy below chance.</s><s>-Yes, but in that case the accuracy of this classifier would no longer be a measure of the information content of the data.</s><s>For example in the simple case where the output of a working classifier is falsified by always returning the opposite classification result (A instead of B and B instead of A), information content would no longer be quantified by a − a 0 , but by −(a − a 0 ).</s><s>The argument in this paper does not refer to classifiers in general, but to the use of classification accuracy and other measures to quantify the information content of data.</s></p><p><s>What does it mean for an effect to be 'typical' in the population?</s><s>-This question essentially asks about the scientific content of statistical inference at the population level.</s><s>Surprisingly, the topic is almost never discussed in statistical scientific papers and textbooks, including those aimed at psychologists and other cognitive scientists.</s><s>Our use of the term 'typical' was inspired by <ref type="bibr" target="#b49">Penny and Holmes (2007)</ref>, who state that in population inference 'one is interested in what is common to the subjects' or in a 'stereotypical effect in the population <ref type="bibr">' (p. 156)</ref>.</s><s>In this paper, we use the term mainly in a negative way: An effect that is only present in a small fraction of the population can hardly be considered typical <ref type="bibr">(or 'common', or 'stereotypical')</ref>.</s><s>In standard univariate analysis concerning the mean of a normal distribution population model, a positive use of the term can be motivated by the fact that the mean is also the mode of the distribution (the most frequent realization) as well as its median (the value that sits 'in the middle' of the distribution).</s><s>But there is another aspect: If in this context we can reject the null hypothesis H 0 : ∆µ = 0 in a one-sided t-test, the inference ∆µ &gt; 0 also means that more than half of the population -the majority -have a positive effect (and less than half a negative effect).</s><s>This statement can be extended to the non-normal case if there is a test that can show that the population median is above zero.</s><s>If we take this observation as a guideline, the natural choice for the threshold in prevalence inference is γ 0 = 0.5 (see also <ref type="bibr" target="#b17">Friston et al., 1999b)</ref>.</s><s>If we can reject this null hypothesis, we can again infer that there is an effect in the majority of subjects in the population, which also implies that the median true effect strength is above-chance (motivating Fig. <ref type="figure" target="#fig_3">4b</ref>). 12</s><s> We therefore propose to call an effect 'typical' if it is present in the majority of subjects in the population.</s></p><p><s>But don't we want to show that the effect generalizes in the sense that it is present in every subject in the population?</s><s>-Maybe ideally, but in practice this is impossible.</s><s>First, it is not known whether any of the effects that are of interest in functional neuroimaging do actually exhibit such an extreme degree of generalization, in particular with respect to a specific anatomical location.</s><s>Second, statistical inference is unable to provide support for such a statement on principle.</s><s>As pointed out above, a univariate one-sided t-test for a normal distribution only provides evidence that a majority of subjects in the population have an effect, not that it generalizes to everyone.</s><s>And in population prevalence inference, at best we could test the prevalence null hypothesis for γ 0 = 0.99 or similar, at the price of very low sensitivity.</s></p><p><s>Concluding we would like to point out that we do not consider the method of 'permutation-based information prevalence inference using the minimum statistic' put forward in the last part to be the definitive solution to the problem of population inference in information-based imaging.</s><s>The main aim of this paper was to demonstrate conclusively that the t-test on accuracies does not provide population inference, and that prevalence inference is an alternative.</s><s>This particular method was presented in order to show that population inference is possible, even based on established methodology only (minimum statistic, first-level permutations).</s><s>However, it does have several shortcomings: The use of the minimum statistic limits the highest γ 0 for which the prevalence null hypothesis can be rejected depending on the number of subjects, and the permutation-based implementation imposes an even lower limit depending on the number of second-level permutations that can be performed.</s><s>Moreover, this method does not provide a way to estimate (instead of just bound) the true population prevalence γ.</s><s>We do however believe that methods focusing on population prevalence are the most promising approach, not just for information-but also for activation-based imaging, because they explicitly provide information about the degree to which an effect generalizes.</s><s>And while we hope that this paper will motivate further methodological work on population inference for information-based imaging, our method does provide a way to improve upon the commonly used t-test on accuracies that is available now. 13  This definition is in line with the use of the term 'true accuracy' by <ref type="bibr" target="#b2">Brodersen et al. (2013)</ref>.</s></p><p><s>It goes beyond the definition of the true accuracy of a particular trained classifier given by <ref type="bibr" target="#b52">Pereira et al. (2009)</ref> as the expectation of â across test data sets (or equivalently, the accuracy determined on an infinite amount of test data).</s><s>Like Brodersen and colleagues, we are not interested in characterizing the performance of a classifier trained on specific random training data, but in characterizing the subject as the source of both training and test data. 14</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Simulated distributions of contrasts and accuracies</head><p><s>We here explain the basic outline of the simulations underlying Sec.</s><s>2.5; for full details of the implementation, see App. C. The distributions arising in this simulation are illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, so that this appendix also serves as an extended explanation of that figure.</s></p><p><s>Single-subject data are generated according to the first-level model of Eq. 28.</s><s>There are n = 25 trials for each condition in each of m = 6 runs.</s><s>At the second level, true activation differences ∆β k for a sample of subjects are generated as independent normally-distributed numbers with mean ∆µ and standard deviation σ 2 (Eq.</s><s>2).</s><s>A sample consists of N = 17 subjects.</s><s>We used a relatively large number of trials and runs in combination with trial-wise classification to obtain fine-grained accuracy estimation distributions well-suited for graphical display (red lines in Fig. <ref type="figure" target="#fig_0">1e and f</ref>), and we chose a sample of 17 subjects to be able to closely approximate the standard significance level of α = 0.05 for subject-wise classification (Fig. <ref type="figure" target="#fig_1">2d</ref>).</s><s>The results remain qualitatively the same with other parameter choices and for classification of run-wise parameter estimates.</s><s>Since multiplying all parameters by a constant factor changes only the scale but not the structure of the data, we choose σ 1 = 1.</s><s>The distributions of activation differences are illustrated in Figure <ref type="figure" target="#fig_0">1a</ref> and b, for the case ∆µ = 1 and σ 2 = 4 and for three subjects with true contrasts ∆β = −5, 1, and 8, respectively.</s></p><p><s>Based on the generative model for single-subject data (Eq.</s><s>28), the amount of information the data contain about the experimental condition can be precisely calculated as a function of the true activation difference ∆β in that subject (Fig. <ref type="figure" target="#fig_0">1c</ref>); it is a symmetric function which reaches its minimum value of 0 bit for ∆β = 0 and saturates towards 1 bit for large |∆β|.</s><s>This demonstrates that in the calculation of information the sign of the true activation difference is discarded, and that there never is negative information.</s></p><p><s>The data are entered into a classification procedure for each subject separately.</s><s>Singletrial data from 5 of the 6 runs are used to train a linear support vector machine (C-SVM with parameter C = 1; implementation by Chang and Lin, 2011, see http://www.csie.</s><s>ntu.edu.tw/~cjlin/libsvm/) which is applied to trials from the left-out run, and this is repeated such that each run is once used for testing (6-fold cross-validation).</s><s>The proportion of correctly classified trials gives the estimated classification accuracy â.</s><s>Fig. <ref type="figure" target="#fig_0">1d</ref> shows the true accuracy a (calculated as the mean of â across many simulations) in a single subject as a function of the true activation difference: a(∆β).</s><s>The shape of this function shows that classification accuracy is an information-like measure: It is a symmetric function of ∆β with a minimum at ∆β = 0, where it reaches the 'chance level' a 0 = 50 %.</s></p><p><s>Actual estimation distributions of accuracies â for the three simulated subjects are shown in Fig. <ref type="figure" target="#fig_0">1e</ref> (red lines).</s><s>Note that in contrast to the assumption of Eq. 4, these distributions are not necessarily close to normal (or binomial; cf.</s><s><ref type="bibr" target="#b59">Schreiber and Krekelberg, 2013;</ref><ref type="bibr" target="#b46">Noirhomme et al., 2014;</ref><ref type="bibr" target="#b35">Jamalabadi et al., 2016)</ref> and may not even be symmetric around the true value a (black bars); this becomes particularly apparent for ∆β = 1 (Fig. <ref type="figure" target="#fig_1">1e, S2</ref>).</s><s>However, the mean of the estimation distribution is by definition identical to the true accuracy, which makes â an unbiased estimator of a.</s></p><p><s>The function a(∆β) (Fig. <ref type="figure" target="#fig_0">1d</ref>) associates each true activation difference ∆β with a corresponding true accuracy a.</s><s>The population distribution of true activation differences ∆β k (Fig. <ref type="figure" target="#fig_0">1b</ref>, black line) therefore determines the population distribution of true accuracies a k (Fig. <ref type="figure" target="#fig_0">1f</ref>, black line).</s><s>For the case illustrated here, this distribution is far from normal (cf.</s><s>Eq. 5); it is limited to a ≥ a 0 but also has a spike at a = a 0 due to the flat minimum of a(∆β), while a weaker secondary maximum at 56 % is due to the nonlinear increase of that function.</s><s>Because â is just an estimate of a, the pronounced profile of this population distribution a k gives rise to a smoother distribution of estimated accuracies across subjects âk (Fig. <ref type="figure" target="#fig_0">1f</ref>, red line), for which the normality assumption of Eq. 6 might be accepted as a rough approximation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Simulation implementation</head><p><s>We here fill in technical details of the numerical calculations and simulations described in App.</s><s>B and presented in Figs.</s><s><ref type="figure" target="#fig_2">1-3</ref>.</s></p><p><s>Single-subject data were simulated using the first-level GLM (Eq.</s><s>28).</s><s>For simplicity, each trial lasted for one time unit (repetition time, 'TR'), each time unit belonged to a trial of one of the two conditions, and the hemodynamic response was assumed to be instantaneous, i.e. x At and x Bt were sequences of 0s and 1s.</s><s>There were 2 • 25 • 6 = 300 time units in total (conditions • trials per run • runs).</s><s>A given activation difference ∆β was implemented by setting β A = 0 and β B = ∆β.</s><s>The error e t consisted of independent (no temporal autocorrelation) normally-distributed pseudo-random numbers with standard deviation √ mn 2 σ 1 , to ensure that Eq. 1 holds.</s><s>The mutual information <ref type="bibr" target="#b6">(Cover and Thomas, 2012)</ref> between the data y and the trial type T ∈ {A, B} encoded in the regressors shown in Fig. <ref type="figure" target="#fig_0">1c</ref> was calculated as</s></p><formula xml:id="formula_43">I(y, T) = H(y) − H(y|T),<label>(29)</label></formula><p><s>where the conditional entropy of y can be determined analytically as</s></p><formula xml:id="formula_44">H(y|T) = 1 2 log 2 (πe mn),<label>(30)</label></formula><p><s>but the marginal entropy</s></p><formula xml:id="formula_45">H(y) = − f y (y) log 2 f y (y) dy<label>(31)</label></formula><p><s>was computed by numerical integration based on the marginal density</s></p><formula xml:id="formula_46">f y (y) = 1 2 N y; 0, mn 2 + 1 2 N y; ∆β, mn 2 . (<label>32</label></formula><formula xml:id="formula_47">)</formula><p><s>Here N (x; µ, σ 2 ) denotes the density of the normal distribution.</s></p><p><s>For the univariate classification results, data at the first level were simulated for 100 values of the true contrast ∆β from 0 to 86.6, at steps that were linearly increasing from 0.00883 to 1.74 to achieve better coverage close to ∆β = 0.</s><s>For each value, 400,000 time series y t were randomly generated and the classification accuracy â was determined by cross-validation.</s><s>Histograms of the resulting estimation distribution of accuracy for three different values of ∆β were used for Fig. <ref type="figure" target="#fig_0">1e</ref>.</s></p><p><s>For each ∆β, the true accuracy a was estimated by the mean, and the width of the estimation distribution ς 1 by the standard deviation of generated values of â.</s><s>To obtain smooth functional relationships used for Fig. <ref type="figure" target="#fig_0">1d</ref> and as a basis for later calculations, both the mean and standard deviation were modeled as functions of the contrast, a(∆β) and ς 1 (∆β), using a cubic smoothing spline.</s></p><p><s>The population density of the true accuracy f a (a) in Fig. <ref type="figure" target="#fig_0">1f</ref> (black line) was computed by transforming the normal population density of the true contrast f ∆β (∆β) (Eq. 2 and Fig. <ref type="figure" target="#fig_0">1b</ref>, black line) through the modeled function a(∆β) using the standard rules of change of variables for densities,</s></p><formula xml:id="formula_48">f a (a) = d da a −1 + (a) f ∆β (a −1 + (a)) + d da a −1 − (a) f ∆β (a −1 − (a)),<label>(33)</label></formula><p><s>where a −1 + (a) and a −1 − (a) denote the positive and negative solution of a(∆β) = a.</s><s>The combined population-and-estimation distribution of accuracies in Fig. <ref type="figure" target="#fig_0">1f</ref> (red line) was computed as an average of histograms of â for different values of ∆β, weighted by the population density of ∆β.</s></p><p><s>The rejection probability of a second-level two-sided t-test on contrast estimates as a function of ∆µ and σ 2 in Fig. <ref type="figure" target="#fig_1">2b</ref> was computed as follows.</s><s>Since the summary statistic ∆β k is normally distributed with expectation ∆µ and combined variance σ 2 c = σ 2 1 + σ 2 2 (Eq.</s><s>3), the t-statistic from N samples is noncentrally t-distributed with N − 1 degrees of freedom and noncentrality parameter ∆µ N/σ 2 c .</s><s>The rejection probability is the probability mass of this distribution exceeding the critical value of the two-sided t-test with N − 1 degrees of freedom in either direction.</s></p><p><s>In a first approximation, the rejection probability of a second-level one-sided t-test on accuracies vs a 0 in Fig. <ref type="figure" target="#fig_1">2a</ref> was computed in the same way, by assuming that the distribution of the summary statistic âk is exactly normal (Eq.</s><s>6 holds), with parameters ā and ς 2 c = ς 2 1 + ς 2 2 .</s><s>Note that we do not assume this to result from a normal distribution of true accuracies in the population (Eq.</s><s>5), but we treat Eq. 6 solely as an approximate description of the actual combined estimation and population variation (Fig. <ref type="figure" target="#fig_0">1f</ref>, red line).</s><s>Under this approximation, the t-statistic from N samples is noncentrally t-distributed with N − 1 degrees of freedom and noncentrality parameter ( ā − a 0 ) N/ς 2 c .</s><s>The distribution moments of the summary statistic âk were numerically calculated from the population distribution of ∆β k (Eq.</s><s>2) combined with the modeled parameters of the estimation distribution of â (Eq.</s><s>4), a(∆β) and ς 1 (∆β), by evaluating ā = a(∆β) N (∆β; ∆µ, σ 2 2 ) d∆β and ς 2 c = ς 2 1 (∆β) + (a(∆β) − ā) 2 N (∆β; ∆µ, σ 2 2 ) d∆β.</s></p><p><s>(34)</s></p><p><s>The rejection probability is the probability mass of the resulting distribution exceeding the critical value of the one-sided t-test with N − 1 degrees of freedom.</s></p><p><s>This approximation was finessed by comparing its results with those of a simulation of t-tests at 26 values of ∆µ (0 to 0.5, equidistant) combined with 101 values of σ 2 (0 to 2, equidistant).</s><s>At each parameter setting, N = 17 values of ∆β were sampled from the population distribution (Eq.</s><s>2), and for each a value of â was drawn from the corresponding estimation distribution.</s><s>For this purpose, we did not simulate time series data and classification again, but re-used the result of the univariate classification simulation described above.</s><s>For each ∆β, the closest of the 100 values realized in that simulation was determined, and â was drawn randomly from the pool of corresponding 400,000 simulation results.</s><s>A one-sided t-test vs a 0 was applied to the N drawn accuracies and it was recorded whether the null hypothesis could be rejected or not.</s><s>This was repeated 5,000,000 times for each parameter setting.</s><s>The resulting simulated rejection probabilities were compared to the approximated rejection probabilities and it was found that the approximation overestimates the rejection probability for larger values (largest discrepancy 0.7270 instead of 0.7169) and underestimates it for smaller values (largest discrepancy 0.1530 instead of 0.1604).</s><s>The relation between simulated and approximated rejection probabilities was modeled using a 4th-order polynomial, which was then used to correct the approximation results for Fig. <ref type="figure" target="#fig_1">2a</ref>.</s><s>This resulted in the smallest occurring approximated rejection probability of 0.05 to be corrected to 0.055.</s><s>The advantage of this combination of approximation and simulation is that the semi-analytic part provides a smooth function of the simulation parameters wellsuited for graphical display, which is guaranteed to be precise by calibrating it using simulation.</s></p><p><s>The rejection probability of FFX analysis applied to contrast estimates in Fig. <ref type="figure" target="#fig_1">2c</ref> was computed as follows.</s><s>FFX analysis also uses a t-statistic, but it compares the mean estimated activation difference 1 N ∑ N k=1 ∆β k not to an estimate of the combined variance σ 2 c = σ 2 1 + σ 2 2 , but of the estimation variance σ 2 1 only.</s><s>σ 2 1 is estimated from the firstlevel GLM residuals with 2(mn − 1) degrees of freedom and the estimate is pooled across N subjects, such that the resulting statistic is t-distributed with 2(mn − 1)N degrees of freedom under the FFX null hypothesis.</s><s>However, under the RFX model including random population variation, the variance of the statistic is actually larger by a factor σ 2 c /σ 2 1 .</s><s>For arbitrary ∆µ, the distribution of the FFX t-statistic is a scaled noncentral t-distribution, with 2(mn − 1)N degrees of freedom, noncentrality parameter ∆µ N/σ 2 c , and scaling factor σ c /σ 1 .</s><s>The rejection probability is the probability mass of this distribution exceeding the critical value of a two-sided t-test with 2(mn − 1)N degrees of freedom in either direction.</s></p><p><s>The distribution of accuracies for classification across subjects depends on the ratio δ = 2 ∆µ/σ c (the subject-level Mahalanobis distance).</s><s>For 100 values of δ from 0 to 10, at steps that were linearly increasing from 0.00102 to 0.2 to achieve better coverage close to 0, second-level univariate data βAk and βBk were generated for N = 17 subjects.</s><s>For 400,000 realizations, the resulting classification accuracy was determined by leave-one-subject-out cross-validation.</s><s>The null hypothesis of no population difference between conditions A and B is realized at δ = 0; from the corresponding simulated null distribution the critical value was determined to be an accuracy of 67.6 %.</s><s>This gives the best possible approximation of the 95th percentile of the discrete distribution of accuracies with 2N + 1 = 35 possible outcomes and leads to a test at a significance level of α = 0.051.</s><s>The rejection probability for all simulated ratios was calculated as the fraction of simulated accuracies that reach or exceed this value, and the rejection probability was modeled as a function of δ using a cubic smoothing spline.</s><s>The rejection probability of classification across subjects in Fig. <ref type="figure" target="#fig_1">2d</ref> was then computed by applying this function to δ(∆µ, σ 2 ) = 2 ∆µ</s></p><formula xml:id="formula_49">σ 2 1 + σ 2 2 (<label>35</label></formula><formula xml:id="formula_50">)</formula><p><s>with σ 2 1 = 1.</s><s>For the multivariate classification results, simulations were performed in the same way as above, but with 10,000 simulated multi-voxel time series for each value of the true contrast, with dimensions p = 2 and 10.</s><s>In contrast to the univariate case, no semi-analytic approximation was used, but the rejection probabilities in Fig. <ref type="figure" target="#fig_2">3a</ref> were directly estimated from 1,000,000 simulated t-tests applied to accuracies resampled from this multivariate simulation, for each of 100 equidistant values of σ 2 from 0 to 2. The rejection probabilities in Fig. <ref type="figure" target="#fig_2">3b</ref> were directly estimated from 5,000,000 simulated t-tests applied to univariate classification accuracies.</s><s>Again, the simulation of time series and classification was not repeated, but the result of the univariate classification simulation described above was re-used.</s><s>N = 17 values of ∆β were sampled from the alternative population model of Eq. 10, and for each a value of â was drawn from the corresponding estimation distribution.</s><s>This was performed for ∆β * = 1, 2, and 5, and for 101 equidistant values of γ from 0 to 1.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc><div><p><s>Figure 1: Distributions of true (black) and estimated (red) values of contrasts and classification accuracies.-a)</s><s>An activation difference estimated from a limited amount of noisy data shows variation (red curves; σ 1 = 1) around the true value (black bars).</s><s>Moreover, the true contrast is different in different subjects (Three panels S1, S2, S3; ∆β = −5, 1, 8. Values for these subjects are also indicated in the following panels by dots or bars.)</s><s>-b) True activation differences (black bars) come from a distribution characterizing the population of possible subjects (black curve; ∆µ = 1, σ 2 = 4).</s><s>Estimated contrasts across subjects show the combined effect of both sources of variation (population + estimation, red curve; √ σ 2 1 +σ 2 2 = 4.12).</s><s>-c) The amount of information single-trial data provide about the trial class (condition) and vice versa, as a function of the true activation difference.</s><s>A negative contrast provides positive information.</s><s>-d) True accuracy a of classification of univariate single-trial data as a function of the true activation difference.</s><s>It is a symmetric function of ∆β, which makes accuracy an information-like measure, with a minimal value a 0 = 50 %.</s><s>-e) Estimation variation of accuracy (6-fold cross-validation) in the three subjects (red curves).</s><s>As apparent for subject 2, the distributions can deviate strongly from normality.</s><s>While estimated accuracies can be below chance (gray line), true accuracies (black bars; a = 67.7,</s><s>51.5, 61.3 %) cannot.</s><s>-f) The population variation (black curve) and combined variation (population + estimation, red curve) of accuracy that result from the population distribution of contrasts ∆β k (b, black line), the functional relationship between ∆β and a (d), and the estimation distributions (e).</s><s>The population distribution is restricted to a ≥ a 0 and in this example shows a spike at 50 % and a weaker maximum at 56 %. -For further details, see App.</s><s>B.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc><div><p><s>Figure2: Rejection probability as a function of simulation parameters ∆µ (the population mean true contrast) and σ 2 (the population variance of true contrasts), for different null hypothesis tests at significance level α = 0.05.</s><s>-a) Second-level one-sided one-sample t-test applied to estimated classification accuracies vs chance level a 0 = 50 %.</s><s>The smallest rejection probability is reached if both ∆µ = 0 and σ 2 = 0. -b) Second-level two-sided one-sample t-test applied to estimated activation differences vs 0 (univariate RFX analysis).</s><s>The smallest rejection probability is reached for ∆µ = 0. -c) Fixed-effects analysis of activation differences.</s><s>The smallest rejection probability is reached if both ∆µ = 0 and σ 2 = 0. -d) Test based on classification across subjects.</s><s>The smallest rejection probability is reached for ∆µ = 0.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc><div><p><s>Figure 3: Rejection probability of a second-level one-sided one-sample t-test applied to estimated classification accuracies vs chance level, as a function of simulation parameters.</s><s>The significance level α = 0.05 is shown as a gray horizontal line.</s><s>-a) Multivariate normally distributed contrasts in p voxels, with variation uncorrelated between voxels and a population mean activation difference of ∆µ = 0 everywhere.</s><s>The rejection probability increases with the population variance σ 2 2 , and the increase is stronger for higher dimensionality p.</s><s>The line for univariate data (p = 1) corresponds to a central vertical section through the rejection probability function of Fig. 2a.</s><s>-b) Population proportion model: Fixed true contrast ∆β * in a proportion γ of the population, and 0 in the rest.</s><s>The rejection probability always reaches α for γ = 0.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc><div><p><s>Figure4: Second-level results for the classification of object category of a visual stimulus (see<ref type="bibr" target="#b5">Cichy et al., 2011)</ref> shown in a sagittal slice through right lateral occipital cortex and fusiform gyrus.</s><s>Statistics are corrected for multiple comparisons.</s><s>-a) Information prevalence inference.</s><s>Highlighted areas are those where the global null hypothesis (prevalence γ = 0) can be rejected at a level of α = 0.05.</s><s>Colors visualize a lower bound γ 0 on the prevalence of category information (confidence level 0.95).</s><s>-b) For those areas where the prevalence null hypothesis γ ≤ γ 0 can be rejected at γ 0 = 0.5, i.e.</s><s>where it can be inferred that the majority of subjects in the population have an effect, colors visualize the median classification accuracy across subjects.-c)</s><s>t-test on accuracies vs chance level a 0 = 50 %.</s><s>For those areas where the null hypothesis ā = a 0 can be rejected at a level of α = 0.05, colors visualize the underlying t-value.</s></p></div></figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that in this paper we only discuss the standard case of MVPA where the pair of experimental conditions is the same for training and test data. In 'cross-decoding' (cf.<ref type="bibr" target="#b28">Haynes and Rees, 2005a)</ref>, where it is tested whether a classifier trained on one pair of conditions is able to extract information corresponding to another pair of conditions, below-chance true accuracies may be possible. Cross-decoding targets not just the presence of information, but also the degree to which its neurophysiological representation is invariant with respect to another experimental manipulation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The specific RFX procedure to apply a second-level t-test or ANOVA to first-level contrast estimates is called the 'summary statistic' approach<ref type="bibr" target="#b50">(Penny and Holmes, 2004)</ref> because it considers only the contrast estimates ∆β k which summarize single-subject data. It is interesting to note that it is only this summary statistic approach that suggests other first-level summary statistics like classification accuracies could simply be plugged into a second-level t-test. For related attempts at a mixed-effects model for classification accuracies in a Bayesian setting see<ref type="bibr" target="#b48">Olivetti et al. (2012)</ref> and<ref type="bibr" target="#b3">Brodersen et al. (2012</ref><ref type="bibr" target="#b2">Brodersen et al. ( , 2013))</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We define as 'information-like' those measures that covary with mutual information. Note that this excludes other quantities that may be of interest in MVPA, in particular classifier weights (see<ref type="bibr" target="#b19">Gaonkar and Davatzikos, 2013)</ref>. Correspondingly, a classifier weight can be negative (cf.<ref type="bibr" target="#b68">Todd et al., 2013)</ref>, in the simplest case if information is coded in the corresponding voxel by a decrease in activation (but cf.<ref type="bibr" target="#b23">Haufe et al., 2014)</ref>.4  We would like to thank an anonymous reviewer for pointing out that this poses an additional distributional problem for the use of the t-test: The standard derivation of the null distribution of test statistics becomes invalid when the null hypothesis lies on the border of the parameter space (seeFahrmeir et al., 2013, Sec. 7.3.4).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5"><ref type="bibr" target="#b68">Todd et al. (2013)</ref> point out a related but different problem in applying a second-level test to a summary statistic that estimates an unsigned quantity: The traditional strategy of balancing or randomizing confounds across subjects (cf.<ref type="bibr" target="#b14">Fisher, 1935)</ref> does no longer work, because after removing signs confounding effects of different direction cannot cancel each other out.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6"><ref type="bibr" target="#b8">Davis et al. (2014)</ref> make a similar observation, but without noting its consequences for population inference.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">A solution might be provided by 'hyperalignment'<ref type="bibr" target="#b26">(Haxby et al., 2011)</ref> which attempts to establish a fine-grained functional correspondence between different subjects' brains. However, as<ref type="bibr" target="#b68">Todd et al. (2013)</ref> point out, aligning patterns effectively discards sign (direction) information, too, unless the hyperalignment parameters are determined from separate data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8"><ref type="bibr" target="#b15">Fisher's (1925)</ref> combined probability test,<ref type="bibr" target="#b67">Tippett's (1931)</ref> minimum p-value, the conjunction test of<ref type="bibr" target="#b70">Worsley and Friston (2000)</ref>,Stouffer et al.'s (1949) combined z-value, Mudholkar and<ref type="bibr" target="#b42">George's (1979)</ref> logit method, and fixed-effects analysis.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9"> Stephan et al.'s Bayesian RFX analysis for DCM has been adapted for GLM model selection by<ref type="bibr" target="#b61">Soch et al. (2016)</ref>, and may be adapted for MGLM (cf.<ref type="bibr" target="#b0">Allefeld and Haynes, 2014)</ref> model selection to support MVPA.14</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">For γ &gt; 0.5 we can consider subjects where the true accuracy is at chance (no information) as 'outliers'. The median is a robust estimator with a breakdown point of 0.5, i.e. it can handle samples where up to half of the values are outliers<ref type="bibr" target="#b34">(Huber and Ronchetti, 2009)</ref>. It is cautious in the sense that it will under-rather than overestimate the the true median above-chance accuracy, because all 'outliers' are on the side of small values.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">With respect to fMRI-MVPA, this phenomenon is informally discussed (see e.g. J. Etzel's blog, http://mvpa.blogspot.de/2013/04/below-chance-classification-accuracy.html), but appears to not yet have given rise to a peer-reviewed publication.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Additionally, this choice is consistent with the use of the 'exceedance probability' in Bayesian RFX<ref type="bibr" target="#b64">(Stephan et al., 2009)</ref>, which is the posterior probability that a given model is more frequent in the population than all other models, if only two models are considered.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">For a true accuracy a the situation is not that simple, because we do not have a generative model of the data y t that is parametrized by a. But, consistent with the RFX model for accuracies and in analogy to GLM parameters, we can define the true accuracy as the expectation value of estimated accuracies, a = â , i.e. the mean across an infinite number of repetions of the experiment with the same subject. This true accuracy a is a complex function of the true GLM parameters for the included voxels and conditions, as well as the error variance and covariance, and may also depend on the classification algorithm.13  An implementation for Matlab can be obtained from the corresponding author or at http://github. com/allefeld/prevalence-permutation/releases, and is also included in The Decoding Toolbox (TDT;<ref type="bibr" target="#b31">Hebart et al., 2015)</ref> from version 3.8.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14">Note that a classifier trained on a limited amount of training data will perform worse than optimally possible. This effect brings the accuracy closer to (but not below) chance level, i.e. what we consider as the true accuracy is generally smaller than the 'optimal true accuracy': a opt ≥ a ≥ a 0 (cf.<ref type="bibr" target="#b71">Wyman et al., 1990)</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p><s>Kai Görgen was supported by the German Research Foundation (DFG grants GRK1589/1 and FK:JA945/3-1).</s></p><p><s>The authors would like to thank Tom Nichols, Jakob Heinzle, Jörn Diedrichsen, Will Penny, María Herrojo Ruiz, Joram Soch, Martin Hebart, Jo Etzel, Yaroslav Halchenko,  and Thomas Christophel for discussions, comments, and hints.</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A What is a true accuracy?</head><p><s>The first-level model (Eq.</s><s>4) for the RFX analysis of accuracies assumes that there is a 'true accuracy' a that underlies the random estimated accuracies â that we actually observe.</s><s>But what does this true value stand for?</s></p><p><s>In the case of a true activation difference ∆β, the answer is simple: The first-level GLM</s></p><p><s>where x At and x Bt are regressor functions and e t is noise, provides a statistical description of the (neural and hemodynamic) process by which we assume the observed fMRI data are generated (a generative model).</s><s>This model includes parameters β A and β B which govern the generative process, and which characterize the respective subject.</s><s>From a given data set y t , we can estimate these parameters, βA and βB , but because the data are noisy, these estimates will not coincide with the underlying true values.</s><s>However, if we could repeat the experiment an infinite number of times with the same subject, the mean of estimates across these repetitions would recover the true values, because the estimators are unbiased (provided the parameters are estimable).</s><s>This mean is also called expectation value; βA = β A , βB = β B , and therefore ∆β = ∆β = β B − β A (cf. Eq. 1).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note</head><p><s>Though the spatially extended prevalence null hypothesis (PN) at γ 0 = 0 is equivalent to the spatially extended global null hypothesis (GN), the stated significance criterion for PN (cf.</s><s>Eq. 21)</s></p><p><s>is not equivalent to the significance criterion for GN</s></p><p><s>Rather, it is conservative due to the additional term [1</s></p><p><s>This discrepancy carries over into the algorithm Step 5b, where the criterion p N (m) ≤ α * (corresponding to Eq. 37) for the prevalence lower bound (Eq.</s><s>23) to be defined is not equivalent to the significance criterion for GN.</s></p><p><s>However, if GN can be rejected (p * N (m) ≤ α), the effective number of tests is moderately large (~10), and a standard significance level α is used (e.g.</s><s>0.05), it holds p N (m)</s></p><p><s>1, so that the difference between the criteria becomes negligible.</s><s>Is summary, Eq.</s><s>21 is not exact but rather formulates (yet another) upper bound, but the error introduced this way is small under normal circumstances.</s><s>In any case, the criterion is conservative and therefore the test remains valid.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Searchlight-based multi-voxel pattern analysis of fMRI by cross-validated MANOVA</title>
		<author>
			<persName><forename type="first">C</forename><surname>Allefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Haynes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="345" to="357" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Steps to an ecology of mind</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bateson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972">1972</date>
			<publisher>University of Chicago Press</publisher>
			<pubPlace>Chicago</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variational Bayesian mixed-effects inference for classification studies</title>
		<author>
			<persName><forename type="first">K</forename><surname>Brodersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daunizeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mathys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chumbley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stephan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="345" to="361" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bayesian mixed-effects inference on classification performance in hierarchical data sets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Brodersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mathys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chumbley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daunizeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Stephan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="3133" to="3176" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoding the identity and location of objects in human LOC</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Cichy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Haynes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="2297" to="2307" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>Hoboken</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Functional magnetic resonance imaging (fMRI) &apos;brain reading&apos;: Detecting and classifying distributed patterns of fMRI activity in human visual cortex</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Savoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="261" to="270" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">What do differences between multi-voxel and univariate analysis mean? How subject-, voxel-, and trial-level variance impact fMRI analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Larocque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mumford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Poldrack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="271" to="283" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Permutation methods: A basis for exact inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="676" to="685" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MVPA permutation schemes: Permutation testing in the land of cross-validation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Etzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Braver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Pattern Recognition in Neuroimaging</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="140" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Searchlight analysis: Promise, pitfalls, and potential</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Etzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Zacks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Braver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="261" to="269" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Regression: Models, methods and applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fahrmeir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kneib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Marx</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The design of experiments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1935">1935</date>
			<publisher>Oliver &amp; Boyd</publisher>
			<pubPlace>Edinburgh</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Statistical methods for research workers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1925">1925</date>
			<publisher>Oliver &amp; Boyd</publisher>
			<pubPlace>Edinburgh</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multisubject fMRI studies and conjunction analyses</title>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Büchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Worsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="385" to="396" />
			<date type="published" when="1999">1999a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">How many subjects constitute a study?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Worsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="1999">1999b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Statistical parametric maps in functional imaging: A general linear approach</title>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Worsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Poline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Frackowiak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Brain Mapping</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="189" to="210" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analytic estimation of statistical significance maps for support vector machine based multi-variate image analysis and classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gaonkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="270" to="283" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interpreting support vector machine models for multivariate group wise analysis in neuroimaging</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gaonkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
		<author>
			<persName><surname>Adni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="190" to="204" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Permutation tests for classification: Towards statistical significance in image-based studies, in: Information Processing in Medical Imaging</title>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fischl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="330" to="341" />
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Detecting, avoiding &amp; eliminating confounds in MVPA / decoding studies. Poster presented at the 20th annual meeting of the Organization for Human Brain Mapping (OHBM)</title>
		<author>
			<persName><forename type="first">K</forename><surname>Görgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Haynes</surname></persName>
		</author>
		<idno type="DOI">10.7490/f1000research.1111808.1</idno>
		<ptr target="http://dx.doi.org/10.7490/f1000research.1111808.1" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On the interpretation of weight vectors of linear models in multivariate neuroimaging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haufe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Meinecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Görgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dähne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Blankertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bießmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="96" to="110" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multivariate pattern analysis of fMRI: The early beginnings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Haxby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="852" to="855" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed and overlapping representations of faces and objects in ventral temporal cortex</title>
		<author>
			<persName><forename type="first">J</forename><surname>Haxby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gobbini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Furey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ishai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schouten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pietrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<biblScope unit="page" from="2425" to="2430" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A common, high-dimensional model of the representational space in human ventral temporal cortex</title>
		<author>
			<persName><forename type="first">J</forename><surname>Haxby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guntupalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Connolly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Halchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gobbini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hanke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ramadge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="404" to="416" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Decoding mental states from brain activity in humans</title>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="523" to="534" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Predicting the stream of consciousness from activity in human visual cortex</title>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1301" to="1307" />
			<date type="published" when="2005">2005a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Predicting the orientation of invisible stimuli from activity in human primary visual cortex</title>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="686" to="691" />
			<date type="published" when="2005">2005b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reading hidden intentions in the human brain</title>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Passingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="323" to="328" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The Decoding Toolbox (TDT): A versatile software package for multivariate analyses of functional imaging data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Görgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Haynes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">88</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalisability, random effects &amp; population inference</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">S754</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving sparse recovery on structured images with bagged clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hoyos-Idrobo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Pattern Recognition in Neuroimaging</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="73" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ronchetti</surname></persName>
		</author>
		<title level="m">Robust statistics</title>
				<meeting><address><addrLine>Hoboken</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Classification based hypothesis testing in neuroscience: Below-chance level classification rates and overlooked statistical properties of linear parametric classifiers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jamalabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leibold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Brain Mapping</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1842" to="1855" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Classification of anti-learnable biological and synthetic data, in: Knowledge Discovery in Databases: PKDD</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kowalczyk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="176" to="187" />
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Analyzing for information, not activation, to exploit high-resolution fMRI</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bandettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="649" to="662" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Information-based functional brain mapping</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bandettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="3863" to="3868" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Combining brains: A survey of methods for statistical pooling of information</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Luna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Eddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="538" to="550" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Testing statistical hypotheses</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>3rd ed</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Classifying brain states and determining the discriminating activation patterns: Support Vector Machine on functional MRI data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mourao-Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L W</forename><surname>Bokde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Born</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hampel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="980" to="995" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The logit method for combining probabilities</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mudholkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>George</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Optimizing Methods in Statistics</title>
				<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1979">1979</date>
			<biblScope unit="page" from="345" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Valid conjunction inference with the minimum statistic</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Poline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="653" to="660" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Nonparametric permutation tests for functional neuroimaging: A primer with examples</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Brain Mapping</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A toolbox for representational similarity analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wingfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Marslen-Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2014">2014. e1003553</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Biased binomial assessment of cross-validated estimation of classification accuracies illustrated in diagnosis predictions</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Noirhomme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lesenfants</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soddu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrouff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Garraux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luxen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laureys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage: Clinical</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="687" to="694" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Beyond mind-reading: Multi-voxel pattern analysis of fMRI data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Polyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Detre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haxby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="424" to="430" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bayesian hypothesis testing for pattern discrimination in brain decoding</title>
		<author>
			<persName><forename type="first">E</forename><surname>Olivetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Veeramachaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nowakowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="2075" to="2084" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Random effects analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Penny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Parametric Mapping</title>
				<editor>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</editor>
		<editor>
			<persName><surname>Others</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Random-effects analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Penny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holmes</surname></persName>
		</author>
		<editor>Frackowiak, R., others</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="843" to="850" />
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
	<note>Human Brain Function</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Information mapping with pattern classifiers: A comparative study</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="476" to="496" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Machine learning classifiers and fMRI: A tutorial overview</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="S199" to="S209" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cognitive conjunction: A new approach to brain activation experiments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="261" to="270" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The robustness of parametric statistical methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Guiard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology Science</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="175" to="208" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Revisiting multi-subject random effects in fMRI: Advocating prevalence estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rosenblatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Benjamini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="113" to="121" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Detecting chance: A solution to the null sensitivity problem in subliminal priming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rouder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Morey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Speckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pratte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="597" to="605" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A universal and efficient method to compute maps from imagebased prediction models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention</title>
				<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The relevance voxel machine (RVoxM): A selftuning Bayesian model for informative image-based prediction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Leemput</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2290" to="2306" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The statistical analysis of multi-voxel patterns in functional imaging</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krekelberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS one</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">e69328</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Variance components</title>
		<author>
			<persName><forename type="first">S</forename><surname>Searle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcculloch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>Hoboken</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">How to avoid mismodelling in GLMbased fMRI data analysis: Cross-validated Bayesian model selection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Soch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allefeld</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuroimage.2016.07.047</idno>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">How distributed is visual category information in human occipito-temporal cortex? An fMRI study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Spiridon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kanwisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1157" to="1165" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Statistical inference and multiple testing correction in classification-based multi-voxel pattern analysis (MVPA): Random permutations and cluster size control</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stelzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="69" to="82" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Bayesian model selection for group studies</title>
		<author>
			<persName><forename type="first">K</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Penny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daunizeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1004" to="1017" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">The American soldier: Combat and its aftermath</title>
		<author>
			<persName><forename type="first">S</forename><surname>Stouffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lumsdaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lumsdaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Williams</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Janis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Star</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cottrell</surname><genName>Jr</genName></persName>
		</author>
		<imprint>
			<date type="published" when="1949">1949</date>
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Dealing with the shortcomings of spatial normalization: Multi-subject parcellation of fMRI datasets</title>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Flandin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pinel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ciuciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Poline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Brain Mapping</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="678" to="693" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">The methods of statistics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tippett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1931">1931</date>
			<publisher>Williams &amp; Norgate</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Confounds in multivariate pattern analysis: Theory and rule representation case study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nystrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="157" to="165" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Support vector machine learningbased fMRI data group analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Childress</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Detre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1139" to="1151" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A test for a conjunction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Worsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics &amp; Probability Letters</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="135" to="140" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A comparison of asymptotic error rate expansions for the sample linear discriminant function</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="775" to="783" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
