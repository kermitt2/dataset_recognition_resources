<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Machine Learning to Differentiate Between Positive and Negative Emotions Using Pupil Diameter</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-12-22">22 December 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Frank</forename><forename type="middle">A</forename><surname>Russo</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Ibrahima</forename><surname>Faye</surname></persName>
							<email>ibrahima_faye@petronas.com.my</email>
							<affiliation key="aff3">
								<orgName type="department">Center for Intelligent Signal and Imaging Research</orgName>
								<orgName type="institution" key="instit1">Universiti Teknologi PETRONAS</orgName>
								<orgName type="institution" key="instit2">Bandar Seri Iskandar</orgName>
								<address>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Fundamental and Applied Sciences</orgName>
								<orgName type="institution" key="instit1">Universiti Teknologi PETRONAS</orgName>
								<orgName type="institution" key="instit2">Bandar Seri Iskandar</orgName>
								<address>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Areej</forename><surname>Babiker</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Center for Intelligent Signal and Imaging Research</orgName>
								<orgName type="institution" key="instit1">Universiti Teknologi PETRONAS</orgName>
								<orgName type="institution" key="instit2">Bandar Seri Iskandar</orgName>
								<address>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution" key="instit1">Universiti Teknologi PETRONAS</orgName>
								<orgName type="institution" key="instit2">Bandar Seri Iskandar</orgName>
								<address>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kristin</forename><surname>Prehn</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of Neurology and NeuroCure Clinical Research Center</orgName>
								<orgName type="institution">Charité Universitätsmedizin Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aamir</forename><surname>Malik</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Center for Intelligent Signal and Imaging Research</orgName>
								<orgName type="institution" key="instit1">Universiti Teknologi PETRONAS</orgName>
								<orgName type="institution" key="instit2">Bandar Seri Iskandar</orgName>
								<address>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="institution" key="instit1">Universiti Teknologi PETRONAS</orgName>
								<orgName type="institution" key="instit2">Bandar Seri Iskandar</orgName>
								<address>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tilburg University</orgName>
								<address>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Duke University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Ryerson University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Machine Learning to Differentiate Between Positive and Negative Emotions Using Pupil Diameter</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-12-22">22 December 2015</date>
						</imprint>
					</monogr>
					<idno type="MD5">55CFE0F27D1AFFAC4EDB1A3488C723EE</idno>
					<idno type="DOI">10.3389/fpsyg.2015.01921</idno>
					<note type="submission">This article was submitted to Emotion Science, a section of the journal Frontiers in Psychology Received: 10 July 2015 Accepted: 30 November 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2-SNAPSHOT" ident="GROBID" when="2022-05-18T11:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>pupillometry</term>
					<term>emotion recognition</term>
					<term>classification</term>
					<term>k-nearest neighbor algorithm</term>
					<term>sensitivity analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p><s>Pupil diameter (PD) has been suggested as a reliable parameter for identifying an individual's emotional state.</s><s>In this paper, we introduce a learning machine technique to detect and differentiate between positive and negative emotions.</s><s>We presented 30 participants with positive and negative sound stimuli and recorded pupillary responses.</s><s>The results showed a significant increase in pupil dilation during the processing of negative and positive sound stimuli with greater increase for negative stimuli.</s><s>We also found a more sustained dilation for negative compared to positive stimuli at the end of the trial, which was utilized to differentiate between positive and negative emotions using a machine learning approach which gave an accuracy of 96.5% with sensitivity of 97.93% and specificity of 98%.</s><s>The obtained results were validated using another dataset designed for a different study and which was recorded while 30 participants processed word pairs with positive and negative emotions.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p><s>Emotions have a significant impact on perception, decision making, action generation, as well as action execution and control (e.g., <ref type="bibr" target="#b43">Surakka and Sams, 1999;</ref><ref type="bibr" target="#b51">Zhu and Thagard, 2002)</ref>.</s><s>Processes such as learning, attention, perception, and memory are affected by emotions.</s><s>Recognizing emotional expressions is important for the development of Human computer interaction (HCI) systems.</s><s>Because of the manifoldness and complexity of emotional expressions, much research has been conducted to understand and explain the mechanisms involved in emotion recognition.</s><s>This is driven by the huge amount of promising usages and benefits such systems might have.</s><s>In the field of social and clinical psychology, emotion detection systems could help to diagnose psychological disorders including fatig, stress, and depression at their very early stages <ref type="bibr" target="#b25">(Kulkarni et al., 2009)</ref>.</s><s>In entertainment and video game industry, artificial characters who interact with the player have been developed <ref type="bibr" target="#b30">(Maes, 1995)</ref>.</s><s>Moreover, in applications where computers play a social role such as companion or instructor, the functionality of the system could be improved dramatically if the system could automatically recognize a user's emotions and take appropriate action.</s><s>Emotion detection has been further applied to learning process through feedback which increases student's motivation and interaction with the learning environment and thus maximizes learning outcomes <ref type="bibr">(Pampouchidou, 2011, Unpublished,)</ref>.</s><s>Automatic face tracking expression analysis has already been integrated in automatic animated tutoring systems <ref type="bibr" target="#b4">(Bartlett et al., 2003)</ref>.</s><s>Other possible applications such as stress classification have also gained intensive attention <ref type="bibr" target="#b35">(Pedrotti et al., 2014)</ref>.</s><s>This paper is an attempt to differentiate positive and negative emotions.</s></p><p><s>Emotions are recognized in different ways: (1) visually (frosm facial expressions displayed on pictures and in videos), (2) by changes in signals of the autonomic nervous system (ANS), and (3) acoustically (from the human voice; <ref type="bibr" target="#b28">Lisetti, 2002)</ref>.</s><s>This paper addresses emotion detection based on ANS signals, and specifically, using Pupil Diameter (PD).</s><s>The pupil is the black hole in the middle of the iris that regulates light entrance to the retina.</s><s>It is regulated by the ANS that consists of three divisions: the sympathetic, the parasympathetic, and the enteric system <ref type="bibr" target="#b34">(Partala and Surakka, 2003;</ref><ref type="bibr" target="#b38">Ren et al., 2011)</ref>.</s><s>The parasympathetic and sympathetic divisions of the ANS govern two sets of muscles in the iris called the sphincter and the dilator, which are both responsible for changes in PD.</s><s>However, <ref type="bibr" target="#b10">Bradley et al. (2008)</ref> supported the hypothesis that pupillary changes are mainly associated with sympathetic activity.</s><s>Changes in PD have been proven to be optimal in measuring human emotion although differences in luminance between stimuli may have some influence in an uncontrolled environment (e.g., <ref type="bibr" target="#b16">Geangu et al., 2011)</ref>.</s><s>Moreover, measurement of the pupil size has important advantages over other physiological signals, such as heart rate and skin conductance, because it is less affected by body gestures.</s><s>Besides this, it depends solely on the ANS that is largely unconscious and difficult to control voluntarily.</s><s>Changes of the PD occur with a short latency and can be recorded by a camera without attaching any sensors which makes data acquisition more convenient than the recording of skin conductance or heart rate (see <ref type="bibr" target="#b34">Partala and Surakka, 2003;</ref><ref type="bibr" target="#b0">Adolphs, 2006;</ref><ref type="bibr" target="#b47">Wang, 2010;</ref><ref type="bibr" target="#b26">Lanata et al., 2011)</ref>.</s><s>The eye tracking system or the technology necessary for accurate measurement is relatively cheaper and simpler to use compared to the technologies for measuring other signals (e.g., EEG).</s></p><p><s>Previous findings and research showed that pupil dilation indicates cognitive load as well as emotions and arousal (e.g., <ref type="bibr" target="#b19">Hess, 1972;</ref><ref type="bibr" target="#b10">Bradley et al., 2008;</ref><ref type="bibr" target="#b47">Wang, 2010)</ref>.</s><s>A study conducted by <ref type="bibr" target="#b21">Hess and Polt (1960)</ref>, was the beginning of pupillary responses research.</s><s>It showed that PD is related to "feeling tone" or emotions caused by picture viewing and was followed by several studies confirming pupillary dilation in response to emotions <ref type="bibr" target="#b49">(Woodmansee, 1967;</ref><ref type="bibr" target="#b22">Janisse, 1974;</ref><ref type="bibr" target="#b1">Andreassi, 2000;</ref><ref type="bibr" target="#b29">Lisetti and Nasoz, 2004;</ref><ref type="bibr" target="#b45">Valverde et al., 2010)</ref>.</s><s>Majority of recent studies have provided evidence for greater dilation in PD during the processing of both positive and negative compared to neutral stimuli <ref type="bibr" target="#b34">(Partala and Surakka, 2003;</ref><ref type="bibr" target="#b10">Bradley et al., 2008)</ref>.</s></p><p><s>Subjectively experienced emotional states can be characterized by the dimensions valence and arousal <ref type="bibr" target="#b50">(Wundt, 1924</ref>; for a review, see Feldman <ref type="bibr" target="#b15">Barrett and Russell, 1999)</ref>.</s><s>Valence represents the hedonic tone of an emotion (i.e., pleasuredispleasure), whereas emotional arousal refers to the energy level of the emotion (i.e., the psycho-physiological level of activation).</s><s>In addition to this dimensional approach, a set of different qualitative emotional states has been suggested comprising happiness, surprise, fear, anger, and disgust (often referred to as basic emotions; <ref type="bibr" target="#b14">Ekman and Friesen, 2003</ref>, for critical discussion see also <ref type="bibr" target="#b3">Barrett, 1998)</ref>.</s></p><p><s>Measurement of emotion is affected by one's emotional status, e.g., subjective experience, physiology, culture, and behavior.</s><s>Here, we applied self-report measurement that is represented in Positive and Negative Affect Schedule -Expanded form (PANAS-X) model <ref type="bibr" target="#b48">(Watson and Clark, 1999)</ref>.</s></p><p><s>Machine learning techniques are consisted of a group of effective statistical methods in the field of pattern recognition, in particular, with high-dimensional problems <ref type="bibr" target="#b44">(Trabelsi and Frasson, 2010;</ref><ref type="bibr" target="#b24">Kragel and Labar, 2013;</ref><ref type="bibr" target="#b12">Chang et al., 2015)</ref>.</s><s>One of the simplest techniques of these groups are the nearest neighbor.</s><s>Their rule identifies the class of unknown data point based on its nearest neighbor whose class is already known <ref type="bibr" target="#b7">(Bhatia and Vandana, 2010)</ref>.</s><s>kNN has been used extensively in emotion recognition researches either individually or combined with other machine learning techniques <ref type="bibr" target="#b32">(Murugappan et al., 2010;</ref><ref type="bibr" target="#b31">Meftah et al., 2012;</ref><ref type="bibr" target="#b18">Hatamikia et al., 2014)</ref> In the present study, we investigated whether pupillary responses during the processing of positive and negative stimuli differ in behavior and whether this difference can be used to classify emotions into positive and negative ones using a machine learning approach.</s><s>Two datasets were used in this study.</s><s>The first dataset, used 20 sound stimuli that were divided into 10 negative sounds and 10 positive sounds (IADS; <ref type="bibr" target="#b8">Bradley and Lang, 1999)</ref>.</s><s>The second dataset was collected by <ref type="bibr" target="#b36">Prehn et al. (2011)</ref>, and used to validate the model.</s><s>The authors presented participants with word pairs with emotional content and participants decided whether word pairs corresponded in both their emotional and conceptual relations.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MATERIALS AND METHODS</head><p><s>To investigate whether there is significant difference in pupil dilation while processing positive and negative stimuli that can be applied to classify pupillary responses using a machine learning approach, we conducted an experiment in which participants heard emotional sounds while their PD was measured.</s><s>The experimental procedures were approved by Universiti Teknologi PETRONAS Ethical Committee.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p><s>Thirty healthy subjects (17 males) with normal and corrected-tonormal vision participated in the study.</s><s>The participants were university students, with mean age of 24.56 years (SD = 2.87).</s><s>The participants were not affected by any medication that could influence pupillary response.</s><s>A briefing about the experiment was given and a consent form was signed by each participant.</s></p><p><s>Another dataset <ref type="bibr" target="#b36">(Prehn et al., 2011)</ref> was used to validate and confirm the result obtained from the first dataset.</s><s>In the second study 30 healthy subjects (11 males) participated with mean age of 23.93 years (SD = 4.34).</s><s>Participants were native German speakers and did not take any medication that could influence pupillary response.</s><s>They also gave written consent and received either course credit or payment (20 Euro) for their participation.</s><s>Throughout the present study, datasets will be referred to as first dataset and second dataset, respectively.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stimuli</head><p><s>Stimulations that are used to trigger emotions are of three types: visual, audio, and audio-visual.</s><s>In this experiment, audio stimulation was used.</s><s>To ensure the spontaneity and occurrence of desired emotional states, a strong effective stimulation was used.</s><s>The twenty sound stimuli were divided into 10 negative sounds and 10 positive sounds and differed significantly in valence [M = 4.5, SD = 1.78, t(9) = −9.32,</s><s>p &lt; 0.001], and in arousal [M = 5.9, SD = 1.99, t(9) = −3.36,</s><s>p = 0.008] 1 (IADS; <ref type="bibr" target="#b8">Bradley and Lang, 1999)</ref>.</s><s>Audio stimulation was chosen to help controlling the environment of the experiment and thus eliminate the possible effect of luminance on pupil size.</s><s>Sounds have also high potential to trigger emotions.</s><s>All sounds were about 6 s long and were presented in randomized order.</s></p><p><s>In the second dataset, <ref type="bibr" target="#b36">Prehn et al. (2011)</ref> developed an analogical reasoning task to describe the processing of cognitive and affective aspects during simultaneous presentation of word pairs.</s><s>Each word pair could be described by an emotional and a conceptual relation and the subjects decided whether both emotional and conceptual relations corresponded or not, see Table <ref type="table" target="#tab_1">1</ref>.</s></p><p><s>Participants had to press one of two buttons that are labeled with "yes" or "no" as quickly and correctly as possible in a response device.</s><s>There were four different conditions: Con = Emo=: conceptual and emotional relations between pairs of words corresponding, n = 108 trials; Con = Emo =: conceptual corresponding but emotional not corresponding, n = 36 trials; Con =Emo=: conceptual relation not corresponding but emotional corresponding, n = 36 trials; Con =Emo =: conceptual and emotional relations not corresponding, n = 36 trials (see <ref type="bibr" target="#b36">Prehn et al., 2011)</ref>.</s></p><p><s>Here, we only analyzed the data from condition Con = Emo= and condition Con =Emo=.</s><s>In both conditions emotional valence of the two word pairs were identical and only one emotional state (either positive or negative) was detected in each trial.</s><s>In the other two conditions (Con = Emo = and Con =Emo =) emotional valence did not correspond, which allows the occurrence of two opposite emotional states (positive and negative) at the same time.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task</head><p><s>In the first dataset, subjects were seated comfortably in luminance-controlled room with approximately 65 cm from the eye-tracking system.</s><s>A five-point calibration was executed before starting the experiment to locate participants' pupils.</s><s>Stimuli were directly delivered through headphones to participant's ear at constant and comfortable level.</s><s>They were given brief written instructions that were also shown in the system screen prior to the beginning of experiment.</s></p><p><s>Sounds were played after a preparation period of 3 s for each trial.</s><s>This preparation period was to get subjects ready and also to allow the PD to return to its normal size.</s><s>Each trial included:</s></p><p><s>• Preparation phase (3 s).</s></p><p><s>• Sound stimulus (6 s).</s></p><p><s>• Rating interval (21 s).</s></p><p><s>Figure <ref type="figure" target="#fig_0">1</ref> summarizes the experimental procedure.</s><s>To assess the subjective feelings of an individual, the PANAS-X model was used as a self-report approach with 60 items that comprises a set of rating scales.</s><s>These scales are: two general dimensions (positive affect and negative affect), basic positive emotion, basic negative emotion and other affective states (consist of eleven specific affects).</s><s>Four of these eleven affects were considered by <ref type="bibr" target="#b14">Ekman and Friesen (2003)</ref> as basic emotions.</s><s>The reason for using this model is basically its discrete nature which is easily interpreted and understood by participants and which is easy to construct.</s><s>Two scales (positive and negative) that are most relevant to the research were assessed and selected.</s><s>The assessment was performed for four basic negative affects: fear, sadness, guilt, and hostility, and three basic positive affects: joviality, self-assurance, and attentiveness as classified by the PANAS-X model at the end of each 6-s-played-sound <ref type="bibr" target="#b48">(Watson and Clark, 1999)</ref>.</s><s>The average of each scale was calculated: basic positive affect = (joviality + self-assurance + attentiveness)/3, basic negative affect = (fear + sadness + guilt + hostility)/4.</s><s>The choice of neutral was made available in case the stimuli failed to trigger any of the participant's emotions.</s></p><p><s>Participants were asked to rate honestly the sound heard in terms of how it made them feel, considering there is no wrong or right answer.</s><s>They rated the 20 sounds using five rating scales ranging from very slightly to extremely felt emotions.</s><s>In order to ensure participants comfort in giving the ratings, they were  First, a preparation sound for 3 s with instruction for participants to get ready for the next sound was displayed (=baseline phase).</s><s>Then, the sound was presented for 6 s (=stimulus presentation phase).</s><s>After stimulus presentation, rating period starts and lasts 21 s.</s><s>Then, a relaxation phase for 3 s starts to allow participants to blink and provide sufficient time for pupil diameter (PD) to get back to normal diameter (=relaxation phase).</s></p><p><s>exposed to three trials (door bell, buzzer, and baby sound) at the beginning of the experiment to familiarize them with task and experimental set-up.</s><s>All participants completed all the played sounds in the specified time.</s></p><p><s>In the second dataset, the experiment took place in a quiet, moderately illuminated room (about 500 lux).</s><s>The participants were seated comfortably in front of a computer screen with a distance of approximately 70 cm.</s><s>Participants had to press one of two buttons that are labeled with "yes" or "no" as quickly and correctly as possible in a response device to rate the aforementioned four condition.</s><s>Immediately after the experiment, the rating was done in two stages for single word pairs: first all word pairs were rated regarding arousal, then they all were rated regarding emotional valence on seven-point rating scale starting with zero (very unpleasant or low arousal) to six (very pleasant or high arousal).</s><s>Each trial consisted of four phases as shown in Figure <ref type="figure" target="#fig_1">2</ref>.</s></p><p><s>• Baseline phase which is a fixation cross that appeared for 1 s.</s></p><p><s>• Stimulus presentation phase by the presentation of the item that consists of the two word pairs.</s><s>• Response of the subject by pressing"yes" or "no".</s></p><p><s>• Pupil relaxation phase with fixation cross for 5 s.</s></p><p><s>• Blinking phase indicated by a smiley face appeared on the screen.</s><s>• Next trial starts when subjects press any response button to end blinking phase.</s></p><p><s>The experiment contained 216 trials presented in randomized order and consisted of two blocks with a break inbetween.</s><s>Then, the item was presented (=stimulus presentation phase).</s><s>As soon as a response button was pressed by the participant, the item disappeared from the screen, followed by another fixation cross for 5 s (=relaxation phase).</s><s>After relaxation phase, a smiley appeared on the screen indicating that participants were now allowed to blink and could start the next trial by pressing one of the response buttons (=blinking phase).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Acquisition</head><p><s>For the first dataset, a computer system was utilized to control timing, stimulation, and instructions presentation with total experiment time of approximately 12 min.</s><s>Data of PD was recorded using the Tobii TX300 eye-tracking system that measures eye movement such as eye gaze: fixation and saccades.</s><s>The system allows large head movement while maintaining the accuracy and precision.</s><s>Pupillary response was sampled at 300 Hz (recording pupil size every 3.3 ms).</s></p><p><s>In the second dataset, the PD of the right eye was recorded using an iView system (SensoMotoric Instruments, Teltow, Germany) at 50 Hz sampling rate (i.e., every 20 ms).</s><s>Stimulations were presented using the experimental control software presentation (Neurobehavioral System Inc, Albany, CA, USA) running on a Microsoft Windows XP operating system.</s><s>The computer used for stimulus presentation collected the behavioral data (response times and error rates) and was connected with another computer for registration and storage of the pupil data for offline analyses.</s><s>The iView system samples PD in terms of pixels.</s><s>Thus, to convert PD from pixels to millimeters for each participant, a black dot of 5 mm was placed on the closed lid of participant's right eye before the experiment.</s><s>PD was measured with accuracy of 0.05 mm.</s><s>For more details, see <ref type="bibr" target="#b36">(Prehn et al., 2011)</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Analyses</head><p><s>The data obtained from Tobii TX300 eye-tracking system (first dataset) were PD, fixation, stimulus onset and offset and a validation code that determines the validity of PD data.</s><s>The data contained values of both right and left pupils.</s><s>Both pupils showed the same behavior in all subjects (p = 0.001) and the average of these values was taken to ease data processing.</s><s>The baseline for each participant's PD for each trial was determined by an average of 3 s before stimulus onset.</s></p><p><s>All corrupted pupil size data associated with eye blink regions or caused by subject's head or pupil movement were removed and trials with over 50% of missing data were eliminated.</s><s>The rest of the trials that had over 20% of missing data were filled in using linear interpolation <ref type="bibr" target="#b42">(Schlomer et al., 2010)</ref>.</s><s>A moving-average filter with span of 7 was then applied to clean the data, improve signal to noise ratio and remove outliers.</s></p><p><s>The second dataset was preprocessed in almost the same way.</s><s>However, in this dataset the baseline was calculated differently.</s><s>To compute the baseline for each trial, an average of PD of 200 ms before presentation of the item was subtracted from the respective trial (baseline correction).</s><s>Moreover, a calibration procedure was used to account for PD differences between subjects.</s><s>This calibration was performed by placing a black dot of 5 mm on the closed lid of participant's right eye before the experiment.</s><s>As also done in the first dataset, trials with over 50% of missing data were eliminated and a moving-average filter with span of 7 was applied to clean the data and remove outliers.</s></p><p><s>The determination of the peak dilation was based on average pupillary responses for all participants rather than individual trials for each participant and each condition because the response of PD is prone to spontaneous fluctuations.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESULTS</head><p><s>This part is divided into two main sections.</s><s>The first section, analyzes subjective rating data of the two datasets (subjective data).</s><s>The second section investigates the possibility of using a part of the signal (last second) to distinguish between positive and negative stimuli on the basis of pupillary responses (objective data).</s><s>It also introduces an optimum classification system to differentiate between two classes: positive and negative emotions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subjective Data</head><p><s>In the first dataset, rating of auditory stimuli was performed during the experiment.</s><s>There were some differences between IADS ratings and participant's ratings.</s><s>Based on the selfassessment report, some sounds were rated by some subjects as neutral though their ratings in IADS were highly pleasant.</s><s>Participant's ratings also differed from one to another.</s><s>For example, a sound was rated by the majority of subjects as negative while two subjects rated it positively.</s><s>There was also a difference between males and females in ratings as described earlier in <ref type="bibr" target="#b2">(Babiker et al., 2013)</ref>.</s><s>Females reported more intense subjective experiences than males, particularly when rating negative stimuli.</s></p><p><s>In the second dataset, the rating of word pairs was performed after the experiment.</s><s>Single word pairs with a neutral valence were rated as neutral (M = 3.05, SD = 0.20) and low-arousing (M = 1.18,</s><s>SD = 1.28) on the seven point rating scale from 0 (very unpleasant, low arousal) to 6 (very pleasant, high arousal).</s><s>Word pairs with a positive valence were rated as more pleasant (M = 4.35, SD = 0.60) and more arousing (M = 3.02, SD = 1.25); word pairs with a negative valence were rated as unpleasant (M = 0.85, SD = 0.51) and highly arousing (M = 3.97, SD = 1.01).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Objective Data</head><p><s>In the first dataset, there was no initial decrease in PD as shown in Figure <ref type="figure" target="#fig_2">3</ref>.</s><s>This is because luminance of the room was controlled and the stimuli used were sounds that do not affect the amount of light entering the pupil the way pictures do <ref type="bibr" target="#b10">(Bradley et al., 2008)</ref>.</s><s>Figure <ref type="figure" target="#fig_2">3</ref> shows 6 s of the difference between positive and negative emotional signals that depended on PANAS-X model ratings.</s><s>These 6 s show a change in PD right after stimulus onset.</s></p><p><s>From Figure <ref type="figure" target="#fig_2">3</ref> one can notice the slower, higher and more sustained pupillary response to negative sound stimuli compared to positive ones.</s><s>Dilation in both cases started almost 0.25 s after stimulus onset and reached the peak almost in 2.2 s after stimulus onset.</s><s>The highest point in dilation of negative emotions was 4.76 mm while in positive ones it was 4.66 mm.</s></p><p><s>In the second dataset, we had two conditions.</s><s>In condition 1 (Con = Emo=) emotional and conceptual relations corresponded whereas in condition 2 (Con =Emo=) only the emotional relations corresponded.</s><s>Figures <ref type="figure" target="#fig_4">4 and 5</ref> below show the smoothed averaged response for both conditions.</s></p><p><s>A notable feature in these two figures is the initial decrease in PD in the 1 st and 2 nd second.</s><s>This is because luminance increment leads to initial decrease in PD.</s><s>Figures <ref type="figure" target="#fig_4">4 and 5</ref> show 5 s of the difference between positive and negative emotional signals.</s></p><p><s>Dilation in both cases started almost 2.2 s after stimulus onset and reached the peak almost in 4.1 s after stimulus onset.</s><s>The highest point in dilation of negative emotions reached 3.88 mm while in positive ones it was 3.8 mm in condition 1 (Con = Emo=) while in condition 2 (Con =Emo=) the highest point of negative emotion was 3.82 mm and 3.74 s mm in positive emotion.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results Using Last Second of Emotional Signal</head><p><s>It can be noticed from Figures <ref type="figure" target="#fig_4">3-5</ref> that the difference between positive and negative signals increases with time.</s><s>Consequently, the last portions of emotional signals of normal subjects were investigated for carrying the significant differences.</s></p><p><s>Repeated measures ANOVA was applied to check the difference between positive and negative signals.</s><s>In the first dataset, the difference was significant at 5% significant level [F(1,179) = 34.066,</s><s>p &lt; 0.001].</s><s>In the second dataset, the first second in both conditions were removed due to the effect of luminance increment that led to initial decrease in PD as mentioned earlier.</s><s>In condition 1 (Con = Emo=) the difference was found significant between the two signals</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>k-Nearest Neighbor (kNN)</head><p><s>To classify positive and negative emotions, k-nearest neighbor (kNN) was applied.</s><s>It uses non-parametric density estimation, which means no functional form is assumed and the density estimates is driven only by the training data.</s><s>Non-parametric density estimation is preferred because it fits better the actual densities encountered in practice.</s><s>In addition, k-nearest neighbor is analytically tractable and simple to implement <ref type="bibr" target="#b23">(Kozma, 2008)</ref>.</s></p><p><s>Using kNN, the data were divided into training set (labeled samples) and testing set (unlabeled samples) that is used to test the trained classifier.</s><s>The training set contained 70% of randomly selected samples from each dataset and the other 30% was assigned to the testing set.</s><s>The label of new instant (unlabeled sample) from the testing set is decided based on the k closest training samples in the feature space that contained the training set.</s><s>This gives a better opportunity for the new instant to be correctly classified.</s><s>The value k = 2 was empirically used in this work.</s><s>The Euclidean distance is used to measure the proximity of instances as it follows n i=1</s></p><formula xml:id="formula_0">(x i − y i ) 2 (1.1)</formula><p><s>where x = unlabeled sample, y = labeled sample and n = the number of features.</s><s>Six mathematical features were extracted from the PD.</s><s>These features were discussed in previous studies (e.g., <ref type="bibr" target="#b37">Qian et al., 2009)</ref> and were chosen experimentally.</s><s>Last seconds in both datasets were used to obtain these six features.</s><s>Dilation of pupil in response to emotional stimuli is suggested to start 400 ms after stimulus onset while the peak dilation is suggested to be reached at 2-3 s later <ref type="bibr" target="#b34">(Partala and Surakka, 2003;</ref><ref type="bibr" target="#b10">Bradley et al., 2008)</ref>.</s><s><ref type="bibr" target="#b19">Hess (1972)</ref> reported that pupil dilation in response to emotional stimuli occurs 2-7 s after stimulus onset.</s><s>Pupil also responds to mental workload and cognition effect about 1-2 s after onset demand <ref type="bibr" target="#b5">(Beatty, 1982a)</ref>.</s><s>The dilation of pupil persists if the demand is sustained <ref type="bibr" target="#b6">(Beatty, 1982b)</ref>.</s></p><p><s>We begin by defining a time series for the pupil dilation duration as following: time period of −1 s &lt; t &lt; 0 s is defined as the before-stimulus period T before, time period of 0 s &lt; t &lt; 6 or 5 s (6 s in the first dataset and 5 s in the second dataset) is defined as the after-stimulus period T after , a smaller time interval, i.e., last second after stimulus onset (6 th s in first dataset and 5 th s in second dataset), regarded as the period in which the pupillary response is most salient in terms of peak acceleration and peak velocity is called T critical .</s></p><p><s>The first feature is pupil dilation, which represents maximum pupil dilation after stimulus onset.</s><s>It is denoted as</s></p><formula xml:id="formula_1">max(D(t a )) (1.2)</formula><p><s>where a∈T after period.</s><s>The maximum accumulated velocity change between the time before stimulus onset and the time after stimulus onset is denoted as</s></p><formula xml:id="formula_2">max i=a i=0 V(t i ) − max ⎛ ⎝ j=b j=0 V(t j ) ⎞ ⎠ (1.3)</formula><p><s>where a∈T after and b∈T before.</s></p><p><s>The third feature is the maximum pupil velocity in time after the stimulus onset is subtracted from the mean velocity before stimulus onset.</s><s>It is denoted as</s></p><formula xml:id="formula_3">max(V(t a )) − mean (V(t b )) (1.4)</formula><p><s>where a∈T after and b∈T before.</s></p><p><s>The maximum points of positive and negative signals reached after stimulus onset for both positive and negative signals is denoted as.</s><s>max(D(t c ))</s></p><p><s>(1.5)</s></p><p><s>where c∈T critical period.</s><s>The fifth feature is the maximum pupil area after 0.2 s following stimulus onset subtracted from the mean pupil area before 0.2 s following stimulus onset.</s><s>It is denoted as</s></p><formula xml:id="formula_4">max(AR(t a )) − mean (AR(t b )) (1.6)</formula><p><s>where a∈T after and b∈T before.</s><s>Finally, the gradient in time after stimulus onset subtracted from the mean gradient before stimulus onset.</s><s>The gradient of a function of two variables, F(x,y) is defined as:</s></p><formula xml:id="formula_5">∇F = ∂F ∂x î + ∂F ∂y ĵ (1.7)</formula><p><s>Average signals were obtained for every participant's positive and negative responses.</s><s>Thus, 30 positive signals and 30 negative signals were used to construct a matrix of 60 × 6 where 60 represents positive and negative emotional signals (30 signals each) and six is the number of features described above.</s></p><p><s>The accuracy of the classification system is shown in Table <ref type="table" target="#tab_2">2</ref>.</s></p><p><s>From the results displayed in the table, this method has achieved high accuracy in classifying positive and negative emotions based on mathematical features.</s><s>The last second (6 th s in the first dataset and 5 th s in the second dataset) were used to obtain these features.</s><s>To further validate and analyze the performance of the proposed method, the sensitivity and specificity were calculated.</s><s>The sensitivity is defined in this context as the probability for a detected positive emotion to be positive while specificity is the probability for a detected negative emotion to be negative.</s><s>The following equations (Eq.1.8),</s><s>(Eq.1.9)</s><s>define sensitivity and specificity:</s></p><formula xml:id="formula_6">Sensitivity = TP (TP + FN) (1.8) Specificity = TN (TN + FN) (1.9)</formula><p><s>where TP (True Positives) is the number of Positive emotions that are correctly detected as positives emotions, TN (True Negatives)  is the number of Negative emotions that are correctly detected as negatives emotions.</s><s>FP (False Positives) is the number of Positive emotions that are incorrectly detected as negative emotions.</s></p><p><s>FN (False Negatives) is the number of Negative emotions that incorrectly detected as positive emotions.</s></p><p><s>Tables <ref type="table" target="#tab_4">3-5</ref> list TP, TN, FP, and FN for each dataset and Table <ref type="table" target="#tab_5">6</ref> summarizes the sensitivity and specificity of the algorithm performances.</s><s>Note that the role of positive and negative emotions could be interchanged in these definitions.</s><s>It would just result in interchanging specificity and sensitivity.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSION</head><p><s>There is a well-recognized need to improve interaction between humans and computers through emotion recognition.</s><s>In this study, PD was used to discriminate between positive and negative emotions.</s><s>Using PD has important advantages over other physiological signals as described earlier.</s><s>Furthermore, the technology necessary for accurate measurement is relatively simple to use with improved accuracy and enhanced sampling rate.</s></p><p><s>Subjective data of the first dataset contained sound's ratings.</s><s>Some ratings of the subjects tested in the present study differed from normative IADS ratings.</s><s>For instance, rain sound (pleasure M = 4.83, arousal M = 4.65; IADS; <ref type="bibr" target="#b8">Bradley and Lang, 1999)</ref> should be negative while some participants rated it positively.</s><s>This is firstly, caused by participant's different experiences and backgrounds <ref type="bibr" target="#b17">(Gross and John, 2003)</ref>.</s><s>The second possible cause is the high arousal of the stimuli used (M = 5.9, SD = 1.99).</s><s>It is suggested that emotional arousal plays an important role for an event's memorability <ref type="bibr" target="#b13">(Christianson, 1992;</ref><ref type="bibr" target="#b27">Libkuman et al., 1999)</ref> and some of these sounds, if not all, were experienced by subjects some time in their life.</s><s>Emotional arousal was also found to enhance memory performance <ref type="bibr" target="#b11">(Brown and Kulik, 1977)</ref>, that is, emotionally arousing events are better remembered than non-emotional ones.</s><s>When an arousing event occurs, an individual -in some cases-tend to link this old event with the newly evoked emotion, i.e., the emotion evoked during the experiment, which explains to some extent the difference between subjects in rating the same sound.</s><s>A third important factor is that emotion consisted of infinite number of overlapping behaviors and cognition <ref type="bibr" target="#b33">(Olsson, 2003)</ref> and that each individual has a unique behavior and mental processing, e.g., thinking, problem solving, knowing, etc.</s><s>So the stimuli presented to participants in this experiment, were subjected to the different mental processing of each participant which might have contributed to differences in ratings.</s></p><p><s>The paper introduced an algorithm to classify positive and negative emotions using only the last portions of pupil dilation signal.</s><s>The technique is used to differentiate between positive and negative experience in subjects when exposed to sound or word stimuli or at least between different evaluations of these stimuli <ref type="bibr" target="#b41">(Scherer, 2009)</ref>.</s><s>At the beginning of the signal, some time was needed for the emotional affect to be reflected in PD.</s><s>Then, a clear dilation of PD was detected while processing both positive and negative stimuli.</s><s>Interestingly, average of negative emotional signals had higher pupil dilation than positive ones.</s><s>After that, negative emotional signal maintained its dilation with slight changes while positive emotional signal decayed with more irregularity.</s><s>Results showed that positive and negative emotional signals normally follow the same trend but starting with the third and/or fourth second, difference between the two signals increases.</s><s>The main advantage of utilizing the last portions of a signal is to decrease classification time and reduce system complexity while keeping system performance.</s><s>Another advantage is that effect on PD at earlier seconds might be biased by luminance or stimulus presentation while at last seconds this effect becomes more sustained and can be related solely to individual's emotion.</s></p><p><s>The k-nearest neighbor algorithm was applied to each dataset separately to detect changes and classify positive and negative emotional signals.</s><s>The algorithm achieved high accuracy within a short time yielding a reliable emotion recognition system.</s><s>In the second dataset, condition 1 (Con = Emo=) achieved slightly higher accuracy -insignificant (≤1%)than the first dataset.</s><s>This might be caused by the type of stimuli.</s><s>Specifically, in the second dataset, the word pairs used had both conceptual and emotional correspondence or only emotional correspondence that required some levels of mental work load which affect pupil dilation as mentioned earlier.</s><s>This was not the case in the first dataset.</s></p><p><s>Nevertheless, the difference in accuracy remained insignificant between the two datasets regardless of the difference in experimental tasks, because we, however, only analyzed data from two conditions in which a decision whether an analogy was given or not could be reached by identifying emotional relations.</s><s>Since emotional relations can be retrieved automatically from long-term memory, the cognitive demand should be comparable <ref type="bibr" target="#b46">(Van der Meer, 1989;</ref><ref type="bibr">Sachs et al., 2008a,b)</ref>.</s></p><p><s>In Table <ref type="table" target="#tab_5">6</ref>, it is noted that in both datasets sensitivity and specificity values were above 97%.</s><s>This indicates that the proposed method performed very well in distinguishing positive emotions (True Positive) and negative emotions (True Negatives).</s><s>Specificity analysis showed a high rate in both datasets due to the low number of false negatives detected.</s><s>This is probably explained by the high arousal in both datasets since arousal affects PD <ref type="bibr" target="#b34">(Partala and Surakka, 2003;</ref><ref type="bibr" target="#b10">Bradley et al., 2008)</ref>.</s></p><p><s>The obtained results thus far indicate that the proposed method is capable of differentiating between positive and negative emotions by utilizing the last second of a stimulation period.</s><s>The results support the claim that the last portions of emotional signal are responsible for a bulk of significant differences between positive and negative responses and show the feasibility of applying machine learning algorithms to pupillary responses to classify emotions with high accuracy.</s><s>However, the proposed method has some limitations.</s><s>It distinguishes accurately between averaged positive and negative emotions but it is not tested in distinguishing singular or specific emotions.</s><s>It also applies kNN classifier that depends on sample size.</s><s>Sample size should be chosen carefully because very large sample size slows down the performance whereas small sample size reduces the accuracy <ref type="bibr" target="#b7">(Bhatia and Vandana, 2010)</ref>.</s><s>Hence, the results of using this method should be replicated using data from other experiments with bigger sample sizes.</s><s>Although the classifier worked comparably using data acquired during a passive listening task and higher cognitive analogical reasoning, further research is needed to validate the classifier with different stimulus material and tasks involving a different amount of cognitive demand.</s><s>In particular, it is necessary to demonstrate that the proposed method can successfully identify emotional valence in different datasets that are both similar and different in terms of the cognitive demand, both using emotionally charged stimuli.</s></p><p><s>The results also support the claim that pupil dilation is a good index of individuals' emotional states and that it dilates with respect to emotional states regardless of whether emotions have positive or negative valence.</s><s>There is significant difference between negative and positive emotions in terms of sustainability and dilation diameter that gets clearer at the end of the signal.</s></p><p><s>Both datasets had significant differences in valence and arousal (according to IADS in the first dataset; self-report ratings in the second dataset), Therefore, it is unclear whether changes in pupil dilation are related to valence or arousal as both concepts are not independent from each other.</s><s>We also cannot exclude the possibility that the method classifies pupil responses based on emotion arousal and/or valence.</s></p><p><s>Finally, the results of this study suggest that pupil could be used for a real time emotion recognition system that can facilitate human-computer interaction.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 |</head><label>1</label><figDesc><div><p><s>FIGURE 1 | Schematic illustration of an experimental trial for the first dataset.</s><s>First, a preparation sound for 3 s with instruction for participants to get ready for the next sound was displayed (=baseline phase).</s><s>Then, the sound was presented for 6 s (=stimulus presentation phase).</s><s>After stimulus presentation, rating period starts and lasts 21 s.</s><s>Then, a relaxation phase for 3 s starts to allow participants to blink and provide sufficient time for pupil diameter (PD) to get back to normal diameter (=relaxation phase).</s></p></div></figDesc><graphic coords="4,52.58,68.79,229.30,195.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 |</head><label>2</label><figDesc><div><p><s>FIGURE 2 | Schematic illustration of an experimental trial for the second dataset.</s><s>First, a fixation cross appeared for 1 s (=baseline phase).</s><s>Then, the item was presented (=stimulus presentation phase).</s><s>As soon as a response button was pressed by the participant, the item disappeared from the screen, followed by another fixation cross for 5 s (=relaxation phase).</s><s>After relaxation phase, a smiley appeared on the screen indicating that participants were now allowed to blink and could start the next trial by pressing one of the response buttons (=blinking phase).</s></p></div></figDesc><graphic coords="4,308.90,68.75,234.00,193.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 3 |</head><label>3</label><figDesc><div><p><s>FIGURE 3 | Averaged pupillary responses for the first dataset (sound stimulation) with positive and negative evaluation.</s></p></div></figDesc><graphic coords="5,315.98,509.98,219.90,164.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 4 |</head><label>4</label><figDesc><div><p><s>FIGURE 4 | Averaged pupillary responses for condition 1 (Con = Emo=) of the second dataset (word pairs stimulation) with positive and negative evaluation.</s></p></div></figDesc><graphic coords="6,50.30,68.63,234.00,158.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 5 |</head><label>5</label><figDesc><div><p><s>FIGURE 5 | Averaged pupillary responses for condition 2 (Con =Emo=) of second dataset (word pairs stimulation) with positive and negative evaluation.</s></p></div></figDesc><graphic coords="6,50.30,298.07,234.00,158.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc><div><p><s>[F(1,119) = 23.78,</s><s>p &lt; 0.001] and also in condition 2 (Con =Emo=) [F(1,119) = 4.199, p = 0.043].</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><div><p><s>. It has several advantages over other traditional approaches such as simple implementation, wide range of parameter's choice, and model freeness.</s><s>In kNN, the data is divided into a training set (labeled examples) and a testing set (unlabeled examples).</s><s>A new instant (unlabeled example) is classified based on its similarity with the examples in the training set.</s></p></div></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 |</head><label>1</label><figDesc><div><p><s>Examples for word material used in second dataset.</s></p></div></figDesc><table /><note><p><s>Emotional relations = Emotional relations = Conceptual relations = TUMOR -BRAIN/RAT -CELLAR n = 108 CANCER -BREAST/SHELL -BEACH n = 36 Conceptual relations = COCKROACH -KITCHEN/BODY -DECAY n = 36 MURDERER -PARK/BIRD -CHIRP n = 36 Emotional relation =, emotional relations corresponding; Emotional relations =, emotional relations non-corresponding; Conceptual relations =, conceptual relations corresponding; Conceptual relations =, conceptual relations non-corresponding.</s></p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 |</head><label>2</label><figDesc><div><p><s>Classification accuracy using kNN algorithm.</s></p></div></figDesc><table><row><cell>Signal</cell><cell></cell><cell>Accuracy</cell><cell></cell></row><row><cell></cell><cell>First dataset</cell><cell>Condition_0</cell><cell>Condition_2</cell></row><row><cell>Features of Table 2</cell><cell>96.5%</cell><cell>97%</cell><cell>96%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 |</head><label>3</label><figDesc><div><p><s>Confusion matrix of the first dataset.</s></p></div></figDesc><table><row><cell>True positives</cell><cell>False positives</cell></row><row><cell>95</cell><cell>5</cell></row><row><cell>False negatives</cell><cell>True negatives</cell></row><row><cell>2</cell><cell>98</cell></row><row><cell cols="2">TABLE 4 | Confusion matrix of condition 1 (Con = Emo=).</cell></row><row><cell>True positives</cell><cell>False positives</cell></row><row><cell>97</cell><cell>3</cell></row><row><cell>False negatives</cell><cell>True negatives</cell></row><row><cell>3</cell><cell>97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5 |</head><label>5</label><figDesc><div><p><s>Confusion matrix of condition 2 (Con =Emo=).</s></p></div></figDesc><table><row><cell>True positives</cell><cell>False positives</cell></row><row><cell>94</cell><cell>6</cell></row><row><cell>False negatives</cell><cell>True negatives</cell></row><row><cell>2</cell><cell>98</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6 |</head><label>6</label><figDesc><div><p><s>Sensitivity and specificity of the two datasets.</s></p></div></figDesc><table><row><cell>Dataset</cell><cell>Sensitivity</cell><cell>Specificity</cell></row><row><cell>First dataset</cell><cell>97.93%</cell><cell>98%</cell></row><row><cell>Condition 1 (Con = Emo=)</cell><cell>97%</cell><cell>97%</cell></row><row><cell>condition 2 (Con =Emo=)</cell><cell>97.9%</cell><cell>98%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Frontiers in Psychology | www.frontiersin.org 1 December 2015 | Volume 6 | Article 1921</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Frontiers in Psychology | www.frontiersin.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">December 2015 | Volume 6 | Article 1921</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The library numbers for the IADS stimuli used in the present study are:102, 110,  170, 224, 225, 261, 275, 278, 365, 378, 380, 400, 420, 626, 627, 712, 728, 730, 812,  817.   </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p><s>This research is supported by the HiCoE grant for CISIR (0153CA-002) from the Ministry of Education (MoE) of Malaysia.</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest Statement:</head><p><s>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A landmark study finds that when we look at sad faces, the size of the pupil we look at influences the size of our own pupil</title>
		<author>
			<persName><forename type="first">R</forename><surname>Adolphs</surname></persName>
		</author>
		<idno type="DOI">10.1093/scan/nsl011</idno>
	</analytic>
	<monogr>
		<title level="j">Soc. Cogn. Affect. Neurosci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="4" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pupillary response and behavior</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Andreassi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychophysiology: Human Behavior &amp; Physiological Response</title>
				<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Andreassi</surname></persName>
		</editor>
		<meeting><address><addrLine>Mahwah, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum Associates, Inc</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="289" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Non-conscious behavior in emotion recognition: gender effect</title>
		<author>
			<persName><forename type="first">A</forename><surname>Babiker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Faye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE 9th International Colloquium on Signal Processing and its Applications</title>
				<meeting>the 2013 IEEE 9th International Colloquium on Signal Processing and its Applications<address><addrLine>Kuala Lumpur</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="page" from="258" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discrete emotions or dimensions? The role of valence focus and arousal focus</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
		<idno type="DOI">10.1080/026999398379574</idno>
	</analytic>
	<monogr>
		<title level="j">Cogn. Emot</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="579" to="599" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Real time face detection and facial expression recognition: development and applications to human computer interaction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference on Computer Vision and Pattern Recognition Workshop</title>
				<meeting>the 2003 Conference on Computer Vision and Pattern Recognition Workshop<address><addrLine>Madison, WI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Task-evoked pupillary responses, processing load, and the structure of processing resources</title>
		<author>
			<persName><forename type="first">J</forename><surname>Beatty</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-2909.91.2.276</idno>
	</analytic>
	<monogr>
		<title level="j">Psychol. Bull</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="276" to="292" />
			<date type="published" when="1982">1982a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Phasic not tonic pupillary responses vary with auditory vigilance performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Beatty</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1469-8986.1982.tb02540.x</idno>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="167" to="172" />
			<date type="published" when="1982">1982b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Survey of nearest neighbor techniques</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vandana</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Sci. Inf. Secur</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="302" to="305" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">International Affective Digitized Sounds: Affective Ratings of Sounds and Insturction Manual</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<idno>B-3</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><surname>Edn</surname></persName>
		</author>
		<author>
			<persName><surname>Gainesville</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>FL: University of Florida</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pupil as a measure of emotional arousal and autonomic activation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Miccoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Escrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1469-8986.2008.00654.x</idno>
	</analytic>
	<monogr>
		<title level="j">Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="602" to="607" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Flushbulb memories</title>
		<author>
			<persName><forename type="first">R</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kulik</surname></persName>
		</author>
		<idno type="DOI">10.1016/0010-0277(77)90018-X</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="73" to="99" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A sensitive and specific neural signature for pictureinduced negative affect</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Gianaros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Manuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Wager</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pbio.1002180</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">e1002180</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Emotional stress and eyewitness memeory: a critical review</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Christianson</surname></persName>
		</author>
		<idno type="DOI">10.1037/0033-2909.112.2.284</idno>
	</analytic>
	<monogr>
		<title level="j">Psychol. Bull</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="248" to="309" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unmasking the Face: A Guide to Recognizing Emotions from Facial Expressions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Malor Books</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The structure of current affect: controversies and emerging consensus</title>
		<author>
			<persName><forename type="first">Feldman</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename></persName>
		</author>
		<idno type="DOI">10.1111/1467-8721.00003</idno>
	</analytic>
	<monogr>
		<title level="j">Curr. Dir. Psychol. Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="10" to="14" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Infant pupil diameter changes in response to others&apos; positive and negative emotions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Geangu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bentz</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0027132</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">e27132</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Individual differences in two emotion regulation processes: implication for affect, relationships, and well-being</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">P</forename><surname>John</surname></persName>
		</author>
		<idno type="DOI">10.1037/0022-3514.85.2.348</idno>
	</analytic>
	<monogr>
		<title level="j">J. Pers. Soc. Psychol</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="348" to="362" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The emotion recognition system based on autoregressive model and sequential forward featre selection of electroencephalogram signals</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hatamikia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maghooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Med. Signals Sens</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="194" to="201" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pupillometrics: a method of studying mental, emotional and sensory processes</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Hess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Psychophysiology</title>
				<imprint>
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Greenfield</surname></persName>
		</author>
		<author>
			<persName><surname>Sternbach</surname></persName>
		</author>
		<imprint>
			<publisher>Rinehart and Winston Inc</publisher>
			<biblScope unit="page" from="491" to="531" />
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pupil size as related to interest value of visual stimuli</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Polt</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.132.3423.349</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="349" to="350" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pupil size, affect and exposure frequency</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Janisse</surname></persName>
		</author>
		<idno type="DOI">10.2224/sbp.1974.2.2.125</idno>
	</analytic>
	<monogr>
		<title level="j">Soc. Behav. Pers</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="125" to="146" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Helsinki University of Technology, Special Course in Computer and Information Science</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kozma</surname></persName>
		</author>
		<ptr target="http://www.lkozma.net/knn2.pdf" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>k Nearest Neighbors algorithm (kNN)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mulitvariate pattern classification reveals autnomic and experiential representations of discrete emotion</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Kragel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Labar</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0031820</idno>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="681" to="690" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Committee neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomed. Eng. Online</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Eye tracking and pupil size variation as response to affective stimuli: a preliminary study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lanata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Armato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valenza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Scilingo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Pervasive Computing Technologies for Healthcare</title>
				<meeting>the 5th International Conference on Pervasive Computing Technologies for Healthcare<address><addrLine>Dublin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="78" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Source of arousal and memory for detail</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Libkuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nichols-Whitehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Griffith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<idno type="DOI">10.1080/09658210244000630</idno>
	</analytic>
	<monogr>
		<title level="j">Mem. Cogn</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="237" to="247" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MAUI: a multimodal affective user interface</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lisetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Multimedia</title>
				<meeting>the Tenth ACM International Conference on Multimedia<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using noninvasive wearable computers to recognize human emotions from physiological signals</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Lisetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nasoz</surname></persName>
		</author>
		<idno type="DOI">10.1155/S1110865704406192</idno>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Appl. Signal Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1672" to="1687" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Artificial life meets entertainment: lifelike autonomous agents</title>
		<author>
			<persName><forename type="first">P</forename><surname>Maes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Communications of the ACM</title>
				<meeting>the Communications of the ACM<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="108" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Emotion recognition using kNN classification for user modelingand sharing of affect states</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Meftah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Le Thanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Amar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing</title>
				<editor>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Leung</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg; Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="234" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Classification of human emotion from EEG using discrete wavelet transform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Murugappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sazali</surname></persName>
		</author>
		<idno type="DOI">10.4236/jbise.2010.34054</idno>
	</analytic>
	<monogr>
		<title level="j">J. Biomed. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="390" to="396" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Emotion and motivation in learning: current research, future directions, and practical implications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Olsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lund Univ. Cogn. Stud</title>
		<imprint>
			<biblScope unit="page" from="6" to="8" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pupil size variation as an indication of affective processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Partala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Surakka</surname></persName>
		</author>
		<idno type="DOI">10.1016/S1071-5819(03</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Hum. Comput. Stud</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic stress classification with pupil diameter analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pedrotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mirzaeic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tedescod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Chardonnetc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mériennec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Benedettobe</surname></persName>
		</author>
		<idno type="DOI">10.1080/10447318.2013.848320</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Hum. Comput. Interact</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Influence of affective significance on different levels of processing using pupil dilation in an analogical reasoning task</title>
		<author>
			<persName><forename type="first">K</forename><surname>Prehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Heekeren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Van Der Meer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijpsycho.2010.10.014</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Psychophysiol</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="236" to="243" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Decision-level fusion of EEG and pupil features for singletrial visual detection analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Zachery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Privitera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Carney</surname></persName>
		</author>
		<idno type="DOI">10.1109/TBME.2009.2016670</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="1929" to="1937" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Affective assessment of computer users based on processing the pupil diameter signal</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adjouadi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<idno type="DOI">10.1109/IEMBS.2011.6090716</idno>
	</analytic>
	<monogr>
		<title level="j">Conf. Proc. IEEE Eng. Med. Biol. Soc</title>
		<imprint>
			<biblScope unit="page" from="2594" to="2597" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Categorical and thematic knowledge representation in the brain: neural correlates of taxonomic and thematic conceptual relations</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kircher</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuropsychologia.2007.08.015</idno>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="409" to="418" />
			<date type="published" when="2008">2008a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automatic processing of semantic relations in fMRI: neural activation during semantic priming of taxonomic and thematic categories</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zellagui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zvyagintsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mathiak</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.brainres.2008.03.045</idno>
	</analytic>
	<monogr>
		<title level="j">Brain Res</title>
		<imprint>
			<biblScope unit="volume">1218</biblScope>
			<biblScope unit="page" from="194" to="205" />
			<date type="published" when="2008">2008b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Emotions are emergent processes: they require a dynamic computational architecture</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.2009.0141</idno>
	</analytic>
	<monogr>
		<title level="j">Philos. Trans. R. Soc. Lond. B Biol. Sci</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="page" from="3459" to="3474" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Best practices for missing data management in counseling psychology</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Schlomer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Card</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0018082</idno>
	</analytic>
	<monogr>
		<title level="j">Couns. Psychol</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Modulation of neutral face evaluation by laterally presented emotional experessions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Surakka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H J</forename><surname>Sams</surname></persName>
		</author>
		<idno type="DOI">10.2466/pms.1999.88.2.595</idno>
	</analytic>
	<monogr>
		<title level="j">Percept. Mot. Skills</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="595" to="606" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The emotional machine: a machine learning approach to online prediction of user&apos;s emotion and intensity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frasson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advanced Learning Technologies (ICALT)</title>
				<meeting>the Advanced Learning Technologies (ICALT)<address><addrLine>Sousse</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="613" to="617" />
		</imprint>
	</monogr>
	<note>IEEE 10th International Conference</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Inferencing emotions through the triangulation of pupil size data, facial heuristics and self-assessment techniques</title>
		<author>
			<persName><forename type="first">L</forename><surname>Valverde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De Lera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Moblile, Hybrid and On-Line Learning</title>
				<meeting>the 2nd International Conference on Moblile, Hybrid and On-Line Learning<address><addrLine>Saint Maarten</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Impacts of emotions on conceptual structures</title>
		<author>
			<persName><forename type="first">E</forename><surname>Van Der Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the XXIV International Congress of Psychology: Cognition in Individual and Social Contexts</title>
				<editor>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Bennett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Mcconkey</surname></persName>
		</editor>
		<meeting>the XXIV International Congress of Psychology: Cognition in Individual and Social Contexts<address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pupil dilation and eye-tracking</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Process Tracing Methods for Decision Research: A Critical Review and User&apos;s Guide</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Schulte-Mecklenbeck</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Kuhberger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Ranyard</surname></persName>
		</editor>
		<meeting><address><addrLine>Hove</addrLine></address></meeting>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">The PANAS-X: Manual for the Positive and Negative Affect Schedule -Expanded Form</title>
		<author>
			<persName><forename type="first">D</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Iowa City, IA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Iowa</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The pupil reaction as an index of positive and negative affect. Paper Presented at the Meeting of the</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Woodmansee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
			<publisher>American Psychological Association</publisher>
			<pubPlace>Washington, DC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">An Introduction to Psychology</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wundt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1924">1924</date>
			<publisher>Allen &amp; Unwin</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Emotion and action</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thagard</surname></persName>
		</author>
		<idno type="DOI">10.1080/09515080120109397</idno>
	</analytic>
	<monogr>
		<title level="j">Philos. Psychol</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
